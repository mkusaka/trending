<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-01T01:28:50Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>deepseek-ai/DeepSeek-Coder-V2</title>
    <updated>2025-01-01T01:28:50Z</updated>
    <id>tag:github.com,2025-01-01:/deepseek-ai/DeepSeek-Coder-V2</id>
    <link href="https://github.com/deepseek-ai/DeepSeek-Coder-V2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/logo.svg?raw=true&#34; width=&#34;60%&#34; alt=&#34;DeepSeek-V2&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;line-height: 1;&#34;&gt; &#xA; &lt;a href=&#34;https://www.deepseek.com/&#34; target=&#34;_blank&#34; style=&#34;margin: 2px;&#34;&gt; &lt;img alt=&#34;Homepage&#34; src=&#34;https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/badge.svg?raw=true&#34; style=&#34;display: inline-block; vertical-align: middle;&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://chat.deepseek.com/&#34; target=&#34;_blank&#34; style=&#34;margin: 2px;&#34;&gt; &lt;img alt=&#34;Chat&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%96%2520Chat-DeepSeek%2520V2-536af5?color=536af5&amp;amp;logoColor=white&#34; style=&#34;display: inline-block; vertical-align: middle;&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/deepseek-ai&#34; target=&#34;_blank&#34; style=&#34;margin: 2px;&#34;&gt; &lt;img alt=&#34;Hugging Face&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;amp;logoColor=white&#34; style=&#34;display: inline-block; vertical-align: middle;&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;line-height: 1;&#34;&gt; &#xA; &lt;a href=&#34;https://discord.gg/Tc7c45Zzu5&#34; target=&#34;_blank&#34; style=&#34;margin: 2px;&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da&#34; style=&#34;display: inline-block; vertical-align: middle;&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/qr.jpeg?raw=true&#34; target=&#34;_blank&#34; style=&#34;margin: 2px;&#34;&gt; &lt;img alt=&#34;Wechat&#34; src=&#34;https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;amp;logoColor=white&#34; style=&#34;display: inline-block; vertical-align: middle;&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://twitter.com/deepseek_ai&#34; target=&#34;_blank&#34; style=&#34;margin: 2px;&#34;&gt; &lt;img alt=&#34;Twitter Follow&#34; src=&#34;https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;amp;logoColor=white&#34; style=&#34;display: inline-block; vertical-align: middle;&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;line-height: 1;&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-V2/raw/main/LICENSE-CODE&#34; style=&#34;margin: 2px;&#34;&gt; &lt;img alt=&#34;Code License&#34; src=&#34;https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;amp;color=f5de53&#34; style=&#34;display: inline-block; vertical-align: middle;&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-V2/raw/main/LICENSE-MODEL&#34; style=&#34;margin: 2px;&#34;&gt; &lt;img alt=&#34;Model License&#34; src=&#34;https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;amp;color=f5de53&#34; style=&#34;display: inline-block; vertical-align: middle;&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder-V2/main/#2-model-downloads&#34;&gt;Model Download&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder-V2/main/#3-evaluation-results&#34;&gt;Evaluation Results&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder-V2/main/#5-api-platform&#34;&gt;API Platform&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder-V2/main/#6-how-to-run-locally&#34;&gt;How to Use&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder-V2/main/#7-license&#34;&gt;License&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder-V2/main/#8-citation&#34;&gt;Citation&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/pdf/2406.11931&#34;&gt;&lt;b&gt;Paper Link&lt;/b&gt;üëÅÔ∏è&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence&lt;/h1&gt; &#xA;&lt;h2&gt;1. Introduction&lt;/h2&gt; &#xA;&lt;p&gt;We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;100%&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder-V2/main/figures/performance.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks. The list of supported programming languages can be found &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder-V2/main/supported_langs.txt&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;2. Model Downloads&lt;/h2&gt; &#xA;&lt;p&gt;We release the DeepSeek-Coder-V2 with 16B and 236B parameters based on the &lt;a href=&#34;https://arxiv.org/pdf/2401.06066&#34;&gt;DeepSeekMoE&lt;/a&gt; framework, which has actived parameters of only 2.4B and 21B , including base and instruct models, to the public.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;#Total Params&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;#Active Params&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Context Length&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DeepSeek-Coder-V2-Lite-Base&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;16B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2.4B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;128k&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Base&#34;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DeepSeek-Coder-V2-Lite-Instruct&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;16B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2.4B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;128k&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct&#34;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DeepSeek-Coder-V2-Base&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;236B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;21B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;128k&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Base&#34;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DeepSeek-Coder-V2-Instruct&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;236B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;21B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;128k&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct&#34;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;3. Evaluation Results&lt;/h2&gt; &#xA;&lt;h3&gt;3.1 Code Generation&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#TP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;HumanEval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MBPP+&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;LiveCodeBench&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;USACO&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Closed-Source Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Gemini-1.5-Pro&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;74.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Claude-3-Opus&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;GPT-4-Turbo-1106&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;GPT-4-Turbo-0409&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;45.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;GPT-4o-0513&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;91.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;18.8&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Open-Source Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;CodeStral&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;DeepSeek-Coder-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Llama3-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;DeepSeek-Coder-V2-Lite-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.4B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;DeepSeek-Coder-V2-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;236B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;90.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;76.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;43.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;12.1&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;3.2 Code Completion&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#TP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;RepoBench (Python)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;RepoBench (Java)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;HumanEval FIM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;CodeStral&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;46.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;45.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;DeepSeek-Coder-Base&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;DeepSeek-Coder-Base&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;86.4&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;DeepSeek-Coder-V2-Lite-Base&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.4B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;86.4&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;3.3 Code Fixing&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#TP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Defects4J&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;SWE-Bench&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Aider&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Closed-Source Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Gemini-1.5-Pro&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Claude-3-Opus&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;GPT-4-Turbo-1106&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;GPT-4-Turbo-0409&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;GPT-4o-0513&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;26.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;26.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;72.9&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Open-Source Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CodeStral&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;DeepSeek-Coder-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama3-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;DeepSeek-Coder-V2-Lite-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.4B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;DeepSeek-Coder-V2-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;236B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;21.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;12.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;73.7&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;3.4 Mathematical Reasoning&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#TP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GSM8K&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MATH&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AIME 2024&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Math Odyssey&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Closed-Source Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Gemini-1.5-Pro&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2/30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Claude-3-Opus&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;95.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2/30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;GPT-4-Turbo-1106&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;91.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1/30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;GPT-4-Turbo-0409&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;93.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;3/30&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;GPT-4o-0513&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;95.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;76.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2/30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;53.2&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Open-Source Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama3-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;93.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1/30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;DeepSeek-Coder-V2-Lite-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.4B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0/30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;DeepSeek-Coder-V2-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;236B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;94.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;75.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;4/30&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;53.7&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;3.5 General Natural Language&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Benchmark&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Domain&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DeepSeek-V2-Lite Chat&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DeepSeek-Coder-V2-Lite Instruct&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DeepSeek-V2 Chat&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DeepSeek-Coder-V2 Instruct&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;BBH&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;English&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;83.9&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;MMLU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;English&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;79.2&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;ARC-Easy&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;English&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;98.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;ARC-Challenge&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;English&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;92.8&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;TriviaQA&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;English&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;86.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;NaturalQuestions&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;English&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;53.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;AGIEval&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;English&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;61.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;CLUEWSC&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chinese&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;89.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;C-Eval&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chinese&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;79.4&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;CMMLU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chinese&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;81.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Arena-Hard&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;65.0&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;AlpaceEval 2.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;38.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;MT-Bench&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.37&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.81&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;8.97&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.77&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Alignbench&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.83&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;7.91&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.84&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;3.6 Context Window&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;80%&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder-V2/main/figures/long_context.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Evaluation results on the &lt;code&gt;Needle In A Haystack&lt;/code&gt; (NIAH) tests. DeepSeek-Coder-V2 performs well across all context window lengths up to &lt;strong&gt;128K&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;4. Chat Website&lt;/h2&gt; &#xA;&lt;p&gt;You can chat with the DeepSeek-Coder-V2 on DeepSeek&#39;s official website: &lt;a href=&#34;https://coder.deepseek.com/sign_in&#34;&gt;coder.deepseek.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;5. API Platform&lt;/h2&gt; &#xA;&lt;p&gt;We also provide OpenAI-Compatible API at DeepSeek Platform: &lt;a href=&#34;https://platform.deepseek.com/&#34;&gt;platform.deepseek.com&lt;/a&gt;, and you can also pay-as-you-go at an unbeatable price.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;40%&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder-V2/main/figures/model_price.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;6. How to run locally&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Here, we provide some examples of how to use DeepSeek-Coder-V2-Lite model. If you want to utilize DeepSeek-Coder-V2 in BF16 format for inference, 80GB*8 GPUs are required.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Inference with Huggingface&#39;s Transformers&lt;/h3&gt; &#xA;&lt;p&gt;You can directly employ &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Huggingface&#39;s Transformers&lt;/a&gt; for model inference.&lt;/p&gt; &#xA;&lt;h4&gt;Code Completion&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;import torch&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;deepseek-ai/DeepSeek-Coder-V2-Lite-Base&#34;, trust_remote_code=True)&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;deepseek-ai/DeepSeek-Coder-V2-Lite-Base&#34;, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()&#xA;input_text = &#34;#write a quick sort algorithm&#34;&#xA;inputs = tokenizer(input_text, return_tensors=&#34;pt&#34;).to(model.device)&#xA;outputs = model.generate(**inputs, max_length=128)&#xA;print(tokenizer.decode(outputs[0], skip_special_tokens=True))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Code Insertion&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;import torch&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;deepseek-ai/DeepSeek-Coder-V2-Lite-Base&#34;, trust_remote_code=True)&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;deepseek-ai/DeepSeek-Coder-V2-Lite-Base&#34;, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()&#xA;input_text = &#34;&#34;&#34;&amp;lt;ÔΩúfim‚ñÅbeginÔΩú&amp;gt;def quick_sort(arr):&#xA;    if len(arr) &amp;lt;= 1:&#xA;        return arr&#xA;    pivot = arr[0]&#xA;    left = []&#xA;    right = []&#xA;&amp;lt;ÔΩúfim‚ñÅholeÔΩú&amp;gt;&#xA;        if arr[i] &amp;lt; pivot:&#xA;            left.append(arr[i])&#xA;        else:&#xA;            right.append(arr[i])&#xA;    return quick_sort(left) + [pivot] + quick_sort(right)&amp;lt;ÔΩúfim‚ñÅendÔΩú&amp;gt;&#34;&#34;&#34;&#xA;inputs = tokenizer(input_text, return_tensors=&#34;pt&#34;).to(model.device)&#xA;outputs = model.generate(**inputs, max_length=128)&#xA;print(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(input_text):])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Chat Completion&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;import torch&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct&#34;, trust_remote_code=True)&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct&#34;, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()&#xA;messages=[&#xA;    { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#34;write a quick sort algorithm in python.&#34;}&#xA;]&#xA;inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=&#34;pt&#34;).to(model.device)&#xA;# tokenizer.eos_token_id is the id of &amp;lt;ÔΩúend‚ñÅof‚ñÅsentenceÔΩú&amp;gt; token&#xA;outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)&#xA;print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The complete chat template can be found within &lt;code&gt;tokenizer_config.json&lt;/code&gt; located in the huggingface model repository.&lt;/p&gt; &#xA;&lt;p&gt;An example of chat template is as belows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;lt;ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú&amp;gt;User: {user_message_1}&#xA;&#xA;Assistant: {assistant_message_1}&amp;lt;ÔΩúend‚ñÅof‚ñÅsentenceÔΩú&amp;gt;User: {user_message_2}&#xA;&#xA;Assistant:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also add an optional system message:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;lt;ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú&amp;gt;{system_message}&#xA;&#xA;User: {user_message_1}&#xA;&#xA;Assistant: {assistant_message_1}&amp;lt;ÔΩúend‚ñÅof‚ñÅsentenceÔΩú&amp;gt;User: {user_message_2}&#xA;&#xA;Assistant:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the last round of dialogue, note that &#34;Assistant:&#34; has no space after the colon. Adding a space might cause the following issues on the 16B-Lite model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;English questions receiving Chinese responses.&lt;/li&gt; &#xA; &lt;li&gt;Responses containing garbled text.&lt;/li&gt; &#xA; &lt;li&gt;Responses repeating excessively.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Older versions of Ollama had this bug (see &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-Coder-V2/issues/12&#34;&gt;https://github.com/deepseek-ai/DeepSeek-Coder-V2/issues/12&lt;/a&gt;), but it has been fixed in the latest version.&lt;/p&gt; &#xA;&lt;h3&gt;Inference with SGLang (recommended)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;SGLang&lt;/a&gt; currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, offering the best latency and throughput among open-source frameworks. Here are some example commands to launch an OpenAI API-compatible server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# BF16, tensor parallelism = 8&#xA;python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-Coder-V2-Instruct --tp 8 --trust-remote-code&#xA;&#xA;# BF16, w/ torch.compile (The compilation can take several minutes)&#xA;python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct --trust-remote-code --enable-torch-compile&#xA;&#xA;# FP8, tensor parallelism = 8, FP8 KV cache&#xA;python3 -m sglang.launch_server --model neuralmagic/DeepSeek-Coder-V2-Instruct-FP8 --tp 8 --trust-remote-code --kv-cache-dtype fp8_e5m2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After launching the server, you can query it with OpenAI API&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import openai&#xA;client = openai.Client(&#xA;    base_url=&#34;http://127.0.0.1:30000/v1&#34;, api_key=&#34;EMPTY&#34;)&#xA;&#xA;# Chat completion&#xA;response = client.chat.completions.create(&#xA;    model=&#34;default&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful AI assistant&#34;},&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;List 3 countries and their capitals.&#34;},&#xA;    ],&#xA;    temperature=0,&#xA;    max_tokens=64,&#xA;)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference with vLLM (recommended)&lt;/h3&gt; &#xA;&lt;p&gt;To utilize &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; for model inference, please merge this Pull Request into your vLLM codebase: &lt;a href=&#34;https://github.com/vllm-project/vllm/pull/4650&#34;&gt;https://github.com/vllm-project/vllm/pull/4650&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer&#xA;from vllm import LLM, SamplingParams&#xA;&#xA;max_model_len, tp_size = 8192, 1&#xA;model_name = &#34;deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct&#34;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;llm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True, enforce_eager=True)&#xA;sampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])&#xA;&#xA;messages_list = [&#xA;    [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Who are you?&#34;}],&#xA;    [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;write a quick sort algorithm in python.&#34;}],&#xA;    [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Write a piece of quicksort code in C++.&#34;}],&#xA;]&#xA;&#xA;prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]&#xA;&#xA;outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)&#xA;&#xA;generated_text = [output.outputs[0].text for output in outputs]&#xA;print(generated_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;7. License&lt;/h2&gt; &#xA;&lt;p&gt;This code repository is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder-V2/main/LICENSE-CODE&#34;&gt;the MIT License&lt;/a&gt;. The use of DeepSeek-Coder-V2 Base/Instruct models is subject to &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder-V2/main/LICENSE-MODEL&#34;&gt;the Model License&lt;/a&gt;. DeepSeek-Coder-V2 series (including Base and Instruct) supports commercial use.&lt;/p&gt; &#xA;&lt;h2&gt;8. Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@article{zhu2024deepseek,&#xA;  title={DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence},&#xA;  author={Zhu, Qihao and Guo, Daya and Shao, Zhihong and Yang, Dejian and Wang, Peiyi and Xu, Runxin and Wu, Y and Li, Yukun and Gao, Huazuo and Ma, Shirong and others},&#xA;  journal={arXiv preprint arXiv:2406.11931},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;9. Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please raise an issue or contact us at &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder-V2/main/service@deepseek.com&#34;&gt;service@deepseek.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>erigontech/erigon</title>
    <updated>2025-01-01T01:28:50Z</updated>
    <id>tag:github.com,2025-01-01:/erigontech/erigon</id>
    <link href="https://github.com/erigontech/erigon" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ethereum implementation on the efficiency frontier https://docs.erigon.tech&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Erigon&lt;/h1&gt; &#xA;&lt;p&gt;Documentation: &lt;strong&gt;&lt;a href=&#34;https://docs.erigon.tech&#34;&gt;docs.erigon.tech&lt;/a&gt;&lt;/strong&gt; Blog: &lt;strong&gt;&lt;a href=&#34;https://erigon.substack.com/&#34;&gt;erigon.substack.com&lt;/a&gt;&lt;/strong&gt; Twitter: &lt;a href=&#34;https://x.com/ErigonEth&#34;&gt;x.com/ErigonEth&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Erigon is an implementation of Ethereum (execution layer with embeddable consensus layer), on the efficiency frontier.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/erigontech/erigon/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;Build status&#34;&gt; &lt;a href=&#34;https://sonarcloud.io/summary/new_code?id=erigontech_erigon&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=erigontech_erigon&amp;amp;metric=coverage&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!--ts--&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#erigon&#34;&gt;Erigon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#system-requirements&#34;&gt;System Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#datadir-structure&#34;&gt;Datadir structure&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#history-on-cheap-disk&#34;&gt;History on cheap disk&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#erigon3-datadir-size&#34;&gt;Erigon3 datadir size&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#erigon3-changes-from-erigon2&#34;&gt;Erigon3 changes from Erigon2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#logging&#34;&gt;Logging&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#modularity&#34;&gt;Modularity&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#embedded-consensus-layer&#34;&gt;Embedded Consensus Layer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#testnets&#34;&gt;Testnets&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#block-production-pos-validator&#34;&gt;Block Production (PoS Validator)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#config-files-toml&#34;&gt;Config Files TOML&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#beacon-chain-consensus-layer&#34;&gt;Beacon Chain (Consensus Layer)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#caplin&#34;&gt;Caplin&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#caplins-usage&#34;&gt;Caplin&#39;s Usage&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#multiple-instances--one-machine&#34;&gt;Multiple Instances / One Machine&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#dev-chain&#34;&gt;Dev Chain&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#key-features&#34;&gt;Key features&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#faster-initial-sync&#34;&gt;Faster Initial Sync&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#more-efficient-state-storage&#34;&gt;More Efficient State Storage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#json-rpc-daemon&#34;&gt;JSON-RPC daemon&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#grafana-dashboard&#34;&gt;Grafana dashboard&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#faq&#34;&gt;FAQ&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#use-as-library&#34;&gt;Use as library&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#default-ports-and-firewalls&#34;&gt;Default Ports and Firewalls&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#erigon-ports&#34;&gt;&lt;code&gt;erigon&lt;/code&gt; ports&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#caplin-ports&#34;&gt;&lt;code&gt;caplin&lt;/code&gt; ports&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#beaconapi-ports&#34;&gt;&lt;code&gt;beaconAPI&lt;/code&gt; ports&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#shared-ports&#34;&gt;&lt;code&gt;shared&lt;/code&gt; ports&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#other-ports&#34;&gt;&lt;code&gt;other&lt;/code&gt; ports&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#hetzner-expecting-strict-firewall-rules&#34;&gt;Hetzner expecting strict firewall rules&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#run-as-a-separate-user---systemd-example&#34;&gt;Run as a separate user - &lt;code&gt;systemd&lt;/code&gt; example&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#grab-diagnostic-for-bug-report&#34;&gt;Grab diagnostic for bug report&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#run-local-devnet&#34;&gt;Run local devnet&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#docker-permissions-error&#34;&gt;Docker permissions error&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#public-rpc&#34;&gt;Public RPC&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#raspberrypi&#34;&gt;RaspberryPI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#run-all-components-by-docker-compose&#34;&gt;Run all components by docker-compose&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#optional-setup-dedicated-user&#34;&gt;Optional: Setup dedicated user&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#environment-variables&#34;&gt;Environment Variables&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#run&#34;&gt;Run&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#how-to-change-db-pagesize&#34;&gt;How to change db pagesize&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#erigon3-perf-tricks&#34;&gt;Erigon3 perf tricks&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#windows&#34;&gt;Windows&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#getting-in-touch&#34;&gt;Getting in touch&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#erigon-discord-server&#34;&gt;Erigon Discord Server&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#blog&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#twitter&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#reporting-security-issuesconcerns&#34;&gt;Reporting security issues/concerns&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#known-issues&#34;&gt;Known issues&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#htop-shows-incorrect-memory-usage&#34;&gt;&lt;code&gt;htop&lt;/code&gt; shows incorrect memory usage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#cloud-network-drives&#34;&gt;Cloud network drives&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#filesystems-background-features-are-expensive&#34;&gt;Filesystem&#39;s background features are expensive&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#gnome-tracker-can-kill-erigon&#34;&gt;Gnome Tracker can kill Erigon&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#the---mount-option-requires-buildkit-error&#34;&gt;the --mount option requires BuildKit error&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--te--&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important defaults&lt;/strong&gt;: Erigon 3 is a Full Node by default. (Erigon 2 was an &lt;a href=&#34;https://ethereum.org/en/developers/docs/nodes-and-clients/archive-nodes/#what-is-an-archive-node&#34;&gt;Archive Node&lt;/a&gt; by default.) Set &lt;code&gt;--prune.mode&lt;/code&gt; to &#34;archive&#34; if you need an archive node or to &#34;minimal&#34; if you run a validator on a small disk (not allowed to change after first start).&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;In-depth links are marked by the microscope sign (üî¨) &lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;System Requirements&lt;/h1&gt; &#xA;&lt;p&gt;RAM: &amp;gt;=32GB, &lt;a href=&#34;https://golang.org/doc/install&#34;&gt;Golang &amp;gt;= 1.22&lt;/a&gt;; GCC 10+ or Clang; On Linux: kernel &amp;gt; v4. 64-bit architecture.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ArchiveNode Ethereum Mainnet: 2TB (April 2024). FullNode: 1.1TB (June 2024)&lt;/li&gt; &#xA; &lt;li&gt;ArchiveNode Gnosis: 1.7TB (March 2024). FullNode: 300GB (June 2024)&lt;/li&gt; &#xA; &lt;li&gt;ArchiveNode Polygon Mainnet: 4.1TB (April 2024). FullNode: 2Tb (April 2024)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;SSD or NVMe. Do not recommend HDD - on HDD Erigon will always stay N blocks behind chain tip, but not fall behind. Bear in mind that SSD performance deteriorates when close to capacity. CloudDrives (like gp3): Blocks Execution is slow on &lt;a href=&#34;https://github.com/erigontech/erigon?tab=readme-ov-file#cloud-network-drives&#34;&gt;cloud-network-drives&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üî¨ More details on &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#erigon3-datadir-size&#34;&gt;Erigon3 datadir size&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üî¨ More details on what type of data stored &lt;a href=&#34;https://ledgerwatch.github.io/turbo_geth_release.html#Disk-space&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/erigontech/erigon/releases&#34;&gt;Release Notes and Binaries&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Build latest release (this will be suitable for most users just wanting to run a node):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone --branch release/&amp;lt;x.xx&amp;gt; --single-branch https://github.com/erigontech/erigon.git&#xA;cd erigon&#xA;make erigon&#xA;./build/bin/erigon&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Increase download speed by &lt;code&gt;--torrent.download.rate=20mb&lt;/code&gt;. &lt;code&gt;üî¨ See &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/cmd/downloader/readme.md&#34;&gt;Downloader docs&lt;/a&gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Use &lt;code&gt;--datadir&lt;/code&gt; to choose where to store data.&lt;/p&gt; &#xA;&lt;p&gt;Use &lt;code&gt;--chain=gnosis&lt;/code&gt; for &lt;a href=&#34;https://www.gnosis.io/&#34;&gt;Gnosis Chain&lt;/a&gt;, &lt;code&gt;--chain=bor-mainnet&lt;/code&gt; for Polygon Mainnet, and &lt;code&gt;--chain=amoy&lt;/code&gt; for Polygon Amoy. For Gnosis Chain you need a &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#beacon-chain-consensus-layer&#34;&gt;Consensus Layer&lt;/a&gt; client alongside Erigon (&lt;a href=&#34;https://docs.gnosischain.com/category/step--3---run-consensus-client&#34;&gt;https://docs.gnosischain.com/category/step--3---run-consensus-client&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Running &lt;code&gt;make help&lt;/code&gt; will list and describe the convenience commands available in the &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/Makefile&#34;&gt;Makefile&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Datadir structure&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;datadir        &#xA;    chaindata     # &#34;Recently-updated Latest State&#34;, &#34;Recent History&#34;, &#34;Recent Blocks&#34;&#xA;    snapshots     # contains `.seg` files - it&#39;s old blocks&#xA;        domain    # Latest State&#xA;        history   # Historical values &#xA;        idx       # InvertedIndices: can search/filtering/union/intersect them - to find historical data. like eth_getLogs or trace_transaction&#xA;        accessors # Additional (generated) indices of history - have &#34;random-touch&#34; read-pattern. They can serve only `Get` requests (no search/filters).&#xA;    txpool        # pending transactions. safe to remove.&#xA;    nodes         # p2p peers. safe to remove.&#xA;    temp          # used to sort data bigger than RAM. can grow to ~100gb. cleaned at startup.&#xA;   &#xA;# There is 4 domains: account, storage, code, commitment &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;History on cheap disk&lt;/h3&gt; &#xA;&lt;p&gt;If you can afford store datadir on 1 nvme-raid - great. If can&#39;t - it&#39;s possible to store history on cheap drive.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# place (or ln -s) `datadir` on slow disk. link some sub-folders to fast (low-latency) disk.&#xA;# Example: what need link to fast disk to speedup execution&#xA;datadir        &#xA;    chaindata   # link to fast disk&#xA;    snapshots   &#xA;        domain    # link to fast disk&#xA;        history   &#xA;        idx       &#xA;        accessors &#xA;    temp # buffers to sort data &amp;gt;&amp;gt; RAM. sequential-buffered IO - is slow-disk-friendly   &#xA;&#xA;# Example: how to speedup history access: &#xA;#   - go step-by-step - first try store `accessors` on fast disk&#xA;#   - if speed is not good enough: `idx`&#xA;#   - if still not enough: `history` &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Erigon3 datadir size&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# eth-mainnet - archive - Nov 2024&#xA;&#xA;du -hsc /erigon/chaindata&#xA;15G &#x9;/erigon/chaindata&#xA;&#xA;du -hsc /erigon/snapshots/* &#xA;120G &#x9;/erigon/snapshots/accessor&#xA;300G&#x9;/erigon/snapshots/domain&#xA;280G&#x9;/erigon/snapshots/history&#xA;430G&#x9;/erigon/snapshots/idx&#xA;2.3T&#x9;/erigon/snapshots&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# bor-mainnet - archive - Nov 2024&#xA;&#xA;du -hsc /erigon/chaindata&#xA;20G &#x9;/erigon/chaindata&#xA;&#xA;du -hsc /erigon/snapshots/* &#xA;360G&#x9;/erigon-data/snapshots/accessor&#xA;1.1T&#x9;/erigon-data/snapshots/domain&#xA;750G&#x9;/erigon-data/snapshots/history&#xA;1.5T&#x9;/erigon-data/snapshots/idx&#xA;4.9T&#x9;/erigon/snapshots&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Erigon3 changes from Erigon2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Initial sync doesn&#39;t re-exec from 0:&lt;/strong&gt; downloading 99% LatestState and History&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Per-Transaction granularity of history&lt;/strong&gt; (Erigon2 had per-block). Means: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Can execute 1 historical transaction - without executing it&#39;s block&lt;/li&gt; &#xA;   &lt;li&gt;If account X change V1-&amp;gt;V2-&amp;gt;V1 within 1 block (different transactions): &lt;code&gt;debug_getModifiedAccountsByNumber&lt;/code&gt; return it&lt;/li&gt; &#xA;   &lt;li&gt;Erigon3 doesn&#39;t store Logs (aka Receipts) - it always re-executing historical txn (but it&#39;s cheaper)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Validator mode&lt;/strong&gt;: added. &lt;code&gt;--internalcl&lt;/code&gt; is enabled by default. to disable use &lt;code&gt;--externalcl&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Store most of data in immutable files (segments/snapshots):&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;can symlink/mount latest state to fast drive and history to cheap drive&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;chaindata&lt;/code&gt; is less than &lt;code&gt;15gb&lt;/code&gt;. It&#39;s ok to &lt;code&gt;rm -rf chaindata&lt;/code&gt;. (to prevent grow: recommend &lt;code&gt;--batchSize &amp;lt;= 1G&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;--prune&lt;/code&gt; flags changed&lt;/strong&gt;: see &lt;code&gt;--prune.mode&lt;/code&gt; (default: &lt;code&gt;full&lt;/code&gt;, archive: &lt;code&gt;archive&lt;/code&gt;, EIP-4444: &lt;code&gt;minimal&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Other changes:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ExecutionStage included many E2 stages: stage_hash_state, stage_trie, log_index, history_index, trace_index&lt;/li&gt; &#xA;   &lt;li&gt;Restart doesn&#39;t loose much partial progress: &lt;code&gt;--sync.loop.block.limit=5_000&lt;/code&gt; enabled by default&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Logging&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Flags:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;verbosity&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;log.console.verbosity&lt;/code&gt; (overriding alias for &lt;code&gt;verbosity&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;log.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;log.console.json&lt;/code&gt; (alias for &lt;code&gt;log.json&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;log.dir.path&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;log.dir.prefix&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;log.dir.verbosity&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;log.dir.json&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In order to log only to the stdout/stderr the &lt;code&gt;--verbosity&lt;/code&gt; (or &lt;code&gt;log.console.verbosity&lt;/code&gt;) flag can be used to supply an int value specifying the highest output log level:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  LvlCrit = 0&#xA;  LvlError = 1&#xA;  LvlWarn = 2&#xA;  LvlInfo = 3&#xA;  LvlDebug = 4&#xA;  LvlTrace = 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To set an output dir for logs to be collected on disk, please set &lt;code&gt;--log.dir.path&lt;/code&gt; If you want to change the filename produced from &lt;code&gt;erigon&lt;/code&gt; you should also set the &lt;code&gt;--log.dir.prefix&lt;/code&gt; flag to an alternate name. The flag &lt;code&gt;--log.dir.verbosity&lt;/code&gt; is also available to control the verbosity of this logging, with the same int value as above, or the string value e.g. &#39; debug&#39; or &#39;info&#39;. Default verbosity is &#39;debug&#39; (4), for disk logging.&lt;/p&gt; &#xA;&lt;p&gt;Log format can be set to json by the use of the boolean flags &lt;code&gt;log.json&lt;/code&gt; or &lt;code&gt;log.console.json&lt;/code&gt;, or for the disk output &lt;code&gt;--log.dir.json&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Modularity&lt;/h3&gt; &#xA;&lt;p&gt;Erigon by default is &#34;all in one binary&#34; solution, but it&#39;s possible start TxPool as separated processes. Same true about: JSON RPC layer (RPCDaemon), p2p layer (Sentry), history download layer (Downloader), consensus. Don&#39;t start services as separated processes unless you have clear reason for it: resource limiting, scale, replace by your own implementation, security. How to start Erigon&#39;s services as separated processes, see in &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/docker-compose.yml&#34;&gt;docker-compose.yml&lt;/a&gt;. Each service has own &lt;code&gt;./cmd/*/README.md&lt;/code&gt; file. &lt;a href=&#34;https://erigon.substack.com/&#34;&gt;Erigon Blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Embedded Consensus Layer&lt;/h3&gt; &#xA;&lt;p&gt;Built-in consensus for Ethereum Mainnet, Sepolia, Holesky, Gnosis. To use external Consensus Layer: &lt;code&gt;--externalcl&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Testnets&lt;/h3&gt; &#xA;&lt;p&gt;If you would like to give Erigon a try: a good option is to start syncing one of the public testnets, Holesky (or Amoy). It syncs much quicker, and does not take so much disk space:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/erigontech/erigon.git&#xA;cd erigon&#xA;make erigon&#xA;./build/bin/erigon --datadir=&amp;lt;your_datadir&amp;gt; --chain=holesky --prune.mode=full&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note the &lt;code&gt;--datadir&lt;/code&gt; option that allows you to store Erigon files in a non-default location. Name of the directory &lt;code&gt;--datadir&lt;/code&gt; does not have to match the name of the chain in &lt;code&gt;--chain&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Block Production (PoS Validator)&lt;/h3&gt; &#xA;&lt;p&gt;Block production is fully supported for Ethereum &amp;amp; Gnosis Chain. It is still experimental for Polygon.&lt;/p&gt; &#xA;&lt;h3&gt;Config Files TOML&lt;/h3&gt; &#xA;&lt;p&gt;You can set Erigon flags through a TOML configuration file with the flag &lt;code&gt;--config&lt;/code&gt;. The flags set in the configuration file can be overwritten by writing the flags directly on Erigon command line&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;./build/bin/erigon --config ./config.toml --chain=sepolia&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Assuming we have &lt;code&gt;chain : &#34;mainnet&#34;&lt;/code&gt; in our configuration file, by adding &lt;code&gt;--chain=sepolia&lt;/code&gt; allows the overwrite of the flag inside of the toml configuration file and sets the chain to sepolia&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;datadir = &#39;your datadir&#39;&#xA;port = 1111&#xA;chain = &#34;mainnet&#34;&#xA;http = true&#xA;&#34;private.api.addr&#34;=&#34;localhost:9090&#34;&#xA;&#xA;&#34;http.api&#34; = [&#34;eth&#34;,&#34;debug&#34;,&#34;net&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Beacon Chain (Consensus Layer)&lt;/h3&gt; &#xA;&lt;p&gt;Erigon can be used as an Execution Layer (EL) for Consensus Layer clients (CL). Default configuration is OK.&lt;/p&gt; &#xA;&lt;p&gt;If your CL client is on a different device, add &lt;code&gt;--authrpc.addr 0.0.0.0&lt;/code&gt; (&lt;a href=&#34;https://github.com/ethereum/execution-apis/raw/main/src/engine&#34;&gt;Engine API&lt;/a&gt; listens on localhost by default) as well as &lt;code&gt;--authrpc.vhosts &amp;lt;CL host&amp;gt;&lt;/code&gt; where &lt;code&gt;&amp;lt;CL host&amp;gt;&lt;/code&gt; is your source host or &lt;code&gt;any&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In order to establish a secure connection between the Consensus Layer and the Execution Layer, a JWT secret key is automatically generated.&lt;/p&gt; &#xA;&lt;p&gt;The JWT secret key will be present in the datadir by default under the name of &lt;code&gt;jwt.hex&lt;/code&gt; and its path can be specified with the flag &lt;code&gt;--authrpc.jwtsecret&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This piece of info needs to be specified in the Consensus Layer as well in order to establish connection successfully. More information can be found &lt;a href=&#34;https://github.com/ethereum/execution-apis/raw/main/src/engine/authentication.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once Erigon is running, you need to point your CL client to &lt;code&gt;&amp;lt;erigon address&amp;gt;:8551&lt;/code&gt;, where &lt;code&gt;&amp;lt;erigon address&amp;gt;&lt;/code&gt; is either &lt;code&gt;localhost&lt;/code&gt; or the IP address of the device running Erigon, and also point to the JWT secret path created by Erigon.&lt;/p&gt; &#xA;&lt;h3&gt;Caplin&lt;/h3&gt; &#xA;&lt;p&gt;Caplin is a full-fledged validating Consensus Client like Prysm, Lighthouse, Teku, Nimbus and Lodestar. Its goal is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;provide better stability&lt;/li&gt; &#xA; &lt;li&gt;Validation of the chain&lt;/li&gt; &#xA; &lt;li&gt;Stay in sync&lt;/li&gt; &#xA; &lt;li&gt;keep the execution of blocks on chain tip&lt;/li&gt; &#xA; &lt;li&gt;serve the Beacon API using a fast and compact data model alongside low CPU and memory usage.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The main reason why developed a new Consensus Layer is to experiment with the possible benefits that could come with it. For example, The Engine API does not work well with Erigon. The Engine API sends data one block at a time, which does not suit how Erigon works. Erigon is designed to handle many blocks simultaneously and needs to sort and process data efficiently. Therefore, it would be better for Erigon to handle the blocks independently instead of relying on the Engine API.&lt;/p&gt; &#xA;&lt;h4&gt;Caplin&#39;s Usage&lt;/h4&gt; &#xA;&lt;p&gt;Caplin is be enabled by default. to disable it and enable the Engine API, use the &lt;code&gt;--externalcl&lt;/code&gt; flag. from that point on, an external Consensus Layer will not be need anymore.&lt;/p&gt; &#xA;&lt;p&gt;Caplin also has an archival mode for historical states and blocks. it can be enabled through the &lt;code&gt;--caplin.archive&lt;/code&gt; flag. In order to enable the caplin&#39;s Beacon API, the flag &lt;code&gt;--beacon.api=&amp;lt;namespaces&amp;gt;&lt;/code&gt; must be added. e.g: &lt;code&gt;--beacon.api=beacon,builder,config,debug,node,validator,lighthouse&lt;/code&gt; will enable all endpoints. **NOTE: Caplin is not staking-ready so aggregation endpoints are still to be implemented. Additionally enabling the Beacon API will lead to a 6 GB higher RAM usage.&lt;/p&gt; &#xA;&lt;h3&gt;Multiple Instances / One Machine&lt;/h3&gt; &#xA;&lt;p&gt;Define 6 flags to avoid conflicts: &lt;code&gt;--datadir --port --http.port --authrpc.port --torrent.port --private.api.addr&lt;/code&gt;. Example of multiple chains on the same machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# mainnet&#xA;./build/bin/erigon --datadir=&#34;&amp;lt;your_mainnet_data_path&amp;gt;&#34; --chain=mainnet --port=30303 --http.port=8545 --authrpc.port=8551 --torrent.port=42069 --private.api.addr=127.0.0.1:9090 --http --ws --http.api=eth,debug,net,trace,web3,erigon&#xA;&#xA;&#xA;# sepolia&#xA;./build/bin/erigon --datadir=&#34;&amp;lt;your_sepolia_data_path&amp;gt;&#34; --chain=sepolia --port=30304 --http.port=8546 --authrpc.port=8552 --torrent.port=42068 --private.api.addr=127.0.0.1:9091 --http --ws --http.api=eth,debug,net,trace,web3,erigon&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Quote your path if it has spaces.&lt;/p&gt; &#xA;&lt;h3&gt;Dev Chain&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt; üî¨ Detailed explanation is &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/DEV_CHAIN.md&#34;&gt;DEV_CHAIN&lt;/a&gt;.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Key features&lt;/h1&gt; &#xA;&lt;h3&gt;Faster Initial Sync&lt;/h3&gt; &#xA;&lt;p&gt;On good network bandwidth EthereumMainnet FullNode syncs in 3 hours: &lt;a href=&#34;https://erigon.substack.com/p/erigon-3-alpha-2-introducing-blazingly&#34;&gt;OtterSync&lt;/a&gt; can sync&lt;/p&gt; &#xA;&lt;h3&gt;More Efficient State Storage&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Flat KV storage.&lt;/strong&gt; Erigon uses a key-value database and storing accounts and storage in a simple way.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt; üî¨ See our detailed DB walkthrough &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/docs/programmers_guide/db_walkthrough.MD&#34;&gt;here&lt;/a&gt;.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Preprocessing&lt;/strong&gt;. For some operations, Erigon uses temporary files to preprocess data before inserting it into the main DB. That reduces write amplification and DB inserts are orders of magnitude quicker.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt; üî¨ See our detailed ETL explanation &lt;a href=&#34;https://github.com/erigontech/erigon/raw/main/erigon-lib/etl/README.md&#34;&gt;here&lt;/a&gt;.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Plain state&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Single accounts/state trie&lt;/strong&gt;. Erigon uses a single Merkle trie for both accounts and the storage.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt; üî¨ &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/eth/stagedsync/README.md&#34;&gt;Staged Sync Readme&lt;/a&gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;JSON-RPC daemon&lt;/h3&gt; &#xA;&lt;p&gt;Most of Erigon&#39;s components (txpool, rpcdaemon, snapshots downloader, sentry, ...) can work inside Erigon and as independent process on same Server (or another Server). Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make erigon rpcdaemon&#xA;./build/bin/erigon --datadir=/my --http=false&#xA;# To run RPCDaemon as separated process: use same `--datadir` as Erigon&#xA;./build/bin/rpcdaemon --datadir=/my --http.api=eth,erigon,web3,net,debug,trace,txpool --ws&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supported JSON-RPC calls: &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/cmd/rpcdaemon/commands/eth_api.go&#34;&gt;eth&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/cmd/rpcdaemon/commands/debug_api.go&#34;&gt;debug&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/cmd/rpcdaemon/commands/net_api.go&#34;&gt;net&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/cmd/rpcdaemon/commands/web3_api.go&#34;&gt;web3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;increase throughput by: &lt;code&gt;--rpc.batch.concurrency&lt;/code&gt;, &lt;code&gt;--rpc.batch.limit&lt;/code&gt;, &lt;code&gt;--db.read.concurrency&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;increase throughput by disabling: &lt;code&gt;--http.compression&lt;/code&gt;, &lt;code&gt;--ws.compression&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;üî¨ See &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/cmd/rpcdaemon/README.md&#34;&gt;RPC-Daemon docs&lt;/a&gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Grafana dashboard&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;docker compose up prometheus grafana&lt;/code&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/cmd/prometheus/Readme.md&#34;&gt;detailed docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h3&gt;Use as library&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;# please use git branch name (or commit hash). don&#39;t use git tags&#xA;go mod edit -replace github.com/erigontech/erigon-lib=github.com/erigontech/erigon/erigon-lib@5498f854e44df5c8f0804ff4f0747c0dec3caad5&#xA;go get github.com/erigontech/erigon@main&#xA;go mod tidy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Default Ports and Firewalls&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;code&gt;erigon&lt;/code&gt; ports&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Port&lt;/th&gt; &#xA;   &lt;th&gt;Protocol&lt;/th&gt; &#xA;   &lt;th&gt;Purpose&lt;/th&gt; &#xA;   &lt;th&gt;Should Expose&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;engine&lt;/td&gt; &#xA;   &lt;td&gt;9090&lt;/td&gt; &#xA;   &lt;td&gt;TCP&lt;/td&gt; &#xA;   &lt;td&gt;gRPC Server&lt;/td&gt; &#xA;   &lt;td&gt;Private&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;engine&lt;/td&gt; &#xA;   &lt;td&gt;42069&lt;/td&gt; &#xA;   &lt;td&gt;TCP &amp;amp; UDP&lt;/td&gt; &#xA;   &lt;td&gt;Snap sync (Bittorrent)&lt;/td&gt; &#xA;   &lt;td&gt;Public&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;engine&lt;/td&gt; &#xA;   &lt;td&gt;8551&lt;/td&gt; &#xA;   &lt;td&gt;TCP&lt;/td&gt; &#xA;   &lt;td&gt;Engine API (JWT auth)&lt;/td&gt; &#xA;   &lt;td&gt;Private&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sentry&lt;/td&gt; &#xA;   &lt;td&gt;30303&lt;/td&gt; &#xA;   &lt;td&gt;TCP &amp;amp; UDP&lt;/td&gt; &#xA;   &lt;td&gt;eth/68 peering&lt;/td&gt; &#xA;   &lt;td&gt;Public&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sentry&lt;/td&gt; &#xA;   &lt;td&gt;30304&lt;/td&gt; &#xA;   &lt;td&gt;TCP &amp;amp; UDP&lt;/td&gt; &#xA;   &lt;td&gt;eth/67 peering&lt;/td&gt; &#xA;   &lt;td&gt;Public&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sentry&lt;/td&gt; &#xA;   &lt;td&gt;9091&lt;/td&gt; &#xA;   &lt;td&gt;TCP&lt;/td&gt; &#xA;   &lt;td&gt;incoming gRPC Connections&lt;/td&gt; &#xA;   &lt;td&gt;Private&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;rpcdaemon&lt;/td&gt; &#xA;   &lt;td&gt;8545&lt;/td&gt; &#xA;   &lt;td&gt;TCP&lt;/td&gt; &#xA;   &lt;td&gt;HTTP &amp;amp; WebSockets &amp;amp; GraphQL&lt;/td&gt; &#xA;   &lt;td&gt;Private&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Typically, 30303 and 30304 are exposed to the internet to allow incoming peering connections. 9090 is exposed only internally for rpcdaemon or other connections, (e.g. rpcdaemon -&amp;gt; erigon). Port 8551 (JWT authenticated) is exposed only internally for &lt;a href=&#34;https://github.com/ethereum/execution-apis/raw/main/src/engine&#34;&gt;Engine API&lt;/a&gt; JSON-RPC queries from the Consensus Layer node.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;caplin&lt;/code&gt; ports&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Port&lt;/th&gt; &#xA;   &lt;th&gt;Protocol&lt;/th&gt; &#xA;   &lt;th&gt;Purpose&lt;/th&gt; &#xA;   &lt;th&gt;Should Expose&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sentinel&lt;/td&gt; &#xA;   &lt;td&gt;4000&lt;/td&gt; &#xA;   &lt;td&gt;UDP&lt;/td&gt; &#xA;   &lt;td&gt;Peering&lt;/td&gt; &#xA;   &lt;td&gt;Public&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sentinel&lt;/td&gt; &#xA;   &lt;td&gt;4001&lt;/td&gt; &#xA;   &lt;td&gt;TCP&lt;/td&gt; &#xA;   &lt;td&gt;Peering&lt;/td&gt; &#xA;   &lt;td&gt;Public&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;In order to configure the ports, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;   --caplin.discovery.addr value                                                    Address for Caplin DISCV5 protocol (default: &#34;127.0.0.1&#34;)&#xA;   --caplin.discovery.port value                                                    Port for Caplin DISCV5 protocol (default: 4000)&#xA;   --caplin.discovery.tcpport value                                                 TCP Port for Caplin DISCV5 protocol (default: 4001)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;code&gt;beaconAPI&lt;/code&gt; ports&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Port&lt;/th&gt; &#xA;   &lt;th&gt;Protocol&lt;/th&gt; &#xA;   &lt;th&gt;Purpose&lt;/th&gt; &#xA;   &lt;th&gt;Should Expose&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;REST&lt;/td&gt; &#xA;   &lt;td&gt;5555&lt;/td&gt; &#xA;   &lt;td&gt;TCP&lt;/td&gt; &#xA;   &lt;td&gt;REST&lt;/td&gt; &#xA;   &lt;td&gt;Public&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;&lt;code&gt;shared&lt;/code&gt; ports&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Port&lt;/th&gt; &#xA;   &lt;th&gt;Protocol&lt;/th&gt; &#xA;   &lt;th&gt;Purpose&lt;/th&gt; &#xA;   &lt;th&gt;Should Expose&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;all&lt;/td&gt; &#xA;   &lt;td&gt;6060&lt;/td&gt; &#xA;   &lt;td&gt;TCP&lt;/td&gt; &#xA;   &lt;td&gt;pprof&lt;/td&gt; &#xA;   &lt;td&gt;Private&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;all&lt;/td&gt; &#xA;   &lt;td&gt;6061&lt;/td&gt; &#xA;   &lt;td&gt;TCP&lt;/td&gt; &#xA;   &lt;td&gt;metrics&lt;/td&gt; &#xA;   &lt;td&gt;Private&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Optional flags can be enabled that enable pprof or metrics (or both). Use &lt;code&gt;--help&lt;/code&gt; with the binary for more info.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;other&lt;/code&gt; ports&lt;/h4&gt; &#xA;&lt;p&gt;Reserved for future use: &lt;strong&gt;gRPC ports&lt;/strong&gt;: &lt;code&gt;9092&lt;/code&gt; consensus engine, &lt;code&gt;9093&lt;/code&gt; snapshot downloader, &lt;code&gt;9094&lt;/code&gt; TxPool&lt;/p&gt; &#xA;&lt;h4&gt;Hetzner expecting strict firewall rules&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;0.0.0.0/8             &#34;This&#34; Network             RFC 1122, Section 3.2.1.3&#xA;10.0.0.0/8            Private-Use Networks       RFC 1918&#xA;100.64.0.0/10         Carrier-Grade NAT (CGN)    RFC 6598, Section 7&#xA;127.16.0.0/12         Private-Use Networks       RFC 1918&#xA;169.254.0.0/16        Link Local                 RFC 3927&#xA;172.16.0.0/12         Private-Use Networks       RFC 1918&#xA;192.0.0.0/24          IETF Protocol Assignments  RFC 5736&#xA;192.0.2.0/24          TEST-NET-1                 RFC 5737&#xA;192.88.99.0/24        6to4 Relay Anycast         RFC 3068&#xA;192.168.0.0/16        Private-Use Networks       RFC 1918&#xA;198.18.0.0/15         Network Interconnect&#xA;Device Benchmark Testing   RFC 2544&#xA;198.51.100.0/24       TEST-NET-2                 RFC 5737&#xA;203.0.113.0/24        TEST-NET-3                 RFC 5737&#xA;224.0.0.0/4           Multicast                  RFC 3171&#xA;240.0.0.0/4           Reserved for Future Use    RFC 1112, Section 4&#xA;255.255.255.255/32    Limited Broadcast          RFC 919, Section 7&#xA;RFC 922, Section 7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Same in &lt;a href=&#34;https://ethereum.stackexchange.com/questions/6386/how-to-prevent-being-blacklisted-for-running-an-ethereum-client/13068#13068&#34;&gt;IpTables syntax&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Run as a separate user - &lt;code&gt;systemd&lt;/code&gt; example&lt;/h3&gt; &#xA;&lt;p&gt;Running erigon from &lt;code&gt;build/bin&lt;/code&gt; as a separate user might produce an error:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;error while loading shared libraries: libsilkworm_capi.so: cannot open shared object file: No such file or directory&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The library needs to be &lt;em&gt;installed&lt;/em&gt; for another user using &lt;code&gt;make DIST=&amp;lt;path&amp;gt; install&lt;/code&gt;. You could use &lt;code&gt;$HOME/erigon&lt;/code&gt; or &lt;code&gt;/opt/erigon&lt;/code&gt; as the installation path, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make DIST=/opt/erigon install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Grab diagnostic for bug report&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Get stack trace: &lt;code&gt;kill -SIGUSR1 &amp;lt;pid&amp;gt;&lt;/code&gt;, get trace and stop: &lt;code&gt;kill -6 &amp;lt;pid&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Get CPU profiling: add &lt;code&gt;--pprof&lt;/code&gt; flag and run&lt;br&gt; &lt;code&gt;go tool pprof -png http://127.0.0.1:6060/debug/pprof/profile\?seconds\=20 &amp;gt; cpu.png&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Get RAM profiling: add &lt;code&gt;--pprof&lt;/code&gt; flag and run&lt;br&gt; &lt;code&gt;go tool pprof -inuse_space -png http://127.0.0.1:6060/debug/pprof/heap &amp;gt; mem.png&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run local devnet&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt; üî¨ Detailed explanation is &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/DEV_CHAIN.md&#34;&gt;here&lt;/a&gt;.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker permissions error&lt;/h3&gt; &#xA;&lt;p&gt;Docker uses user erigon with UID/GID 1000 (for security reasons). You can see this user being created in the Dockerfile. Can fix by giving a host&#39;s user ownership of the folder, where the host&#39;s user UID/GID is the same as the docker&#39;s user UID/GID (1000). More details in &lt;a href=&#34;https://www.fullstaq.com/knowledge-hub/blogs/docker-and-the-host-filesystem-owner-matching-problem&#34;&gt;post&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Public RPC&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--txpool.nolocals=true&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;don&#39;t add &lt;code&gt;admin&lt;/code&gt; in &lt;code&gt;--http.api&lt;/code&gt; list&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--http.corsdomain=&#34;*&#34;&lt;/code&gt; is bad-practice: set exact hostname or IP&lt;/li&gt; &#xA; &lt;li&gt;protect from DOS by reducing: &lt;code&gt;--rpc.batch.concurrency&lt;/code&gt;, &lt;code&gt;--rpc.batch.limit&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RaspberryPI&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mathMakesArt/Erigon-on-RPi-4&#34;&gt;https://github.com/mathMakesArt/Erigon-on-RPi-4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Run all components by docker-compose&lt;/h3&gt; &#xA;&lt;p&gt;Docker allows for building and running Erigon via containers. This alleviates the need for installing build dependencies onto the host OS.&lt;/p&gt; &#xA;&lt;h4&gt;Optional: Setup dedicated user&lt;/h4&gt; &#xA;&lt;p&gt;User UID/GID need to be synchronized between the host OS and container so files are written with correct permission.&lt;/p&gt; &#xA;&lt;p&gt;You may wish to setup a dedicated user/group on the host OS, in which case the following &lt;code&gt;make&lt;/code&gt; targets are available.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# create &#34;erigon&#34; user&#xA;make user_linux&#xA;# or&#xA;make user_macos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Environment Variables&lt;/h4&gt; &#xA;&lt;p&gt;There is a &lt;code&gt;.env.example&lt;/code&gt; file in the root of the repo.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;DOCKER_UID&lt;/code&gt; - The UID of the docker user&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DOCKER_GID&lt;/code&gt; - The GID of the docker user&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;XDG_DATA_HOME&lt;/code&gt; - The data directory which will be mounted to the docker containers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If not specified, the UID/GID will use the current user.&lt;/p&gt; &#xA;&lt;p&gt;A good choice for &lt;code&gt;XDG_DATA_HOME&lt;/code&gt; is to use the &lt;code&gt;~erigon/.ethereum&lt;/code&gt; directory created by helper targets &lt;code&gt;make user_linux&lt;/code&gt; or &lt;code&gt;make user_macos&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Run&lt;/h4&gt; &#xA;&lt;p&gt;Check permissions: In all cases, &lt;code&gt;XDG_DATA_HOME&lt;/code&gt; (specified or default) must be writable by the user UID/GID in docker, which will be determined by the &lt;code&gt;DOCKER_UID&lt;/code&gt; and &lt;code&gt;DOCKER_GID&lt;/code&gt; at build time. If a build or service startup is failing due to permissions, check that all the directories, UID, and GID controlled by these environment variables are correct.&lt;/p&gt; &#xA;&lt;p&gt;Next command starts: Erigon on port 30303, rpcdaemon on port 8545, prometheus on port 9090, and grafana on port 3000.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;#&#xA;# Will mount ~/.local/share/erigon to /home/erigon/.local/share/erigon inside container&#xA;#&#xA;make docker-compose&#xA;&#xA;#&#xA;# or&#xA;#&#xA;# if you want to use a custom data directory&#xA;# or, if you want to use different uid/gid for a dedicated user&#xA;#&#xA;# To solve this, pass in the uid/gid parameters into the container.&#xA;#&#xA;# DOCKER_UID: the user id&#xA;# DOCKER_GID: the group id&#xA;# XDG_DATA_HOME: the data directory (default: ~/.local/share)&#xA;#&#xA;# Note: /preferred/data/folder must be read/writeable on host OS by user with UID/GID given&#xA;#       if you followed above instructions&#xA;#&#xA;# Note: uid/gid syntax below will automatically use uid/gid of running user so this syntax&#xA;#       is intended to be run via the dedicated user setup earlier&#xA;#&#xA;DOCKER_UID=$(id -u) DOCKER_GID=$(id -g) XDG_DATA_HOME=/preferred/data/folder DOCKER_BUILDKIT=1 COMPOSE_DOCKER_CLI_BUILD=1 make docker-compose&#xA;&#xA;#&#xA;# if you want to run the docker, but you are not logged in as the $ERIGON_USER&#xA;# then you&#39;ll need to adjust the syntax above to grab the correct uid/gid&#xA;#&#xA;# To run the command via another user, use&#xA;#&#xA;ERIGON_USER=erigon&#xA;sudo -u ${ERIGON_USER} DOCKER_UID=$(id -u ${ERIGON_USER}) DOCKER_GID=$(id -g ${ERIGON_USER}) XDG_DATA_HOME=~${ERIGON_USER}/.ethereum DOCKER_BUILDKIT=1 COMPOSE_DOCKER_CLI_BUILD=1 make docker-compose&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Makefile creates the initial directories for erigon, prometheus and grafana. The PID namespace is shared between erigon and rpcdaemon which is required to open Erigon&#39;s DB from another process (RPCDaemon local-mode). See: &lt;a href=&#34;https://github.com/erigontech/erigon/pull/2392/files&#34;&gt;https://github.com/erigontech/erigon/pull/2392/files&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If your docker installation requires the docker daemon to run as root (which is by default), you will need to prefix the command above with &lt;code&gt;sudo&lt;/code&gt;. However, it is sometimes recommended running docker (and therefore its containers) as a non-root user for security reasons. For more information about how to do this, refer to &lt;a href=&#34;https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user&#34;&gt;this article&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;How to change db pagesize&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/erigontech/erigon/raw/main/cmd/integration/Readme.md#copy-data-to-another-db&#34;&gt;post&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Erigon3 perf tricks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;on BorMainnet may help: &lt;code&gt;--sync.loop.block.limit=10_000&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;on cloud-drives (good throughput, bad latency) - can enable OS&#39;s brain to pre-fetch: &lt;code&gt;SNAPSHOT_MADV_RND=false&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;can lock latest state in RAM - to prevent from eviction (node may face high historical RPC traffic without impacting Chain-Tip perf):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;vmtouch -vdlw /mnt/erigon/snapshots/domain/*bt&#xA;ls /mnt/erigon/snapshots/domain/*.kv | parallel vmtouch -vdlw&#xA;&#xA;# if it failing with &#34;can&#39;t allocate memory&#34;, try: &#xA;sync &amp;amp;&amp;amp; sudo sysctl vm.drop_caches=3&#xA;echo 1 &amp;gt; /proc/sys/vm/compact_memory&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;Windows users may run erigon in 3 possible ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Build executable binaries natively for Windows using provided &lt;code&gt;wmake.ps1&lt;/code&gt; PowerShell script. Usage syntax is the same as &lt;code&gt;make&lt;/code&gt; command so you have to run &lt;code&gt;.\wmake.ps1 [-target] &amp;lt;targetname&amp;gt;&lt;/code&gt;. Example: &lt;code&gt;.\wmake.ps1 erigon&lt;/code&gt; builds erigon executable. All binaries are placed in &lt;code&gt;.\build\bin\&lt;/code&gt; subfolder. There are some requirements for a successful native build on windows :&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;Git&lt;/a&gt; for Windows must be installed. If you&#39;re cloning this repository is very likely you already have it&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://golang.org/dl/&#34;&gt;GO Programming Language&lt;/a&gt; must be installed. Minimum required version is 1.22&lt;/li&gt; &#xA;   &lt;li&gt;GNU CC Compiler at least version 13 (is highly suggested that you install &lt;code&gt;chocolatey&lt;/code&gt; package manager - see following point)&lt;/li&gt; &#xA;   &lt;li&gt;If you need to build MDBX tools (i.e. &lt;code&gt;.\wmake.ps1 db-tools&lt;/code&gt;) then &lt;a href=&#34;https://chocolatey.org/&#34;&gt;Chocolatey package manager&lt;/a&gt; for Windows must be installed. By Chocolatey you need to install the following components : &lt;code&gt;cmake&lt;/code&gt;, &lt;code&gt;make&lt;/code&gt;, &lt;code&gt;mingw&lt;/code&gt; by &lt;code&gt;choco install cmake make mingw&lt;/code&gt;. Make sure Windows System &#34;Path&#34; variable has: C:\ProgramData\chocolatey\lib\mingw\tools\install\mingw64\bin&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Important note about Anti-Viruses&lt;/strong&gt; During MinGW&#39;s compiler detection phase some temporary executables are generated to test compiler capabilities. It&#39;s been reported some anti-virus programs detect those files as possibly infected by &lt;code&gt;Win64/Kryptic.CIS&lt;/code&gt; trojan horse (or a variant of it). Although those are false positives we have no control over 100+ vendors of security products for Windows and their respective detection algorithms and we understand this might make your experience with Windows builds uncomfortable. To workaround the issue you might either set exclusions for your antivirus specifically for &lt;code&gt;build\bin\mdbx\CMakeFiles&lt;/code&gt; sub-folder of the cloned repo or you can run erigon using the following other two options&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use Docker : see &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/docker-compose.yml&#34;&gt;docker-compose.yml&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use WSL (Windows Subsystem for Linux) &lt;strong&gt;strictly on version 2&lt;/strong&gt;. Under this option you can build Erigon just as you would on a regular Linux distribution. You can point your data also to any of the mounted Windows partitions ( eg. &lt;code&gt;/mnt/c/[...]&lt;/code&gt;, &lt;code&gt;/mnt/d/[...]&lt;/code&gt; etc) but in such case be advised performance is impacted: this is due to the fact those mount points use &lt;code&gt;DrvFS&lt;/code&gt; which is a &lt;a href=&#34;https://github.com/erigontech/erigon?tab=readme-ov-file#cloud-network-drives&#34;&gt;network file system&lt;/a&gt; and, additionally, MDBX locks the db for exclusive access which implies only one process at a time can access data. This has consequences on the running of &lt;code&gt;rpcdaemon&lt;/code&gt; which has to be configured as &lt;a href=&#34;https://raw.githubusercontent.com/erigontech/erigon/main/#for-remote-db&#34;&gt;Remote DB&lt;/a&gt; even if it is executed on the very same computer. If instead your data is hosted on the native Linux filesystem non limitations apply. &lt;strong&gt;Please also note the default WSL2 environment has its own IP address which does not match the one of the network interface of Windows host: take this into account when configuring NAT for port 30303 on your router.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting in touch&lt;/h1&gt; &#xA;&lt;h3&gt;Erigon Discord Server&lt;/h3&gt; &#xA;&lt;p&gt;The main discussions are happening on our Discord server. To get an invite, send an email to &lt;code&gt;bloxster [at] proton.me&lt;/code&gt; with your name, occupation, a brief explanation of why you want to join the Discord, and how you heard about Erigon.&lt;/p&gt; &#xA;&lt;h3&gt;Blog&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://erigon.substack.com/&#34;&gt;erigon.substack.com&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Twitter&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://x.com/ErigonEth&#34;&gt;x.com/ErigonEth&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Reporting security issues/concerns&lt;/h3&gt; &#xA;&lt;p&gt;Send an email to &lt;code&gt;security [at] torquem.ch&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Known issues&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;code&gt;htop&lt;/code&gt; shows incorrect memory usage&lt;/h3&gt; &#xA;&lt;p&gt;Erigon&#39;s internal DB (MDBX) using &lt;code&gt;MemoryMap&lt;/code&gt; - when OS does manage all &lt;code&gt;read, write, cache&lt;/code&gt; operations instead of Application (&lt;a href=&#34;https://linux-kernel-labs.github.io/refs/heads/master/labs/memory_mapping.html&#34;&gt;linux&lt;/a&gt; , &lt;a href=&#34;https://docs.microsoft.com/en-us/windows/win32/memory/file-mapping&#34;&gt;windows&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;htop&lt;/code&gt; on column &lt;code&gt;res&lt;/code&gt; shows memory of &#34;App + OS used to hold page cache for given App&#34;, but it&#39;s not informative, because if &lt;code&gt;htop&lt;/code&gt; says that app using 90% of memory you still can run 3 more instances of app on the same machine - because most of that &lt;code&gt;90%&lt;/code&gt; is &#34;OS pages cache&#34;. OS automatically frees this cache any time it needs memory. Smaller &#34;page cache size&#34; may not impact performance of Erigon at all.&lt;/p&gt; &#xA;&lt;p&gt;Next tools show correct memory usage of Erigon:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;vmmap -summary PID | grep -i &#34;Physical footprint&#34;&lt;/code&gt;. Without &lt;code&gt;grep&lt;/code&gt; you can see details &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;section MALLOC ZONE column Resident Size&lt;/code&gt; shows App memory usage, &lt;code&gt;section REGION TYPE column Resident Size&lt;/code&gt; shows OS pages cache size.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Prometheus&lt;/code&gt; dashboard shows memory of Go app without OS pages cache (&lt;code&gt;make prometheus&lt;/code&gt;, open in browser &lt;code&gt;localhost:3000&lt;/code&gt;, credentials &lt;code&gt;admin/admin&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cat /proc/&amp;lt;PID&amp;gt;/smaps&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Erigon uses ~4Gb of RAM during genesis sync and ~1Gb during normal work. OS pages cache can utilize unlimited amount of memory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; Multiple instances of Erigon on same machine will touch Disk concurrently, it impacts performance - one of main Erigon optimizations: &#34;reduce Disk random access&#34;. &#34;Blocks Execution stage&#34; still does many random reads - this is reason why it&#39;s slowest stage. We do not recommend running multiple genesis syncs on same Disk. If genesis sync passed, then it&#39;s fine to run multiple Erigon instances on same Disk.&lt;/p&gt; &#xA;&lt;h3&gt;Cloud network drives&lt;/h3&gt; &#xA;&lt;p&gt;(Like gp3) You may read: &lt;a href=&#34;https://github.com/erigontech/erigon/issues/1516#issuecomment-811958891&#34;&gt;https://github.com/erigontech/erigon/issues/1516#issuecomment-811958891&lt;/a&gt; In short: network-disks are bad for blocks execution - because they are designed for parallel/batch workloads (databases with many parallel requests). But blocks execution in Erigon is non-parallel and using blocking-io.&lt;/p&gt; &#xA;&lt;p&gt;What can do:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;reduce disk latency (not throughput, not iops) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;use latency-critical cloud-drives&lt;/li&gt; &#xA;   &lt;li&gt;or attached-NVMe (at least for initial sync)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;increase RAM&lt;/li&gt; &#xA; &lt;li&gt;if you throw enough RAM, then can set env variable &lt;code&gt;ERIGON_SNAPSHOT_MADV_RND=false&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;--db.pagesize=64kb&lt;/code&gt; (less fragmentation, more IO)&lt;/li&gt; &#xA; &lt;li&gt;Or buy/download synced archive node from some 3-rd party Erigon2 snapshots provider&lt;/li&gt; &#xA; &lt;li&gt;Or use Erigon3 (it also sensitive for disk-latency - but it will download 99% of history)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Filesystem&#39;s background features are expensive&lt;/h3&gt; &#xA;&lt;p&gt;For example: btrfs&#39;s autodefrag option - may increase write IO 100x times&lt;/p&gt; &#xA;&lt;h3&gt;Gnome Tracker can kill Erigon&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://wiki.gnome.org/Attic/Tracker&#34;&gt;Gnome Tracker&lt;/a&gt; - detecting miners and kill them.&lt;/p&gt; &#xA;&lt;h3&gt;the --mount option requires BuildKit error&lt;/h3&gt; &#xA;&lt;p&gt;For anyone else that was getting the BuildKit error when trying to start Erigon the old way you can use the below...&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;XDG_DATA_HOME=/preferred/data/folder DOCKER_BUILDKIT=1 COMPOSE_DOCKER_CLI_BUILD=1 make docker-compose&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>crewAIInc/crewAI</title>
    <updated>2025-01-01T01:28:50Z</updated>
    <id>tag:github.com,2025-01-01:/crewAIInc/crewAI</id>
    <link href="https://github.com/crewAIInc/crewAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/crewai_logo.png&#34; alt=&#34;Logo of CrewAI, two people rowing on a boat&#34;&gt;&lt;/p&gt; &#xA; &lt;h1&gt;&lt;strong&gt;CrewAI&lt;/strong&gt;&lt;/h1&gt; &#xA; &lt;p&gt;ü§ñ &lt;strong&gt;CrewAI&lt;/strong&gt;: Production-grade framework for orchestrating sophisticated AI agent systems. From simple automations to complex real-world applications, CrewAI provides precise control and deep customization. By fostering collaborative intelligence through flexible, production-ready architecture, CrewAI empowers agents to work together seamlessly, tackling complex business challenges with predictable, consistent results.&lt;/p&gt; &#xA; &lt;h3&gt; &lt;p&gt;&lt;a href=&#34;https://www.crewai.com/&#34;&gt;Homepage&lt;/a&gt; | &lt;a href=&#34;https://docs.crewai.com/&#34;&gt;Documentation&lt;/a&gt; | &lt;a href=&#34;https://chatg.pt/DWjSBZn&#34;&gt;Chat with Docs&lt;/a&gt; | &lt;a href=&#34;https://github.com/crewAIInc/crewAI-examples&#34;&gt;Examples&lt;/a&gt; | &lt;a href=&#34;https://community.crewai.com&#34;&gt;Discourse&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/crewAIInc/crewAI&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/joaomdmoura/crewAI&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-green.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#why-crewai&#34;&gt;Why CrewAI?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#key-features&#34;&gt;Key Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#understanding-flows-and-crews&#34;&gt;Understanding Flows and Crews&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#how-crewai-compares&#34;&gt;CrewAI vs LangGraph&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#examples&#34;&gt;Examples&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#quick-tutorial&#34;&gt;Quick Tutorial&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#write-job-descriptions&#34;&gt;Write Job Descriptions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#trip-planner&#34;&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#stock-analysis&#34;&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#using-crews-and-flows-together&#34;&gt;Using Crews and Flows Together&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#connecting-your-crew-to-a-model&#34;&gt;Connecting Your Crew to a Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#how-crewai-compares&#34;&gt;How CrewAI Compares&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#frequently-asked-questions-faq&#34;&gt;Frequently Asked Questions (FAQ)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#contribution&#34;&gt;Contribution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#telemetry&#34;&gt;Telemetry&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why CrewAI?&lt;/h2&gt; &#xA;&lt;p&gt;The power of AI collaboration has too much to offer. CrewAI is a standalone framework, built from the ground up without dependencies on Langchain or other agent frameworks. It&#39;s designed to enable AI agents to assume roles, share goals, and operate in a cohesive unit - much like a well-oiled crew. Whether you&#39;re building a smart assistant platform, an automated customer service ensemble, or a multi-agent research team, CrewAI provides the backbone for sophisticated multi-agent interactions.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Learning Resources&lt;/h3&gt; &#xA;&lt;p&gt;Learn CrewAI through our comprehensive courses:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/&#34;&gt;Multi AI Agent Systems with CrewAI&lt;/a&gt; - Master the fundamentals of multi-agent systems&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/&#34;&gt;Practical Multi AI Agents and Advanced Use Cases&lt;/a&gt; - Deep dive into advanced implementations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Understanding Flows and Crews&lt;/h3&gt; &#xA;&lt;p&gt;CrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Crews&lt;/strong&gt;: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Natural, autonomous decision-making between agents&lt;/li&gt; &#xA;   &lt;li&gt;Dynamic task delegation and collaboration&lt;/li&gt; &#xA;   &lt;li&gt;Specialized roles with defined goals and expertise&lt;/li&gt; &#xA;   &lt;li&gt;Flexible problem-solving approaches&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flows&lt;/strong&gt;: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fine-grained control over execution paths for real-world scenarios&lt;/li&gt; &#xA;   &lt;li&gt;Secure, consistent state management between tasks&lt;/li&gt; &#xA;   &lt;li&gt;Clean integration of AI agents with production Python code&lt;/li&gt; &#xA;   &lt;li&gt;Conditional branching for complex business logic&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build complex, production-grade applications&lt;/li&gt; &#xA; &lt;li&gt;Balance autonomy with precise control&lt;/li&gt; &#xA; &lt;li&gt;Handle sophisticated real-world scenarios&lt;/li&gt; &#xA; &lt;li&gt;Maintain clean, maintainable code structure&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Getting Started with Installation&lt;/h3&gt; &#xA;&lt;p&gt;To get started with CrewAI, follow these simple steps:&lt;/p&gt; &#xA;&lt;h3&gt;1. Installation&lt;/h3&gt; &#xA;&lt;p&gt;Ensure you have Python &amp;gt;=3.10 &amp;lt;3.13 installed on your system. CrewAI uses &lt;a href=&#34;https://docs.astral.sh/uv/&#34;&gt;UV&lt;/a&gt; for dependency management and package handling, offering a seamless setup and execution experience.&lt;/p&gt; &#xA;&lt;p&gt;First, install CrewAI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install crewai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to install the &#39;crewai&#39; package along with its optional features that include additional tools for agents, you can do so by using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install &#39;crewai[tools]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The command above installs the basic package and also adds extra components which require more dependencies to function.&lt;/p&gt; &#xA;&lt;h3&gt;Troubleshooting Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;If you encounter issues during installation or usage, here are some common solutions:&lt;/p&gt; &#xA;&lt;h4&gt;Common Issues&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ModuleNotFoundError: No module named &#39;tiktoken&#39;&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Install tiktoken explicitly: &lt;code&gt;pip install &#39;crewai[embeddings]&#39;&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;If using embedchain or other tools: &lt;code&gt;pip install &#39;crewai[tools]&#39;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Failed building wheel for tiktoken&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ensure Rust compiler is installed (see installation steps above)&lt;/li&gt; &#xA;   &lt;li&gt;For Windows: Verify Visual C++ Build Tools are installed&lt;/li&gt; &#xA;   &lt;li&gt;Try upgrading pip: &lt;code&gt;pip install --upgrade pip&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;If issues persist, use a pre-built wheel: &lt;code&gt;pip install tiktoken --prefer-binary&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;2. Setting Up Your Crew with the YAML Configuration&lt;/h3&gt; &#xA;&lt;p&gt;To create a new CrewAI project, run the following CLI (Command Line Interface) command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;crewai create crew &amp;lt;project_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command creates a new project folder with the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;my_project/&#xA;‚îú‚îÄ‚îÄ .gitignore&#xA;‚îú‚îÄ‚îÄ pyproject.toml&#xA;‚îú‚îÄ‚îÄ README.md&#xA;‚îú‚îÄ‚îÄ .env&#xA;‚îî‚îÄ‚îÄ src/&#xA;    ‚îî‚îÄ‚îÄ my_project/&#xA;        ‚îú‚îÄ‚îÄ __init__.py&#xA;        ‚îú‚îÄ‚îÄ main.py&#xA;        ‚îú‚îÄ‚îÄ crew.py&#xA;        ‚îú‚îÄ‚îÄ tools/&#xA;        ‚îÇ   ‚îú‚îÄ‚îÄ custom_tool.py&#xA;        ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py&#xA;        ‚îî‚îÄ‚îÄ config/&#xA;            ‚îú‚îÄ‚îÄ agents.yaml&#xA;            ‚îî‚îÄ‚îÄ tasks.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now start developing your crew by editing the files in the &lt;code&gt;src/my_project&lt;/code&gt; folder. The &lt;code&gt;main.py&lt;/code&gt; file is the entry point of the project, the &lt;code&gt;crew.py&lt;/code&gt; file is where you define your crew, the &lt;code&gt;agents.yaml&lt;/code&gt; file is where you define your agents, and the &lt;code&gt;tasks.yaml&lt;/code&gt; file is where you define your tasks.&lt;/p&gt; &#xA;&lt;h4&gt;To customize your project, you can:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modify &lt;code&gt;src/my_project/config/agents.yaml&lt;/code&gt; to define your agents.&lt;/li&gt; &#xA; &lt;li&gt;Modify &lt;code&gt;src/my_project/config/tasks.yaml&lt;/code&gt; to define your tasks.&lt;/li&gt; &#xA; &lt;li&gt;Modify &lt;code&gt;src/my_project/crew.py&lt;/code&gt; to add your own logic, tools, and specific arguments.&lt;/li&gt; &#xA; &lt;li&gt;Modify &lt;code&gt;src/my_project/main.py&lt;/code&gt; to add custom inputs for your agents and tasks.&lt;/li&gt; &#xA; &lt;li&gt;Add your environment variables into the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Example of a simple crew with a sequential process:&lt;/h4&gt; &#xA;&lt;p&gt;Instantiate your crew:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;crewai create crew latest-ai-development&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Modify the files as needed to fit your use case:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;agents.yaml&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# src/my_project/config/agents.yaml&#xA;researcher:&#xA;  role: &amp;gt;&#xA;    {topic} Senior Data Researcher&#xA;  goal: &amp;gt;&#xA;    Uncover cutting-edge developments in {topic}&#xA;  backstory: &amp;gt;&#xA;    You&#39;re a seasoned researcher with a knack for uncovering the latest&#xA;    developments in {topic}. Known for your ability to find the most relevant&#xA;    information and present it in a clear and concise manner.&#xA;&#xA;reporting_analyst:&#xA;  role: &amp;gt;&#xA;    {topic} Reporting Analyst&#xA;  goal: &amp;gt;&#xA;    Create detailed reports based on {topic} data analysis and research findings&#xA;  backstory: &amp;gt;&#xA;    You&#39;re a meticulous analyst with a keen eye for detail. You&#39;re known for&#xA;    your ability to turn complex data into clear and concise reports, making&#xA;    it easy for others to understand and act on the information you provide.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;tasks.yaml&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# src/my_project/config/tasks.yaml&#xA;research_task:&#xA;  description: &amp;gt;&#xA;    Conduct a thorough research about {topic}&#xA;    Make sure you find any interesting and relevant information given&#xA;    the current year is 2024.&#xA;  expected_output: &amp;gt;&#xA;    A list with 10 bullet points of the most relevant information about {topic}&#xA;  agent: researcher&#xA;&#xA;reporting_task:&#xA;  description: &amp;gt;&#xA;    Review the context you got and expand each topic into a full section for a report.&#xA;    Make sure the report is detailed and contains any and all relevant information.&#xA;  expected_output: &amp;gt;&#xA;    A fully fledge reports with the mains topics, each with a full section of information.&#xA;    Formatted as markdown without &#39;```&#39;&#xA;  agent: reporting_analyst&#xA;  output_file: report.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;crew.py&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# src/my_project/crew.py&#xA;from crewai import Agent, Crew, Process, Task&#xA;from crewai.project import CrewBase, agent, crew, task&#xA;from crewai_tools import SerperDevTool&#xA;&#xA;@CrewBase&#xA;class LatestAiDevelopmentCrew():&#xA;&#x9;&#34;&#34;&#34;LatestAiDevelopment crew&#34;&#34;&#34;&#xA;&#xA;&#x9;@agent&#xA;&#x9;def researcher(self) -&amp;gt; Agent:&#xA;&#x9;&#x9;return Agent(&#xA;&#x9;&#x9;&#x9;config=self.agents_config[&#39;researcher&#39;],&#xA;&#x9;&#x9;&#x9;verbose=True,&#xA;&#x9;&#x9;&#x9;tools=[SerperDevTool()]&#xA;&#x9;&#x9;)&#xA;&#xA;&#x9;@agent&#xA;&#x9;def reporting_analyst(self) -&amp;gt; Agent:&#xA;&#x9;&#x9;return Agent(&#xA;&#x9;&#x9;&#x9;config=self.agents_config[&#39;reporting_analyst&#39;],&#xA;&#x9;&#x9;&#x9;verbose=True&#xA;&#x9;&#x9;)&#xA;&#xA;&#x9;@task&#xA;&#x9;def research_task(self) -&amp;gt; Task:&#xA;&#x9;&#x9;return Task(&#xA;&#x9;&#x9;&#x9;config=self.tasks_config[&#39;research_task&#39;],&#xA;&#x9;&#x9;)&#xA;&#xA;&#x9;@task&#xA;&#x9;def reporting_task(self) -&amp;gt; Task:&#xA;&#x9;&#x9;return Task(&#xA;&#x9;&#x9;&#x9;config=self.tasks_config[&#39;reporting_task&#39;],&#xA;&#x9;&#x9;&#x9;output_file=&#39;report.md&#39;&#xA;&#x9;&#x9;)&#xA;&#xA;&#x9;@crew&#xA;&#x9;def crew(self) -&amp;gt; Crew:&#xA;&#x9;&#x9;&#34;&#34;&#34;Creates the LatestAiDevelopment crew&#34;&#34;&#34;&#xA;&#x9;&#x9;return Crew(&#xA;&#x9;&#x9;&#x9;agents=self.agents, # Automatically created by the @agent decorator&#xA;&#x9;&#x9;&#x9;tasks=self.tasks, # Automatically created by the @task decorator&#xA;&#x9;&#x9;&#x9;process=Process.sequential,&#xA;&#x9;&#x9;&#x9;verbose=True,&#xA;&#x9;&#x9;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;main.py&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python&#xA;# src/my_project/main.py&#xA;import sys&#xA;from latest_ai_development.crew import LatestAiDevelopmentCrew&#xA;&#xA;def run():&#xA;    &#34;&#34;&#34;&#xA;    Run the crew.&#xA;    &#34;&#34;&#34;&#xA;    inputs = {&#xA;        &#39;topic&#39;: &#39;AI Agents&#39;&#xA;    }&#xA;    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Running Your Crew&lt;/h3&gt; &#xA;&lt;p&gt;Before running your crew, make sure you have the following keys set as environment variables in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;OpenAI API key&lt;/a&gt; (or other LLM API key): &lt;code&gt;OPENAI_API_KEY=sk-...&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;A &lt;a href=&#34;https://serper.dev/&#34;&gt;Serper.dev&lt;/a&gt; API key: &lt;code&gt;SERPER_API_KEY=YOUR_KEY_HERE&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Lock the dependencies and install them by using the CLI command but first, navigate to your project directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd my_project&#xA;crewai install (Optional)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run your crew, execute the following command in the root of your project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;crewai run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/my_project/main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If an error happens due to the usage of poetry, please run the following command to update your crewai package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;crewai update&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see the output in the console and the &lt;code&gt;report.md&lt;/code&gt; file should be created in the root of your project with the full final report.&lt;/p&gt; &#xA;&lt;p&gt;In addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results. &lt;a href=&#34;https://docs.crewai.com/core-concepts/Processes/&#34;&gt;See more about the processes here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: CrewAI is a standalone framework built from the ground up, without dependencies on Langchain or other agent frameworks.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep Customization&lt;/strong&gt;: Build sophisticated agents with full control over the system - from overriding inner prompts to accessing low-level APIs. Customize roles, goals, tools, and behaviors while maintaining clean abstractions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Autonomous Inter-Agent Delegation&lt;/strong&gt;: Agents can autonomously delegate tasks and inquire amongst themselves, enabling complex problem-solving in real-world scenarios.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible Task Management&lt;/strong&gt;: Define and customize tasks with granular control, from simple operations to complex multi-step processes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Production-Grade Architecture&lt;/strong&gt;: Support for both high-level abstractions and low-level customization, with robust error handling and state management.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Predictable Results&lt;/strong&gt;: Ensure consistent, accurate outputs through programmatic guardrails, agent training capabilities, and flow-based execution control. See our &lt;a href=&#34;https://docs.crewai.com/how-to/guardrails/&#34;&gt;documentation on guardrails&lt;/a&gt; for implementation details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model Flexibility&lt;/strong&gt;: Run your crew using OpenAI or open source models with production-ready integrations. See &lt;a href=&#34;https://docs.crewai.com/how-to/LLM-Connections/&#34;&gt;Connect CrewAI to LLMs&lt;/a&gt; for detailed configuration options.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Event-Driven Flows&lt;/strong&gt;: Build complex, real-world workflows with precise control over execution paths, state management, and conditional logic.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Process Orchestration&lt;/strong&gt;: Achieve any workflow pattern through flows - from simple sequential and hierarchical processes to complex, custom orchestration patterns with conditional branching and parallel execution.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/crewAI-mindmap.png&#34; alt=&#34;CrewAI Mind Map&#34; title=&#34;CrewAI Mind Map&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;You can test different real life examples of AI crews in the &lt;a href=&#34;https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file&#34;&gt;CrewAI-examples repo&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/crewAIInc/crewAI-examples/tree/main/landing_page_generator&#34;&gt;Landing Page Generator&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.crewai.com/how-to/Human-Input-on-Execution&#34;&gt;Having Human input on the execution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/crewAIInc/crewAI-examples/tree/main/trip_planner&#34;&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/crewAIInc/crewAI-examples/tree/main/stock_analysis&#34;&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Quick Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=tnejrr-0a94&#34; title=&#34;CrewAI Tutorial&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg&#34; alt=&#34;CrewAI Tutorial&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Write Job Descriptions&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/crewAIInc/crewAI-examples/tree/main/job-posting&#34;&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=u98wEMz-9to&#34; title=&#34;Jobs postings&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg&#34; alt=&#34;Jobs postings&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Trip Planner&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/crewAIInc/crewAI-examples/tree/main/trip_planner&#34;&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xis7rWp-hjs&#34; title=&#34;Trip Planner&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg&#34; alt=&#34;Trip Planner&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Stock Analysis&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/crewAIInc/crewAI-examples/tree/main/stock_analysis&#34;&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=e0Uj4yWdaAg&#34; title=&#34;Stock Analysis&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg&#34; alt=&#34;Stock Analysis&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Using Crews and Flows Together&lt;/h3&gt; &#xA;&lt;p&gt;CrewAI&#39;s power truly shines when combining Crews with Flows to create sophisticated automation pipelines. Here&#39;s how you can orchestrate multiple Crews within a Flow:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from crewai.flow.flow import Flow, listen, start, router&#xA;from crewai import Crew, Agent, Task&#xA;from pydantic import BaseModel&#xA;&#xA;# Define structured state for precise control&#xA;class MarketState(BaseModel):&#xA;    sentiment: str = &#34;neutral&#34;&#xA;    confidence: float = 0.0&#xA;    recommendations: list = []&#xA;&#xA;class AdvancedAnalysisFlow(Flow[MarketState]):&#xA;    @start()&#xA;    def fetch_market_data(self):&#xA;        # Demonstrate low-level control with structured state&#xA;        self.state.sentiment = &#34;analyzing&#34;&#xA;        return {&#34;sector&#34;: &#34;tech&#34;, &#34;timeframe&#34;: &#34;1W&#34;}  # These parameters match the task description template&#xA;&#xA;    @listen(fetch_market_data)&#xA;    def analyze_with_crew(self, market_data):&#xA;        # Show crew agency through specialized roles&#xA;        analyst = Agent(&#xA;            role=&#34;Senior Market Analyst&#34;,&#xA;            goal=&#34;Conduct deep market analysis with expert insight&#34;,&#xA;            backstory=&#34;You&#39;re a veteran analyst known for identifying subtle market patterns&#34;&#xA;        )&#xA;        researcher = Agent(&#xA;            role=&#34;Data Researcher&#34;,&#xA;            goal=&#34;Gather and validate supporting market data&#34;,&#xA;            backstory=&#34;You excel at finding and correlating multiple data sources&#34;&#xA;        )&#xA;        &#xA;        analysis_task = Task(&#xA;            description=&#34;Analyze {sector} sector data for the past {timeframe}&#34;,&#xA;            expected_output=&#34;Detailed market analysis with confidence score&#34;,&#xA;            agent=analyst&#xA;        )&#xA;        research_task = Task(&#xA;            description=&#34;Find supporting data to validate the analysis&#34;,&#xA;            expected_output=&#34;Corroborating evidence and potential contradictions&#34;,&#xA;            agent=researcher&#xA;        )&#xA;        &#xA;        # Demonstrate crew autonomy&#xA;        analysis_crew = Crew(&#xA;            agents=[analyst, researcher],&#xA;            tasks=[analysis_task, research_task],&#xA;            process=Process.sequential,&#xA;            verbose=True&#xA;        )&#xA;        return analysis_crew.kickoff(inputs=market_data)  # Pass market_data as named inputs&#xA;&#xA;    @router(analyze_with_crew)&#xA;    def determine_next_steps(self):&#xA;        # Show flow control with conditional routing&#xA;        if self.state.confidence &amp;gt; 0.8:&#xA;            return &#34;high_confidence&#34;&#xA;        elif self.state.confidence &amp;gt; 0.5:&#xA;            return &#34;medium_confidence&#34;&#xA;        return &#34;low_confidence&#34;&#xA;&#xA;    @listen(&#34;high_confidence&#34;)&#xA;    def execute_strategy(self):&#xA;        # Demonstrate complex decision making&#xA;        strategy_crew = Crew(&#xA;            agents=[&#xA;                Agent(role=&#34;Strategy Expert&#34;,&#xA;                      goal=&#34;Develop optimal market strategy&#34;)&#xA;            ],&#xA;            tasks=[&#xA;                Task(description=&#34;Create detailed strategy based on analysis&#34;,&#xA;                     expected_output=&#34;Step-by-step action plan&#34;)&#xA;            ]&#xA;        )&#xA;        return strategy_crew.kickoff()&#xA;&#xA;    @listen(&#34;medium_confidence&#34;, &#34;low_confidence&#34;)&#xA;    def request_additional_analysis(self):&#xA;        self.state.recommendations.append(&#34;Gather more data&#34;)&#xA;        return &#34;Additional analysis required&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This example demonstrates how to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use Python code for basic data operations&lt;/li&gt; &#xA; &lt;li&gt;Create and execute Crews as steps in your workflow&lt;/li&gt; &#xA; &lt;li&gt;Use Flow decorators to manage the sequence of operations&lt;/li&gt; &#xA; &lt;li&gt;Implement conditional branching based on Crew results&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Connecting Your Crew to a Model&lt;/h2&gt; &#xA;&lt;p&gt;CrewAI supports using various LLMs through a variety of connection options. By default your agents will use the OpenAI API when querying the model. However, there are several other ways to allow your agents to connect to models. For example, you can configure your agents to use a local model via the Ollama tool.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://docs.crewai.com/how-to/LLM-Connections/&#34;&gt;Connect CrewAI to LLMs&lt;/a&gt; page for details on configuring you agents&#39; connections to models.&lt;/p&gt; &#xA;&lt;h2&gt;How CrewAI Compares&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;CrewAI&#39;s Advantage&lt;/strong&gt;: CrewAI combines autonomous agent intelligence with precise workflow control through its unique Crews and Flows architecture. The framework excels at both high-level orchestration and low-level customization, enabling complex, production-grade systems with granular control.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: While LangGraph provides a foundation for building agent workflows, its approach requires significant boilerplate code and complex state management patterns. The framework&#39;s tight coupling with LangChain can limit flexibility when implementing custom agent behaviors or integrating with external systems.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;P.S. CrewAI demonstrates significant performance advantages over LangGraph, executing 5.76x faster in certain cases like this QA task example (&lt;a href=&#34;https://github.com/crewAIInc/crewAI-examples/tree/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/QA%20Agent&#34;&gt;see comparison&lt;/a&gt;) while achieving higher evaluation scores with faster completion times in certain coding tasks, like in this example (&lt;a href=&#34;https://github.com/crewAIInc/crewAI-examples/raw/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/Coding%20Assistant/coding_assistant_eval.ipynb&#34;&gt;detailed analysis&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Autogen&lt;/strong&gt;: While Autogen excels at creating conversational agents capable of working together, it lacks an inherent concept of process. In Autogen, orchestrating agents&#39; interactions requires additional programming, which can become complex and cumbersome as the scale of tasks grows.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ChatDev&lt;/strong&gt;: ChatDev introduced the idea of processes into the realm of AI agents, but its implementation is quite rigid. Customizations in ChatDev are limited and not geared towards production environments, which can hinder scalability and flexibility in real-world applications.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;CrewAI is open-source and we welcome contributions. If you&#39;re looking to contribute, please:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork the repository.&lt;/li&gt; &#xA; &lt;li&gt;Create a new branch for your feature.&lt;/li&gt; &#xA; &lt;li&gt;Add your feature or improvement.&lt;/li&gt; &#xA; &lt;li&gt;Send a pull request.&lt;/li&gt; &#xA; &lt;li&gt;We appreciate your input!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installing Dependencies&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv lock&#xA;uv sync&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Virtual Env&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pre-commit hooks&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pre-commit install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Tests&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv run pytest .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running static type checks&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uvx mypy src&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Packaging&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installing Locally&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install dist/*.tar.gz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Telemetry&lt;/h2&gt; &#xA;&lt;p&gt;CrewAI uses anonymous telemetry to collect usage data with the main purpose of helping us improve the library by focusing our efforts on the most used features, integrations and tools.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s pivotal to understand that &lt;strong&gt;NO data is collected&lt;/strong&gt; concerning prompts, task descriptions, agents&#39; backstories or goals, usage of tools, API calls, responses, any data processed by the agents, or secrets and environment variables, with the exception of the conditions mentioned. When the &lt;code&gt;share_crew&lt;/code&gt; feature is enabled, detailed data including task descriptions, agents&#39; backstories or goals, and other specific attributes are collected to provide deeper insights while respecting user privacy. Users can disable telemetry by setting the environment variable OTEL_SDK_DISABLED to true.&lt;/p&gt; &#xA;&lt;p&gt;Data collected includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Version of CrewAI &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;So we can understand how many users are using the latest version&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Version of Python &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;So we can decide on what versions to better support&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;General OS (e.g. number of CPUs, macOS/Windows/Linux) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;So we know what OS we should focus on and if we could build specific OS related features&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Number of agents and tasks in a crew &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;So we make sure we are testing internally with similar use cases and educate people on the best practices&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Crew Process being used &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Understand where we should focus our efforts&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;If Agents are using memory or allowing delegation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Understand if we improved the features or maybe even drop them&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;If Tasks are being executed in parallel or sequentially &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Understand if we should focus more on parallel execution&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Language model being used &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Improved support on most used languages&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Roles of agents in a crew &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Understand high level use cases so we can build better tools, integrations and examples about it&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Tools names available &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Understand out of the publicly available tools, which ones are being used the most so we can improve them&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Users can opt-in to Further Telemetry, sharing the complete telemetry data by setting the &lt;code&gt;share_crew&lt;/code&gt; attribute to &lt;code&gt;True&lt;/code&gt; on their Crews. Enabling &lt;code&gt;share_crew&lt;/code&gt; results in the collection of detailed crew and task execution data, including &lt;code&gt;goal&lt;/code&gt;, &lt;code&gt;backstory&lt;/code&gt;, &lt;code&gt;context&lt;/code&gt;, and &lt;code&gt;output&lt;/code&gt; of tasks. This enables a deeper insight into usage patterns while respecting the user&#39;s choice to share.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;CrewAI is released under the &lt;a href=&#34;https://github.com/crewAIInc/crewAI/raw/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Frequently Asked Questions (FAQ)&lt;/h2&gt; &#xA;&lt;h3&gt;Q: What is CrewAI?&lt;/h3&gt; &#xA;&lt;p&gt;A: CrewAI is a cutting-edge framework for orchestrating role-playing, autonomous AI agents. It enables agents to work together seamlessly, tackling complex tasks through collaborative intelligence.&lt;/p&gt; &#xA;&lt;h3&gt;Q: How do I install CrewAI?&lt;/h3&gt; &#xA;&lt;p&gt;A: You can install CrewAI using pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install crewai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For additional tools, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install &#39;crewai[tools]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Q: Can I use CrewAI with local models?&lt;/h3&gt; &#xA;&lt;p&gt;A: Yes, CrewAI supports various LLMs, including local models. You can configure your agents to use local models via tools like Ollama &amp;amp; LM Studio. Check the &lt;a href=&#34;https://docs.crewai.com/how-to/LLM-Connections/&#34;&gt;LLM Connections documentation&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Q: What are the key features of CrewAI?&lt;/h3&gt; &#xA;&lt;p&gt;A: Key features include role-based agent design, autonomous inter-agent delegation, flexible task management, process-driven execution, output saving as files, and compatibility with both open-source and proprietary models.&lt;/p&gt; &#xA;&lt;h3&gt;Q: How does CrewAI compare to other AI orchestration tools?&lt;/h3&gt; &#xA;&lt;p&gt;A: CrewAI is designed with production in mind, offering flexibility similar to Autogen&#39;s conversational agents and structured processes like ChatDev, but with more adaptability for real-world applications.&lt;/p&gt; &#xA;&lt;h3&gt;Q: Is CrewAI open-source?&lt;/h3&gt; &#xA;&lt;p&gt;A: Yes, CrewAI is open-source and welcomes contributions from the community.&lt;/p&gt; &#xA;&lt;h3&gt;Q: Does CrewAI collect any data?&lt;/h3&gt; &#xA;&lt;p&gt;A: CrewAI uses anonymous telemetry to collect usage data for improvement purposes. No sensitive data (like prompts, task descriptions, or API calls) is collected. Users can opt-in to share more detailed data by setting &lt;code&gt;share_crew=True&lt;/code&gt; on their Crews.&lt;/p&gt; &#xA;&lt;h3&gt;Q: Where can I find examples of CrewAI in action?&lt;/h3&gt; &#xA;&lt;p&gt;A: You can find various real-life examples in the &lt;a href=&#34;https://github.com/crewAIInc/crewAI-examples&#34;&gt;CrewAI-examples repository&lt;/a&gt;, including trip planners, stock analysis tools, and more.&lt;/p&gt; &#xA;&lt;h3&gt;Q: What is the difference between Crews and Flows?&lt;/h3&gt; &#xA;&lt;p&gt;A: Crews and Flows serve different but complementary purposes in CrewAI. Crews are teams of AI agents working together to accomplish specific tasks through role-based collaboration, delivering accurate and predictable results. Flows, on the other hand, are event-driven workflows that can orchestrate both Crews and regular Python code, allowing you to build complex automation pipelines with secure state management and conditional execution paths.&lt;/p&gt; &#xA;&lt;h3&gt;Q: How can I contribute to CrewAI?&lt;/h3&gt; &#xA;&lt;p&gt;A: Contributions are welcome! You can fork the repository, create a new branch for your feature, add your improvement, and send a pull request. Check the Contribution section in the README for more details.&lt;/p&gt;</summary>
  </entry>
</feed>