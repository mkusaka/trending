<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-04T01:29:01Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AIGC-Audio/AudioGPT</title>
    <updated>2023-05-04T01:29:01Z</updated>
    <id>tag:github.com,2023-05-04:/AIGC-Audio/AudioGPT</id>
    <link href="https://github.com/AIGC-Audio/AudioGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.12995&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AIGC-Audio/AudioGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AIGC-Audio/AudioGPT?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://visitor-badge.glitch.me/badge?page_id=AIGC-Audio.AudioGPT&#34; alt=&#34;visitors&#34;&gt; &lt;a href=&#34;https://huggingface.co/spaces/AIGC-Audio/AudioGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We provide our implementation and pretrained models as open source in this repository.&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/AIGC-Audio/AudioGPT/main/run.md&#34;&gt;run.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Capabilities&lt;/h2&gt; &#xA;&lt;p&gt;Here we list the capability of AudioGPT at this time. More supported models and tasks are coming soon. For prompt examples, refer to &lt;a href=&#34;https://raw.githubusercontent.com/AIGC-Audio/AudioGPT/main/assets/README.md&#34;&gt;asset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Speech&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Foundation Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-to-Speech&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;FastSpeech&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;SyntaSpeech&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;VITS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Style Transfer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;GenerSpeech&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speech Recognition&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;whisper&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Conformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speech Enhancement&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;ConvTasNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speech Separation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;TF-GridNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speech Translation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Multi-decoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WIP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Mono-to-Binaural&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;NeuralWarp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Sing&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Foundation Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-to-Sing&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;DiffSinger&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;VISinger&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Audio&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Foundation Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-to-Audio&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Make-An-Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Audio Inpainting&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Make-An-Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-to-Audio&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Make-An-Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Sound Detection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Audio-transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Target Sound Detection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;TSDNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Sound Extraction&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;LASSNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Talking Head&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Foundation Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Talking Head Synthesis&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;GeneFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate the open source of the following projects:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/espnet/espnet&#34;&gt;ESPNet&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/NATSpeech/NATSpeech&#34;&gt;NATSpeech&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/microsoft/visual-chatgpt&#34;&gt;Visual ChatGPT&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/huggingface&#34;&gt;Hugging Face&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; ‚ÄÇ&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>gventuri/pandas-ai</title>
    <updated>2023-05-04T01:29:01Z</updated>
    <id>tag:github.com,2023-05-04:/gventuri/pandas-ai</id>
    <link href="https://github.com/gventuri/pandas-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Pandas AI is a Python library that integrates generative artificial intelligence capabilities into Pandas, making dataframes conversational&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PandasAI üêº&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gventuri/pandas-ai/actions/workflows/ci.yml/badge.svg&#34;&gt;&lt;img src=&#34;https://github.com/gventuri/pandas-ai/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;lint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/kF7FqH2FwS&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/kF7FqH2FwS?style=flat&amp;amp;compact=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/pandasai&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/pandasai/month&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1rKz7TudOeCeKGHekw7JFNL4sagN9hon-?usp=sharing&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Pandas AI is a Python library that adds generative artificial intelligence capabilities to Pandas, the popular data analysis and manipulation tool. It is designed to be used in conjunction with Pandas, and is not a replacement for it.&lt;/p&gt; &#xA;&lt;!-- Add images/pandas-ai.png --&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gventuri/pandas-ai/main/images/pandas-ai.png?raw=true&#34; alt=&#34;PandasAI&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Try out PandasAI in your browser:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1rKz7TudOeCeKGHekw7JFNL4sagN9hon-?usp=sharing&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pandasai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;PandasAI is designed to be used in conjunction with Pandas. It makes Pandas conversational, allowing you to ask questions about your data and get answers back, in the form of Pandas DataFrames. For example, you can ask PandasAI to find all the rows in a DataFrame where the value of a column is greater than 5, and it will return a DataFrame containing only those rows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd&#xA;from pandasai import PandasAI&#xA;&#xA;# Sample DataFrame&#xA;df = pd.DataFrame({&#xA;    &#34;country&#34;: [&#34;United States&#34;, &#34;United Kingdom&#34;, &#34;France&#34;, &#34;Germany&#34;, &#34;Italy&#34;, &#34;Spain&#34;, &#34;Canada&#34;, &#34;Australia&#34;, &#34;Japan&#34;, &#34;China&#34;],&#xA;    &#34;gdp&#34;: [21400000, 2940000, 2830000, 3870000, 2160000, 1350000, 1780000, 1320000, 516000, 14000000],&#xA;    &#34;happiness_index&#34;: [7.3, 7.2, 6.5, 7.0, 6.0, 6.3, 7.3, 7.3, 5.9, 5.0]&#xA;})&#xA;&#xA;# Instantiate a LLM&#xA;from pandasai.llm.openai import OpenAI&#xA;llm = OpenAI()&#xA;&#xA;pandas_ai = PandasAI(llm)&#xA;pandas_ai.run(df, prompt=&#39;Which are the 5 happiest countries?&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above code will return the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;0     United States&#xA;6            Canada&#xA;7         Australia&#xA;1    United Kingdom&#xA;3           Germany&#xA;Name: country, dtype: object&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Of course, you can also ask PandasAI to perform more complex queries. For example, you can ask PandasAI to find the sum of the GDPs of the 2 unhappiest countries:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pandas_ai.run(df, prompt=&#39;What is the sum of the GDPs of the 2 unhappiest countries?&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above code will return the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;14516000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also ask PandasAI to draw a graph:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pandas_ai.run(&#xA;    df,&#xA;    &#34;Plot the histogram of countries showing for each the gpd, using different colors for each bar&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gventuri/pandas-ai/main/images/histogram-chart.png?raw=true&#34; alt=&#34;Chart&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can find more examples in the &lt;a href=&#34;https://raw.githubusercontent.com/gventuri/pandas-ai/main/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Environment Variables&lt;/h2&gt; &#xA;&lt;p&gt;In order to set the API key for the LLM (Hugging Face Hub, OpenAI), you need to set the appropriate environment variables. You can do this by copying the &lt;code&gt;.env.example&lt;/code&gt; file to &lt;code&gt;.env&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, edit the &lt;code&gt;.env&lt;/code&gt; file and set the appropriate values.&lt;/p&gt; &#xA;&lt;p&gt;As an alternative, you can also pass the environment variables directly to the constructor of the LLM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OpenAI&#xA;llm = OpenAI(api_token=&#34;YOUR_OPENAI_API_KEY&#34;)&#xA;&#xA;# OpenAssistant&#xA;llm = OpenAssistant(api_token=&#34;YOUR_HF_API_KEY&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PandasAI is licensed under the MIT License. See the LICENSE file for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! Please check out the todos below, and feel free to open a pull request. For more information, please see the &lt;a href=&#34;https://raw.githubusercontent.com/gventuri/pandas-ai/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Todo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add support for more LLMs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Make PandasAI available from a CLI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create a web interface for PandasAI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add unit tests&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add contributing guidelines&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add CI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add support for conversational responses&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>georgia-tech-db/eva</title>
    <updated>2023-05-04T01:29:01Z</updated>
    <id>tag:github.com,2023-05-04:/georgia-tech-db/eva</id>
    <link href="https://github.com/georgia-tech-db/eva" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI-Relational Database System | SQL meets Deep Learning&lt;/p&gt;&lt;hr&gt;&lt;div&gt; &#xA; &lt;a href=&#34;https://evadb.readthedocs.io/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/georgia-tech-db/eva/master/docs/images/eva/eva-banner.png&#34; alt=&#34;EVA&#34; width=&#34;1000px&#34; margin-left=&#34;-5px&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;EVA AI-Relational Database System&lt;/h1&gt; &#xA;&lt;div&gt; &#xA; &lt;a href=&#34;https://colab.research.google.com/github/georgia-tech-db/eva/blob/master/tutorials/03-emotion-analysis.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open EVA on Colab&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://join.slack.com/t/eva-db/shared_invite/zt-1i10zyddy-PlJ4iawLdurDv~aIAq90Dg&#34;&gt; &lt;img alt=&#34;Slack&#34; src=&#34;https://img.shields.io/badge/slack-eva-ff69b4.svg?logo=slack&#34;&gt; &lt;/a&gt; &#xA; &lt;img alt=&#34;PyPI&#34; src=&#34;https://img.shields.io/pypi/v/evadb.svg?sanitize=true&#34;&gt; &#xA; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/badge/license-Apache%202-brightgreen.svg?logo=apache&#34;&gt; &#xA; &lt;img alt=&#34;Coverage Status&#34; src=&#34;https://coveralls.io/repos/github/georgia-tech-db/eva/badge.svg?branch=master&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/orgs/georgia-tech-db/projects/3&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/eva-roadmap-ff3423&#34; alt=&#34;Roadmap&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://pepy.tech/project/evadb&#34;&gt; &lt;img alt=&#34;Downloads&#34; src=&#34;https://static.pepy.tech/badge/evadb/month&#34;&gt; &lt;/a&gt; &#xA; &lt;img alt=&#34;Python Versions&#34; src=&#34;https://img.shields.io/badge/Python--versions-3.7%20|%203.8%20|%203.9%20|%203.10-brightgreen&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;&lt;/b&gt;&lt;/p&gt;&#xA;&lt;h3&gt;&lt;b&gt;EVA is a database system for building simpler and faster AI-powered applications.&lt;/b&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;EVA is designed for supporting database applications that operate on both structured (tables, feature vectors) and unstructured data (videos, podcasts, PDFs, etc.) using deep learning models. It accelerates AI pipelines by 10-100x using a collection of optimizations inspired by time-tested relational database systems, including function caching, sampling, and cost-based predicate reordering. EVA supports an AI-oriented SQL-like query language tailored for analyzing unstructured data. It comes with a wide range of models for analyzing unstructured data, including models for image classification, object detection, OCR, text sentiment classification, face detection, etc. It is fully implemented in Python and licensed under the Apache license.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/georgia-tech-db/eva/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/georgia-tech-db/eva/master/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/georgia-tech-db/eva/master/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/orgs/georgia-tech-db/projects/3&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/georgia-tech-db/eva/master/#demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/georgia-tech-db/eva/master/#illustrative-applications&#34;&gt;Illustrative Applications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/georgia-tech-db/eva/master/#community-and-support&#34;&gt;Community and Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/georgia-tech-db/eva/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/georgia-tech-db/eva/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üîÆ Build simpler AI-powered applications using short SQL-like queries&lt;/li&gt; &#xA; &lt;li&gt;‚ö°Ô∏è 10-100x faster AI pipelines using AI-centric query optimization&lt;/li&gt; &#xA; &lt;li&gt;üí∞ Save money spent on GPU-driven inference&lt;/li&gt; &#xA; &lt;li&gt;üöÄ First-class support for your custom deep learning models through user-defined functions&lt;/li&gt; &#xA; &lt;li&gt;üì¶ Built-in caching to eliminate redundant model invocations across queries&lt;/li&gt; &#xA; &lt;li&gt;‚å®Ô∏è First-class support for PyTorch and HuggingFace models&lt;/li&gt; &#xA; &lt;li&gt;üêç Installable via pip and fully implemented in Python&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Here are some illustrative EVA-backed applications (all of them are Jupyter notebooks that can be opened in Google Colab):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üîÆ &lt;a href=&#34;https://evadb.readthedocs.io/en/stable/source/tutorials/02-object-detection.html&#34;&gt;Analysing traffic flow at an intersection &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üîÆ &lt;a href=&#34;https://evadb.readthedocs.io/en/stable/source/tutorials/03-emotion-analysis.html&#34;&gt;Examining the emotion palette of actors in a movie&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üîÆ &lt;a href=&#34;https://evadb.readthedocs.io/en/stable/source/tutorials/01-mnist.html&#34;&gt;Classifying images based on their content&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üîÆ &lt;a href=&#34;https://evadb.readthedocs.io/en/latest/source/tutorials/07-object-segmentation-huggingface.html&#34;&gt;Image Segmentation using Hugging Face&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üîÆ &lt;a href=&#34;https://github.com/georgia-tech-db/license-plate-recognition&#34;&gt;Recognizing license plates &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üîÆ &lt;a href=&#34;https://github.com/georgia-tech-db/toxicity-classification&#34;&gt;Analysing toxicity of social media memes &lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://evadb.readthedocs.io/&#34;&gt;Detailed Documentation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you are wondering why you might need an AI-relational database system, start with the page on &lt;a href=&#34;https://evadb.readthedocs.io/en/stable/source/overview/video.html#&#34;&gt;Video Database Systems&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The &lt;a href=&#34;https://evadb.readthedocs.io/en/stable/source/overview/installation.html&#34;&gt;Getting Started&lt;/a&gt; page shows how you can use EVA for different AI pipelines, and how you can easily extend EVA by defining an user-defined function that wraps around your custom deep learning model.&lt;/li&gt; &#xA;   &lt;li&gt;The &lt;a href=&#34;https://evadb.readthedocs.io/en/stable/source/tutorials/index.html&#34;&gt;User Guides&lt;/a&gt; section contains Jupyter Notebooks that demonstrate how to use various features of EVA. Each notebook includes a link to Google Colab to run the code.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/georgia-tech-db/eva/raw/master/tutorials/03-emotion-analysis.ipynb&#34;&gt;Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/eva-db/shared_invite/zt-1i10zyddy-PlJ4iawLdurDv~aIAq90Dg&#34;&gt;Join us on Slack!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/orgs/georgia-tech-db/projects/3&#34;&gt;Medium-Term Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/georgia-tech-db/eva/raw/master/tutorials/03-emotion-analysis.ipynb&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install EVA using the pip package manager. EVA supports Python versions &amp;gt;= 3.7.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install evadb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To launch and connect to an EVA server in a Jupyter notebook, check out this &lt;a href=&#34;https://github.com/georgia-tech-db/eva/raw/master/tutorials/03-emotion-analysis.ipynb&#34;&gt;illustrative emotion analysis notebook&lt;/a&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cursor = connect_to_server()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Load a video onto the EVA server (we use &lt;a href=&#34;https://raw.githubusercontent.com/georgia-tech-db/eva/master/data/ua_detrac/ua_detrac.mp4&#34;&gt;ua_detrac.mp4&lt;/a&gt; for illustration):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mysql&#34;&gt;LOAD VIDEO &#34;data/ua_detrac/ua_detrac.mp4&#34; INTO TrafficVideo;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;That&#39;s it! You can now run queries over the loaded video:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mysql&#34;&gt;SELECT id, data FROM TrafficVideo WHERE id &amp;lt; 5;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Search for frames in the video that contain a car&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mysql&#34;&gt;SELECT id, data FROM TrafficVideo WHERE [&#39;car&#39;] &amp;lt;@ YoloV5(data).labels;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Source Video&lt;/th&gt; &#xA;   &lt;th&gt;Query Result&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Source Video&#34; src=&#34;https://github.com/georgia-tech-db/eva/releases/download/v0.1.0/traffic-input.webp&#34; width=&#34;300&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Query Result&#34; src=&#34;https://github.com/georgia-tech-db/eva/releases/download/v0.1.0/traffic-output.webp&#34; width=&#34;300&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Search for frames in the video that contain a pedestrian and a car&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mysql&#34;&gt;SELECT id, data FROM TrafficVideo WHERE [&#39;pedestrian&#39;, &#39;car&#39;] &amp;lt;@ YoloV5(data).labels;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Search for frames with more than three cars&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mysql&#34;&gt;SELECT id, data FROM TrafficVideo WHERE ArrayCount(YoloV5(data).labels, &#39;car&#39;) &amp;gt; 3;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Use your custom deep learning model in queries&lt;/strong&gt; with a user-defined function (UDF):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mysql&#34;&gt;CREATE UDF IF NOT EXISTS MyUDF&#xA;INPUT  (frame NDARRAY UINT8(3, ANYDIM, ANYDIM))&#xA;OUTPUT (labels NDARRAY STR(ANYDIM), bboxes NDARRAY FLOAT32(ANYDIM, 4),&#xA;        scores NDARRAY FLOAT32(ANYDIM))&#xA;TYPE  Classification&#xA;IMPL  &#39;eva/udfs/fastrcnn_object_detector.py&#39;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Compose multiple models in a single query&lt;/strong&gt; to set up useful AI pipelines.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mysql&#34;&gt;   -- Analyse emotions of faces in a video&#xA;   SELECT id, bbox, EmotionDetector(Crop(data, bbox)) &#xA;   FROM MovieVideo JOIN LATERAL UNNEST(FaceDetector(data)) AS Face(bbox, conf)  &#xA;   WHERE id &amp;lt; 15;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;EVA runs queries faster using its AI-centric query optimizer&lt;/strong&gt;. Two key optimizations are:&lt;/p&gt; &lt;p&gt;üíæ &lt;strong&gt;Caching&lt;/strong&gt;: EVA automatically caches and reuses previous query results (especially model inference results), eliminating redundant computation and reducing query processing time.&lt;/p&gt; &lt;p&gt;üéØ &lt;strong&gt;Predicate Reordering&lt;/strong&gt;: EVA optimizes the order in which the query predicates are evaluated (e.g., runs the faster, more selective model first), leading to faster queries and lower inference costs.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Consider these two exploratory queries on a dataset of üêï images: &lt;img align=&#34;right&#34; style=&#34;display:inline;&#34; width=&#34;40%&#34; src=&#34;https://github.com/georgia-tech-db/eva/raw/master/data/assets/eva_performance_comparison.png?raw=true&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mysql&#34;&gt;  -- Query 1: Find all images of black-colored dogs&#xA;  SELECT id, bbox FROM dogs &#xA;  JOIN LATERAL UNNEST(YoloV5(data)) AS Obj(label, bbox, score) &#xA;  WHERE Obj.label = &#39;dog&#39; &#xA;    AND Color(Crop(data, bbox)) = &#39;black&#39;; &#xA;&#xA;  -- Query 2: Find all Great Danes that are black-colored&#xA;  SELECT id, bbox FROM dogs &#xA;  JOIN LATERAL UNNEST(YoloV5(data)) AS Obj(label, bbox, score) &#xA;  WHERE Obj.label = &#39;dog&#39; &#xA;    AND DogBreedClassifier(Crop(data, bbox)) = &#39;great dane&#39; &#xA;    AND Color(Crop(data, bbox)) = &#39;black&#39;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By reusing the results of the first query and reordering the predicates based on the available cached inference results, EVA runs the second query &lt;strong&gt;10x faster&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Illustrative Applications&lt;/h2&gt; &#xA;&lt;h3&gt;üîÆ &lt;a href=&#34;https://evadb.readthedocs.io/en/stable/source/tutorials/02-object-detection.html&#34;&gt;Traffic Analysis&lt;/a&gt; (Object Detection Model)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Source Video&lt;/th&gt; &#xA;   &lt;th&gt;Query Result&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Source Video&#34; src=&#34;https://github.com/georgia-tech-db/eva/releases/download/v0.1.0/traffic-input.webp&#34; width=&#34;300&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Query Result&#34; src=&#34;https://github.com/georgia-tech-db/eva/releases/download/v0.1.0/traffic-output.webp&#34; width=&#34;300&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;üîÆ &lt;a href=&#34;https://evadb.readthedocs.io/en/stable/source/tutorials/01-mnist.html&#34;&gt;MNIST Digit Recognition&lt;/a&gt; (Image Classification Model)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Source Video&lt;/th&gt; &#xA;   &lt;th&gt;Query Result&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Source Video&#34; src=&#34;https://github.com/georgia-tech-db/eva/releases/download/v0.1.0/mnist-input.webp&#34; width=&#34;150&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Query Result&#34; src=&#34;https://github.com/georgia-tech-db/eva/releases/download/v0.1.0/mnist-output.webp&#34; width=&#34;150&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;üîÆ &lt;a href=&#34;https://evadb.readthedocs.io/en/stable/source/tutorials/03-emotion-analysis.html&#34;&gt;Movie Emotion Analysis&lt;/a&gt; (Face Detection + Emotion Classfication Models)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Source Video&lt;/th&gt; &#xA;   &lt;th&gt;Query Result&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Source Video&#34; src=&#34;https://github.com/georgia-tech-db/eva/releases/download/v0.1.0/gangubai-input.webp&#34; width=&#34;400&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Query Result&#34; src=&#34;https://github.com/georgia-tech-db/eva/releases/download/v0.1.0/gangubai-output.webp&#34; width=&#34;400&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;üîÆ &lt;a href=&#34;https://github.com/georgia-tech-db/eva-application-template&#34;&gt;License Plate Recognition&lt;/a&gt; (Plate Detection + OCR Extraction Models)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Query Result&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Query Result&#34; src=&#34;https://github.com/georgia-tech-db/license-plate-recognition/raw/main/README_files/README_12_3.png&#34; width=&#34;300&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;üîÆ &lt;a href=&#34;https://github.com/georgia-tech-db/toxicity-classification&#34;&gt;Meme Toxicity Classification&lt;/a&gt; (OCR Extraction + Toxicity Classification Models)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Query Result&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img alt=&#34;Query Result&#34; src=&#34;https://raw.githubusercontent.com/georgia-tech-db/toxicity-classification/main/README_files/README_16_2.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Community and Support&lt;/h2&gt; &#xA;&lt;p&gt;üëã If you have general questions about EVA, want to say hello or just follow along, we&#39;d like to invite you to join our &lt;a href=&#34;https://join.slack.com/t/eva-db/shared_invite/zt-1i10zyddy-PlJ4iawLdurDv~aIAq90Dg&#34;&gt;Slack Community&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;a href=&#34;https://join.slack.com/t/eva-db/shared_invite/zt-1i10zyddy-PlJ4iawLdurDv~aIAq90Dg&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/georgia-tech-db/eva/master/docs/images/eva/eva-slack.png&#34; alt=&#34;EVA Slack Channel&#34; width=&#34;500&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;If you run into any problems or issues, please create a Github issue and we&#39;ll try our best to help.&lt;/p&gt; &#xA;&lt;p&gt;Don&#39;t see a feature in the list? Search our issue tracker if someone has already requested it and add a comment to it explaining your use-case, or open a new issue if not. We prioritize our roadmap based on user feedback, so we&#39;d love to hear from you.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/evadb&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/evadb.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://circleci.com/gh/georgia-tech-db/eva&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/georgia-tech-db/eva.svg?style=svg&#34; alt=&#34;CI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://evadb.readthedocs.io/en/stable/index.html&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/evadb/badge/?version=stable&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;EVA is the beneficiary of many &lt;a href=&#34;https://github.com/georgia-tech-db/eva/graphs/contributors&#34;&gt;contributors&lt;/a&gt;. All kinds of contributions to EVA are appreciated. To file a bug or to request a feature, please use &lt;a href=&#34;https://github.com/georgia-tech-db/eva/issues&#34;&gt;GitHub issues&lt;/a&gt;. &lt;a href=&#34;https://github.com/georgia-tech-db/eva/pulls&#34;&gt;Pull requests&lt;/a&gt; are welcome.&lt;/p&gt; &#xA;&lt;p&gt;For more information, see our &lt;a href=&#34;https://evadb.readthedocs.io/en/stable/source/contribute/index.html&#34;&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright (c) 2018-present &lt;a href=&#34;http://db.cc.gatech.edu/&#34;&gt;Georgia Tech Database Group&lt;/a&gt;. Licensed under &lt;a href=&#34;https://raw.githubusercontent.com/georgia-tech-db/eva/master/LICENSE&#34;&gt;Apache License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>