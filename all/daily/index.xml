<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-16T01:23:44Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tonyke-bot/ore-miner</title>
    <updated>2024-04-16T01:23:44Z</updated>
    <id>tag:github.com,2024-04-16:/tonyke-bot/ore-miner</id>
    <link href="https://github.com/tonyke-bot/ore-miner" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ORE Miner built on top of Jito bundle with both CPU and GPU support.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;$ORE Miner&lt;/h1&gt; &#xA;&lt;p&gt;ORE Miner built on top of Jito bundle service by &lt;a href=&#34;https://x.com/tonyke_bot&#34;&gt;@tonyke_bot&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/shoucccc&#34;&gt;@shoucccc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Shipped with both CPU and GPU hashing support.&lt;/p&gt; &#xA;&lt;p&gt;Each miner is able to carry 400 wallets on a single RTX 4090 card. Should expect 10~20% improvement if the code is optimized.&lt;/p&gt; &#xA;&lt;h2&gt;Preparations&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Get a reliable, fastest Solana RPC&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repo and build&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/tonyke-bot/ore-miner.git&#xA;cd ore-miner&#xA;cargo build --release&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Optional) Install CUDA development environment&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Optional) Build CUDA miner&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./build-cuda-worker.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate wallets and fund them with SOL&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Feature&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Evenly consumed SOL: Choose richest wallet to tip bundle and richest wallet in a transaction to pay the transaction fee.&lt;/li&gt; &#xA; &lt;li&gt;Adaptive tip: Automatically adjust tip based on the Jito tip stream.&lt;/li&gt; &#xA; &lt;li&gt;Bulk operation support: mine, register, claim, batch transfer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h4&gt;Mine with GPU&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;export CUDA_VISIBLE_DEVICES=&amp;lt;GPU_INDEX&amp;gt;&#xA;&#xA;cargo run --release -- \&#xA;    --rpc &amp;lt;RPC_URL&amp;gt; \&#xA;    --priority-fee 500000 \                     # Tip used for Jito bundle. If max adaptive tip is set, this will be the initial tip.&#xA;    bundle-mine-gpu \&#xA;    --key-folder &amp;lt;FOLDER_CONTAINS_YOUR_KEYS&amp;gt; \  # Folder contains your Solana keys&#xA;    --max-adaptive-tip 400000 \                 # Max tip used, if this is set, use tip min(tips.p50, max)****&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Multi Claim&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo run --release -- \&#xA;    --rpc &amp;lt;RPC_URL&amp;gt; \&#xA;    --priority-fee 500000 \                     # Tip used for Jito bundle. &#xA;    claim \&#xA;    --key-folder &amp;lt;FOLDER_CONTAINS_YOUR_KEYS&amp;gt; \  # Folder contains your Solana keys&#xA;    --beneficiary &amp;lt;YOUR_PUBKEY_TO_RECEIVE_ORE&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Register&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo run --release -- \&#xA;    --rpc &amp;lt;RPC_URL&amp;gt; \&#xA;    --priority-fee 500000 \                     # Tip used for Jito bundle. &#xA;    register \&#xA;    --key-folder &amp;lt;FOLDER_CONTAINS_YOUR_KEYS&amp;gt; \  # Folder contains your Solana keys&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Buy me â˜•ï¸&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SOL: &lt;code&gt;tonyi4UznxNzae5RBinHTU8Gxr91RRGBcdx7mmimN8F&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;EVM: &lt;code&gt;0x45Fce32abB76fd0722882326FBf2d1182e6b982B&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Appreciate your support!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenBMB/MiniCPM-V</title>
    <updated>2024-04-16T01:23:44Z</updated>
    <id>tag:github.com,2024-04-16:/OpenBMB/MiniCPM-V</id>
    <link href="https://github.com/OpenBMB/MiniCPM-V" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MiniCPM-V 2.0: An Efficient End-side MLLM with Strong OCR and Understanding Capabilities&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;!-- &lt;h1 style=&#34;color: #33A6B8; font-family: Helvetica&#34;&gt; OmniLMM &lt;/h1&gt; --&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv-omnilmm.png&#34; width=&#34;400em&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;æ€§èƒ½é¢†å…ˆä¸”éƒ¨ç½²é«˜æ•ˆçš„å¤šæ¨¡æ€å¤§æ¨¡å‹&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;ä¸­æ–‡ | &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/README_en.md&#34;&gt;English&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; MiniCPM-V 2.0 &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2/&#34;&gt;ğŸ¤—&lt;/a&gt; &lt;a href=&#34;http://120.92.209.146:80/&#34;&gt;ğŸ¤–&lt;/a&gt; | OmniLMM-12B &lt;a href=&#34;https://huggingface.co/openbmb/OmniLMM-12B/&#34;&gt;ğŸ¤—&lt;/a&gt; &lt;a href=&#34;http://120.92.209.146:8081&#34;&gt;ğŸ¤–&lt;/a&gt; | &lt;a href=&#34;https://openbmb.vercel.app/minicpm-v-2&#34;&gt;MiniCPM-V 2.0 æŠ€æœ¯åšå®¢&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;MiniCPM-V&lt;/strong&gt;å’Œ&lt;strong&gt;OmniLMM&lt;/strong&gt; æ˜¯é¢å‘å›¾æ–‡ç†è§£çš„å¼€æºå¤šæ¨¡æ€å¤§æ¨¡å‹ç³»åˆ—ã€‚è¯¥ç³»åˆ—æ¨¡å‹æ¥å—å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶æä¾›é«˜è´¨é‡çš„æ–‡æœ¬è¾“å‡ºã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸¤ä¸ªç‰ˆæœ¬çš„æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°&lt;strong&gt;é¢†å…ˆçš„æ€§èƒ½å’Œé«˜æ•ˆçš„éƒ¨ç½²&lt;/strong&gt;ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-V 2.8B&lt;/strong&gt;ï¼šå¯åœ¨ç»ˆç«¯è®¾å¤‡ä¸Šéƒ¨ç½²çš„å…ˆè¿›å¤šæ¨¡æ€å¤§æ¨¡å‹ã€‚æœ€æ–°å‘å¸ƒçš„ MiniCPM-V 2.0 å¯ä»¥æ¥å— 180 ä¸‡åƒç´ çš„ä»»æ„é•¿å®½æ¯”å›¾åƒè¾“å…¥ï¼Œå®ç°äº†å’Œ Gemini Pro ç›¸è¿‘çš„åœºæ™¯æ–‡å­—è¯†åˆ«èƒ½åŠ›ä»¥åŠå’Œ GPT-4V ç›¸åŒ¹çš„ä½å¹»è§‰ç‡ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;OmniLMM-12B&lt;/strong&gt;ï¼šç›¸æ¯”åŒè§„æ¨¡å…¶ä»–æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å…·æœ‰é¢†å…ˆæ€§èƒ½ï¼Œå®ç°äº†ç›¸æ¯” GPT-4V æ›´ä½çš„å¹»è§‰ç‡ã€‚&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æ›´æ–°æ—¥å¿— &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024.04.12] æˆ‘ä»¬å¼€æºäº† MiniCPM-V 2.0ï¼Œè¯¥æ¨¡å‹åˆ·æ–°äº† OCRBench å¼€æºæ¨¡å‹æœ€ä½³æˆç»©ï¼Œåœ¨åœºæ™¯æ–‡å­—è¯†åˆ«èƒ½åŠ›ä¸Šæ¯”è‚© Gemini Proï¼ŒåŒæ—¶è¿˜åœ¨ç»¼åˆäº† 11 ä¸ªä¸»æµå¤šæ¨¡æ€å¤§æ¨¡å‹è¯„æµ‹åŸºå‡†çš„ &lt;a href=&#34;https://rank.opencompass.org.cn/leaderboard-multimodal&#34;&gt;OpenCompass&lt;/a&gt; æ¦œå•ä¸Šè¶…è¿‡äº† Qwen-VL-Chat 10Bã€CogVLM-Chat 17B å’Œ Yi-VL 34B ç­‰æ›´å¤§å‚æ•°è§„æ¨¡çš„æ¨¡å‹ï¼ç‚¹å‡»&lt;a href=&#34;https://openbmb.vercel.app/minicpm-v-2&#34;&gt;è¿™é‡Œ&lt;/a&gt;æŸ¥çœ‹ MiniCPM-V 2.0 æŠ€æœ¯åšå®¢ã€‚&lt;/li&gt; &#xA; &lt;li&gt;[2024.03.14] MiniCPM-V ç°åœ¨æ”¯æŒ SWIFT æ¡†æ¶ä¸‹çš„&lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md&#34;&gt;å¾®è°ƒ&lt;/a&gt;äº†ï¼Œæ„Ÿè°¢ &lt;a href=&#34;https://github.com/Jintao-Huang&#34;&gt;Jintao&lt;/a&gt; çš„è´¡çŒ®ï¼&lt;/li&gt; &#xA; &lt;li&gt;[2024.03.01] MiniCPM-V ç°åœ¨æ”¯æŒåœ¨ Mac ç”µè„‘ä¸Šè¿›è¡Œéƒ¨ç½²ï¼&lt;/li&gt; &#xA; &lt;li&gt;[2024.02.01] æˆ‘ä»¬å¼€æºäº† MiniCPM-V å’Œ OmniLMM-12Bï¼Œåˆ†åˆ«å¯ä»¥æ”¯æŒé«˜æ•ˆçš„ç«¯ä¾§éƒ¨ç½²å’ŒåŒè§„æ¨¡é¢†å…ˆçš„å¤šæ¨¡æ€èƒ½åŠ›ï¼&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç›®å½• &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;!-- TOC --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-v-28b&#34;&gt;MiniCPM-V 2.8B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#omnilmm-12b&#34;&gt;OmniLMM-12B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E5%AE%89%E8%A3%85&#34;&gt;å®‰è£…&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E6%8E%A8%E7%90%86&#34;&gt;æ¨ç†&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E6%A8%A1%E5%9E%8B%E5%BA%93&#34;&gt;æ¨¡å‹åº“&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D&#34;&gt;å¤šè½®å¯¹è¯&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#mac-%E6%8E%A8%E7%90%86&#34;&gt;Mac æ¨ç†&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E6%89%8B%E6%9C%BA%E7%AB%AF%E9%83%A8%E7%BD%B2&#34;&gt;æ‰‹æœºç«¯éƒ¨ç½²&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E6%9C%AA%E6%9D%A5%E8%AE%A1%E5%88%92&#34;&gt;æœªæ¥è®¡åˆ’&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- /TOC --&gt; &#xA;&lt;!-- /TOC --&gt; &#xA;&lt;h2&gt;MiniCPM-V 2.8B&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;MiniCPM-V 2.8B&lt;/strong&gt;å¯ä»¥é«˜æ•ˆéƒ¨ç½²åˆ°ç»ˆç«¯è®¾å¤‡ã€‚è¯¥æ¨¡å‹åŸºäº SigLip-400M å’Œ &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM/&#34;&gt;MiniCPM-2.4B&lt;/a&gt;æ„å»ºï¼Œé€šè¿‡perceiver resamplerè¿æ¥ã€‚æœ€æ–°å‘å¸ƒçš„ MiniCPM-V 2.0 çš„ç‰¹ç‚¹åŒ…æ‹¬ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ”¥ &lt;strong&gt;ä¼˜ç§€çš„æ€§èƒ½ã€‚&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 åœ¨å¤šä¸ªæµ‹è¯•åŸºå‡†ï¼ˆå¦‚ OCRBench, TextVQA, MME, MMB, MathVista ç­‰ï¼‰ä¸­å®ç°äº† 7B ä»¥ä¸‹æ¨¡å‹çš„&lt;strong&gt;æœ€ä½³æ€§èƒ½&lt;/strong&gt;ã€‚&lt;strong&gt;åœ¨ç»¼åˆäº† 11 ä¸ªä¸»æµå¤šæ¨¡æ€å¤§æ¨¡å‹è¯„æµ‹åŸºå‡†çš„ OpenCompass æ¦œå•ä¸Šè¶…è¿‡äº† Qwen-VL-Chat 9.6Bã€CogVLM-Chat 17.4B å’Œ Yi-VL 34B ç­‰æ›´å¤§å‚æ•°è§„æ¨¡çš„æ¨¡å‹&lt;/strong&gt;ã€‚MiniCPM-V 2.0 è¿˜å±•ç°å‡º&lt;strong&gt;é¢†å…ˆçš„ OCR èƒ½åŠ›&lt;/strong&gt;ï¼Œåœ¨åœºæ™¯æ–‡å­—è¯†åˆ«èƒ½åŠ›ä¸Š&lt;strong&gt;æ¥è¿‘ Gemini Pro&lt;/strong&gt;ï¼ŒOCRBench å¾—åˆ†è¾¾åˆ°&lt;strong&gt;å¼€æºæ¨¡å‹ç¬¬ä¸€&lt;/strong&gt;ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ† &lt;strong&gt;å¯ä¿¡è¡Œä¸ºã€‚&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;å¤šæ¨¡æ€å¤§æ¨¡å‹æ·±å—å¹»è§‰é—®é¢˜å›°æ‰°ï¼Œæ¨¡å‹ç»å¸¸ç”Ÿæˆå’Œå›¾åƒä¸­çš„äº‹å®ä¸ç¬¦çš„æ–‡æœ¬ã€‚MiniCPM-V 2.0 æ˜¯ &lt;strong&gt;ç¬¬ä¸€ä¸ªé€šè¿‡å¤šæ¨¡æ€ RLHF å¯¹é½çš„ç«¯ä¾§å¤šæ¨¡æ€å¤§æ¨¡å‹&lt;/strong&gt;ï¼ˆå€ŸåŠ© &lt;a href=&#34;https://rlhf-v.github.io/&#34;&gt;RLHF-V&lt;/a&gt; [CVPR&#39;24] ç³»åˆ—æŠ€æœ¯ï¼‰ã€‚è¯¥æ¨¡å‹åœ¨ &lt;a href=&#34;https://arxiv.org/abs/2312.00849&#34;&gt;Object HalBench&lt;/a&gt; è¾¾åˆ°&lt;strong&gt;å’Œ GPT-4V ç›¸ä»¿&lt;/strong&gt;çš„æ€§èƒ½ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸŒŸ &lt;strong&gt;é«˜æ¸…å›¾åƒé«˜æ•ˆç¼–ç ã€‚&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 å¯ä»¥æ¥å— &lt;strong&gt;180 ä¸‡åƒç´ çš„ä»»æ„é•¿å®½æ¯”å›¾åƒè¾“å…¥&lt;/strong&gt;ï¼ˆåŸºäºæœ€æ–°çš„&lt;a href=&#34;https://arxiv.org/pdf/2403.11703.pdf&#34;&gt;LLaVA-UHD&lt;/a&gt; æŠ€æœ¯ï¼‰ï¼Œè¿™ä½¿å¾—æ¨¡å‹å¯ä»¥æ„ŸçŸ¥åˆ°å°ç‰©ä½“ã€å¯†é›†æ–‡å­—ç­‰æ›´åŠ ç»†ç²’åº¦çš„è§†è§‰ä¿¡æ¯ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;âš¡ï¸ &lt;strong&gt;é«˜æ•ˆéƒ¨ç½²ã€‚&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 å¯ä»¥&lt;strong&gt;é«˜æ•ˆéƒ¨ç½²åœ¨å¤§å¤šæ•°æ¶ˆè´¹çº§æ˜¾å¡å’Œä¸ªäººç”µè„‘ä¸Š&lt;/strong&gt;ï¼ŒåŒ…æ‹¬&lt;strong&gt;ç§»åŠ¨æ‰‹æœºç­‰ç»ˆç«¯è®¾å¤‡&lt;/strong&gt;ã€‚åœ¨è§†è§‰ç¼–ç æ–¹é¢ï¼Œæˆ‘ä»¬é€šè¿‡perceiver resamplerå°†å›¾åƒè¡¨ç¤ºå‹ç¼©ä¸ºæ›´å°‘çš„ tokenã€‚è¿™ä½¿å¾— MiniCPM-V 2.0 å³ä¾¿æ˜¯&lt;strong&gt;é¢å¯¹é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œä¹Ÿèƒ½å ç”¨è¾ƒä½çš„å­˜å‚¨å¹¶å±•ç°ä¼˜ç§€çš„æ¨ç†é€Ÿåº¦&lt;/strong&gt;ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ™Œ &lt;strong&gt;åŒè¯­æ”¯æŒã€‚&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 &lt;strong&gt;æä¾›é¢†å…ˆçš„ä¸­è‹±åŒè¯­å¤šæ¨¡æ€èƒ½åŠ›æ”¯æŒ&lt;/strong&gt;ã€‚ è¯¥èƒ½åŠ›é€šè¿‡ &lt;a href=&#34;https://arxiv.org/abs/2308.12038&#34;&gt;VisCPM&lt;/a&gt; [ICLR&#39;24] è®ºæ–‡ä¸­æå‡ºçš„å¤šæ¨¡æ€èƒ½åŠ›çš„è·¨è¯­è¨€æ³›åŒ–æŠ€æœ¯å®ç°ã€‚&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;æ€§èƒ½è¯„ä¼° &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv-2-peformance.png&#34; width=&#34;66%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;TextVQA, DocVQA, OCRBench, OpenCompass, MME, MMBench, MMMU, MathVista, LLaVA Bench, Object HalBench ä¸Šçš„è¯¦ç»†è¯„æµ‹ç»“æœã€‚ &lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;table style=&#34;margin: 0px auto;&#34;&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;     &lt;th&gt;Size&lt;/th&gt; &#xA;     &lt;th&gt;TextVQA val&lt;/th&gt; &#xA;     &lt;th&gt;DocVQA test&lt;/th&gt; &#xA;     &lt;th&gt;OCRBench&lt;/th&gt; &#xA;     &lt;th&gt;OpenCompass&lt;/th&gt; &#xA;     &lt;th nowrap&gt;MME&lt;/th&gt; &#xA;     &lt;th&gt;MMB dev(en)&lt;/th&gt; &#xA;     &lt;th&gt;MMB dev(zh)&lt;/th&gt; &#xA;     &lt;th&gt;MMMU val&lt;/th&gt; &#xA;     &lt;th&gt;MathVista&lt;/th&gt; &#xA;     &lt;th&gt;LLaVA Bench&lt;/th&gt; &#xA;     &lt;th nowrap&gt;Object HalBench&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody align=&#34;center&#34;&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;12&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Proprietary models&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Gemini Pro Vision&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;74.6&lt;/td&gt; &#xA;     &lt;td&gt;88.1&lt;/td&gt; &#xA;     &lt;td&gt;680&lt;/td&gt; &#xA;     &lt;td&gt;63.8&lt;/td&gt; &#xA;     &lt;td&gt;2148.9&lt;/td&gt; &#xA;     &lt;td&gt;75.2&lt;/td&gt; &#xA;     &lt;td&gt;74.0&lt;/td&gt; &#xA;     &lt;td&gt;48.9&lt;/td&gt; &#xA;     &lt;td&gt;45.8&lt;/td&gt; &#xA;     &lt;td&gt;79.9&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;GPT-4V&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;78.0&lt;/td&gt; &#xA;     &lt;td&gt;88.4&lt;/td&gt; &#xA;     &lt;td&gt;645&lt;/td&gt; &#xA;     &lt;td&gt;63.2&lt;/td&gt; &#xA;     &lt;td&gt;1771.5&lt;/td&gt; &#xA;     &lt;td&gt;75.1&lt;/td&gt; &#xA;     &lt;td&gt;75.0&lt;/td&gt; &#xA;     &lt;td&gt;53.8&lt;/td&gt; &#xA;     &lt;td&gt;47.8&lt;/td&gt; &#xA;     &lt;td&gt;93.1&lt;/td&gt; &#xA;     &lt;td&gt;86.4 / 92.7&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;12&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open-source models 6B~34B&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Yi-VL-6B&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;6.7B&lt;/td&gt; &#xA;     &lt;td&gt;45.5*&lt;/td&gt; &#xA;     &lt;td&gt;17.1*&lt;/td&gt; &#xA;     &lt;td&gt;290&lt;/td&gt; &#xA;     &lt;td&gt;49.3&lt;/td&gt; &#xA;     &lt;td&gt;1915.1 &lt;/td&gt; &#xA;     &lt;td&gt;68.6 &lt;/td&gt; &#xA;     &lt;td&gt;68.3 &lt;/td&gt; &#xA;     &lt;td&gt;40.3 &lt;/td&gt; &#xA;     &lt;td&gt;28.8 &lt;/td&gt; &#xA;     &lt;td&gt;51.9 &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Qwen-VL-Chat&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;9.6B&lt;/td&gt; &#xA;     &lt;td&gt;61.5&lt;/td&gt; &#xA;     &lt;td&gt;62.6&lt;/td&gt; &#xA;     &lt;td&gt;488 &lt;/td&gt; &#xA;     &lt;td&gt;52.1 &lt;/td&gt; &#xA;     &lt;td&gt;1860.0 &lt;/td&gt; &#xA;     &lt;td&gt;60.6 &lt;/td&gt; &#xA;     &lt;td&gt;56.7 &lt;/td&gt; &#xA;     &lt;td&gt;37.0 &lt;/td&gt; &#xA;     &lt;td&gt;33.8 &lt;/td&gt; &#xA;     &lt;td&gt;67.7 &lt;/td&gt; &#xA;     &lt;td&gt;56.2 / 80.0&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Yi-VL-34B&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;34B&lt;/td&gt; &#xA;     &lt;td&gt;43.4*&lt;/td&gt; &#xA;     &lt;td&gt;16.9*&lt;/td&gt; &#xA;     &lt;td&gt;290&lt;/td&gt; &#xA;     &lt;td&gt;52.6 &lt;/td&gt; &#xA;     &lt;td&gt;2050.2&lt;/td&gt; &#xA;     &lt;td&gt;71.1&lt;/td&gt; &#xA;     &lt;td&gt;71.4&lt;/td&gt; &#xA;     &lt;td&gt;45.1&lt;/td&gt; &#xA;     &lt;td&gt;30.7&lt;/td&gt; &#xA;     &lt;td&gt;62.3&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;DeepSeek-VL-7B&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;7.3B&lt;/td&gt; &#xA;     &lt;td&gt;64.7*&lt;/td&gt; &#xA;     &lt;td&gt;47.0* &lt;/td&gt; &#xA;     &lt;td&gt;435&lt;/td&gt; &#xA;     &lt;td&gt;55.6 &lt;/td&gt; &#xA;     &lt;td&gt;1765.4 &lt;/td&gt; &#xA;     &lt;td&gt;74.1 &lt;/td&gt; &#xA;     &lt;td&gt;72.8 &lt;/td&gt; &#xA;     &lt;td&gt;38.3 &lt;/td&gt; &#xA;     &lt;td&gt;36.8&lt;/td&gt; &#xA;     &lt;td&gt;77.8 &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;TextMonkey&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;9.7B&lt;/td&gt; &#xA;     &lt;td&gt;64.3&lt;/td&gt; &#xA;     &lt;td&gt;66.7 &lt;/td&gt; &#xA;     &lt;td&gt;558&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;CogVLM-Chat&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;17.4B&lt;/td&gt; &#xA;     &lt;td&gt;70.4&lt;/td&gt; &#xA;     &lt;td&gt;33.3*&lt;/td&gt; &#xA;     &lt;td&gt;590 &lt;/td&gt; &#xA;     &lt;td&gt;52.5 &lt;/td&gt; &#xA;     &lt;td&gt;1736.6 &lt;/td&gt; &#xA;     &lt;td&gt;63.7 &lt;/td&gt; &#xA;     &lt;td&gt;53.8 &lt;/td&gt; &#xA;     &lt;td&gt;37.3 &lt;/td&gt; &#xA;     &lt;td&gt;34.7 &lt;/td&gt; &#xA;     &lt;td&gt;73.9 &lt;/td&gt; &#xA;     &lt;td&gt;73.6 / 87.4 &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;12&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open-source models 1B~3B &lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;DeepSeek-VL-1.3B&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;1.7B&lt;/td&gt; &#xA;     &lt;td&gt;58.4*&lt;/td&gt; &#xA;     &lt;td&gt;37.9*&lt;/td&gt; &#xA;     &lt;td&gt;413&lt;/td&gt; &#xA;     &lt;td&gt;46.0 &lt;/td&gt; &#xA;     &lt;td&gt;1531.6 &lt;/td&gt; &#xA;     &lt;td&gt;64.0 &lt;/td&gt; &#xA;     &lt;td&gt;61.2 &lt;/td&gt; &#xA;     &lt;td&gt;33.8 &lt;/td&gt; &#xA;     &lt;td&gt;29.4 &lt;/td&gt; &#xA;     &lt;td&gt;51.1 &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MobileVLM V2&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;3.1B&lt;/td&gt; &#xA;     &lt;td&gt;57.5&lt;/td&gt; &#xA;     &lt;td&gt;19.4*&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;1440.5(P) &lt;/td&gt; &#xA;     &lt;td&gt;63.2 &lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Mini-Gemini&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;2.2B&lt;/td&gt; &#xA;     &lt;td&gt;56.2&lt;/td&gt; &#xA;     &lt;td&gt;34.2*&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;1653.0 &lt;/td&gt; &#xA;     &lt;td&gt;59.8 &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;31.7 &lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-V&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;2.8B &lt;/td&gt; &#xA;     &lt;td&gt;60.6&lt;/td&gt; &#xA;     &lt;td&gt;38.2 &lt;/td&gt; &#xA;     &lt;td&gt;366&lt;/td&gt; &#xA;     &lt;td&gt;47.6&lt;/td&gt; &#xA;     &lt;td&gt;1650.2 &lt;/td&gt; &#xA;     &lt;td&gt;67.9 &lt;/td&gt; &#xA;     &lt;td&gt;65.3 &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;38.3&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;28.9&lt;/td&gt; &#xA;     &lt;td&gt;51.3 &lt;/td&gt; &#xA;     &lt;td&gt;78.4 / 88.5 &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;&lt;strong&gt;MiniCPM-V 2.0&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;2.8B &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;74.1&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;71.9&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;605&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;55.0&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;1808.6&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;69.6&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;68.1&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;38.2 &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;38.7&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;69.2&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;85.5 / 92.2 &lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt; * æˆ‘ä»¬è‡ªå·±è¯„æµ‹äº†æ­£å¼å¼€æºçš„æ¨¡å‹æƒé‡ã€‚ &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;å…¸å‹ç¤ºä¾‹ &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv2-cases_2.png&#34; width=&#34;95%/&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt;  &#xA;&lt;/table&gt; &#xA;&lt;p&gt;æˆ‘ä»¬å°† MiniCPM-V 2.0 éƒ¨ç½²åœ¨å°ç±³ 14 Pro ä¸Šï¼Œå¹¶å½•åˆ¶äº†ä»¥ä¸‹æ¼”ç¤ºè§†é¢‘ï¼Œæœªç»ä»»ä½•è§†é¢‘å‰ªè¾‘ã€‚&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/gif_cases/station.gif&#34; width=&#34;36%/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/gif_cases/london_car.gif&#34; width=&#34;36%/&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt;  &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;MiniCPM-V 1.0 &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p&gt;è¯·å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/minicpm_v1.md&#34;&gt;è¿™é‡Œ&lt;/a&gt;äº†è§£ MiniCPM-V 1.0 çš„ä¿¡æ¯å’Œä½¿ç”¨æ•™ç¨‹ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;OmniLMM-12B&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;OmniLMM-12B&lt;/strong&gt; æ˜¯å½“å‰ç³»åˆ—ä¸­æ€§èƒ½æœ€ä½³çš„ç‰ˆæœ¬ã€‚è¯¥æ¨¡å‹åŸºäºEVA02-5Bå’ŒZephyr-7B-Î²åˆå§‹åŒ–æ„å»ºï¼Œå¹¶ä½¿ç”¨perceiver resamplerè¿æ¥ï¼Œé‡‡ç”¨äº†è¯¾ç¨‹å­¦ä¹ çš„æ–¹æ³•åœ¨å¤šæ¨¡æ€æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¯¥æ¨¡å‹å…·æœ‰ä¸‰ä¸ªç‰¹ç‚¹ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ”¥ &lt;strong&gt;æ€§èƒ½é¢†å…ˆã€‚&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;OmniLMM-12B ç›¸æ¯”å…¶ä»–åŒè§„æ¨¡æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—&lt;strong&gt;é¢†å…ˆçš„æ€§èƒ½&lt;/strong&gt;ï¼ˆåŒ…æ‹¬ MMEã€MMBenchã€SEED-Bench ç­‰ï¼‰ï¼Œæ¨¡å‹æŒæ¡äº†è¾ƒä¸ºä¸°å¯Œçš„å¤šæ¨¡æ€ä¸–ç•ŒçŸ¥è¯†ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ† &lt;strong&gt;è¡Œä¸ºå¯ä¿¡ã€‚&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;å¤šæ¨¡æ€å¤§æ¨¡å‹çš„å¹»è§‰é—®é¢˜å¤‡å—å…³æ³¨ï¼Œæ¨¡å‹ç»å¸¸ç”Ÿæˆå’Œå›¾åƒä¸­çš„äº‹å®ä¸ç¬¦çš„æ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œç¡®ä¿¡åœ°æè¿°å›¾ç‰‡ä¸­å¹¶ä¸å­˜åœ¨çš„ç‰©ä½“ï¼‰ã€‚OmniLMM-12Bæ˜¯ &lt;strong&gt;ç¬¬ä¸€ä¸ªé€šè¿‡å¤šæ¨¡æ€ RLHF å¯¹é½çš„ç»¼åˆèƒ½åŠ›ä¼˜ç§€çš„å¼€æºå¤šæ¨¡æ€å¤§æ¨¡å‹&lt;/strong&gt;ï¼ˆå€ŸåŠ© &lt;a href=&#34;https://rlhf-v.github.io/&#34;&gt;RLHF-V&lt;/a&gt; [CVPR&#39;24] ç³»åˆ—æŠ€æœ¯ï¼‰ã€‚è¯¥æ¨¡å‹åœ¨ &lt;a href=&#34;https://huggingface.co/datasets/Shengcao1006/MMHal-Bench&#34;&gt;MMHal-Bench&lt;/a&gt; å¹»è§‰è¯„æµ‹åŸºå‡†ä¸Šè¾¾åˆ°&lt;strong&gt;å¼€æºæ¨¡å‹æœ€ä½³æ°´å¹³&lt;/strong&gt;ï¼Œå¹¶åœ¨ &lt;a href=&#34;https://arxiv.org/abs/2312.00849&#34;&gt;Object HalBench&lt;/a&gt; ä¸­&lt;strong&gt;ä¼˜äºGPT-4V&lt;/strong&gt;ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ•¹ &lt;strong&gt;å®æ—¶å¤šæ¨¡æ€äº¤äº’ã€‚&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;æˆ‘ä»¬å°è¯•ç»“åˆOmniLMM-12Bå’ŒGPT-3.5 (çº¯æ–‡æœ¬æ¨¡å‹) ï¼Œå®ç°&lt;strong&gt;å®æ—¶å¤šæ¨¡æ€äº¤äº’åŠ©æ‰‹&lt;/strong&gt;ã€‚è¯¥æ¨¡å‹æ¥å—æ¥è‡ªæ‘„åƒå¤´çš„è§†é¢‘æµï¼Œå¹¶å€ŸåŠ©å·¥å…·å¤„ç†è¯­éŸ³è¾“å…¥è¾“å‡ºã€‚è™½ç„¶è¿˜å¾ˆåˆæ­¥ï¼Œæˆ‘ä»¬å‘ç°è¯¥æ¨¡å‹æ— éœ€è§†é¢‘ç¼–è¾‘å¯ä»¥&lt;strong&gt;å¤ç°Geminiæ¼”ç¤ºè§†é¢‘ä¸­çš„ä¸€äº›æœ‰è¶£ä¾‹å­&lt;/strong&gt;ã€‚&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;è¯„æµ‹ç»“æœ &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/radar_omnilmm12b.png&#34; width=&#34;66%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; MME, MMBench, MMMU, MMBench, MMHal-Bench, Object HalBench, SeedBench, LLaVA Bench W, MathVista ä¸Šçš„è¯¦ç»†è¯„æµ‹ç»“æœã€‚ &lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Size&lt;/th&gt; &#xA;    &lt;th&gt;MME&lt;/th&gt; &#xA;    &lt;th nowrap&gt;MMB dev (en)&lt;/th&gt; &#xA;    &lt;th nowrap&gt;MMMU val&lt;/th&gt; &#xA;    &lt;th nowrap&gt;MMHal-Bench&lt;/th&gt; &#xA;    &lt;th nowrap&gt;Object HalBench&lt;/th&gt; &#xA;    &lt;th nowrap&gt;SeedBench-I&lt;/th&gt; &#xA;    &lt;th&gt;MathVista&lt;/th&gt; &#xA;    &lt;th nowrap&gt;LLaVA Bench&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody align=&#34;center&#34;&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;GPT-4Vâ€ &lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;1771.5&lt;/td&gt; &#xA;    &lt;td&gt;75.1 &lt;/td&gt; &#xA;    &lt;td&gt;56.8&lt;/td&gt; &#xA;    &lt;td&gt;3.53 / 70.8&lt;/td&gt; &#xA;    &lt;td&gt;86.4 / 92.7&lt;/td&gt; &#xA;    &lt;td&gt;71.6 &lt;/td&gt; &#xA;    &lt;td&gt;47.8 &lt;/td&gt; &#xA;    &lt;td&gt;93.1 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;Qwen-VL-Plusâ€ &lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;2183.4&lt;/td&gt; &#xA;    &lt;td&gt;66.2 &lt;/td&gt; &#xA;    &lt;td&gt;45.2&lt;/td&gt; &#xA;    &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;td&gt;65.7 &lt;/td&gt; &#xA;    &lt;td&gt;36.0 &lt;/td&gt; &#xA;    &lt;td&gt;73.7 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Yi-VL 6B&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;6.7B &lt;/td&gt; &#xA;    &lt;td&gt;1915.1 &lt;/td&gt; &#xA;    &lt;td&gt;68.6 &lt;/td&gt; &#xA;    &lt;td&gt;40.3 &lt;/td&gt; &#xA;    &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;td&gt;67.5 &lt;/td&gt; &#xA;    &lt;td&gt;28.8 &lt;/td&gt; &#xA;    &lt;td&gt;51.9 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;Qwen-VL-Chat&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;9.6B&lt;/td&gt; &#xA;    &lt;td&gt;1860.0&lt;/td&gt; &#xA;    &lt;td&gt;60.6 &lt;/td&gt; &#xA;    &lt;td&gt;35.9&lt;/td&gt; &#xA;    &lt;td&gt;2.93 / 59.4&lt;/td&gt; &#xA;    &lt;td&gt;56.2 / 80.0&lt;/td&gt; &#xA;    &lt;td&gt;64.8 &lt;/td&gt; &#xA;    &lt;td&gt;33.8 &lt;/td&gt; &#xA;    &lt;td&gt;67.7 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;CogVLM-Chat&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;17.4B&lt;/td&gt; &#xA;    &lt;td&gt;1736.6&lt;/td&gt; &#xA;    &lt;td&gt;63.7 &lt;/td&gt; &#xA;    &lt;td&gt;32.1 &lt;/td&gt; &#xA;    &lt;td&gt;2.68 / 52.1 &lt;/td&gt; &#xA;    &lt;td&gt;73.6 / 87.4 &lt;/td&gt; &#xA;    &lt;td&gt;68.8 &lt;/td&gt; &#xA;    &lt;td&gt;34.7 &lt;/td&gt; &#xA;    &lt;td&gt;73.9 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;LLaVA 1.5&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;13.6B &lt;/td&gt; &#xA;    &lt;td&gt;1808.4 &lt;/td&gt; &#xA;    &lt;td&gt;68.2 &lt;/td&gt; &#xA;    &lt;td&gt;36.4 &lt;/td&gt; &#xA;    &lt;td&gt;2.71 / 51.0 &lt;/td&gt; &#xA;    &lt;td&gt;53.7 / 77.4 &lt;/td&gt; &#xA;    &lt;td&gt;68.1 &lt;/td&gt; &#xA;    &lt;td&gt;26.4 &lt;/td&gt; &#xA;    &lt;td&gt;64.6 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;&lt;b&gt;OmniLMM-12B&lt;/b&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;11.6B &lt;/td&gt; &#xA;    &lt;td&gt;1935.8 &lt;/td&gt; &#xA;    &lt;td&gt;71.6 &lt;/td&gt; &#xA;    &lt;td&gt;40.7 &lt;/td&gt; &#xA;    &lt;td&gt;3.45 / 68.8 &lt;/td&gt; &#xA;    &lt;td&gt;90.3 / 95.5 &lt;/td&gt; &#xA;    &lt;td&gt;71.1 &lt;/td&gt; &#xA;    &lt;td&gt;34.9 &lt;/td&gt; &#xA;    &lt;td&gt;72.0 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;small&gt;â€ : é—­æºæ¨¡å‹&lt;/small&gt; &#xA; &lt;br&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;å…¸å‹ç¤ºä¾‹ &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/omnilmm-12b-examples_2.png&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt;  &#xA;&lt;/table&gt; &#xA;&lt;p&gt;æˆ‘ä»¬ç»“åˆ OmniLMM-12B å’Œ ChatGPT-3.5 (çº¯æ–‡æœ¬æ¨¡å‹) å°è¯•æ„å»º &lt;strong&gt;å®æ—¶å¤šæ¨¡æ€äº¤äº’åŠ©æ‰‹&lt;/strong&gt;. OmniLMM-12B å°†è§†é¢‘å¸§è½¬ä¸ºå¯¹åº”çš„å›¾åƒæè¿°å¹¶è¾“å…¥ç»™ChatGPT-3.5æ¥ç”Ÿæˆå¯¹ç”¨æˆ·æŒ‡ä»¤çš„å“åº”ã€‚æ¼”ç¤ºè§†é¢‘æœªç»ç¼–è¾‘ã€‚&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;video controls src=&#34;https://github.com/OpenBMB/OmniLMM/assets/157115220/8fec13bf-bb47-4bf8-8f8c-d0b716a964ec&#34; type=&#34;video/mp4&#34; width=&#34;80%/&#34;&gt; &#xA; &lt;/video&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;æ¬¢è¿é€šè¿‡ä»¥ä¸‹é“¾æ¥ä½¿ç”¨æˆ‘ä»¬çš„ç½‘é¡µç«¯æ¨ç†æœåŠ¡ï¼š &lt;a href=&#34;http://120.92.209.146:8081&#34;&gt;OmniLMM-12B&lt;/a&gt; ï½œ &lt;a href=&#34;http://120.92.209.146:80&#34;&gt;MiniCPM-V 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;å®‰è£…&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;å…‹éš†æˆ‘ä»¬çš„ä»“åº“å¹¶è·³è½¬åˆ°ç›¸åº”ç›®å½•&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/OpenBMB/MiniCPM-V.git&#xA;cd MiniCPM-V&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;åˆ›å»º conda ç¯å¢ƒ&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n MiniCPMV python=3.10 -y&#xA;conda activate MiniCPMV&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;å®‰è£…ä¾èµ–&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;æ¨ç†&lt;/h2&gt; &#xA;&lt;h3&gt;æ¨¡å‹åº“&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;æ¨¡å‹&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;ç®€ä»‹&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ä¸‹è½½é“¾æ¥&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-V 2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;æœ€æ–°ç‰ˆæœ¬ï¼Œæä¾›é«˜æ•ˆè€Œé¢†å…ˆçš„ç«¯ä¾§åŒè¯­å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2&#34;&gt;ğŸ¤—&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-V-2/files&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-V&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç¬¬ä¸€ç‰ˆ MiniCPM-V&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V&#34;&gt;ğŸ¤—&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-V/files&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;OmniLMM-12B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;æ€§èƒ½æœ€å¼ºçš„ç‰ˆæœ¬&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/OmniLMM-12B&#34;&gt;ğŸ¤—&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/OmniLMM-12B/files&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;å¤šè½®å¯¹è¯&lt;/h3&gt; &#xA;&lt;p&gt;è¯·å‚è€ƒä»¥ä¸‹ä»£ç ä½¿ç”¨ &lt;code&gt;MiniCPM-V&lt;/code&gt; å’Œ &lt;code&gt;OmniLMM&lt;/code&gt; è¿›è¡Œæ¨ç†ã€‚&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/hk_OCR.jpg&#34; width=&#34;500px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from chat import OmniLMMChat, img2base64&#xA;&#xA;chat_model = OmniLMMChat(&#39;openbmb/MiniCPM-V-2&#39;) # or &#39;openbmb/OmniLMM-12B&#39;&#xA;&#xA;im_64 = img2base64(&#39;./assets/hk_OCR.jpg&#39;)&#xA;&#xA;# First round chat &#xA;msgs = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Where should I go to buy a camera?&#34;}]&#xA;&#xA;inputs = {&#34;image&#34;: im_64, &#34;question&#34;: json.dumps(msgs)}&#xA;answer = chat_model.chat(inputs)&#xA;print(answer)&#xA;&#xA;# Second round chat &#xA;# pass history context of multi-turn conversation&#xA;msgs.append({&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: answer})&#xA;msgs.append({&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Where is this store in the image?&#34;})&#xA;&#xA;inputs = {&#34;image&#34;: im_64, &#34;question&#34;: json.dumps(msgs)}&#xA;answer = chat_model.chat(inputs)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å¯ä»¥å¾—åˆ°ä»¥ä¸‹è¾“å‡º:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;You should go to the Canon store for a camera.&#34;&#xA;&#xA;&#34;The Canon store is located on the right side of the image.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Mac æ¨ç†&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;ç‚¹å‡»æŸ¥çœ‹ MiniCPM-V 2.0 åŸºäºMac MPSè¿è¡Œ (Apple silicon or AMD GPUs)çš„ç¤ºä¾‹ã€‚ &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# test.py&#xA;import torch&#xA;from PIL import Image&#xA;from transformers import AutoModel, AutoTokenizer&#xA;&#xA;model = AutoModel.from_pretrained(&#39;openbmb/MiniCPM-V-2&#39;, trust_remote_code=True, torch_dtype=torch.bfloat16)&#xA;model = model.to(device=&#39;mps&#39;, dtype=torch.float16)&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;openbmb/MiniCPM-V-2&#39;, trust_remote_code=True)&#xA;model.eval()&#xA;&#xA;image = Image.open(&#39;./assets/hk_OCR.jpg&#39;).convert(&#39;RGB&#39;)&#xA;question = &#39;Where is this photo taken?&#39;&#xA;msgs = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: question}]&#xA;&#xA;answer, context, _ = model.chat(&#xA;    image=image,&#xA;    msgs=msgs,&#xA;    context=None,&#xA;    tokenizer=tokenizer,&#xA;    sampling=True&#xA;)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;è¿è¡Œ:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;æ‰‹æœºç«¯éƒ¨ç½²&lt;/h3&gt; &#xA;&lt;p&gt;MiniCPM-V 2.0 ç›®å‰å¯ä»¥éƒ¨ç½²åœ¨Androidå’ŒHarmonyæ“ä½œç³»ç»Ÿçš„æ‰‹æœºä¸Šã€‚ ğŸš€ ç‚¹å‡»&lt;a href=&#34;https://github.com/OpenBMB/mlc-MiniCPM&#34;&gt;è¿™é‡Œ&lt;/a&gt;å¼€å§‹æ‰‹æœºç«¯éƒ¨ç½²ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;æœªæ¥è®¡åˆ’&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; æ”¯æŒæ¨¡å‹å¾®è°ƒ&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; æœ¬åœ°ç”¨æˆ·å›¾å½¢ç•Œé¢éƒ¨ç½²&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; å®æ—¶å¤šæ¨¡æ€äº¤äº’ä»£ç å¼€æº&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æ¨¡å‹åè®® &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬ä»“åº“ä¸­ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æº&lt;/p&gt; &#xA;&lt;p&gt;OmniLMM æ¨¡å‹æƒé‡çš„ä½¿ç”¨éµå¾ª â€œ&lt;a href=&#34;https://github.com/OpenBMB/General-Model-License/raw/main/%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE-%E6%9D%A5%E6%BA%90%E8%AF%B4%E6%98%8E-%E5%AE%A3%E4%BC%A0%E9%99%90%E5%88%B6-%E5%95%86%E4%B8%9A%E6%8E%88%E6%9D%83.md&#34;&gt;é€šç”¨æ¨¡å‹è®¸å¯åè®®-æ¥æºè¯´æ˜-å®£ä¼ é™åˆ¶-å•†ä¸šæˆæƒ&lt;/a&gt;â€ã€‚&lt;/p&gt; &#xA;&lt;p&gt;OmniLMM æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¦‚éœ€å°†æ¨¡å‹ç”¨äºå•†ä¸šç”¨é€”ï¼Œè¯·è”ç³» &lt;a href=&#34;mailto:cpm@modelbest.cn&#34;&gt;cpm@modelbest.cn&lt;/a&gt; æ¥è·å–ä¹¦é¢æˆæƒï¼Œç™»è®°åå¯ä»¥å…è´¹å•†ä¸šä½¿ç”¨ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;å£°æ˜ &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;ä½œä¸ºå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ŒMiniCPM-V å’Œ OmniLMM é€šè¿‡å­¦ä¹ å¤§é‡çš„å¤šæ¨¡æ€æ•°æ®æ¥ç”Ÿæˆå†…å®¹ï¼Œä½†å®ƒæ— æ³•ç†è§£ã€è¡¨è¾¾ä¸ªäººè§‚ç‚¹æˆ–ä»·å€¼åˆ¤æ–­ï¼Œå®ƒæ‰€è¾“å‡ºçš„ä»»ä½•å†…å®¹éƒ½ä¸ä»£è¡¨æ¨¡å‹å¼€å‘è€…çš„è§‚ç‚¹å’Œç«‹åœºã€‚&lt;/p&gt; &#xA;&lt;p&gt;å› æ­¤ç”¨æˆ·åœ¨ä½¿ç”¨ MiniCPM-V å’Œ OmniLMM ç”Ÿæˆçš„å†…å®¹æ—¶ï¼Œåº”è‡ªè¡Œè´Ÿè´£å¯¹å…¶è¿›è¡Œè¯„ä¼°å’ŒéªŒè¯ã€‚å¦‚æœç”±äºä½¿ç”¨ OmniLMM å¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;æœºæ„ &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®ç”±ä»¥ä¸‹æœºæ„å…±åŒå¼€å‘ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/thunlp.png&#34; width=&#34;28px&#34;&gt; &lt;a href=&#34;https://nlp.csai.tsinghua.edu.cn/&#34;&gt;æ¸…åå¤§å­¦è‡ªç„¶è¯­è¨€å¤„ç†å®éªŒå®¤&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelbest.png&#34; width=&#34;28px&#34;&gt; &lt;a href=&#34;https://modelbest.cn/&#34;&gt;é¢å£æ™ºèƒ½&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/zhihu.webp&#34; width=&#34;28px&#34;&gt; &lt;a href=&#34;https://www.zhihu.com/&#34;&gt;çŸ¥ä¹&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>stanford-oval/storm</title>
    <updated>2024-04-16T01:23:44Z</updated>
    <id>tag:github.com,2024-04-16:/stanford-oval/storm</id>
    <link href="https://github.com/stanford-oval/storm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the code for our NAACL 2024 paper &lt;a href=&#34;https://arxiv.org/abs/2402.14207&#34;&gt;Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models&lt;/a&gt; by &lt;a href=&#34;https://cs.stanford.edu/~shaoyj&#34;&gt;Yijia Shao&lt;/a&gt;, &lt;a href=&#34;https://yucheng-jiang.github.io/&#34;&gt;Yucheng Jiang&lt;/a&gt;, Theodore A. Kanell, Peter Xu, &lt;a href=&#34;https://omarkhattab.com/&#34;&gt;Omar Khattab&lt;/a&gt;, and &lt;a href=&#34;https://suif.stanford.edu/~lam/&#34;&gt;Monica S. Lam&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Overview &lt;a href=&#34;https://storm.genie.stanford.edu/&#34;&gt;(Try STORM now!)&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/overview.png&#34; style=&#34;width: 90%; height: auto;&#34;&gt; &lt;/p&gt; STORM is a LLM system that writes Wikipedia-like articles from scratch based on Internet search. &#xA;&lt;p&gt;While the system cannot produce publication-ready articles that often require a significant number of edits, experienced Wikipedia editors have found it helpful in their pre-writing stage.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Try out our &lt;a href=&#34;https://storm.genie.stanford.edu/&#34;&gt;live demo&lt;/a&gt; to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system ğŸ™!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Research Before Writing&lt;/h2&gt; &#xA;&lt;p&gt;STORM breaks down generating long articles with citations into two steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pre-writing stage&lt;/strong&gt;: The system conducts Internet-based research to collect references and generates an outline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Writing stage&lt;/strong&gt;: The system uses the outline and references to generate the full-length article with citations.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/two_stages.jpg&#34; style=&#34;width: 60%; height: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;STORM identifies the core of automating the research process as automatically coming up with good questions to ask. Directly prompting the language model to ask questions does not work well. To improve the depth and breadth of the questions, STORM adopts two strategies:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Perspective-Guided Question Asking&lt;/strong&gt;: Given the input topic, STORM discovers different perspectives by surveying existing articles from similar topics and uses them to control the question-asking process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simulated Conversation&lt;/strong&gt;: STORM simulates a conversation between a Wikipedia writer and a topic expert grounded in Internet sources to enable the language model to update its understanding of the topic and ask follow-up questions.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Based on the separation of the two stages, STORM is implemented in a highly modular way (see &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/engine.py&#34;&gt;engine.py&lt;/a&gt;) using &lt;a href=&#34;https://github.com/stanfordnlp/dspy&#34;&gt;dspy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;We view STORM as an example of automated knowledge curation. We are working on enhancing our codebase to increase its extensibility. Stay tuned!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Below, we provide a quick start guide to run STORM locally to reproduce our experiments.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the required packages. &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n storm python=3.11&#xA;conda activate storm&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Set up OpenAI API key and &lt;a href=&#34;https://api.you.com/&#34;&gt;You.com search API&lt;/a&gt; key. Create a file &lt;code&gt;secrets.toml&lt;/code&gt; under the root directory and add the following content: &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Set up OpenAI API key.&#xA;OPENAI_API_KEY=&amp;lt;your_openai_api_key&amp;gt;&#xA;# If you are using the API service provided by OpenAI, include the following line:&#xA;OPENAI_API_TYPE=&#34;openai&#34;&#xA;# If you are using the API service provided by Microsoft Azure, include the following lines:&#xA;OPENAI_API_TYPE=&#34;azure&#34;&#xA;AZURE_API_BASE=&amp;lt;your_azure_api_base_url&amp;gt;&#xA;AZURE_API_VERSION=&amp;lt;your_azure_api_version&amp;gt;&#xA;# Set up You.com search API key.&#xA;YDC_API_KEY=&amp;lt;your_youcom_api_key&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Paper Experiments&lt;/h2&gt; &#xA;&lt;p&gt;The FreshWiki dataset used in our experiments can be found in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/FreshWiki&#34;&gt;./FreshWiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Run the following commands under &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src&#34;&gt;./src&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-writing Stage&lt;/h3&gt; &#xA;&lt;p&gt;For batch experiment on FreshWiki dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_prewriting --input-source file --input-path ../FreshWiki/topic_list.csv  --engine gpt-4 --do-research --max-conv-turn 5 --max-perspective 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--engine&lt;/code&gt; (choices=[&lt;code&gt;gpt-4&lt;/code&gt;, &lt;code&gt;gpt-35-turbo&lt;/code&gt;]): the LLM engine used for generating the outline&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--do-research&lt;/code&gt;: if True, simulate conversation to research the topic; otherwise, load the results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max-conv-turn&lt;/code&gt;: the maximum number of questions for each information-seeking conversation&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max-perspective&lt;/code&gt;: the maximum number of perspectives to be considered, each perspective corresponds to an information-seeking conversation. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;STORM also uses a general conversation to collect basic information about the topic. So, the maximum number of QA pairs is &lt;code&gt;max_turn * (max_perspective + 1)&lt;/code&gt;. &lt;span&gt;ğŸ’¡&lt;/span&gt; Reducing &lt;code&gt;max_turn&lt;/code&gt; or &lt;code&gt;max_perspective&lt;/code&gt; can speed up the process and reduce the cost but may result in less comprehensive outline.&lt;/li&gt; &#xA;   &lt;li&gt;The parameter will not have any effect if &lt;code&gt;--disable-perspective&lt;/code&gt; is set (the perspective-driven question asking is disabled).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To run the experiment on a single topic:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_prewriting --input-source console --engine gpt-4 --max-conv-turn 5 --max-perspective 5 --do-research&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The script will ask you to enter the &lt;code&gt;Topic&lt;/code&gt; and the &lt;code&gt;Ground truth url&lt;/code&gt; that will be excluded. If you do not have any url to exclude, leave that field empty.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The generated outline will be saved in &lt;code&gt;{output_dir}/{topic}/storm_gen_outline.txt&lt;/code&gt; and the collected references will be saved in &lt;code&gt;{output_dir}/{topic}/raw_search_results.json&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Writing Stage&lt;/h3&gt; &#xA;&lt;p&gt;For batch experiment on FreshWiki dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_writing --input-source file --input-path ../FreshWiki/topic_list.csv --engine gpt-4 --do-polish-article --remove-duplicate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--do-polish-article&lt;/code&gt;: if True, polish the article by adding a summarization section and removing duplicate content if &lt;code&gt;--remove-duplicate&lt;/code&gt; is set True.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To run the experiment on a single topic:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_writing --input-source console --engine gpt-4 --do-polish-article --remove-duplicate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The script will ask you to enter the &lt;code&gt;Topic&lt;/code&gt;. Please enter the same topic as the one used in the pre-writing stage.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The generated article will be saved in &lt;code&gt;{output_dir}/{topic}/storm_gen_article.txt&lt;/code&gt; and the references corresponding to citation index will be saved in &lt;code&gt;{output_dir}/{topic}/url_to_info.json&lt;/code&gt;. If &lt;code&gt;--do-polish-article&lt;/code&gt; is set, the polished article will be saved in &lt;code&gt;{output_dir}/{topic}/storm_gen_article_polished.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Customize the STORM Configurations&lt;/h2&gt; &#xA;&lt;p&gt;We set up the default LLM configuration in &lt;code&gt;LLMConfigs&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/modules/utils.py&#34;&gt;src/modules/utils.py&lt;/a&gt;. You can use &lt;code&gt;set_conv_simulator_lm()&lt;/code&gt;,&lt;code&gt;set_question_asker_lm()&lt;/code&gt;, &lt;code&gt;set_outline_gen_lm()&lt;/code&gt;, &lt;code&gt;set_article_gen_lm()&lt;/code&gt;, &lt;code&gt;set_article_polish_lm()&lt;/code&gt; to override the default configuration. These functions take in an instance from &lt;code&gt;dspy.dsp.LM&lt;/code&gt; or &lt;code&gt;dspy.dsp.HFModel&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;ğŸ’¡&lt;/span&gt; &lt;strong&gt;For a good practice,&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;choose a cheaper/faster model for &lt;code&gt;conv_simulator_lm&lt;/code&gt; which is used to split queries, synthesize answers in the conversation.&lt;/li&gt; &#xA; &lt;li&gt;if you need to conduct the actual writing step, choose a more powerful model for &lt;code&gt;article_gen_lm&lt;/code&gt;. Based on our experiments, weak models are bad at generating text with citations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Automatic Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;In our paper, we break down the evaluation into two parts: outline quality and full-length article quality.&lt;/p&gt; &#xA;&lt;h3&gt;Outline Quality&lt;/h3&gt; &#xA;&lt;p&gt;We introduce &lt;em&gt;heading soft recall&lt;/em&gt; and &lt;em&gt;heading entity recall&lt;/em&gt; to evaluate the outline quality. This makes it easier to prototype methods for pre-writing.&lt;/p&gt; &#xA;&lt;p&gt;Run the following command under &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval&#34;&gt;./eval&lt;/a&gt; to compute the metrics on FreshWiki dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python eval_outline_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --pred-file-name storm_gen_outline.txt --result-output-path ../results/storm_outline_quality.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Full-length Article Quality&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval/eval_article_quality.py&#34;&gt;eval/eval_article_quality.py&lt;/a&gt; provides the entry point of evaluating full-length article quality using ROUGE, entity recall, and rubric grading. Run the following command under &lt;code&gt;eval&lt;/code&gt; to compute the metrics:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python eval_article_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --gt-dir ../FreshWiki --output-dir ../results/storm_article_eval_results --pred-file-name storm_gen_article_polished.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Use the Metric Yourself&lt;/h3&gt; &#xA;&lt;p&gt;The similarity-based metrics (i.e., ROUGE, entity recall, and heading entity recall) are implemented in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval/metrics.py&#34;&gt;eval/metrics.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For rubric grading, we use the &lt;a href=&#34;https://huggingface.co/kaist-ai/prometheus-13b-v1.0&#34;&gt;prometheus-13b-v1.0&lt;/a&gt; introduced in &lt;a href=&#34;https://arxiv.org/abs/2310.08491&#34;&gt;this paper&lt;/a&gt;. &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval/evaluation_prometheus.py&#34;&gt;eval/evaluation_prometheus.py&lt;/a&gt; provides the entry point of using the metric.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or suggestions, please feel free to open an issue or pull request. We welcome contributions to improve the system and the codebase!&lt;/p&gt; &#xA;&lt;p&gt;Contact person: &lt;a href=&#34;mailto:shaoyj@stanford.edu&#34;&gt;Yijia Shao&lt;/a&gt; and &lt;a href=&#34;mailto:yuchengj@stanford.edu&#34;&gt;Yucheng Jiang&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite our paper if you use this code or part of it in your work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{shao2024assisting,&#xA;      title={{Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}}, &#xA;      author={Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam},&#xA;      year={2024},&#xA;      booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>