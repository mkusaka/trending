<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-26T01:24:52Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hiyouga/LLaMA-Factory</title>
    <updated>2024-04-26T01:24:52Z</updated>
    <id>tag:github.com,2024-04-26:/hiyouga/LLaMA-Factory</id>
    <link href="https://github.com/hiyouga/LLaMA-Factory" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unify Efficient Fine-Tuning of 100+ LLMs&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/logo.png&#34; alt=&#34;# LLaMA Factory&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/hiyouga/LLaMA-Factory&#34; alt=&#34;GitHub Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/commits/main&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory&#34; alt=&#34;GitHub last commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/llmtuner/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/llmtuner&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/llmtuner/&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/llmtuner&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#projects-using-llama-factory&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-34-green&#34; alt=&#34;Citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-blue&#34; alt=&#34;GitHub pull request&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/rKfvV9r9FK&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&amp;amp;style=flat&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/llamafactory_ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/llamafactory_ai&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/hiyouga/LLaMA-Board&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue&#34; alt=&#34;Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://modelscope.cn/studios/hiyouga/LLaMA-Board&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue&#34; alt=&#34;Studios&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ðŸ‘‹ Join our &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/wechat.jpg&#34;&gt;WeChat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[ English | &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/README_zh.md&#34;&gt;ä¸­æ–‡&lt;/a&gt; ]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fine-tuning a large language model can be easy as...&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/assets/16256802/9840a653-7e9c-41c8-ae89-7ace5698baf6&#34;&gt;https://github.com/hiyouga/LLaMA-Factory/assets/16256802/9840a653-7e9c-41c8-ae89-7ace5698baf6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Choose your path:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Colab&lt;/strong&gt;: &lt;a href=&#34;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local machine&lt;/strong&gt;: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started&#34;&gt;usage&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#benchmark&#34;&gt;Benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#changelog&#34;&gt;Changelog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-models&#34;&gt;Supported Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-training-approaches&#34;&gt;Supported Training Approaches&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#provided-datasets&#34;&gt;Provided Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#requirement&#34;&gt;Requirement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#projects-using-llama-factory&#34;&gt;Projects using LLaMA Factory&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Various models&lt;/strong&gt;: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Yi, Gemma, Baichuan, ChatGLM, Phi, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Integrated methods&lt;/strong&gt;: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO and ORPO.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scalable resources&lt;/strong&gt;: 32-bit full-tuning, 16-bit freeze-tuning, 16-bit LoRA and 2/4/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced algorithms&lt;/strong&gt;: GaLore, BAdam, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and Agent tuning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Practical tricks&lt;/strong&gt;: FlashAttention-2, Unsloth, RoPE scaling, NEFTune and rsLoRA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Experiment monitors&lt;/strong&gt;: LlamaBoard, TensorBoard, Wandb, MLflow, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Faster inference&lt;/strong&gt;: OpenAI-style API, Gradio UI and CLI with vLLM worker.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Compared to ChatGLM&#39;s &lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning&#34;&gt;P-Tuning&lt;/a&gt;, LLaMA Factory&#39;s LoRA tuning offers up to &lt;strong&gt;3.7 times faster&lt;/strong&gt; training speed with a better Rouge score on the advertising text generation task. By leveraging 4-bit quantization technique, LLaMA Factory&#39;s QLoRA further improves the efficiency regarding the GPU memory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/benchmark.svg?sanitize=true&#34; alt=&#34;benchmark&#34;&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Definitions&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Training Speed&lt;/strong&gt;: the number of training samples processed per second during the training. (bs=4, cutoff_len=1024)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Rouge Score&lt;/strong&gt;: Rouge-2 score on the development set of the &lt;a href=&#34;https://aclanthology.org/D19-1321.pdf&#34;&gt;advertising text generation&lt;/a&gt; task. (bs=4, cutoff_len=1024)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;GPU Memory&lt;/strong&gt;: Peak GPU memory usage in 4-bit quantized training. (bs=1, cutoff_len=1024)&lt;/li&gt; &#xA;  &lt;li&gt;We adopt &lt;code&gt;pre_seq_len=128&lt;/code&gt; for ChatGLM&#39;s P-Tuning and &lt;code&gt;lora_rank=32&lt;/code&gt; for LLaMA Factory&#39;s LoRA tuning.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;[24/04/26] We supported fine-tuning the &lt;strong&gt;LLaVA-1.5&lt;/strong&gt; multimodal LLMs. See &lt;code&gt;examples/lora_single_gpu/sft_mllm.sh&lt;/code&gt; for usage.&lt;/p&gt; &#xA;&lt;p&gt;[24/04/22] We provided a &lt;strong&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&#34;&gt;Colab notebook&lt;/a&gt;&lt;/strong&gt; for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check &lt;a href=&#34;https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat&#34;&gt;Llama3-8B-Chinese-Chat&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/zhichen/Llama3-Chinese&#34;&gt;Llama3-Chinese&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;[24/04/21] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.02258&#34;&gt;Mixture-of-Depths&lt;/a&gt;&lt;/strong&gt; according to &lt;a href=&#34;https://github.com/astramind-ai/Mixture-of-depths&#34;&gt;AstraMindAI&#39;s implementation&lt;/a&gt;. See &lt;code&gt;examples/extras/mod&lt;/code&gt; for usage.&lt;/p&gt; &#xA;&lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.02827&#34;&gt;BAdam&lt;/a&gt;&lt;/strong&gt;. See &lt;code&gt;examples/extras/badam&lt;/code&gt; for usage.&lt;/p&gt; &#xA;&lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href=&#34;https://github.com/unslothai/unsloth&#34;&gt;unsloth&lt;/a&gt;&lt;/strong&gt;&#39;s long-sequence training (Llama-2-7B-56k within 24GB). It achieves &lt;strong&gt;117%&lt;/strong&gt; speed and &lt;strong&gt;50%&lt;/strong&gt; memory compared with FlashAttention-2, more benchmarks can be found in &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison&#34;&gt;this page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Full Changelog&lt;/summary&gt; &#xA; &lt;p&gt;[24/03/31] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.07691&#34;&gt;ORPO&lt;/a&gt;&lt;/strong&gt;. See &lt;code&gt;examples/lora_single_gpu&lt;/code&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/03/21] Our paper &#34;&lt;a href=&#34;https://arxiv.org/abs/2403.13372&#34;&gt;LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models&lt;/a&gt;&#34; is available at arXiv!&lt;/p&gt; &#xA; &lt;p&gt;[24/03/20] We supported &lt;strong&gt;FSDP+QLoRA&lt;/strong&gt; that fine-tunes a 70B model on 2x24GB GPUs. See &lt;code&gt;examples/extras/fsdp_qlora&lt;/code&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/03/13] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.12354&#34;&gt;LoRA+&lt;/a&gt;&lt;/strong&gt;. See &lt;code&gt;examples/extras/loraplus&lt;/code&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/03/07] We supported gradient low-rank projection (&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.03507&#34;&gt;GaLore&lt;/a&gt;&lt;/strong&gt;) algorithm. See &lt;code&gt;examples/extras/galore&lt;/code&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/03/07] We integrated &lt;strong&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;&lt;/strong&gt; for faster and concurrent inference. Try &lt;code&gt;--infer_backend vllm&lt;/code&gt; to enjoy &lt;strong&gt;270%&lt;/strong&gt; inference speed. (LoRA is not yet supported, merge it first.)&lt;/p&gt; &#xA; &lt;p&gt;[24/02/28] We supported weight-decomposed LoRA (&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.09353&#34;&gt;DoRA&lt;/a&gt;&lt;/strong&gt;). Try &lt;code&gt;--use_dora&lt;/code&gt; to activate DoRA training.&lt;/p&gt; &#xA; &lt;p&gt;[24/02/15] We supported &lt;strong&gt;block expansion&lt;/strong&gt; proposed by &lt;a href=&#34;https://github.com/TencentARC/LLaMA-Pro&#34;&gt;LLaMA Pro&lt;/a&gt;. See &lt;code&gt;examples/extras/llama_pro&lt;/code&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this &lt;a href=&#34;https://qwenlm.github.io/blog/qwen1.5/&#34;&gt;blog post&lt;/a&gt; for details.&lt;/p&gt; &#xA; &lt;p&gt;[24/01/18] We supported &lt;strong&gt;agent tuning&lt;/strong&gt; for most models, equipping model with tool using abilities by fine-tuning with &lt;code&gt;--dataset glaive_toolcall&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;[23/12/23] We supported &lt;strong&gt;&lt;a href=&#34;https://github.com/unslothai/unsloth&#34;&gt;unsloth&lt;/a&gt;&lt;/strong&gt;&#39;s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try &lt;code&gt;--use_unsloth&lt;/code&gt; argument to activate unsloth patch. It achieves &lt;strong&gt;170%&lt;/strong&gt; speed in our benchmark, check &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison&#34;&gt;this page&lt;/a&gt; for details.&lt;/p&gt; &#xA; &lt;p&gt;[23/12/12] We supported fine-tuning the latest MoE model &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x7B-v0.1&#34;&gt;Mixtral 8x7B&lt;/a&gt;&lt;/strong&gt; in our framework. See hardware requirement &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#hardware-requirement&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;[23/12/01] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href=&#34;https://modelscope.cn/models&#34;&gt;ModelScope Hub&lt;/a&gt;&lt;/strong&gt; for Chinese mainland users. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-modelscope-hub-optional&#34;&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; &#xA; &lt;p&gt;[23/10/21] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.05914&#34;&gt;NEFTune&lt;/a&gt;&lt;/strong&gt; trick for fine-tuning. Try &lt;code&gt;--neftune_noise_alpha&lt;/code&gt; argument to activate NEFTune, e.g., &lt;code&gt;--neftune_noise_alpha 5&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;[23/09/27] We supported &lt;strong&gt;$S^2$-Attn&lt;/strong&gt; proposed by &lt;a href=&#34;https://github.com/dvlab-research/LongLoRA&#34;&gt;LongLoRA&lt;/a&gt; for the LLaMA models. Try &lt;code&gt;--shift_attn&lt;/code&gt; argument to enable shift short attention.&lt;/p&gt; &#xA; &lt;p&gt;[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#evaluation&#34;&gt;this example&lt;/a&gt; to evaluate your models.&lt;/p&gt; &#xA; &lt;p&gt;[23/09/10] We supported &lt;strong&gt;&lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention-2&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;--flash_attn fa2&lt;/code&gt; argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.&lt;/p&gt; &#xA; &lt;p&gt;[23/08/12] We supported &lt;strong&gt;RoPE scaling&lt;/strong&gt; to extend the context length of the LLaMA models. Try &lt;code&gt;--rope_scaling linear&lt;/code&gt; argument in training and &lt;code&gt;--rope_scaling dynamic&lt;/code&gt; argument at inference to extrapolate the position embeddings.&lt;/p&gt; &#xA; &lt;p&gt;[23/08/11] We supported &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.18290&#34;&gt;DPO training&lt;/a&gt;&lt;/strong&gt; for instruction-tuned models. See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#dpo-training&#34;&gt;this example&lt;/a&gt; to train your models.&lt;/p&gt; &#xA; &lt;p&gt;[23/07/31] We supported &lt;strong&gt;dataset streaming&lt;/strong&gt;. Try &lt;code&gt;--streaming&lt;/code&gt; and &lt;code&gt;--max_steps 10000&lt;/code&gt; arguments to load your dataset in streaming mode.&lt;/p&gt; &#xA; &lt;p&gt;[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos (&lt;a href=&#34;https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat&#34;&gt;LLaMA-2&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/hiyouga/Baichuan-13B-sft&#34;&gt;Baichuan&lt;/a&gt;) for details.&lt;/p&gt; &#xA; &lt;p&gt;[23/07/18] We developed an &lt;strong&gt;all-in-one Web UI&lt;/strong&gt; for training, evaluation and inference. Try &lt;code&gt;train_web.py&lt;/code&gt; to fine-tune models in your Web browser. Thank &lt;a href=&#34;https://github.com/KanadeSiina&#34;&gt;@KanadeSiina&lt;/a&gt; and &lt;a href=&#34;https://github.com/codemayq&#34;&gt;@codemayq&lt;/a&gt; for their efforts in the development.&lt;/p&gt; &#xA; &lt;p&gt;[23/07/09] We released &lt;strong&gt;&lt;a href=&#34;https://github.com/hiyouga/FastEdit&#34;&gt;FastEdit&lt;/a&gt;&lt;/strong&gt; âš¡ðŸ©¹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow &lt;a href=&#34;https://github.com/hiyouga/FastEdit&#34;&gt;FastEdit&lt;/a&gt; if you are interested.&lt;/p&gt; &#xA; &lt;p&gt;[23/06/29] We provided a &lt;strong&gt;reproducible example&lt;/strong&gt; of training a chat model using instruction-following datasets, see &lt;a href=&#34;https://huggingface.co/hiyouga/Baichuan-7B-sft&#34;&gt;Baichuan-7B-sft&lt;/a&gt; for details.&lt;/p&gt; &#xA; &lt;p&gt;[23/06/22] We aligned the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/api_demo.py&#34;&gt;demo API&lt;/a&gt; with the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;OpenAI&#39;s&lt;/a&gt; format where you can insert the fine-tuned model in &lt;strong&gt;arbitrary ChatGPT-based applications&lt;/strong&gt;.&lt;/p&gt; &#xA; &lt;p&gt;[23/06/03] We supported quantized training and inference (aka &lt;strong&gt;&lt;a href=&#34;https://github.com/artidoro/qlora&#34;&gt;QLoRA&lt;/a&gt;&lt;/strong&gt;). Try &lt;code&gt;--quantization_bit 4/8&lt;/code&gt; argument to work with quantized models.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Model size&lt;/th&gt; &#xA;   &lt;th&gt;Default module&lt;/th&gt; &#xA;   &lt;th&gt;Template&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/baichuan-inc&#34;&gt;Baichuan2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B&lt;/td&gt; &#xA;   &lt;td&gt;W_pack&lt;/td&gt; &#xA;   &lt;td&gt;baichuan2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigscience&#34;&gt;BLOOM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;560M/1.1B/1.7B/3B/7.1B/176B&lt;/td&gt; &#xA;   &lt;td&gt;query_key_value&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigscience&#34;&gt;BLOOMZ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;560M/1.1B/1.7B/3B/7.1B/176B&lt;/td&gt; &#xA;   &lt;td&gt;query_key_value&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/THUDM&#34;&gt;ChatGLM3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6B&lt;/td&gt; &#xA;   &lt;td&gt;query_key_value&lt;/td&gt; &#xA;   &lt;td&gt;chatglm3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/CohereForAI&#34;&gt;Command-R&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;35B/104B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;cohere&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai&#34;&gt;DeepSeek (MoE)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/16B/67B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;deepseek&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tiiuae&#34;&gt;Falcon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/40B/180B&lt;/td&gt; &#xA;   &lt;td&gt;query_key_value&lt;/td&gt; &#xA;   &lt;td&gt;falcon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/google&#34;&gt;Gemma/CodeGemma&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2B/7B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;gemma&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/internlm&#34;&gt;InternLM2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/20B&lt;/td&gt; &#xA;   &lt;td&gt;wqkv&lt;/td&gt; &#xA;   &lt;td&gt;intern2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B/33B/65B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;LLaMA-2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B/70B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;llama2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;LLaMA-3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8B/70B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;llama3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llava-hf&#34;&gt;LLaVA-1.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;vicuna&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai&#34;&gt;Mistral/Mixtral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/8x7B/8x22B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;mistral&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/allenai&#34;&gt;OLMo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1B/7B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft&#34;&gt;Phi-1.5/2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.3B/2.7B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft&#34;&gt;Phi-3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.8B&lt;/td&gt; &#xA;   &lt;td&gt;qkv_proj&lt;/td&gt; &#xA;   &lt;td&gt;phi&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Qwen&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.8B/7B/14B/72B&lt;/td&gt; &#xA;   &lt;td&gt;c_attn&lt;/td&gt; &#xA;   &lt;td&gt;qwen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Qwen1.5 (Code/MoE)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.5B/1.8B/4B/7B/14B/32B/72B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;qwen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigcode&#34;&gt;StarCoder2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3B/7B/15B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/xverse&#34;&gt;XVERSE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B/65B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;xverse&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/01-ai&#34;&gt;Yi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6B/9B/34B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;yi&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/IEITYuan&#34;&gt;Yuan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2B/51B/102B&lt;/td&gt; &#xA;   &lt;td&gt;q_proj,v_proj&lt;/td&gt; &#xA;   &lt;td&gt;yuan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] &lt;strong&gt;Default module&lt;/strong&gt; is used for the &lt;code&gt;--lora_target&lt;/code&gt; argument, you can use &lt;code&gt;--lora_target all&lt;/code&gt; to specify all the available modules for better convergence.&lt;/p&gt; &#xA; &lt;p&gt;For the &#34;base&#34; models, the &lt;code&gt;--template&lt;/code&gt; argument can be chosen from &lt;code&gt;default&lt;/code&gt;, &lt;code&gt;alpaca&lt;/code&gt;, &lt;code&gt;vicuna&lt;/code&gt; etc. But make sure to use the &lt;strong&gt;corresponding template&lt;/strong&gt; for the &#34;instruct/chat&#34; models.&lt;/p&gt; &#xA; &lt;p&gt;Remember to use the &lt;strong&gt;SAME&lt;/strong&gt; template in training and inference.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llmtuner/extras/constants.py&#34;&gt;constants.py&lt;/a&gt; for a full list of models we supported.&lt;/p&gt; &#xA;&lt;p&gt;You also can add a custom chat template to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llmtuner/data/template.py&#34;&gt;template.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Training Approaches&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Approach&lt;/th&gt; &#xA;   &lt;th&gt;Full-tuning&lt;/th&gt; &#xA;   &lt;th&gt;Freeze-tuning&lt;/th&gt; &#xA;   &lt;th&gt;LoRA&lt;/th&gt; &#xA;   &lt;th&gt;QLoRA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pre-Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Supervised Fine-Tuning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Reward Modeling&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PPO Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DPO Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ORPO Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;âœ…&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Provided Datasets&lt;/h2&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Pre-training datasets&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/wiki_demo.txt&#34;&gt;Wiki Demo (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/tiiuae/falcon-refinedweb&#34;&gt;RefinedWeb (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2&#34;&gt;RedPajama V2 (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/olm/olm-wikipedia-20221220&#34;&gt;Wikipedia (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered&#34;&gt;Wikipedia (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/EleutherAI/pile&#34;&gt;Pile (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Skywork/SkyPile-150B&#34;&gt;SkyPile (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/bigcode/the-stack&#34;&gt;The Stack (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/bigcode/starcoderdata&#34;&gt;StarCoder (en)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Supervised fine-tuning datasets&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Stanford Alpaca (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;Alpaca GPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/self_cognition.json&#34;&gt;Self Cognition (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1&#34;&gt;Open Assistant (multilingual)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Chinese-instruction-collection&#34;&gt;ShareGPT (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&#34;&gt;Guanaco Dataset (multilingual)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_2M_CN&#34;&gt;BELLE 2M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_1M_CN&#34;&gt;BELLE 1M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_0.5M_CN&#34;&gt;BELLE 0.5M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M&#34;&gt;BELLE Dialogue 0.4M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/school_math_0.25M&#34;&gt;BELLE School Math 0.25M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M&#34;&gt;BELLE Multiturn Chat 0.8M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/thunlp/UltraChat&#34;&gt;UltraChat (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/GAIR/lima&#34;&gt;LIMA (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/garage-bAInd/Open-Platypus&#34;&gt;OpenPlatypus (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k&#34;&gt;CodeAlpaca 20k (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT&#34;&gt;Alpaca CoT (multilingual)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Open-Orca/OpenOrca&#34;&gt;OpenOrca (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Open-Orca/SlimOrca&#34;&gt;SlimOrca (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/TIGER-Lab/MathInstruct&#34;&gt;MathInstruct (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M&#34;&gt;Firefly 1.1M (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/wiki_qa&#34;&gt;Wiki QA (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/suolyer/webqa&#34;&gt;Web QA (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/zxbsmk/webnovel_cn&#34;&gt;WebNovel (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/berkeley-nest/Nectar&#34;&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data&#34;&gt;deepctrl (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HasturOfficial/adgen&#34;&gt;Ad Gen (zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k&#34;&gt;ShareGPT Hyperfiltered (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/shibing624/sharegpt_gpt4&#34;&gt;ShareGPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k&#34;&gt;UltraChat 200k (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/THUDM/AgentInstruct&#34;&gt;AgentInstruct (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/lmsys/lmsys-chat-1m&#34;&gt;LMSYS Chat 1M (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k&#34;&gt;Evol Instruct V2 (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2&#34;&gt;Glaive Function Calling V2 (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceTB/cosmopedia&#34;&gt;Cosmopedia (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/oasst_de&#34;&gt;Open Assistant (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de&#34;&gt;Dolly 15k (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de&#34;&gt;Alpaca GPT4 (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de&#34;&gt;OpenSchnabeltier (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de&#34;&gt;Evol Instruct (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/dolphin_de&#34;&gt;Dolphin (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/booksum_de&#34;&gt;Booksum (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de&#34;&gt;Airoboros (de)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de&#34;&gt;Ultrachat (de)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Preference datasets&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Anthropic/hh-rlhf&#34;&gt;HH-RLHF (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1&#34;&gt;Open Assistant (multilingual)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4 Generated Data (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Intel/orca_dpo_pairs&#34;&gt;Orca DPO (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/berkeley-nest/Nectar&#34;&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k&#34;&gt;DPO mix (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de&#34;&gt;Orca DPO (de)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade huggingface_hub&#xA;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Requirement&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Mandatory&lt;/th&gt; &#xA;   &lt;th&gt;Minimum&lt;/th&gt; &#xA;   &lt;th&gt;Recommend&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;python&lt;/td&gt; &#xA;   &lt;td&gt;3.8&lt;/td&gt; &#xA;   &lt;td&gt;3.10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;torch&lt;/td&gt; &#xA;   &lt;td&gt;1.13.1&lt;/td&gt; &#xA;   &lt;td&gt;2.2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;transformers&lt;/td&gt; &#xA;   &lt;td&gt;4.37.2&lt;/td&gt; &#xA;   &lt;td&gt;4.39.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;datasets&lt;/td&gt; &#xA;   &lt;td&gt;2.14.3&lt;/td&gt; &#xA;   &lt;td&gt;2.18.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;accelerate&lt;/td&gt; &#xA;   &lt;td&gt;0.27.2&lt;/td&gt; &#xA;   &lt;td&gt;0.28.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;peft&lt;/td&gt; &#xA;   &lt;td&gt;0.9.0&lt;/td&gt; &#xA;   &lt;td&gt;0.10.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;trl&lt;/td&gt; &#xA;   &lt;td&gt;0.8.1&lt;/td&gt; &#xA;   &lt;td&gt;0.8.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Optional&lt;/th&gt; &#xA;   &lt;th&gt;Minimum&lt;/th&gt; &#xA;   &lt;th&gt;Recommend&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CUDA&lt;/td&gt; &#xA;   &lt;td&gt;11.6&lt;/td&gt; &#xA;   &lt;td&gt;12.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;deepspeed&lt;/td&gt; &#xA;   &lt;td&gt;0.10.0&lt;/td&gt; &#xA;   &lt;td&gt;0.14.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bitsandbytes&lt;/td&gt; &#xA;   &lt;td&gt;0.39.0&lt;/td&gt; &#xA;   &lt;td&gt;0.43.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;flash-attn&lt;/td&gt; &#xA;   &lt;td&gt;2.3.0&lt;/td&gt; &#xA;   &lt;td&gt;2.5.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Hardware Requirement&lt;/h3&gt; &#xA;&lt;p&gt;* &lt;em&gt;estimated&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Bits&lt;/th&gt; &#xA;   &lt;th&gt;7B&lt;/th&gt; &#xA;   &lt;th&gt;13B&lt;/th&gt; &#xA;   &lt;th&gt;30B&lt;/th&gt; &#xA;   &lt;th&gt;70B&lt;/th&gt; &#xA;   &lt;th&gt;8x7B&lt;/th&gt; &#xA;   &lt;th&gt;8x22B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Full&lt;/td&gt; &#xA;   &lt;td&gt;AMP&lt;/td&gt; &#xA;   &lt;td&gt;120GB&lt;/td&gt; &#xA;   &lt;td&gt;240GB&lt;/td&gt; &#xA;   &lt;td&gt;600GB&lt;/td&gt; &#xA;   &lt;td&gt;1200GB&lt;/td&gt; &#xA;   &lt;td&gt;900GB&lt;/td&gt; &#xA;   &lt;td&gt;2400GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Full&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;60GB&lt;/td&gt; &#xA;   &lt;td&gt;120GB&lt;/td&gt; &#xA;   &lt;td&gt;300GB&lt;/td&gt; &#xA;   &lt;td&gt;600GB&lt;/td&gt; &#xA;   &lt;td&gt;400GB&lt;/td&gt; &#xA;   &lt;td&gt;1200GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Freeze&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;40GB&lt;/td&gt; &#xA;   &lt;td&gt;80GB&lt;/td&gt; &#xA;   &lt;td&gt;200GB&lt;/td&gt; &#xA;   &lt;td&gt;160GB&lt;/td&gt; &#xA;   &lt;td&gt;400GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA/GaLore/BAdam&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;16GB&lt;/td&gt; &#xA;   &lt;td&gt;32GB&lt;/td&gt; &#xA;   &lt;td&gt;64GB&lt;/td&gt; &#xA;   &lt;td&gt;160GB&lt;/td&gt; &#xA;   &lt;td&gt;120GB&lt;/td&gt; &#xA;   &lt;td&gt;320GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;10GB&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;40GB&lt;/td&gt; &#xA;   &lt;td&gt;80GB&lt;/td&gt; &#xA;   &lt;td&gt;60GB&lt;/td&gt; &#xA;   &lt;td&gt;160GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;6GB&lt;/td&gt; &#xA;   &lt;td&gt;12GB&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;   &lt;td&gt;48GB&lt;/td&gt; &#xA;   &lt;td&gt;30GB&lt;/td&gt; &#xA;   &lt;td&gt;96GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;4GB&lt;/td&gt; &#xA;   &lt;td&gt;8GB&lt;/td&gt; &#xA;   &lt;td&gt;16GB&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;   &lt;td&gt;18GB&lt;/td&gt; &#xA;   &lt;td&gt;48GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Data Preparation&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/README.md&#34;&gt;data/README.md&lt;/a&gt; for checking the details about the format of dataset files. You can either use datasets on HuggingFace / ModelScope hub or load the dataset in local disk.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Dependence Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hiyouga/LLaMA-Factory.git&#xA;conda create -n llama_factory python=3.10&#xA;conda activate llama_factory&#xA;cd LLaMA-Factory&#xA;pip install -e .[metrics]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Extra dependencies available: deepspeed, metrics, unsloth, galore, badam, vllm, bitsandbytes, gptq, awq, aqlm, qwen, modelscope, quality&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;For Windows users&lt;/summary&gt; &#xA; &lt;p&gt;If you want to enable the quantized LoRA (QLoRA) on the Windows platform, you will be required to install a pre-built version of &lt;code&gt;bitsandbytes&lt;/code&gt; library, which supports CUDA 11.1 to 12.2, please select the appropriate &lt;a href=&#34;https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels&#34;&gt;release version&lt;/a&gt; based on your CUDA version.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To enable FlashAttention-2 on the Windows platform, you need to install the precompiled &lt;code&gt;flash-attn&lt;/code&gt; library, which supports CUDA 12.1 to 12.2. Please download the corresponding version from &lt;a href=&#34;https://github.com/bdashore3/flash-attention/releases&#34;&gt;flash-attention&lt;/a&gt; based on your requirements.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Train with LLaMA Board GUI (powered by &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] LLaMA Board GUI only supports training on a single GPU, please use &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#command-line-interface&#34;&gt;CLI&lt;/a&gt; for distributed training.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Use local environment&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=0 # `set CUDA_VISIBLE_DEVICES=0` for Windows&#xA;export GRADIO_SERVER_PORT=7860 # `set GRADIO_SERVER_PORT=7860` for Windows&#xA;python src/train_web.py # or python -m llmtuner.webui.interface&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;For Alibaba Cloud users&lt;/summary&gt; &#xA; &lt;p&gt;If you encountered display problems in LLaMA Board on Alibaba Cloud, try using the following command to set environment variables before starting LLaMA Board:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export GRADIO_ROOT_PATH=/${JUPYTER_NAME}/proxy/7860/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Use Docker&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -f ./Dockerfile -t llama-factory:latest .&#xA;docker run --gpus=all \&#xA;    -v ./hf_cache:/root/.cache/huggingface/ \&#xA;    -v ./data:/app/data \&#xA;    -v ./output:/app/output \&#xA;    -e CUDA_VISIBLE_DEVICES=0 \&#xA;    -p 7860:7860 \&#xA;    --shm-size 16G \&#xA;    --name llama_factory \&#xA;    -d llama-factory:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Use Docker Compose&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose -f ./docker-compose.yml up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Details about volume&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;hf_cache: Utilize Hugging Face cache on the host machine. Reassignable if a cache already exists in a different directory.&lt;/li&gt; &#xA;  &lt;li&gt;data: Place datasets on this dir of the host machine so that they can be selected on LLaMA Board GUI.&lt;/li&gt; &#xA;  &lt;li&gt;output: Set export dir to this location so that the merged result can be accessed directly on the host machine.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Train with Command Line Interface&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&#34;&gt;examples/README.md&lt;/a&gt; for usage.&lt;/p&gt; &#xA;&lt;p&gt;Use &lt;code&gt;python src/train_bash.py -h&lt;/code&gt; to display arguments description.&lt;/p&gt; &#xA;&lt;h3&gt;Deploy with OpenAI-style API and vLLM&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0,1 API_PORT=8000 python src/api_demo.py \&#xA;    --model_name_or_path meta-llama/Meta-Llama-3-8B-Instruct \&#xA;    --template llama3 \&#xA;    --infer_backend vllm \&#xA;    --vllm_enforce_eager&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download from ModelScope Hub&lt;/h3&gt; &#xA;&lt;p&gt;If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train the model by specifying a model ID of the ModelScope Hub as the &lt;code&gt;--model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href=&#34;https://modelscope.cn/models&#34;&gt;ModelScope Hub&lt;/a&gt;, e.g., &lt;code&gt;LLM-Research/Meta-Llama-3-8B-Instruct&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Projects using LLaMA Factory&lt;/h2&gt; &#xA;&lt;p&gt;If you have a project that should be incorporated, please contact via email or create a pull request.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Click to show&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. &lt;a href=&#34;https://arxiv.org/abs/2308.02223&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. &lt;a href=&#34;https://arxiv.org/abs/2308.10092&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. &lt;a href=&#34;https://arxiv.org/abs/2308.10526&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. &lt;a href=&#34;https://arxiv.org/abs/2311.07816&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. &lt;a href=&#34;https://arxiv.org/abs/2312.15710&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. 2024. &lt;a href=&#34;https://arxiv.org/abs/2401.04319&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. 2024. &lt;a href=&#34;https://arxiv.org/abs/2401.07286&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.05904&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.07625&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11176&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11187&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11746&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11801&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11809&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.11819&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.12204&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.14714&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2402.15043&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.02333&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.03419&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.08228&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zhang et al. EDT: Improving Large Language Models&#39; Generation by Entropy-based Dynamic Temperature Sampling. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.14541&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.15246&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.16008&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. &lt;a href=&#34;https://arxiv.org/abs/2403.16443&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.00604&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.02827&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.04167&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. &lt;a href=&#34;https://arxiv.org/abs/2404.07084&#34;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/Yu-Yang-Li/StarWhisper&#34;&gt;StarWhisper&lt;/a&gt;&lt;/strong&gt;: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/FudanDISC/DISC-LawLLM&#34;&gt;DISC-LawLLM&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/thomas-yanxin/Sunsimiao&#34;&gt;Sunsimiao&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/WangRongsheng/CareGPT&#34;&gt;CareGPT&lt;/a&gt;&lt;/strong&gt;: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/Machine-Mindset/&#34;&gt;MachineMindset&lt;/a&gt;&lt;/strong&gt;: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please follow the model licenses to use the corresponding model weights: &lt;a href=&#34;https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf&#34;&gt;Baichuan2&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/spaces/bigscience/license&#34;&gt;BLOOM&lt;/a&gt; / &lt;a href=&#34;https://github.com/THUDM/ChatGLM3/raw/main/MODEL_LICENSE&#34;&gt;ChatGLM3&lt;/a&gt; / &lt;a href=&#34;https://cohere.com/c4ai-cc-by-nc-license&#34;&gt;Command-R&lt;/a&gt; / &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/main/LICENSE-MODEL&#34;&gt;DeepSeek&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt&#34;&gt;Falcon&lt;/a&gt; / &lt;a href=&#34;https://ai.google.dev/gemma/terms&#34;&gt;Gemma&lt;/a&gt; / &lt;a href=&#34;https://github.com/InternLM/InternLM#license&#34;&gt;InternLM2&lt;/a&gt; / &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&#34;&gt;LLaMA&lt;/a&gt; / &lt;a href=&#34;https://ai.meta.com/llama/license/&#34;&gt;LLaMA-2/LLaVA-1.5&lt;/a&gt; / &lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;LLaMA-3&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&#34;&gt;Mistral&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&#34;&gt;OLMo&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx&#34;&gt;Phi-1.5/2&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE&#34;&gt;Phi-3&lt;/a&gt; / &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT&#34;&gt;Qwen&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement&#34;&gt;StarCoder2&lt;/a&gt; / &lt;a href=&#34;https://github.com/xverse-ai/XVERSE-13B/raw/main/MODEL_LICENSE.pdf&#34;&gt;XVERSE&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE&#34;&gt;Yi&lt;/a&gt; / &lt;a href=&#34;https://github.com/IEIT-Yuan/Yuan-2.0/raw/main/LICENSE-Yuan&#34;&gt;Yuan&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If this work is helpful, please kindly cite as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{zheng2024llamafactory,&#xA;  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},&#xA;  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Yongqiang Ma},&#xA;  journal={arXiv preprint arXiv:2403.13372},&#xA;  year={2024},&#xA;  url={http://arxiv.org/abs/2403.13372}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo benefits from &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/trl&#34;&gt;TRL&lt;/a&gt;, &lt;a href=&#34;https://github.com/artidoro/qlora&#34;&gt;QLoRA&lt;/a&gt; and &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>whoeevee/EeveeSpotify</title>
    <updated>2024-04-26T01:24:52Z</updated>
    <id>tag:github.com,2024-04-26:/whoeevee/EeveeSpotify</id>
    <link href="https://github.com/whoeevee/EeveeSpotify" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A tweak to get Spotify Premium for free, just like Spotilife&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/whoeevee/EeveeSpotify/swift/Images/banner.png&#34; alt=&#34;Banner&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;EeveeSpotify&lt;/h1&gt; &#xA;&lt;p&gt;This tweak makes Spotify think that you have a Premium subscription, granting free listening, just like Spotilife.&lt;/p&gt; &#xA;&lt;h2&gt;The History&lt;/h2&gt; &#xA;&lt;p&gt;Several months ago, Spotilife, the only tweak to get Spotify Premium, stopped working on new Spotify versions. I decompiled Spotilife, reverse-engineered Spotify, intercepted requests, etc., and created this tweak.&lt;/p&gt; &#xA;&lt;h2&gt;Restrictions&lt;/h2&gt; &#xA;&lt;p&gt;Please refrain from opening issues about the following features, as they are server-sided and will &lt;strong&gt;NEVER&lt;/strong&gt; work:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using &#34;Very High&#34; audio quality&lt;/li&gt; &#xA; &lt;li&gt;Downloading songs/playlists (podcast downloads work, though)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In theory, implementing downloading/offline mode locally &lt;em&gt;should&lt;/em&gt; be possible, but it will &lt;strong&gt;not&lt;/strong&gt; be included in this tweak.&lt;/p&gt; &#xA;&lt;h2&gt;How It Works&lt;/h2&gt; &#xA;&lt;p&gt;Upon login, Spotify fetches user data, including active subscription, and caches it in the &lt;code&gt;offline.bnk&lt;/code&gt; file in the &lt;code&gt;/Library/Application Support/PersistentCache&lt;/code&gt; directory. It uses its proprietary binary format to store data, incorporating a length byte before each value, among other conventions. Certain keys, such as &lt;code&gt;player-license&lt;/code&gt;, &lt;code&gt;financial-product&lt;/code&gt;, and &lt;code&gt;type&lt;/code&gt;, determines the user abilities.&lt;/p&gt; &#xA;&lt;p&gt;The tweak patches this file while initializing; Spotify loads it and assumes you have Premium. To be honest, it doesn&#39;t really patch due to challenges with dynamic length and varied bytes. Ideally, there should be a parser capable of deserializing and serializing such format. However, for now, the tweak simply extracts the username from the current &lt;code&gt;offline.bnk&lt;/code&gt; file and inserts it into &lt;code&gt;premiumblank.bnk&lt;/code&gt; (a file containing all premium values preset), replacing &lt;code&gt;offline.bnk&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/whoeevee/EeveeSpotify/swift/Images/hex.png&#34; alt=&#34;Hex&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tweak also changes query parameters &lt;code&gt;trackRows&lt;/code&gt; and &lt;code&gt;video&lt;/code&gt; in HTTP requests to true, so Spotify loads videos and not just track names at the artist page. Sorry if the code seems cringe; the main focus is on the concept. It can stop working just like Spotilife, but so far, it works on the latest Spotify 8.9.## (Spotilife also patches &lt;code&gt;offline.bnk&lt;/code&gt;, but it changes obscure bytes that do nothing on new versions). Spotify reloads user data from time to time (and on changing network, for example), so if Premium stops working, simply restart the app.&lt;/p&gt; &#xA;&lt;p&gt;To open Spotify links in sideloaded app, use &lt;a href=&#34;https://github.com/BillyCurtis/OpenSpotifySafariExtension&#34;&gt;OpenSpotifySafariExtension&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mentalDetector/Quansheng_UV-K5_PCB_R51-V1.4_PCB_Reversing_Rev._0.9</title>
    <updated>2024-04-26T01:24:52Z</updated>
    <id>tag:github.com,2024-04-26:/mentalDetector/Quansheng_UV-K5_PCB_R51-V1.4_PCB_Reversing_Rev._0.9</id>
    <link href="https://github.com/mentalDetector/Quansheng_UV-K5_PCB_R51-V1.4_PCB_Reversing_Rev._0.9" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Reverse engineering of the Quansheng UV-K5 V1.4 PCB in KiCad 7&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Quansheng UV-K5 Reverse Engineering Project&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This project aims to reverse-engineer the hardware design of the Quansheng UV-K5 handheld amateur radio transceiver, specifically focusing on V1.4 of the PCB, which is also used in UV-K5(8) and similar models. The goal was to create a KiCad design that closely resembles the original hardware. Feel free to use this KiCad project to explore more about the mentioned radio or as a starting point for enhancing Quansheng&#39;s original design, such as improving RF filtering.&lt;/p&gt; &#xA;&lt;h2&gt;Thanks!&lt;/h2&gt; &#xA;&lt;p&gt;This project is a community effort, and many people have contributed in various ways (e.g., suggesting make and model of previously unidentified hardware components, component measurements, 3D model creation, reviews, hardware donations, etc.). I particularly want to thank Manuel (&lt;a href=&#34;https://github.com/manujedi&#34;&gt;Manuel&#39;s GitHub&lt;/a&gt;) for donating the PCB and measuring components, and Ludwich (&lt;a href=&#34;https://github.com/ludwich66&#34;&gt;Ludwich&#39;s GitHub&lt;/a&gt;) for thorough documentation (including maintaining the wiki) and improving my initial schematic.&lt;/p&gt; &#xA;&lt;h2&gt;The Process&lt;/h2&gt; &#xA;&lt;p&gt;Manuel desoldered all components from his PCB after his radio malfunctioned and began measuring them. For components he couldn&#39;t measure, he sent them along with his PCB to me.&lt;/p&gt; &#xA;&lt;p&gt;I measured the remaining components using a nanoVNA, soldering each to a test board to ensure precise measurements.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/mentalDetector/Quansheng_UV-K5_PCB_R51-V1.4_PCB_Reversing_Rev._0.9/raw/main/images/vna_measurement.png?raw=true&#34; width=&#34;1024&#34;&gt; &#xA;&lt;p&gt;All documentation is available here: &lt;a href=&#34;https://github.com/ludwich66/Quansheng_UV-K5_Wiki/wiki/Parts---Teile&#34;&gt;Parts&lt;/a&gt;. This resource was invaluable during the reverse engineering process.&lt;/p&gt; &#xA;&lt;p&gt;While we might have encountered measurement errors and may not have chosen the correct component values every time, overall, I believe we&#39;ve done a decent job. If you spot an error, please inform Ludwich or me.&lt;/p&gt; &#xA;&lt;p&gt;Afterward, I photographed the PCB to obtain high-resolution images. I also sanded down the PCB to expose the two inner layers, documenting the process here: &lt;a href=&#34;https://github.com/mentalDetector/Quansheng_UV-K5_PCB_R51-V1.4&#34;&gt;PCB Layer Preparation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/mentalDetector/Quansheng_UV-K5_PCB_R51-V1.4_PCB_Reversing_Rev._0.9/raw/main/images/Quansheng_UV-K5_PCB_R51-V1.4_manual_sanding_process.jpeg?raw=true&#34; width=&#34;512&#34;&gt; &#xA;&lt;p&gt;I then processed the layer pictures, imported them into the KiCad PCB editor, and aligned everything as accurately as possible. You can still find these pictures in the PCB editor for reference, contributing to the project&#39;s size (approximately 60 MB).&lt;/p&gt; &#xA;&lt;p&gt;Next came the laborious tasks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Setting up the PCB view with all PCB borders&lt;/li&gt; &#xA; &lt;li&gt;Selecting a random component on the PCB picture&lt;/li&gt; &#xA; &lt;li&gt;Placing it onto the schematic&lt;/li&gt; &#xA; &lt;li&gt;Switching to the PCB editor and placing the component&lt;/li&gt; &#xA; &lt;li&gt;Checking the 3D view of the PCB&lt;/li&gt; &#xA; &lt;li&gt;Linking a 3D model for the component if necessary (a time-consuming task)&lt;/li&gt; &#xA; &lt;li&gt;Placing additional components&lt;/li&gt; &#xA; &lt;li&gt;Connecting the components in the schematics editor&lt;/li&gt; &#xA; &lt;li&gt;Switching to the PCB view, updating it&lt;/li&gt; &#xA; &lt;li&gt;Running DRC on the schematics and PCB and correcting gating issues&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All of my work was completed over a period of about three months, mostly on weekends.&lt;/p&gt; &#xA;&lt;h2&gt;The Results&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/mentalDetector/Quansheng_UV-K5_PCB_R51-V1.4_PCB_Reversing_Rev._0.9/raw/main/images/Quansheng_UV-K5_PCB_R51-V1.4_Rev_0.9_Schematic.pdf?raw=true&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/mentalDetector/Quansheng_UV-K5_PCB_R51-V1.4_PCB_Reversing_Rev._0.9/raw/main/images/schematic.png?raw=true&#34; width=&#34;1024&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://github.com/mentalDetector/Quansheng_UV-K5_PCB_R51-V1.4_PCB_Reversing_Rev._0.9/raw/main/images/Quansheng_UV-K5_PCB_R51-V1.4_Rev_0.9_Layers.pdf?raw=true&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/mentalDetector/Quansheng_UV-K5_PCB_R51-V1.4_PCB_Reversing_Rev._0.9/raw/main/images/pcb.png?raw=true&#34; width=&#34;1024&#34;&gt; &lt;/a&gt; &#xA;&lt;img src=&#34;https://github.com/mentalDetector/Quansheng_UV-K5_PCB_R51-V1.4_PCB_Reversing_Rev._0.9/raw/main/images/3D_pcb_front.png?raw=true&#34; width=&#34;1024&#34;&gt; &#xA;&lt;img src=&#34;https://github.com/mentalDetector/Quansheng_UV-K5_PCB_R51-V1.4_PCB_Reversing_Rev._0.9/raw/main/images/3D_pcb_back.png?raw=true&#34; width=&#34;1024&#34;&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Why is this design labeled as Rev. 0.9?&lt;/strong&gt;&lt;br&gt; A: I&#39;m not completely satisfied with the current schematics&#39; appearance and plan to transition the project to KiCad 8. Consider this a version for public review, and please provide feedback for improvement.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Why wasn&#39;t the project set up in KiCad 8 from the beginning?&lt;/strong&gt;&lt;br&gt; A: KiCad 8 wasn&#39;t available when I started the reverse engineering, and I didn&#39;t want to switch mid-way.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: The 3D rendering of the PCB is incomplete.&lt;/strong&gt;&lt;br&gt; A: Some components lack proper 3D models or have incorrect ones from similar components, causing them to be missing in the PCB&#39;s 3D view. If you have .wrl or .stl files to contribute, please let me know.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Why are there many errors when I run DRC on the PCB?&lt;/strong&gt;&lt;br&gt; A: This project aims to understand the Quansheng UV-K5 radio&#39;s hardware design, not to manufacture PCBs directly. You can&#39;t use the PCB files for manufacturing without modifications to meet specific manufacturer rules.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Some PCB tracks aren&#39;t routed correctly on the grid.&lt;/strong&gt;&lt;br&gt; A: Yes, I&#39;m aware. It was a compromise to complete the project in less time. However, all connections should be accurate.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Why don&#39;t the reference designators in this project line up with those in Ludwich&#39;s wiki?&lt;/strong&gt;&lt;br&gt; A: I chose not to align them to save time.&lt;/p&gt; &#xA;&lt;h2&gt;Rev 0.9 Errata&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;C192 in the schematics has a wrong value.&lt;br&gt; Wrong: 10 nF&lt;br&gt; Correct: 20 pF&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>