<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-01T01:28:40Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/RD-Agent</title>
    <updated>2024-10-01T01:28:40Z</updated>
    <id>tag:github.com,2024-10-01:/microsoft/RD-Agent</id>
    <link href="https://github.com/microsoft/RD-Agent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Research and development (R&amp;D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&amp;D are mainly focused on data and models. We are committed to automate these high-value generic R&amp;D processes through our open source R&amp;D automation tool RD-Agent, which let AI drive data-driven AI.&lt;/p&gt;&lt;hr&gt;&lt;h4 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/logo.png&#34; alt=&#34;RA-Agent logo&#34; style=&#34;width:70%; &#34;&gt; &lt;p&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net&#34; target=&#34;_blank&#34;&gt;üñ•Ô∏è Live Demo&lt;/a&gt; | &lt;a href=&#34;https://rdagent.azurewebsites.net/factor_loop&#34; target=&#34;_blank&#34;&gt;üé• Demo Video&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=JJ4JYO3HscM&amp;amp;list=PLALmKB0_N3_i52fhUmPQiL4jsO354uopR&#34; target=&#34;_blank&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt; | &lt;a href=&#34;https://rdagent.readthedocs.io/en/latest/index.html&#34; target=&#34;_blank&#34;&gt;üìñ Documentation&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/#-paperwork-list&#34;&gt; üìÉ Papers &lt;/a&gt;&lt;/p&gt; &lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql/badge.svg?sanitize=true&#34; alt=&#34;CodeQL&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates/badge.svg?sanitize=true&#34; alt=&#34;Dependabot Updates&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml/badge.svg?sanitize=true&#34; alt=&#34;Lint PR Title&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/release.yml&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/release.yml/badge.svg?sanitize=true&#34; alt=&#34;Release.yml&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/rdagent/#files&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/platform-Linux-blue&#34; alt=&#34;Platform&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/rdagent/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/rdagent&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/rdagent/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/rdagent&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/RD-Agent/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/microsoft/RD-Agent&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/RD-Agent/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/microsoft/RD-Agent&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pre-commit/pre-commit&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&#34; alt=&#34;pre-commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://mypy-lang.org/&#34;&gt;&lt;img src=&#34;https://www.mypy-lang.org/static/mypy_badge.svg?sanitize=true&#34; alt=&#34;Checked with mypy&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/astral-sh/ruff&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&#34; alt=&#34;Ruff&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/ybQ97B6Jjy&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat-discord-blue&#34; alt=&#34;Chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml/badge.svg?sanitize=true&#34; alt=&#34;Readthedocs Preview&#34;&gt;&lt;/a&gt; &#xA; &lt;!-- this badge is too long, please place it in the last one to make it pretty --&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üì∞ News&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;üóûÔ∏è News&lt;/th&gt; &#xA;   &lt;th&gt;üìù Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Official WeChat group release&lt;/td&gt; &#xA;   &lt;td&gt;We created a WeChat group, welcome to join! (üó™&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/WeChat_QR_code.jpg&#34;&gt;QR Code&lt;/a&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Official Discord release&lt;/td&gt; &#xA;   &lt;td&gt;We launch our first chatting channel in Discord (üó™&lt;a href=&#34;https://discord.gg/ybQ97B6Jjy&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat-discord-blue&#34; alt=&#34;Chat&#34;&gt;&lt;/a&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;First release&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;RDAgent&lt;/strong&gt; is released on GitHub&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;üåü Introduction&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/scen.png&#34; alt=&#34;Our focused scenario&#34; style=&#34;width:80%; &#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;RDAgent aims to automate the most critical and valuable aspects of the industrial R&amp;amp;D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data. Methodologically, we have identified a framework with two key components: &#39;R&#39; for proposing new ideas and &#39;D&#39; for implementing them. We believe that the automatic evolution of R&amp;amp;D will lead to solutions of significant industrial value.&lt;/p&gt; &#xA;&lt;!-- Tag Cloud --&gt; &#xA;&lt;p&gt;R&amp;amp;D is a very general scenario. The advent of RDAgent can be your&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üí∞ &lt;strong&gt;Automatic Quant Factory&lt;/strong&gt; (&lt;a href=&#34;https://rdagent.azurewebsites.net/factor_loop&#34;&gt;üé•Demo Video&lt;/a&gt;|&lt;a href=&#34;https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;amp;t=6s&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ &lt;strong&gt;Data Mining Agent:&lt;/strong&gt; Iteratively proposing data &amp;amp; models (&lt;a href=&#34;https://rdagent.azurewebsites.net/model_loop&#34;&gt;üé•Demo Video 1&lt;/a&gt;|&lt;a href=&#34;https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;amp;t=104s&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;) (&lt;a href=&#34;https://rdagent.azurewebsites.net/dmm&#34;&gt;üé•Demo Video 2&lt;/a&gt;|&lt;a href=&#34;https://www.youtube.com/watch?v=VIaSTZuoZg4&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;) and implementing them by gaining knowledge from data.&lt;/li&gt; &#xA; &lt;li&gt;ü¶æ &lt;strong&gt;Research Copilot:&lt;/strong&gt; Auto read research papers (&lt;a href=&#34;https://rdagent.azurewebsites.net/report_model&#34;&gt;üé•Demo Video&lt;/a&gt;|&lt;a href=&#34;https://www.youtube.com/watch?v=BiA2SfdKQ7o&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;) / financial reports (&lt;a href=&#34;https://rdagent.azurewebsites.net/report_factor&#34;&gt;üé•Demo Video&lt;/a&gt;|&lt;a href=&#34;https://www.youtube.com/watch?v=ECLTXVcSx-c&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;) and implement model structures or building datasets.&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can click the links above to view the demo. We&#39;re continuously adding more methods and scenarios to the project to enhance your R&amp;amp;D processes and boost productivity.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, you can take a closer look at the examples in our &lt;strong&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/&#34;&gt;üñ•Ô∏è Live Demo&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://rdagent.azurewebsites.net/&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/demo.png&#34; alt=&#34;Watch the demo&#34; width=&#34;80%&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;‚ö° Quick start&lt;/h1&gt; &#xA;&lt;p&gt;You can try above demos by running the following command:&lt;/p&gt; &#xA;&lt;h3&gt;üê≥ Docker installation.&lt;/h3&gt; &#xA;&lt;p&gt;Users must ensure Docker is installed before attempting most scenarios. Please refer to the &lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;official üê≥Docker page&lt;/a&gt; for installation instructions.&lt;/p&gt; &#xA;&lt;h3&gt;üêç Create a Conda Environment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI): &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda create -n rdagent python=3.10&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Activate the environment: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda activate rdagent&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üõ†Ô∏è Install the RDAgent&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can directly install the RDAgent package from PyPI: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install rdagent&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;‚öôÔ∏è Configuration&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You have to config your GPT model in the &lt;code&gt;.env&lt;/code&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env&#xA;OPENAI_API_KEY=&amp;lt;your_api_key&amp;gt;&#xA;# EMBEDDING_MODEL=text-embedding-3-small&#xA;CHAT_MODEL=gpt-4-turbo&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üöÄ Run the Application&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;strong&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/&#34;&gt;üñ•Ô∏è Live Demo&lt;/a&gt;&lt;/strong&gt; is implemented by the following commands(each item represents one demo, you can select the one you prefer):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Factors Evolution&lt;/strong&gt;: &lt;a href=&#34;http://github.com/microsoft/qlib&#34;&gt;Qlib&lt;/a&gt; self-loop factor proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;rdagent fin_factor&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Model Evolution&lt;/strong&gt;: &lt;a href=&#34;http://github.com/microsoft/qlib&#34;&gt;Qlib&lt;/a&gt; self-loop model proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;rdagent fin_model&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Medical Prediction Model Evolution&lt;/strong&gt;: Medical self-loop model proposal and implementation application&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;(1) Apply for an account at &lt;a href=&#34;https://physionet.org/&#34;&gt;PhysioNet&lt;/a&gt;. &lt;br&gt; (2) Request access to FIDDLE preprocessed data: &lt;a href=&#34;https://physionet.org/content/mimic-eicu-fiddle-feature/1.0.0/&#34;&gt;FIDDLE Dataset&lt;/a&gt;. &lt;br&gt; (3) Place your username and password in &lt;code&gt;.env&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt;&amp;gt; .env&#xA;DM_USERNAME=&amp;lt;your_username&amp;gt;&#xA;DM_PASSWORD=&amp;lt;your_password&amp;gt;&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;rdagent med_model&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Factors Extraction from Financial Reports&lt;/strong&gt;: Run the &lt;a href=&#34;http://github.com/microsoft/qlib&#34;&gt;Qlib&lt;/a&gt; factor extraction and implementation application based on financial reports&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 1. Generally, you can run this scenario using the following command:&#xA;rdagent fin_factor_report --report_folder=&amp;lt;Your financial reports folder path&amp;gt;&#xA;&#xA;# 2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:&#xA;wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip&#xA;unzip all_reports.zip -d git_ignore_folder/reports&#xA;rdagent fin_factor_report --report_folder=git_ignore_folder/reports&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Model Research &amp;amp; Development Copilot&lt;/strong&gt;: model extraction and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 1. Generally, you can run your own papers/reports with the following command:&#xA;rdagent general_model &amp;lt;Your paper URL&amp;gt;&#xA;&#xA;# 2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:&#xA;rdagent general_model  &#34;https://arxiv.org/pdf/2210.09789&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üñ•Ô∏è Monitor the Application Results&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can serve our demo app to monitor the RD loop by running the following command: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;rdagent ui --port 80 --log_dir &amp;lt;your log folder like &#34;log/&#34;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üè≠ Scenarios&lt;/h1&gt; &#xA;&lt;p&gt;We have applied RD-Agent to multiple valuable data-driven industrial scenarios.&lt;/p&gt; &#xA;&lt;h2&gt;üéØ Goal: Agent for Data-driven R&amp;amp;D&lt;/h2&gt; &#xA;&lt;p&gt;In this project, we are aiming to build an Agent to automate Data-Driven R&amp;amp;D that can&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìÑ Read real-world material (reports, papers, etc.) and &lt;strong&gt;extract&lt;/strong&gt; key formulas, descriptions of interested &lt;strong&gt;features&lt;/strong&gt; and &lt;strong&gt;models&lt;/strong&gt;, which are the key components of data-driven R&amp;amp;D .&lt;/li&gt; &#xA; &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Implement&lt;/strong&gt; the extracted formulas (e.g., features, factors, and models) in runnable codes. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Due to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;üí° Propose &lt;strong&gt;new ideas&lt;/strong&gt; based on current knowledge and observations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- ![Data-Centric R&amp;D Overview](docs/_static/overview.png) --&gt; &#xA;&lt;h2&gt;üìà Scenarios/Demos&lt;/h2&gt; &#xA;&lt;p&gt;In the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: ü¶æCopilot and ü§ñAgent.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The ü¶æCopilot follows human instructions to automate repetitive tasks.&lt;/li&gt; &#xA; &lt;li&gt;The ü§ñAgent, being more autonomous, actively proposes ideas for better results in the future.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The supported scenarios are listed below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Scenario/Target&lt;/th&gt; &#xA;   &lt;th&gt;Model Implementation&lt;/th&gt; &#xA;   &lt;th&gt;Data Building&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üíπ Finance&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü§ñ &lt;a href=&#34;https://rdagent.azurewebsites.net/model_loop&#34;&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;amp;t=104s&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü§ñ &lt;a href=&#34;https://rdagent.azurewebsites.net/factor_loop&#34;&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;amp;t=6s&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt; &lt;br&gt; ü¶æ &lt;a href=&#34;https://rdagent.azurewebsites.net/report_factor&#34;&gt;Auto reports reading &amp;amp; implementation&lt;/a&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ECLTXVcSx-c&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;ü©∫ Medical&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü§ñ &lt;a href=&#34;https://rdagent.azurewebsites.net/dmm&#34;&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VIaSTZuoZg4&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üè≠ General&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü¶æ &lt;a href=&#34;https://rdagent.azurewebsites.net/report_model&#34;&gt;Auto paper reading &amp;amp; implementation&lt;/a&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=BiA2SfdKQ7o&#34;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Different scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.&lt;/p&gt; &#xA;&lt;p&gt;Here is a gallery of &lt;a href=&#34;https://github.com/SunsetWolf/rdagent_resource/releases/download/demo_traces/demo_traces.zip&#34;&gt;successful explorations&lt;/a&gt; (5 traces showed in &lt;strong&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net/&#34;&gt;üñ•Ô∏è Live Demo&lt;/a&gt;&lt;/strong&gt;). You can download and view the execution trace using the command below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rdagent ui --port 80 --log_dir ./demo_traces&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;strong&gt;&lt;a href=&#34;https://rdagent.readthedocs.io/en/latest/scens/catalog.html&#34;&gt;üìñreadthedocs_scen&lt;/a&gt;&lt;/strong&gt; for more details of the scenarios.&lt;/p&gt; &#xA;&lt;h1&gt;‚öôÔ∏è Framework&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/Framework-RDAgent.png&#34; alt=&#34;Framework-RDAgent&#34; width=&#34;85%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Automating the R&amp;amp;D process in data science is a highly valuable yet underexplored area in industry. We propose a framework to push the boundaries of this important research field.&lt;/p&gt; &#xA;&lt;p&gt;The research questions within this framework can be divided into three main categories:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Research Area&lt;/th&gt; &#xA;   &lt;th&gt;Paper/Work List&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Benchmark the R&amp;amp;D abilities&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/#benchmark&#34;&gt;Benchmark&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Idea proposal:&lt;/strong&gt; Explore new ideas or refine existing ones&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/#research&#34;&gt;Research&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Ability to realize ideas:&lt;/strong&gt; Implement and execute ideas&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/RD-Agent/main/#development&#34;&gt;Development&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We believe that the key to delivering high-quality solutions lies in the ability to evolve R&amp;amp;D capabilities. Agents should learn like human experts, continuously improving their R&amp;amp;D skills.&lt;/p&gt; &#xA;&lt;p&gt;More documents can be found in the &lt;strong&gt;&lt;a href=&#34;https://rdagent.readthedocs.io/&#34;&gt;üìñ readthedocs&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;üìÉ Paper/Work list&lt;/h1&gt; &#xA;&lt;h2&gt;üìä Benchmark&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.11276&#34;&gt;Towards Data-Centric Automatic R&amp;amp;D&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{chen2024datacentric,&#xA;    title={Towards Data-Centric Automatic R&amp;amp;D},&#xA;    author={Haotian Chen and Xinjie Shen and Zeqi Ye and Wenjun Feng and Haoxue Wang and Xiao Yang and Xu Yang and Weiqing Liu and Jiang Bian},&#xA;    year={2024},&#xA;    eprint={2404.11276},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/494f55d3-de9e-4e73-ba3d-a787e8f9e841&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üîç Research&lt;/h2&gt; &#xA;&lt;p&gt;In a data mining expert&#39;s daily research and development process, they propose a hypothesis (e.g., a model structure like RNN can capture patterns in time-series data), design experiments (e.g., finance data contains time-series and we can verify the hypothesis in this scenario), implement the experiment as code (e.g., Pytorch model structure), and then execute the code to get feedback (e.g., metrics, loss curve, etc.). The experts learn from the feedback and improve in the next iteration.&lt;/p&gt; &#xA;&lt;p&gt;Based on the principles above, we have established a basic method framework that continuously proposes hypotheses, verifies them, and gets feedback from the real-world practice. This is the first scientific research automation framework that supports linking with real-world verification.&lt;/p&gt; &#xA;&lt;p&gt;For more detail, please refer to our &lt;strong&gt;&lt;a href=&#34;https://rdagent.azurewebsites.net&#34;&gt;üñ•Ô∏è Live Demo page&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Development&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.18690&#34;&gt;Collaborative Evolving Strategy for Automatic Data-Centric Development&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{yang2024collaborative,&#xA;    title={Collaborative Evolving Strategy for Automatic Data-Centric Development},&#xA;    author={Xu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian},&#xA;    year={2024},&#xA;    eprint={2407.18690},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/75d9769b-0edd-4caf-9d45-57d1e577054b&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ü§ù Contributing&lt;/h1&gt; &#xA;&lt;h2&gt;üìù Guidelines&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Contributing to this project is straightforward and rewarding. Whether it&#39;s solving an issue, addressing a bug, enhancing documentation, or even correcting a typo, every contribution is valuable and helps improve RDAgent.&lt;/p&gt; &#xA;&lt;p&gt;To get started, you can explore the issues list, or search for &lt;code&gt;TODO:&lt;/code&gt; comments in the codebase by running the command &lt;code&gt;grep -r &#34;TODO:&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://img.shields.io/github/contributors-anon/microsoft/RD-Agent&#34;&gt; &#xA;&lt;a href=&#34;https://github.com/microsoft/RD-Agent/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=microsoft/RD-Agent&amp;amp;max=100&amp;amp;columns=15&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Before we released RD-Agent as an open-source project on GitHub, it was an internal project within our group. Unfortunately, the internal commit history was not preserved when we removed some confidential code. As a result, some contributions from our group members, including Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, and Jinhui Li, were not included in the public commits.&lt;/p&gt; &#xA;&lt;h1&gt;‚öñÔ∏è Legal disclaimer&lt;/h1&gt; &#xA;&lt;p style=&#34;line-height: 1; font-style: italic;&#34;&gt;The RD-agent is provided ‚Äúas is‚Äù, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. The RD-agent is aimed to facilitate research and development process in the financial industry and not ready-to-use for any financial investment or advice. Users shall independently assess and test the risks of the RD-agent in a specific use scenario, ensure the responsible use of AI technology, including but not limited to developing and integrating risk mitigation measures, and comply with all applicable laws and regulations in all applicable jurisdictions. The RD-agent does not provide financial opinions or reflect the opinions of Microsoft, nor is it designed to replace the role of qualified financial professionals in formulating, assessing, and approving finance products. The inputs and outputs of the RD-agent belong to the users and users shall assume all liability under any theory of liability, whether in contract, torts, regulatory, negligence, products liability, or otherwise, associated with use of the RD-agent and any inputs and outputs thereof.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Hannibal046/Awesome-LLM</title>
    <updated>2024-10-01T01:28:40Z</updated>
    <id>tag:github.com,2024-10-01:/Hannibal046/Awesome-LLM</id>
    <link href="https://github.com/Hannibal046/Awesome-LLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Awesome-LLM: a curated list of Large Language Model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome-LLM &lt;a href=&#34;https://awesome.re&#34;&gt;&lt;img src=&#34;https://awesome.re/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/resources/image8.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;üî• Large Language Models(LLM) have taken the &lt;del&gt;NLP community&lt;/del&gt; &lt;del&gt;AI community&lt;/del&gt; &lt;strong&gt;the Whole World&lt;/strong&gt; by storm. Here is a curated list of papers about large language models, especially relating to ChatGPT. It also contains frameworks for LLM training, tools to deploy LLM, courses and tutorials about LLM and all publicly available LLM checkpoints and APIs.&lt;/p&gt; &#xA;&lt;h2&gt;Trending LLM Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hacksider/Deep-Live-Cam&#34;&gt;Deep-Live-Cam&lt;/a&gt; - real time face swap and one-click video deepfake with only a single image (uncensored).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenBMB/MiniCPM-V&#34;&gt;MiniCPM-V 2.6&lt;/a&gt; - A GPT-4V Level MLLM for Single Image, Multi Image and Video on Your Phone&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;GPT-SoVITS&lt;/a&gt; - 1 min voice data can also be used to train a good TTS model! (few shot voice cloning).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Content&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/#awesome-llm-&#34;&gt;Awesome-LLM &lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/#milestone-papers&#34;&gt;Milestone Papers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/#other-papers&#34;&gt;Other Papers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/#llm-leaderboard&#34;&gt;LLM Leaderboard&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/#open-llm&#34;&gt;Open LLM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/#llm-data&#34;&gt;LLM Data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/#llm-evaluation&#34;&gt;LLM Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/#llm-training-frameworks&#34;&gt;LLM Training Framework&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/#llm-deployment&#34;&gt;LLM Deployment&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/#llm-applications&#34;&gt;LLM Applications&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/#llm-books&#34;&gt;LLM Books&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/#great-thoughts-about-llm&#34;&gt;Great thoughts about LLM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/#miscellaneous&#34;&gt;Miscellaneous&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Milestone Papers&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;keywords&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Institute&lt;/th&gt; &#xA;   &lt;th&gt;Paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2017-06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Transformers&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;Attention Is All You Need&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2018-06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPT 1.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf&#34;&gt;Improving Language Understanding by Generative Pre-Training&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2018-10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BERT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/N19-1423.pdf&#34;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2019-02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPT 2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;Language Models are Unsupervised Multitask Learners&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2019-09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Megatron-LM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NVIDIA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1909.08053.pdf&#34;&gt;Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2019-10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;T5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://jmlr.org/papers/v21/20-074.html&#34;&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2019-10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ZeRO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Microsoft&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.02054.pdf&#34;&gt;ZeRO: Memory Optimizations Toward Training Trillion Parameter Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2020-01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scaling Law&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2001.08361.pdf&#34;&gt;Scaling Laws for Neural Language Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2020-05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPT 3.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf&#34;&gt;Language models are few-shot learners&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2021-01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Switch Transformers&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2101.03961.pdf&#34;&gt;Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2021-08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Codex&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2107.03374.pdf&#34;&gt;Evaluating Large Language Models Trained on Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2021-08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Foundation Models&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stanford&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2108.07258.pdf&#34;&gt;On the Opportunities and Risks of Foundation Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2021-09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FLAN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openreview.net/forum?id=gEZrGCozdqR&#34;&gt;Finetuned Language Models are Zero-Shot Learners&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2021-10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;T0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;HuggingFace et al.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.08207&#34;&gt;Multitask Prompted Training Enables Zero-Shot Task Generalization&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2021-12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GLaM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2112.06905.pdf&#34;&gt;GLaM: Efficient Scaling of Language Models with Mixture-of-Experts&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2021-12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WebGPT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.semanticscholar.org/paper/WebGPT%3A-Browser-assisted-question-answering-with-Nakano-Hilton/2f3efe44083af91cef562c1a3451eee2f8601d22&#34;&gt;WebGPT: Browser-assisted question-answering with human feedback&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2021-12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Retro&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepMind&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens&#34;&gt;Improving language models by retrieving from trillions of tokens&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2021-12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Gopher&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepMind&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2112.11446.pdf&#34;&gt;Scaling Language Models: Methods, Analysis &amp;amp; Insights from Training Gopher&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2201.11903.pdf&#34;&gt;Chain-of-Thought Prompting Elicits Reasoning in Large Language Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LaMDA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2201.08239.pdf&#34;&gt;LaMDA: Language Models for Dialog Applications&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Minerva&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.14858&#34;&gt;Solving Quantitative Reasoning Problems with Language Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Megatron-Turing NLG&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Microsoft&amp;amp;NVIDIA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2201.11990.pdf&#34;&gt;Using Deep and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;InstructGPT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.02155.pdf&#34;&gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PaLM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2204.02311.pdf&#34;&gt;PaLM: Scaling Language Modeling with Pathways&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chinchilla&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepMind&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2408.00724&#34;&gt;An empirical analysis of compute-optimal large language model training&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OPT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meta&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2205.01068.pdf&#34;&gt;OPT: Open Pre-trained Transformer Language Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UL2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.05131v1&#34;&gt;Unifying Language Learning Paradigms&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Emergent Abilities&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openreview.net/pdf?id=yzkSU5zdwD&#34;&gt;Emergent Abilities of Large Language Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BIG-bench&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google/BIG-bench&#34;&gt;Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;METALM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Microsoft&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2206.06336.pdf&#34;&gt;Language Models are General-Purpose Interfaces&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Sparrow&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepMind&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2209.14375.pdf&#34;&gt;Improving alignment of dialogue agents via targeted human judgements&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Flan-T5/PaLM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2210.11416.pdf&#34;&gt;Scaling Instruction-Finetuned Language Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GLM-130B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Tsinghua&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2210.02414.pdf&#34;&gt;GLM-130B: An Open Bilingual Pre-trained Model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;HELM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stanford&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2211.09110.pdf&#34;&gt;Holistic Evaluation of Language Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BLOOM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BigScience&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2211.05100.pdf&#34;&gt;BLOOM: A 176B-Parameter Open-Access Multilingual Language Model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Galactica&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meta&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2211.09085.pdf&#34;&gt;Galactica: A Large Language Model for Science&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OPT-IML&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meta&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2212.12017&#34;&gt;OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Flan 2022 Collection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2301.13688.pdf&#34;&gt;The Flan Collection: Designing Data and Methods for Effective Instruction Tuning&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaMA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meta&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/&#34;&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Kosmos-1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Microsoft&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.14045&#34;&gt;Language Is Not All You Need: Aligning Perception with Language Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LRU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepMind&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.06349&#34;&gt;Resurrecting Recurrent Neural Networks for Long Sequences&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PaLM-E&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://palm-e.github.io&#34;&gt;PaLM-E: An Embodied Multimodal Language Model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPT 4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openai.com/research/gpt-4&#34;&gt;GPT-4 Technical Report&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaVA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UW‚ÄìMadison&amp;amp;Microsoft&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;Visual Instruction Tuning&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Pythia&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EleutherAI et al.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.01373&#34;&gt;Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dromedary&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CMU et al.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.03047&#34;&gt;Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PaLM 2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.google/static/documents/palm2techreport.pdf&#34;&gt;PaLM 2 Technical Report&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RWKV&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Bo Peng&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.13048&#34;&gt;RWKV: Reinventing RNNs for the Transformer Era&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DPO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stanford&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.18290.pdf&#34;&gt;Direct Preference Optimization: Your Language Model is Secretly a Reward Model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ToT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google&amp;amp;Princeton&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.10601.pdf&#34;&gt;Tree of Thoughts: Deliberate Problem Solving with Large Language Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaMA2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meta&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2307.09288.pdf&#34;&gt;Llama 2: Open Foundation and Fine-Tuned Chat Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Mistral 7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Mistral&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.06825.pdf&#34;&gt;Mistral 7B&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Mamba&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CMU&amp;amp;Princeton&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2312.00752&#34;&gt;Mamba: Linear-Time Sequence Modeling with Selective State Spaces&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2024-01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepSeek-v2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepSeek&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.04434&#34;&gt;DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2024-03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Jamba&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AI21 Labs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2403.19887&#34;&gt;Jamba: A Hybrid Transformer-Mamba Language Model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2024-05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Mamba2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CMU&amp;amp;Princeton&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.21060&#34;&gt;Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2024-05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meta&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.21783&#34;&gt;The Llama 3 Herd of Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Other Papers&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re interested in the field of LLM, you may find the above list of milestone papers helpful to explore its history and state-of-the-art. However, each direction of LLM offers a unique set of insights and contributions, which are essential to understanding the field as a whole. For a detailed list of papers in various subfields, please refer to the following link:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/LuckyyySTA/Awesome-LLM-hallucination&#34;&gt;Awesome-LLM-hallucination&lt;/a&gt; - LLM hallucination paper list.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/EdinburghNLP/awesome-hallucination-detection&#34;&gt;awesome-hallucination-detection&lt;/a&gt; - List of papers on hallucination detection in LLMs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Mooler0410/LLMsPracticalGuide&#34;&gt;LLMsPracticalGuide&lt;/a&gt; - A curated list of practical guide resources of LLMs&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/f/awesome-chatgpt-prompts&#34;&gt;Awesome ChatGPT Prompts&lt;/a&gt; - A collection of prompt examples to be used with the ChatGPT model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/PlexPt/awesome-chatgpt-prompts-zh&#34;&gt;awesome-chatgpt-prompts-zh&lt;/a&gt; - A Chinese collection of prompt examples to be used with the ChatGPT model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/humanloop/awesome-chatgpt&#34;&gt;Awesome ChatGPT&lt;/a&gt; - Curated list of resources for ChatGPT and GPT-3 from OpenAI.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Timothyxxx/Chain-of-ThoughtsPapers&#34;&gt;Chain-of-Thoughts Papers&lt;/a&gt; - A trend starts from &#34;Chain of Thought Prompting Elicits Reasoning in Large Language Models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/logikon-ai/awesome-deliberative-prompting&#34;&gt;Awesome Deliberative Prompting&lt;/a&gt; - How to ask LLMs to produce reliable reasoning and make reason-responsive decisions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/SinclairCoder/Instruction-Tuning-Papers&#34;&gt;Instruction-Tuning-Papers&lt;/a&gt; - A trend starts from &lt;code&gt;Natrural-Instruction&lt;/code&gt; (ACL 2022), &lt;code&gt;FLAN&lt;/code&gt; (ICLR 2022) and &lt;code&gt;T0&lt;/code&gt; (ICLR 2022).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/crazyofapple/Reading_groups/&#34;&gt;LLM Reading List&lt;/a&gt; - A paper &amp;amp; resource list of large language models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/atfortes/LM-Reasoning-Papers&#34;&gt;Reasoning using Language Models&lt;/a&gt; - Collection of papers and resources on Reasoning using Language Models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/FranxYao/chain-of-thought-hub&#34;&gt;Chain-of-Thought Hub&lt;/a&gt; - Measuring LLMs&#39; Reasoning Performance&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/formulahendry/awesome-gpt&#34;&gt;Awesome GPT&lt;/a&gt; - A curated list of awesome projects and resources related to GPT, ChatGPT, OpenAI, LLM, and more.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/elyase/awesome-gpt3&#34;&gt;Awesome GPT-3&lt;/a&gt; - a collection of demos and articles about the &lt;a href=&#34;https://openai.com/blog/openai-api/&#34;&gt;OpenAI GPT-3 API&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/PolisAI/awesome-llm-human-preference-datasets&#34;&gt;Awesome LLM Human Preference Datasets&lt;/a&gt; - a collection of human preference datasets for LLM instruction tuning, RLHF and evaluation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Hannibal046/RWKV-howto&#34;&gt;RWKV-howto&lt;/a&gt; - possibly useful materials and tutorial for learning RWKV.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/zjunlp/ModelEditingPapers&#34;&gt;ModelEditingPapers&lt;/a&gt; - A paper &amp;amp; resource list on model editing for large language models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/corca-ai/awesome-llm-security&#34;&gt;Awesome LLM Security&lt;/a&gt; - A curation of awesome tools, documents and projects about LLM Security.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/GaryYufei/AlignLLMHumanSurvey&#34;&gt;Awesome-Align-LLM-Human&lt;/a&gt; - A collection of papers and resources about aligning large language models (LLMs) with human.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/huybery/Awesome-Code-LLM&#34;&gt;Awesome-Code-LLM&lt;/a&gt; - An awesome and curated list of best code-LLM for research.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/HuangOwen/Awesome-LLM-Compression&#34;&gt;Awesome-LLM-Compression&lt;/a&gt; - Awesome LLM compression research papers and tools.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/AmberLJC/LLMSys-PaperList&#34;&gt;Awesome-LLM-Systems&lt;/a&gt; - Awesome LLM systems research papers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/snowfort-ai/awesome-llm-webapps&#34;&gt;awesome-llm-webapps&lt;/a&gt; - A collection of open source, actively maintained web apps for LLM applications.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/llm-jp/awesome-japanese-llm&#34;&gt;awesome-japanese-llm&lt;/a&gt; - Êó•Êú¨Ë™ûLLM„Åæ„Å®„ÇÅ - Overview of Japanese LLMs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/mingze-yuan/Awesome-LLM-Healthcare&#34;&gt;Awesome-LLM-Healthcare&lt;/a&gt; - The paper list of the review on LLMs in medicine.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/DefTruth/Awesome-LLM-Inference&#34;&gt;Awesome-LLM-Inference&lt;/a&gt; - A curated list of Awesome LLM Inference Paper with codes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ActiveVisionLab/Awesome-LLM-3D&#34;&gt;Awesome-LLM-3D&lt;/a&gt; - A curated list of Multi-modal Large Language Model in 3D world, including 3D understanding, reasoning, generation, and embodied agents.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Zjh-819/LLMDataHub&#34;&gt;LLMDatahub&lt;/a&gt; - a curated collection of datasets specifically designed for chatbot training, including links, size, language, usage, and a brief description of each dataset&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/HqWu-HITCS/Awesome-Chinese-LLM&#34;&gt;Awesome-Chinese-LLM&lt;/a&gt; - Êï¥ÁêÜÂºÄÊ∫êÁöÑ‰∏≠ÊñáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºå‰ª•ËßÑÊ®°ËæÉÂ∞è„ÄÅÂèØÁßÅÊúâÂåñÈÉ®ÁΩ≤„ÄÅËÆ≠ÁªÉÊàêÊú¨ËæÉ‰ΩéÁöÑÊ®°Âûã‰∏∫‰∏ªÔºåÂåÖÊã¨Â∫ïÂ∫ßÊ®°ÂûãÔºåÂûÇÁõ¥È¢ÜÂüüÂæÆË∞ÉÂèäÂ∫îÁî®ÔºåÊï∞ÊçÆÈõÜ‰∏éÊïôÁ®ãÁ≠â„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/FeiLiu36/LLM4Opt&#34;&gt;LLM4Opt&lt;/a&gt; - Applying Large language models (LLMs) for diverse optimization tasks (Opt) is an emerging research area. This is a collection of references and papers of LLM4Opt.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Furyton/awesome-language-model-analysis&#34;&gt;awesome-language-model-analysis&lt;/a&gt; - This paper list focuses on the theoretical or empirical analysis of language models, e.g., the learning dynamics, expressive capacity, interpretability, generalization, and other interesting topics.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM Leaderboard&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard&#34;&gt;Chatbot Arena Leaderboard&lt;/a&gt; - a benchmark platform for large language models (LLMs) that features anonymous, randomized battles in a crowdsourced manner.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mixeval.github.io/#leaderboard&#34;&gt;MixEval Leaderboard&lt;/a&gt; - a ground-truth-based dynamic benchmark derived from off-the-shelf benchmark mixtures, which evaluates LLMs with a highly capable model ranking (i.e., 0.96 correlation with Chatbot Arena) while running locally and quickly (6% the time and cost of running MMLU).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tatsu-lab.github.io/alpaca_eval/&#34;&gt;AlpacaEval Leaderboard&lt;/a&gt; - An Automatic Evaluator for Instruction-following Language Models using Nous benchmark suite.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#34;&gt;Open LLM Leaderboard&lt;/a&gt; - aims to track, rank and evaluate LLMs and chatbots as they are released.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rank.opencompass.org.cn/leaderboard-llm-v2&#34;&gt;OpenCompass 2.0 LLM Leaderboard&lt;/a&gt; - OpenCompass is an LLM evaluation platform, supporting a wide range of models (InternLM2,GPT-4,LLaMa2, Qwen,GLM, Claude, etc) over 100+ datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gorilla.cs.berkeley.edu/leaderboard.html&#34;&gt;Berkeley Function-Calling Leaderboard&lt;/a&gt; - evaluates LLM&#39;s ability to call external functions / tools&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Open LLM&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Meta &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://llama.meta.com/&#34;&gt;Llama 3.1-8|70|405B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://llama.meta.com/llama3/&#34;&gt;Llama 3-8|70B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://llama.meta.com/llama2/&#34;&gt;Llama 2-7|13|70B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&#34;&gt;Llama 1-7|13|33|65B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.01068&#34;&gt;OPT-1.3|6.7|13|30|66B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Mistral AI &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mistral.ai/news/codestral/&#34;&gt;Codestral-7|22B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mistral.ai/news/announcing-mistral-7b/&#34;&gt;Mistral-7B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mistral.ai/news/mixtral-of-experts/&#34;&gt;Mixtral-8x7B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mistral.ai/news/mixtral-8x22b/&#34;&gt;Mixtral-8x22B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Google &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://blog.google/technology/developers/google-gemma-2/&#34;&gt;Gemma2-9|27B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://blog.google/technology/developers/gemma-open-models/&#34;&gt;Gemma-2|7B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/google-deepmind/recurrentgemma&#34;&gt;RecurrentGemma-2B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.10683&#34;&gt;T5&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Apple &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/apple/OpenELM&#34;&gt;OpenELM-1.1|3B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Microsoft &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/microsoft/phi-1&#34;&gt;Phi1-1.3B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/microsoft/phi-2&#34;&gt;Phi2-2.7B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/microsoft/Phi-3-mini-4k-instruct&#34;&gt;Phi3-3.8|7|14B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;AllenAI &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/allenai/olmo-suite-65aeaae8fe5b6b2122b46778&#34;&gt;OLMo-7B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;xAI &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://x.ai/blog/grok-os&#34;&gt;Grok-1-314B-MoE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Cohere &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/CohereForAI/c4ai-command-r-v01&#34;&gt;Command R-35B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DeepSeek &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/deepseek-ai/deepseek-math-65f2962739da11599e441681&#34;&gt;DeepSeek-Math-7B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/deepseek-ai/deepseek-coder-65f295d7d8a0a29fe39b4ec4&#34;&gt;DeepSeek-Coder-1.3|6.7|7|33B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/deepseek-ai/deepseek-vl-65f295948133d9cf92b706d3&#34;&gt;DeepSeek-VL-1.3|7B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/deepseek-ai/deepseek-moe-65f29679f5cf26fe063686bf&#34;&gt;DeepSeek-MoE-16B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.04434&#34;&gt;DeepSeek-v2-236B-MoE&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-Coder-V2&#34;&gt;DeepSeek-Coder-v2-16|236B-MOE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Alibaba &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/Qwen/qwen-65c0e50c3f1ab89cb8704144&#34;&gt;Qwen-1.8B|7B|14B|72B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen1.5/&#34;&gt;Qwen1.5-0.5B|1.8B|4B|7B|14B|32B|72B|110B|MoE-A2.7B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2&#34;&gt;Qwen2-0.5B|1.5B|7B|57B-A14B-MoE|72B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5/&#34;&gt;Qwen2.5-0.5B|1.5B|3B|7B|14B|32B|72B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/codeqwen1.5/&#34;&gt;CodeQwen1.5-7B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-coder/&#34;&gt;Qwen2.5-Coder-1.5B|7B|32B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2-math/&#34;&gt;Qwen2-Math-1.5B|7B|72B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-math/&#34;&gt;Qwen2.5-Math-1.5B|7B|72B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen-VL&#34;&gt;Qwen-VL-7B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2-vl/&#34;&gt;Qwen2-VL-2B|7B|72B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2-audio/&#34;&gt;Qwen2-Audio-7B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;01-ai &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/01-ai/yi-2023-11-663f3f19119ff712e176720f&#34;&gt;Yi-34B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/01-ai/yi-15-2024-05-663f3ecab5f815a3eaca7ca8&#34;&gt;Yi1.5-6|9|34B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/01-ai/yi-vl-663f557228538eae745769f3&#34;&gt;Yi-VL-6B|34B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Baichuan &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/baichuan-inc&#34;&gt;Baichuan-7|13B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/baichuan-inc&#34;&gt;Baichuan2-7|13B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Nvidia &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/nvidia/Nemotron-4-340B-Instruct&#34;&gt;Nemotron-4-340B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;BLOOM &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloomz&#34;&gt;BLOOMZ&amp;amp;mT0&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Zhipu AI &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/THUDM&#34;&gt;GLM-2|6|10|13|70B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/THUDM/cogvlm2-6645f36a29948b67dc4eef75&#34;&gt;CogVLM2-19B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;OpenBMB &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/openbmb/minicpm-2b-65d48bf958302b9fd25b698f&#34;&gt;MiniCPM-2B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/openbmb/OmniLMM-12B&#34;&gt;OmniLLM-12B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/openbmb/VisCPM-Chat&#34;&gt;VisCPM-10B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/openbmb/cpm-bee-65d491cc84fc93350d789361&#34;&gt;CPM-Bee-1|2|5|10B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;RWKV Foundation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/RWKV&#34;&gt;RWKV-v4|5|6&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ElutherAI &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/EleutherAI/pythia&#34;&gt;Pythia-1|1.4|2.8|6.9|12B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Stability AI &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/stabilityai/stable-lm-650852cfd55dd4e15cdcb30a&#34;&gt;StableLM-3B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/stabilityai/stable-lm-650852cfd55dd4e15cdcb30a&#34;&gt;StableLM-v2-1.6|12B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/stabilityai/stable-code-64f9dfb4ebc8a1be0a3f7650&#34;&gt;StableCode-3B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;BigCode &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/bigcode/%E2%AD%90-starcoder-64f9bd5740eb5daaeb81dbec&#34;&gt;StarCoder-1|3|7B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/bigcode/starcoder2-65de6da6e87db3383572be1a&#34;&gt;StarCoder2-3|7|15B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DataBricks &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.databricks.com/blog/mpt-7b&#34;&gt;MPT-7B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm&#34;&gt;DBRX-132B-MoE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Shanghai AI Laboratory &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/internlm/internlm2-65b0ce04970888799707893c&#34;&gt;InternLM2-1.8|7|20B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/internlm/internlm2-math-65b0ce88bf7d3327d0a5ad9f&#34;&gt;InternLM-Math-7B|20B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/internlm/internlm-xcomposer2-65b3706bf5d76208998e7477&#34;&gt;InternLM-XComposer2-1.8|7B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/OpenGVLab/internvl-65b92d6be81c86166ca0dde4&#34;&gt;InternVL-2|6|14|26&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM Data&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Zjh-819/LLMDataHub&#34;&gt;LLMDataHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IBM/data-prep-kit&#34;&gt;IBM data-prep-kit&lt;/a&gt; - Open-Source Toolkit for Efficient Unstructured Data Processing with Pre-built Modules and Local to Cluster Scalability.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM Evaluation:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;lm-evaluation-harness&lt;/a&gt; - A framework for few-shot evaluation of language models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Psycoy/MixEval&#34;&gt;MixEval&lt;/a&gt; - A reliable click-and-go evaluation suite compatible with both open-source and proprietary models, supporting MixEval and other benchmarks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/lighteval&#34;&gt;lighteval&lt;/a&gt; - a lightweight LLM evaluation suite that Hugging Face has been using internally.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/allenai/OLMo-Eval&#34;&gt;OLMO-eval&lt;/a&gt; - a repository for evaluating open language models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/declare-lab/instruct-eval&#34;&gt;instruct-eval&lt;/a&gt; - This repository contains code to quantitatively evaluate instruction-tuned models such as Alpaca and Flan-T5 on held-out tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/simple-evals&#34;&gt;simple-evals&lt;/a&gt; - Eval tools by OpenAI.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Giskard-AI/giskard&#34;&gt;Giskard&lt;/a&gt; - Testing &amp;amp; evaluation library for LLM applications, in particular RAGs&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.langchain.com/langsmith&#34;&gt;LangSmith&lt;/a&gt; - a unified platform from LangChain framework for: evaluation, collaboration HITL (Human In The Loop), logging and monitoring LLM applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/explodinggradients/ragas&#34;&gt;Ragas&lt;/a&gt; - a framework that helps you evaluate your Retrieval Augmented Generation (RAG) pipelines.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM Training Frameworks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/DeepSpeed&#34;&gt;DeepSpeed&lt;/a&gt; - DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/Megatron-DeepSpeed&#34;&gt;Megatron-DeepSpeed&lt;/a&gt; - DeepSpeed version of NVIDIA&#39;s Megatron-LM that adds additional support for several features such as MoE model training, Curriculum Learning, 3D Parallelism, and others.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/torchtune&#34;&gt;torchtune&lt;/a&gt; - A Native-PyTorch Library for LLM Fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/torchtitan&#34;&gt;torchtitan&lt;/a&gt; - A native PyTorch Library for large model training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;Megatron-LM&lt;/a&gt; - Ongoing research training transformer models at scale.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hpcaitech/ColossalAI&#34;&gt;Colossal-AI&lt;/a&gt; - Making large AI models cheaper, faster, and more accessible.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenBMB/BMTrain&#34;&gt;BMTrain&lt;/a&gt; - Efficient Training for Big Models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/mesh&#34;&gt;Mesh Tensorflow&lt;/a&gt; - Mesh TensorFlow: Model Parallelism Made Easier.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/maxtext&#34;&gt;maxtext&lt;/a&gt; - A simple, performant and scalable Jax LLM!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://alpa.ai/index.html&#34;&gt;Alpa&lt;/a&gt; - Alpa is a system for training and serving large-scale neural networks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX&lt;/a&gt; - An implementation of model parallel autoregressive transformers on GPUs, based on the DeepSpeed library.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM Deployment&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Reference: &lt;a href=&#34;https://github.com/mani-kantap/llm-inference-solutions&#34;&gt;llm-inference-solutions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;SGLang&lt;/a&gt; - SGLang is a fast serving framework for large language models and vision language models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; - A high-throughput and memory-efficient inference and serving engine for LLMs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/text-generation-inference/en/index&#34;&gt;TGI&lt;/a&gt; - a toolkit for deploying and serving Large Language Models (LLMs).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/turboderp/exllama&#34;&gt;exllama&lt;/a&gt; - A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; - LLM inference in C/C++.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;ollama&lt;/a&gt; - Get up and running with Llama 3, Mistral, Gemma, and other large language models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langfuse/langfuse&#34;&gt;Langfuse&lt;/a&gt; - Open Source LLM Engineering Platform ü™¢ Tracing, Evaluations, Prompt Management, Evaluations and Playground.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt; - A distributed multi-model LLM serving system with web UI and OpenAI-compatible RESTful APIs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EricLBuehler/mistral.rs&#34;&gt;mistral.rs&lt;/a&gt; - Blazingly fast LLM inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mindinventory/MindSQL&#34;&gt;MindSQL&lt;/a&gt; - A python package for Txt-to-SQL with self hosting functionalities and RESTful APIs compatible with proprietary as well as open source LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/skypilot-org/skypilot&#34;&gt;SkyPilot&lt;/a&gt; - Run LLMs and batch jobs on any cloud. Get maximum cost savings, highest GPU availability, and managed execution -- all with a simple interface.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://haystack.deepset.ai/&#34;&gt;Haystack&lt;/a&gt; - an open-source NLP framework that allows you to use LLMs and transformer-based models from Hugging Face, OpenAI and Cohere to interact with your own data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ai-sidekick/sidekick&#34;&gt;Sidekick&lt;/a&gt; - Data integration platform for LLMs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/reid41/QA-Pilot&#34;&gt;QA-Pilot&lt;/a&gt; - An interactive chat project that leverages Ollama/OpenAI/MistralAI LLMs for rapid understanding and navigation of GitHub code repository or compressed file resources.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/reid41/shell-pilot&#34;&gt;Shell-Pilot&lt;/a&gt; - Interact with LLM using Ollama models(or openAI, mistralAI)via pure shell scripts on your Linux(or MacOS) system, enhancing intelligent system management without any dependencies.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt; - Building applications with LLMs through composability&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FloomAI/Floom&#34;&gt;Floom&lt;/a&gt; AI gateway and marketplace for developers, enables streamlined integration of AI features into products&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Dicklesworthstone/swiss_army_llama&#34;&gt;Swiss Army Llama&lt;/a&gt; - Comprehensive set of tools for working with local LLMs for various tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rogeriochaves/litechain&#34;&gt;LiteChain&lt;/a&gt; - Lightweight alternative to LangChain for composing LLMs&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jackmpcollins/magentic&#34;&gt;magentic&lt;/a&gt; - Seamlessly integrate LLMs as Python functions&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fuergaosi233/wechat-chatgpt&#34;&gt;wechat-chatgpt&lt;/a&gt; - Use ChatGPT On Wechat via wechaty&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/typpo/promptfoo&#34;&gt;promptfoo&lt;/a&gt; - Test your prompts. Evaluate and compare LLM outputs, catch regressions, and improve prompt quality.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/agenta-ai/agenta&#34;&gt;Agenta&lt;/a&gt; - Easily build, version, evaluate and deploy your LLM-powered apps.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/serge-chat/serge&#34;&gt;Serge&lt;/a&gt; - a chat interface crafted with llama.cpp for running Alpaca models. No API keys, entirely self-hosted!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langroid/langroid&#34;&gt;Langroid&lt;/a&gt; - Harness LLMs with Multi-Agent Programming&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/embedchain/embedchain&#34;&gt;Embedchain&lt;/a&gt; - Framework to create ChatGPT like bots over your dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/comet-ml/opik&#34;&gt;Opik&lt;/a&gt; - Confidently evaluate, test, and ship LLM applications with a suite of observability tools to calibrate language model outputs across your dev and production lifecycle.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/intelligentnode/IntelliServer&#34;&gt;IntelliServer&lt;/a&gt; - simplifies the evaluation of LLMs by providing a unified microservice to access and test multiple AI models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bentoml/OpenLLM&#34;&gt;OpenLLM&lt;/a&gt; - Fine-tune, serve, deploy, and monitor any open-source LLMs in production. Used in production at &lt;a href=&#34;https://bentoml.com/&#34;&gt;BentoML&lt;/a&gt; for LLMs-based applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/DeepSpeed-MII&#34;&gt;DeepSpeed-Mii&lt;/a&gt; - MII makes low-latency and high-throughput inference, similar to vLLM powered by DeepSpeed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/text-embeddings-inference&#34;&gt;Text-Embeddings-Inference&lt;/a&gt; - Inference for text-embeddings in Rust, HFOIL Licence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/michaelfeil/infinity&#34;&gt;Infinity&lt;/a&gt; - Inference for text-embeddings in Python&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;TensorRT-LLM&lt;/a&gt; - Nvidia Framework for LLM Inference&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/FasterTransformer&#34;&gt;FasterTransformer&lt;/a&gt; - NVIDIA Framework for LLM Inference(Transitioned to TensorRT-LLM)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;Flash-Attention&lt;/a&gt; - A method designed to enhance the efficiency of Transformer models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chatchat-space/Langchain-Chatchat&#34;&gt;Langchain-Chatchat&lt;/a&gt; - Formerly langchain-ChatGLM, local knowledge based LLM (like ChatGLM) QA app with langchain.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/leptonai/search_with_lepton&#34;&gt;Search with Lepton&lt;/a&gt; - Build your own conversational search engine using less than 500 lines of code by &lt;a href=&#34;https://github.com/leptonai&#34;&gt;LeptonAI&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/robocorp/robocorp&#34;&gt;Robocorp&lt;/a&gt; - Create, deploy and operate Actions using Python anywhere to enhance your AI agents and assistants. Batteries included with an extensive set of libraries, helpers and logging.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/InternLM/lmdeploy&#34;&gt;LMDeploy&lt;/a&gt; - A high-throughput and low-latency inference and serving framework for LLMs and VLs&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://studio.tune.app/&#34;&gt;Tune Studio&lt;/a&gt; - Playground for devs to finetune &amp;amp; deploy LLMs&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nilsherzig/LLocalSearch&#34;&gt;LLocalSearch&lt;/a&gt; - Locally running websearch using LLM chains&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Portkey-AI/gateway&#34;&gt;AI Gateway&lt;/a&gt; ‚Äî Gateway streamlines requests to 100+ open &amp;amp; closed source models with a unified API. It is also production-ready with support for caching, fallbacks, retries, timeouts, loadbalancing, and can be edge-deployed for minimum latency.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/talkdai/dialog&#34;&gt;talkd.ai dialog&lt;/a&gt; - Simple API for deploying any RAG or LLM that you want adding plugins.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ngxson/wllama&#34;&gt;Wllama&lt;/a&gt; - WebAssembly binding for llama.cpp - Enabling in-browser LLM inference&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gpustack/gpustack&#34;&gt;GPUStack&lt;/a&gt; - An open-source GPU cluster manager for running LLMs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM Applications&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SylphAI-Inc/AdalFlow&#34;&gt;AdalFlow&lt;/a&gt; - AdalFlow: The PyTorch library for LLM applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stanfordnlp/dspy&#34;&gt;dspy&lt;/a&gt; - DSPy: The framework for programming‚Äînot prompting‚Äîfoundation models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YiVal/YiVal&#34;&gt;YiVal&lt;/a&gt; ‚Äî Evaluate and Evolve: YiVal is an open-source GenAI-Ops tool for tuning and evaluating prompts, configurations, and model parameters using customizable datasets, evaluation methods, and improvement strategies.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/guidance&#34;&gt;Guidance&lt;/a&gt; ‚Äî A handy looking Python library from Microsoft that uses Handlebars templating to interleave generation, prompting, and logical control.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt; ‚Äî A popular Python/JavaScript library for chaining sequences of language model prompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/evidentlyai/evidently&#34;&gt;Evidently&lt;/a&gt; ‚Äî An open-source framework to evaluate, test and monitor ML and LLM-powered systems.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://microsoft.github.io/FLAML/docs/Getting-Started/&#34;&gt;FLAML (A Fast Library for Automated Machine Learning &amp;amp; Tuning)&lt;/a&gt;: A Python library for automating selection of models, hyperparameters, and other tunable choices.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.chainlit.io/overview&#34;&gt;Chainlit&lt;/a&gt; ‚Äî A Python library for making chatbot interfaces.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.guardrailsai.com/docs/&#34;&gt;Guardrails.ai&lt;/a&gt; ‚Äî A Python library for validating outputs and retrying failures. Still in alpha, so expect sharp edges and bugs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/semantic-kernel&#34;&gt;Semantic Kernel&lt;/a&gt; ‚Äî A Python/C#/Java library from Microsoft that supports prompt templating, function chaining, vectorized memory, and intelligent planning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hegelai/prompttools&#34;&gt;Prompttools&lt;/a&gt; ‚Äî Open-source Python tools for testing and evaluating models, vector DBs, and prompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/normal-computing/outlines&#34;&gt;Outlines&lt;/a&gt; ‚Äî A Python library that provides a domain-specific language to simplify prompting and constrain generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/promptslab/Promptify&#34;&gt;Promptify&lt;/a&gt; ‚Äî A small Python library for using language models to perform NLP tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scale.com/spellbook&#34;&gt;Scale Spellbook&lt;/a&gt; ‚Äî A paid product for building, comparing, and shipping language model apps.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://promptperfect.jina.ai/prompts&#34;&gt;PromptPerfect&lt;/a&gt; ‚Äî A paid product for testing and improving prompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wandb.ai/site/solutions/llmops&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt; ‚Äî A paid product for tracking model training and prompt engineering experiments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/evals&#34;&gt;OpenAI Evals&lt;/a&gt; ‚Äî An open-source library for evaluating task performance of language models and prompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jerryjliu/llama_index&#34;&gt;LlamaIndex&lt;/a&gt; ‚Äî A Python library for augmenting LLM apps with data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.arthur.ai/get-started&#34;&gt;Arthur Shield&lt;/a&gt; ‚Äî A paid product for detecting toxicity, hallucination, prompt injection, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lmql.ai&#34;&gt;LMQL&lt;/a&gt; ‚Äî A programming language for LLM interaction with support for typed prompting, control flow, constraints, and tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lgrammel/modelfusion&#34;&gt;ModelFusion&lt;/a&gt; - A TypeScript library for building apps with LLMs and other ML models (speech-to-text, text-to-speech, image generation).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pleisto/flappy&#34;&gt;Flappy&lt;/a&gt; ‚Äî Production-Ready LLM Agent SDK for Every Developer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gpt-router.writesonic.com/&#34;&gt;GPTRouter&lt;/a&gt; - GPTRouter is an open source LLM API Gateway that offers a universal API for 30+ LLMs, vision, and image models, with smart fallbacks based on uptime and latency, automatic retries, and streaming. Stay operational even when OpenAI is down&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything&#34;&gt;QAnything&lt;/a&gt; - A local knowledge base question-answering system designed to support a wide range of file formats and databases.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openspg.yuque.com/ndx6g9/ps5q6b/vfoi61ks3mqwygvy&#34;&gt;OneKE&lt;/a&gt; ‚Äî A bilingual Chinese-English knowledge extraction model with knowledge graphs and natural language processing technologies.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/llm-ui-kit/llm-ui&#34;&gt;llm-ui&lt;/a&gt; - A React library for building LLM UIs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wordware.ai&#34;&gt;Wordware&lt;/a&gt; - A web-hosted IDE where non-technical domain experts work with AI Engineers to build task-specific AI agents. We approach prompting as a new programming language rather than low/no-code blocks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WallarooLabs&#34;&gt;Wallaroo.AI&lt;/a&gt; - Deploy, manage, optimize any model at scale across any environment from cloud to edge. Let&#39;s you go from python notebook to inferencing in minutes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langgenius/dify&#34;&gt;Dify&lt;/a&gt; - An open-source LLM app development platform with an intuitive interface that streamlines AI workflows, model management, and production deployment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LazyAGI/LazyLLM&#34;&gt;LazyLLM&lt;/a&gt; - An open-source LLM app for building multi-agent LLMs applications in an easy and lazy way, supports model deployment and fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/memfreeme/memfree&#34;&gt;MemFree&lt;/a&gt; - Open Source Hybrid AI Search Engine, Instantly Get Accurate Answers from the Internet, Bookmarks, Notes, and Docs. Support One-Click Deployment&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/unslothai/unsloth&#34;&gt;unslothai&lt;/a&gt; - A framework that specializes in efficient fine-tuning. On its GitHub page, you can find ready-to-use fine-tuning templates for various LLMs, allowing you to easily train your own data for free on the Google Colab cloud.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM Tutorials and Courses&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mlabonne/llm-course&#34;&gt;llm-course&lt;/a&gt; - Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/&#34;&gt;UWaterloo CS 886&lt;/a&gt; - Recent Advances on Foundation Models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/class/cs25/&#34;&gt;CS25-Transformers United&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/&#34;&gt;ChatGPT Prompt Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cs.princeton.edu/courses/archive/fall22/cos597G/&#34;&gt;Princeton: Understanding Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stanford-cs324.github.io/winter2022/&#34;&gt;CS324 - Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&#34;&gt;State of GPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://maartengrootendorst.substack.com/p/a-visual-guide-to-mamba-and-state?utm_source=multiple-personal-recommendations-email&amp;amp;utm_medium=email&amp;amp;open=false&#34;&gt;A Visual Guide to Mamba and State Space Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kCc8FmEb1nY&#34;&gt;Let&#39;s build GPT: from scratch, in code, spelled out.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zduSFxRajkE&amp;amp;t=1157s&#34;&gt;minbpe&lt;/a&gt; - Minimal, clean code for the Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/keyvank/femtoGPT&#34;&gt;femtoGPT&lt;/a&gt; - Pure Rust implementation of a minimal Generative Pretrained Transformer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nips.cc/virtual/2022/tutorial/55796&#34;&gt;Neurips2022-Foundational Robustness of Foundation Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://icml.cc/virtual/2022/tutorial/18440&#34;&gt;ICML2022-Welcome to the &#34;Big Model&#34; Era: Techniques and Systems to Train and Serve Bigger Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jaykmody.com/blog/gpt-from-scratch/&#34;&gt;GPT in 60 Lines of NumPy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM Books&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://amzn.to/3GUlRng&#34;&gt;Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs&lt;/a&gt; - it comes with a &lt;a href=&#34;https://github.com/benman1/generative_ai_with_langchain&#34;&gt;GitHub repository&lt;/a&gt; that showcases a lot of the functionality&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.manning.com/books/build-a-large-language-model-from-scratch&#34;&gt;Build a Large Language Model (From Scratch)&lt;/a&gt; - A guide to building your own working LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/dp/9152799727?ref_=cm_sw_r_cp_ud_dp_W3ZHCD6QWM3DPPC0ARTT_1&#34;&gt;BUILD GPT: HOW AI WORKS&lt;/a&gt; - explains how to code a Generative Pre-trained Transformer, or GPT, from scratch.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Great thoughts about LLM&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jingfengyang.github.io/gpt&#34;&gt;Why did all of the public reproduction of GPT-3 fail?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yaofu.notion.site/June-2023-A-Stage-Review-of-Instruction-Tuning-f59dbfc36e2d4e12a33443bd6b2012c2&#34;&gt;A Stage Review of Instruction Tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2023-06-23-agent/&#34;&gt;LLM Powered Autonomous Agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fqVLjtvWgq8&#34;&gt;Why you should work on AI AGENTS!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.semianalysis.com/p/google-we-have-no-moat-and-neither&#34;&gt;Google &#34;We Have No Moat, And Neither Does OpenAI&#34;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://petergabriel.com/news/ai-competition-statement/&#34;&gt;AI competition statement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&#34;&gt;Prompt Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html&#34;&gt;Noam Chomsky: The False Promise of ChatGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://orenleung.super.site/is-chatgpt-175-billion-parameters-technical-analysis&#34;&gt;Is ChatGPT 175 Billion Parameters? Technical Analysis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.notion.so/Awesome-LLM-40c8aa3f2b444ecc82b79ae8bbd2696b&#34;&gt;The Next Generation Of Large Language Models &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://research.aimultiple.com/large-language-model-training/&#34;&gt;Large Language Model Training in 2023&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1&#34;&gt;How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=p9IxoSkvZ-M&amp;amp;t=4s&#34;&gt;Open Pretrained Transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1EUV7W7X_w0BDrscDhPg7lMGzJCkeaPkGCJ3bN8dluXc/edit?pli=1&amp;amp;resourcekey=0-7Nz5A7y8JozyVrnDtcEKJA#slide=id.g16197112905_0_0&#34;&gt;Scaling, emergence, and reasoning in large language models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Miscellaneous&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://phoenix.arize.com/&#34;&gt;Arize-Phoenix&lt;/a&gt; - Open-source tool for ML observability that runs in your notebook environment. Monitor and fine tune LLM, CV and Tabular Models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.emergentmind.com&#34;&gt;Emergent Mind&lt;/a&gt; - The latest AI news, curated &amp;amp; explained by GPT-4.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sharegpt.com&#34;&gt;ShareGPT&lt;/a&gt; - Share your wildest ChatGPT conversations with one click.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1bmpDdLZxvTCleLGVPgzoMTQ0iDP2-7v7QziPrzPdHyM/edit#gid=0&#34;&gt;Major LLMs + Data Availability&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vaulted-polonium-23c.notion.site/500-Best-AI-Tools-e954b36bf688404ababf74a13f98d126&#34;&gt;500+ Best AI Tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://txt.cohere.ai/summarize-beta/&#34;&gt;Cohere Summarize Beta&lt;/a&gt; - Introducing Cohere Summarize Beta: A New Endpoint for Text Summarization&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mmabrouk/chatgpt-wrapper&#34;&gt;chatgpt-wrapper&lt;/a&gt; - ChatGPT Wrapper is an open-source unofficial Python API and CLI that lets you interact with ChatGPT.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-evals/evals&#34;&gt;Open-evals&lt;/a&gt; - A framework extend openai&#39;s &lt;a href=&#34;https://github.com/openai/evals&#34;&gt;Evals&lt;/a&gt; for different language model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cursor.so&#34;&gt;Cursor&lt;/a&gt; - Write, edit, and chat about your code with a powerful AI.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Significant-Gravitas/Auto-GPT&#34;&gt;AutoGPT&lt;/a&gt; - an experimental open-source application showcasing the capabilities of the GPT-4 language model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/agiresearch/OpenAGI&#34;&gt;OpenAGI&lt;/a&gt; - When LLM Meets Domain Experts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zjunlp/EasyEdit&#34;&gt;EasyEdit&lt;/a&gt; - An easy-to-use framework to edit large language models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/guyShilo/chatgpt-shroud&#34;&gt;chatgpt-shroud&lt;/a&gt; - A Chrome extension for OpenAI&#39;s ChatGPT, enhancing user privacy by enabling easy hiding and unhiding of chat history. Ideal for privacy during screen shares.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This is an active repository and your contributions are always welcome!&lt;/p&gt; &#xA;&lt;p&gt;I will keep some pull requests open if I&#39;m not sure if they are awesome for LLM, you could vote for them by adding üëç to them.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If you have any question about this opinionated list, do not hesitate to contact me &lt;a href=&#34;mailto:chengxin1998@stu.pku.edu.cn&#34;&gt;chengxin1998@stu.pku.edu.cn&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[^1]: This is not legal advice. Please contact the original authors of the models for more information.&lt;/p&gt;</summary>
  </entry>
</feed>