<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-26T01:22:20Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mshumer/gpt-investor</title>
    <updated>2024-03-26T01:22:20Z</updated>
    <id>tag:github.com,2024-03-26:/mshumer/gpt-investor</id>
    <link href="https://github.com/mshumer/gpt-investor" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gpt-investor&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/mattshumer_&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/mattshumer_?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Claude-Investor | The first release in the gpt-investor repo&lt;/h1&gt; &#xA;&lt;p&gt;Claude-Investor is an experimental investment analysis agent that utilizes the Claude 3 Opus and Haiku models to provide comprehensive insights and recommendations for stocks in a given industry. It retrieves financial data, news articles, and analyst ratings for companies, performs sentiment analysis, and generates comparative analyses to rank the companies based on their investment potential.&lt;/p&gt; &#xA;&lt;h2&gt;Workflow&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generates a list of ticker symbols for major companies in a specified industry&lt;/li&gt; &#xA; &lt;li&gt;Retrieves historical price data, balance sheets, financial statements, and news articles for each company&lt;/li&gt; &#xA; &lt;li&gt;Performs sentiment analysis on news articles to gauge market sentiment&lt;/li&gt; &#xA; &lt;li&gt;Retrieves analyst ratings and price targets for each company&lt;/li&gt; &#xA; &lt;li&gt;Conducts industry and sector analysis to understand market trends and competitive landscape&lt;/li&gt; &#xA; &lt;li&gt;Generates comparative analyses between the selected company and its peers&lt;/li&gt; &#xA; &lt;li&gt;Provides a final investment recommendation for each company based on the comprehensive analysis, including price targets&lt;/li&gt; &#xA; &lt;li&gt;Ranks the companies within the industry based on their investment attractiveness&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;To run Claude-Investor, you need an Anthropic API key for accessing the Claude AI model.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the &lt;code&gt;claude_investor.ipynb&lt;/code&gt; notebook in Google Colab or Jupyter Notebook.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Replace the placeholder for &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; in the notebook with your Anthropic API key.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the notebook cells sequentially to execute the code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When prompted, enter the industry you want to analyze.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Wait for the AI system to produce a report.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Claude-Investor is an educational and informational tool designed to assist in investment analysis. It should not be considered as financial advice or a substitute for professional investment guidance. Always conduct thorough research and consult with a qualified financial advisor before making any investment decisions.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/mshumer/gpt-investor/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! If you find any issues or have suggestions for improvements, please open an issue or submit a pull request.&lt;/p&gt; &#xA;&lt;p&gt;Some known improvement areas:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;improve the industry analysis module to use real-time data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Matt Shumer - &lt;a href=&#34;https://twitter.com/mattshumer_&#34;&gt;@mattshumer_&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lastly, if you want to try something even cooler than this, sign up for &lt;a href=&#34;https://app.hyperwriteai.com/personalassistant&#34;&gt;HyperWrite Personal Assistant&lt;/a&gt; (most of my time is spent on this). It&#39;s basically an AI with access to real-time information that a) is incredible at writing naturally, and b) can operate your web browser to complete tasks for you.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SakanaAI/evolutionary-model-merge</title>
    <updated>2024-03-26T01:22:20Z</updated>
    <id>tag:github.com,2024-03-26:/SakanaAI/evolutionary-model-merge</id>
    <link href="https://github.com/SakanaAI/evolutionary-model-merge" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official repository of Evolutionary Optimization of Model Merging Recipes&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üêü Evolutionary Optimization of Model Merging Recipes&lt;/h1&gt; &#xA;&lt;p&gt;ü§ó &lt;a href=&#34;https://huggingface.co/SakanaAI&#34;&gt;Models&lt;/a&gt; | üëÄ &lt;a href=&#34;https://huggingface.co/spaces/SakanaAI/EvoVLM-JP&#34;&gt;Demo&lt;/a&gt; | üìö &lt;a href=&#34;https://arxiv.org/abs/2403.13187&#34;&gt;Paper&lt;/a&gt; | üìù &lt;a href=&#34;https://sakana.ai/evolutionary-model-merge/&#34;&gt;Blog&lt;/a&gt; | üê¶ &lt;a href=&#34;https://twitter.com/SakanaAILabs&#34;&gt;Twitter&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/SakanaAI/evolutionary-model-merge/main/assets/method.gif&#34; alt=&#34;Method&#34; title=&#34;method&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This repository serves as a central hub for SakanaAI&#39;s &lt;a href=&#34;https://arxiv.org/abs/2403.13187&#34;&gt;Evolutionary Model Merge&lt;/a&gt; series, showcasing its releases and resources. It includes models and code for reproducing the evaluation presented in our paper. Look forward to more updates and additions coming soon.&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;h3&gt;Our Models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;License&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoLLM-JP-v1-7B&#34;&gt;EvoLLM-JP-v1-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Microsoft Research License&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/augmxnt/shisa-gamma-7b-v1&#34;&gt;shisa-gamma-7b-v1&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/WizardLM/WizardMath-7B-V1.1&#34;&gt;WizardMath-7B-V1.1&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/GAIR/Abel-7B-002&#34;&gt;GAIR/Abel-7B-002&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoLLM-JP-v1-10B&#34;&gt;EvoLLM-JP-v1-10B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;10B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Microsoft Research License&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;EvoLLM-JP-v1-7B, &lt;a href=&#34;https://huggingface.co/augmxnt/shisa-gamma-7b-v1&#34;&gt;shisa-gamma-7b-v1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoLLM-JP-A-v1-7B&#34;&gt;EvoLLM-JP-A-v1-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Apache 2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/augmxnt/shisa-gamma-7b-v1&#34;&gt;shisa-gamma-7b-v1&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/upaya07/Arithmo2-Mistral-7B&#34;&gt;Arithmo2-Mistral-7B&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/GAIR/Abel-7B-002&#34;&gt;GAIR/Abel-7B-002&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoVLM-JP-v1-7B&#34;&gt;EvoVLM-JP-v1-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Apache 2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b&#34;&gt;LLaVA-1.6-Mistral-7B&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/augmxnt/shisa-gamma-7b-v1&#34;&gt;shisa-gamma-7b-v1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Comparing EvoLLM-JP w/ Source LLMs&lt;/h3&gt; &#xA;&lt;p&gt;For details on the evaluation, please refer to Section 4.1 of the paper.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;MGSM-JA (acc ‚Üë)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;&lt;a href=&#34;https://github.com/Stability-AI/lm-evaluation-harness/tree/jp-stable&#34;&gt;lm-eval-harness&lt;/a&gt; (avg ‚Üë)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/augmxnt/shisa-gamma-7b-v1&#34;&gt;Shisa Gamma 7B v1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;66.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/WizardLM/WizardMath-7B-V1.1&#34;&gt;WizardMath 7B V1.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;18.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;60.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/GAIR/Abel-7B-002&#34;&gt;Abel 7B 002&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;30.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;56.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/upaya07/Arithmo2-Mistral-7B&#34;&gt;Arithmo2 Mistral 7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;24.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;56.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoLLM-JP-A-v1-7B&#34;&gt;EvoLLM-JP-A-v1-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;52.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;69.0&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoLLM-JP-v1-7B&#34;&gt;EvoLLM-JP-v1-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;52.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;70.5&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoLLM-JP-v1-10B&#34;&gt;EvoLLM-JP-v1-10B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;55.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;66.2&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Comparing EvoVLM-JP w/ Existing VLMs&lt;/h3&gt; &#xA;&lt;p&gt;For details on the evaluation, please see Section 4.2 of the paper.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;JA-VG-VQA-500 (ROUGE-L ‚Üë)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;JA-VLM-Bench-In-the-Wild (ROUGE-L ‚Üë)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://llava-vl.github.io/blog/2024-01-30-llava-next/&#34;&gt;LLaVA-1.6-Mistral-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;14.32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;41.10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/japanese-stable-vlm&#34;&gt;Japanese Stable VLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;sup&gt;*1&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;40.50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k&#34;&gt;Heron BLIP Japanese StableLM Base 7B llava-620k&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;8.73&lt;sup&gt;*2&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;27.37&lt;sup&gt;*2&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoVLM-JP-v1-7B&#34;&gt;EvoVLM-JP-v1-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;19.70&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;51.25&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;*1: Japanese Stable VLM cannot be evaluated using the JA-VG-VQA-500 dataset because this model has used this dataset for training.&lt;/li&gt; &#xA; &lt;li&gt;*2: We are checking with the authors to see if this current results are valid.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reproducing the Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;1. Clone the Repo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/SakanaAI/evolutionary-model-merge.git&#xA;cd evolutionary-model-merge&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Download fastext Model&lt;/h3&gt; &#xA;&lt;p&gt;We use fastext to detect language for evaluation. Please download &lt;code&gt;lid.176.ftz&lt;/code&gt; from &lt;a href=&#34;https://fasttext.cc/docs/en/language-identification.html&#34;&gt;this link&lt;/a&gt; and place it in your current directory. If you place the file in a directory other than the current directory, specify the path to the file using the &lt;code&gt;LID176FTZ_PATH&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;h3&gt;3. Install Libraries&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We conducted our tests in the following environment: Python Version 3.10.12 and CUDA Version 12.3. We cannot guarantee that it will work in other environments.&lt;/p&gt; &#xA;&lt;h3&gt;4. Run&lt;/h3&gt; &#xA;&lt;p&gt;To launch evaluation, run the following script with a certain config. All configs used for the paper are in &lt;code&gt;configs&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python evaluate.py --config_path {path-to-config}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank the developers of the source models for their contributions and for making their work available. Our math evaluation code builds on the WizardMath repository, and we are grateful for their work.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mistralai-sf24/hackathon</title>
    <updated>2024-03-26T01:22:20Z</updated>
    <id>tag:github.com,2024-03-26:/mistralai-sf24/hackathon</id>
    <link href="https://github.com/mistralai-sf24/hackathon" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mistral Transformer&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains minimal code to run our 7B model and to finetune it.&lt;br&gt; Blog: &lt;a href=&#34;https://mistral.ai/news/announcing-mistral-7b/&#34;&gt;https://mistral.ai/news/announcing-mistral-7b/&lt;/a&gt;&lt;br&gt; Discord: &lt;a href=&#34;https://discord.com/invite/mistralai&#34;&gt;https://discord.com/invite/mistralai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;Download the model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget https://models.mistralcdn.com/mistral-7b-v0-1/mistral-7B-v0.1.tar&#xA;tar -xf mistral-7B-v0.1.tar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The unzipped folder can be used as &lt;code&gt;initial_model_path:&lt;/code&gt; in the training config.&lt;/p&gt; &#xA;&lt;h3&gt;Installation Hackathon&lt;/h3&gt; &#xA;&lt;p&gt;Upon running the &lt;a href=&#34;http://ghcr.io/coreweave/ml-containers/torch-extras:a5a99e8-nccl-cuda12.2.2-ubuntu22.04-nccl2.19.3-1-torch2.2.0-vision0.17.0-audio2.2.0&#34;&gt;Docker container&lt;/a&gt;, all necessary dependencies can be installed with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements_hackathon.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using the trained model&lt;/h2&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;deploy&lt;/code&gt; folder contains code to build a &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; image with the required dependencies to serve the Mistral AI model. In the image, the &lt;a href=&#34;https://github.com/huggingface/transformers/&#34;&gt;transformers&lt;/a&gt; library is used instead of the reference implementation. To build it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build deploy --build-arg MAX_JOBS=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instructions to run the image can be found in the &lt;a href=&#34;https://docs.mistral.ai/quickstart&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Run the model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m main demo /path/to/mistral-7B-v0.1/&#xA;# To give your own prompts&#xA;python -m main interactive /path/to/mistral-7B-v0.1/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Change &lt;code&gt;temperature&lt;/code&gt; or &lt;code&gt;max_tokens&lt;/code&gt; using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m main interactive /path/to/mistral-7B-v0.1/ --max_tokens 256 --temperature 1.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want a self-contained implementation, look at &lt;code&gt;one_file_ref.py&lt;/code&gt;, or run it with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m one_file_ref /path/to/mistral-7B-v0.1/&#xA;&#xA;This is a test of the emergency broadcast system. This is only a test.&#xA;&#xA;If this were a real emergency, you would be told what to do.&#xA;&#xA;This is a test&#xA;=====================&#xA;This is another test of the new blogging software. I‚Äôm not sure if I‚Äôm going to keep it or not. I‚Äôm not sure if I‚Äôm going to keep&#xA;=====================&#xA;This is a third test, mistral AI is very good at testing. üôÇ&#xA;&#xA;This is a third test, mistral AI is very good at testing. üôÇ&#xA;&#xA;This&#xA;=====================&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run logits equivalence through chunking and sliding window, launch&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m test_generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fine-tune the model&lt;/h2&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;Data must be stored in jsonl format files.&lt;/p&gt; &#xA;&lt;p&gt;You can build two types of data files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Pretrain&lt;/em&gt;: plain text data stored in the &lt;code&gt;&#34;text&#34;&lt;/code&gt; key. E.g:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-jsonl&#34;&gt;{&#34;text&#34;: &#34;Text contained in document n¬∞1&#34;}&#xA;{&#34;text&#34;: &#34;Text contained in document n¬∞2&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Instruct&lt;/em&gt;: conversational data stored in the &lt;code&gt;&#34;interactions&#34;&lt;/code&gt; key in the form of a list. Each list item is a dictionary containing the &lt;code&gt;&#34;text&#34;&lt;/code&gt; and &lt;code&gt;&#34;is_user&#34;&lt;/code&gt; keys. &lt;code&gt;is_user&lt;/code&gt; is a boolean, if it is equal to True the loss will not be calculated on these tokens. E.g.:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-jsonl&#34;&gt;{&#34;interactions&#34;: [{&#34;is_user&#34;: true, &#34;text&#34;: &#34;User interaction n¬∞1 contained in document n¬∞1&#34;}, {&#34;is_user&#34;: false, &#34;text&#34;: &#34;Bot interaction n¬∞1 contained in document n¬∞1&#34;}, {&#34;is_user&#34;: true, &#34;text&#34;: &#34;User interaction n¬∞2 contained in document n¬∞1&#34;}, {&#34;is_user&#34;: false, &#34;text&#34;: &#34;Bot interaction n¬∞2 contained in document n¬∞1&#34;}]}&#xA;{&#34;interactions&#34;: [{&#34;is_user&#34;: true, &#34;text&#34;: &#34;User interaction n¬∞1 contained in document n¬∞2&#34;}, {&#34;is_user&#34;: false, &#34;text&#34;: &#34;Bot interaction n¬∞1 contained in document n¬∞2&#34;}, {&#34;is_user&#34;: true, &#34;text&#34;: &#34;User interaction n¬∞2 contained in document n¬∞2&#34;}, {&#34;is_user&#34;: false, &#34;text&#34;: &#34;Bot interaction n¬∞2 contained in document n¬∞2&#34;}, {&#34;is_user&#34;: true, &#34;text&#34;: &#34;User interaction n¬∞3 contained in document n¬∞2&#34;}, {&#34;is_user&#34;: false, &#34;text&#34;: &#34;Bot interaction n¬∞3 contained in document n¬∞2&#34;}]}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LoRA Finetuning&lt;/h3&gt; &#xA;&lt;p&gt;To benefit from a memory-efficient and performant finetuning, we recommend to use &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt;. The idea is to freeze weights and to only learn 1-2% additional weights in the form of low-rank matrix perturbations.&lt;/p&gt; &#xA;&lt;p&gt;With proper tuning (carefully calibrated learning rate, rank, LoRA dropout, learning the LoRA weights as well as the normalization layers), LoRA finetuning effectively recovers the performance of full finetuning. We support DDP on top of that, meaning that training speed can be increased on multiple GPUs.&lt;/p&gt; &#xA;&lt;p&gt;After the training, we merge the LoRA weights: hence, the saved checkpoint is exacly in the same format as one would get with full finetuning. To run a LoRA finetuning on a single GPU, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc-per-node 1 --master_port $RANDOM -m train reference/7B_lora.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>