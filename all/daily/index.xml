<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-20T01:29:47Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hwchase17/langchainjs</title>
    <updated>2023-02-20T01:29:47Z</updated>
    <id>tag:github.com,2023-02-20:/hwchase17/langchainjs</id>
    <link href="https://github.com/hwchase17/langchainjs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ü¶úÔ∏èüîó LangChain.js&lt;/h1&gt; &#xA;&lt;p&gt;‚ö° Building applications with LLMs through composability ‚ö°&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Production Support:&lt;/strong&gt; As you move your LangChains into production, we&#39;d love to offer more comprehensive support. Please fill out &lt;a href=&#34;https://forms.gle/57d8AmXBYp8PP8tZA&#34;&gt;this form&lt;/a&gt; and we&#39;ll set up a dedicated support Slack channel.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Install&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;yarn add langchain&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { OpenAI } from &#34;langchain/llms&#34;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü§î What is this?&lt;/h2&gt; &#xA;&lt;p&gt;Large language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. But using these LLMs in isolation is often not enough to create a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.&lt;/p&gt; &#xA;&lt;p&gt;This library is aimed at assisting in the development of those types of applications.&lt;/p&gt; &#xA;&lt;h2&gt;Relationship with Python LangChain&lt;/h2&gt; &#xA;&lt;p&gt;This is built to integrate as seamlessly as possible with the &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain Python package&lt;/a&gt;. Specifically, this means all objects (prompts, LLMs, chains, etc) are designed in a way where they can be serialized and shared between languages.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/hwchase17/langchain-hub&#34;&gt;LangChainHub&lt;/a&gt; is a central place for the serialized versions of these prompts, chains, and agents.&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Documentation&lt;/h2&gt; &#xA;&lt;p&gt;For full documentation of prompts, chains, agents and more, please see &lt;a href=&#34;https://hwchase17.github.io/langchainjs/docs/overview&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üíÅ Contributing&lt;/h2&gt; &#xA;&lt;p&gt;As an open source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infra, or better documentation.&lt;/p&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/hwchase17/langchainjs/main/CONTRIBUTING.md&#34;&gt;our contributing guidelines&lt;/a&gt; for instructions on how to contribute.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hpcaitech/EnergonAI</title>
    <updated>2023-02-20T01:29:47Z</updated>
    <id>tag:github.com,2023-02-20:/hpcaitech/EnergonAI</id>
    <link href="https://github.com/hpcaitech/EnergonAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Large-scale model inference.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Energon-AI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Made%20with-ColossalAI-blueviolet?style=flat&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI-Inference/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/hpcaitech/FastFold&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A service framework for large-scale model inference, Energon-AI has the following characteristics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parallelism for Large-scale Models:&lt;/strong&gt; With tensor parallel operations, pipeline parallel wrapper, distributed checkpoint loading, and customized CUDA kernel, EnergonAI can enable efficient parallel inference for larges-scale models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pre-built large models:&lt;/strong&gt; There are pre-built implementation for popular models, such as OPT. It supports the cache technique for the generation task and distributed parameter loading.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Engine encapsulationÔºö&lt;/strong&gt; There has an abstraction layer called engine. It encapsulates the single instance multiple devices (SIMD) execution with the remote procedure call, making it acts as the single instance single device (SISD) execution.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;An online service system:&lt;/strong&gt; Based on FastAPI, users can launch a web service of the distributed infernce quickly. The online service makes special optimizations for the generation task. It adopts both left padding and bucket batching techniques for improving the efficiency.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For models trained by &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI&#34;&gt;Colossal-AI&lt;/a&gt;, they can be easily transferred to Energon-AI. For single-device models, they require manual coding works to introduce tensor parallelism and pipeline parallelism.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install from source&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone git@github.com:hpcaitech/EnergonAI.git&#xA;$ pip install -r requirements.txt&#xA;$ pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Use docker&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker pull hpcaitech/energon-ai:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build an online OPT service in 5 minutes&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Download OPT model:&lt;/strong&gt; To launch the distributed inference service quickly, you can download the checkpoint of OPT-125M &lt;a href=&#34;https://huggingface.co/patrickvonplaten/opt_metaseq_125m/blob/main/model/restored.pt&#34;&gt;here&lt;/a&gt;. You can get details for loading other sizes of models &lt;a href=&#34;https://github.com/hpcaitech/EnergonAI/tree/main/examples/opt/script&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Launch an HTTP service:&lt;/strong&gt; To launch a service, we need to provide python scripts to describe the model type and related configurations, and start an http service. An OPT example is &lt;a href=&#34;https://github.com/hpcaitech/EnergonAI/tree/main/examples/opt&#34;&gt;EnergonAI/examples/opt&lt;/a&gt;.&lt;br&gt; The entrance of the service is a bash script &lt;em&gt;&lt;strong&gt;server.sh&lt;/strong&gt;&lt;/em&gt;. The config of the service is at &lt;em&gt;&lt;strong&gt;opt_config.py&lt;/strong&gt;&lt;/em&gt;, which defines the model type, the checkpoint file path, the parallel strategy, and http settings. You can adapt it for your own case. For example, set the model class as opt_125M and set the correct checkpoint path as follows. Set the tensor parallelism degree the same as your gpu number.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    model_class = opt_125M&#xA;    checkpoint = &#39;your_file_path&#39;&#xA;    tp_init_size = #gpu&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, we can launch a service:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    bash server.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then open &lt;em&gt;&lt;strong&gt;https://[ip]:[port]/docs&lt;/strong&gt;&lt;/em&gt; in your browser and try out!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Publication&lt;/h3&gt; &#xA;&lt;p&gt;You can find technical details in our blog and manuscript:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.colossalai.org/docs/advanced_tutorials/opt_service/&#34;&gt;Build an online OPT service using Colossal-AI in 5 minutes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2209.02341.pdf&#34;&gt;EnergonAI: An Inference System for 10-100 Billion Parameter Transformer Models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{du2022energonai, &#xA;      title={EnergonAI: An Inference System for 10-100 Billion Parameter Transformer Models}, &#xA;      author={Jiangsu Du and Ziming Liu and Jiarui Fang and Shenggui Li and Yongbin Li and Yutong Lu and Yang You},&#xA;      year={2022},&#xA;      eprint={2209.02341},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;p&gt;If interested in making your own contribution to the project, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/EnergonAI/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt; for guidance.&lt;/p&gt; &#xA;&lt;p&gt;Thanks so much!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>askrella/whatsapp-chatgpt</title>
    <updated>2023-02-20T01:29:47Z</updated>
    <id>tag:github.com,2023-02-20:/askrella/whatsapp-chatgpt</id>
    <link href="https://github.com/askrella/whatsapp-chatgpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ChatGPT + DALL-E + WhatsApp = AI Assistant üöÄ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GPT + DALL-E + Whatsapp = AI Assistant üöÄ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/askrella/whatsapp-chatgpt/actions/workflows/docker.yml/badge.svg?sanitize=true&#34; alt=&#34;Docker&#34;&gt; &lt;img src=&#34;https://github.com/askrella/whatsapp-chatgpt/actions/workflows/prettier.yml/badge.svg?sanitize=true&#34; alt=&#34;Prettier&#34;&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/9VJaRXKwd3&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/9VJaRXKwd3&#34; alt=&#34;Discord Invite&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This WhatsApp bot uses OpenAI&#39;s GPT and DALL-E 2 to respond to user inputs.&lt;/p&gt; &#xA;&lt;img width=&#34;904&#34; alt=&#34;Example prompts&#34; src=&#34;https://user-images.githubusercontent.com/6507938/219959783-96cac29a-d786-4586-a1fc-4dca827c4344.png&#34;&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Node.js (18 or newer)&lt;/li&gt; &#xA; &lt;li&gt;A recent version of npm&lt;/li&gt; &#xA; &lt;li&gt;An OpenAI Account&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository&lt;/li&gt; &#xA; &lt;li&gt;Install the required packages by running &lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Put your OpenAI API key into the &lt;code&gt;.env&lt;/code&gt; file &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example file: &lt;a href=&#34;https://github.com/askrella/whatsapp-chatgpt/raw/master/.env-example&#34;&gt;.env-example&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;You can obtain an API key &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run the bot using &lt;code&gt;npm run start&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Scan the QR code with WhatsApp (link a device)&lt;/li&gt; &#xA; &lt;li&gt;Now you&#39;re ready to go! People can send you messages, and the bot will respond to them&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;Make sure to edit the &lt;code&gt;docker-compose.yml&lt;/code&gt; file and set your own variables there.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To use the bot, simply send a message with the &lt;code&gt;!gpt&lt;/code&gt;/&lt;code&gt;!dalle&lt;/code&gt; command followed by your prompt. For example:&lt;/p&gt; &#xA;&lt;p&gt;GPT:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;!gpt What is the meaning of life?&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;DALLE:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;!dalle A frog with a red hat is walking on a bridge.&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disable prefix&lt;/h2&gt; &#xA;&lt;p&gt;You can disable the &lt;code&gt;!gpt&lt;/code&gt;/&lt;code&gt;!dalle&lt;/code&gt; prefix by setting &lt;code&gt;PREFIX_ENABLED&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;br&gt; If you disable the prefix, the bot will not support DALL-E, and only GPT will be used.&lt;/p&gt; &#xA;&lt;h2&gt;Sending messages to yourself&lt;/h2&gt; &#xA;&lt;p&gt;This bot also supports sending messages to yourself.&lt;/p&gt; &#xA;&lt;p&gt;To use this feature, simply send a message to your own phone number using the WhatsApp link &lt;code&gt;https://wa.me/&amp;lt;your_phone_number&amp;gt;&lt;/code&gt;. This will take you to your own chat window.&lt;/p&gt; &#xA;&lt;p&gt;After gaining access to your own chat, you can send a message to yourself and the bot will respond.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;The operations performed by this bot are not free. You will be charged by OpenAI for each request you make.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/askrella/whatsapp-chatgpt/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=askrella/whatsapp-chatgpt&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Used libraries&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pedroslopez/whatsapp-web.js&#34;&gt;https://github.com/pedroslopez/whatsapp-web.js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/transitive-bullshit/chatgpt-api&#34;&gt;https://github.com/transitive-bullshit/chatgpt-api&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>