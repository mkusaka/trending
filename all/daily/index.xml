<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-16T01:32:03Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>manycore-research/SpatialLM</title>
    <updated>2025-08-16T01:32:03Z</updated>
    <id>tag:github.com,2025-08-16:/manycore-research/SpatialLM</id>
    <link href="https://github.com/manycore-research/SpatialLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SpatialLM: Training Large Language Models for Structured Indoor Modeling&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SpatialLM&lt;/h1&gt; &#xA;&lt;!-- markdownlint-disable first-line-h1 --&gt; &#xA;&lt;!-- markdownlint-disable html --&gt; &#xA;&lt;!-- markdownlint-disable no-duplicate-header --&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/logo_light.png#gh-light-mode-only&#34; width=&#34;60%&#34; alt=&#34;SpatialLM&#34; /&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/logo_dark.png#gh-dark-mode-only&#34; width=&#34;60%&#34; alt=&#34;SpatialLM&#34; /&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr style=&#34;margin-top: 0; margin-bottom: 8px;&#34; /&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;margin-top: 0; padding-top: 0; line-height: 1;&#34;&gt; &#xA; &lt;a href=&#34;https://manycore-research.github.io/SpatialLM&#34; target=&#34;_blank&#34; style=&#34;margin: 2px;&#34;&gt;&lt;img alt=&#34;Project&#34; src=&#34;https://img.shields.io/badge/üåê%20Website-SpatialLM-ffc107?color=42a5f5&amp;amp;logoColor=white&#34; style=&#34;display: inline-block; vertical-align: middle;&#34; /&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2506.07491&#34; target=&#34;_blank&#34; style=&#34;margin: 2px;&#34;&gt;&lt;img alt=&#34;arXiv&#34; src=&#34;https://img.shields.io/badge/arXiv-Techreport-b31b1b?logo=arxiv&amp;amp;logoColor=white&#34; style=&#34;display: inline-block; vertical-align: middle;&#34; /&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/manycore-research/SpatialLM&#34; target=&#34;_blank&#34; style=&#34;margin: 2px;&#34;&gt;&lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/badge/GitHub-SpatialLM-24292e?logo=github&amp;amp;logoColor=white&#34; style=&#34;display: inline-block; vertical-align: middle;&#34; /&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;line-height: 1;&#34;&gt; &#xA; &lt;a href=&#34;https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B&#34; target=&#34;_blank&#34; style=&#34;margin: 2px;&#34;&gt;&lt;img alt=&#34;Hugging Face&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SpatialLM-ffc107?color=ffc107&amp;amp;logoColor=white&#34; style=&#34;display: inline-block; vertical-align: middle;&#34; /&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/datasets/manycore-research/SpatialLM-Testset&#34; target=&#34;_blank&#34; style=&#34;margin: 2px;&#34;&gt;&lt;img alt=&#34;Dataset&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-Testset-ffc107?color=ffc107&amp;amp;logoColor=white&#34; style=&#34;display: inline-block; vertical-align: middle;&#34; /&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;‚ú® News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Jun, 2025] Check out our new models: &lt;a href=&#34;https://huggingface.co/manycore-research/SpatialLM1.1-Llama-1B&#34;&gt;SpatialLM1.1-Llama-1B&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B&#34;&gt;SpatialLM1.1-Qwen-0.5B&lt;/a&gt;, now available on Hugging Face. SpatialLM1.1 doubles the point cloud resolution, incorporates a more powerful point cloud encoder &lt;a href=&#34;https://xywu.me/sonata/&#34;&gt;Sonata&lt;/a&gt; and supports detection with user-specified categories.&lt;/li&gt; &#xA; &lt;li&gt;[Jun, 2025] SpatialLM &lt;a href=&#34;https://arxiv.org/abs/2506.07491&#34;&gt;Technical Report&lt;/a&gt; is now on arXiv.&lt;/li&gt; &#xA; &lt;li&gt;[Mar, 2025] We&#39;re excited to release the &lt;a href=&#34;https://huggingface.co/manycore-research/SpatialLM-Llama-1B&#34;&gt;SpatialLM-Llama-1B&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/manycore-research/SpatialLM-Qwen-0.5B&#34;&gt;SpatialLM-Qwen-0.5B&lt;/a&gt; on Hugging Face.&lt;/li&gt; &#xA; &lt;li&gt;[Mar, 2025] Initial release of SpatialLM!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;SpatialLM is a 3D large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object bounding boxes with their semantic categories. Unlike previous methods that require specialized equipment for data collection, SpatialLM can handle point clouds from diverse sources such as monocular video sequences, RGBD images, and LiDAR sensors. This multimodal architecture effectively bridges the gap between unstructured 3D geometric data and structured 3D representations, offering high-level semantic understanding. It enhances spatial reasoning capabilities for applications in embodied robotics, autonomous navigation, and other complex 3D scene analysis tasks.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/c0218d6a-f676-41f8-ae76-bba228866306&#34; poster=&#34;figures/cover.png&#34;&gt; &#xA; &lt;/video&gt; &#xA; &lt;p&gt;&lt;i&gt;SpatialLM reconstructs 3D layout from a monocular RGB video with MASt3R-SLAM. Results aligned to video with GT cameras for visualization.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;SpatialLM Models&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;SpatialLM1.1-Llama-1B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/manycore-research/SpatialLM1.1-Llama-1B&#34;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;SpatialLM1.1-Qwen-0.5B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B&#34;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;SpatialLM1.0-Llama-1B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/manycore-research/SpatialLM-Llama-1B&#34;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;SpatialLM1.0-Qwen-0.5B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/manycore-research/SpatialLM-Qwen-0.5B&#34;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Tested with the following environment:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.11&lt;/li&gt; &#xA; &lt;li&gt;Pytorch 2.4.1&lt;/li&gt; &#xA; &lt;li&gt;CUDA Version 12.4&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# clone the repository&#xA;git clone https://github.com/manycore-research/SpatialLM.git&#xA;cd SpatialLM&#xA;&#xA;# create a conda environment with cuda 12.4&#xA;conda create -n spatiallm python=3.11&#xA;conda activate spatiallm&#xA;conda install -y -c nvidia/label/cuda-12.4.0 cuda-toolkit conda-forge::sparsehash&#xA;&#xA;# Install dependencies with poetry&#xA;pip install poetry &amp;amp;&amp;amp; poetry config virtualenvs.create false --local&#xA;poetry install&#xA;# SpatialLM1.0 dependency&#xA;poe install-torchsparse # Building wheel for torchsparse will take a while&#xA;# SpatialLM1.1 dependency&#xA;poe install-sonata # Building wheel for flash-attn will take a while&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;In the current version of SpatialLM, input point clouds are considered axis-aligned where the z-axis is the up axis. This orientation is crucial for maintaining consistency in spatial understanding and scene interpretation across different datasets and applications. Example preprocessed point clouds, reconstructed from RGB videos using &lt;a href=&#34;https://github.com/rmurai0610/MASt3R-SLAM&#34;&gt;MASt3R-SLAM&lt;/a&gt;, are available in &lt;a href=&#34;https://raw.githubusercontent.com/manycore-research/SpatialLM/main/#spatiallm-testset&#34;&gt;SpatialLM-Testset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Download an example point cloud:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli download manycore-research/SpatialLM-Testset pcd/scene0000_00.ply --repo-type dataset --local-dir .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --point_cloud pcd/scene0000_00.ply --output scene0000_00.txt --model_path manycore-research/SpatialLM1.1-Qwen-0.5B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Detection with user-specified categories&lt;/h3&gt; &#xA;&lt;p&gt;SpatialLM1.1 supports object detection conditioned on user-specified categories by leveraging the flexibility of LLMs.&lt;/p&gt; &#xA;&lt;p&gt;SpatialLM1.1 offers three variants of structured indoor modeling tasks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Structured Reconstruction&lt;/strong&gt;: Detect walls, doors, windows, boxes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Layout Estimation&lt;/strong&gt;: Detect walls, doors, windows.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;3D Object Detection&lt;/strong&gt;: Detect boxes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For tasks that include object box estimation, you can specify a subset of the 59 furniture categories, and the model will only predict objects within those specified categories. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --point_cloud pcd/scene0000_00.ply --output scene0000_00.txt --model_path manycore-research/SpatialLM1.1-Qwen-0.5B --detect_type object --category bed nightstand&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualization&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;code&gt;rerun&lt;/code&gt; to visualize the point cloud and the predicted structured 3D layout output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Convert the predicted layout to Rerun format&#xA;python visualize.py --point_cloud pcd/scene0000_00.ply --layout scene0000_00.txt --save scene0000_00.rrd&#xA;&#xA;# Visualize the point cloud and the predicted layout&#xA;rerun scene0000_00.rrd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;To evaluate the performance of SpatialLM, we provide &lt;code&gt;eval.py&lt;/code&gt; script that reports the benchmark results on the SpatialLM-Testset in the table below in section &lt;a href=&#34;https://raw.githubusercontent.com/manycore-research/SpatialLM/main/#benchmark-results&#34;&gt;Benchmark Results&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Download the testset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli download manycore-research/SpatialLM-Testset --repo-type dataset --local-dir SpatialLM-Testset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run inference on the PLY point clouds in folder SpatialLM-Testset/pcd with SpatialLM1.1-Qwen-0.5B model&#xA;python inference.py --point_cloud SpatialLM-Testset/pcd --output SpatialLM-Testset/pred --model_path manycore-research/SpatialLM1.1-Qwen-0.5B&#xA;&#xA;# Evaluate the predicted layouts&#xA;python eval.py --metadata SpatialLM-Testset/test.csv --gt_dir SpatialLM-Testset/layout --pred_dir SpatialLM-Testset/pred --label_mapping SpatialLM-Testset/benchmark_categories.tsv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example using a custom video&lt;/h3&gt; &#xA;&lt;p&gt;We provide an example of how to use our model to estimate scene layout starting from a RGB video with the newly released &lt;a href=&#34;https://github.com/PKU-VCL-3DV/SLAM3R&#34;&gt;SLAM3R&lt;/a&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/manycore-research/SpatialLM/main/EXAMPLE.md&#34;&gt;EXAMPLE.md&lt;/a&gt;. These steps work for MASt3R-SLAM, and other reconstruction methods as well.&lt;/p&gt; &#xA;&lt;h2&gt;SpatialLM Testset&lt;/h2&gt; &#xA;&lt;p&gt;We provide a test set of 107 preprocessed point clouds, reconstructed from RGB videos using &lt;a href=&#34;https://github.com/rmurai0610/MASt3R-SLAM&#34;&gt;MASt3R-SLAM&lt;/a&gt;. SpatialLM-Testset is quite challenging compared to prior clean RGBD scans datasets due to the noises and occlusions in the point clouds reconstructed from monocular RGB videos.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;SpatialLM-Testset&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/manycore-research/SpatialLM-TestSet&#34;&gt;ü§ó Datasets&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Benchmark Results&lt;/h2&gt; &#xA;&lt;h3&gt;Layout Estimation&lt;/h3&gt; &#xA;&lt;p&gt;Layout estimation focuses on predicting architectural elements, i.e., walls, doors, and windows, within an indoor scene. We evaluated this task on the &lt;a href=&#34;https://structured3d-dataset.org&#34;&gt;Structured3D&lt;/a&gt; dataset. For &lt;a href=&#34;https://github.com/ywyue/RoomFormer&#34;&gt;RoomFormer&lt;/a&gt;, we directly downloaded the model checkpoint. SceneScript and SpatialLM were first trained on our dataset, and further fine-tuned on Structured3D.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;RoomFormer&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;SceneScript (finetuned)&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;SpatialLM1.1-Qwen-0.5B (finetuned)&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;F1 @.25 IoU&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;70.4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;83.1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;86.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;F1 @.5 IoU&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;67.2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;80.8&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;84.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;3D Object Detection&lt;/h3&gt; &#xA;&lt;p&gt;We evaluate 3D object detection on &lt;a href=&#34;http://www.scan-net.org&#34;&gt;ScanNet&lt;/a&gt; with annotations of 18 object categories. For &lt;a href=&#34;https://github.com/V-DETR/V-DETR&#34;&gt;V-DETR&lt;/a&gt;, we directly download the model checkpoint. SceneScript and SpatialLM were first trained on our dataset, and further fine-tuned on ScanNet.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;V-DETR&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;SceneScript (finetuned)&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;SpatialLM1.1-Qwen-0.5B (finetuned)&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;F1 @.25 IoU&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;65.1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;49.1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;65.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;F1 @.5 IoU&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;56.8&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;36.8&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;52.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Zero-shot Detection on Videos&lt;/h3&gt; &#xA;&lt;p&gt;Zero-shot detection results on the challenging SpatialLM-Testset are reported in the following table:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;SpatialLM1.1-Llama-1B&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;SpatialLM1.1-Qwen-0.5B&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Layout&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;F1 @.25 IoU (2D)&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;F1 @.25 IoU (2D)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;wall&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;68.9&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;68.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;door&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;46.3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;43.1&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;window&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;43.8&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;47.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Objects&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;F1 @.25 IoU (3D)&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;F1 @.25 IoU (2D)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;curtain&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;34.9&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;37.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;nightstand&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;62.8&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;67.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;chandelier&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;53.5&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;36.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;wardrobe&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;29.4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;39.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;bed&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;96.8&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;95.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;sofa&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;66.9&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;69.1&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;chair&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;20.8&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;32.3&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;cabinet&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;15.2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;11.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;dining table&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;40.7&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;24.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;plants&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;29.5&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;26.3&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;tv cabinet&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;34.4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;27.3&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;coffee table&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;56.4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;64.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;side table&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;14.6&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;9.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;air conditioner&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;16.7&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;24.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;dresser&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;46.7&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;46.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;stool&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;17.6&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;30.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;refrigerator&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.0&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;16.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;painting&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;34.9&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;38.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;carpet&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;40.3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;24.1&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;tv&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;16.0&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;18.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Result Visualizations&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Layout Estimation&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Object Detection&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Zero-shot Reconstruction&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/stru3d.jpg&#34; alt=&#34;Structured3D&#34; /&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/scannet.jpg&#34; alt=&#34;ScanNet&#34; /&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/zeroshot.jpg&#34; alt=&#34;Zero-shot&#34; /&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_layout.html&#34;&gt;Structured3D Results&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_object.html&#34;&gt;ScanNet Results&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_zeroshot.html&#34;&gt;Zeroshot Results&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;SpatialLM-Llama-1B is derived from Llama3.2-1B-Instruct, which is licensed under the Llama3.2 license. SpatialLM-Qwen-0.5B is derived from the Qwen-2.5 series, originally licensed under the Apache 2.0 License.&lt;/p&gt; &#xA;&lt;p&gt;SpatialLM1.0 are built upon the SceneScript point cloud encoder, licensed under the CC-BY-NC-4.0 License. TorchSparse, utilized in this project, is licensed under the MIT License.&lt;/p&gt; &#xA;&lt;p&gt;SpatialLM1.1 are built upon Sonata point cloud encoder, model weight is licensed under the CC-BY-NC-4.0 License. Code built on Pointcept is licensed under the Apache 2.0 License.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this work useful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{SpatialLM,&#xA;    title         = {SpatialLM: Training Large Language Models for Structured Indoor Modeling},&#xA;    author        = {Mao, Yongsen and Zhong, Junhao and Fang, Chuan and Zheng, Jia and Tang, Rui and Zhu, Hao and Tan, Ping and Zhou, Zihan},&#xA;    journal       = {arXiv preprint},&#xA;    year          = {2025},&#xA;    eprint        = {2506.07491},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass  = {cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank the following projects that made this work possible:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/meta-llama&#34;&gt;Llama3.2&lt;/a&gt; | &lt;a href=&#34;https://github.com/QwenLM/Qwen2.5&#34;&gt;Qwen2.5&lt;/a&gt; | &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt; | &lt;a href=&#34;https://github.com/facebookresearch/scenescript&#34;&gt;SceneScript&lt;/a&gt; | &lt;a href=&#34;https://github.com/mit-han-lab/torchsparse&#34;&gt;TorchSparse&lt;/a&gt; | &lt;a href=&#34;https://xywu.me/sonata/&#34;&gt;Sonata&lt;/a&gt; | &lt;a href=&#34;https://github.com/Pointcept/Pointcept&#34;&gt;Pointcept&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/magentic-ui</title>
    <updated>2025-08-16T01:32:03Z</updated>
    <id>tag:github.com,2025-08-16:/microsoft/magentic-ui</id>
    <link href="https://github.com/microsoft/magentic-ui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A research prototype of a human-centered web agent&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-readme-logo.svg?sanitize=true&#34; alt=&#34;Magentic-UI Logo&#34; /&gt; &#xA; &lt;p&gt;&lt;em&gt;Automate your web tasks while you stay in control&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.python.org/pypi/magentic_ui&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/magentic_ui.svg?sanitize=true&#34; alt=&#34;image&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/magentic_ui&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/magentic_ui.svg?sanitize=true&#34; alt=&#34;image&#34; /&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue&#34; alt=&#34;Python Versions&#34; /&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr /&gt; &#xA;&lt;p&gt;Magentic-UI is a &lt;strong&gt;research prototype&lt;/strong&gt; of a human-centered interface powered by a multi-agent system that can browse and perform actions on the web, generate and execute code, and generate and analyze files.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/7975fc26-1a18-4acb-8bf9-321171eeade7&#34;&gt;https://github.com/user-attachments/assets/7975fc26-1a18-4acb-8bf9-321171eeade7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s how you can get started with Magentic-UI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1. Setup environment&#xA;python3 -m venv .venv&#xA;source .venv/bin/activate&#xA;pip install magentic-ui --upgrade&#xA;&#xA;# 2. Set your API key&#xA;export OPENAI_API_KEY=&#34;your-api-key-here&#34;&#xA;&#xA;# 3. Launch Magentic-UI&#xA;magentic-ui --port 8081&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then open &lt;a href=&#34;http://localhost:8081&#34;&gt;http://localhost:8081&lt;/a&gt; in your browser to interact with Magentic-UI!&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: Requires Docker and Python 3.10+. Windows users should use WSL2. See &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/#%EF%B8%8F-installation&#34;&gt;detailed installation&lt;/a&gt; for more info.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;‚ú® What&#39;s New&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;File Upload Support&lt;/strong&gt;: Upload any file through the UI for analysis or modification&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MCP Agents&lt;/strong&gt;: Extend capabilities with your favorite MCP servers&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easier Installation&lt;/strong&gt;: We have uploaded our docker containers to GHCR so you no longer need to build any containers! Installation time now is much quicker.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Alternative Usage Options&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Without Docker&lt;/strong&gt; (limited functionality: no code execution):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;magentic-ui --run-without-docker --port 8081&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Command Line Interface&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;magentic-cli --work-dir PATH/TO/STORE/DATA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Custom LLM Clients&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Azure&#xA;pip install magentic-ui[azure]&#xA;&#xA;# Ollama (local models)&#xA;pip install magentic-ui[ollama]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then pass a config file to the &lt;code&gt;magentic-ui&lt;/code&gt; command (&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/#model-client-configuration&#34;&gt; client config&lt;/a&gt;) or change the model client inside the UI settings.&lt;/p&gt; &#xA;&lt;p&gt;For further details on installation please read the &lt;a href=&#34;#Ô∏è-installation&#34;&gt;üõ†Ô∏è Installation&lt;/a&gt; section. For common installation issues and their solutions, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md&#34;&gt;troubleshooting document&lt;/a&gt;. See advanced usage instructions with the command &lt;code&gt;magentic-ui --help&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Navigation:&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/#-how-it-works&#34;&gt;üü™ How it Works&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href=&#34;#Ô∏è-installation&#34;&gt;üõ†Ô∏è Installation&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/#troubleshooting&#34;&gt;‚ö†Ô∏è Troubleshooting&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/#contributing&#34;&gt;ü§ù Contributing&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/#license&#34;&gt;üìÑ License&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;üü™ How it Works&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magenticui_running.png&#34; alt=&#34;Magentic-UI&#34; height=&#34;400&#34; /&gt; &lt;/p&gt; &#xA;&lt;p&gt;Magentic-UI is especially useful for web tasks that require actions on the web (e.g., filling a form, customizing a food order), deep navigation through websites not indexed by search engines (e.g., filtering flights, finding a link from a personal site) or tasks that need web navigation and code execution (e.g., generate a chart from online data).&lt;/p&gt; &#xA;&lt;p&gt;The interface of Magentic-UI is displayed in the screenshot above and consists of two panels. The left side panel is the sessions navigator where users can create new sessions to solve new tasks, switch between sessions and check on session progress with the session status indicators (üî¥ needs input, ‚úÖ task done, ‚Ü∫ task in progress).&lt;/p&gt; &#xA;&lt;p&gt;The right-side panel displays the session selected. This is where you can type your query to Magentic-UI alongside any file attachments and observe detailed task progress as well as interact with the agents. The session display itself is split in two panels: the left side is where Magentic-UI presents the plan, task progress and asks for action approvals, the right side is a browser view where you can see web agent actions in real time and interact with the browser. Finally, at the top of the session display is a progress bar that updates as Magentic-UI makes progress.&lt;/p&gt; &#xA;&lt;p&gt;The example below shows a step by step user interaction with Magentic-UI:&lt;/p&gt; &#xA;&lt;!-- Screenshots --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-landing.png&#34; alt=&#34;Magentic-UI Landing&#34; width=&#34;45%&#34; style=&#34;margin:10px;&#34; /&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-coplanning.png&#34; alt=&#34;Co-Planning UI&#34; width=&#34;45%&#34; style=&#34;margin:10px;&#34; /&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-cotasking.png&#34; alt=&#34;Co-Tasking UI&#34; width=&#34;45%&#34; style=&#34;margin:10px;&#34; /&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-actionguard.png&#34; alt=&#34;Action Guard UI&#34; width=&#34;45%&#34; style=&#34;margin:10px;&#34; /&gt; &lt;/p&gt; &#xA;&lt;p&gt;What differentiates Magentic-UI from other browser use offerings is its transparent and controllable interface that allows for efficient human-in-the-loop involvement. Magentic-UI is built using &lt;a href=&#34;https://github.com/microsoft/autogen&#34;&gt;AutoGen&lt;/a&gt; and provides a platform to study human-agent interaction and experiment with web agents. Key features include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üßë‚Äçü§ù‚Äçüßë &lt;strong&gt;Co-Planning&lt;/strong&gt;: Collaboratively create and approve step-by-step plans using chat and the plan editor.&lt;/li&gt; &#xA; &lt;li&gt;ü§ù &lt;strong&gt;Co-Tasking&lt;/strong&gt;: Interrupt and guide the task execution using the web browser directly or through chat. Magentic-UI can also ask for clarifications and help when needed.&lt;/li&gt; &#xA; &lt;li&gt;üõ°Ô∏è &lt;strong&gt;Action Guards&lt;/strong&gt;: Sensitive actions are only executed with explicit user approvals.&lt;/li&gt; &#xA; &lt;li&gt;üß† &lt;strong&gt;Plan Learning and Retrieval&lt;/strong&gt;: Learn from previous runs to improve future task automation and save them in a plan gallery. Automatically or manually retrieve saved plans in future tasks.&lt;/li&gt; &#xA; &lt;li&gt;üîÄ &lt;strong&gt;Parallel Task Execution&lt;/strong&gt;: You can run multiple tasks in parallel and session status indicators will let you know when Magentic-UI needs your input or has completed the task.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.youtube.com/watch?v=wOs-5SR8xOc&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.youtube.com/vi/wOs-5SR8xOc/maxresdefault.jpg&#34; alt=&#34;Watch the demo video&#34; width=&#34;600&#34; /&gt; &lt;/a&gt; &#xA; &lt;br /&gt; ‚ñ∂Ô∏è &#xA; &lt;em&gt; Click to watch a video and learn more about Magentic-UI &lt;/em&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Autonomous Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;To evaluate its autonomous capabilities, Magentic-UI has been tested against several benchmarks when running with o4-mini: &lt;a href=&#34;https://huggingface.co/datasets/gaia-benchmark/GAIA&#34;&gt;GAIA&lt;/a&gt; test set (42.52%), which assesses general AI assistants across reasoning, tool use, and web interaction tasks ; &lt;a href=&#34;https://huggingface.co/AssistantBench&#34;&gt;AssistantBench&lt;/a&gt; test set (27.60%), focusing on realistic, time-consuming web tasks; &lt;a href=&#34;https://github.com/MinorJerry/WebVoyager&#34;&gt;WebVoyager&lt;/a&gt; (82.2%), measuring end-to-end web navigation in real-world scenarios; and &lt;a href=&#34;https://webgames.convergence.ai/&#34;&gt;WebGames&lt;/a&gt; (45.5%), evaluating general-purpose web-browsing agents through interactive challenges. To reproduce these experimental results, please see the following &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/experiments/eval/README.md&#34;&gt;instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re interested in reading more checkout our &lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/magentic-ui-report.pdf&#34;&gt;technical report&lt;/a&gt; and &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/magentic-ui-an-experimental-human-centered-web-agent/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Pre-Requisites&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you&#39;re using Windows, we highly recommend using &lt;a href=&#34;https://docs.microsoft.com/en-us/windows/wsl/install&#34;&gt;WSL2&lt;/a&gt; (Windows Subsystem for Linux).&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If running on &lt;strong&gt;Windows&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt; you should use &lt;a href=&#34;https://www.docker.com/products/docker-desktop/&#34;&gt;Docker Desktop&lt;/a&gt; or if inside WSL2 you can install Docker directly inside WSL &lt;a href=&#34;https://gist.github.com/dehsilvadeveloper/c3bdf0f4cdcc5c177e2fe9be671820c7&#34;&gt;docker in WSL2 guide&lt;/a&gt;. If running on &lt;strong&gt;Linux&lt;/strong&gt;, you should use &lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Docker Engine&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If using Docker Desktop, make sure it is set up to use WSL2: - Go to Settings &amp;gt; Resources &amp;gt; WSL Integration - Enable integration with your development distro You can find more detailed instructions about this step &lt;a href=&#34;https://docs.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;During the Installation step, you will need to set up your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;. To use other models, review the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/#model-client-configuration&#34;&gt;Model Client Configuration&lt;/a&gt; section below.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You need at least &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python 3.10&lt;/a&gt; installed.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you are on Windows, we recommend to run Magentic-UI inside &lt;a href=&#34;https://docs.microsoft.com/en-us/windows/wsl/install&#34;&gt;WSL2&lt;/a&gt; (Windows Subsystem for Linux) for correct Docker and file path compatibility.&lt;/p&gt; &#xA;&lt;h3&gt;PyPI Installation&lt;/h3&gt; &#xA;&lt;p&gt;Magentic-UI is available on PyPI. We recommend using a virtual environment to avoid conflicts with other packages.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m venv .venv&#xA;source .venv/bin/activate&#xA;pip install magentic-ui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, if you use &lt;a href=&#34;https://docs.astral.sh/uv/getting-started/installation/&#34;&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; for dependency management, you can install Magentic-UI with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv venv --python=3.12 .venv&#xA;. .venv/bin/activate&#xA;uv pip install magentic-ui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Magentic-UI&lt;/h3&gt; &#xA;&lt;p&gt;To run Magentic-UI, make sure that Docker is running, then run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;magentic-ui --port 8081&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Running this command for the first time will pull two docker images required for the Magentic-UI agents. If you encounter problems, you can build them directly with the following command:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docker&#xA;sh build-all.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you face issues with Docker, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md&#34;&gt;TROUBLESHOOTING.md&lt;/a&gt; document.&lt;/p&gt; &#xA;&lt;p&gt;Once the server is running, you can access the UI at &lt;a href=&#34;http://localhost:8081&#34;&gt;http://localhost:8081&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;h4&gt;Model Client Configuration&lt;/h4&gt; &#xA;&lt;p&gt;If you want to use a different OpenAI key, or if you want to configure use with Azure OpenAI or Ollama, you can do so inside the UI by navigating to settings (top right icon) and changing model configuration. Another option is to pass a yaml config file when you start Magentic-UI which will override any settings in the UI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;magentic-ui --port 8081 --config config.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where the &lt;code&gt;config.yaml&lt;/code&gt; should look as follows with an AutoGen model client configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;gpt4o_client: &amp;amp;gpt4o_client&#xA;    provider: OpenAIChatCompletionClient&#xA;    config:&#xA;      model: gpt-4o-2024-08-06&#xA;      api_key: null&#xA;      base_url: null&#xA;      max_retries: 5&#xA;&#xA;orchestrator_client: *gpt4o_client&#xA;coder_client: *gpt4o_client&#xA;web_surfer_client: *gpt4o_client&#xA;file_surfer_client: *gpt4o_client&#xA;action_guard_client: *gpt4o_client&#xA;plan_learning_client: *gpt4o_client&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can change the client for each of the agents using the config file and use AzureOpenAI (&lt;code&gt;AzureOpenAIChatCompletionClient&lt;/code&gt;), Ollama and other clients.&lt;/p&gt; &#xA;&lt;h4&gt;MCP Server Configuration&lt;/h4&gt; &#xA;&lt;p&gt;You can also extend Magentic-UI&#39;s capabilities by adding custom &#34;McpAgents&#34; to the multi-agent team. Each McpAgent can have access to one or more MCP Servers. You can specify these agents via the &lt;code&gt;mcp_agent_configs&lt;/code&gt; parameter in your &lt;code&gt;config.yaml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For example, here&#39;s an agent called &#34;airbnb_surfer&#34; that has access to the OpenBnb MCP Server running locally via Stdio.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;mcp_agent_configs:&#xA;  - name: airbnb_surfer&#xA;    description: &#34;The airbnb_surfer has direct access to AirBnB.&#34;&#xA;    model_client: &#xA;      provider: OpenAIChatCompletionClient&#xA;      config:&#xA;        model: gpt-4.1-2025-04-14&#xA;      max_retries: 10&#xA;    system_message: |-&#xA;      You are AirBnb Surfer, a helpful digital assistant that can help users acces AirBnB.&#xA;&#xA;      You have access to a suite of tools provided by the AirBnB API. Use those tools to satisfy the users requests.&#xA;    reflect_on_tool_use: false&#xA;    mcp_servers:&#xA;      - server_name: AirBnB&#xA;        server_params:&#xA;          type: StdioServerParams&#xA;          command: npx&#xA;          args:&#xA;            - -y&#xA;            - &#34;@openbnb/mcp-server-airbnb&#34;&#xA;            - --ignore-robots-txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Under the hood, each &lt;code&gt;McpAgent&lt;/code&gt; is just a &lt;code&gt;autogen_agentchat.agents.AssistantAgent&lt;/code&gt; with the set of MCP Servers exposed as an &lt;code&gt;AggregateMcpWorkbench&lt;/code&gt; which is simply a named collection of &lt;code&gt;autogen_ext.tools.mcp.McpWorkbench&lt;/code&gt; objects (one per MCP Server).&lt;/p&gt; &#xA;&lt;p&gt;Currently the supported MCP Server types are &lt;code&gt;autogen_ext.tools.mcp.StdioServerParams&lt;/code&gt; and &lt;code&gt;autogen_ext.tools.mcp.SseServerParams&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Building Magentic-UI from source&lt;/h3&gt; &#xA;&lt;p&gt;This step is primarily for users seeking to make modifications to the code, are having trouble with the pypi installation or want the latest code before a pypi version release.&lt;/p&gt; &#xA;&lt;h4&gt;1. Make sure the above prerequisites are installed, and that Docker is running.&lt;/h4&gt; &#xA;&lt;h4&gt;2. Clone the repository to your local machine:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/microsoft/magentic-ui.git&#xA;cd magentic-ui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Install Magentic-UI&#39;s dependencies with uv or your favorite package manager:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# install uv through https://docs.astral.sh/uv/getting-started/installation/&#xA;uv venv --python=3.12 .venv&#xA;uv sync --all-extras&#xA;source .venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;4. Build the frontend:&lt;/h4&gt; &#xA;&lt;p&gt;First make sure to install node:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# install nvm to install node&#xA;curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash&#xA;nvm install node&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then install the frontend:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd frontend&#xA;npm install -g gatsby-cli&#xA;npm install --global yarn&#xA;yarn install&#xA;yarn build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;5. Run Magentic-UI, as usual.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;magentic-ui --port 8081&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Running the UI from source&lt;/h4&gt; &#xA;&lt;p&gt;If you are making changes to the source code of the UI, you can run the frontend in development mode so that it will automatically update when you make changes for faster development.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open a separate terminal and change directory to the frontend&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd frontend&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;.env.development&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp .env.default .env.development&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Launch frontend server&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Then run the UI:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;magentic-ui --port 8081&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The frontend from source will be available at &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt;, and the compiled frontend will be available at &lt;a href=&#34;http://localhost:8081&#34;&gt;http://localhost:8081&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;If you were unable to get Magentic-UI running, do not worry! The first step is to make sure you have followed the steps outlined above, particularly with the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/#pre-requisites&#34;&gt;pre-requisites&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For common issues and their solutions, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md&#34;&gt;TROUBLESHOOTING.md&lt;/a&gt; file in this repository. If you do not see your problem there, please open a &lt;code&gt;GitHub Issue&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. For information about contributing to Magentic-UI, please see our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; guide, which includes current issues to be resolved and other forms of contributing.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information, see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Microsoft, and any contributors, grant you a license to any code in the repository under the &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;MIT License&lt;/a&gt;. See the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/magentic-ui/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;Microsoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries. The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks. Microsoft&#39;s general trademark guidelines can be found at &lt;a href=&#34;http://go.microsoft.com/fwlink/?LinkID=254653&#34;&gt;http://go.microsoft.com/fwlink/?LinkID=254653&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt; &#xA;&lt;p&gt;Privacy information can be found at &lt;a href=&#34;https://go.microsoft.com/fwlink/?LinkId=521839&#34;&gt;https://go.microsoft.com/fwlink/?LinkId=521839&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents, or trademarks, whether by implication, estoppel, or otherwise.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>budtmo/docker-android</title>
    <updated>2025-08-16T01:32:03Z</updated>
    <id>tag:github.com,2025-08-16:/budtmo/docker-android</id>
    <link href="https://github.com/budtmo/docker-android" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Android in docker solution with noVNC supported and video recording&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img id=&#34;header&#34; src=&#34;https://raw.githubusercontent.com/budtmo/docker-android/master/images/logo_docker-android.png&#34; /&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://paypal.me/budtmo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/paypal-donate-blue.svg?sanitize=true&#34; alt=&#34;Paypal Donate&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/budtmo/docker-android&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/budtmo/docker-android/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/budtmo/docker-android?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/budtmo/docker-android.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/budtmo/docker-android&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/budtmo/docker-android/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/budtmo/docker-android.svg?sanitize=true&#34; alt=&#34;GitHub release&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Docker-Android is a docker image built to be used for everything related to Android. It can be used for Application development and testing (native, web and hybrid-app).&lt;/p&gt; &#xA;&lt;h2&gt;Advantages of using this project&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Emulator with different device profile and skins, such as Samsung Galaxy S6, LG Nexus 4, HTC Nexus One and more.&lt;/li&gt; &#xA; &lt;li&gt;Support vnc to be able to see what happen inside docker container&lt;/li&gt; &#xA; &lt;li&gt;Support log sharing feature where all logs can be accessed from web-UI&lt;/li&gt; &#xA; &lt;li&gt;Ability to control emulator from outside container by using adb connect&lt;/li&gt; &#xA; &lt;li&gt;Integrated with other cloud solutions, e.g. &lt;a href=&#34;https://www.genymotion.com/cloud/&#34;&gt;Genymotion Cloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;It can be used to build Android project&lt;/li&gt; &#xA; &lt;li&gt;It can be used to run unit and UI-Test with different test-frameworks, e.g. Appium, Espresso, etc.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;List of Docker-Images&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Android&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;API&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Image with latest release version&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Image with specific release version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;budtmo/docker-android:emulator_9.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;budtmo/docker-android:emulator_9.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;budtmo/docker-android:emulator_10.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;budtmo/docker-android:emulator_10.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;budtmo/docker-android:emulator_11.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;budtmo/docker-android:emulator_11.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;12.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;budtmo/docker-android:emulator_12.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;budtmo/docker-android:emulator_12.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;33&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;budtmo/docker-android:emulator_13.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;budtmo/docker-android:emulator_13.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;34&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;budtmo/docker-android:emulator_14.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;budtmo/docker-android:emulator_14.0_&amp;lt;release_version&amp;gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;budtmo/docker-android:genymotion&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;budtmo/docker-android:genymotion_&amp;lt;release_version&amp;gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;List of Devices&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Device Name&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phone&lt;/td&gt; &#xA;   &lt;td&gt;Samsung Galaxy S10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phone&lt;/td&gt; &#xA;   &lt;td&gt;Samsung Galaxy S9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phone&lt;/td&gt; &#xA;   &lt;td&gt;Samsung Galaxy S8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phone&lt;/td&gt; &#xA;   &lt;td&gt;Samsung Galaxy S7 Edge&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phone&lt;/td&gt; &#xA;   &lt;td&gt;Samsung Galaxy S7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phone&lt;/td&gt; &#xA;   &lt;td&gt;Samsung Galaxy S6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phone&lt;/td&gt; &#xA;   &lt;td&gt;Nexus 4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phone&lt;/td&gt; &#xA;   &lt;td&gt;Nexus 5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phone&lt;/td&gt; &#xA;   &lt;td&gt;Nexus One&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phone&lt;/td&gt; &#xA;   &lt;td&gt;Nexus S&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tablet&lt;/td&gt; &#xA;   &lt;td&gt;Nexus 7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tablet&lt;/td&gt; &#xA;   &lt;td&gt;Pixel C&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Docker is installed on your system.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;If you use &lt;em&gt;&lt;strong&gt;Ubuntu OS&lt;/strong&gt;&lt;/em&gt; on your host machine, you can skip this step. For &lt;em&gt;&lt;strong&gt;OSX&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;Windows OS&lt;/strong&gt;&lt;/em&gt; user, you need to use Virtual Machine that support Virtualization with Ubuntu OS because the image can be run under &lt;em&gt;&lt;strong&gt;Ubuntu OS only&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Your machine should support virtualization. To check if the virtualization is enabled is:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt install cpu-checker&#xA;kvm-ok&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run Docker-Android container&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 6080:6080 -e EMULATOR_DEVICE=&#34;Samsung Galaxy S10&#34; -e WEB_VNC=true --device /dev/kvm --name android-container budtmo/docker-android:emulator_11.0&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;http://localhost:6080&#34;&gt;http://localhost:6080&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt; to see inside running container.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To check the status of the emulator&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker exec -it android-container cat device_status&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Persisting data&lt;/h2&gt; &#xA;&lt;p&gt;The default behaviour is to destroy the emulated device on container restart. To persist data, you need to mount a volume at &lt;code&gt;/home/androidusr&lt;/code&gt;: &lt;code&gt;docker run -v data:/home/androidusr budtmo/docker-android:emulator_11.0&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;WSL2 Hardware acceleration (Windows 11 only)&lt;/h2&gt; &#xA;&lt;p&gt;Credit goes to &lt;a href=&#34;https://www.paralint.com/2022/11/find-new-modified-and-unversioned-subversion-files-on-windows&#34;&gt;Guillaume - The Parallel Interface blog&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/windows/wsl/wsl-config&#34;&gt;Microsoft - Advanced settings configuration in WSL&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Add yourself to the &lt;code&gt;kvm&lt;/code&gt; usergroup.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo usermod -a -G kvm ${USER}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add necessary flags to &lt;code&gt;/etc/wsl2.conf&lt;/code&gt; to their respective sections.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[boot]&#xA;command = /bin/bash -c &#39;chown -v root:kvm /dev/kvm &amp;amp;&amp;amp; chmod 660 /dev/kvm&#39;&#xA;&#xA;[wsl2]&#xA;nestedVirtualization=true&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Restart WSL2 via CMD prompt or Powershell&lt;/p&gt; &lt;pre&gt;&lt;code&gt;wsl --shutdown&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;code&gt;command = /bin/bash -c &#39;chown -v root:kvm /dev/kvm &amp;amp;&amp;amp; chmod 660 /dev/kvm&#39;&lt;/code&gt; sets &lt;code&gt;/dev/kvm&lt;/code&gt; to &lt;code&gt;kvm&lt;/code&gt; usergroup rather than the default &lt;code&gt;root&lt;/code&gt; usergroup on WSL2 startup.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;nestedVirtualization&lt;/code&gt; flag is only available to Windows 11.&lt;/p&gt; &#xA;&lt;h2&gt;Use-Cases&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_BUILD_ANDROID_PROJECT.md&#34;&gt;Build Android project&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_APPIUM.md&#34;&gt;UI-Test with Appium&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_CONTROL_EMULATOR.md&#34;&gt;Control Android emulator on host machine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_SMS.md&#34;&gt;SMS Simulation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_JENKINS.md&#34;&gt;Jenkins&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/USE_CASE_CLOUD.md&#34;&gt;Deploying on cloud (Azure, AWS, GCP)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Custom-Configurations&lt;/h2&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/CUSTOM_CONFIGURATIONS.md&#34;&gt;document&lt;/a&gt; contains information about configurations that can be used to enable some features, e.g. log-sharing, etc.&lt;/p&gt; &#xA;&lt;h2&gt;Genymotion&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img id=&#34;geny&#34; src=&#34;https://raw.githubusercontent.com/budtmo/docker-android/master/images/logo_genymotion_and_dockerandroid.png&#34; /&gt; &lt;/p&gt; &#xA;&lt;p&gt;For you who do not have ressources to maintain the simulator or to buy machines or need different device profiles, you can give a try by using &lt;a href=&#34;https://cloud.geny.io/&#34;&gt;Genymotion SAAS&lt;/a&gt;. Docker-Android is &lt;a href=&#34;https://www.genymotion.com/blog/partner_tag/docker/&#34;&gt;integrated with Genymotion&lt;/a&gt; on different cloud services, e.g. Genymotion SAAS, AWS, GCP, Alibaba Cloud. Please follow &lt;a href=&#34;https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/THIRD_PARTY_GENYMOTION.md&#34;&gt;this document&lt;/a&gt; for more detail.&lt;/p&gt; &#xA;&lt;h2&gt;Emulator Skins&lt;/h2&gt; &#xA;&lt;p&gt;The Emulator skins are taken from &lt;a href=&#34;https://developer.android.com/studio&#34;&gt;Android Studio IDE&lt;/a&gt; and &lt;a href=&#34;https://developer.samsung.com/&#34;&gt;Samsung Developer Website&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;USERS&lt;/h2&gt; &#xA;&lt;a href=&#34;https://lookerstudio.google.com/s/iGaemHJqQvg&#34;&gt; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/budtmo/docker-android/master/images/docker-android_users.png&#34; alt=&#34;docker-android-users&#34; width=&#34;800&#34; height=&#34;600&#34; /&gt; &lt;/p&gt; &lt;/a&gt; &#xA;&lt;h2&gt;PRO VERSION&lt;/h2&gt; &#xA;&lt;p&gt;Due to high requests for help and to be able to actively maintain the projects, the creator has decided to create docker-android-pro. Docker-Android-Pro is a sponsor based project which mean that the docker image of pro-version can be pulled only by &lt;a href=&#34;https://github.com/sponsors/budtmo&#34;&gt;active sponsor&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The differences between normal version and pro version are:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Feature&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Normal&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Pro&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Comment&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;user-behavior-analytics&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;proxy&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Set up company proxy on Android emulator on fly&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;language&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Set up language on Android emulator on fly&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Newer Android version&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Support other newer Android version e.g. Android 15, Android 16, etc&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;root-privileged&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Able to run command with security privileged&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;headless-mode&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Save resources by using headless mode&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Selenium 4.x integration&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Running Appium UI-Tests againt one (Selenium Hub) endpoint for Android- and iOS emulator(s) / device(s)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;multiple Android-Simulators&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yes (soon)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Save resources by having multiple Android-Simulators on one docker-container&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Google Play Store&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yes (soon)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Video Recording&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yes (soon)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Helpful for debugging&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://raw.githubusercontent.com/budtmo/docker-android/master/documentations/DOCKER-ANDROID-PRO.md&#34;&gt;document&lt;/a&gt; contains detail information about how to use docker-android-pro.&lt;/p&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/budtmo/docker-android/master/LICENSE.md&#34;&gt;License&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>