<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-24T01:30:07Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>devfullcycle/imersao13</title>
    <updated>2023-06-24T01:30:07Z</updated>
    <id>tag:github.com,2023-06-24:/devfullcycle/imersao13</id>
    <link href="https://github.com/devfullcycle/imersao13" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>s0md3v/sd-webui-roop</title>
    <updated>2023-06-24T01:30:07Z</updated>
    <id>tag:github.com,2023-06-24:/s0md3v/sd-webui-roop</id>
    <link href="https://github.com/s0md3v/sd-webui-roop" rel="alternate"></link>
    <summary type="html">&lt;p&gt;roop extension for StableDiffusion web-ui&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;roop for StableDiffusion&lt;/h1&gt; &#xA;&lt;p&gt;This is an extension for StableDiffusion&#39;s &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/&#34;&gt;AUTOMATIC1111 web-ui&lt;/a&gt; that allows face-replacement in images. It is based on &lt;a href=&#34;https://github.com/s0md3v/roop&#34;&gt;roop&lt;/a&gt; but will be developed seperately.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/s0md3v/sd-webui-roop/main/example/example.png&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Disclaimer&lt;/h3&gt; &#xA;&lt;p&gt;This software is meant to be a productive contribution to the rapidly growing AI-generated media industry. It will help artists with tasks such as animating a custom character or using the character as a model for clothing etc.&lt;/p&gt; &#xA;&lt;p&gt;The developers of this software are aware of its possible unethical applicaitons and are committed to take preventative measures against them. It has a built-in check which prevents the program from working on inappropriate media. We will continue to develop this project in the positive direction while adhering to law and ethics. This project may be shut down or include watermarks on the output if requested by law.&lt;/p&gt; &#xA;&lt;p&gt;Users of this software are expected to use this software responsibly while abiding the local law. If face of a real person is being used, users are suggested to get consent from the concerned person and clearly mention that it is a deepfake when posting content online. Developers of this software will not be responsible for actions of end-users.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;First of all, if you can&#39;t install it for some reason, don&#39;t open an issue here. Google your errors.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;On Windows, download and install &lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34;&gt;Visual Studio&lt;/a&gt;. During the install, make sure to include the Python and C++ packages.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run this command: &lt;code&gt;pip install insightface==0.7.3&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;In web-ui, go to the &#34;Extensions&#34; tab and use this URL &lt;code&gt;https://github.com/s0md3v/sd-webui-roop&lt;/code&gt; in the &#34;install from URL&#34; tab.&lt;/li&gt; &#xA; &lt;li&gt;Close webui and run it again&lt;/li&gt; &#xA; &lt;li&gt;If you encounter &lt;code&gt;&#39;NoneType&#39; object has no attribute &#39;get&#39;&lt;/code&gt; error, download the &lt;a href=&#34;https://huggingface.co/henryruhs/roop/resolve/main/inswapper_128.onnx&#34;&gt;inswapper_128.onnx&lt;/a&gt; model and put it inside &lt;code&gt;&amp;lt;webui_dir&amp;gt;/models/roop/&lt;/code&gt; directory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For rest of the errors, use google. Good luck.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Under &#34;roop&#34; drop-down menu, import an image containing a face.&lt;/li&gt; &#xA; &lt;li&gt;Turn on the &#34;Enable&#34; checkbox&lt;/li&gt; &#xA; &lt;li&gt;That&#39;s it, now the generated result will have the face you selected&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Tips&lt;/h2&gt; &#xA;&lt;h4&gt;Getting good quality results&lt;/h4&gt; &#xA;&lt;p&gt;First of all, make sure the &#34;Restore Face&#34; option is enabled. You can also try the &#34;Upscaler&#34; option or for more finer control, use an upscaler from the &#34;Extras&#34; tab.&lt;/p&gt; &#xA;&lt;p&gt;For even better quality, use img2img with denoise set to &lt;code&gt;0.1&lt;/code&gt; and gradually increase it until you get a balance of quality and resembelance.&lt;/p&gt; &#xA;&lt;h4&gt;Replacing specific faces&lt;/h4&gt; &#xA;&lt;p&gt;If there are multiple faces in an image, select the face numbers you wish to swap using the &#34;Comma separated face number(s)&#34; option.&lt;/p&gt; &#xA;&lt;h4&gt;The face didn&#39;t get swapped?&lt;/h4&gt; &#xA;&lt;p&gt;Did you click &#34;Enable&#34;?&lt;/p&gt; &#xA;&lt;p&gt;If you did and your console doesn&#39;t show any errors, it means roop detected that your image is either NSFW or wasn&#39;t able to detect a face at all.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>vllm-project/vllm</title>
    <updated>2023-06-24T01:30:07Z</updated>
    <id>tag:github.com,2023-06-24:/vllm-project/vllm</id>
    <link href="https://github.com/vllm-project/vllm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A high-throughput and memory-efficient inference and serving engine for LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png&#34;&gt; &#xA;  &lt;img alt=&#34;vLLM&#34; src=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png&#34; width=&#34;55%&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; Easy, fast, and cheap LLM serving for everyone &lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://vllm.readthedocs.io/en/latest/&#34;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://vllm.ai&#34;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/vllm-project/vllm/discussions&#34;&gt;&lt;b&gt;Discussions&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; ðŸ”¥&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/06] We officially released vLLM! vLLM has powered &lt;a href=&#34;https://chat.lmsys.org&#34;&gt;LMSYS Vicuna and Chatbot Arena&lt;/a&gt; since mid April. Check out our &lt;a href=&#34;https://vllm.ai&#34;&gt;blog post&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;vLLM is a fast and easy-to-use library for LLM inference and serving.&lt;/p&gt; &#xA;&lt;p&gt;vLLM is fast with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;State-of-the-art serving throughput&lt;/li&gt; &#xA; &lt;li&gt;Efficient management of attention key and value memory with &lt;strong&gt;PagedAttention&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Dynamic batching of incoming requests&lt;/li&gt; &#xA; &lt;li&gt;Optimized CUDA kernels&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;vLLM is flexible and easy to use with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Seamless integration with popular HuggingFace models&lt;/li&gt; &#xA; &lt;li&gt;High-throughput serving with various decoding algorithms, including &lt;em&gt;parallel sampling&lt;/em&gt;, &lt;em&gt;beam search&lt;/em&gt;, and more&lt;/li&gt; &#xA; &lt;li&gt;Tensor parallelism support for distributed inference&lt;/li&gt; &#xA; &lt;li&gt;Streaming outputs&lt;/li&gt; &#xA; &lt;li&gt;OpenAI-compatible API server&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;vLLM seamlessly supports many Huggingface models, including the following architectures:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPT-2 (&lt;code&gt;gpt2&lt;/code&gt;, &lt;code&gt;gpt2-xl&lt;/code&gt;, etc.)&lt;/li&gt; &#xA; &lt;li&gt;GPT BigCode (&lt;code&gt;bigcode/starcoder&lt;/code&gt;, &lt;code&gt;bigcode/gpt_bigcode-santacoder&lt;/code&gt;, etc.)&lt;/li&gt; &#xA; &lt;li&gt;GPT-NeoX (&lt;code&gt;EleutherAI/gpt-neox-20b&lt;/code&gt;, &lt;code&gt;databricks/dolly-v2-12b&lt;/code&gt;, &lt;code&gt;stabilityai/stablelm-tuned-alpha-7b&lt;/code&gt;, etc.)&lt;/li&gt; &#xA; &lt;li&gt;LLaMA (&lt;code&gt;lmsys/vicuna-13b-v1.3&lt;/code&gt;, &lt;code&gt;young-geng/koala&lt;/code&gt;, &lt;code&gt;openlm-research/open_llama_13b&lt;/code&gt;, etc.)&lt;/li&gt; &#xA; &lt;li&gt;OPT (&lt;code&gt;facebook/opt-66b&lt;/code&gt;, &lt;code&gt;facebook/opt-iml-max-30b&lt;/code&gt;, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Install vLLM with pip or &lt;a href=&#34;https://vllm.readthedocs.io/en/latest/getting_started/installation.html#build-from-source&#34;&gt;from source&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install vllm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Visit our &lt;a href=&#34;https://vllm.readthedocs.io/en/latest/&#34;&gt;documentation&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vllm.readthedocs.io/en/latest/getting_started/installation.html&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vllm.readthedocs.io/en/latest/models/supported_models.html&#34;&gt;Supported Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;vLLM outperforms HuggingFace Transformers (HF) by up to 24x and Text Generation Inference (TGI) by up to 3.5x, in terms of throughput. For details, check out our &lt;a href=&#34;https://vllm.ai&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/figures/perf_a10g_n1_dark.png&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/figures/perf_a10g_n1_light.png&#34; width=&#34;45%&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/figures/perf_a100_n1_dark.png&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/figures/perf_a100_n1_light.png&#34; width=&#34;45%&#34;&gt; &#xA; &lt;/picture&gt; &lt;br&gt; &lt;em&gt; Serving throughput when each request asks for 1 output completion. &lt;/em&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/figures/perf_a10g_n3_dark.png&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/figures/perf_a10g_n3_light.png&#34; width=&#34;45%&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/figures/perf_a100_n3_dark.png&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/figures/perf_a100_n3_light.png&#34; width=&#34;45%&#34;&gt; &#xA; &lt;/picture&gt; &lt;br&gt; &lt;em&gt; Serving throughput when each request asks for 3 output completions. &lt;/em&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome and value any contributions and collaborations. Please check out &lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/vllm/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for how to get involved.&lt;/p&gt;</summary>
  </entry>
</feed>