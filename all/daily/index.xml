<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-12T02:18:52Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Moonvy/OpenPromptStudio</title>
    <updated>2023-04-12T02:18:52Z</updated>
    <id>tag:github.com,2023-04-12:/Moonvy/OpenPromptStudio</id>
    <link href="https://github.com/Moonvy/OpenPromptStudio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🥣 AIGC 提示词可视化编辑器&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🥣 OPS/OpenPromptStudio&lt;/h1&gt; &#xA;&lt;h2&gt;提示词工作室 | 可视化编辑提示词&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;1430&#34; alt=&#34;OPS-cover&#34; src=&#34;https://user-images.githubusercontent.com/82231420/230757122-5cf5659e-9e1a-4288-80fd-84ec229a063e.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://moonvy.com/apps/ops/&#34;&gt;&lt;strong&gt;🥣 立即试试&lt;/strong&gt; moonvy.com/apps/ops/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;这是一个旨在把 AIGC 提示词（现在支持 Midjourney）可视化并提供编辑功能的工具，有以下特性&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;显示英文提示词的中文翻译&lt;/li&gt; &#xA; &lt;li&gt;翻译输入的中文提示词到英文（因为 Midjourney 仅支持英文提示词）&lt;/li&gt; &#xA; &lt;li&gt;为提示词进行分类（普通、样式、质量、命令）&lt;/li&gt; &#xA; &lt;li&gt;轻松的排序、隐藏提示词&lt;/li&gt; &#xA; &lt;li&gt;把提示词可视化结果导出为图片&lt;/li&gt; &#xA; &lt;li&gt;常用提示词词典&lt;/li&gt; &#xA; &lt;li&gt;通过 Notion 管理提示词词典&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;使用教程&lt;/h2&gt; &#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV15N411P7D3/?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=1f6edbc8e03c44932da52d02c0c11c1c&#34; target=&#34;_blank&#34;&gt; &lt;img width=&#34;300&#34; alt=&#34;OPS-cover&#34; src=&#34;https://user-images.githubusercontent.com/82231420/230757939-dde301f1-bf68-4455-83c6-f7dd2214c68b.png&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV15N411P7D3/?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=1f6edbc8e03c44932da52d02c0c11c1c&#34;&gt;📺 B 站视频教程&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;如何连接的我的 Noiton 来管理自己的词典&lt;/h2&gt; &#xA;&lt;p&gt;OPS 支持使用 &lt;a href=&#34;https://www.notion.so/&#34;&gt;Noiton&lt;/a&gt; 来管理自己的词典，使用 Notion 管理相对简单，可自定义程度也很高。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Moonvy/OpenPromptStudio/master/doc/assets/notion-me.gif&#34; alt=&#34; &#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1. 复制「演示-AIGC 提示词库」&lt;/h3&gt; &#xA;&lt;p&gt;复制我们的演示文档的自己的 Notion 工作区中&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://moonvy.notion.site/b768c5c1852f4e2fbaee1b4a99f26d49?v=346e91e8114648c59079eeea2d9d56c7&#34;&gt;&lt;strong&gt;📕 演示-AIGC 提示词库&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;720&#34; src=&#34;https://raw.githubusercontent.com/Moonvy/OpenPromptStudio/master/doc/assets/notion-demo.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;保持表头定义： &lt;code&gt;text&lt;/code&gt;, &lt;code&gt;subType&lt;/code&gt;、&lt;code&gt;dir&lt;/code&gt;、&lt;code&gt;lang_zh&lt;/code&gt; 不要变（或者你可以新建一个 Notion 数据库，只要有这些表头 OPS 就能连接的这个数据库）&lt;/p&gt; &#xA;&lt;h4&gt;Notion 表头定义&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;表头&lt;/th&gt; &#xA;   &lt;th&gt;作用&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;text&lt;/td&gt; &#xA;   &lt;td&gt;提示词原文（不区分大小写）&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lang_zh&lt;/td&gt; &#xA;   &lt;td&gt;对应的中文翻译&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;subType&lt;/td&gt; &#xA;   &lt;td&gt;提示词在 OPS 中的分类（&lt;code&gt;普通&lt;/code&gt;、&lt;code&gt;风格&lt;/code&gt;、&lt;code&gt;质量&lt;/code&gt;、&lt;code&gt;命令&lt;/code&gt;）&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;dir&lt;/td&gt; &#xA;   &lt;td&gt;词典中的分类，子分类用&lt;code&gt;/&lt;/code&gt;分隔如：&lt;code&gt;风格/绘画风格&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;alias&lt;/td&gt; &#xA;   &lt;td&gt;别名，可以有多个，用&lt;code&gt;,&lt;/code&gt; 分隔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;2. 创建自己的 Noiton 集成插件（integrations）&lt;/h3&gt; &#xA;&lt;p&gt;要让 OPS 连接到自己的 Notion 数据库，需要创建一个自己的集成（integrations）。OPS 会通过此集成的权限连接到你的数据库。&lt;/p&gt; &#xA;&lt;h4&gt;2.1 打开集成开发页面&lt;/h4&gt; &#xA;&lt;p&gt;打开 Notion 的集成开发页面 &lt;a href=&#34;https://www.notion.so/my-integrations&#34;&gt;🔗 www.notion.so/my-integrations&lt;/a&gt;&lt;br&gt; 点击 「+ new integrations」按钮创建一个新集成插件&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;720&#34; src=&#34;https://raw.githubusercontent.com/Moonvy/OpenPromptStudio/master/doc/assets/Myintegrations-1@2x.jpeg&#34;&gt; &lt;/p&gt; &#xA;&lt;h4&gt;2.2 创建集成插件&lt;/h4&gt; &#xA;&lt;p&gt;在集成插件页面中选择允许访问的 Notion 工作区（Workspace），你的 Notion 数据库需要创建在此工作区下，OPS 才能通过集成插件访问。&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;720&#34; src=&#34;https://raw.githubusercontent.com/Moonvy/OpenPromptStudio/master/doc/assets/Myintegrations-2@2x.jpeg&#34;&gt; &lt;/p&gt; &#xA;&lt;h4&gt;2.3 获取集成插件 Token 密钥&lt;/h4&gt; &#xA;&lt;p&gt;集成插件创建完毕后，复制 Token 秘钥保存下来，你将使用此 Token 作为访问凭证，请妥善保管不要在公开场合泄露。&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;720&#34; src=&#34;https://raw.githubusercontent.com/Moonvy/OpenPromptStudio/master/doc/assets/Myintegrations-3@2x.jpeg&#34;&gt; &lt;/p&gt; &#xA;&lt;h4&gt;2.4 在数据库页面链接到你的集成&lt;/h4&gt; &#xA;&lt;p&gt;集成插件创建后，还需要在你的 Notion 数据库的菜单中连接到你的集成插件：&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;720&#34; src=&#34;https://user-images.githubusercontent.com/82231420/230757501-7630d405-adcc-4611-aa8a-07875ce5a932.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;3. 在 OPS 中配置 Notion&lt;/h3&gt; &#xA;&lt;p&gt;在 OPS 右上角打开提示词词典，鼠标放在「连接我的 Noiton」按钮上，展开设置面板&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;「Integrations Token」 里面填入前面我们生成的集成 Token 秘钥（秘钥只会保存在浏览器本地（localStorage），不会被上传到任何地方）&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;「Database ID」里粘贴你 Notion 数据库的访问地址&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;然后点击「载入」按钮&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;720&#34; src=&#34;https://user-images.githubusercontent.com/82231420/230758301-57f5304e-b83b-4ee6-a91c-0c030e84213a.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h4&gt;获取 Notion 数据库的访问地址（&lt;code&gt;DatabaseID&lt;/code&gt;）&lt;/h4&gt; &#xA;&lt;p&gt;在 Notion 数据库菜单中点击 「Copy link to view」 就可以了，粘贴 Notion 数据库地址到 OPS 的配置输入框后会自动提取 &lt;code&gt;DatabaseID&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;720&#34; src=&#34;https://user-images.githubusercontent.com/82231420/230758271-c2ee8ba3-e694-45db-a209-55c4d1744171.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;更好的体验&lt;/h2&gt; &#xA;&lt;p&gt;你可以在 &lt;a href=&#34;https://moonvy.com/zeroG/&#34;&gt;zeroG 浏览器&lt;/a&gt; 里让 OPS 与 Discord 在一个无限画布中使用，获得更好的体验&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Moonvy/OpenPromptStudio/master/doc%2Fassets%2F%E6%88%AA%E5%B1%8F2023-04-06%2015.51.23.png&#34; alt=&#34;截屏2023-04-06 15.51.23.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;开发者&lt;/h2&gt; &#xA;&lt;p&gt;本地运行需要 NodeJS 环境&lt;/p&gt; &#xA;&lt;p&gt;运行打开后访问 &lt;code&gt;localhost:12833/apps/ops/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;如果你不想安装 NodeJS 环境，可以使用 Docker 运行，参考 &lt;a href=&#34;https://github.com/Moonvy/OpenPromptStudio/tree/master/docker/&#34;&gt;./docker&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;如何修改默认提示词词典&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;在 &lt;a href=&#34;https://github.com/Moonvy/OpenPromptStudio/tree/master/data/src&#34;&gt;./data/src&lt;/a&gt; 中编辑 &lt;code&gt;.csv&lt;/code&gt; 文件，你可以用 Excel、Numbers 或者纯文本编辑器编辑。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;在 &lt;a href=&#34;https://www.notion.so/&#34;&gt;Notion&lt;/a&gt; 中编辑（&lt;a href=&#34;https://github.com/Moonvy/OpenPromptStudio/tree/master/data/src/notion/fromNotion.js&#34;&gt;./data/src/notion/fromNotion.js&lt;/a&gt; ）&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;翻译服务&lt;/h3&gt; &#xA;&lt;p&gt;由于可靠的翻译 API 都是收费的，请连接自己的翻译服务&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Instruction-Tuning-with-GPT-4/GPT-4-LLM</title>
    <updated>2023-04-12T02:18:52Z</updated>
    <id>tag:github.com,2023-04-12:/Instruction-Tuning-with-GPT-4/GPT-4-LLM</id>
    <link href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Instruction Tuning with GPT-4&lt;/h1&gt; &#xA;&lt;p&gt;Baolin Peng*, Chunyuan Li*, Pengcheng He*, Michel Galley, Jianfeng Gao (*Equal Contribution)&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://instruction-tuning-with-gpt-4.github.io/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2304.03277&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://instruction-tuning-with-gpt-4.github.io/images/gpt4llama_logo.png&#34; width=&#34;50%&#34;&gt; &lt;br&gt; Pronounced as &#34;GPT-4-LLM&#34; or &#34;GPT-for-LLM&#34;, image is generated by &lt;a href=&#34;https://gligen.github.io/&#34;&gt;GLIGEN&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the repo for the GPT-4-LLM, which aims to share data generated by GPT-4 for building an instruction-following LLMs with supervised learning and reinforcement learning. The repo contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;English Instruction-Following &lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/#data-release&#34;&gt;Data&lt;/a&gt; generated by GPT-4 using Alpaca prompts for fine-tuning LLMs.&lt;/li&gt; &#xA; &lt;li&gt;Chinese Instruction-Following &lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/#data-release&#34;&gt;Data&lt;/a&gt; generated by GPT-4 using Chinese prompts translated from Alpaca by ChatGPT.&lt;/li&gt; &#xA; &lt;li&gt;Comparison &lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/#data-release&#34;&gt;Data&lt;/a&gt; ranked by GPT-4 to train reward models.&lt;/li&gt; &#xA; &lt;li&gt;Answers on Unnatural Instructions &lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/#data-release&#34;&gt;Data&lt;/a&gt; from GPT-4 to quantify the gap between GPT-4 and instruction-tuned models at scale.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage and License Notices&lt;/strong&gt;: The data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/#data-release&#34;&gt;GPT-4 Data Release&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/#how-good-is-the-data&#34;&gt;How Good is the Data?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/#fine-tuning-with-the-data&#34;&gt;Fine-tuning with the Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/#collect-results-and-reproduce-figure-plots&#34;&gt;Reproduce Figure Plots&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;🔥&lt;/span&gt; News&lt;/h2&gt; &#xA;&lt;!--&#xA;* **[2023.04.07]** Data restored.&#xA;* **[2023.04.07]** &lt;span&gt;⚠&lt;/span&gt; We turn off the data downloading temporarily.&#xA;--&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.04.06]&lt;/strong&gt; Paper and data are released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Large Language Models (LLMs) have shown impressive generalization capabilities such as in-context-learning and chain-of-thoughts reasoning. To enable LLMs to follow natural language instructions and complete real-world tasks, researchers have been exploring methods of instruction-tuning of LLMs. To advance the state of the art of instruction-tuning for LLMs, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning.&lt;/p&gt; &#xA;&lt;h2&gt;Data Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json&#34;&gt;&lt;code&gt;alpaca_gpt4_data.json&lt;/code&gt;&lt;/a&gt; contains 52K instruction-following data generated by GPT-4 with prompts in Alpaca. This JSON file has the same format as Alpaca data, except the output is generated by GPT-4:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;instruction&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, describes the task the model should perform. Each of the 52K instructions is unique.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;input&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, optional context or input for the task.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;output&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, the answer to the instruction as generated by &lt;code&gt;GPT-4&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data_zh.json&#34;&gt;&lt;code&gt;alpaca_gpt4_data_zh.json&lt;/code&gt;&lt;/a&gt; contains 52K instruction-following data generated by GPT-4 with Alpaca prompts translated into Chinese by ChatGPT. This JSON file has the same format.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/comparision_data.json&#34;&gt;&lt;code&gt;comparision_data.json&lt;/code&gt;&lt;/a&gt; ranked responses from three models, including GPT-4, GPT-3.5 and OPT-IML by asking GPT-4 to rate the quality.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;user_input&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, prompts used for quering LLMs.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;completion_a&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, a model completion which is ranked higher than completion_b.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;completion_b&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, a different model completion which has a lower quality score.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/unnatural_instruction_gpt4_data.json&#34;&gt;&lt;code&gt;unnatural_instruction_gpt4_data.json&lt;/code&gt;&lt;/a&gt; contains 9K instruction-following data generated by GPT-4 with prompts in Unnatural Instruction. This JSON file has the same format as Alpaca data.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How Good is the Data&lt;/h2&gt; &#xA;&lt;p&gt;Human evaluation was performed on model generation results using Amazon Mechanical Turk following Helpfulness, Honestness and Harmlessness criteria by &lt;a href=&#34;https://arxiv.org/abs/2112.00861&#34;&gt;Anthropic AI&lt;/a&gt;. The results are summarized as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Two instruction-tuned LLaMA models were compared, fine-tuned on data generated by GPT-4 and GPT-3 respectively.&lt;/li&gt; &#xA; &lt;li&gt;LLaMA-GPT-4 performs substantially better than LLaMA-GPT-3 in the &#34;Helpfulness&#34; criterion.&lt;/li&gt; &#xA; &lt;li&gt;LLaMA-GPT-4 performs similarly to the original GPT-4 in all three criteria, suggesting a promising direction for developing state-of-the-art instruction-following LLMs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/static/pie_llama_gpt3_vs_llam_gpt4.png&#34; alt=&#34;LLaMA-GPT4 vs Alpaca (i.e., LLaMA-GPT3)&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/static/pie_llama_gpt4_vs_gpt4.png&#34; alt=&#34;LLaMA-GPT4 vs GPT-4&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning with the data&lt;/h2&gt; &#xA;&lt;p&gt;We follow the same reciple to fine-tune LLaMA as Alpaca using standard Hugging Face training code.&lt;/p&gt; &#xA;&lt;p&gt;To reproduce our results with LLaMA 7B, first setup Alpaca repo and run the following CMDs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## cmd we used to train LLaMA on 16*V100&#xA;torchrun --nproc_per_node=16 &#xA;--master_port=12345 train.py &#xA;--model_name_or_path PATH/TO/LLaMA&#xA;--data_path ./data/alpaca_gpt4_data.json &#xA;--output_dir PATH/TO/SAVE&#xA;--num_train_epochs 3 &#xA;--per_device_train_batch_size 1 &#xA;--per_device_eval_batch_size 1 &#xA;--gradient_accumulation_steps 4 &#xA;--evaluation_strategy &#34;no&#34; &#xA;--save_strategy &#34;steps&#34; &#xA;--save_steps 200 &#xA;--save_total_limit 1 &#xA;--learning_rate 2e-5 &#xA;--weight_decay 0. &#xA;--warmup_ratio 0.03 &#xA;--lr_scheduler_type &#34;cosine&#34; &#xA;--logging_steps 1 &#xA;--deepspeed configs/ds_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To evaluate the results, we highly recommend users refer to &lt;a href=&#34;https://vicuna.lmsys.org/&#34;&gt;Vicuna&lt;/a&gt; as they have provided awesome serving scripts and evaluation piplelines.&lt;/p&gt; &#xA;&lt;h2&gt;Collect results and reproduce figure plots&lt;/h2&gt; &#xA;&lt;p&gt;The results can be plotted using the included IPython notebook plots/main_plots.ipynb. Start the IPython Notebook server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd plots&#xA;$ ipython notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Select the &lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/plots/main_plots.ipynb&#34;&gt;&lt;code&gt;main_plots.ipynb&lt;/code&gt;&lt;/a&gt; notebook and execute the included code. Note that without modification, we have copyed our extracted results into the notebook, and script will output figures in the paper. Some related data for plots have been provided in &lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/plots/data&#34;&gt;data&lt;/a&gt;, the generated plots are saved in &lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/plots/output&#34;&gt;plots/output&lt;/a&gt; If you&#39;ve run your own training and wish to plot results, you&#39;ll have to organize your results in the same format instead.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Shortcut: to skip all the work and just see the results, take a look at this notebook with &lt;a href=&#34;https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/plots/main_plots.ipynb&#34;&gt;cached plots&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{peng2023instruction,&#xA;  title={Instruction Tuning with GPT-4},&#xA;  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},&#xA;  journal={arXiv preprint arXiv:2304.03277},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo benefits from &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt;, &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Alpaca&lt;/a&gt;, and &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sdatkinson/neural-amp-modeler</title>
    <updated>2023-04-12T02:18:52Z</updated>
    <id>tag:github.com,2023-04-12:/sdatkinson/neural-amp-modeler</id>
    <link href="https://github.com/sdatkinson/neural-amp-modeler" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Neural network emulator for guitar amplifiers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NAM: neural amp modeler&lt;/h1&gt; &#xA;&lt;p&gt;This repository handles training, reamping, and exporting the weights of a model. For playing trained models in real time in a standalone application or plugin, see the partner repo, &lt;a href=&#34;https://github.com/sdatkinson/NeuralAmpModelerPlugin&#34;&gt;NeuralAmpModelerPlugin&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to use (Google Colab)&lt;/h2&gt; &#xA;&lt;p&gt;If you don&#39;t have a good computer for training ML models, you use Google Colab to train in the cloud using the pre-made notebooks under &lt;code&gt;bin\train&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For the very easiest experience, simply go to &lt;a href=&#34;https://colab.research.google.com/github/sdatkinson/neural-amp-modeler/blob/main/bin/train/easy_colab.ipynb&#34;&gt;https://colab.research.google.com/github/sdatkinson/neural-amp-modeler/blob/main/bin/train/easy_colab.ipynb&lt;/a&gt; and follow the steps!&lt;/p&gt; &#xA;&lt;p&gt;For a little more visibility under the hood, you can use &lt;a href=&#34;https://colab.research.google.com/github/sdatkinson/neural-amp-modeler/blob/main/bin/train/colab.ipynb&#34;&gt;colab.ipynb&lt;/a&gt; instead.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No local installation required!&lt;/li&gt; &#xA; &lt;li&gt;Decent GPUs are available if you don&#39;t have one on your computer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Uploading your data can take a long time.&lt;/li&gt; &#xA; &lt;li&gt;The session will time out after a few hours (for free accounts), so extended training runs aren&#39;t really feasible. Also, there&#39;s a usage limit so you can&#39;t hang out all day. I&#39;ve tried to set you up with a good model that should train reasonably quickly!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to use (Local)&lt;/h2&gt; &#xA;&lt;p&gt;Alternatively, you can clone this repo to your computer and use it locally.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Installation uses &lt;a href=&#34;https://www.anaconda.com/&#34;&gt;Anaconda&lt;/a&gt; for package management.&lt;/p&gt; &#xA;&lt;p&gt;For computers with a CUDA-capable GPU (recommended):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment_gpu.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Otherwise, for a CPU-only install (will train much more slowly):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment_cpu.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then activate the environment you&#39;ve created with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate nam&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Train models (GUI)&lt;/h3&gt; &#xA;&lt;p&gt;After installing, you can open a GUI trainer by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nam&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;from the terminal.&lt;/p&gt; &#xA;&lt;h3&gt;Train models (Python script)&lt;/h3&gt; &#xA;&lt;p&gt;For users looking to get more fine-grained control over the modeling process, NAM includes a training script that can be run from the terminal. In order to run it&lt;/p&gt; &#xA;&lt;h4&gt;Download audio files&lt;/h4&gt; &#xA;&lt;p&gt;Download the &lt;a href=&#34;https://drive.google.com/file/d/1v2xFXeQ9W2Ks05XrqsMCs2viQcKPAwBk/view?usp=share_link&#34;&gt;v1_1_1.wav&lt;/a&gt; and &lt;a href=&#34;https://drive.google.com/file/d/14w2utgL16NozmESzAJO_I0_VCt-5Wgpv/view?usp=share_link&#34;&gt;overdrive.wav&lt;/a&gt; to a folder of your choice&lt;/p&gt; &#xA;&lt;h4&gt;Update data configuration&lt;/h4&gt; &#xA;&lt;p&gt;Edit &lt;code&gt;bin/train/data/single_pair.json&lt;/code&gt; to point to relevant audio files&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;    &#34;common&#34;: {&#xA;        &#34;x_path&#34;: &#34;C:\\path\\to\\v1_1_1.wav&#34;,&#xA;        &#34;y_path&#34;: &#34;C:\\path\\to\\overdrive.wav&#34;,&#xA;        &#34;delay&#34;: 0&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Run training script&lt;/h4&gt; &#xA;&lt;p&gt;Open up a terminal. Activate your nam environment and call the training with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python bin/train/main.py \&#xA;bin/train/inputs/data/single_pair.json \&#xA;bin/train/inputs/models/demonet.json \&#xA;bin/train/inputs/learning/demo.json \&#xA;bin/train/outputs/MyAmp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;data/single_pair.json&lt;/code&gt; contains the information about the data you&#39;re training on&lt;br&gt; &lt;code&gt;models/demonet.json&lt;/code&gt; contains information about the model architecture that is being trained. The example used here uses a &lt;code&gt;feather&lt;/code&gt; configured &lt;code&gt;wavenet&lt;/code&gt;.&lt;br&gt; &lt;code&gt;learning/demo.json&lt;/code&gt; contains information about the training run itself (e.g. number of epochs).&lt;/p&gt; &#xA;&lt;p&gt;The configuration above runs a short (demo) training. For a real training you may prefer to run something like,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python bin/train/main.py \&#xA;bin/train/inputs/data/single_pair.json \&#xA;bin/train/inputs/models/wavenet.json \&#xA;bin/train/inputs/learning/default.json \&#xA;bin/train/outputs/MyAmp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a side note, NAM uses &lt;a href=&#34;https://lightning.ai/pages/open-source/&#34;&gt;PyTorch Lightning&lt;/a&gt; under the hood as a modeling framework, and you can control many of the Pytorch Lightning configuration options from &lt;code&gt;bin/train/inputs/learning/default.json&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Export a model (to use with &lt;a href=&#34;https://github.com/sdatkinson/NeuralAmpModelerPlugin&#34;&gt;the plugin&lt;/a&gt;)&lt;/h4&gt; &#xA;&lt;p&gt;Exporting the trained model to a &lt;code&gt;.nam&lt;/code&gt; file for use with the plugin can be done with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python bin/export.py \&#xA;path/to/config_model.json \&#xA;path/to/checkpoints/epoch=123_val_loss=0.000010.ckpt \&#xA;path/to/exported_models/MyAmp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, point the plugin at the exported &lt;code&gt;model.nam&lt;/code&gt; file and you&#39;re good to go!&lt;/p&gt; &#xA;&lt;h3&gt;Other utilities&lt;/h3&gt; &#xA;&lt;h4&gt;Run a model on an input signal (&#34;reamping&#34;)&lt;/h4&gt; &#xA;&lt;p&gt;Handy if you want to just check it out without needing to use the plugin:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python bin/run.py \&#xA;path/to/source.wav \&#xA;path/to/config_model.json \&#xA;path/to/checkpoints/epoch=123_val_loss=0.000010.ckpt \&#xA;path/to/output.wav&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>