<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-20T01:23:26Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>farhanashrafdev/90DaysOfCyberSecurity</title>
    <updated>2023-09-20T01:23:26Z</updated>
    <id>tag:github.com,2023-09-20:/farhanashrafdev/90DaysOfCyberSecurity</id>
    <link href="https://github.com/farhanashrafdev/90DaysOfCyberSecurity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains a 90-day cybersecurity study plan, along with resources and materials for learning various cybersecurity concepts and technologies. The plan is organized into daily tasks, covering topics such as Network+, Security+, Linux, Python, Traffic Analysis, Git, ELK, AWS, Azure, and Hacking. The repository also includes a `LEARN.md&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;90-Day Cybersecurity Study Plan&lt;/h1&gt; &#xA;&lt;h2&gt;Day 1-7: Network+&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Watch videos from Professor Messer&#39;s N10-008 Playlist: &lt;a href=&#34;https://youtube.com/playlist?list=PLG49S3nxzAnlCJiCrOYuRYb6cne864a7G&#34;&gt;https://youtube.com/playlist?list=PLG49S3nxzAnlCJiCrOYuRYb6cne864a7G&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Complete any related practice questions or exercises.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 8-14: Security+&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Watch videos from Professor Messer&#39;s SYO-601 Playlist: &lt;a href=&#34;https://youtube.com/playlist?list=PLG49S3nxzAnkL2ulFS3132mOVKuzzBxA8&#34;&gt;https://youtube.com/playlist?list=PLG49S3nxzAnkL2ulFS3132mOVKuzzBxA8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Complete any related practice questions or exercises.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 15-28: Linux&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow the tutorials on Ryan&#39;s Tutorials: &lt;a href=&#34;https://ryanstutorials.net/linuxtutorial/&#34;&gt;https://ryanstutorials.net/linuxtutorial/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Take the Linux course on EdX: &lt;a href=&#34;https://edx.org/learn/linux&#34;&gt;https://edx.org/learn/linux&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read through the Linux Documentation Project (LDP): &lt;a href=&#34;http://tldp.org&#34;&gt;http://tldp.org&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 29-42: Python&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Complete Codecademy&#39;s Learn Python Track: &lt;a href=&#34;https://codecademy.com/learn/learn-python&#34;&gt;https://codecademy.com/learn/learn-python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Follow SoloLearn&#39;s Python Tutorial: &lt;a href=&#34;https://sololearn.com/Course/Python/&#34;&gt;https://sololearn.com/Course/Python/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read &#34;Learn Python the Hard Way&#34;: &lt;a href=&#34;https://learnpythonthehardway.org&#34;&gt;https://learnpythonthehardway.org&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 43-56: Traffic Analysis&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Take the Wireshark University course: &lt;a href=&#34;https://wireshark.org/training/&#34;&gt;https://wireshark.org/training/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Follow the Wireshark Tutorial on guru99: &lt;a href=&#34;https://guru99.com/wireshark-tutorial.html&#34;&gt;https://guru99.com/wireshark-tutorial.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read through the TCPdump Tutorial on DanielMiessler: &lt;a href=&#34;https://danielmiessler.com/study/tcpdump/&#34;&gt;https://danielmiessler.com/study/tcpdump/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Watch the Suricata IDS/IPS System Tutorial on YouTube: &lt;a href=&#34;https://youtube.com/watch?v=DZl7mW8OvZg&#34;&gt;https://youtube.com/watch?v=DZl7mW8OvZg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read through the Suricata on pfSense guide: &lt;a href=&#34;https://doc.pfsense.org/index.php/Suricata&#34;&gt;https://doc.pfsense.org/index.php/Suricata&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 57-63: Git&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Complete Codecademy&#39;s Git for Beginners course: &lt;a href=&#34;https://codecademy.com/learn/learn-git&#34;&gt;https://codecademy.com/learn/learn-git&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Follow the Git Immersion tutorial: &lt;a href=&#34;http://gitimmersion.com&#34;&gt;http://gitimmersion.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Try Git: &lt;a href=&#34;https://try.github.io&#34;&gt;https://try.github.io&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 64-70: ELK&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow the ELK Stack Tutorial on Logz.io: &lt;a href=&#34;https://logz.io/learn/complete-elk-stack-tutorial/&#34;&gt;https://logz.io/learn/complete-elk-stack-tutorial/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Browse through the ELK Stack tutorials on Elastic: &lt;a href=&#34;https://elastic.co/learn/elastic-stack&#34;&gt;https://elastic.co/learn/elastic-stack&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 71-77: AWS&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Explore the AWS Getting Started Resource Center: &lt;a href=&#34;https://aws.amazon.com/getting-started/&#34;&gt;https://aws.amazon.com/getting-started/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Browse through the AWS Tutorials: &lt;a href=&#34;https://aws.amazon.com/tutorials/&#34;&gt;https://aws.amazon.com/tutorials/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 78-84: Azure&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read the Introduction to Microsoft Azure: &lt;a href=&#34;https://bowtiedcyber.substack.com/p/introduction-to-microsoft-azure&#34;&gt;https://bowtiedcyber.substack.com/p/introduction-to-microsoft-azure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Go through Azure Fundamentals: &lt;a href=&#34;https://docs.microsoft.com/en-us/learn/azure/&#34;&gt;https://docs.microsoft.com/en-us/learn/azure/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 85-90: Hacking&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Practice on Cyber Talents: &lt;a href=&#34;https://cybertalents&#34;&gt;https://cybertalents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Try to hack the challenges on Hack the Box: &lt;a href=&#34;https://hackthebox.com&#34;&gt;https://hackthebox.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Practice on vulnerable machines on Vulnhub: &lt;a href=&#34;https://vulnhub.com&#34;&gt;https://vulnhub.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 91-92: One Page Resume&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use the provided resume template: &lt;a href=&#34;https://bowtiedcyber.substack.com/p/killer-cyber-resume-part-ii&#34;&gt;https://bowtiedcyber.substack.com/p/killer-cyber-resume-part-ii&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Cybersecurity Resume Template from Hiration: &lt;a href=&#34;https://www.hiration.com/resume-templates/cybersecurity-resume-template/&#34;&gt;https://www.hiration.com/resume-templates/cybersecurity-resume-template/&lt;/a&gt; This template is specifically designed for cybersecurity professionals and includes sections for skills, certifications, and professional experience, along with a summary and a skills section.&lt;/li&gt; &#xA; &lt;li&gt;Cybersecurity Resume from MyPerfectResume: &lt;a href=&#34;https://www.myperfectresume.com/resume-templates/cybersecurity&#34;&gt;https://www.myperfectresume.com/resume-templates/cybersecurity&lt;/a&gt; This template is also designed for cybersecurity professionals and includes sections for skills, certifications, and experience, along with a summary, and a section for education.&lt;/li&gt; &#xA; &lt;li&gt;Cybersecurity Resume from Resume-Now: &lt;a href=&#34;https://www.resume-now.com/templates/cyber-security-resume&#34;&gt;https://www.resume-now.com/templates/cyber-security-resume&lt;/a&gt; This template also has sections for skills, certifications, and experience, along with a summary, and a section for education. It also includes a section for technical skills.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 93-95: Where and How to Apply&lt;/h2&gt; &#xA;&lt;p&gt;Search for jobs on Indeed: &lt;a href=&#34;https://indeed.com&#34;&gt;https://indeed.com&lt;/a&gt; Look for opportunities on LinkedIn: &lt;a href=&#34;https://linkedin.com&#34;&gt;https://linkedin.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NExT-GPT/NExT-GPT</title>
    <updated>2023-09-20T01:23:26Z</updated>
    <id>tag:github.com,2023-09-20:/NExT-GPT/NExT-GPT</id>
    <link href="https://github.com/NExT-GPT/NExT-GPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code and models for NExT-GPT: Any-to-Any Multimodal Large Language Model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/code/nextgpt.png&#34; style=&#34;width: 5%&#34;&gt; NExT-GPT: Any-to-Any Multimodal LLM&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chocowu.github.io/&#34;&gt;Shengqiong Wu&lt;/a&gt;, &lt;a href=&#34;http://haofei.vip/&#34;&gt;Hao Fei&lt;/a&gt;*, &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/#&#34;&gt;Leigang Qu&lt;/a&gt;, &lt;a href=&#34;https://jiwei0523.github.io/&#34;&gt;Wei Ji&lt;/a&gt;, and &lt;a href=&#34;https://www.chuatatseng.com/&#34;&gt;Tat-Seng Chua&lt;/a&gt;. (*Correspondence )&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.nextcenter.org/&#34;&gt;NExT++&lt;/a&gt;, School of Computing, National University of Singapore&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://next-gpt.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/#&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-Page-purple&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2309.05519&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-PDF-orange&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/License-BSD-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=aqw2SCWeWD0&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository hosts the code, data and model weight of &lt;strong&gt;NExT-GPT&lt;/strong&gt;, the first end-to-end MM-LLM that perceives input and generates output in arbitrary combinations (any-to-any) of text, image, video, and audio and beyond.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;🎉 News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; [2023.09.15] 🚀🚀 Release the code of NExT-GPT in version &lt;code&gt;7b_tiva_v0&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;👉 TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release checkpoints (projection layers).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release MosIT data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Updating NExT-GPT in more types&amp;amp;sizes of LLMs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Empowering NExT-GPT with more modalities of inputs&amp;amp;outputs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; ...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Example Demos&lt;/h2&gt; &#xA;&lt;p&gt;Here we showcase examples generated from NExT-GPT. For more examples, kindly visit the &lt;a href=&#34;https://next-gpt.github.io/&#34;&gt;webpage&lt;/a&gt;, or the online live &lt;a href=&#34;https://9f10951d8cbe53e698.gradio.live&#34;&gt;demo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NExT-GPT/NExT-GPT/assets/18722770/0c2b3d88-a533-4899-ab44-65580fe54538&#34;&gt;https://github.com/NExT-GPT/NExT-GPT/assets/18722770/0c2b3d88-a533-4899-ab44-65580fe54538&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NExT-GPT/NExT-GPT/assets/18722770/eb1319a6-38aa-4546-a96e-163207e7de93&#34;&gt;https://github.com/NExT-GPT/NExT-GPT/assets/18722770/eb1319a6-38aa-4546-a96e-163207e7de93&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NExT-GPT/NExT-GPT/assets/18722770/36bec0ad-9bad-4bcf-bc37-92b028f1bc6a&#34;&gt;https://github.com/NExT-GPT/NExT-GPT/assets/18722770/36bec0ad-9bad-4bcf-bc37-92b028f1bc6a&lt;/a&gt;&lt;/p&gt; &#xA;&lt;span id=&#34;introduction&#34;&gt;&lt;/span&gt; &#xA;&lt;h2&gt;Brief Introduction&lt;/h2&gt; &#xA;&lt;p&gt;NExt-GPT is built on top of existing pre-trained LLM, multimodal encoder and SoTA diffusion models, with sufficient end-to-end instruction tuning.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/figures/framework.png&#34; alt=&#34;Video-LLaMA&#34; style=&#34;width: 90%; min-width: 200px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multimodal Encoding Stage.&lt;/strong&gt; Leveraging established encoders to encode inputs in various modalities, where these representations are projected into language-like representations comprehensible to the LLM through a projection layer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM Understanding and Reasoning Stage.&lt;/strong&gt; Harnessing an existing open-sourced LLM as the core to process input information for semantic understanding and reasoning. The LLM not only directly generates text tokens but also produces unique “modality signal” tokens that serve as instructions to dictate the decoding layers whether &amp;amp; what modal content to output correspondingly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multimodal Generation Stage.&lt;/strong&gt; Receiving the multimodal signals with specific instructions from LLM (if any), the Transformer-based output projection layers map the signal token representations into the ones that are understandable to following multimodal decoders.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more technical details, kindly refer to the &lt;a href=&#34;https://arxiv.org/pdf/2309.05519.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;span id=&#34;Usage&#34;&gt;&lt;/span&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;span id=&#34;all_catelogue&#34;&gt;&lt;/span&gt; &#xA;&lt;h3&gt;Table of Contents:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;#Code Structure&#34;&gt;1. Code Structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;#Environment Preparation&#34;&gt;2. Environment Preparation &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;#Training on Your Own&#34;&gt;3. Training/Adapting NExt-GPT on Your Own&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;#Prepare Pre-trained Checkpoint&#34;&gt;3.1. Preparing Pre-trained Checkpoint&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;#Prepare Dataset&#34;&gt;3.2. Preparing Dataset &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;#Precompute Embeddings&#34;&gt;3.3. Precomputing Embeddings&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;#Train NExT-GPT&#34;&gt;3.4. Training NExT-GPT&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;#Run NExT-GPT System&#34;&gt;4. Running NExT-GPT System&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;#Prepare checkpoints&#34;&gt;4.1. Preparing checkpoints&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;#Deploy Demo System&#34;&gt;4.2. Deploying Demo System&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;span id=&#34;Code Structure&#34;&gt;&lt;/span&gt; &#xA;&lt;h3&gt;1. Code Structure&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;├── figures&#xA;├── data&#xA;│   ├── T-X_pair_data  &#xA;│   │   ├── audiocap                      # text-autio pairs data&#xA;│   │   │   ├── audios                    # audio files&#xA;│   │   │   └── audiocap.json             # the audio captions&#xA;│   │   ├── cc3m                          # text-image paris data&#xA;│   │   │   ├── images                    # image files&#xA;│   │   │   └── cc3m.json                 # the image captions&#xA;│   │   └── webvid                        # text-video pairs data&#xA;│   │   │   ├── videos                    # video files&#xA;│   │   │   └── webvid.json               # the video captions&#xA;│   ├── IT_data                           # instruction data&#xA;│   │   ├── T+X-T_data                    # text+[image/audio/video] to text instruction data&#xA;│   │   │   ├── alpaca                    # textual instruction data&#xA;│   │   │   ├── llava                     # visual instruction data&#xA;│   │   ├── T-T+X                         # synthesized text to text+[image/audio/video] instruction data&#xA;│   │   └── MosIT                         # Modality-switching Instruction Tuning instruction data&#xA;├── code&#xA;│   ├── config&#xA;│   │   ├── base.yaml                     # the model configuration &#xA;│   │   ├── stage_1.yaml                  # enc-side alignment training configuration&#xA;│   │   ├── stage_2.yaml                  # dec-side alignment training configuration&#xA;│   │   └── stage_3.yaml                  # instruction-tuning configuration&#xA;│   ├── dsconfig&#xA;│   │   ├── stage_1.json                  # deepspeed configuration for enc-side alignment training&#xA;│   │   ├── stage_2.json                  # deepspeed configuration for dec-side alignment training&#xA;│   │   └── stage_3.json                  # deepspeed configuration for instruction-tuning training&#xA;│   ├── datast&#xA;│   │   ├── base_dataset.py&#xA;│   │   ├── cc3m_datast.py                # process and load text-image pair dataset&#xA;│   │   ├── audiocap_datast.py            # process and load text-audio pair dataset&#xA;│   │   ├── webvid_dataset.py             # process and load text-video pair dataset&#xA;│   │   └── instruction_dataset.py        # process and load instruction pair dataset&#xA;│   ├── model                     &#xA;│   │   ├── ImageBind                     # the code from ImageBind Model&#xA;│   │   ├── common&#xA;│   │   ├── anyToImageVideoAudio.py       # the main model file&#xA;│   │   ├── agent.py&#xA;│   │   ├── modeling_llama.py&#xA;│   │   ├── custom_ad.py                  # the audio diffusion &#xA;│   │   ├── custom_sd.py                  # the image diffusion&#xA;│   │   ├── custom_vd.py                  # the video diffusion&#xA;│   │   ├── layers.py                     # the output projection layers&#xA;│   │   └── ...  &#xA;│   ├── scripts&#xA;│   │   ├── train.sh                      # training NExT-GPT script&#xA;│   │   └── app.sh                        # deploying demo script&#xA;│   ├── header.py&#xA;│   ├── process_embeddings.py             # precompute the captions embeddings&#xA;│   ├── train.py                          # training&#xA;│   ├── inference.py                      # inference&#xA;│   ├── demo_app.py                       # deploy Gradio demonstration &#xA;│   └── ...&#xA;├── ckpt                           &#xA;│   ├── delta_ckpt                        # tunable NExT-GPT params&#xA;│   │   ├── nextgpt         &#xA;│   │   │   ├── 7b_tiva_v0                # the directory to save the log file&#xA;│   │   │   │   ├── log                   # the logs&#xA;│   └── ...       &#xA;│   ├── pretrained_ckpt                   # frozen params of pretrained modules&#xA;│   │   ├── imagebind_ckpt&#xA;│   │   │   ├──huge                       # version&#xA;│   │   │   │   └──imagebind_huge.pth&#xA;│   │   ├── vicuna_ckpt&#xA;│   │   │   ├── 7b_v0                     # version&#xA;│   │   │   │   ├── config.json&#xA;│   │   │   │   ├── pytorch_model-00001-of-00002.bin&#xA;│   │   │   │   ├── tokenizer.model&#xA;│   │   │   │   └── ...&#xA;├── LICENCE.md&#xA;├── README.md&#xA;└── requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;span id=&#34;Environment Preparation&#34;&gt;&lt;/span&gt; &#xA;&lt;h3&gt;2. Environment Preparation &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/#all_catelogue&#34;&gt;[Back to Top]&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Please first clone the repo and install the required environment, which can be done by running the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -n nextgpt python=3.8&#xA;&#xA;conda activate nextgpt&#xA;&#xA;# CUDA 11.6&#xA;conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia&#xA;&#xA;git clone https://github.com/NExT-GPT/NExT-GPT.git&#xA;cd NExT-GPT&#xA;&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;span id=&#34;Training on Your Own&#34;&gt;&lt;/span&gt; &#xA;&lt;h3&gt;3. Training/Adapting NExt-GPT on Your Own&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;/h4&gt; &#xA;&lt;span id=&#34;Prepare Pre-trained Checkpoint&#34;&gt;&lt;/span&gt; &#xA;&lt;h4&gt;3.1. Preparing Pre-trained Checkpoint &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/#all_catelogue&#34;&gt;[Back to Top]&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;NExT-GPT is trained based on following excellent existing models. Please follow the instructions to prepare the checkpoints.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;ImageBind&lt;/code&gt; is the unified image/video/audio encoder. The pre-trained checkpoint can be downloaded from &lt;a href=&#34;https://dl.fbaipublicfiles.com/imagebind/imagebind_huge.pth&#34;&gt;here&lt;/a&gt; with version &lt;code&gt;huge&lt;/code&gt;. Afterward, put the &lt;code&gt;imagebind_huge.pth&lt;/code&gt; file at &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/ckpt/pretrained_ckpt/imagebind_ckpt/&#34;&gt;[./ckpt/pretrained_ckpt/imagebind_ckpt/huge]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Vicuna&lt;/code&gt;: first prepare the LLaMA by following the instructions &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/ckpt/pretrained_ckpt/prepare_vicuna.md&#34;&gt;[here]&lt;/a&gt;. Then put the pre-trained model at &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/ckpt/pretrained_ckpt/vicuna_ckpt/&#34;&gt;[./ckpt/pretrained_ckpt/vicuna_ckpt/]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Image Diffusion&lt;/code&gt; is used to generate images. NExT-GPT uses &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;Stable Diffusion&lt;/a&gt; with version &lt;code&gt; v1-5&lt;/code&gt;. (&lt;em&gt;will be automatically downloaded&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Audio Diffusion&lt;/code&gt; for producing audio content. NExT-GPT employs &lt;a href=&#34;https://github.com/haoheliu/AudioLDM&#34;&gt;AudioLDM&lt;/a&gt; with version &lt;code&gt;l-full&lt;/code&gt;. (&lt;em&gt;will be automatically downloaded&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Video Diffusion&lt;/code&gt; for the video generation. We employ &lt;a href=&#34;https://huggingface.co/cerspense/zeroscope_v2_576w&#34;&gt;ZeroScope&lt;/a&gt; with version &lt;code&gt;v2_576w&lt;/code&gt;. (&lt;em&gt;will be automatically downloaded&lt;/em&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;span id=&#34;Prepare Dataset&#34;&gt;&lt;/span&gt; &#xA;&lt;h4&gt;3.2. Preparing Dataset &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/#all_catelogue&#34;&gt;[Back to Top]&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Please download the following datasets used for model training:&lt;/p&gt; &#xA;&lt;p&gt;A) T-X pairs data&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;CC3M&lt;/code&gt; of &lt;em&gt;&lt;strong&gt;text-image&lt;/strong&gt;&lt;/em&gt; pairs, please follow this instruction &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/T-X_pair_data/cc3m/prepare.md&#34;&gt;[here]&lt;/a&gt;. Then put the data at &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/T-X_pair_data/cc3m&#34;&gt;[./data/T-X_pair_data/cc3m]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;WebVid&lt;/code&gt; of &lt;em&gt;&lt;strong&gt;text-video&lt;/strong&gt;&lt;/em&gt; pairs, see the &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/T-X_pair_data/webvid/prepare.md&#34;&gt;[instruction]&lt;/a&gt;. The file should be saved at &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/T-X_pair_data/webvid&#34;&gt;[./data/T-X_pair_data/webvid]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AudioCap&lt;/code&gt; of &lt;em&gt;&lt;strong&gt;text-audio&lt;/strong&gt;&lt;/em&gt; pairs, see the &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/T-X_pair_data/audiocap/prepare.md&#34;&gt;[instruction]&lt;/a&gt;. Save the data in &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/T-X_pair_data/audiocap&#34;&gt;[./data/T-X_pair_data/audiocap]&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;B) Instruction data&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;T+X-T&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;LLaVA&lt;/code&gt; of the &lt;em&gt;&lt;strong&gt;visual instruction data&lt;/strong&gt;&lt;/em&gt;, download it from &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Data.md&#34;&gt;here&lt;/a&gt;, and then put it at &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/IT_data/T+X-T_data/llava/&#34;&gt;[./data/IT_data/T+X-T_data/llava]&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Alpaca&lt;/code&gt; of the &lt;em&gt;&lt;strong&gt;textual instruction data&lt;/strong&gt;&lt;/em&gt;, download it from &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;here&lt;/a&gt;, and then put it at &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/IT_data/T+X-T_data/alpaca/&#34;&gt;[./data/IT_data/T+X-T_data/alpaca/]&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;VideoChat&lt;/code&gt;, download the &lt;em&gt;&lt;strong&gt;video instruction data&lt;/strong&gt;&lt;/em&gt; &lt;a href=&#34;https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data&#34;&gt;here&lt;/a&gt;, and then put it at &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/IT_data/T+X-T_data/videochat/&#34;&gt;[./data/IT_data/T+X-T_data/videochat/]&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;T-X+T&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Run the following commands to construct the data. Please ensure the above &lt;code&gt;T+X-T&lt;/code&gt; datasets are prepared. Afterward, the &lt;code&gt;T-X+T&lt;/code&gt; file &lt;code&gt;instruction_data.json&lt;/code&gt; will be saved at &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/IT_data/T-T+X_data&#34;&gt;[./data/IT_data/T-T+X_data]&lt;/a&gt;. &lt;pre&gt;&lt;code class=&#34;language-angular2html&#34;&gt;cd ./code/dataset/&#xA;python instruction_dataset.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;MosIT&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Download the file from &lt;a href=&#34;&#34;&gt;here&lt;/a&gt;, put them in &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/IT_data/MosIT_data/&#34;&gt;[./data/IT_data/MosIT_data/]&lt;/a&gt;. (&lt;em&gt;We are in the process of finalizing the data and handling the copyright issue. Will release later.&lt;/em&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;span id=&#34;Precompute Embeddings&#34;&gt;&lt;/span&gt; &#xA;&lt;h4&gt;3.3. Precomputing Embeddings &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/#all_catelogue&#34;&gt;[Back to Top]&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;In decoding-side alignment training, we minimize the distance between the representation of signal tokens and captions. To save costs of time and memory, we precompute the text embeddings for image, audio and video captions using the text encoder within the respective diffusion models.&lt;/p&gt; &#xA;&lt;p&gt;Please run this command before the following training of NExT-GPT, where the produced &lt;code&gt;embedding&lt;/code&gt; file will be saved at &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/embed&#34;&gt;[./data/embed]&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-angular2html&#34;&gt;cd ./code/&#xA;python process_embeddings.py ../data/T-X_pair_data/cc3m/cc3m.json image ../data/embed/ runwayml/stable-diffusion-v1-5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note of arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;args[1]: path of caption file;&lt;/li&gt; &#xA; &lt;li&gt;args[2]: modality, which can be &lt;code&gt;image&lt;/code&gt;, &lt;code&gt;video&lt;/code&gt;, and &lt;code&gt;audio&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;args[3]: saving path of embedding file;&lt;/li&gt; &#xA; &lt;li&gt;args[4]: corresponding pre-trained diffusion model name.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;span id=&#34;Train NExT-GPT&#34;&gt;&lt;/span&gt; &#xA;&lt;h4&gt;3.4. Training NExT-GPT &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/#all_catelogue&#34;&gt;[Back to Top]&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;First of all, please refer to the base configuration file &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/code/config/base.yaml&#34;&gt;[./code/config/base.yaml]&lt;/a&gt; for the basic system setting of overall modules.&lt;/p&gt; &#xA;&lt;p&gt;Then, the training of NExT-GPT starts with this script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-angular2html&#34;&gt;cd ./code&#xA;bash scripts/train.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Specifying the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-angular2html&#34;&gt;deepspeed --include localhost:0 --master_addr 127.0.0.1 --master_port 28459 train.py \&#xA;    --model nextgpt \&#xA;    --stage 1\&#xA;    --dataset cc3m\&#xA;    --data_path  ../data/T-X_pair_data/cc3m/cc3m.json\&#xA;    --mm_root_path ../data/T-X_pair_data/cc3m/images/\&#xA;    --embed_path ../data/embed/\&#xA;    --save_path  ../ckpt/delta_ckpt/nextgpt/7b/\&#xA;    --log_path ../ckpt/delta_ckpt/nextgpt/7b/log/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where the key arguments are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--include&lt;/code&gt;: &lt;code&gt;localhost:0&lt;/code&gt; indicating the GPT cuda number &lt;code&gt;0&lt;/code&gt; of deepspeed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--stage&lt;/code&gt;: training stage.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--dataset&lt;/code&gt;: the dataset name for training model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--data_path&lt;/code&gt;: the data path for the training file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--mm_root_path&lt;/code&gt;: the data path for the image/video/audio file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--embed_path&lt;/code&gt;: the data path for the text embedding file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--save_path&lt;/code&gt;: the directory which saves the trained delta weights. This directory will be automatically created.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--log_path&lt;/code&gt;: the directory which saves the log file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The whole NExT-GPT training involves 3 steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Step-1&lt;/strong&gt;: Encoding-side LLM-centric Multimodal Alignment. This stage trains the &lt;em&gt;&lt;strong&gt;input projection layer&lt;/strong&gt;&lt;/em&gt; while freezing the ImageBind, LLM, output projection layer.&lt;/p&gt; &lt;p&gt;Just run the above &lt;code&gt;train.sh&lt;/code&gt; script by setting:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;--stage 1&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--dataset x&lt;/code&gt;, where &lt;code&gt;x&lt;/code&gt; varies from [&lt;code&gt;cc3m&lt;/code&gt;, &lt;code&gt;webvid&lt;/code&gt;, &lt;code&gt;audiocap&lt;/code&gt;]&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--data_path ../.../xxx.json&lt;/code&gt;, where &lt;code&gt;xxx&lt;/code&gt; is the file name of the data in &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/T-X_pair_data&#34;&gt;[./data/T-X_pair_data]&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--mm_root_path .../.../x&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt; varies from [&lt;code&gt;images&lt;/code&gt;, &lt;code&gt;audios&lt;/code&gt;, &lt;code&gt;videos&lt;/code&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;Also refer to the running config file &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/code/config/stage_1.yaml&#34;&gt;[./code/config/stage_1.yaml]&lt;/a&gt; and deepspeed config file &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/code/dsconfig/stage_1.yaml&#34;&gt;[./code/dsconfig/stage_1.yaml]&lt;/a&gt; for more step-wise configurations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Step-2&lt;/strong&gt;: Decoding-side Instruction-following Alignment. This stage trains the &lt;em&gt;&lt;strong&gt;output projection layers&lt;/strong&gt;&lt;/em&gt; while freezing the ImageBind, LLM, input projection layers.&lt;/p&gt; &lt;p&gt;Just run the above &lt;code&gt;train.sh&lt;/code&gt; script by setting:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;--stage 2&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--dataset x&lt;/code&gt;, where &lt;code&gt;x&lt;/code&gt; varies from [&lt;code&gt;cc3m&lt;/code&gt;, &lt;code&gt;webvid&lt;/code&gt;, &lt;code&gt;audiocap&lt;/code&gt;]&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--data_path ../.../xxx.json&lt;/code&gt;, where &lt;code&gt;xxx&lt;/code&gt; is the file name of the data in &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/T-X_pair_data&#34;&gt;[./data/T-X_pair_data]&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--mm_root_path .../.../x&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt; varies from [&lt;code&gt;images&lt;/code&gt;, &lt;code&gt;audios&lt;/code&gt;, &lt;code&gt;videos&lt;/code&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;Also refer to the running config file &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/code/config/stage_2.yaml&#34;&gt;[./code/config/stage_2.yaml]&lt;/a&gt; and deepspeed config file &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/code/dsconfig/stage_2.yaml&#34;&gt;[./code/dsconfig/stage_2.yaml]&lt;/a&gt; for more step-wise configurations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Step-3&lt;/strong&gt;: Instruction Tuning. This stage instruction-tune 1) the &lt;em&gt;&lt;strong&gt;LLM&lt;/strong&gt;&lt;/em&gt; via LoRA, 2) &lt;em&gt;&lt;strong&gt;input projection layer&lt;/strong&gt;&lt;/em&gt; and 3) &lt;em&gt;&lt;strong&gt;output projection layer&lt;/strong&gt;&lt;/em&gt; on the instruction dataset.&lt;/p&gt; &lt;p&gt;Just run the above &lt;code&gt;train.sh&lt;/code&gt; script by setting:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;--stage 3&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--dataset instruction&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--data_path ../.../xxx.json&lt;/code&gt;, where &lt;code&gt;xxx&lt;/code&gt; is the file name of the data in &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/IT_data/T+X-T_data&#34;&gt;[./data/IT_data/T+X-T_data]&lt;/a&gt; or data in &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/IT_data/T+X-T_data&#34;&gt;[./data/IT_data/T+X-T_data]&lt;/a&gt; or data in &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/data/IT_data/MosIT_data&#34;&gt;[./data/IT_data/MosIT_data]&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--mm_root_path .../.../x&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt; varies from [&lt;code&gt;images&lt;/code&gt;, &lt;code&gt;audios&lt;/code&gt;, &lt;code&gt;videos&lt;/code&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;Also refer to the running config file &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/code/config/stage_3.yaml&#34;&gt;[./code/config/stage_3.yaml]&lt;/a&gt; and deepspeed config file &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/code/dsconfig/stage_3.yaml&#34;&gt;[./code/dsconfig/stage_3.yaml]&lt;/a&gt; for more step-wise configurations.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;span id=&#34;Run NExT-GPT System&#34;&gt;&lt;/span&gt; &#xA;&lt;h2&gt;4. Running NExT-GPT System &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/#all_catelogue&#34;&gt;[Back to Top]&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;span id=&#34;Prepare checkpoints&#34;&gt;&lt;/span&gt; &#xA;&lt;h4&gt;4.1. Preparing Checkpoints&lt;/h4&gt; &#xA;&lt;p&gt;First, loading the pre-trained NExT-GPT system.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Step-1&lt;/strong&gt;: load &lt;code&gt;Frozen parameters&lt;/code&gt;. Please refer to &lt;a href=&#34;#Prepare Pre-trained Checkpoint&#34;&gt;3.1 Preparing Pre-trained Checkpoint&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Step-2&lt;/strong&gt;: load &lt;code&gt;Tunable parameters&lt;/code&gt;. Please put the NExT-GPT system in &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/ckpt/delta_ckpt/nextgpt/7b_tiva_v0&#34;&gt;[./ckpt/delta_ckpt/nextgpt/7b_tiva_v0]&lt;/a&gt;. You may either 1) use the params trained yourselves, or 2) download our checkpoints from &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/#&#34;&gt;here&lt;/a&gt;. (&lt;em&gt;We are still working hard on optimizing the system, and will release the params shortly.&lt;/em&gt;)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;span id=&#34;Deploy Demo System&#34;&gt;&lt;/span&gt; &#xA;&lt;h4&gt;4.2. Deploying Gradio Demo&lt;/h4&gt; &#xA;&lt;p&gt;Upon completion of the checkpoint loading, you can run the demo locally via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-angular2html&#34;&gt;cd ./code&#xA;bash scripts/app.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Specifying the key arguments as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--nextgpt_ckpt_path&lt;/code&gt;: the path of pre-trained NExT-GPT params.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;For any questions or feedback, feel free to contact &lt;a href=&#34;mailto:swu@u.nus.edu&#34;&gt;Shengqiong Wu&lt;/a&gt; and &lt;a href=&#34;mailto:haofei37@nus.edu.sg&#34;&gt;Hao Fei&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find NextGPT useful in your research or applications, please kindly cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@articles{wu2023nextgpt,&#xA;  title={NExT-GPT: Any-to-Any Multimodal LLM},&#xA;  author={Shengqiong Wu and Hao Fei and Leigang Qu and Wei Ji and Tat-Seng Chua},&#xA;  journal = {CoRR},&#xA;  volume = {abs/2309.05519},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;You may refer to related work that serves as foundations for our framework and code repository, &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/ImageBind&#34;&gt;ImageBind&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img&#34;&gt;Stable Diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/haoheliu/AudioLDM&#34;&gt;AudioLDM&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/cerspense/zeroscope_v2_576w&#34;&gt;Zeroscope&lt;/a&gt;. We also partially draw inspirations from &lt;a href=&#34;https://github.com/yxuansu/PandaGPT&#34;&gt;PandaGPT&lt;/a&gt;, &lt;a href=&#34;https://vpgtrans.github.io/&#34;&gt;VPGTrans&lt;/a&gt;, &lt;a href=&#34;https://github.com/kohjingyu/gill/&#34;&gt;GILL&lt;/a&gt;, &lt;a href=&#34;https://codi-gen.github.io/&#34;&gt;CoDi&lt;/a&gt;, &lt;a href=&#34;https://github.com/DAMO-NLP-SG/Video-LLaMA&#34;&gt;Video-LLaMA&lt;/a&gt;, and &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;MiniGPT-4&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; &#xA;&lt;h2&gt;License Notices&lt;/h2&gt; &#xA;&lt;p&gt;This repository is under &lt;a href=&#34;https://raw.githubusercontent.com/NExT-GPT/NExT-GPT/main/LICENSE.txt&#34;&gt;BSD 3-Clause License&lt;/a&gt;. NExT-GPT is a research project intended for non-commercial use only. One must NOT use the code of NExT-GPT for any illegal, harmful, violent, racist, or sexual purposes. One is strictly prohibited from engaging in any activity that will potentially violate these guidelines. Any potential commercial use of this code should be approved by the authors.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sczhou/ProPainter</title>
    <updated>2023-09-20T01:23:26Z</updated>
    <id>tag:github.com,2023-09-20:/sczhou/ProPainter</id>
    <link href="https://github.com/sczhou/ProPainter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICCV 2023] ProPainter: Improving Propagation and Transformer for Video Inpainting&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;div class=&#34;logo&#34;&gt; &#xA;  &lt;a href=&#34;https://shangchenzhou.com/projects/ProPainter/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/propainter_logo1_glow.png&#34; style=&#34;width:180px&#34;&gt; &lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;h1&gt;ProPainter: Improving Propagation and Transformer for Video Inpainting&lt;/h1&gt; &#xA; &lt;div&gt; &#xA;  &lt;a href=&#34;https://shangchenzhou.com/&#34; target=&#34;_blank&#34;&gt;Shangchen Zhou&lt;/a&gt;  &#xA;  &lt;a href=&#34;https://li-chongyi.github.io/&#34; target=&#34;_blank&#34;&gt;Chongyi Li&lt;/a&gt;  &#xA;  &lt;a href=&#34;https://ckkelvinchan.github.io/&#34; target=&#34;_blank&#34;&gt;Kelvin C.K. Chan&lt;/a&gt;  &#xA;  &lt;a href=&#34;https://www.mmlab-ntu.com/person/ccloy/&#34; target=&#34;_blank&#34;&gt;Chen Change Loy&lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;   S-Lab, Nanyang Technological University  &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;strong&gt;ICCV 2023&lt;/strong&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;https://shangchenzhou.com/projects/ProPainter&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/🐳-Project%20Page-blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.03897&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/arXiv-2309.03897-b31b1b.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://youtu.be/Cc89WF-2zz0&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Demo%20Video-%23FF0000.svg?logo=YouTube&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;img src=&#34;https://api.infinitescript.com/badgen/count?name=sczhou/ProPainter&#34;&gt; &lt;/h4&gt; &#xA; &lt;/div&gt; &#xA; &lt;p&gt;⭐ If ProPainter is helpful to your projects, please help star this repo. Thanks! 🤗&lt;/p&gt; &#xA; &lt;p&gt;&lt;span&gt;📖&lt;/span&gt; For more visual results, go checkout our &lt;a href=&#34;https://shangchenzhou.com/projects/ProPainter/&#34; target=&#34;_blank&#34;&gt;project page&lt;/a&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.09.07&lt;/strong&gt;: Our code and model are publicly available. &lt;span&gt;🐳&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.09.01&lt;/strong&gt;: This repo is created.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h4&gt;🏂 Object Removal&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/object_removal1.gif&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/object_removal2.gif&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h4&gt;🌈 Watermark Removal&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/watermark_removal1.gif&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/watermark_removal2.gif&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h4&gt;🎨 Video Completion&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/video_completion1.gif&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/video_completion2.gif&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/ProPainter_pipeline.png&#34; alt=&#34;overall_structure&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies and Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone Repo&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/sczhou/ProPainter.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create Conda Environment and Install Dependencies&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment.yaml&#xA;conda activate propainter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Python &amp;gt;= 3.7&lt;/li&gt; &#xA;   &lt;li&gt;PyTorch &amp;gt;= 1.6.0&lt;/li&gt; &#xA;   &lt;li&gt;CUDA &amp;gt;= 9.2&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmcv/tree/v1.4.8#installation&#34;&gt;mmcv-full&lt;/a&gt; (refer the command table to install v1.4.8)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prepare pretrained models&lt;/h3&gt; &#xA;&lt;p&gt;Download our pretrained models from &lt;a href=&#34;https://github.com/sczhou/ProPainter/releases/tag/v0.1.0&#34;&gt;Releases V0.1.0&lt;/a&gt; to the &lt;code&gt;weights&lt;/code&gt; folder. (All pretrained models can also be automatically downloaded during the first inference.)&lt;/p&gt; &#xA;&lt;p&gt;The directory structure will be arranged as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;weights&#xA;   |- ProPainter.pth&#xA;   |- recurrent_flow_completion.pth&#xA;   |- raft-things.pth&#xA;   |- i3d_rgb_imagenet.pt (for evaluating VFID metric)&#xA;   |- README.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quick test&lt;/h3&gt; &#xA;&lt;p&gt;We provide some examples in the &lt;a href=&#34;https://raw.githubusercontent.com/sczhou/ProPainter/main/inputs&#34;&gt;&lt;code&gt;inputs&lt;/code&gt;&lt;/a&gt; folder. Run the following commands to try it out:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# The first example (object removal)&#xA;python inference_propainter.py --video inputs/object_removal/bmx-trees --mask inputs/object_removal/bmx-trees_mask &#xA;# The second example (watermark removal)&#xA;python inference_propainter.py --video inputs/watermark_removal/running_car.mp4 --mask inputs/watermark_removal/mask.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results will be saved in the &lt;code&gt;results&lt;/code&gt; folder. To test your own videos, please prepare the input &lt;code&gt;mp4 video&lt;/code&gt; (or &lt;code&gt;split frames&lt;/code&gt;) and &lt;code&gt;frame-wise mask(s)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to specify the video resolution for processing or avoid running out of memory, you can use the &lt;code&gt;--set_size&lt;/code&gt; flag and set the video size of &lt;code&gt;--width&lt;/code&gt; and &lt;code&gt;--height&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# process a 576x320 video&#xA;python inference_propainter.py --video inputs/watermark_removal/running_car.mp4 --mask inputs/watermark_removal/mask.png --set_size --height 320 --width 576 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Regarding the issue of &lt;strong&gt;running out of memory&lt;/strong&gt;, you can mitigate it by reducing the number of local neighbors through decreasing the &lt;code&gt;--neighbor_length&lt;/code&gt; (default 20) or reducing the number of global references by increasing the &lt;code&gt;--ref_stride&lt;/code&gt; (default 10).&lt;/p&gt; &#xA;&lt;h3&gt;Dataset preparation for training and evaluation&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;YouTube-VOS&lt;/th&gt; &#xA;   &lt;th&gt;DAVIS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Description&lt;/td&gt; &#xA;   &lt;td&gt;For training (3,471) and evaluation (508)&lt;/td&gt; &#xA;   &lt;td&gt;For evaluation (50 in 90)&lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Images&lt;/td&gt; &#xA;   &lt;td&gt; [&lt;a href=&#34;https://competitions.codalab.org/competitions/19544#participate-get-data&#34;&gt;Official Link&lt;/a&gt;] (Download train and test all frames) &lt;/td&gt; &#xA;   &lt;td&gt; [&lt;a href=&#34;https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-480p.zip&#34;&gt;Official Link&lt;/a&gt;] (2017, 480p, TrainVal) &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Masks&lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt; [&lt;a href=&#34;https://drive.google.com/file/d/1dFTneS_zaJAHjglxU10gYzr1-xALgHa4/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;] [&lt;a href=&#34;https://pan.baidu.com/s/1JC-UKmlQfjhVtD81196cxA?pwd=87e3&#34;&gt;Baidu Disk&lt;/a&gt;] (For reproducing paper results; provided in &lt;a href=&#34;https://github.com/MCG-NKU/E2FGVI&#34;&gt;E&lt;sup&gt;2&lt;/sup&gt;FGVI&lt;/a&gt; paper) &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The training and test split files are provided in &lt;code&gt;datasets/&amp;lt;dataset_name&amp;gt;&lt;/code&gt;. For each dataset, you should place &lt;code&gt;JPEGImages&lt;/code&gt; to &lt;code&gt;datasets/&amp;lt;dataset_name&amp;gt;&lt;/code&gt;. Resize all video frames to size &lt;code&gt;432x240&lt;/code&gt; for training. Unzip downloaded mask files to &lt;code&gt;datasets&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;datasets&lt;/code&gt; directory structure will be arranged as: (&lt;strong&gt;Note&lt;/strong&gt;: please check it carefully)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;datasets&#xA;   |- davis&#xA;      |- JPEGImages_432_240&#xA;         |- &amp;lt;video_name&amp;gt;&#xA;            |- 00000.jpg&#xA;            |- 00001.jpg&#xA;      |- test_masks&#xA;         |- &amp;lt;video_name&amp;gt;&#xA;            |- 00000.png&#xA;            |- 00001.png   &#xA;      |- train.json&#xA;      |- test.json&#xA;   |- youtube-vos&#xA;      |- JPEGImages_432_240&#xA;         |- &amp;lt;video_name&amp;gt;&#xA;            |- 00000.jpg&#xA;            |- 00001.jpg&#xA;      |- test_masks&#xA;         |- &amp;lt;video_name&amp;gt;&#xA;            |- 00000.png&#xA;            |- 00001.png&#xA;      |- train.json&#xA;      |- test.json   &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Run one of the following commands for evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt; # For evaluating flow completion model&#xA; python scripts/evaluate_flow_completion.py --dataset &amp;lt;dataset_name&amp;gt; --video_root &amp;lt;video_root&amp;gt; --mask_root &amp;lt;mask_root&amp;gt; --save_results&#xA; # For evaluating ProPainter model&#xA; python scripts/evaluate_propainter.py --dataset &amp;lt;dataset_name&amp;gt; --video_root &amp;lt;video_root&amp;gt; --mask_root &amp;lt;mask_root&amp;gt; --save_results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The scores and results will also be saved in the &lt;code&gt;results_eval&lt;/code&gt; folder. Please &lt;code&gt;--save_results&lt;/code&gt; for further &lt;a href=&#34;https://github.com/phoenix104104/fast_blind_video_consistency#evaluation&#34;&gt;evaluating temporal warping error&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Our training configures are provided in &lt;a href=&#34;https://raw.githubusercontent.com/sczhou/ProPainter/main/configs/train_flowcomp.json&#34;&gt;&lt;code&gt;train_flowcomp.json&lt;/code&gt;&lt;/a&gt; (for Recurrent Flow Completion Network) and &lt;a href=&#34;https://raw.githubusercontent.com/sczhou/ProPainter/main/configs/train_propainter.json&#34;&gt;&lt;code&gt;train_propainter.json&lt;/code&gt;&lt;/a&gt; (for ProPainter).&lt;/p&gt; &#xA;&lt;p&gt;Run one of the following commands for training:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt; # For training Recurrent Flow Completion Network&#xA; python train.py -c configs/train_flowcomp.json&#xA; # For training ProPainter&#xA; python train.py -c configs/train_propainter.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can run the &lt;strong&gt;same command&lt;/strong&gt; to &lt;strong&gt;resume&lt;/strong&gt; your training.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our repo useful for your research, please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{zhou2023propainter,&#xA;   title={{ProPainter}: Improving Propagation and Transformer for Video Inpainting},&#xA;   author={Zhou, Shangchen and Li, Chongyi and Chan, Kelvin C.K and Loy, Chen Change},&#xA;   booktitle={Proceedings of IEEE International Conference on Computer Vision (ICCV)},&#xA;   year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under &lt;a rel=&#34;license&#34; href=&#34;https://raw.githubusercontent.com/sczhou/ProPainter/main/LICENSE&#34;&gt;NTU S-Lab License 1.0&lt;/a&gt;. Redistribution and use should follow this license.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please feel free to reach me out at &lt;code&gt;shangchenzhou@gmail.com&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This code is based on &lt;a href=&#34;https://github.com/MCG-NKU/E2FGVI&#34;&gt;E&lt;sup&gt;2&lt;/sup&gt;FGVI&lt;/a&gt; and &lt;a href=&#34;https://github.com/researchmm/STTN&#34;&gt;STTN&lt;/a&gt;. Some code are brought from &lt;a href=&#34;https://github.com/ckkelvinchan/BasicVSR_PlusPlus&#34;&gt;BasicVSR++&lt;/a&gt;. Thanks for their awesome works.&lt;/p&gt;</summary>
  </entry>
</feed>