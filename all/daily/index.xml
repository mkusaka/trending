<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-07-18T01:28:55Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tokio-rs/tokio</title>
    <updated>2024-07-18T01:28:55Z</updated>
    <id>tag:github.com,2024-07-18:/tokio-rs/tokio</id>
    <link href="https://github.com/tokio-rs/tokio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A runtime for writing reliable asynchronous applications with Rust. Provides I/O, networking, scheduling, timers, ...&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tokio&lt;/h1&gt; &#xA;&lt;p&gt;A runtime for writing reliable, asynchronous, and slim applications with the Rust programming language. It is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fast&lt;/strong&gt;: Tokio&#39;s zero-cost abstractions give you bare-metal performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reliable&lt;/strong&gt;: Tokio leverages Rust&#39;s ownership, type system, and concurrency model to reduce bugs and ensure thread safety.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Scalable&lt;/strong&gt;: Tokio has a minimal footprint, and handles backpressure and cancellation naturally.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://crates.io/crates/tokio&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/tokio.svg?sanitize=true&#34; alt=&#34;Crates.io&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tokio-rs/tokio/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;MIT licensed&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tokio-rs/tokio/actions?query=workflow%3ACI+branch%3Amaster&#34;&gt;&lt;img src=&#34;https://github.com/tokio-rs/tokio/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/tokio&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/500028886025895936.svg?logo=discord&amp;amp;style=flat-square&#34; alt=&#34;Discord chat&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tokio.rs&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://tokio.rs/tokio/tutorial&#34;&gt;Guides&lt;/a&gt; | &lt;a href=&#34;https://docs.rs/tokio/latest/tokio&#34;&gt;API Docs&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/tokio&#34;&gt;Chat&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Tokio is an event-driven, non-blocking I/O platform for writing asynchronous applications with the Rust programming language. At a high level, it provides a few major components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A multithreaded, work-stealing based task &lt;a href=&#34;https://docs.rs/tokio/latest/tokio/runtime/index.html&#34;&gt;scheduler&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A reactor backed by the operating system&#39;s event queue (epoll, kqueue, IOCP, etc...).&lt;/li&gt; &#xA; &lt;li&gt;Asynchronous &lt;a href=&#34;https://docs.rs/tokio/latest/tokio/net/index.html&#34;&gt;TCP and UDP&lt;/a&gt; sockets.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These components provide the runtime components necessary for building an asynchronous application.&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;A basic TCP echo server with Tokio.&lt;/p&gt; &#xA;&lt;p&gt;Make sure you activated the full features of the tokio crate on Cargo.toml:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[dependencies]&#xA;tokio = { version = &#34;1.38.1&#34;, features = [&#34;full&#34;] }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, on your main.rs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-rust,no_run&#34;&gt;use tokio::net::TcpListener;&#xA;use tokio::io::{AsyncReadExt, AsyncWriteExt};&#xA;&#xA;#[tokio::main]&#xA;async fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {&#xA;    let listener = TcpListener::bind(&#34;127.0.0.1:8080&#34;).await?;&#xA;&#xA;    loop {&#xA;        let (mut socket, _) = listener.accept().await?;&#xA;&#xA;        tokio::spawn(async move {&#xA;            let mut buf = [0; 1024];&#xA;&#xA;            // In a loop, read data from the socket and write the data back.&#xA;            loop {&#xA;                let n = match socket.read(&amp;amp;mut buf).await {&#xA;                    // socket closed&#xA;                    Ok(n) if n == 0 =&amp;gt; return,&#xA;                    Ok(n) =&amp;gt; n,&#xA;                    Err(e) =&amp;gt; {&#xA;                        eprintln!(&#34;failed to read from socket; err = {:?}&#34;, e);&#xA;                        return;&#xA;                    }&#xA;                };&#xA;&#xA;                // Write the data back&#xA;                if let Err(e) = socket.write_all(&amp;amp;buf[0..n]).await {&#xA;                    eprintln!(&#34;failed to write to socket; err = {:?}&#34;, e);&#xA;                    return;&#xA;                }&#xA;            }&#xA;        });&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More examples can be found &lt;a href=&#34;https://github.com/tokio-rs/tokio/tree/master/examples&#34;&gt;here&lt;/a&gt;. For a larger &#34;real world&#34; example, see the &lt;a href=&#34;https://github.com/tokio-rs/mini-redis/&#34;&gt;mini-redis&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;p&gt;To see a list of the available features flags that can be enabled, check our &lt;a href=&#34;https://docs.rs/tokio/#feature-flags&#34;&gt;docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Help&lt;/h2&gt; &#xA;&lt;p&gt;First, see if the answer to your question can be found in the &lt;a href=&#34;https://tokio.rs/tokio/tutorial&#34;&gt;Guides&lt;/a&gt; or the &lt;a href=&#34;https://docs.rs/tokio/latest/tokio&#34;&gt;API documentation&lt;/a&gt;. If the answer is not there, there is an active community in the &lt;a href=&#34;https://discord.gg/tokio&#34;&gt;Tokio Discord server&lt;/a&gt;. We would be happy to try to answer your question. You can also ask your question on &lt;a href=&#34;https://github.com/tokio-rs/tokio/discussions&#34;&gt;the discussions page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;🎈&lt;/span&gt; Thanks for your help improving the project! We are so happy to have you! We have a &lt;a href=&#34;https://github.com/tokio-rs/tokio/raw/master/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to help you get involved in the Tokio project.&lt;/p&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;p&gt;In addition to the crates in this repository, the Tokio project also maintains several other libraries, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/tokio-rs/axum&#34;&gt;&lt;code&gt;axum&lt;/code&gt;&lt;/a&gt;: A web application framework that focuses on ergonomics and modularity.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/hyperium/hyper&#34;&gt;&lt;code&gt;hyper&lt;/code&gt;&lt;/a&gt;: A fast and correct HTTP/1.1 and HTTP/2 implementation for Rust.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/hyperium/tonic&#34;&gt;&lt;code&gt;tonic&lt;/code&gt;&lt;/a&gt;: A gRPC over HTTP/2 implementation focused on high performance, interoperability, and flexibility.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/seanmonstar/warp&#34;&gt;&lt;code&gt;warp&lt;/code&gt;&lt;/a&gt;: A super-easy, composable, web server framework for warp speeds.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/tower-rs/tower&#34;&gt;&lt;code&gt;tower&lt;/code&gt;&lt;/a&gt;: A library of modular and reusable components for building robust networking clients and servers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/tokio-rs/tracing&#34;&gt;&lt;code&gt;tracing&lt;/code&gt;&lt;/a&gt; (formerly &lt;code&gt;tokio-trace&lt;/code&gt;): A framework for application-level tracing and async-aware diagnostics.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/tokio-rs/mio&#34;&gt;&lt;code&gt;mio&lt;/code&gt;&lt;/a&gt;: A low-level, cross-platform abstraction over OS I/O APIs that powers &lt;code&gt;tokio&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/tokio-rs/bytes&#34;&gt;&lt;code&gt;bytes&lt;/code&gt;&lt;/a&gt;: Utilities for working with bytes, including efficient byte buffers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/tokio-rs/loom&#34;&gt;&lt;code&gt;loom&lt;/code&gt;&lt;/a&gt;: A testing tool for concurrent Rust code.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;The Tokio repository contains multiple crates. Each crate has its own changelog.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;tokio&lt;/code&gt; - &lt;a href=&#34;https://github.com/tokio-rs/tokio/raw/master/tokio/CHANGELOG.md&#34;&gt;view changelog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tokio-util&lt;/code&gt; - &lt;a href=&#34;https://github.com/tokio-rs/tokio/raw/master/tokio-util/CHANGELOG.md&#34;&gt;view changelog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tokio-stream&lt;/code&gt; - &lt;a href=&#34;https://github.com/tokio-rs/tokio/raw/master/tokio-stream/CHANGELOG.md&#34;&gt;view changelog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tokio-macros&lt;/code&gt; - &lt;a href=&#34;https://github.com/tokio-rs/tokio/raw/master/tokio-macros/CHANGELOG.md&#34;&gt;view changelog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tokio-test&lt;/code&gt; - &lt;a href=&#34;https://github.com/tokio-rs/tokio/raw/master/tokio-test/CHANGELOG.md&#34;&gt;view changelog&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Rust Versions&lt;/h2&gt; &#xA;&lt;!--&#xA;When updating this, also update:&#xA;- .github/workflows/ci.yml&#xA;- CONTRIBUTING.md&#xA;- README.md&#xA;- tokio/README.md&#xA;- tokio/Cargo.toml&#xA;- tokio-util/Cargo.toml&#xA;- tokio-test/Cargo.toml&#xA;- tokio-stream/Cargo.toml&#xA;--&gt; &#xA;&lt;p&gt;Tokio will keep a rolling MSRV (minimum supported rust version) policy of &lt;strong&gt;at least&lt;/strong&gt; 6 months. When increasing the MSRV, the new Rust version must have been released at least six months ago. The current MSRV is 1.63.&lt;/p&gt; &#xA;&lt;p&gt;Note that the MSRV is not increased automatically, and only as part of a minor release. The MSRV history for past minor releases can be found below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1.30 to now - Rust 1.63&lt;/li&gt; &#xA; &lt;li&gt;1.27 to 1.29 - Rust 1.56&lt;/li&gt; &#xA; &lt;li&gt;1.17 to 1.26 - Rust 1.49&lt;/li&gt; &#xA; &lt;li&gt;1.15 to 1.16 - Rust 1.46&lt;/li&gt; &#xA; &lt;li&gt;1.0 to 1.14 - Rust 1.45&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that although we try to avoid the situation where a dependency transitively increases the MSRV of Tokio, we do not guarantee that this does not happen. However, every minor release will have some set of versions of dependencies that works with the MSRV of that minor release.&lt;/p&gt; &#xA;&lt;h2&gt;Release schedule&lt;/h2&gt; &#xA;&lt;p&gt;Tokio doesn&#39;t follow a fixed release schedule, but we typically make one to two new minor releases each month. We make patch releases for bugfixes as necessary.&lt;/p&gt; &#xA;&lt;h2&gt;Bug patching policy&lt;/h2&gt; &#xA;&lt;p&gt;For the purposes of making patch releases with bugfixes, we have designated certain minor releases as LTS (long term support) releases. Whenever a bug warrants a patch release with a fix for the bug, it will be backported and released as a new patch release for each LTS minor version. Our current LTS releases are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;1.32.x&lt;/code&gt; - LTS release until September 2024. (MSRV 1.63)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;1.36.x&lt;/code&gt; - LTS release until March 2025. (MSRV 1.63)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each LTS release will continue to receive backported fixes for at least a year. If you wish to use a fixed minor release in your project, we recommend that you use an LTS release.&lt;/p&gt; &#xA;&lt;p&gt;To use a fixed minor version, you can specify the version with a tilde. For example, to specify that you wish to use the newest &lt;code&gt;1.32.x&lt;/code&gt; patch release, you can use the following dependency specification:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;tokio = { version = &#34;~1.32&#34;, features = [...] }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Previous LTS releases&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;1.8.x&lt;/code&gt; - LTS release until February 2022.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;1.14.x&lt;/code&gt; - LTS release until June 2022.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;1.18.x&lt;/code&gt; - LTS release until June 2023.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;1.20.x&lt;/code&gt; - LTS release until September 2023.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;1.25.x&lt;/code&gt; - LTS release until March 2024.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://github.com/tokio-rs/tokio/raw/master/LICENSE&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Contribution&lt;/h3&gt; &#xA;&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in Tokio by you, shall be licensed as MIT, without any additional terms or conditions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sebastianstarke/AI4Animation</title>
    <updated>2024-07-18T01:28:55Z</updated>
    <id>tag:github.com,2024-07-18:/sebastianstarke/AI4Animation</id>
    <link href="https://github.com/sebastianstarke/AI4Animation" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bringing Characters to Life with Computer Brains in Unity&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI4Animation: Deep Learning for Character Control&lt;/h1&gt; &#xA;&lt;p&gt;This repository explores the opportunities of deep learning for character animation and control. It aims to be a comprehensive framework for data-driven character animation, including data processing, neural network training and runtime control, developed in Unity3D / PyTorch. The various projects below demonstrate such capabilities using neural networks for animating biped locomotion, quadruped locomotion, and character-scene interactions with objects and the environment, plus sports and fighting games, as well as embodied avatar motions in AR/VR. Further advances on this research will continue being added to this project.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;SIGGRAPH 2024&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Categorical Codebook Matching for Embodied Character Controllers&lt;/strong&gt;&lt;br&gt; &lt;sub&gt; &lt;a href=&#34;https://www.linkedin.com/in/sebastian-starke-b281a6148/&#34;&gt;Sebastian Starke&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/paul-starke-0787211b4/&#34;&gt;Paul Starke&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/nicky-sijia-he-92240590/&#34;&gt;Nicky He&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/taku-komura-571b32b/&#34;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/yuting-ye-77a75332/&#34;&gt;Yuting Ye&lt;/a&gt;, ACM Trans. Graph. 43, 4, Article 142. &lt;sub&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2024/Teaser.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Translating motions from a real user onto a virtual embodied avatar is a key challenge for character animation in the metaverse. In this work, we present a novel generative framework that enables mapping from a set of sparse sensor signals to a full body avatar motion in real-time while faithfully preserving the motion context of the user. In contrast to existing techniques that require training a motion prior and its mapping from control to motion separately, our framework is able to learn the motion manifold as well as how to sample from it at the same time in an end-to-end manner. To achieve that, we introduce a technique called codebook matching which matches the probability distribution between two categorical codebooks for the inputs and outputs for synthesizing the character motions. We demonstrate this technique can successfully handle ambiguity in motion generation and produce high quality character controllers from unstructured motion capture data. Our method is especially useful for interactive applications like virtual reality or video games where high accuracy and responsiveness are needed. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; style=&#34;font-size:1.25em;&#34;&gt; - &lt;a href=&#34;https://youtu.be/NyLRcY0c0p4&#34;&gt;Video&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2024/Paper.pdf&#34;&gt;Paper&lt;/a&gt; - &lt;a href=&#34;https://starke-consult.de/AI4Animation/SIGGRAPH_2024/Cranberry_Dataset.zip&#34;&gt;Dataset&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2024/&#34;&gt;Code (+VR Demo)&lt;/a&gt; - &lt;a href=&#34;https://starke-consult.de/AI4Animation/SIGGRAPH_2024/Demo_Win.zip&#34;&gt;Windows Demo&lt;/a&gt; - &lt;a href=&#34;https://starke-consult.de/AI4Animation/SIGGRAPH_2024/Demo_Mac.zip&#34;&gt;Mac Demo&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2024/ReadMe.md&#34;&gt;ReadMe&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2024/Architecture.png&#34;&gt; &#xA;&lt;p&gt;Unlike existing methods for kinematic character control that learn a direct mapping between inputs and outputs or utilize a motion prior that is trained on the motion data alone, our framework learns from both the inputs and outputs simultaneously to form a motion manifold that is informed about the control signals. To learn such setup in a supervised manner, we propose a technique that we call Codebook Matching which enforces similarity between both latent probability distributions $Z_𝑋$ and $Z_𝑌$. In the context of motion generation, instead of directly predicting the motions outputs from the control inputs, we only predict their probabilities for each of them to appear. By introducing a matching loss between both categorical probability distributions, our codebook matching technique allows to substitute $Z_𝑌$ by $Z_𝑋$ during test time.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-math&#34;&gt;Training:&#xA;\begin{cases}&#xA;    Y \rightarrow Z_Y \rightarrow Y&#xA;    \\&#xA;    X \rightarrow Z_X&#xA;    \\&#xA;    Z_X \sim Z_Y&#xA;\end{cases}&#xA;&#xA;Inference: &#xA;X \rightarrow Z_X \rightarrow Y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our method is not limited to three-point inputs but we can also use it to generate embodied character movements with additional joystick or button controls by what we call hybrid control mode. In this setting, the user, engineer or artist can additionally tell the character where to go via a simple goal location while preserving the original context of motion from three-point tracking signals. This changes the scope of applications we can address by walking / running / crouching in the virtual world while standing or even sitting in the real world.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2024/Collection.png&#34;&gt; &#xA;&lt;p&gt;Furthermore, our codebook matching architecture shares many similarities with motion matching and is able to learn a similar structure in an end-to-end manner. While motion matching can bypass ambiguity in the mapping from control to motion by selecting among candidates with similar query distances, our setup selects possible outcomes from predicted probabilities and naturally projects against valid output motions if their probabilities are similar. However, in contrast to database searches, our codebook matching is able to effectively compress the motion data where same motions map to same codes, and can bypass ambiguity issues which existing learning-based methods such as standard feed-forward networks (MLP) or variational models (CVAE) may struggle with. We demonstrate such capabilities by reconstructing the ambiguous toy example functions in the figure below.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2024/ToyExample.png&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://youtu.be/NyLRcY0c0p4&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2024/Thumbnail.png&#34; , width=&#34;100%&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;SIGGRAPH 2022&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;DeepPhase: Periodic Autoencoders for Learning Motion Phase Manifolds&lt;/strong&gt;&lt;br&gt; &lt;sub&gt; &lt;a href=&#34;https://www.linkedin.com/in/sebastian-starke-b281a6148/&#34;&gt;Sebastian Starke&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/ian-mason-134197105/&#34;&gt;Ian Mason&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/taku-komura-571b32b/&#34;&gt;Taku Komura&lt;/a&gt;, ACM Trans. Graph. 41, 4, Article 136. &lt;sub&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2022/Teaser.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Learning the spatial-temporal structure of body movements is a fundamental problem for character motion synthesis. In this work, we propose a novel neural network architecture called the Periodic Autoencoder that can learn periodic features from large unstructured motion datasets in an unsupervised manner. The character movements are decomposed into multiple latent channels that capture the non-linear periodicity of different body segments while progressing forward in time. Our method extracts a multi-dimensional phase space from full-body motion data, which effectively clusters animations and produces a manifold in which computed feature distances provide a better similarity measure than in the original motion space to achieve better temporal and spatial alignment. We demonstrate that the learned periodic embedding can significantly help to improve neural motion synthesis in a number of tasks, including diverse locomotion skills, style-based movements, dance motion synthesis from music, synthesis of dribbling motions in football, and motion query for matching poses within large animation databases. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2022/Manifolds.png&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=YhH4PYEkVnY&#34;&gt;Video&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2022/Paper.pdf&#34;&gt;Paper&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2022/PyTorch&#34;&gt;PAE Code &amp;amp; Demo&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2022/Unity&#34;&gt;Animation Code &amp;amp; Demo&lt;/a&gt; - &lt;a href=&#34;https://www.ianxmason.com/posts/PAE/&#34;&gt;Explanation and Addendum&lt;/a&gt; - &lt;a href=&#34;https://youtu.be/3ASGrxNDd0k&#34;&gt;Tutorial&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=YhH4PYEkVnY&#34;&gt; &lt;img width=&#34;47%&#34; src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2022/Thumbnail.png&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=3ASGrxNDd0k&#34;&gt; &lt;img align=&#34;right&#34; width=&#34;47%&#34; src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2022/PAEthumbnail.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://github.com/pauzii/PhaseBetweener&#34;&gt;Motion In-Betweening System&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/pauzii/PhaseBetweener/raw/main/Media/Teaser.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;SIGGRAPH 2021&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Neural Animation Layering for Synthesizing Martial Arts Movements&lt;/strong&gt;&lt;br&gt; &lt;sub&gt; &lt;a href=&#34;https://www.linkedin.com/in/sebastian-starke-b281a6148/&#34;&gt;Sebastian Starke&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/evan-yiwei-zhao-18584a105/&#34;&gt;Yiwei Zhao&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/fabio-zinno-1a77331/&#34;&gt;Fabio Zinno&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/taku-komura-571b32b/&#34;&gt;Taku Komura&lt;/a&gt;, ACM Trans. Graph. 40, 4, Article 92. &lt;sub&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2021/Teaser.jpg&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2021/Layering.png&#34; width=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Interactively synthesizing novel combinations and variations of character movements from different motion skills is a key problem in computer animation. In this research, we propose a deep learning framework to produce a large variety of martial arts movements in a controllable manner from raw motion capture data. Our method imitates animation layering using neural networks with the aim to overcome typical challenges when mixing, blending and editing movements from unaligned motion sources. The system can be used for offline and online motion generation alike, provides an intuitive interface to integrate with animator workflows, and is relevant for real-time applications such as computer games. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=SkJNxLYNwN0&#34;&gt;Video&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2021/Paper.pdf&#34;&gt;Paper&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=SkJNxLYNwN0&#34;&gt; &lt;img width=&#34;60%&#34; src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2021/Thumbnail.jpg&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;SIGGRAPH 2020&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Local Motion Phases for Learning Multi-Contact Character Movements&lt;/strong&gt;&lt;br&gt; &lt;sub&gt; &lt;a href=&#34;https://www.linkedin.com/in/sebastian-starke-b281a6148/&#34;&gt;Sebastian Starke&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/evan-yiwei-zhao-18584a105/&#34;&gt;Yiwei Zhao&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/taku-komura-571b32b/&#34;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/kazizaman/&#34;&gt;Kazi Zaman&lt;/a&gt;. ACM Trans. Graph. 39, 4, Article 54. &lt;sub&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2020/Teaser.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Not sure how to align complex character movements? Tired of phase labeling? Unclear how to squeeze everything into a single phase variable? Don&#39;t worry, a solution exists! &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2020/Court.jpg&#34; width=&#34;60%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Controlling characters to perform a large variety of dynamic, fast-paced and quickly changing movements is a key challenge in character animation. In this research, we present a deep learning framework to interactively synthesize such animations in high quality, both from unstructured motion data and without any manual labeling. We introduce the concept of local motion phases, and show our system being able to produce various motion skills, such as ball dribbling and professional maneuvers in basketball plays, shooting, catching, avoidance, multiple locomotion modes as well as different character and object interactions, all generated under a unified framework. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=Rzj3k3yerDk&#34;&gt;Video&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2020/Paper.pdf&#34;&gt;Paper&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2020&#34;&gt;Code&lt;/a&gt; - &lt;a href=&#34;https://starke-consult.de/AI4Animation/SIGGRAPH_2020/Demo_Windows.zip&#34;&gt;Windows Demo&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2020/ReadMe.md&#34;&gt;ReadMe&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=Rzj3k3yerDk&#34;&gt; &lt;img width=&#34;60%&#34; src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2020/Thumbnail.jpg&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;SIGGRAPH Asia 2019&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Neural State Machine for Character-Scene Interactions&lt;/strong&gt;&lt;br&gt; &lt;sub&gt; &lt;a href=&#34;https://www.linkedin.com/in/sebastian-starke-b281a6148/&#34;&gt;Sebastian Starke&lt;/a&gt;&lt;sup&gt;+&lt;/sup&gt;, &lt;a href=&#34;https://www.linkedin.com/in/he-zhang-148467165/&#34;&gt;He Zhang&lt;/a&gt;&lt;sup&gt;+&lt;/sup&gt;, &lt;a href=&#34;https://www.linkedin.com/in/taku-komura-571b32b/&#34;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jun-saito/&#34;&gt;Jun Saito&lt;/a&gt;. ACM Trans. Graph. 38, 6, Article 178. &lt;/sub&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(+Joint First Authors)&lt;/sup&gt; &lt;sub&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_Asia_2019/Teaser.jpg&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Animating characters can be an easy or difficult task - interacting with objects is one of the latter. In this research, we present the Neural State Machine, a data-driven deep learning framework for character-scene interactions. The difficulty in such animations is that they require complex planning of periodic as well as aperiodic movements to complete a given task. Creating them in a production-ready quality is not straightforward and often very time-consuming. Instead, our system can synthesize different movements and scene interactions from motion capture data, and allows the user to seamlessly control the character in real-time from simple control commands. Since our model directly learns from the geometry, the motions can naturally adapt to variations in the scene. We show that our system can generate a large variety of movements, icluding locomotion, sitting on chairs, carrying boxes, opening doors and avoiding obstacles, all from a single model. The model is responsive, compact and scalable, and is the first of such frameworks to handle scene interaction tasks for data-driven character animation. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=7c6oQP1u2eQ&#34;&gt;Video&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_Asia_2019/Paper.pdf&#34;&gt;Paper&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_Asia_2019&#34;&gt;Code &amp;amp; Demo&lt;/a&gt; - &lt;a href=&#34;https://starke-consult.de/AI4Animation/SIGGRAPH_Asia_2019/MotionCapture.zip&#34;&gt;Mocap Data&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_Asia_2019/ReadMe.md&#34;&gt;ReadMe&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=7c6oQP1u2eQ&#34;&gt; &lt;img width=&#34;60%&#34; src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_Asia_2019/Thumbnail.jpg&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;SIGGRAPH 2018&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Mode-Adaptive Neural Networks for Quadruped Motion Control&lt;/strong&gt;&lt;br&gt; &lt;sub&gt; &lt;a href=&#34;https://www.linkedin.com/in/he-zhang-148467165/&#34;&gt;He Zhang&lt;/a&gt;&lt;sup&gt;+&lt;/sup&gt;, &lt;a href=&#34;https://www.linkedin.com/in/sebastian-starke-b281a6148/&#34;&gt;Sebastian Starke&lt;/a&gt;&lt;sup&gt;+&lt;/sup&gt;, &lt;a href=&#34;https://www.linkedin.com/in/taku-komura-571b32b/&#34;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jun-saito/&#34;&gt;Jun Saito&lt;/a&gt;. ACM Trans. Graph. 37, 4, Article 145. &lt;/sub&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(+Joint First Authors)&lt;/sup&gt; &lt;sub&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2018/Teaser.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Animating characters can be a pain, especially those four-legged monsters! This year, we will be presenting our recent research on quadruped animation and character control at the SIGGRAPH 2018 in Vancouver. The system can produce natural animations from real motion data using a novel neural network architecture, called Mode-Adaptive Neural Networks. Instead of optimising a fixed group of weights, the system learns to dynamically blend a group of weights into a further neural network, based on the current state of the character. That said, the system does not require labels for the phase or locomotion gaits, but can learn from unstructured motion capture data in an end-to-end fashion. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=uFJvRYtjQ4c&#34;&gt;Video&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2018/Paper.pdf&#34;&gt;Paper&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2018&#34;&gt;Code&lt;/a&gt; - &lt;a href=&#34;https://starke-consult.de/AI4Animation/SIGGRAPH_2018/MotionCapture.zip&#34;&gt;Mocap Data&lt;/a&gt; - &lt;a href=&#34;https://starke-consult.de/AI4Animation/SIGGRAPH_2018/Demo_Windows.zip&#34;&gt;Windows Demo&lt;/a&gt; - &lt;a href=&#34;https://starke-consult.de/AI4Animation/SIGGRAPH_2018/Demo_Linux.zip&#34;&gt;Linux Demo&lt;/a&gt; - &lt;a href=&#34;https://starke-consult.de/AI4Animation/SIGGRAPH_2018/Demo_Mac.zip&#34;&gt;Mac Demo&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2018/ReadMe.md&#34;&gt;ReadMe&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=uFJvRYtjQ4c&#34;&gt; &lt;img width=&#34;60%&#34; src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2018/Thumbnail.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://github.com/pauzii/AnimationAuthoring&#34;&gt;Animation Authoring Tool&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/pauzii/AnimationAuthoring/raw/main/Media/Teaser.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;SIGGRAPH 2017&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Phase-Functioned Neural Networks for Character Control&lt;/strong&gt;&lt;br&gt; &lt;sub&gt; &lt;a href=&#34;https://www.linkedin.com/in/daniel-holden-300b871b/&#34;&gt;Daniel Holden&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/taku-komura-571b32b/&#34;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jun-saito/&#34;&gt;Jun Saito&lt;/a&gt;. ACM Trans. Graph. 36, 4, Article 42. &lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2017/Adam.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; This work continues the recent work on PFNN (Phase-Functioned Neural Networks) for character control. A demo in Unity3D using the original weights for terrain-adaptive locomotion is contained in the Assets/Demo/SIGGRAPH_2017/Original folder. Another demo on flat ground using the Adam character is contained in the Assets/Demo/SIGGRAPH_2017/Adam folder. In order to run them, you need to download the neural network weights from the link provided in the Link.txt file, extract them into the /NN folder, and store the parameters via the custom inspector button. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=Ul0Gilv5wvY&#34;&gt;Video&lt;/a&gt; - &lt;a href=&#34;http://theorangeduck.com/media/uploads/other_stuff/phasefunction.pdf&#34;&gt;Paper&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2017&#34;&gt;Code (Unity)&lt;/a&gt; - &lt;a href=&#34;https://starke-consult.de/AI4Animation/SIGGRAPH_2017/Demo_Windows.zip&#34;&gt;Windows Demo&lt;/a&gt; - &lt;a href=&#34;https://starke-consult.de/AI4Animation/SIGGRAPH_2017/Demo_Linux.zip&#34;&gt;Linux Demo&lt;/a&gt; - &lt;a href=&#34;https://starke-consult.de/AI4Animation/SIGGRAPH_2017/Demo_Mac.zip&#34;&gt;Mac Demo&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=Ul0Gilv5wvY&#34;&gt; &lt;img width=&#34;60%&#34; src=&#34;https://img.youtube.com/vi/Ul0Gilv5wvY/0.jpg&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Thesis Fast Forward Presentation from SIGGRAPH 2020&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=wNqpSk4FhSw&#34;&gt; &lt;img width=&#34;100%&#34; src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/Other/ThesisFastForward.jpg&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Copyright Information&lt;/h1&gt; &#xA;&lt;p&gt;This project is only for research or education purposes, and not freely available for commercial use or redistribution. The motion capture data is available only under the terms of the &lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/legalcode&#34;&gt;Attribution-NonCommercial 4.0 International&lt;/a&gt; (CC BY-NC 4.0) license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>git-ecosystem/git-credential-manager</title>
    <updated>2024-07-18T01:28:55Z</updated>
    <id>tag:github.com,2024-07-18:/git-ecosystem/git-credential-manager</id>
    <link href="https://github.com/git-ecosystem/git-credential-manager" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Secure, cross-platform Git credential storage with authentication to GitHub, Azure Repos, and other popular Git hosting services.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Git Credential Manager&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/git-ecosystem/git-credential-manager/actions/workflows/continuous-integration.yml&#34;&gt;&lt;img src=&#34;https://github.com/git-ecosystem/git-credential-manager/actions/workflows/continuous-integration.yml/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/git-ecosystem/git-credential-manager&#34;&gt;Git Credential Manager&lt;/a&gt; (GCM) is a secure &lt;a href=&#34;https://git-scm.com/docs/gitcredentials&#34;&gt;Git credential helper&lt;/a&gt; built on &lt;a href=&#34;https://dotnet.microsoft.com&#34;&gt;.NET&lt;/a&gt; that runs on Windows, macOS, and Linux. It aims to provide a consistent and secure authentication experience, including multi-factor auth, to every major source control hosting service and platform.&lt;/p&gt; &#xA;&lt;p&gt;GCM supports (in alphabetical order) &lt;a href=&#34;https://azure.microsoft.com/en-us/products/devops&#34;&gt;Azure DevOps&lt;/a&gt;, Azure DevOps Server (formerly Team Foundation Server), Bitbucket, GitHub, and GitLab. Compare to Git&#39;s &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage&#34;&gt;built-in credential helpers&lt;/a&gt; (Windows: wincred, macOS: osxkeychain, Linux: gnome-keyring/libsecret), which provide single-factor authentication support for username/password only.&lt;/p&gt; &#xA;&lt;p&gt;GCM replaces both the .NET Framework-based &lt;a href=&#34;https://github.com/microsoft/Git-Credential-Manager-for-Windows&#34;&gt;Git Credential Manager for Windows&lt;/a&gt; and the Java-based &lt;a href=&#34;https://github.com/microsoft/Git-Credential-Manager-for-Mac-and-Linux&#34;&gt;Git Credential Manager for Mac and Linux&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/git-ecosystem/git-credential-manager/raw/release/docs/install.md&#34;&gt;installation instructions&lt;/a&gt; for the current version of GCM for install options for your operating system.&lt;/p&gt; &#xA;&lt;h2&gt;Current status&lt;/h2&gt; &#xA;&lt;p&gt;Git Credential Manager is currently available for Windows, macOS, and Linux*. GCM only works with HTTP(S) remotes; you can still use Git with SSH:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/devops/repos/git/use-ssh-keys-to-authenticate?view=azure-devops&#34;&gt;Azure DevOps SSH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/articles/connecting-to-github-with-ssh&#34;&gt;GitHub SSH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://confluence.atlassian.com/bitbucket/ssh-keys-935365775.html&#34;&gt;Bitbucket SSH&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Feature&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Windows&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;macOS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Linux*&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Installer/uninstaller&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Secure platform credential storage &lt;a href=&#34;https://github.com/git-ecosystem/git-credential-manager/raw/release/docs/credstores.md&#34;&gt;(see more)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi-factor authentication support for Azure DevOps&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Two-factor authentication support for GitHub&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Two-factor authentication support for Bitbucket&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Two-factor authentication support for GitLab&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows Integrated Authentication (NTLM/Kerberos) support&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;N/A&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;N/A&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Basic HTTP authentication support&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Proxy support&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;amd64&lt;/code&gt; support&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;x86&lt;/code&gt; support&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;N/A&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✗&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;arm64&lt;/code&gt; support&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;best effort&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;best effort, no packages&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;armhf&lt;/code&gt; support&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;N/A&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;N/A&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;best effort, no packages&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;(*) GCM guarantees support only for &lt;a href=&#34;https://learn.microsoft.com/en-us/dotnet/core/install/linux&#34;&gt;the Linux distributions that are officially supported by dotnet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Git versions&lt;/h2&gt; &#xA;&lt;p&gt;Git Credential Manager tries to be compatible with the broadest set of Git versions (within reason). However there are some know problematic releases of Git that are not compatible.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Git 1.x&lt;/p&gt; &lt;p&gt;The initial major version of Git is not supported or tested with GCM.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Git 2.26.2&lt;/p&gt; &lt;p&gt;This version of Git introduced a breaking change with parsing credential configuration that GCM relies on. This issue was fixed in commit &lt;a href=&#34;https://github.com/git/git/commit/12294990c90e043862be9eb7eb22c3784b526340&#34;&gt;&lt;code&gt;12294990&lt;/code&gt;&lt;/a&gt; of the Git project, and released in Git 2.27.0.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;p&gt;Once it&#39;s installed and configured, Git Credential Manager is called implicitly by Git. You don&#39;t have to do anything special, and GCM isn&#39;t intended to be called directly by the user. For example, when pushing (&lt;code&gt;git push&lt;/code&gt;) to &lt;a href=&#34;https://azure.microsoft.com/en-us/products/devops&#34;&gt;Azure DevOps&lt;/a&gt;, &lt;a href=&#34;https://bitbucket.org&#34;&gt;Bitbucket&lt;/a&gt;, or &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt;, a window will automatically open and walk you through the sign-in process. (This process will look slightly different for each Git host, and even in some cases, whether you&#39;ve connected to an on-premises or cloud-hosted Git host.) Later Git commands in the same repository will re-use existing credentials or tokens that GCM has stored for as long as they&#39;re valid.&lt;/p&gt; &#xA;&lt;p&gt;Read full command line usage &lt;a href=&#34;https://github.com/git-ecosystem/git-credential-manager/raw/release/docs/usage.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Configuring a proxy&lt;/h3&gt; &#xA;&lt;p&gt;See detailed information &lt;a href=&#34;https://github.com/git-ecosystem/git-credential-manager/raw/release/docs/netconfig.md#http-proxy&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Additional Resources&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/git-ecosystem/git-credential-manager/raw/release/docs/README.md&#34;&gt;documentation index&lt;/a&gt; for links to additional resources.&lt;/p&gt; &#xA;&lt;h2&gt;Experimental Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/git-ecosystem/git-credential-manager/raw/release/docs/windows-broker.md&#34;&gt;Windows broker (experimental)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Future features&lt;/h2&gt; &#xA;&lt;p&gt;Curious about what&#39;s coming next in the GCM project? Take a look at the &lt;a href=&#34;https://github.com/git-ecosystem/git-credential-manager/milestones?direction=desc&amp;amp;sort=due_date&amp;amp;state=open&#34;&gt;project roadmap&lt;/a&gt;! You can find more details about the construction of the roadmap and how to interpret it &lt;a href=&#34;https://github.com/git-ecosystem/git-credential-manager/discussions/1203&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. See the &lt;a href=&#34;https://raw.githubusercontent.com/git-ecosystem/git-credential-manager/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;p&gt;This project follows &lt;a href=&#34;https://raw.githubusercontent.com/git-ecosystem/git-credential-manager/main/CODE_OF_CONDUCT.md&#34;&gt;GitHub&#39;s Open Source Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re &lt;a href=&#34;https://raw.githubusercontent.com/git-ecosystem/git-credential-manager/main/LICENSE&#34;&gt;MIT&lt;/a&gt; licensed. When using GitHub logos, please be sure to follow the &lt;a href=&#34;https://github.com/logos&#34;&gt;GitHub logo guidelines&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>