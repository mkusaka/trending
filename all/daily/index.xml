<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-18T01:29:20Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>deepseek-ai/DeepEP</title>
    <updated>2025-06-18T01:29:20Z</updated>
    <id>tag:github.com,2025-06-18:/deepseek-ai/DeepEP</id>
    <link href="https://github.com/deepseek-ai/DeepEP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DeepEP: an efficient expert-parallel communication library&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepEP&lt;/h1&gt; &#xA;&lt;p&gt;DeepEP is a communication library tailored for Mixture-of-Experts (MoE) and expert parallelism (EP). It provides high-throughput and low-latency all-to-all GPU kernels, which are also known as MoE dispatch and combine. The library also supports low-precision operations, including FP8.&lt;/p&gt; &#xA;&lt;p&gt;To align with the group-limited gating algorithm proposed in the &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-V3&#34;&gt;DeepSeek-V3&lt;/a&gt; paper, DeepEP offers a set of kernels optimized for asymmetric-domain bandwidth forwarding, such as forwarding data from NVLink domain to RDMA domain. These kernels deliver high throughput, making them suitable for both training and inference prefilling tasks. Additionally, they support SM (Streaming Multiprocessors) number control.&lt;/p&gt; &#xA;&lt;p&gt;For latency-sensitive inference decoding, DeepEP includes a set of low-latency kernels with pure RDMA to minimize delays. The library also introduces a hook-based communication-computation overlapping method that does not occupy any SM resource.&lt;/p&gt; &#xA;&lt;p&gt;Notice: the implementation in this library may have some slight differences from the &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-V3&#34;&gt;DeepSeek-V3&lt;/a&gt; paper.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;h3&gt;Normal kernels with NVLink and RDMA forwarding&lt;/h3&gt; &#xA;&lt;p&gt;We test normal kernels on H800 (~160 GB/s NVLink maximum bandwidth), with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (~50 GB/s maximum bandwidth). And we follow the DeepSeek-V3/R1 pretraining setting (4096 tokens per batch, 7168 hidden, top-4 groups, top-8 experts, FP8 dispatching and BF16 combining).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dispatch #EP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Bottleneck bandwidth&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Combine #EP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Bottleneck bandwidth&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Intranode&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;153 GB/s (NVLink)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;158 GB/s (NVLink)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Internode&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43 GB/s (RDMA)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43 GB/s (RDMA)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Internode&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58 GB/s (RDMA)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57 GB/s (RDMA)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Internode&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51 GB/s (RDMA)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50 GB/s (RDMA)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;News (2025.04.22)&lt;/strong&gt;: with optimizations from Tencent Network Platform Department, performance was enhanced by up to 30%, see &lt;a href=&#34;https://github.com/deepseek-ai/DeepEP/pull/130&#34;&gt;#130&lt;/a&gt; for more details. Thanks for the contribution!&lt;/p&gt; &#xA;&lt;h3&gt;Low-latency kernels with pure RDMA&lt;/h3&gt; &#xA;&lt;p&gt;We test low-latency kernels on H800 with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (~50 GB/s maximum bandwidth). And we follow a typical DeepSeek-V3/R1 production setting (128 tokens per batch, 7168 hidden, top-8 experts, FP8 dispatching and BF16 combining).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dispatch #EP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Latency&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;RDMA bandwidth&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Combine #EP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Latency&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;RDMA bandwidth&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77 us&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;98 GB/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;114 us&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;127 GB/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;118 us&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63 GB/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;195 us&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74 GB/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;155 us&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48 GB/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;273 us&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53 GB/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;173 us&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43 GB/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;314 us&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46 GB/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;192 us&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39 GB/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;369 us&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39 GB/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;194 us&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39 GB/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;360 us&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40 GB/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;News (2025.06.05)&lt;/strong&gt;: low-latency kernels now leverage NVLink as much as possible, see &lt;a href=&#34;https://github.com/deepseek-ai/DeepEP/pull/173&#34;&gt;#173&lt;/a&gt; for more details. Thanks for the contribution!&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ampere (SM80), Hopper (SM90) GPUs, or other architectures with SM90 PTX ISA support&lt;/li&gt; &#xA; &lt;li&gt;Python 3.8 and above&lt;/li&gt; &#xA; &lt;li&gt;CUDA version &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;CUDA 11.0 and above for SM80 GPUs&lt;/li&gt; &#xA;   &lt;li&gt;CUDA 12.3 and above for SM90 GPUs&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;PyTorch 2.1 and above&lt;/li&gt; &#xA; &lt;li&gt;NVLink for intranode communication&lt;/li&gt; &#xA; &lt;li&gt;RDMA network for internode communication&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Download and install NVSHMEM dependency&lt;/h3&gt; &#xA;&lt;p&gt;DeepEP also depends on our modified NVSHMEM. Please refer to our &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepEP/main/third-party/README.md&#34;&gt;NVSHMEM Installation Guide&lt;/a&gt; for instructions.&lt;/p&gt; &#xA;&lt;h3&gt;Development&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Build and make symbolic links for SO files&#xA;NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py build&#xA;# You may modify the specific SO names according to your own platform&#xA;ln -s build/lib.linux-x86_64-cpython-38/deep_ep_cpp.cpython-38-x86_64-linux-gnu.so&#xA;&#xA;# Run test cases&#xA;# NOTES: you may modify the `init_dist` function in `tests/utils.py`&#xA;# according to your own cluster settings, and launch into multiple nodes &#xA;python tests/test_intranode.py&#xA;python tests/test_internode.py&#xA;python tests/test_low_latency.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Installation environment variables&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;NVSHMEM_DIR&lt;/code&gt;: the path to the NVSHMEM directory, disable all internode and low-latency features if not specified&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DISABLE_SM90_FEATURES&lt;/code&gt;: 0 or 1, whether to disable SM90 features, it is required for SM90 devices or CUDA 11&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TORCH_CUDA_ARCH_LIST&lt;/code&gt;: the list of target architectures, e.g. &lt;code&gt;TORCH_CUDA_ARCH_LIST=&#34;9.0&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DISABLE_AGGRESSIVE_PTX_INSTRS&lt;/code&gt;: 0 or 1, whether to disable aggressive load/store instructions, see &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepEP/main/#undefined-behavior-ptx-usage&#34;&gt;Undefine behavior PTX usage&lt;/a&gt; for more details&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then, import &lt;code&gt;deep_ep&lt;/code&gt; in your Python project, and enjoy!&lt;/p&gt; &#xA;&lt;h2&gt;Network configurations&lt;/h2&gt; &#xA;&lt;p&gt;DeepEP is fully tested with InfiniBand networks. However, it is theoretically compatible with RDMA over Converged Ethernet (RoCE) as well.&lt;/p&gt; &#xA;&lt;h3&gt;Traffic isolation&lt;/h3&gt; &#xA;&lt;p&gt;Traffic isolation is supported by InfiniBand through Virtual Lanes (VL).&lt;/p&gt; &#xA;&lt;p&gt;To prevent interference between different types of traffic, we recommend segregating workloads across different virtual lanes as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;workloads using normal kernels&lt;/li&gt; &#xA; &lt;li&gt;workloads using low-latency kernels&lt;/li&gt; &#xA; &lt;li&gt;other workloads&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For DeepEP, you can control the virtual lane assignment by setting the &lt;code&gt;NVSHMEM_IB_SL&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;h3&gt;Adaptive routing&lt;/h3&gt; &#xA;&lt;p&gt;Adaptive routing is an advanced routing feature provided by InfiniBand switches that can evenly distribute traffic across multiple paths. Enabling adaptive routing can completely eliminate network congestion caused by routing conflicts, but it also introduces additional latency. We recommend the following configuration for optimal performance:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;enable adaptive routing in environments with heavy network loads&lt;/li&gt; &#xA; &lt;li&gt;use static routing in environments with light network loads&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Congestion control&lt;/h3&gt; &#xA;&lt;p&gt;Congestion control is disabled as we have not observed significant congestion in our production environment.&lt;/p&gt; &#xA;&lt;h2&gt;Interfaces and examples&lt;/h2&gt; &#xA;&lt;h3&gt;Example use in model training or inference prefilling&lt;/h3&gt; &#xA;&lt;p&gt;The normal kernels can be used in model training or the inference prefilling phase (without the backward part) as the below example code shows.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import torch.distributed as dist&#xA;from typing import List, Tuple, Optional, Union&#xA;&#xA;from deep_ep import Buffer, EventOverlap&#xA;&#xA;# Communication buffer (will allocate at runtime)&#xA;_buffer: Optional[Buffer] = None&#xA;&#xA;# Set the number of SMs to use&#xA;# NOTES: this is a static variable&#xA;Buffer.set_num_sms(24)&#xA;&#xA;&#xA;# You may call this function at the framework initialization&#xA;def get_buffer(group: dist.ProcessGroup, hidden_bytes: int) -&amp;gt; Buffer:&#xA;    global _buffer&#xA;    &#xA;    # NOTES: you may also replace `get_*_config` with your auto-tuned results via all the tests&#xA;    num_nvl_bytes, num_rdma_bytes = 0, 0&#xA;    for config in (Buffer.get_dispatch_config(group.size()), Buffer.get_combine_config(group.size())):&#xA;        num_nvl_bytes = max(config.get_nvl_buffer_size_hint(hidden_bytes, group.size()), num_nvl_bytes)&#xA;        num_rdma_bytes = max(config.get_rdma_buffer_size_hint(hidden_bytes, group.size()), num_rdma_bytes)&#xA;&#xA;    # Allocate a buffer if not existed or not enough buffer size&#xA;    if _buffer is None or _buffer.group != group or _buffer.num_nvl_bytes &amp;lt; num_nvl_bytes or _buffer.num_rdma_bytes &amp;lt; num_rdma_bytes:&#xA;        _buffer = Buffer(group, num_nvl_bytes, num_rdma_bytes)&#xA;    return _buffer&#xA;&#xA;&#xA;def get_hidden_bytes(x: torch.Tensor) -&amp;gt; int:&#xA;    t = x[0] if isinstance(x, tuple) else x&#xA;    return t.size(1) * max(t.element_size(), 2)&#xA;&#xA;&#xA;def dispatch_forward(x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],&#xA;                     topk_idx: torch.Tensor, topk_weights: torch.Tensor,&#xA;                     num_experts: int, previous_event: Optional[EventOverlap] = None) -&amp;gt; \&#xA;        Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], torch.Tensor, torch.Tensor, List, Tuple, EventOverlap]:&#xA;    # NOTES: an optional `previous_event` means a CUDA event captured that you want to make it as a dependency &#xA;    # of the dispatch kernel, it may be useful with communication-computation overlap. For more information, please&#xA;    # refer to the docs of `Buffer.dispatch`&#xA;    global _buffer&#xA;&#xA;    # Calculate layout before actual dispatch&#xA;    num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert, is_token_in_rank, previous_event = \&#xA;        _buffer.get_dispatch_layout(topk_idx, num_experts,&#xA;                                    previous_event=previous_event, async_finish=True,&#xA;                                    allocate_on_comm_stream=previous_event is not None)&#xA;    # Do MoE dispatch&#xA;    # NOTES: the CPU will wait for GPU&#39;s signal to arrive, so this is not compatible with CUDA graph&#xA;    # Unless you specify `num_worst_tokens`, but this flag is for intranode only&#xA;    # For more advanced usages, please refer to the docs of the `dispatch` function&#xA;    recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event = \&#xA;        _buffer.dispatch(x, topk_idx=topk_idx, topk_weights=topk_weights,&#xA;                         num_tokens_per_rank=num_tokens_per_rank, num_tokens_per_rdma_rank=num_tokens_per_rdma_rank,&#xA;                         is_token_in_rank=is_token_in_rank, num_tokens_per_expert=num_tokens_per_expert,&#xA;                         previous_event=previous_event, async_finish=True,&#xA;                         allocate_on_comm_stream=True)&#xA;    # For event management, please refer to the docs of the `EventOverlap` class&#xA;    return recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event&#xA;&#xA;&#xA;def dispatch_backward(grad_recv_x: torch.Tensor, grad_recv_topk_weights: torch.Tensor, handle: Tuple) -&amp;gt; \&#xA;        Tuple[torch.Tensor, torch.Tensor, EventOverlap]:&#xA;    global _buffer&#xA;&#xA;    # The backward process of MoE dispatch is actually a combine&#xA;    # For more advanced usages, please refer to the docs of the `combine` function&#xA;    combined_grad_x, combined_grad_recv_topk_weights, event = \&#xA;        _buffer.combine(grad_recv_x, handle, topk_weights=grad_recv_topk_weights, async_finish=True)&#xA;&#xA;    # For event management, please refer to the docs of the `EventOverlap` class&#xA;    return combined_grad_x, combined_grad_recv_topk_weights, event&#xA;&#xA;&#xA;def combine_forward(x: torch.Tensor, handle: Tuple, previous_event: Optional[EventOverlap] = None) -&amp;gt; \&#xA;        Tuple[torch.Tensor, EventOverlap]:&#xA;    global _buffer&#xA;&#xA;    # Do MoE combine&#xA;    # For more advanced usages, please refer to the docs of the `combine` function&#xA;    combined_x, _, event = _buffer.combine(x, handle, async_finish=True, previous_event=previous_event,&#xA;                                           allocate_on_comm_stream=previous_event is not None)&#xA;&#xA;    # For event management, please refer to the docs of the `EventOverlap` class&#xA;    return combined_x, event&#xA;&#xA;&#xA;def combine_backward(grad_combined_x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],&#xA;                     handle: Tuple, previous_event: Optional[EventOverlap] = None) -&amp;gt; \&#xA;        Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], EventOverlap]:&#xA;    global _buffer&#xA;&#xA;    # The backward process of MoE combine is actually a dispatch&#xA;    # For more advanced usages, please refer to the docs of the `dispatch` function&#xA;    grad_x, _, _, _, _, event = _buffer.dispatch(grad_combined_x, handle=handle, async_finish=True,&#xA;                                                 previous_event=previous_event,&#xA;                                                 allocate_on_comm_stream=previous_event is not None)&#xA;&#xA;    # For event management, please refer to the docs of the `EventOverlap` class&#xA;    return grad_x, event&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Moreover, inside the dispatch function, we may not know how many tokens to receive for the current rank. So an implicit CPU wait for GPU received count signal will be involved, as the following figure shows.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepEP/main/figures/normal.png&#34; alt=&#34;normal&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Example use in inference decoding&lt;/h3&gt; &#xA;&lt;p&gt;The low latency kernels can be used in the inference decoding phase as the below example code shows.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import torch.distributed as dist&#xA;from typing import Tuple, Optional&#xA;&#xA;from deep_ep import Buffer&#xA;&#xA;# Communication buffer (will allocate at runtime)&#xA;# NOTES: there is no SM control API for the low-latency kernels&#xA;_buffer: Optional[Buffer] = None&#xA;&#xA;&#xA;# You may call this function at the framework initialization&#xA;def get_buffer(group: dist.ProcessGroup, num_max_dispatch_tokens_per_rank: int, hidden: int, num_experts: int) -&amp;gt; Buffer:&#xA;    # NOTES: the low-latency mode will consume much more space than the normal mode&#xA;    # So we recommend that `num_max_dispatch_tokens_per_rank` (the actual batch size in the decoding engine) should be less than 256&#xA;    global _buffer&#xA;    num_rdma_bytes = Buffer.get_low_latency_rdma_size_hint(num_max_dispatch_tokens_per_rank, hidden, group.size(), num_experts)&#xA;&#xA;    # Allocate a buffer if not existed or not enough buffer size&#xA;    if _buffer is None or _buffer.group != group or not _buffer.low_latency_mode or _buffer.num_rdma_bytes &amp;lt; num_rdma_bytes:&#xA;        # NOTES: for the best performance, the QP number **must** be equal to the number of the local experts&#xA;        assert num_experts % group.size() == 0&#xA;        _buffer = Buffer(group, 0, num_rdma_bytes, low_latency_mode=True, num_qps_per_rank=num_experts // group.size())&#xA;    return _buffer&#xA;&#xA;&#xA;def low_latency_dispatch(hidden_states: torch.Tensor, topk_idx: torch.Tensor, num_max_dispatch_tokens_per_rank: int, num_experts: int):&#xA;    global _buffer&#xA;&#xA;    # Do MoE dispatch, compatible with CUDA graph (but you may restore some buffer status once you replay)&#xA;    recv_hidden_states, recv_expert_count, handle, event, hook = \&#xA;        _buffer.low_latency_dispatch(hidden_states, topk_idx, num_max_dispatch_tokens_per_rank, num_experts,&#xA;                                     async_finish=False, return_recv_hook=True)&#xA;&#xA;    # NOTES: the actual tensor will not be received only if you call `hook()`,&#xA;    # it is useful for double-batch overlapping, but **without any SM occupation**&#xA;    # If you don&#39;t want to overlap, please set `return_recv_hook=False`&#xA;    # Later, you can use our GEMM library to do the computation with this specific format&#xA;    return recv_hidden_states, recv_expert_count, handle, event, hook&#xA;&#xA;&#xA;def low_latency_combine(hidden_states: torch.Tensor,&#xA;                        topk_idx: torch.Tensor, topk_weights: torch.Tensor, handle: Tuple):&#xA;    global _buffer&#xA;&#xA;    # Do MoE combine, compatible with CUDA graph (but you may restore some buffer status once you replay)&#xA;    combined_hidden_states, event_overlap, hook = \&#xA;        _buffer.low_latency_combine(hidden_states, topk_idx, topk_weights, handle,&#xA;                                    async_finish=False, return_recv_hook=True)&#xA;&#xA;    # NOTES: the same behavior as described in the dispatch kernel&#xA;    return combined_hidden_states, event_overlap, hook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For two-micro-batch overlapping, you can refer to the following figure. With our receiving hook interface, the RDMA network traffic is happening in the background, without costing any GPU SMs from the computation part. But notice, the overlapped parts can be adjusted, i.e., the 4 parts of attention/dispatch/MoE/combine may not have the exact same execution time. You may adjust the stage settings according to your workload.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepEP/main/figures/low-latency.png&#34; alt=&#34;low-latency&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; AR support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Refactor low-latency mode AR code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; A100 support (intranode only)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support BF16 for the low-latency dispatch kernel&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support NVLink protocol for intranode low-latency kernels&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TMA copy instead of LD/ST &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Intranode kernels&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Internode kernels&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Low-latency kernels&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; SM-free kernels and refactors&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notices&lt;/h2&gt; &#xA;&lt;h4&gt;Easier potential overall design&lt;/h4&gt; &#xA;&lt;p&gt;The current DeepEP implementation uses queues for communication buffers which save memory but introduce complexity and potential deadlocks. If you&#39;re implementing your own version based on DeepEP, consider using fixed-size buffers allocated to maximum capacity for simplicity and better performance. For a detailed discussion of this alternative approach, see &lt;a href=&#34;https://github.com/deepseek-ai/DeepEP/issues/39&#34;&gt;https://github.com/deepseek-ai/DeepEP/issues/39&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Undefined-behavior PTX usage&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For extreme performance, we discover and use an undefined-behavior PTX usage: using read-only PTX &lt;code&gt;ld.global.nc.L1::no_allocate.L2::256B&lt;/code&gt; to &lt;strong&gt;read volatile data&lt;/strong&gt;. The PTX modifier &lt;code&gt;.nc&lt;/code&gt; indicates that a non-coherent cache is used. But the correctness is tested to be guaranteed with &lt;code&gt;.L1::no_allocate&lt;/code&gt; on Hopper architectures, and performance will be much better. The reason we guess may be: the non-coherent cache is unified with L1, and the L1 modifier is not just a hint but a strong option, so that the correctness can be guaranteed by no dirty data in L1.&lt;/li&gt; &#xA; &lt;li&gt;Initially, because NVCC could not automatically unroll volatile read PTX, we tried using &lt;code&gt;__ldg&lt;/code&gt; (i.e., &lt;code&gt;ld.nc&lt;/code&gt;). Even compared to manually unrolled volatile reads, it was significantly faster (likely due to additional compiler optimizations). However, the results could be incorrect or dirty. After consulting the PTX documentation, we discovered that L1 and non-coherent cache are unified on Hopper architectures. We speculated that &lt;code&gt;.L1::no_allocate&lt;/code&gt; might resolve the issue, leading to this discovery.&lt;/li&gt; &#xA; &lt;li&gt;If you find kernels not working on some other platforms, you may add &lt;code&gt;DISABLE_AGGRESSIVE_PTX_INSTRS=1&lt;/code&gt; to &lt;code&gt;setup.py&lt;/code&gt; and disable this, or file an issue.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Auto-tuning on your cluster&lt;/h4&gt; &#xA;&lt;p&gt;For better performance on your cluster, we recommend to run all the tests and use the best auto-tuned configuration. The default configurations are optimized on the DeepSeek&#39;s internal cluster.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This code repository is released under &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepEP/main/LICENSE&#34;&gt;the MIT License&lt;/a&gt;, except for codes that reference NVSHMEM (including &lt;code&gt;csrc/kernels/ibgda_device.cuh&lt;/code&gt; and &lt;code&gt;third-party/nvshmem.patch&lt;/code&gt;), which are subject to &lt;a href=&#34;https://docs.nvidia.com/nvshmem/api/sla.html&#34;&gt;NVSHMEM SLA&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community Forks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Infrawaves/DeepEP_ibrc_dual-ports_multiQP&#34;&gt;Infrawaves/DeepEP_ibrc_dual-ports_multiQP&lt;/a&gt; - Adds multi-QP solution and dual-port NIC support in IBRC transport&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this codebase or otherwise find our work valuable, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{deepep2025,&#xA;      title={DeepEP: an efficient expert-parallel communication library},&#xA;      author={Chenggang Zhao and Shangyan Zhou and Liyue Zhang and Chengqi Deng and Zhean Xu and Yuxuan Liu and Kuai Yu and Jiashi Li and Liang Zhao},&#xA;      year={2025},&#xA;      publisher = {GitHub},&#xA;      howpublished = {\url{https://github.com/deepseek-ai/DeepEP}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>menloresearch/jan</title>
    <updated>2025-06-18T01:29:20Z</updated>
    <id>tag:github.com,2025-06-18:/menloresearch/jan</id>
    <link href="https://github.com/menloresearch/jan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Jan is an open source alternative to ChatGPT that runs 100% offline on your computer&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Jan - Local AI Assistant&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/menloresearch/jan/dev/JanBanner.png&#34; alt=&#34;Jan banner&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section --&gt; &lt;img alt=&#34;GitHub commit activity&#34; src=&#34;https://img.shields.io/github/commit-activity/m/menloresearch/jan&#34;&gt; &lt;img alt=&#34;Github Last Commit&#34; src=&#34;https://img.shields.io/github/last-commit/menloresearch/jan&#34;&gt; &lt;img alt=&#34;Github Contributors&#34; src=&#34;https://img.shields.io/github/contributors/menloresearch/jan&#34;&gt; &lt;img alt=&#34;GitHub closed issues&#34; src=&#34;https://img.shields.io/github/issues-closed/menloresearch/jan&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/1107178041848909847?label=discord&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://jan.ai/docs/quickstart&#34;&gt;Getting Started&lt;/a&gt; - &lt;a href=&#34;https://jan.ai/docs&#34;&gt;Docs&lt;/a&gt; - &lt;a href=&#34;https://jan.ai/changelog&#34;&gt;Changelog&lt;/a&gt; - &lt;a href=&#34;https://github.com/menloresearch/jan/issues&#34;&gt;Bug reports&lt;/a&gt; - &lt;a href=&#34;https://discord.gg/AsJ8krTT3N&#34;&gt;Discord&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Jan is your AI Assistant that runs entirely offline on your desktop. Because in a world where everything is shared, sometimes you just want to keep your conversations between you and your computer‚Äîcall us old-fashioned.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Jan is in active development.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Because clicking a button is still the easiest way to get started:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;Platform&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Stable&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Beta&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Nightly&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;Windows&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.jan.ai/download/latest/win-x64&#34;&gt;jan.exe&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.jan.ai/download/beta/win-x64&#34;&gt;jan.exe&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.jan.ai/download/nightly/win-x64&#34;&gt;jan.exe&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;macOS&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.jan.ai/download/latest/mac-universal&#34;&gt;jan.dmg&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.jan.ai/download/beta/mac-universal&#34;&gt;jan.dmg&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.jan.ai/download/nightly/mac-universal&#34;&gt;jan.dmg&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;Linux (deb)&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.jan.ai/download/latest/linux-amd64-deb&#34;&gt;jan.deb&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.jan.ai/download/beta/linux-amd64-deb&#34;&gt;jan.deb&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.jan.ai/download/nightly/linux-amd64-deb&#34;&gt;jan.deb&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;Linux (AppImage)&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.jan.ai/download/latest/linux-amd64-appimage&#34;&gt;jan.AppImage&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.jan.ai/download/beta/linux-amd64-appimage&#34;&gt;jan.AppImage&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://app.jan.ai/download/nightly/linux-amd64-appimage&#34;&gt;jan.AppImage&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Download from &lt;a href=&#34;https://jan.ai/&#34;&gt;jan.ai&lt;/a&gt; or &lt;a href=&#34;https://github.com/menloresearch/jan/releases&#34;&gt;GitHub Releases&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;video width=&#34;100%&#34; controls&gt; &#xA; &lt;source src=&#34;./docs/public/assets/videos/enable-tool-call-for-models.mp4&#34; type=&#34;video/mp4&#34;&gt; Your browser does not support the video tag. &#xA;&lt;/video&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local AI Models&lt;/strong&gt;: Download and run LLMs (Llama, Gemma, Qwen, etc.) from HuggingFace&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cloud Integration&lt;/strong&gt;: Connect to OpenAI, Anthropic, Mistral, Groq, and others&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Custom Assistants&lt;/strong&gt;: Create specialized AI assistants for your tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI-Compatible API&lt;/strong&gt;: Local server at &lt;code&gt;localhost:1337&lt;/code&gt; for other applications&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model Context Protocol&lt;/strong&gt;: MCP integration for enhanced capabilities&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Privacy First&lt;/strong&gt;: Everything runs locally when you want it to&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build from Source&lt;/h2&gt; &#xA;&lt;p&gt;For those who enjoy the scenic route:&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Node.js ‚â• 20.0.0&lt;/li&gt; &#xA; &lt;li&gt;Yarn ‚â• 1.22.0&lt;/li&gt; &#xA; &lt;li&gt;Make ‚â• 3.81&lt;/li&gt; &#xA; &lt;li&gt;Rust (for Tauri)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/menloresearch/jan&#xA;cd jan&#xA;make dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This handles everything: installs dependencies, builds core components, and launches the app.&lt;/p&gt; &#xA;&lt;h3&gt;Alternative Commands&lt;/h3&gt; &#xA;&lt;p&gt;If you prefer the verbose approach:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Setup and development&#xA;yarn install&#xA;yarn build:core&#xA;yarn build:extensions&#xA;yarn dev&#xA;&#xA;# Production build&#xA;yarn build&#xA;&#xA;# Clean slate (when things inevitably break)&#xA;make clean&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Available Make Targets&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;make dev&lt;/code&gt; - Full development setup and launch (recommended)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make dev-tauri&lt;/code&gt; - Tauri development (deprecated, use &lt;code&gt;make dev&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make build&lt;/code&gt; - Production build&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make install-and-build&lt;/code&gt; - Install dependencies and build core/extensions&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make test&lt;/code&gt; - Run tests and linting&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make lint&lt;/code&gt; - Check your code doesn&#39;t offend the linters&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt; - Nuclear option: delete everything and start fresh&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;System Requirements&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Minimum specs for a decent experience:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: 13.6+ (8GB RAM for 3B models, 16GB for 7B, 32GB for 13B)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: 10+ with GPU support for NVIDIA/AMD/Intel Arc&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: Most distributions work, GPU acceleration available&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For detailed compatibility, check our &lt;a href=&#34;https://jan.ai/docs/installation&#34;&gt;installation guides&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;When things go sideways (they will):&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Check our &lt;a href=&#34;https://jan.ai/docs/troubleshooting&#34;&gt;troubleshooting docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copy your error logs and system specs&lt;/li&gt; &#xA; &lt;li&gt;Ask for help in our &lt;a href=&#34;https://discord.gg/FTk2MvZwJH&#34;&gt;Discord&lt;/a&gt; &lt;code&gt;#üÜò|jan-help&lt;/code&gt; channel&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We keep logs for 24 hours, so don&#39;t procrastinate on reporting issues.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions welcome. See &lt;a href=&#34;https://raw.githubusercontent.com/menloresearch/jan/dev/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for the full spiel.&lt;/p&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jan.ai/docs&#34;&gt;Documentation&lt;/a&gt; - The manual you should read&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jan.ai/api-reference&#34;&gt;API Reference&lt;/a&gt; - For the technically inclined&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jan.ai/changelog&#34;&gt;Changelog&lt;/a&gt; - What we broke and fixed&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/FTk2MvZwJH&#34;&gt;Discord&lt;/a&gt; - Where the community lives&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bugs&lt;/strong&gt;: &lt;a href=&#34;https://github.com/menloresearch/jan/issues&#34;&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Business&lt;/strong&gt;: &lt;a href=&#34;mailto:hello@jan.ai&#34;&gt;hello@jan.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jobs&lt;/strong&gt;: &lt;a href=&#34;mailto:hr@jan.ai&#34;&gt;hr@jan.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;General Discussion&lt;/strong&gt;: &lt;a href=&#34;https://discord.gg/FTk2MvZwJH&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Trust &amp;amp; Safety&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Friendly reminder&lt;/strong&gt;: We&#39;re not trying to scam you.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We won&#39;t ask for personal information&lt;/li&gt; &#xA; &lt;li&gt;Jan is completely free (no premium version exists)&lt;/li&gt; &#xA; &lt;li&gt;We don&#39;t have a cryptocurrency or ICO&lt;/li&gt; &#xA; &lt;li&gt;We&#39;re bootstrapped and not seeking your investment (yet)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache 2.0 - Because sharing is caring.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Built on the shoulders of giants:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;Llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tauri.app/&#34;&gt;Tauri&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/scalar/scalar&#34;&gt;Scalar&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>