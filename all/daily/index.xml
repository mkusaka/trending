<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-09T01:27:41Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hacksider/Deep-Live-Cam</title>
    <updated>2024-08-09T01:27:41Z</updated>
    <id>tag:github.com,2024-08-09:/hacksider/Deep-Live-Cam</id>
    <link href="https://github.com/hacksider/Deep-Live-Cam" rel="alternate"></link>
    <summary type="html">&lt;p&gt;real time face swap and one-click video deepfake with only a single image (uncensored)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/demo.gif&#34; alt=&#34;demo-gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This software is meant to be a productive contribution to the rapidly growing AI-generated media industry. It will help artists with tasks such as animating a custom character or using the character as a model for clothing etc.&lt;/p&gt; &#xA;&lt;p&gt;The developers of this software are aware of its possible unethical applicaitons and are committed to take preventative measures against them. It has a built-in check which prevents the program from working on inappropriate media including but not limited to nudity, graphic content, sensitive material such as war footage etc. We will continue to develop this project in the positive direction while adhering to law and ethics. This project may be shut down or include watermarks on the output if requested by law.&lt;/p&gt; &#xA;&lt;p&gt;Users of this software are expected to use this software responsibly while abiding the local law. If face of a real person is being used, users are suggested to get consent from the concerned person and clearly mention that it is a deepfake when posting content online. Developers of this software will not be responsible for actions of end-users.&lt;/p&gt; &#xA;&lt;h2&gt;How do I install it?&lt;/h2&gt; &#xA;&lt;h3&gt;Basic: It is more likely to work on your computer but it will also be very slow. You can follow instructions for the basic install (This usually runs via &lt;strong&gt;CPU&lt;/strong&gt;)&lt;/h3&gt; &#xA;&lt;h4&gt;1.Setup your platform&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python (3.10 recommended)&lt;/li&gt; &#xA; &lt;li&gt;pip&lt;/li&gt; &#xA; &lt;li&gt;git&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=OlNWCpFdVMA&#34;&gt;ffmpeg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://visualstudio.microsoft.com/visual-cpp-build-tools/&#34;&gt;visual studio 2022 runtimes (windows)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Clone Repository&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/hacksider/Deep-Live-Cam.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Download Models&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth&#34;&gt;GFPGANv1.4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx&#34;&gt;inswapper_128_fp16.onnx&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Then put those 2 files on the &#34;&lt;strong&gt;models&lt;/strong&gt;&#34; folder&lt;/p&gt; &#xA;&lt;h4&gt;4. Install dependency&lt;/h4&gt; &#xA;&lt;p&gt;We highly recommend to work with a &lt;code&gt;venv&lt;/code&gt; to avoid issues.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;DONE!!! If you dont have any GPU, You should be able to run roop using &lt;code&gt;python run.py&lt;/code&gt; command. Keep in mind that while running the program for first time, it will download some models which can take time depending on your network connection.&lt;/h5&gt; &#xA;&lt;h3&gt;*Proceed if you want to use GPU Acceleration&lt;/h3&gt; &#xA;&lt;h3&gt;CUDA Execution Provider (Nvidia)*&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://developer.nvidia.com/cuda-11-8-0-download-archive&#34;&gt;CUDA Toolkit 11.8&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-gpu&#xA;pip install onnxruntime-gpu==1.16.3&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --execution-provider cuda&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/s0md3v/roop/wiki/2.-Acceleration#coreml-execution-provider-apple-silicon&#34;&gt;&lt;/a&gt;CoreML Execution Provider (Apple Silicon)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-silicon&#xA;pip install onnxruntime-silicon==1.13.1&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --execution-provider coreml&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/s0md3v/roop/wiki/2.-Acceleration#coreml-execution-provider-apple-legacy&#34;&gt;&lt;/a&gt;CoreML Execution Provider (Apple Legacy)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-coreml&#xA;pip install onnxruntime-coreml==1.13.1&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --execution-provider coreml&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/s0md3v/roop/wiki/2.-Acceleration#directml-execution-provider-windows&#34;&gt;&lt;/a&gt;DirectML Execution Provider (Windows)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-directml&#xA;pip install onnxruntime-directml==1.15.1&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --execution-provider directml&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/s0md3v/roop/wiki/2.-Acceleration#openvino-execution-provider-intel&#34;&gt;&lt;/a&gt;OpenVINO™ Execution Provider (Intel)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip uninstall onnxruntime onnxruntime-openvino&#xA;pip install onnxruntime-openvino==1.15.0&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Usage in case the provider is available:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --execution-provider openvino&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How do I use it?&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: When you run this program for the first time, it will download some models ~300MB in size.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Executing &lt;code&gt;python run.py&lt;/code&gt; command will launch this window: &lt;img src=&#34;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/instruction.png&#34; alt=&#34;gui-demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Choose a face (image with desired face) and the target image/video (image/video in which you want to replace the face) and click on &lt;code&gt;Start&lt;/code&gt;. Open file explorer and navigate to the directory you select your output to be in. You will find a directory named &lt;code&gt;&amp;lt;video_title&amp;gt;&lt;/code&gt; where you can see the frames being swapped in realtime. Once the processing is done, it will create the output file. That&#39;s it.&lt;/p&gt; &#xA;&lt;h2&gt;For the webcam mode&lt;/h2&gt; &#xA;&lt;p&gt;Just follow the clicks on the screenshot&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Select a face&lt;/li&gt; &#xA; &lt;li&gt;Click live&lt;/li&gt; &#xA; &lt;li&gt;Wait for a few second (it takes a longer time, usually 10 to 30 seconds before the preview shows up)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/demo.gif&#34; alt=&#34;demo-gif&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Just use your favorite screencapture to stream like OBS&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: In case you want to change your face, just select another picture, the preview mode will then restart (so just wait a bit).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Additional command line arguments are given below. To learn out what they do, check &lt;a href=&#34;https://github.com/s0md3v/roop/wiki/Advanced-Options&#34;&gt;this guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;options:&#xA;  -h, --help                                               show this help message and exit&#xA;  -s SOURCE_PATH, --source SOURCE_PATH                     select an source image&#xA;  -t TARGET_PATH, --target TARGET_PATH                     select an target image or video&#xA;  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory&#xA;  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)&#xA;  --keep-fps                                               keep original fps&#xA;  --keep-audio                                             keep original audio&#xA;  --keep-frames                                            keep temporary frames&#xA;  --many-faces                                             process every face&#xA;  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder&#xA;  --video-quality [0-51]                                   adjust output video quality&#xA;  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB&#xA;  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)&#xA;  --execution-threads EXECUTION_THREADS                    number of execution threads&#xA;  -v, --version                                            show program&#39;s version number and exit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/henryruhs&#34;&gt;henryruhs&lt;/a&gt;: for being an irreplaceable contributor to the project&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ffmpeg.org/&#34;&gt;ffmpeg&lt;/a&gt;: for making video related operations easy&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepinsight&#34;&gt;deepinsight&lt;/a&gt;: for their &lt;a href=&#34;https://github.com/deepinsight/insightface&#34;&gt;insightface&lt;/a&gt; project which provided a well-made library and models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/havok2-htwo&#34;&gt;havok2-htwo&lt;/a&gt; : for sharing the code for webcam&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/GosuDRM/nsfw-roop&#34;&gt;GosuDRM&lt;/a&gt; : for uncensoring roop&lt;/li&gt; &#xA; &lt;li&gt;and all developers behind libraries used in this project.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>teableio/teable</title>
    <updated>2024-08-09T01:27:41Z</updated>
    <id>tag:github.com,2024-08-09:/teableio/teable</id>
    <link href="https://github.com/teableio/teable" rel="alternate"></link>
    <summary type="html">&lt;p&gt;✨ The Next Gen Airtable Alternative: No-Code Postgres&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;static/assets/images/teable-vertical-dark.png&#34;&gt; &#xA;   &lt;img alt=&#34;teable logo&#34; height=&#34;150&#34; src=&#34;https://raw.githubusercontent.com/teableio/teable/develop/static/assets/images/teable-vertical-light.png&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/h1&gt; &#xA; &lt;h3 align=&#34;center&#34;&gt;&lt;strong&gt;Postgres-Airtable Fusion&lt;/strong&gt;&lt;/h3&gt; &#xA; &lt;p&gt;Teable is a Super fast, Real-time, Professional, Developer friendly, No-code database built on Postgres. It uses a simple, spreadsheet-like interface to create complex enterprise-level database applications. Unlock efficient app development with no-code, free from the hurdles of data security and scalability. &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://teable.io&#34;&gt;Home&lt;/a&gt; | &lt;a target=&#34;_blank&#34; href=&#34;https://help.teable.io&#34;&gt;Help&lt;/a&gt; | &lt;a target=&#34;_blank&#34; href=&#34;https://Blog.teable.io&#34;&gt;Blog&lt;/a&gt; | &lt;a target=&#34;_blank&#34; href=&#34;https://template.teable.io&#34;&gt;Template&lt;/a&gt; | &lt;a target=&#34;_blank&#34; href=&#34;https://help.teable.io/developer/api&#34;&gt;API&lt;/a&gt; | &lt;a target=&#34;_blank&#34; href=&#34;https://app.teable.io/share/shr04TEw1u9EOQojPmG/view&#34;&gt;Roadmap&lt;/a&gt; | &lt;a target=&#34;_blank&#34; href=&#34;https://discord.gg/uZwp7tDE5W&#34;&gt;Discord&lt;/a&gt; | &lt;a target=&#34;_blank&#34; href=&#34;https://twitter.com/teableio&#34;&gt;Twitter&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a aria-label=&#34;Build&#34; href=&#34;https://github.com/teableio/teable/actions?query=Build%20and%20Push%20to%20Docker%20Registry&#34;&gt; &lt;img alt=&#34;build&#34; src=&#34;https://img.shields.io/github/actions/workflow/status/teableio/teable/docker-push.yml?label=Build&amp;amp;logo=github&amp;amp;style=flat-quare&amp;amp;labelColor=000000&#34;&gt; &lt;/a&gt; &lt;a aria-label=&#34;Codefactor grade&#34; href=&#34;https://www.codefactor.io/repository/github/teableio/teable&#34;&gt; &lt;img alt=&#34;Codefactor&#34; src=&#34;https://img.shields.io/codefactor/grade/github/teableio/teable?label=Codefactor&amp;amp;logo=codefactor&amp;amp;style=flat-quare&amp;amp;labelColor=000000&#34;&gt; &lt;/a&gt; &lt;a aria-label=&#34;Coverage Status&#34; href=&#34;https://coveralls.io/github/teableio/teable?branch=develop&#34;&gt; &lt;img alt=&#34;Coverage&#34; src=&#34;https://coveralls.io/repos/github/teableio/teable/badge.svg?branch=develop&#34;&gt; &lt;/a&gt; &lt;a aria-label=&#34;CodeClimate technical debt&#34; href=&#34;https://codeclimate.com/github/teableio/teable&#34;&gt; &lt;img alt=&#34;Techdebt&#34; src=&#34;https://img.shields.io/codeclimate/tech-debt/teableio/teable?label=TechDebt&amp;amp;logo=code-climate&amp;amp;style=flat-quare&amp;amp;labelColor=000000&#34;&gt; &lt;/a&gt; &lt;a aria-label=&#34;Codacy grade&#34; href=&#34;https://www.codacy.com/gh/teableio/teable/dashboard?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=teableio/teable&amp;amp;utm_campaign=Badge_Grade&#34;&gt; &lt;img alt=&#34;Codacy grade&#34; src=&#34;https://img.shields.io/codacy/grade/dff9c944af284a0fad4e165eb1727467?logo=codacy&amp;amp;style=flat-square&amp;amp;labelColor=000&amp;amp;label=Codacy&#34;&gt; &lt;/a&gt; &lt;a aria-label=&#34;Top language&#34; href=&#34;https://github.com/teableio/teable/search?l=typescript&#34;&gt; &lt;img alt=&#34;GitHub top language&#34; src=&#34;https://img.shields.io/github/languages/top/teableio/teable?style=flat-square&amp;amp;labelColor=000&amp;amp;color=blue&#34;&gt; &lt;/a&gt; &lt;a aria-label=&#34;Licence&#34; href=&#34;https://github.com/teableio/teable/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;Licence&#34; src=&#34;https://img.shields.io/github/license/teableio/teable?style=flat-quare&amp;amp;labelColor=000000&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;static/assets/images/teable-interface-dark.png&#34;&gt; &#xA;  &lt;img alt=&#34;teable interface&#34; width=&#34;100%&#34; src=&#34;https://raw.githubusercontent.com/teableio/teable/develop/static/assets/images/teable-interface-light.png&#34;&gt; &#xA; &lt;/picture&gt; &lt;/h1&gt; &#xA;&lt;h2&gt;Quick Guide&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Looking for a quick experience? Select a scenario from the &lt;a href=&#34;https://template.teable.io&#34;&gt;template center&lt;/a&gt; and click &#34;Use this template&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Seeking high performance? Try the &lt;a href=&#34;https://app.teable.io/share/shrVgdLiOvNQABtW0yX/view&#34;&gt;1 million rows demo&lt;/a&gt; to feel the speed of Teable.&lt;/li&gt; &#xA; &lt;li&gt;Want to learn to use it quickly? Click on this &lt;a href=&#34;https://help.teable.io/quick-start/build-a-simple-base&#34;&gt;tutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Interested in deploying it yourself? Click &lt;a href=&#34;https://railway.app/template/wada5e?referralCode=rE4BjB&#34;&gt;Deploy on Railway&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;✨Features&lt;/h2&gt; &#xA;&lt;h4&gt;📊 Spreadsheet-like Interface&lt;/h4&gt; &#xA;&lt;p&gt;All you want is here&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cell Editing: Directly click and edit content within cells.&lt;/li&gt; &#xA; &lt;li&gt;Formula Support: Input mathematical and logical formulas to auto-calculate values.&lt;/li&gt; &#xA; &lt;li&gt;Data Sorting and Filtering: Sort data based on a column or multiple columns; use filters to view specific rows of data.&lt;/li&gt; &#xA; &lt;li&gt;Aggregation Function: Automatically summarize statistics for each column, providing instant calculations like sum, average, count, max, and min for streamlined data analysis.&lt;/li&gt; &#xA; &lt;li&gt;Data Formatting: formatting numbers, dates, etc.&lt;/li&gt; &#xA; &lt;li&gt;Grouping: Organize rows into collapsible groups based on column values for easier data analysis and navigation.&lt;/li&gt; &#xA; &lt;li&gt;Freeze Columns: Freeze the left column of the table so they remain visible while scrolling.&lt;/li&gt; &#xA; &lt;li&gt;Import/Export Capabilities: Import and export data from other formats, e.g., .csv, .xlsx.&lt;/li&gt; &#xA; &lt;li&gt;Row Styling &amp;amp; Conditional Formatting: Change row styles automatically based on specific conditions. (coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Charts &amp;amp; Visualization Tools: Create charts from table data such as bar charts, pie charts, line graphs, etc. (coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Data Validation: Limit or validate data that are entered into cells. (coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Undo/Redo: Undo or redo recent changes. (coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Comments &amp;amp; Annotations: Attach comments to rows, providing explanations or feedback for other users. (coming soon)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;🗂️ Multiple Views&lt;/h4&gt; &#xA;&lt;p&gt;Visualize and interact with data in various ways best suited for their specific tasks.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Grid View: The default view of the table, which displays data in a spreadsheet-like format.&lt;/li&gt; &#xA; &lt;li&gt;Form View: Input data in a form format, which is useful for collecting data.&lt;/li&gt; &#xA; &lt;li&gt;Kanban View: Displays data in a Kanban board, which is a visual representation of data in columns and cards.&lt;/li&gt; &#xA; &lt;li&gt;Calendar View: Displays data in a calendar format, which is useful for tracking dates and events. (coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Gallery View: Displays data in a gallery format, which is useful for displaying images and other media. (coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Gantt View: Displays data in a Gantt chart, which is useful for tracking project schedules. (coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Timeline View: Displays data in a timeline format, which is useful for tracking events over time. (coming soon)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;🚀 Super Fast&lt;/h4&gt; &#xA;&lt;p&gt;Amazing response speed and data capacity&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Millions of data are easily processed, and there is no pressure to filter and sort&lt;/li&gt; &#xA; &lt;li&gt;Automatic database indexing for maximum speed&lt;/li&gt; &#xA; &lt;li&gt;Supports batch data operations at one time&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;👨‍💻 Full-featured SQL Support&lt;/h4&gt; &#xA;&lt;p&gt;Seamless integration with the software you are familiar with&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;BI tools like Metabase PowerBi...&lt;/li&gt; &#xA; &lt;li&gt;No-code tools like Appsmith...&lt;/li&gt; &#xA; &lt;li&gt;Direct retrieve data with native SQL&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;🔒 Privacy-First&lt;/h4&gt; &#xA;&lt;p&gt;You own your data, in spite of the cloud&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bring your own database (coming soon)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;⚡️ Real-time Collaboration&lt;/h4&gt; &#xA;&lt;p&gt;Designed for teams&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No need to refresh the page, data is updated in real-time&lt;/li&gt; &#xA; &lt;li&gt;Seamlessly integrate collaboration member invitation and management&lt;/li&gt; &#xA; &lt;li&gt;Perfect permission management mechanism, from table to column level&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;🧩 Extensions (Coming Soon)&lt;/h4&gt; &#xA;&lt;p&gt;Expand infinite possibilities&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Backend-less programming capability based on React&lt;/li&gt; &#xA; &lt;li&gt;Customize your own application with extremely low cost&lt;/li&gt; &#xA; &lt;li&gt;Extremely easy-to-use script extensions mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;🤖 Automation (Coming Soon)&lt;/h4&gt; &#xA;&lt;p&gt;Empower data-driven workflows effortlessly and seamlessly&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Design your workflow with AI or Visual programming&lt;/li&gt; &#xA; &lt;li&gt;Super easy to retrieve data from the table&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;🧠 Copilot (Coming Soon)&lt;/h4&gt; &#xA;&lt;p&gt;Native Integrated AI ability&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chat 2 App. &#34;Create a project management app for me&#34;&lt;/li&gt; &#xA; &lt;li&gt;Chat 2 Chart. &#34;Analyze the data in the order table using a bar chart&#34;&lt;/li&gt; &#xA; &lt;li&gt;Chat 2 View. &#34;I want to see the schedule for the past week and only display participants&#34;&lt;/li&gt; &#xA; &lt;li&gt;Chat 2 Action. &#34;After the order is paid and completed, an email notification will be sent to the customer&#34;&lt;/li&gt; &#xA; &lt;li&gt;More actions...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;🗄️ Support for Multiple Databases (Coming Soon)&lt;/h4&gt; &#xA;&lt;p&gt;Choose the SQL database you like&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sqlite, PostgreSQL, MySQL, MariaDB, TiDB...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Structure&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitpod.io/#https://github.com/teableio/teable&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Open%20In-Gitpod.io-%231966D2?style=for-the-badge&amp;amp;logo=gitpod&#34; alt=&#34;Open in Gitpod&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;├── apps&#xA;│   ├── nextjs-app          (front-end, include a nextjs app)&#xA;│   └── nestjs-backend      (backend, include a nestjs app)&#xA;└── packages&#xA;    ├── common-i18n         (locales)&#xA;    ├── core                (share code and interface)&#xA;    ├── sdk                 (sdk for extensions)&#xA;    ├── db-main-prisma      (schema, migrations, prisma client)&#xA;    ├── eslint-config-bases (to shared eslint configs)&#xA;    └── ui-lib              (ui component)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Deploy&lt;/h2&gt; &#xA;&lt;h3&gt;Deploy With Docker&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd dockers/examples/standalone/&#xA;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for more details, see &lt;a href=&#34;https://raw.githubusercontent.com/teableio/teable/develop/dockers/examples&#34;&gt;dockers/examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;One Click Deployment&lt;/h3&gt; &#xA;&lt;p&gt;These platforms are easy to deploy with one click and come with free credits.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://railway.app/template/wada5e?referralCode=rE4BjB&#34;&gt;&lt;img src=&#34;https://railway.app/button.svg?sanitize=true&#34; alt=&#34;Deploy on Railway&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zeabur.com/templates/QF8695&#34;&gt;&lt;img src=&#34;https://zeabur.com/button.svg?sanitize=true&#34; alt=&#34;Deploy on Zeabur&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cloud.sealos.io/?openapp=system-template%3FtemplateName%3Dteable&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/labring-actions/templates/main/Deploy-on-Sealos.svg?sanitize=true&#34; alt=&#34;Deploy on Sealos&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;h4&gt;1. Initialize&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Enabling the Help Management Package Manager&#xA;corepack enable&#xA;&#xA;# Install project dependencies&#xA;pnpm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Select Database&lt;/h4&gt; &#xA;&lt;p&gt;we currently support &lt;code&gt;sqlite&lt;/code&gt; and &lt;code&gt;postgres&lt;/code&gt;, you can switch between them by running the following command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make switch-db-mode&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Custom Environment Variables（Optional）&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd apps/nextjs-app&#xA;copy .env.development .env.development.local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;4. Run Dev Server&lt;/h4&gt; &#xA;&lt;p&gt;you just need to start backend, it will start next server for frontend automatically, file change will be auto reload&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd apps/nestjs-backend&#xA;pnpm dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Why Teable?&lt;/h2&gt; &#xA;&lt;p&gt;No-code tools have significantly speed up how we get things done, allowing non-tech users to build amazing apps and changing the way many work and live. People like using spreadsheet-like UI to handle their data because it&#39;s easy, flexible, and great for team collaboration. They also prefer designing their app screens without being stuck with clunky templates.&lt;/p&gt; &#xA;&lt;p&gt;Giving non-techy people the ability to create their software sounds exciting. But that&#39;s just the start:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;As businesses expand, their data needs intensify. No one wishes to hear that once their orders reach 100k, they&#39;ll outgrow their current interface. Yet, many no-code platforms falter at such scales.&lt;/li&gt; &#xA; &lt;li&gt;Most no-code platforms are cloud-based. This means your important data sits with the provider, and switching to another platform can be a headache.&lt;/li&gt; &#xA; &lt;li&gt;Sometimes, no-code tools can&#39;t do what you want because of their limitations, leaving users stuck.&lt;/li&gt; &#xA; &lt;li&gt;If a tool becomes essential, you&#39;ll eventually need some tech expertise. But developers often find these platforms tricky.&lt;/li&gt; &#xA; &lt;li&gt;Maintaining systems with complex setups can be hard for developers, especially if these aren&#39;t built using common software standards.&lt;/li&gt; &#xA; &lt;li&gt;Systems that don&#39;t use these standards might need revamping or replacing, costing more in the long run. It might even mean ditching the no-code route and going back to traditional coding.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;What We Think the Future Of No-code Products Look Like&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An interface that anyone can use to build applications easily.&lt;/li&gt; &#xA; &lt;li&gt;Easy access to data, letting users grab, move, and reuse their information as they wish.&lt;/li&gt; &#xA; &lt;li&gt;Data privacy and choice, whether that&#39;s in the cloud, on-premise, or even just on your local.&lt;/li&gt; &#xA; &lt;li&gt;It needs to work for developers too, not just non-tech users.&lt;/li&gt; &#xA; &lt;li&gt;It should handle lots of data, so it can grow with your business.&lt;/li&gt; &#xA; &lt;li&gt;Flexibility to integrate with other software, combining strengths to get the job done.&lt;/li&gt; &#xA; &lt;li&gt;Last, native AI integration to takes usability to the next level.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In essence, Teable isn&#39;t just another no-code solution, it&#39;s a comprehensive answer to the evolving demands of modern software development, ensuring that everyone, regardless of their technical proficiency, has a platform tailored to their needs.&lt;/p&gt; &#xA;&lt;h2&gt;Sponsors &lt;span&gt;❤️&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;p&gt;If you are enjoying some this project in your company, I&#39;d really appreciate a &lt;a href=&#34;https://github.com/sponsors/teableio&#34;&gt;sponsorship&lt;/a&gt;, a &lt;a href=&#34;https://ko-fi.com/teable&#34;&gt;coffee&lt;/a&gt; or a dropped star. That gives me some more time to improve it to the next level.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;AGPL-3.0&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LLaVA-VL/LLaVA-NeXT</title>
    <updated>2024-08-09T01:27:41Z</updated>
    <id>tag:github.com,2024-08-09:/LLaVA-VL/LLaVA-NeXT</id>
    <link href="https://github.com/LLaVA-VL/LLaVA-NeXT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://i.postimg.cc/pL17YtG4/WX20240508-220230-2x.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;LLaVA-NeXT: Open Large Multimodal Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2408.03326&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_onevision-paper-green&#34; alt=&#34;Static Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://llava-vl.github.io/blog/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-blog-green&#34; alt=&#34;llava_next-blog&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://llava-onevision.lmms-lab.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_onevision-demo-red&#34; alt=&#34;llava_onevision-demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/lmms-lab/LLaVA-NeXT-Interleave-Demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-interleave_demo-red&#34; alt=&#34;llava_next-interleave_demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/WildVision/vision-arena&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-video_demo-red&#34; alt=&#34;llava_next-video_demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-onevision-66a259c3526e15166d6bba37&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_onevision-checkpoints-blue&#34; alt=&#34;llava_onevision-checkpoints&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-interleave-66763c55c411b340b35873d1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-interleave_checkpoints-blue&#34; alt=&#34;llava_next-interleave_checkpoints&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-video_checkpoints-blue&#34; alt=&#34;llava_next-video_checkpoints&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/lmms-lab&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-image_checkpoints-blue&#34; alt=&#34;llava_next-image_checkpoints&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Release Notes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/08/06] 🔥 &lt;strong&gt;LLaVA-OneVision&lt;/strong&gt; is &lt;a href=&#34;https://llava-vl.github.io/blog/2024-08-05-llava-onevision/&#34;&gt;released&lt;/a&gt;. The new 0.5/7/72B model achieves the state-of-the-art level and comparable to most powerful commercial models performance on several single-image, multi-image, and video benchmarks. We benchmarked on a total of 47 benchmarks to comprehensively reflect our model&#39;s true capabilities in diverse domains. We also release our training code, and single-image/multi-image data mixture in &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data&#34;&gt;LLaVA-OneVision Data&lt;/a&gt;! Our video part data will be released via next upgrade of video specific model, stay tuned! Our training code can be directly used to train on single-image, multi-image and video data.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Check our &lt;a href=&#34;https://arxiv.org/abs/2408.03326&#34;&gt;Paper&lt;/a&gt; for more details and to see our insights on training one model to rule them all.&lt;/li&gt; &#xA;   &lt;li&gt;Check our &lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-NeXT/raw/main/docs/LLaVA_OneVision.md&#34;&gt;LLaVA-OneVision Doc&lt;/a&gt; for inference and evaluation guidance.&lt;/li&gt; &#xA;   &lt;li&gt;Check our &lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-NeXT/raw/main/scripts/train&#34;&gt;Training Scripts&lt;/a&gt; to start training models on single-image/multi-image/video data.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/07/16] 🔥 &lt;strong&gt;LLaVA-NeXT-Video&lt;/strong&gt; has been upgraded. The new 32B model achieves the best open-source performance on several video benchmarks, including &lt;a href=&#34;https://video-mme.github.io/home_page.html#leaderboard&#34;&gt;Video-MME&lt;/a&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Video_0716.md&#34;&gt;this page&lt;/a&gt; for details, refer to &lt;a href=&#34;https://huggingface.co/spaces/WildVision/vision-arena&#34;&gt;llava_next-video_demo&lt;/a&gt; for demo.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/06/23] 🔥 &lt;strong&gt;LLaVA-NeXT-Interleave&lt;/strong&gt; is released. We utilize image-text interleaved format to unify multi-image, video, and 3D tasks in one LLM and achieve &lt;strong&gt;SoTA&lt;/strong&gt; performance on a wide range of benchmarks. Check out &lt;a href=&#34;https://arxiv.org/pdf/2407.07895&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/&#34;&gt;blog&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-interleave-66763c55c411b340b35873d1&#34;&gt;checkpoints&lt;/a&gt; to see new capabilities and improved performance! We have released 0.5b, 7b, and 7b-dpo models.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An all-round LLM for multi-image, video, and 3D with strong performance [&lt;a href=&#34;https://huggingface.co/spaces/lmms-lab/LLaVA-NeXT-Interleave-Demo&#34;&gt;demo&lt;/a&gt;]&lt;/li&gt; &#xA;   &lt;li&gt;Construct interleave training data &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/M4-Instruct-Data&#34;&gt;&lt;strong&gt;M4-Instruct&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Construct multi-image benchmark &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/LLaVA-NeXT-Interleave-Bench&#34;&gt;&lt;strong&gt;LLaVA-Interleave Bench&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/05/25] 🔥 Wondering &#34;&lt;a href=&#34;https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/&#34;&gt;What Else Influences Visual Instruction Tuning Beyond Data?&lt;/a&gt;&#34; Our new &lt;a href=&#34;https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/&#34;&gt;blog&lt;/a&gt; summarizes empirical explorations to ablate the various design choices in improving LMMs except instruct data itself. Meanwhile, open-source the recapioned high-quality data using LLaVA-NeXT-34B on &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-118K&#34;&gt;[COCO]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-558K&#34;&gt;[LCS]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-CC3M&#34;&gt;[CC3M]&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Architectures (LMM &amp;amp; Vision Encoder)&lt;/li&gt; &#xA;   &lt;li&gt;Visual Representations (Resolution &amp;amp; # Tokens)&lt;/li&gt; &#xA;   &lt;li&gt;Training Strategies (High-quality data &amp;amp; Trainable modules)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/05/10] 🔥 &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; (Stronger) models are released, with support of stronger LMM inlcuding LLama-3 (8B) and Qwen-1.5 (72B/110B) Check out [&lt;a href=&#34;https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/&#34;&gt;blog&lt;/a&gt;] and [&lt;a href=&#34;https://huggingface.co/lmms-lab&#34;&gt;checkpoints&lt;/a&gt;] to see improved performance!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/05/10] 🔥 &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; (Video) is released. The image-only-trained LLaVA-NeXT model is surprisingly strong on video tasks with zero-shot modality transfer. DPO training with AI feedback on videos can yield significant improvement. [&lt;a href=&#34;https://llava-vl.github.io/blog/2024-04-30-llava-next-video/&#34;&gt;Blog&lt;/a&gt;], [&lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944&#34;&gt;checkpoints&lt;/a&gt;] and [&lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;sglang&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/01/30] 🔥 &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; is out! With additional scaling to LLaVA-1.5, LLaVA-NeXT-34B outperforms Gemini Pro on some benchmarks. It can now process 4x more pixels and perform more tasks/applications than before. Check out the &lt;a href=&#34;https://llava-vl.github.io/blog/2024-01-30-llava-next/&#34;&gt;blog post&lt;/a&gt;, and explore the &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;! Models are available in &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;. Training/eval data and scripts coming soon.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;More&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024/03/10] 🔥 Releasing &lt;strong&gt;LMMs-Eval&lt;/strong&gt;, a highly efficient evaluation pipeline we used when developing LLaVA-NeXT. It supports the evaluation of LMMs on dozens of public datasets and allows new dataset onboarding, making the dev of new LMMs much faster. [&lt;a href=&#34;https://lmms-lab.github.io/lmms-eval-blog/lmms-eval-0.1/&#34;&gt;Blog&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EvolvingLMMs-Lab/lmms-eval&#34;&gt;Codebase&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/11/10] &lt;a href=&#34;https://llava-vl.github.io/llava-plus/&#34;&gt;LLaVA-Plus&lt;/a&gt; is released: Learning to Use Tools for Creating Multimodal Agents, with LLaVA-Plus (LLaVA that Plug and Learn to Use Skills). [&lt;a href=&#34;https://llava-vl.github.io/llava-plus/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://llavaplus.ngrok.io/&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-Plus-Codebase&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2311.05437&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/11/02] &lt;a href=&#34;https://llava-vl.github.io/llava-interactive/&#34;&gt;LLaVA-Interactive&lt;/a&gt; is released: Experience the future of human-AI multimodal interaction with an all-in-one demo for Image Chat, Segmentation, Generation and Editing. [&lt;a href=&#34;https://llava-vl.github.io/llava-interactive/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://llavainteractive.ngrok.io/&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-Interactive-Demo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2311.00571&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/26] 🔥 LLaVA-1.5 with LoRA achieves comparable performance as full-model finetuning, with a reduced GPU RAM requirement (&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md#llava-v15&#34;&gt;ckpts&lt;/a&gt;, &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#train&#34;&gt;script&lt;/a&gt;). We also provide a &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Finetune_Custom_Data.md&#34;&gt;doc&lt;/a&gt; on how to finetune LLaVA-1.5 on your own dataset with LoRA.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/12] Check out the Korean LLaVA (Ko-LLaVA), created by ETRI, who has generously supported our research! [&lt;a href=&#34;https://huggingface.co/spaces/etri-vilab/Ko-LLaVA&#34;&gt;🤗 Demo&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/05] 🔥 LLaVA-1.5 is out! Achieving SoTA on 11 benchmarks, with just simple modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods like Qwen-VL-Chat that use billion-scale data. Check out the &lt;a href=&#34;https://arxiv.org/abs/2310.03744&#34;&gt;technical report&lt;/a&gt;, and explore the &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;! Models are available in &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;. The training data and scripts of LLaVA-1.5 are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#train&#34;&gt;here&lt;/a&gt;, and evaluation scripts are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/09/26] LLaVA is improved with reinforcement learning from human feedback (RLHF) to improve fact grounding and reduce hallucination. Check out the new SFT and RLHF checkpoints at project &lt;a href=&#34;https://llava-rlhf.github.io/&#34;&gt;[LLavA-RLHF]&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/09/22] &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;LLaVA&lt;/a&gt; is accepted by NeurIPS 2023 as &lt;strong&gt;oral presentation&lt;/strong&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2306.00890&#34;&gt;LLaVA-Med&lt;/a&gt; is accepted by NeurIPS 2023 Datasets and Benchmarks Track as &lt;strong&gt;spotlight presentation&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/11/06] Support &lt;strong&gt;Intel&lt;/strong&gt; dGPU and CPU platforms. &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/tree/intel/docs/intel&#34;&gt;More details here.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/12] LLaVA is now supported in &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/pull/3436&#34;&gt;llama.cpp&lt;/a&gt; with 4-bit / 5-bit quantization support!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/11] The training data and scripts of LLaVA-1.5 are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#train&#34;&gt;here&lt;/a&gt;, and evaluation scripts are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/10] &lt;a href=&#34;https://blog.roboflow.com/first-impressions-with-llava-1-5/&#34;&gt;Roboflow Deep Dive&lt;/a&gt;: First Impressions with LLaVA-1.5.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/09/20] We summarize our empirical study of training 33B and 65B LLaVA models in a &lt;a href=&#34;https://arxiv.org/abs/2309.09958&#34;&gt;note&lt;/a&gt;. Further, if you are interested in the comprehensive review, evolution and trend of multimodal foundation models, please check out our recent survey paper &lt;a href=&#34;https://arxiv.org/abs/2309.10020&#34;&gt;``Multimodal Foundation Models: From Specialists to General-Purpose Assistants&#39;&#39;.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings/raw/main/images/mfm_evolution.jpeg?raw=true&#34; width=&#34;50%/&#34;&gt; &lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;[2023/07/19] 🔥 We release a major upgrade, including support for LLaMA-2, LoRA training, 4-/8-bit inference, higher resolution (336x336), and a lot more. We release &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_Bench.md&#34;&gt;LLaVA Bench&lt;/a&gt; for benchmarking open-ended visual chat with results from Bard and Bing-Chat. We also support and verify training with RTX 3090 and RTX A6000. Check out &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_from_LLaMA2.md&#34;&gt;LLaVA-from-LLaMA-2&lt;/a&gt;, and our &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;model zoo&lt;/a&gt;!&lt;/li&gt; &#xA;  &lt;li&gt;[2023/06/26] &lt;a href=&#34;https://vlp-tutorial.github.io/&#34;&gt;CVPR 2023 Tutorial&lt;/a&gt; on &lt;strong&gt;Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4&lt;/strong&gt;! Please check out [&lt;a href=&#34;https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf&#34;&gt;Slides&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2306.14895&#34;&gt;Notes&lt;/a&gt;] [&lt;a href=&#34;https://youtu.be/mkI7EPD1vp8&#34;&gt;YouTube&lt;/a&gt;] [&lt;a href=&#34;https://www.bilibili.com/video/BV1Ng4y1T7v3/&#34;&gt;Bilibli&lt;/a&gt;].&lt;/li&gt; &#xA;  &lt;li&gt;[2023/06/11] We released the preview for the most requested feature: DeepSpeed and LoRA support! Please see documentations &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LoRA.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/06/01] We released &lt;strong&gt;LLaVA-Med: Large Language and Vision Assistant for Biomedicine&lt;/strong&gt;, a step towards building biomedical domain large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2306.00890&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://github.com/microsoft/LLaVA-Med&#34;&gt;page&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/05/06] We are releasing &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview&#34;&gt;LLaVA-Lighting-MPT-7B-preview&lt;/a&gt;, based on MPT-7B-Chat! See &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/#LLaVA-MPT-7b&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/05/02] 🔥 We are releasing LLaVA-Lighting! Train a lite, multimodal GPT-4 with just $40 in 3 hours! See &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/#train-llava-lightning&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/04/27] Thanks to the community effort, LLaVA-13B with 4-bit quantization allows you to run on a GPU with as few as 12GB VRAM! Try it out &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/tree/main/extensions/llava&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/04/17] 🔥 We released &lt;strong&gt;LLaVA: Large Language and Vision Assistant&lt;/strong&gt;. We propose visual instruction tuning, towards building large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;!-- &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;&lt;img src=&#34;assets/demo.gif&#34; width=&#34;70%&#34;&gt;&lt;/a&gt; --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage and License Notices&lt;/strong&gt;: This project utilizes certain datasets and checkpoints that are subject to their respective original licenses. Users must comply with all terms and conditions of these original licenses, including but not limited to the &lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;OpenAI Terms of Use&lt;/a&gt; for the dataset and the specific licenses for base language models for checkpoints trained using the dataset (e.g. &lt;a href=&#34;https://ai.meta.com/llama/license/&#34;&gt;Llama-1/2 community license&lt;/a&gt; for LLaMA-2 and Vicuna-v1.5, &lt;a href=&#34;https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/blob/main/LICENSE&#34;&gt;Tongyi Qianwen RESEARCH LICENSE AGREEMENT&lt;/a&gt; and &lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;Llama-3 Research License&lt;/a&gt;). This project does not impose any additional constraints beyond those stipulated in the original licenses. Furthermore, users are reminded to ensure that their use of the dataset and checkpoints is in compliance with all applicable laws and regulations.&lt;/p&gt; &#xA;&lt;h2&gt;Models &amp;amp; Scripts&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;h4&gt;1. &lt;strong&gt;Clone this repository and navigate to the LLaVA folder:&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/LLaVA-VL/LLaVA-NeXT&#xA;cd LLaVA-NeXT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. &lt;strong&gt;Install the inference package:&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n llava python=3.10 -y&#xA;conda activate llava&#xA;pip install --upgrade pip  # Enable PEP 660 support.&#xA;pip install -e &#34;.[train]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Project Navigation&lt;/h3&gt; &#xA;&lt;p&gt;Please checkout the following page for more inference &amp;amp; evaluation details.&lt;/p&gt; &#xA;&lt;h4&gt;- &lt;strong&gt;LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT.md&#34;&gt;LLaVA-NeXT-Image&lt;/a&gt;: for image demo inference and evaluation of stronger LMMs using &lt;a href=&#34;https://github.com/EvolvingLMMs-Lab/lmms-eval&#34;&gt;lmms-eval&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;- LLaVA-NeXT: A Strong Zero-shot Video Understanding Model&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Video.md&#34;&gt;LLaVA-NeXT-Video&lt;/a&gt;: for video inference and evaluation scripts. We recommend to use &lt;a href=&#34;https://lmms-lab.github.io/posts/lmms-eval-0.2/&#34;&gt;LMMs-video&lt;/a&gt; for evaluation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;- LLaVA-NeXT: Tackling Multi-image, Video, and 3D in Large Multimodal Models&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Interleave.md&#34;&gt;LLaVA-NeXT-Interleave&lt;/a&gt;: for multi-image demo and evaluation scripts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;SGLang for SpeedUp Inference and Deployment&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;SGLang&lt;/a&gt; to speed up inference and deployment of LLaVA-NeXT. You could make LLaVA-NeXT as a backend API service with SGLang.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prepare Environment&lt;/strong&gt;: Following the instruction in the &lt;a href=&#34;https://github.com/sgl-project/sglang?tab=readme-ov-file#install&#34;&gt;sglang&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;LLaVA-NeXT (Image)&lt;/h3&gt; &#xA;&lt;p&gt;Checkout the HTTP Post/Get and SRT usage at &lt;a href=&#34;https://github.com/sgl-project/sglang/raw/main/examples/usage/llava&#34;&gt;sglang/examples/usage/llava&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;LLaVA-NeXT (Video)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Launch and Run on (K) Nodes&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go to sglang project &lt;pre&gt;&lt;code&gt;cd PATH_TO/sglang&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;First node: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash examples/usage/llava_video/srt_example_llava_v.sh K 0 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO&#xA;(e.g. bash examples/usage/llava_video/srt_example_llava_v.sh K 0 examples/usage/llava_video/videos/Q98Z4OTh8RwmDonc.mp4 lmms-lab/LLaVA-NeXT-Video-7B-DPO 16)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Second node: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash examples/usage/llava_video/srt_example_llava_v.sh K 1 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;The K node: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash examples/usage/llava_video/srt_example_llava_v.sh K K-1 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find it useful for your research and applications, please cite related papers/blogs using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{li2024llava,&#xA;  title={LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models},&#xA;  author={Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},&#xA;  journal={arXiv preprint arXiv:2407.07895},&#xA;  year={2024}&#xA;}&#xA;&#xA;@misc{li2024llavanext-ablations,&#xA;&#x9;title={LLaVA-NeXT: What Else Influences Visual Instruction Tuning Beyond Data?},&#xA;&#x9;url={https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/},&#xA;&#x9;author={Li, Bo and Zhang, Hao and Zhang, Kaichen and Guo, Dong and Zhang, Yuanhan and Zhang, Renrui and Li, Feng and Liu, Ziwei and Li, Chunyuan},&#xA;&#x9;month={May},&#xA;&#x9;year={2024}&#xA;}&#xA;&#xA;@misc{li2024llavanext-strong,&#xA;    title={LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild},&#xA;    url={https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/},&#xA;    author={Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan},&#xA;    month={May},&#xA;    year={2024}&#xA;}&#xA;&#xA;@misc{zhang2024llavanext-video,&#xA;  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},&#xA;  url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},&#xA;  author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},&#xA;  month={April},&#xA;  year={2024}&#xA;}&#xA;&#xA;@misc{liu2024llavanext,&#xA;    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},&#xA;    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},&#xA;    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},&#xA;    month={January},&#xA;    year={2024}&#xA;}&#xA;&#xA;@misc{liu2023improvedllava,&#xA;      title={Improved Baselines with Visual Instruction Tuning}, &#xA;      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},&#xA;      publisher={arXiv:2310.03744},&#xA;      year={2023},&#xA;}&#xA;&#xA;@misc{liu2023llava,&#xA;      title={Visual Instruction Tuning}, &#xA;      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},&#xA;      publisher={NeurIPS},&#xA;      year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;: the codebase we built upon, and our base model Vicuna-13B that has the amazing language capabilities!&lt;/li&gt; &#xA; &lt;li&gt;The LLaVA-NeXT project is currently maintained by the team along with our contributors (listed alphabetically by the first names): &lt;a href=&#34;https://brianboli.com/&#34;&gt;Bo Li&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/dongguoset/&#34;&gt;Dong Guo&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=ybRe9GcAAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate&#34;&gt;Feng Li&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=B8hPxMQAAAAJ&amp;amp;hl=en&#34;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/kaichen-zhang-014b17219/?originalSubdomain=sg&#34;&gt;Kaichen Zhang&lt;/a&gt;, &lt;a href=&#34;https://zrrskywalker.github.io/&#34;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&#34;https://zhangyuanhan-ai.github.io/&#34;&gt;Yuanhan Zhang&lt;/a&gt;, led by &lt;a href=&#34;https://chunyuan.li/&#34;&gt;Chunyuan Li&lt;/a&gt; and with the guidance and help from &lt;a href=&#34;https://hliu.cc/&#34;&gt;Haotian Liu&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;﻿lmms-eval&lt;/code&gt; framework and its core contributors, including Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, and Kairui Hu, for their support on the evaluation side.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;Instruction Tuning with GPT-4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/LLaVA-Med&#34;&gt;LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Luodian/Otter&#34;&gt;Otter: In-Context Multi-Modal Instruction Tuning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For future project ideas, please check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34;&gt;SEEM: Segment Everything Everywhere All at Once&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt; to detect, segment, and generate anything by marrying &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment-Anything&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>