<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-23T01:21:50Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>CorentinJ/Real-Time-Voice-Cloning</title>
    <updated>2023-09-23T01:21:50Z</updated>
    <id>tag:github.com,2023-09-23:/CorentinJ/Real-Time-Voice-Cloning</id>
    <link href="https://github.com/CorentinJ/Real-Time-Voice-Cloning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Real-Time Voice Cloning&lt;/h1&gt; &#xA;&lt;p&gt;This repository is an implementation of &lt;a href=&#34;https://arxiv.org/pdf/1806.04558.pdf&#34;&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. This was my &lt;a href=&#34;https://matheo.uliege.be/handle/2268.2/6801&#34;&gt;master&#39;s thesis&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-O_hYhToKoA&#34;&gt;&lt;img src=&#34;https://i.imgur.com/8lFUlgz.png&#34; alt=&#34;Toolbox demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Papers implemented&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;   &lt;th&gt;Designation&lt;/th&gt; &#xA;   &lt;th&gt;Title&lt;/th&gt; &#xA;   &lt;th&gt;Implementation source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1806.04558.pdf&#34;&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1802.08435.pdf&#34;&gt;1802.08435&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WaveRNN (vocoder)&lt;/td&gt; &#xA;   &lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fatchord/WaveRNN&#34;&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.10135.pdf&#34;&gt;1703.10135&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron (synthesizer)&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fatchord/WaveRNN&#34;&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.10467.pdf&#34;&gt;1710.10467&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GE2E (encoder)&lt;/td&gt; &#xA;   &lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Heads up&lt;/h2&gt; &#xA;&lt;p&gt;Like everything else in Deep Learning, this repo is quickly getting old. Many other open-source repositories or SaaS apps (often paying) will give you a better audio quality than this repository will. If you care about the fidelity of the voice you&#39;re cloning, and its expressivity, here are some personal recommendations of alternative voice cloning solutions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check out &lt;a href=&#34;https://github.com/coqui-ai/tts&#34;&gt;CoquiTTS&lt;/a&gt; for an open source repository that is more up-to-date, with a better voice cloning quality and more functionalities.&lt;/li&gt; &#xA; &lt;li&gt;Check out &lt;a href=&#34;https://paperswithcode.com/task/speech-synthesis/&#34;&gt;paperswithcode&lt;/a&gt; for other repositories and recent research in the field of speech synthesis.&lt;/li&gt; &#xA; &lt;li&gt;Check out &lt;a href=&#34;https://www.resemble.ai/&#34;&gt;Resemble.ai&lt;/a&gt; (disclaimer: I work there) for state of the art voice cloning with little hassle.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;1. Install Requirements&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Both Windows and Linux are supported. A GPU is recommended for training and for inference speed, but is not mandatory.&lt;/li&gt; &#xA; &lt;li&gt;Python 3.7 is recommended. Python 3.5 or greater should work, but you&#39;ll probably have to tweak the dependencies&#39; versions. I recommend setting up a virtual environment using &lt;code&gt;venv&lt;/code&gt;, but this is optional.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://ffmpeg.org/download.html#get-packages&#34;&gt;ffmpeg&lt;/a&gt;. This is necessary for reading audio files.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt;. Pick the latest stable version, your operating system, your package manager (pip by default) and finally pick any of the proposed CUDA versions if you have a GPU, otherwise pick CPU. Run the given command.&lt;/li&gt; &#xA; &lt;li&gt;Install the remaining requirements with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;2. (Optional) Download Pretrained Models&lt;/h3&gt; &#xA;&lt;p&gt;Pretrained models are now downloaded automatically. If this doesn&#39;t work for you, you can manually download them &lt;a href=&#34;https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;3. (Optional) Test Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Before you download any dataset, you can begin by testing your configuration with:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python demo_cli.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If all tests pass, you&#39;re good to go.&lt;/p&gt; &#xA;&lt;h3&gt;4. (Optional) Download Datasets&lt;/h3&gt; &#xA;&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href=&#34;https://www.openslr.org/resources/12/train-clean-100.tar.gz&#34;&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href=&#34;https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets&#34;&gt;here&lt;/a&gt;. You&#39;re free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt; &#xA;&lt;h3&gt;5. Launch the Toolbox&lt;/h3&gt; &#xA;&lt;p&gt;You can then try the toolbox:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;br&gt; or&lt;br&gt; &lt;code&gt;python demo_toolbox.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;depending on whether you downloaded any datasets. If you are running an X-server or if you have the error &lt;code&gt;Aborted (core dumped)&lt;/code&gt;, see &lt;a href=&#34;https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590&#34;&gt;this issue&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>coder2gwy/coder2gwy</title>
    <updated>2023-09-23T01:21:50Z</updated>
    <id>tag:github.com,2023-09-23:/coder2gwy/coder2gwy</id>
    <link href="https://github.com/coder2gwy/coder2gwy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;互联网首份程序员考公指南，由3位已经进入体制内的前大厂程序员联合献上。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;程序员考公指南&lt;/h1&gt; &#xA;&lt;p&gt;3个来自同一家大厂的程序员组团在职备考一年，上岸成功率100%。我们为想进体制内的程序员写了这份&lt;strong&gt;考公务员/事业编制/教师&lt;/strong&gt;的指南。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;后端阿强考上了一线城市公务员，前端阿珍进了离家车程10分钟的事业单位，我在回家省内高校当教师：我们都有光明的前途。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;本指南的内容概要和目标人群&lt;/h2&gt; &#xA;&lt;p&gt;该指南由本人主笔，我叫阿特，我与这两位同事17年进了某大厂同一个部门。走出校门的我们走进996的部门。996一年多以后，项目“顺利”被砍，通宵达旦写出的每一行代码都被下掉。&lt;/p&gt; &#xA;&lt;p&gt;18年虽然职场PUA这个词还没如同现在这般流行，但愈发激烈的竞争与倒排工期的项目也让我逐渐失去了对原本对编程的兴趣。我开始备考各种体制内的岗位，也鼓励这两位好友与我一起备考。&lt;/p&gt; &#xA;&lt;p&gt;我们在公司“遵守劳动法”的上班制度里面见缝插针地备考。经过一年多的在职备考，我们终于都上岸了，他们俩分别去了&lt;strong&gt;一线城市市直公务员岗、二线城市离家很近的事业编&lt;/strong&gt;。而我则放弃了广州的要熬夜的行政编岗位，选择了老家省会的高校教师。&lt;/p&gt; &#xA;&lt;p&gt;这一年多的备考里面，我们遇到了各种阻碍：996没时间、家人不支持，但都一一克服。在觥筹交错的散伙饭之后，我写下您眼前的这篇指南，希望它能帮助到相同处境的朋友。&lt;/p&gt; &#xA;&lt;p&gt;本指南的目标人群是想要&lt;strong&gt;想进体制内的程序员&lt;/strong&gt;，尤其适合那些经过深思熟虑之后，发现程序员这份职业不能满足自己需求的人。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;建议、吐槽、提问请点进&lt;a href=&#34;https://github.com/coder2gwy/coder2gwy/issues/1&#34;&gt;这个issue&lt;/a&gt;。写作不易，您的star✨是我继续完善指南的动力。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;PS.由于高校带编教师岗位少、门槛愈发高，我会花较多的笔墨和精力在考公务员/事业编制方面。&lt;/p&gt; &#xA;&lt;h2&gt;更新&amp;amp;交流&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;后续更新&lt;/strong&gt;：建议关注我的公众号「程序员考公」，往后所有文章会首发于公众号，坚持高质量原创。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;交流&lt;/strong&gt;：想进入程序员考公交流群，请在「程序员考公」的公众号点击「拉我进群」，添加客服微信后即被拉进交流群。（若进群邀请提示 有同事已经进群，可以私信客服进另外的群。）&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/coder2gwy/coder2gwy/main/img/%E4%BA%8C%E7%BB%B4%E7%A0%81.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;正文内容&lt;/h2&gt; &#xA;&lt;p&gt;请移步：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coder2gwy/coder2gwy/main/%E4%B8%8A%E5%B2%B8%E7%BB%8F%E5%8E%86/&#34;&gt;程序员成功上岸经历&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coder2gwy/coder2gwy/main/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/&#34;&gt;程序员备考的最佳实践&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coder2gwy/coder2gwy/main/%E9%81%87%E5%88%B0%E9%97%AE%E9%A2%98/&#34;&gt;程序员备考过程中会遇到哪些问题？&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coder2gwy/coder2gwy/main/%E5%9F%BA%E6%9C%AC%E8%AE%A4%E8%AF%86/&#34;&gt;公考公平吗，35岁再去考可以么？&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coder2gwy/coder2gwy/main/%E7%9B%B8%E5%85%B3/&#34;&gt;资料、工具推荐和扩展阅读&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>zhudotexe/kani</title>
    <updated>2023-09-23T01:21:50Z</updated>
    <id>tag:github.com,2023-09-23:/zhudotexe/kani</id>
    <link href="https://github.com/zhudotexe/kani" rel="alternate"></link>
    <summary type="html">&lt;p&gt;kani (カニ) is a highly hackable microframework for chat-based language models with tool usage/function calling.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;256&#34; height=&#34;256&#34; alt=&#34;kani&#34; src=&#34;https://raw.githubusercontent.com/zhudotexe/kani/main/docs/_static/kani-logo@256.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/zhudotexe/kani/actions/workflows/pytest.yml&#34;&gt; &lt;img alt=&#34;Test Package&#34; src=&#34;https://github.com/zhudotexe/kani/actions/workflows/pytest.yml/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://kani.readthedocs.io/en/latest/?badge=latest&#34;&gt; &lt;img alt=&#34;Documentation Status&#34; src=&#34;https://readthedocs.org/projects/kani/badge/?version=latest&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/kani/&#34;&gt; &lt;img alt=&#34;PyPI&#34; src=&#34;https://img.shields.io/pypi/v/kani&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/zhudotexe/kani/blob/main/examples/colab_examples.ipynb&#34;&gt; &lt;img alt=&#34;Quickstart in Colab&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/eTepTNDxYT&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/1150902904773935214?color=5865F2&amp;amp;label=discord&amp;amp;logo=discord&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;kani (カニ)&lt;/h1&gt; &#xA;&lt;p&gt;kani (カニ) is a lightweight and highly hackable framework for chat-based language models with tool usage/function calling.&lt;/p&gt; &#xA;&lt;p&gt;Compared to other LM frameworks, kani is less opinionated and offers more fine-grained customizability over the parts of the control flow that matter, making it the perfect choice for NLP researchers, hobbyists, and developers alike.&lt;/p&gt; &#xA;&lt;p&gt;kani comes with support for OpenAI models and LLaMA v2 out of the box, with a model-agnostic framework to add support for many more.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://kani.readthedocs.io/&#34;&gt;Read the docs on ReadTheDocs!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.05542&#34;&gt;Read our preprint on arXiv!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lightweight and high-level&lt;/strong&gt; - kani implements common boilerplate to interface with language models without forcing you to use opinionated prompt frameworks or complex library-specific tooling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model agnostic&lt;/strong&gt; - kani provides a simple interface to implement: token counting and completion generation. Implement these two, and kani can run with any language model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic chat memory management&lt;/strong&gt; - Allow chat sessions to flow without worrying about managing the number of tokens in the history - kani takes care of it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Function calling with model feedback and retry&lt;/strong&gt; - Give models access to functions in just one line of code. kani elegantly provides feedback about hallucinated parameters and errors and allows the model to retry calls.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;You control the prompts&lt;/strong&gt; - There are no hidden prompt hacks. We will never decide for you how to format your own data, unlike other popular language model libraries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast to iterate and intuitive to learn&lt;/strong&gt; - With kani, you only write Python - we handle the rest.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Asynchronous design from the start&lt;/strong&gt; - kani can scale to run multiple chat sessions in parallel easily, without having to manage multiple processes or programs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;a href=&#34;https://colab.research.google.com/github/zhudotexe/kani/blob/main/examples/colab_examples.ipynb&#34;&gt; &lt;img alt=&#34;Quickstart in Colab&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;kani requires Python 3.10 or above.&lt;/p&gt; &#xA;&lt;p&gt;First, install the library. In this quickstart, we&#39;ll use the OpenAI engine, though kani is &lt;a href=&#34;https://kani.readthedocs.io/en/latest/engines.html&#34;&gt;model-agnostic&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ pip install &#34;kani[openai]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, let&#39;s use kani to create a simple chatbot using ChatGPT as a backend.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# import the library&#xA;from kani import Kani, chat_in_terminal&#xA;from kani.engines.openai import OpenAIEngine&#xA;&#xA;# Replace this with your OpenAI API key: https://platform.openai.com/account/api-keys&#xA;api_key = &#34;sk-...&#34;&#xA;&#xA;# kani uses an Engine to interact with the language model. You can specify other model &#xA;# parameters here, like temperature=0.7.&#xA;engine = OpenAIEngine(api_key, model=&#34;gpt-3.5-turbo&#34;)&#xA;&#xA;# The kani manages the chat state, prompting, and function calling. Here, we only give &#xA;# it the engine to call ChatGPT, but you can specify other parameters like &#xA;# system_prompt=&#34;You are...&#34; here.&#xA;ai = Kani(engine)&#xA;&#xA;# kani comes with a utility to interact with a kani through your terminal! Check out &#xA;# the docs for how to use kani programmatically.&#xA;chat_in_terminal(ai)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;kani makes the time to set up a working chat model short, while offering the programmer deep customizability over every prompt, function call, and even the underlying language model.&lt;/p&gt; &#xA;&lt;h2&gt;Function Calling&lt;/h2&gt; &#xA;&lt;p&gt;Function calling gives language models the ability to choose when to call a function you provide based off its documentation.&lt;/p&gt; &#xA;&lt;p&gt;With kani, you can write functions in Python and expose them to the model with just one line of code: the &lt;code&gt;@ai_function&lt;/code&gt; decorator.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# import the library&#xA;from typing import Annotated&#xA;from kani import AIParam, Kani, ai_function, chat_in_terminal&#xA;from kani.engines.openai import OpenAIEngine&#xA;&#xA;# set up the engine as above&#xA;api_key = &#34;sk-...&#34;&#xA;engine = OpenAIEngine(api_key, model=&#34;gpt-3.5-turbo&#34;)&#xA;&#xA;&#xA;# subclass Kani to add AI functions&#xA;class MyKani(Kani):&#xA;    # Adding the annotation to a method exposes it to the AI&#xA;    @ai_function()&#xA;    def get_weather(&#xA;        self,&#xA;        # and you can provide extra documentation about specific parameters&#xA;        location: Annotated[str, AIParam(desc=&#34;The city and state, e.g. San Francisco, CA&#34;)],&#xA;    ):&#xA;        &#34;&#34;&#34;Get the current weather in a given location.&#34;&#34;&#34;&#xA;        # In this example, we mock the return, but you could call a real weather API&#xA;        return f&#34;Weather in {location}: Sunny, 72 degrees fahrenheit.&#34;&#xA;&#xA;&#xA;ai = MyKani(engine)&#xA;chat_in_terminal(ai)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;kani guarantees that function calls are valid by the time they reach your methods while allowing you to focus on writing code. For more information, check out &lt;a href=&#34;https://kani.readthedocs.io/en/latest/function_calling.html&#34;&gt;the function calling docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Why kani?&lt;/h2&gt; &#xA;&lt;p&gt;Existing frameworks for language models like langchain and simpleaichat are opinionated and/or heavyweight - they edit developers&#39; prompts under the hood, are challenging to learn, and are difficult to customize without adding a lot of high-maintenance bloat to your codebase.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img style=&#34;max-width: 800px;&#34; alt=&#34;kani&#34; src=&#34;https://raw.githubusercontent.com/zhudotexe/kani/main/docs/_static/lib-comparison_white.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We built kani to be more flexible, simple, and robust. kani is appropriate for everyone from academic researchers to industry professionals to hobbyists to use without worrying about under-the-hood hacks.&lt;/p&gt; &#xA;&lt;h2&gt;Docs&lt;/h2&gt; &#xA;&lt;p&gt;To learn more about how to &lt;a href=&#34;https://kani.readthedocs.io/en/latest/customization.html&#34;&gt;customize kani with your own prompt wrappers&lt;/a&gt;, &lt;a href=&#34;https://kani.readthedocs.io/en/latest/function_calling.html&#34;&gt;function calling&lt;/a&gt;, and more, &lt;a href=&#34;http://kani.readthedocs.io/&#34;&gt;read the docs!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or take a look at the hands-on examples &lt;a href=&#34;https://github.com/zhudotexe/kani/tree/main/examples&#34;&gt;in this repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Want to see kani in action? Using 4-bit quantization to shrink the model, we run LLaMA v2 as part of our test suite right on GitHub Actions:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhudotexe/kani/actions/workflows/pytest.yml?query=branch%3Amain+is%3Asuccess&#34;&gt;https://github.com/zhudotexe/kani/actions/workflows/pytest.yml?query=branch%3Amain+is%3Asuccess&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Simply click on the latest build to see LLaMA&#39;s output!&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use Kani, please cite us as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{zhu2023kani,&#xA;      title={Kani: A Lightweight and Highly Hackable Framework for Building Language Model Applications}, &#xA;      author={Andrew Zhu and Liam Dugan and Alyssa Hwang and Chris Callison-Burch},&#xA;      year={2023},&#xA;      eprint={2309.05542},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.SE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!--&#xA;For developers:&#xA;&#xA;## Build and Publish&#xA;&#xA;`fastlmi` uses Hatchling to build.&#xA;&#xA;Make sure to bump the version in pyproject.toml before publishing.&#xA;&#xA;```shell&#xA;rm -r dist/&#xA;python -m build&#xA;python -m twine upload dist/*&#xA;```&#xA;--&gt;</summary>
  </entry>
</feed>