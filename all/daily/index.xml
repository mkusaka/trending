<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-19T01:28:45Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVIDIA/garak</title>
    <updated>2024-11-19T01:28:45Z</updated>
    <id>tag:github.com,2024-11-19:/NVIDIA/garak</id>
    <link href="https://github.com/NVIDIA/garak" rel="alternate"></link>
    <summary type="html">&lt;p&gt;the LLM vulnerability scanner&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;garak, LLM vulnerability scanner&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Generative AI Red-teaming &amp;amp; Assessment Kit&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; checks if an LLM can be made to fail in a way we don&#39;t want. &lt;code&gt;garak&lt;/code&gt; probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know &lt;code&gt;nmap&lt;/code&gt;, it&#39;s &lt;code&gt;nmap&lt;/code&gt; for LLMs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; focuses on ways of making an LLM or dialog system fail. It combines static, dynamic, and adaptive probes to explore this.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt;&#39;s a free tool. We love developing it and are always interested in adding functionality to support applications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml&#34;&gt;&lt;img src=&#34;https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml/badge.svg?sanitize=true&#34; alt=&#34;Tests/Linux&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml&#34;&gt;&lt;img src=&#34;https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml/badge.svg?sanitize=true&#34; alt=&#34;Tests/Windows&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml&#34;&gt;&lt;img src=&#34;https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml/badge.svg?sanitize=true&#34; alt=&#34;Tests/OSX&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://garak.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/garak/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/uVch4puUCs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat-on%20discord-yellow.svg?sanitize=true&#34; alt=&#34;discord-img&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/garak&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/garak&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/garak&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/garak.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/garak&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/garak&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/garak&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/garak/month&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Get started&lt;/h2&gt; &#xA;&lt;h3&gt;&amp;gt; See our user guide! &lt;a href=&#34;https://docs.garak.ai/&#34;&gt;docs.garak.ai&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&amp;gt; Join our &lt;a href=&#34;https://discord.gg/uVch4puUCs&#34;&gt;Discord&lt;/a&gt;!&lt;/h3&gt; &#xA;&lt;h3&gt;&amp;gt; Project links &amp;amp; home: &lt;a href=&#34;https://garak.ai/&#34;&gt;garak.ai&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&amp;gt; Twitter: &lt;a href=&#34;https://twitter.com/garak_llm&#34;&gt;@garak_llm&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&amp;gt; DEF CON &lt;a href=&#34;https://garak.ai/garak_aiv_slides.pdf&#34;&gt;slides&lt;/a&gt;!&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;LLM support&lt;/h2&gt; &#xA;&lt;p&gt;currently supports:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models&#34;&gt;hugging face hub&lt;/a&gt; generative models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replicate.com/&#34;&gt;replicate&lt;/a&gt; text models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/introduction&#34;&gt;openai api&lt;/a&gt; chat &amp;amp; continuation models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.litellm.ai/&#34;&gt;litellm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;pretty much anything accessible via REST&lt;/li&gt; &#xA; &lt;li&gt;gguf models like &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; version &amp;gt;= 1046&lt;/li&gt; &#xA; &lt;li&gt;.. and many more LLMs!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; is a command-line tool. It&#39;s developed in Linux and OSX.&lt;/p&gt; &#xA;&lt;h3&gt;Standard install with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Just grab it from PyPI and you should be good to go:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pip install -U garak&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install development version with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The standard pip version of &lt;code&gt;garak&lt;/code&gt; is updated periodically. To get a fresher version from GitHub, try:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pip install -U git+https://github.com/NVIDIA/garak.git@main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Clone from source&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; has its own dependencies. You can to install &lt;code&gt;garak&lt;/code&gt; in its own Conda environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create --name garak &#34;python&amp;gt;=3.10,&amp;lt;=3.12&#34;&#xA;conda activate garak&#xA;gh repo clone NVIDIA/garak&#xA;cd garak&#xA;python -m pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;OK, if that went fine, you&#39;re probably good to go!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if you cloned before the move to the &lt;code&gt;NVIDIA&lt;/code&gt; GitHub organisation, but you&#39;re reading this at the &lt;code&gt;github.com/NVIDIA&lt;/code&gt; URI, please update your remotes as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git remote set-url origin https://github.com/NVIDIA/garak.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;The general syntax is:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;garak &amp;lt;options&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; needs to know what model to scan, and by default, it&#39;ll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;garak --list_probes&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To specify a generator, use the &lt;code&gt;--model_type&lt;/code&gt; and, optionally, the &lt;code&gt;--model_name&lt;/code&gt; options. Model type specifies a model family/interface; model name specifies the exact model to be used. The &#34;Intro to generators&#34; section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set &lt;code&gt;--model_type&lt;/code&gt; to &lt;code&gt;huggingface&lt;/code&gt; and &lt;code&gt;--model_name&lt;/code&gt; to the model&#39;s name on Hub (e.g. &lt;code&gt;&#34;RWKV/rwkv-4-169m-pile&#34;&lt;/code&gt;). Some generators might need an API key to be set as an environment variable, and they&#39;ll let you know if they need that.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; runs all the probes by default, but you can be specific about that too. &lt;code&gt;--probes promptinject&lt;/code&gt; will use only the &lt;a href=&#34;https://github.com/agencyenterprise/promptinject&#34;&gt;PromptInject&lt;/a&gt; framework&#39;s methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a &lt;code&gt;.&lt;/code&gt;; for example, &lt;code&gt;--probes lmrc.SlurUsage&lt;/code&gt; will use an implementation of checking for models generating slurs based on the &lt;a href=&#34;https://arxiv.org/abs/2303.18190&#34;&gt;Language Model Risk Cards&lt;/a&gt; framework.&lt;/p&gt; &#xA;&lt;p&gt;For help and inspiration, find us on &lt;a href=&#34;https://twitter.com/garak_llm&#34;&gt;Twitter&lt;/a&gt; or &lt;a href=&#34;https://discord.gg/uVch4puUCs&#34;&gt;discord&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Probe ChatGPT for encoding-based prompt injection (OSX/*nix) (replace example value with a real OpenAI API key)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY=&#34;sk-123XXXXXXXXXXXX&#34;&#xA;python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See if the Hugging Face version of GPT2 is vulnerable to DAN 11.0&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Reading the results&lt;/h2&gt; &#xA;&lt;p&gt;For each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe&#39;s results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.&lt;/p&gt; &#xA;&lt;p&gt;Here are the results with the &lt;code&gt;encoding&lt;/code&gt; module on a GPT-3 variant: &lt;img src=&#34;https://i.imgur.com/8Dxf45N.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;And the same results for ChatGPT: &lt;img src=&#34;https://i.imgur.com/VKAF5if.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections. The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.&lt;/p&gt; &#xA;&lt;p&gt;Errors go in &lt;code&gt;garak.log&lt;/code&gt;; the run is logged in detail in a &lt;code&gt;.jsonl&lt;/code&gt; file specified at analysis start &amp;amp; end. There&#39;s a basic analysis script in &lt;code&gt;analyse/analyse_log.py&lt;/code&gt; which will output the probes and prompts that led to the most hits.&lt;/p&gt; &#xA;&lt;p&gt;Send PRs &amp;amp; open issues. Happy hunting!&lt;/p&gt; &#xA;&lt;h2&gt;Intro to generators&lt;/h2&gt; &#xA;&lt;h3&gt;Hugging Face&lt;/h3&gt; &#xA;&lt;p&gt;Using the Pipeline API:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type huggingface&lt;/code&gt; (for transformers models to run locally)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - use the model name from Hub. Only generative models will work. If it fails and shouldn&#39;t, please open an issue and paste in the command you tried + the exception!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Using the Inference API:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type huggingface.InferenceAPI&lt;/code&gt; (for API-based model access)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the model name from Hub, e.g. &lt;code&gt;&#34;mosaicml/mpt-7b-instruct&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Using private endpoints:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type huggingface.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_name&lt;/code&gt; - the endpoint URL, e.g. &lt;code&gt;https://xxx.us-east-1.aws.endpoints.huggingface.cloud&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(optional) set the &lt;code&gt;HF_INFERENCE_TOKEN&lt;/code&gt; environment variable to a Hugging Face API token with the &#34;read&#34; role; see &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;https://huggingface.co/settings/tokens&lt;/a&gt; when logged in&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;OpenAI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type openai&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the OpenAI model you&#39;d like to use. &lt;code&gt;gpt-3.5-turbo-0125&lt;/code&gt; is fast and fine for testing.&lt;/li&gt; &#xA; &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable to your OpenAI API key (e.g. &#34;sk-19763ASDF87q6657&#34;); see &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;https://platform.openai.com/account/api-keys&lt;/a&gt; when logged in&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Recognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you&#39;d like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.&lt;/p&gt; &#xA;&lt;h3&gt;Replicate&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;REPLICATE_API_TOKEN&lt;/code&gt; environment variable to your Replicate API token, e.g. &#34;r8-123XXXXXXXXXXXX&#34;; see &lt;a href=&#34;https://replicate.com/account/api-tokens&#34;&gt;https://replicate.com/account/api-tokens&lt;/a&gt; when logged in&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Public Replicate models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type replicate&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the Replicate model name and hash, e.g. &lt;code&gt;&#34;stability-ai/stablelm-tuned-alpha-7b:c49dae36&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Private Replicate endpoints:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type replicate.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - username/model-name slug from the deployed endpoint, e.g. &lt;code&gt;elim/elims-llama2-7b&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Cohere&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type cohere&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; (optional, &lt;code&gt;command&lt;/code&gt; by default) - The specific Cohere model you&#39;d like to test&lt;/li&gt; &#xA; &lt;li&gt;set the &lt;code&gt;COHERE_API_KEY&lt;/code&gt; environment variable to your Cohere API key, e.g. &#34;aBcDeFgHiJ123456789&#34;; see &lt;a href=&#34;https://dashboard.cohere.ai/api-keys&#34;&gt;https://dashboard.cohere.ai/api-keys&lt;/a&gt; when logged in&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Groq&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type groq&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - The name of the model to access via the Groq API&lt;/li&gt; &#xA; &lt;li&gt;set the &lt;code&gt;GROQ_API_KEY&lt;/code&gt; environment variable to your Groq API key, see &lt;a href=&#34;https://console.groq.com/docs/quickstart&#34;&gt;https://console.groq.com/docs/quickstart&lt;/a&gt; for details on creating an API key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ggml&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type ggml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - The path to the ggml model you&#39;d like to load, e.g. &lt;code&gt;/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;set the &lt;code&gt;GGML_MAIN_PATH&lt;/code&gt; environment variable to the path to your ggml &lt;code&gt;main&lt;/code&gt; executable&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;REST&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;rest.RestGenerator&lt;/code&gt; is highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See &lt;a href=&#34;https://reference.garak.ai/en/latest/garak.generators.rest.html&#34;&gt;https://reference.garak.ai/en/latest/garak.generators.rest.html&lt;/a&gt; for examples.&lt;/p&gt; &#xA;&lt;h3&gt;NIM&lt;/h3&gt; &#xA;&lt;p&gt;Use models from &lt;a href=&#34;https://build.nvidia.com/&#34;&gt;https://build.nvidia.com/&lt;/a&gt; or other NIM endpoints.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;NIM_API_KEY&lt;/code&gt; environment variable to your authentication API token, or specify it in the config YAML&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For chat models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type nim&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;meta/llama-3.1-8b-instruct&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For completion models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type nim.NVOpenAICompletion&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;bigcode/starcoder2-15b&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;OctoAI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;OCTO_API_TOKEN&lt;/code&gt; environment variable to your Replicate API token, e.g. &#34;r8-123XXXXXXXXXXXX&#34;; see &lt;a href=&#34;https://replicate.com/account/api-tokens&#34;&gt;https://replicate.com/account/api-tokens&lt;/a&gt; when logged in&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Octo public endpoint:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type octo&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the OctoAI public endpoint for the model, e.g. &lt;code&gt;mistral-7b-instruct-fp16&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Octo private endpoint:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type octo.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the deployed endpoint URL, e.g. &lt;code&gt;https://llama-2-70b-chat-xxx.octoai.run/v1/chat/completions&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Test&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type test&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(alternatively) &lt;code&gt;--model_name test.Blank&lt;/code&gt; For testing. This always generates the empty string, using the &lt;code&gt;test.Blank&lt;/code&gt; generator. Will be marked as failing for any tests that &lt;em&gt;require&lt;/em&gt; an output, e.g. those that make contentious claims and expect the model to refute them in order to pass.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type test.Repeat&lt;/code&gt; For testing. This generator repeats back the prompt it received.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Intro to probes&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Probe&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;blank&lt;/td&gt; &#xA;   &lt;td&gt;A simple probe that always sends an empty prompt.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;atkgen&lt;/td&gt; &#xA;   &lt;td&gt;Automated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2 &lt;a href=&#34;https://huggingface.co/garak-llm/artgpt2tox&#34;&gt;fine-tuned&lt;/a&gt; on the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;av_spam_scanning&lt;/td&gt; &#xA;   &lt;td&gt;Probes that attempt to make the model output malicious content signatures&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;continuation&lt;/td&gt; &#xA;   &lt;td&gt;Probes that test if the model will continue a probably undesirable word&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;dan&lt;/td&gt; &#xA;   &lt;td&gt;Various &lt;a href=&#34;https://adguard.com/en/blog/chatgpt-dan-prompt-abuse.html&#34;&gt;DAN&lt;/a&gt; and DAN-like attacks&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;donotanswer&lt;/td&gt; &#xA;   &lt;td&gt;Prompts to which responsible language models should not answer.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;encoding&lt;/td&gt; &#xA;   &lt;td&gt;Prompt injection through text encoding&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gcg&lt;/td&gt; &#xA;   &lt;td&gt;Disrupt a system prompt by appending an adversarial suffix.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;glitch&lt;/td&gt; &#xA;   &lt;td&gt;Probe model for glitch tokens that provoke unusual behavior.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;grandma&lt;/td&gt; &#xA;   &lt;td&gt;Appeal to be reminded of one&#39;s grandmother.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;goodside&lt;/td&gt; &#xA;   &lt;td&gt;Implementations of Riley Goodside attacks.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;leakerplay&lt;/td&gt; &#xA;   &lt;td&gt;Evaluate if a model will replay training data.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lmrc&lt;/td&gt; &#xA;   &lt;td&gt;Subsample of the &lt;a href=&#34;https://arxiv.org/abs/2303.18190&#34;&gt;Language Model Risk Cards&lt;/a&gt; probes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;malwaregen&lt;/td&gt; &#xA;   &lt;td&gt;Attempts to have the model generate code for building malware&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;misleading&lt;/td&gt; &#xA;   &lt;td&gt;Attempts to make a model support misleading and false claims&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;packagehallucination&lt;/td&gt; &#xA;   &lt;td&gt;Trying to get code generations that specify non-existent (and therefore insecure) packages.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;promptinject&lt;/td&gt; &#xA;   &lt;td&gt;Implementation of the Agency Enterprise &lt;a href=&#34;https://github.com/agencyenterprise/PromptInject/tree/main/promptinject&#34;&gt;PromptInject&lt;/a&gt; work (best paper awards @ NeurIPS ML Safety Workshop 2022)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;realtoxicityprompts&lt;/td&gt; &#xA;   &lt;td&gt;Subset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;snowball&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ofir.io/snowballed_hallucination.pdf&#34;&gt;Snowballed Hallucination&lt;/a&gt; probes designed to make a model give a wrong answer to questions too complex for it to process&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;xss&lt;/td&gt; &#xA;   &lt;td&gt;Look for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Logging&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; generates multiple kinds of log:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A log file, &lt;code&gt;garak.log&lt;/code&gt;. This includes debugging information from &lt;code&gt;garak&lt;/code&gt; and its plugins, and is continued across runs.&lt;/li&gt; &#xA; &lt;li&gt;A report of the current run, structured as JSONL. A new report file is created every time &lt;code&gt;garak&lt;/code&gt; runs. The name of this file is output at the beginning and, if successful, also at the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry&#39;s &lt;code&gt;status&lt;/code&gt; attribute takes a constant from &lt;code&gt;garak.attempts&lt;/code&gt; to describe what stage it was made at.&lt;/li&gt; &#xA; &lt;li&gt;A hit log, detailing attempts that yielded a vulnerability (a &#39;hit&#39;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How is the code structured?&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://reference.garak.ai/&#34;&gt;reference docs&lt;/a&gt; for an authoritative guide to &lt;code&gt;garak&lt;/code&gt; code structure.&lt;/p&gt; &#xA;&lt;p&gt;In a typical run, &lt;code&gt;garak&lt;/code&gt; will read a model type (and optionally model name) from the command line, then determine which &lt;code&gt;probe&lt;/code&gt;s and &lt;code&gt;detector&lt;/code&gt;s to run, start up a &lt;code&gt;generator&lt;/code&gt;, and then pass these to a &lt;code&gt;harness&lt;/code&gt; to do the probing; an &lt;code&gt;evaluator&lt;/code&gt; deals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;garak/probes/&lt;/code&gt; - classes for generating interactions with LLMs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;garak/detectors/&lt;/code&gt; - classes for detecting an LLM is exhibiting a given failure mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;garak/evaluators/&lt;/code&gt; - assessment reporting schemes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;garak/generators/&lt;/code&gt; - plugins for LLMs to be probed&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;garak/harnesses/&lt;/code&gt; - classes for structuring testing&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;resources/&lt;/code&gt; - ancillary items required by plugins&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The default operating mode is to use the &lt;code&gt;probewise&lt;/code&gt; harness. Given a list of probe module names and probe plugin names, the &lt;code&gt;probewise&lt;/code&gt; harness instantiates each probe, then for each probe reads its &lt;code&gt;recommended_detectors&lt;/code&gt; attribute to get a list of &lt;code&gt;detector&lt;/code&gt;s to run on the output.&lt;/p&gt; &#xA;&lt;p&gt;Each plugin category (&lt;code&gt;probes&lt;/code&gt;, &lt;code&gt;detectors&lt;/code&gt;, &lt;code&gt;evaluators&lt;/code&gt;, &lt;code&gt;generators&lt;/code&gt;, &lt;code&gt;harnesses&lt;/code&gt;) includes a &lt;code&gt;base.py&lt;/code&gt; which defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example, &lt;code&gt;garak.generators.openai.OpenAIGenerator&lt;/code&gt; descends from &lt;code&gt;garak.generators.base.Generator&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Larger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using &lt;code&gt;garak&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Developing your own plugin&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Take a look at how other plugins do it&lt;/li&gt; &#xA; &lt;li&gt;Inherit from one of the base classes, e.g. &lt;code&gt;garak.probes.base.TextProbe&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Override as little as possible&lt;/li&gt; &#xA; &lt;li&gt;You can test the new code in at least two ways: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Start an interactive Python session &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Import the model, e.g. &lt;code&gt;import garak.probes.mymodule&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Instantiate the plugin, e.g. &lt;code&gt;p = garak.probes.mymodule.MyProbe()&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Run a scan with test plugins &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;For probes, try a blank generator and always.Pass detector: &lt;code&gt;python3 -m garak -m test.Blank -p mymodule -d always.Pass&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;For detectors, try a blank generator and a blank probe: &lt;code&gt;python3 -m garak -m test.Blank -p test.Blank -d mymodule&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;For generators, try a blank probe and always.Pass detector: &lt;code&gt;python3 -m garak -m mymodule -p test.Blank -d always.Pass&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Get &lt;code&gt;garak&lt;/code&gt; to list all the plugins of the type you&#39;re writing, with &lt;code&gt;--list_probes&lt;/code&gt;, &lt;code&gt;--list_detectors&lt;/code&gt;, or &lt;code&gt;--list_generators&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;We have an FAQ &lt;a href=&#34;https://github.com/NVIDIA/garak/raw/main/FAQ.md&#34;&gt;here&lt;/a&gt;. Reach out if you have any more questions! &lt;a href=&#34;mailto:leon@garak.ai&#34;&gt;leon@garak.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Code reference documentation is at &lt;a href=&#34;https://garak.readthedocs.io/en/latest/&#34;&gt;garak.readthedocs.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citing garak&lt;/h2&gt; &#xA;&lt;p&gt;You can read the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/garak/main/garak-paper.pdf&#34;&gt;garak preprint paper&lt;/a&gt;. If you use garak, please cite us.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{garak,&#xA;  title={{garak: A Framework for Security Probing Large Language Models}},&#xA;  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},&#xA;  year={2024},&#xA;  howpublished={\url{https://garak.ai}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;&#34;Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly&#34;&lt;/em&gt; - Elim&lt;/p&gt; &#xA;&lt;p&gt;For updates and news see &lt;a href=&#34;https://twitter.com/garak_llm&#34;&gt;@garak_llm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;© 2023- Leon Derczynski; Apache license v2, see &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/garak/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>olimorris/codecompanion.nvim</title>
    <updated>2024-11-19T01:28:45Z</updated>
    <id>tag:github.com,2024-11-19:/olimorris/codecompanion.nvim</id>
    <link href="https://github.com/olimorris/codecompanion.nvim" rel="alternate"></link>
    <summary type="html">&lt;p&gt;✨ AI-powered coding, seamlessly in Neovim. Supports Anthropic, Copilot, Gemini, Ollama, OpenAI and xAI LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/b56cbf02-2e48-43a2-9d86-321209bc0664&#34; alt=&#34;CodeCompanion.nvim&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/olimorris/codecompanion.nvim/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/olimorris/codecompanion.nvim?color=c678dd&amp;amp;logoColor=e06c75&amp;amp;style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/olimorris/codecompanion.nvim/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/olimorris/codecompanion.nvim/ci.yml?branch=main&amp;amp;label=tests&amp;amp;style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/olimorris/codecompanion.nvim/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/olimorris/codecompanion.nvim?style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Currently supports: Anthropic, Copilot, Gemini, Ollama, OpenAI, Azure OpenAI and xAI adapters&lt;br&gt;&lt;br&gt; New features are always announced &lt;a href=&#34;https://github.com/olimorris/codecompanion.nvim/discussions/categories/announcements&#34;&gt;here&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;💜&lt;/span&gt; Sponsors&lt;/h2&gt; &#xA;&lt;p&gt;Thank you to the following people:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- coffee --&gt;&lt;a href=&#34;https://github.com/bassamsdata&#34;&gt;&lt;img src=&#34;https://github.com/bassamsdata.png&#34; width=&#34;60px&#34; alt=&#34;Bassam Data&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/ivo-toby&#34;&gt;&lt;img src=&#34;https://github.com/ivo-toby.png&#34; width=&#34;60px&#34; alt=&#34;Ivo Toby&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/KTSCode&#34;&gt;&lt;img src=&#34;https://github.com/KTSCode.png&#34; width=&#34;60px&#34; alt=&#34;KTS Code&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://x.com/luxus&#34;&gt;&lt;img src=&#34;https://pbs.twimg.com/profile_images/744754093495844864/GwnEJygG_400x400.jpg&#34; width=&#34;60px&#34; alt=&#34;Luxus&#34;&gt;&lt;/a&gt;&#xA; &lt;!-- coffee --&gt;&#xA; &lt;!-- sponsors --&gt;&lt;a href=&#34;https://github.com/zhming0&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1054703?u=b173a2c1afc61fa25d9343704659630406e3dea7&amp;amp;v=4&#34; width=&#34;60px&#34; alt=&#34;Zhiming Guo&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/carlosflorencio&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1500881?u=6b4f80028aea4589bc3632739a40191bbcf58d22&amp;amp;v=4&#34; width=&#34;60px&#34; alt=&#34;Carlos Florêncio&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/GitMurf&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/64155612?u=b4aa60589d92918092f4ed0843d31ce8c1768f47&amp;amp;v=4&#34; width=&#34;60px&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/adam-e-trepanier&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/91675?u=524ffd6d070b7b6cacd796d08d830cb788e6efc6&amp;amp;v=4&#34; width=&#34;60px&#34; alt=&#34;Adam Trepanier&#34;&gt;&lt;/a&gt;&#xA; &lt;!-- sponsors --&gt; &lt;/p&gt; &#xA;&lt;!-- panvimdoc-ignore-end --&gt; &#xA;&lt;h2&gt;&lt;span&gt;✨&lt;/span&gt; Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;💬&lt;/span&gt; &lt;a href=&#34;https://github.com/features/copilot&#34;&gt;Copilot Chat&lt;/a&gt; meets &lt;a href=&#34;https://zed.dev/blog/zed-ai&#34;&gt;Zed AI&lt;/a&gt;, in Neovim&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🔌&lt;/span&gt; Support for Anthropic, Copilot, Gemini, Ollama, OpenAI, Azure OpenAI and xAI LLMs (or bring your own!)&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🚀&lt;/span&gt; Inline transformations, code creation and refactoring&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🤖&lt;/span&gt; Variables, Slash Commands, Agents/Tools and Workflows to improve LLM output&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;✨&lt;/span&gt; Built in prompt library for common tasks like advice on LSP errors and code explanations&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🏗&lt;/span&gt; Create your own custom prompts, Variables and Slash Commands&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;📚&lt;/span&gt; Have multiple chats open at the same time&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;💪&lt;/span&gt; Async execution for fast performance&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- panvimdoc-ignore-start --&gt; &#xA;&lt;h2&gt;&lt;span&gt;📸&lt;/span&gt; Screenshots&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;https://github.com/user-attachments/assets/04a2bed3-7af0-4c07-b58f-f644cef1c4bb&lt;/p&gt; &#xA; &lt;p&gt;https://github.com/user-attachments/assets/4e2a3680-cef5-4134-bf94-e2be93242b38&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- panvimdoc-ignore-end --&gt; &#xA;&lt;h2&gt;&lt;span&gt;⚡&lt;/span&gt; Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;curl&lt;/code&gt; library&lt;/li&gt; &#xA; &lt;li&gt;Neovim 0.10.0 or greater&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;(Optional)&lt;/em&gt; An API key for your chosen LLM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;📦&lt;/span&gt; Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install the plugin with your preferred package manager:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/folke/lazy.nvim&#34;&gt;Lazy.nvim&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;{&#xA;  &#34;olimorris/codecompanion.nvim&#34;,&#xA;  dependencies = {&#xA;    &#34;nvim-lua/plenary.nvim&#34;,&#xA;    &#34;nvim-treesitter/nvim-treesitter&#34;,&#xA;    &#34;hrsh7th/nvim-cmp&#34;, -- Optional: For using slash commands and variables in the chat buffer&#xA;    &#34;nvim-telescope/telescope.nvim&#34;, -- Optional: For using slash commands&#xA;    { &#34;MeanderingProgrammer/render-markdown.nvim&#34;, ft = { &#34;markdown&#34;, &#34;codecompanion&#34; } }, -- Optional: For prettier markdown rendering&#xA;    { &#34;stevearc/dressing.nvim&#34;, opts = {} }, -- Optional: Improves `vim.ui.select`&#xA;  },&#xA;  config = true&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/wbthomason/packer.nvim&#34;&gt;Packer&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;use({&#xA;  &#34;olimorris/codecompanion.nvim&#34;,&#xA;  config = function()&#xA;    require(&#34;codecompanion&#34;).setup()&#xA;  end,&#xA;  requires = {&#xA;    &#34;nvim-lua/plenary.nvim&#34;,&#xA;    &#34;nvim-treesitter/nvim-treesitter&#34;,&#xA;    &#34;hrsh7th/nvim-cmp&#34;, -- Optional: For using slash commands and variables in the chat buffer&#xA;    &#34;nvim-telescope/telescope.nvim&#34;, -- Optional: For using slash commands&#xA;    { &#34;MeanderingProgrammer/render-markdown.nvim&#34;, ft = { &#34;markdown&#34;, &#34;codecompanion&#34; } }, -- Optional: For prettier markdown rendering&#xA;    &#34;stevearc/dressing.nvim&#34; -- Optional: Improves `vim.ui.select`&#xA;  }&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/junegunn/vim-plug&#34;&gt;vim-plug&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-vim&#34;&gt;call plug#begin()&#xA;&#xA;Plug &#39;nvim-lua/plenary.nvim&#39;&#xA;Plug &#39;nvim-treesitter/nvim-treesitter&#39;&#xA;Plug &#39;hrsh7th/nvim-cmp&#39;, &#34; Optional: For using slash commands and variables in the chat buffer&#xA;Plug &#39;nvim-telescope/telescope.nvim&#39;, &#34; Optional: For using slash commands&#xA;Plug &#39;stevearc/dressing.nvim&#39; &#34; Optional: Improves `vim.ui.select`&#xA;Plug &#39;MeanderingProgrammer/render-markdown.nvim&#39; &#34; Optional: For prettier markdown rendering&#xA;Plug &#39;olimorris/codecompanion.nvim&#39;&#xA;&#xA;call plug#end()&#xA;&#xA;lua &amp;lt;&amp;lt; EOF&#xA;  require(&#34;codecompanion&#34;).setup()&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] The plugin requires the markdown Tree-sitter parser to be installed with &lt;code&gt;:TSInstall markdown&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nvim-telescope/telescope.nvim&#34;&gt;Telescope.nvim&lt;/a&gt; is a suggested inclusion as it makes leveraging the Slash Commands a little bit prettier. However, other providers are available. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/#speech_balloon-the-chat-buffer&#34;&gt;Chat Buffer&lt;/a&gt; section for more information.&lt;/p&gt; &#xA;&lt;p&gt;As per &lt;a href=&#34;https://github.com/olimorris/codecompanion.nvim/issues/377&#34;&gt;#377&lt;/a&gt;, if you pin your plugins to the latest releases, consider setting plenary.nvim to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;{ &#34;nvim-lua/plenary.nvim&#34;, branch = &#34;master&#34; },&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;🚀&lt;/span&gt; Quickstart&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Okay, okay...it&#39;s not quite a quickstart as you&#39;ll need to configure an &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/#electric_plug-adapters&#34;&gt;adapter&lt;/a&gt; and I recommend starting from the &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/#gear-configuration&#34;&gt;configuration&lt;/a&gt; section to understand how the plugin works.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Chat Buffer&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;!-- panvimdoc-ignore-start --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/597299d2-36b3-469e-b69c-4d8fd14838f8&#34; alt=&#34;Chat buffer&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- panvimdoc-ignore-end --&gt; &#xA;&lt;p&gt;Run &lt;code&gt;:CodeCompanionChat&lt;/code&gt; to open the chat buffer. Type your prompt and press &lt;code&gt;&amp;lt;CR&amp;gt;&lt;/code&gt;. Or, run &lt;code&gt;:CodeCompanionChat why are Lua and Neovim so perfect together?&lt;/code&gt; to send a prompt directly to the chat buffer. Toggle the chat buffer with &lt;code&gt;:CodeCompanionChat Toggle&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can add context from your code base by using &lt;em&gt;Variables&lt;/em&gt; and &lt;em&gt;Slash Commands&lt;/em&gt; in the chat buffer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Variables&lt;/em&gt;, accessed via &lt;code&gt;#&lt;/code&gt;, contain data about the present state of Neovim:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;#buffer&lt;/code&gt; - Shares the current buffer&#39;s code. You can also specify line numbers with &lt;code&gt;#buffer:8-20&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;#lsp&lt;/code&gt; - Shares LSP information and code for the current buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;#viewport&lt;/code&gt; - Shares the buffers and lines that you see in the Neovim viewport&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Slash commands&lt;/em&gt;, accessed via &lt;code&gt;/&lt;/code&gt;, run commands to insert additional context into the chat buffer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;/buffer&lt;/code&gt; - Insert open buffers&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/fetch&lt;/code&gt; - Insert URL contents&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/file&lt;/code&gt; - Insert a file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/help&lt;/code&gt; - Insert content from help tags&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/now&lt;/code&gt; - Insert the current date and time&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/symbols&lt;/code&gt; - Insert symbols from a selected file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/terminal&lt;/code&gt; - Insert terminal output&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Tools&lt;/em&gt;, accessed via &lt;code&gt;@&lt;/code&gt;, allow the LLM to function as an agent and carry out actions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;@cmd_runner&lt;/code&gt; - The LLM will run shell commands (subject to approval)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;@editor&lt;/code&gt; - The LLM will edit code in a Neovim buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;@files&lt;/code&gt; - The LLM will can work with files on the file system (subject to approval)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;@rag&lt;/code&gt; - The LLM will browse and search the internet for real-time information to supplement its response&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Tools can also be grouped together to form &lt;em&gt;Agents&lt;/em&gt;, which are also accessed via &lt;code&gt;@&lt;/code&gt; in the chat buffer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;@full_stack_dev&lt;/code&gt; - Contains the &lt;code&gt;cmd_runner&lt;/code&gt;, &lt;code&gt;editor&lt;/code&gt; and &lt;code&gt;files&lt;/code&gt; tools.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Press &lt;code&gt;?&lt;/code&gt; in the chat buffer to reveal the keymaps and options that are available.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inline Assistant&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;!-- panvimdoc-ignore-start --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/21568a7f-aea8-4928-b3d4-f39c6566a23c&#34; alt=&#34;Inline Assistant&#34;&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] The diff provider was selected as &lt;code&gt;mini_pick&lt;/code&gt; in the video above&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- panvimdoc-ignore-end --&gt; &#xA;&lt;p&gt;Run &lt;code&gt;:CodeCompanion &amp;lt;your prompt&amp;gt;&lt;/code&gt; to call the inline assistant. The assistant will evaluate the prompt and either write code or open a chat buffer. You can also make a visual selection and call the assistant.&lt;/p&gt; &#xA;&lt;p&gt;The assistant has knowledge of your last conversation from a chat buffer. A prompt such as &lt;code&gt;:CodeCompanion add the new function here&lt;/code&gt; will see the assistant add a code block directly into the current buffer.&lt;/p&gt; &#xA;&lt;p&gt;For convenience, you can call prompts from the &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/#clipboard-prompt-library&#34;&gt;prompt library&lt;/a&gt; via the assistant such as &lt;code&gt;:&#39;&amp;lt;,&#39;&amp;gt;CodeCompanion /buffer what does this file do?&lt;/code&gt;. The prompt library comes with the following defaults:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;/buffer&lt;/code&gt; - Send the current buffer to the LLM alongside a prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/commit&lt;/code&gt; - Generate a commit message&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/explain&lt;/code&gt; - Explain how selected code in a buffer works&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/fix&lt;/code&gt; - Fix the selected code&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/lsp&lt;/code&gt; - Explain the LSP diagnostics for the selected code&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/tests&lt;/code&gt; - Generate unit tests for selected code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are keymaps available to accept or reject edits from the LLM in the &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/#pencil2-inline-assistant&#34;&gt;inline assistant&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Action Palette&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;!-- panvimdoc-ignore-start --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/0d427d6d-aa5f-405c-ba14-583830251740&#34; alt=&#34;Action Palette&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- panvimdoc-ignore-end --&gt; &#xA;&lt;p&gt;Run &lt;code&gt;:CodeCompanionActions&lt;/code&gt; to open the action palette, which gives you access to all functionality of the plugin. By default the plugin uses &lt;code&gt;vim.ui.select&lt;/code&gt;, however, you can change the provider by altering the &lt;code&gt;display.action_palette.provider&lt;/code&gt; config value to be &lt;code&gt;telescope&lt;/code&gt; or &lt;code&gt;mini_pick&lt;/code&gt;. You can also call the Telescope extension with &lt;code&gt;:Telescope codecompanion&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Some actions and prompts will only be visible if you&#39;re in &lt;em&gt;Visual mode&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;List of commands&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The plugin has three core commands:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanion&lt;/code&gt; - Open the inline assistant&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionChat&lt;/code&gt; - Open a chat buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionActions&lt;/code&gt; - Open the &lt;em&gt;Action Palette&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;However, there are multiple options available:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanion &amp;lt;your prompt&amp;gt;&lt;/code&gt; - Prompt the inline assistant&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanion /&amp;lt;prompt library&amp;gt;&lt;/code&gt; - Use the &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/#clipboard-prompt-library&#34;&gt;prompt library&lt;/a&gt; with the inline assistant e.g. &lt;code&gt;/commit&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionChat &amp;lt;prompt&amp;gt;&lt;/code&gt; - Send a prompt to the LLM via a chat buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionChat &amp;lt;adapter&amp;gt;&lt;/code&gt; - Open a chat buffer with a specific adapter&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionChat Toggle&lt;/code&gt; - Toggle a chat buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionChat Add&lt;/code&gt; - Add visually selected chat to the current chat buffer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Suggested plugin workflow&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For an optimum plugin workflow, I recommend the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;vim.api.nvim_set_keymap(&#34;n&#34;, &#34;&amp;lt;C-a&amp;gt;&#34;, &#34;&amp;lt;cmd&amp;gt;CodeCompanionActions&amp;lt;cr&amp;gt;&#34;, { noremap = true, silent = true })&#xA;vim.api.nvim_set_keymap(&#34;v&#34;, &#34;&amp;lt;C-a&amp;gt;&#34;, &#34;&amp;lt;cmd&amp;gt;CodeCompanionActions&amp;lt;cr&amp;gt;&#34;, { noremap = true, silent = true })&#xA;vim.api.nvim_set_keymap(&#34;n&#34;, &#34;&amp;lt;LocalLeader&amp;gt;a&#34;, &#34;&amp;lt;cmd&amp;gt;CodeCompanionChat Toggle&amp;lt;cr&amp;gt;&#34;, { noremap = true, silent = true })&#xA;vim.api.nvim_set_keymap(&#34;v&#34;, &#34;&amp;lt;LocalLeader&amp;gt;a&#34;, &#34;&amp;lt;cmd&amp;gt;CodeCompanionChat Toggle&amp;lt;cr&amp;gt;&#34;, { noremap = true, silent = true })&#xA;vim.api.nvim_set_keymap(&#34;v&#34;, &#34;ga&#34;, &#34;&amp;lt;cmd&amp;gt;CodeCompanionChat Add&amp;lt;cr&amp;gt;&#34;, { noremap = true, silent = true })&#xA;&#xA;-- Expand &#39;cc&#39; into &#39;CodeCompanion&#39; in the command line&#xA;vim.cmd([[cab cc CodeCompanion]])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] You can also assign prompts from the library to specific mappings. See the &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/#clipboard-prompt-library&#34;&gt;prompt library&lt;/a&gt; section for more information.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;&lt;span&gt;⚙&lt;/span&gt; Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Before configuring the plugin, it&#39;s important to understand how it&#39;s structured.&lt;/p&gt; &#xA;&lt;p&gt;The plugin uses adapters to connect to LLMs. Out of the box, the plugin supports:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Anthropic (&lt;code&gt;anthropic&lt;/code&gt;) - Requires an API key and supports &lt;a href=&#34;https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching&#34;&gt;prompt caching&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copilot (&lt;code&gt;copilot&lt;/code&gt;) - Requires a token which is created via &lt;code&gt;:Copilot setup&lt;/code&gt; in &lt;a href=&#34;https://github.com/github/copilot.vim&#34;&gt;Copilot.vim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gemini (&lt;code&gt;gemini&lt;/code&gt;) - Requires an API key&lt;/li&gt; &#xA; &lt;li&gt;Ollama (&lt;code&gt;ollama&lt;/code&gt;) - Both local and remotely hosted&lt;/li&gt; &#xA; &lt;li&gt;OpenAI (&lt;code&gt;openai&lt;/code&gt;) - Requires an API key&lt;/li&gt; &#xA; &lt;li&gt;Azure OpenAI (&lt;code&gt;azure_openai&lt;/code&gt;) - Requires an Azure OpenAI service with a model deployment&lt;/li&gt; &#xA; &lt;li&gt;xAI (&lt;code&gt;xai&lt;/code&gt;) - Requires an API key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The plugin utilises objects called Strategies. These are the different ways that a user can interact with the plugin. The &lt;em&gt;chat&lt;/em&gt; strategy harnesses a buffer to allow direct conversation with the LLM. The &lt;em&gt;inline&lt;/em&gt; strategy allows for output from the LLM to be written directly into a pre-existing Neovim buffer. The &lt;em&gt;agent&lt;/em&gt; and &lt;em&gt;workflow&lt;/em&gt; strategies are wrappers for the &lt;em&gt;chat&lt;/em&gt; strategy, allowing for &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/#robot-agents--tools&#34;&gt;tool use&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/#world_map-agentic-workflows&#34;&gt;agentic workflows&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The plugin allows you to specify adapters for each strategy and also for each &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/#clipboard-prompt-library&#34;&gt;prompt library&lt;/a&gt; entry.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;🛠&lt;/span&gt; Changing the Defaults&lt;/h3&gt; &#xA;&lt;p&gt;The default config can be found in the &lt;a href=&#34;https://github.com/olimorris/codecompanion.nvim/raw/main/lua/codecompanion/config.lua&#34;&gt;config.lua&lt;/a&gt; file and the defaults can be changed by calling the &lt;code&gt;setup&lt;/code&gt; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;codecompanion&#34;).setup({&#xA;  display = {&#xA;    diff = {&#xA;      provider = &#34;mini_diff&#34;,&#xA;    },&#xA;  },&#xA;  opts = {&#xA;    log_level = &#34;DEBUG&#34;,&#xA;  },&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/#electric_plug-adapters&#34;&gt;adapter&lt;/a&gt; section below in order to configure adapters.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Changing the System Prompt&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The default system prompt has been carefully curated to deliver responses which are similar to GitHub Copilot Chat, no matter which LLM you use. That is, you&#39;ll receive responses which are terse, professional and with expertise in coding. However, you can modify the &lt;code&gt;opts.system_prompt&lt;/code&gt; table in the config to suit your needs. You can also set it as a function which can receive the current chat buffer&#39;s adapter as a parameter, giving you the option of setting system prompts that are LLM or model specific:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;codecompanion&#34;).setup({&#xA;  opts = {&#xA;    ---@param adapter CodeCompanion.Adapter&#xA;    ---@return string&#xA;    system_prompt = function(opts)&#xA;      if opts.adapter.schema.model.default == &#34;llama3.1:latest&#34; then&#xA;        return &#34;My custom system prompt&#34;&#xA;      end&#xA;      return &#34;My default system prompt&#34;&#xA;    end&#xA;  }&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Changing the Language&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;CodeCompanion supports multiple languages for non-code responses. You can configure this in your setup:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;codecompanion&#39;).setup({&#xA;  opts = {&#xA;    language = &#34;English&#34; -- Default is &#34;English&#34;&#xA;  }&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using with render-markdown.nvim&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you use the fantastic &lt;a href=&#34;https://github.com/MeanderingProgrammer/render-markdown.nvim&#34;&gt;render-markdown.nvim&lt;/a&gt; plugin, then please ensure you turn off the &lt;code&gt;render_headers&lt;/code&gt; display option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;codecompanion&#34;).setup({&#xA;  display = {&#xA;    chat = {&#xA;      render_headers = false,&#xA;    }&#xA;  }&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;span&gt;🔌&lt;/span&gt; Adapters&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to your &lt;a href=&#34;https://github.com/olimorris/codecompanion.nvim/tree/main/lua/codecompanion/adapters&#34;&gt;chosen adapter&lt;/a&gt; to understand its configuration. You will need to set an API key for non-locally hosted LLMs.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] To create your own adapter or better understand how they work, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/doc/ADAPTERS.md&#34;&gt;ADAPTERS&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Changing the Default Adapter&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To specify a different adapter to the default (&lt;code&gt;openai&lt;/code&gt;), simply change the &lt;code&gt;strategies.*&lt;/code&gt; table:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;codecompanion&#34;).setup({&#xA;  strategies = {&#xA;    chat = {&#xA;      adapter = &#34;anthropic&#34;,&#xA;    },&#xA;    inline = {&#xA;      adapter = &#34;copilot&#34;,&#xA;    },&#xA;  },&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Setting an API Key&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;codecompanion&#34;).setup({&#xA;  adapters = {&#xA;    anthropic = function()&#xA;      return require(&#34;codecompanion.adapters&#34;).extend(&#34;anthropic&#34;, {&#xA;        env = {&#xA;          api_key = &#34;MY_OTHER_ANTHROPIC_KEY&#34;&#xA;        },&#xA;      })&#xA;    end,&#xA;  },&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the example above, we&#39;re using the base of the Anthropic adapter but changing the name of the default API key which it uses.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Setting an API Key Using a Command&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Having API keys in plain text in your shell is not always safe. Thanks to &lt;a href=&#34;https://github.com/olimorris/codecompanion.nvim/pull/24&#34;&gt;this PR&lt;/a&gt;, you can run commands from within your config by prefixing them with &lt;code&gt;cmd:&lt;/code&gt;. In the example below, we&#39;re using the 1Password CLI to read an OpenAI credential.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;codecompanion&#34;).setup({&#xA;  adapters = {&#xA;    openai = function()&#xA;      return require(&#34;codecompanion.adapters&#34;).extend(&#34;openai&#34;, {&#xA;        env = {&#xA;          api_key = &#34;cmd:op read op://personal/OpenAI/credential --no-newline&#34;,&#xA;        },&#xA;      })&#xA;    end,&#xA;  },&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using Ollama Remotely&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To use Ollama remotely, change the URL in the &lt;code&gt;env&lt;/code&gt; table, set an API key and pass it via an &#34;Authorization&#34; header:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;codecompanion&#34;).setup({&#xA;  adapters = {&#xA;    ollama = function()&#xA;      return require(&#34;codecompanion.adapters&#34;).extend(&#34;ollama&#34;, {&#xA;        env = {&#xA;          url = &#34;https://my_ollama_url&#34;,&#xA;          api_key = &#34;OLLAMA_API_KEY&#34;,&#xA;        },&#xA;        headers = {&#xA;          [&#34;Content-Type&#34;] = &#34;application/json&#34;,&#xA;          [&#34;Authorization&#34;] = &#34;Bearer ${api_key}&#34;,&#xA;        },&#xA;        parameters = {&#xA;          sync = true,&#xA;        },&#xA;      })&#xA;    end,&#xA;  },&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using OpenAI compatible Models like LMStudio or self-hosted models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To use any other OpenAI compatible models, change the URL in the &lt;code&gt;env&lt;/code&gt; table, set an API key:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;codecompanion&#34;).setup({&#xA;  adapters = {&#xA;    ollama = function()&#xA;      return require(&#34;codecompanion.adapters&#34;).extend(&#34;openai_compatible&#34;, {&#xA;        env = {&#xA;          url = &#34;http[s]://open_compatible_ai_url&#34;, -- optional: default value is ollama url http://127.0.0.1:11434&#xA;          api_key = &#34;OpenAI_API_KEY&#34;, -- optional: if your endpoint is authenticated&#xA;          chat_url = &#34;/v1/chat/completions&#34;, -- optional: default value, override if different&#xA;        },&#xA;      })&#xA;    end,&#xA;  },&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using Azure OpenAI&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To use Azure OpenAI, you need to have an Azure OpenAI service, an API key, and a model deployment. Follow these steps to configure the adapter:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create an Azure OpenAI service in your Azure portal.&lt;/li&gt; &#xA; &lt;li&gt;Deploy a model in the Azure OpenAI service.&lt;/li&gt; &#xA; &lt;li&gt;Obtain the API key from the Azure portal.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Then, configure the adapter in your setup as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;codecompanion&#34;).setup({&#xA;  strategies = {&#xA;    chat = {&#xA;      adapter = &#34;azure_openai&#34;,&#xA;    },&#xA;    inline = {&#xA;      adapter = &#34;azure_openai&#34;,&#xA;    },&#xA;  },&#xA;  adapters = {&#xA;    azure_openai = function()&#xA;      return require(&#34;codecompanion.adapters&#34;).extend(&#34;azure_openai&#34;, {&#xA;        env = {&#xA;          api_key = &#39;YOUR_AZURE_OPENAI_API_KEY&#39;,&#xA;          endpoint = &#39;YOUR_AZURE_OPENAI_ENDPOINT&#39;,&#xA;        },&#xA;        schema = {&#xA;          model = &#34;YOUR_DEPLOYMENT_NAME&#34;,&#xA;        },&#xA;      })&#xA;    end,&#xA;  },&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Connecting via a Proxy&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also connect via a proxy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;codecompanion&#34;).setup({&#xA;  adapters = {&#xA;    opts = {&#xA;      allow_insecure = true, -- Use if required&#xA;      proxy = &#34;socks5://127.0.0.1:9999&#34;&#xA;    }&#xA;  },&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Changing an Adapter&#39;s Default Model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;A common ask is to change an adapter&#39;s default model. This can be done by altering the &lt;code&gt;schema.model.default&lt;/code&gt; table:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;codecompanion&#34;).setup({&#xA;  adapters = {&#xA;    anthropic = function()&#xA;      return require(&#34;codecompanion.adapters&#34;).extend(&#34;anthropic&#34;, {&#xA;        schema = {&#xA;          model = {&#xA;            default = &#34;claude-3-opus-20240229&#34;,&#xA;          },&#xA;        },&#xA;      })&#xA;    end,&#xA;  },&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Configuring Adapter Settings&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;LLMs have many settings such as &lt;em&gt;model&lt;/em&gt;, &lt;em&gt;temperature&lt;/em&gt; and &lt;em&gt;max_tokens&lt;/em&gt;. In an adapter, these sit within a schema table and can be configured during setup:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;codecompanion&#34;).setup({&#xA;  adapters = {&#xA;    llama3 = function()&#xA;      return require(&#34;codecompanion.adapters&#34;).extend(&#34;ollama&#34;, {&#xA;        name = &#34;llama3&#34;, -- Give this adapter a different name to differentiate it from the default ollama adapter&#xA;        schema = {&#xA;          model = {&#xA;            default = &#34;llama3:latest&#34;,&#xA;          },&#xA;          num_ctx = {&#xA;            default = 16384,&#xA;          },&#xA;          num_predict = {&#xA;            default = -1,&#xA;          },&#xA;        },&#xA;      })&#xA;    end,&#xA;  },&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Set a Global Adapter with a Global Variable&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In some cases, it may be helpful to set a global adapter across both the &lt;code&gt;chat&lt;/code&gt; and &lt;code&gt;inline&lt;/code&gt; strategies, on the fly. Perhaps, if your LLM of choice is down or you&#39;re without internet. This can be achieved by setting the &lt;code&gt;vim.g.codecompanion_adapter&lt;/code&gt; variable to the name of an adapter in the config. This also prevents you from having to go into every chat buffer that you have open to manually set the adapter.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;💡&lt;/span&gt; Advanced Usage&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;span&gt;📋&lt;/span&gt; Prompt Library&lt;/h3&gt; &#xA;&lt;p&gt;The plugin comes with a number of pre-built prompts. As per &lt;a href=&#34;https://github.com/olimorris/codecompanion.nvim/raw/main/lua/codecompanion/config.lua&#34;&gt;the config&lt;/a&gt;, these can be called via keymaps or via the cmdline. These prompts have been carefully curated to mimic those in &lt;a href=&#34;https://docs.github.com/en/copilot/using-github-copilot/asking-github-copilot-questions-in-your-ide&#34;&gt;GitHub&#39;s Copilot Chat&lt;/a&gt;. Of course, you can create your own prompts and add them to the Action Palette or even to the slash command completion menu in the chat buffer. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/doc/RECIPES.md&#34;&gt;RECIPES&lt;/a&gt; guide for more information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using Keymaps&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can call a prompt from the library via a keymap using the &lt;code&gt;prompt&lt;/code&gt; helper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;vim.api.nvim_set_keymap(&#34;v&#34;, &#34;&amp;lt;LocalLeader&amp;gt;ce&#34;, &#34;&#34;, {&#xA;  callback = function()&#xA;    require(&#34;codecompanion&#34;).prompt(&#34;explain&#34;)&#xA;  end,&#xA;  noremap = true,&#xA;  silent = true,&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the example above, we&#39;ve set a visual keymap that will trigger the Explain prompt. Providing the &lt;code&gt;short_name&lt;/code&gt; of the prompt as an argument to the helper (e.g. &#34;commit&#34;) will resolve the strategy down to an action.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;💬&lt;/span&gt; The Chat Buffer&lt;/h3&gt; &#xA;&lt;p&gt;The chat buffer is where you converse with an LLM from within Neovim. The chat buffer has been designed to be turn based, whereby you send a message and the LLM replies. Messages are segmented by H2 headers and once a message has been sent, it cannot be edited. You can also have multiple chat buffers open at the same.&lt;/p&gt; &#xA;&lt;p&gt;The look and feel of the chat buffer can be customised as per the &lt;code&gt;display.chat&lt;/code&gt; table in the &lt;a href=&#34;https://github.com/olimorris/codecompanion.nvim/raw/main/lua/codecompanion/config.lua&#34;&gt;config&lt;/a&gt;. You can also add additional &lt;em&gt;Variables&lt;/em&gt; and &lt;em&gt;Slash Commands&lt;/em&gt; which can then be referenced in the chat buffer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Keymaps&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;When in the chat buffer, press &lt;code&gt;?&lt;/code&gt; to bring up a menu that lists the available keymaps, variables, slash commands and tools. Currently, the keymaps available to you in normal mode are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;CR&amp;gt;|&amp;lt;C-s&amp;gt;&lt;/code&gt; to send a message to the LLM&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;C-c&amp;gt;&lt;/code&gt; to close the chat buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;q&lt;/code&gt; to stop the current request&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ga&lt;/code&gt; to change the adapter for the currentchat&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gc&lt;/code&gt; to insert a codeblock in the chat buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gd&lt;/code&gt; to view/debug the chat buffer&#39;s contents&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gf&lt;/code&gt; to fold any codeblocks in the chat buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gr&lt;/code&gt; to regenerate the last response&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gs&lt;/code&gt; to toggle the system prompt on/off&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gx&lt;/code&gt; to clear the chat buffer&#39;s contents&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gy&lt;/code&gt; to yank the last codeblock in the chat buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[[&lt;/code&gt; to move to the previous header&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;]]&lt;/code&gt; to move to the next header&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;{&lt;/code&gt; to move to the previous chat&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;}&lt;/code&gt; to move to the next chat&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] There are also corresponding insert mode mappings available.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Settings&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can display your selected adapter&#39;s schema at the top of the buffer, if &lt;code&gt;display.chat.show_settings&lt;/code&gt; is set to &lt;code&gt;true&lt;/code&gt;. This allows you to vary the response from the LLM.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Slash Commands&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;As outlined in the &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/#rocket-quickstart&#34;&gt;Quickstart&lt;/a&gt; section, Slash Commands allow you to easily share additional context with your LLM from the chat buffer. Some of the commands also allow for multiple providers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;/buffer&lt;/code&gt; - Has &lt;code&gt;default&lt;/code&gt;, &lt;code&gt;telescope&lt;/code&gt; and &lt;code&gt;fzf_lua&lt;/code&gt; providers&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/files&lt;/code&gt; - Has &lt;code&gt;default&lt;/code&gt;, &lt;code&gt;telescope&lt;/code&gt;, &lt;code&gt;mini_pick&lt;/code&gt; and &lt;code&gt;fzf_lua&lt;/code&gt; providers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://github.com/olimorris/codecompanion.nvim/raw/main/lua/codecompanion/config.lua&#34;&gt;the config&lt;/a&gt; to see how to change the default provider.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;✏&lt;/span&gt; Inline Assistant&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] If you&#39;ve set &lt;code&gt;opts.send_code = false&lt;/code&gt; in your config then the plugin will endeavour to ensure no code is sent to the LLM.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;One of the challenges with inline editing is determining how the LLM&#39;s response should be handled in the buffer. If you&#39;ve prompted the LLM to &lt;em&gt;&#34;create a table of 5 common text editors&#34;&lt;/em&gt; then you may wish for the response to be placed at the cursor&#39;s position in the current buffer. However, if you asked the LLM to &lt;em&gt;&#34;refactor this function&#34;&lt;/em&gt; then you&#39;d expect the response to &lt;em&gt;replace&lt;/em&gt; a visual selection. The plugin will use the inline LLM you&#39;ve specified in your config to determine if the response should...&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;replace&lt;/em&gt; - replace a visual selection you&#39;ve made&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;add&lt;/em&gt; - be added in the current buffer at the cursor position&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;new&lt;/em&gt; - be placed in a new buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;chat&lt;/em&gt; - be placed in a chat buffer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By default, an inline assistant prompt will trigger the diff feature, showing differences between the original buffer and the changes from the LLM. This can be turned off in your config via the &lt;code&gt;display.diff.provider&lt;/code&gt; table. You can also choose to accept or reject the LLM&#39;s suggestions with the following keymaps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;ga&lt;/code&gt; - Accept an inline edit&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gr&lt;/code&gt; - Reject an inline edit&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;span&gt;🤖&lt;/span&gt; Agents / Tools&lt;/h3&gt; &#xA;&lt;p&gt;As outlined by Andrew Ng in &lt;a href=&#34;https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use&#34;&gt;Agentic Design Patterns Part 3, Tool Use&lt;/a&gt;, LLMs can act as agents by leveraging external tools. Andrew notes some common examples such as web searching or code execution that have obvious benefits when using LLMs.&lt;/p&gt; &#xA;&lt;p&gt;In the plugin, tools are simply context that&#39;s given to an LLM via a &lt;code&gt;system&lt;/code&gt; prompt and Agents are groupings of tools. These give LLM&#39;s knowledge and a defined schema which can be included in the response for the plugin to parse, execute and feedback on. Agents and tools can be added as a participant to the chat buffer by using the &lt;code&gt;@&lt;/code&gt; key.&lt;/p&gt; &#xA;&lt;p&gt;More information on how agents and tools work and how you can create your own can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/doc/TOOLS.md&#34;&gt;TOOLS&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;🗺&lt;/span&gt; Agentic Workflows&lt;/h3&gt; &#xA;&lt;p&gt;Agentic Workflows prompt an LLM multiple times, giving them the ability to build their answer step-by-step instead of at once. This leads to much better output as &lt;a href=&#34;https://www.deeplearning.ai/the-batch/issue-242/&#34;&gt;outlined&lt;/a&gt; by Andrew Ng. Infact, it&#39;s possible for older models like GPT 3.5 to outperform newer models (using traditional zero-shot inference).&lt;/p&gt; &#xA;&lt;p&gt;Implementing Andrew&#39;s advice, at various stages of a pre-defined workflow, the plugin will automatically prompt the LLM without any input or triggering required from the user. The plugin contains a default &lt;code&gt;Code workflow&lt;/code&gt;, as part of the prompt library, which guides the LLM into writing better code.&lt;/p&gt; &#xA;&lt;p&gt;Of course you can add new workflows by following the &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/doc/RECIPES.md&#34;&gt;RECIPES&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;🍭&lt;/span&gt; Extras&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Highlight Groups&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The plugin sets the following highlight groups during setup:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionChatHeader&lt;/code&gt; - The headers in the chat buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionChatSeparator&lt;/code&gt; - Separator between headings in the chat buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionChatTokens&lt;/code&gt; - Virtual text in the chat buffer showing the token count&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionChatAgent&lt;/code&gt; - Agents in the chat buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionChatTool&lt;/code&gt; - Tools in the chat buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionChatVariable&lt;/code&gt; - Variables in the chat buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionVirtualText&lt;/code&gt; - All other virtual text in the plugin&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Events/Hooks&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The plugin fires many events during its lifecycle:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionChatClosed&lt;/code&gt; - Fired after a chat has been closed&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionChatAdapter&lt;/code&gt; - Fired after the adapter has been set in the chat&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionChatModel&lt;/code&gt; - Fired after the model has been set in the chat&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionToolAdded&lt;/code&gt; - Fired when a tool has been added to a chat&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionAgentStarted&lt;/code&gt; - Fired when an agent has been initiated in the chat&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionAgentFinished&lt;/code&gt; - Fired when an agent has finished all tool executions&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionInlineStarted&lt;/code&gt; - Fired at the start of the Inline strategy&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionInlineFinished&lt;/code&gt; - Fired at the end of the Inline strategy&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionRequestStarted&lt;/code&gt; - Fired at the start of any API request&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionRequestFinished&lt;/code&gt; - Fired at the end of any API request&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionDiffAttached&lt;/code&gt; - Fired when in Diff mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CodeCompanionDiffDetached&lt;/code&gt; - Fired when exiting Diff mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Some events are sent with a data payload which can be leveraged.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Events can be hooked into as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local group = vim.api.nvim_create_augroup(&#34;CodeCompanionHooks&#34;, {})&#xA;&#xA;vim.api.nvim_create_autocmd({ &#34;User&#34; }, {&#xA;  pattern = &#34;CodeCompanionInline*&#34;,&#xA;  group = group,&#xA;  callback = function(request)&#xA;    if request.match == &#34;CodeCompanionInlineFinished&#34; then&#xA;      -- Format the buffer after the inline request has completed&#xA;      require(&#34;conform&#34;).format({ bufnr = request.buf })&#xA;    end&#xA;  end,&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Statuslines&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can incorporate a visual indication to show when the plugin is communicating with an LLM in your Neovim configuration. Below are examples for two popular statusline plugins.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;lualine.nvim:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local M = require(&#34;lualine.component&#34;):extend()&#xA;&#xA;M.processing = false&#xA;M.spinner_index = 1&#xA;&#xA;local spinner_symbols = {&#xA;  &#34;⠋&#34;,&#xA;  &#34;⠙&#34;,&#xA;  &#34;⠹&#34;,&#xA;  &#34;⠸&#34;,&#xA;  &#34;⠼&#34;,&#xA;  &#34;⠴&#34;,&#xA;  &#34;⠦&#34;,&#xA;  &#34;⠧&#34;,&#xA;  &#34;⠇&#34;,&#xA;  &#34;⠏&#34;,&#xA;}&#xA;local spinner_symbols_len = 10&#xA;&#xA;-- Initializer&#xA;function M:init(options)&#xA;  M.super.init(self, options)&#xA;&#xA;  local group = vim.api.nvim_create_augroup(&#34;CodeCompanionHooks&#34;, {})&#xA;&#xA;  vim.api.nvim_create_autocmd({ &#34;User&#34; }, {&#xA;    pattern = &#34;CodeCompanionRequest*&#34;,&#xA;    group = group,&#xA;    callback = function(request)&#xA;      if request.match == &#34;CodeCompanionRequestStarted&#34; then&#xA;        self.processing = true&#xA;      elseif request.match == &#34;CodeCompanionRequestFinished&#34; then&#xA;        self.processing = false&#xA;      end&#xA;    end,&#xA;  })&#xA;end&#xA;&#xA;-- Function that runs every time statusline is updated&#xA;function M:update_status()&#xA;  if self.processing then&#xA;    self.spinner_index = (self.spinner_index % spinner_symbols_len) + 1&#xA;    return spinner_symbols[self.spinner_index]&#xA;  else&#xA;    return nil&#xA;  end&#xA;end&#xA;&#xA;return M&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;heirline.nvim:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local CodeCompanion = {&#xA;  static = {&#xA;    processing = false,&#xA;  },&#xA;  update = {&#xA;    &#34;User&#34;,&#xA;    pattern = &#34;CodeCompanionRequest*&#34;,&#xA;    callback = function(self, args)&#xA;      if args.match == &#34;CodeCompanionRequestStarted&#34; then&#xA;        self.processing = true&#xA;      elseif args.match == &#34;CodeCompanionRequestFinished&#34; then&#xA;        self.processing = false&#xA;      end&#xA;      vim.cmd(&#34;redrawstatus&#34;)&#xA;    end,&#xA;  },&#xA;  {&#xA;    condition = function(self)&#xA;      return self.processing&#xA;    end,&#xA;    provider = &#34; &#34;,&#xA;    hl = { fg = &#34;yellow&#34; },&#xA;  },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Legendary.nvim&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The plugin also supports the amazing &lt;a href=&#34;https://github.com/mrjones2014/legendary.nvim&#34;&gt;legendary.nvim&lt;/a&gt; plugin. Simply enable it in your config:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;legendary&#39;).setup({&#xA;  extensions = {&#xA;    codecompanion = true,&#xA;  },&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mini.Diff&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re using &lt;a href=&#34;https://github.com/echasnovski/mini.diff&#34;&gt;mini.diff&lt;/a&gt; you can put an icon in the statusline to indicate which diff is currently in use in a buffer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local function diff_source()&#xA;  local bufnr, diff_source, icon&#xA;  bufnr = vim.api.nvim_get_current_buf()&#xA;  diff_source = vim.b[bufnr].diffCompGit&#xA;  if not diff_source then&#xA;    return &#34;&#34;&#xA;  end&#xA;  if diff_source == &#34;git&#34; then&#xA;    icon = &#34;󰊤 &#34;&#xA;  elseif diff_source == &#34;codecompanion&#34; then&#xA;    icon = &#34; &#34;&#xA;  end&#xA;  return string.format(&#34;%%#StatusLineLSP#%s&#34;, icon)&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;🧰&lt;/span&gt; Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;Before raising an &lt;a href=&#34;https://github.com/olimorris/codecompanion.nvim/issues&#34;&gt;issue&lt;/a&gt;, there are a number of steps you can take to troubleshoot a problem:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Checkhealth&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run &lt;code&gt;:checkhealth codecompanion&lt;/code&gt; and check all dependencies are installed correctly. Also take note of the log file path.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Turn on logging&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Update your config and turn debug logging on:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;opts = {&#xA;  log_level = &#34;DEBUG&#34;, -- or &#34;TRACE&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and inspect the log file as per the location from the checkhealth command.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Try with a &lt;code&gt;minimal.lua&lt;/code&gt; file&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;A large proportion of issues which are raised in Neovim plugins are to do with a user&#39;s own config. That&#39;s why I always ask users to fill in a &lt;code&gt;minimal.lua&lt;/code&gt; file when they raise an issue. We can rule out their config being an issue and it allows me to recreate the problem.&lt;/p&gt; &#xA;&lt;p&gt;For this purpose, I have included a &lt;a href=&#34;https://github.com/olimorris/codecompanion.nvim/raw/main/minimal.lua&#34;&gt;minimal.lua&lt;/a&gt; file in the repository for you to test out if you&#39;re facing issues. Simply copy the file, edit it and run neovim with &lt;code&gt;nvim --clean -u minimal.lua&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;!-- panvimdoc-ignore-start --&gt; &#xA;&lt;h2&gt;&lt;span&gt;🎁&lt;/span&gt; Contributing&lt;/h2&gt; &#xA;&lt;p&gt;I am open to contributions but they will be implemented at my discretion. Feel free to open up a discussion before embarking on a PR and please read the &lt;a href=&#34;https://raw.githubusercontent.com/olimorris/codecompanion.nvim/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;👏&lt;/span&gt; Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stevearc&#34;&gt;Steven Arcangeli&lt;/a&gt; for his genius creation of the chat buffer and his feedback early on&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/manoelcampos&#34;&gt;Manoel Campos&lt;/a&gt; for the &lt;a href=&#34;https://github.com/manoelcampos/xml2lua&#34;&gt;xml2lua&lt;/a&gt; library that&#39;s used in the tools implementation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/S1M0N38/dante.nvim&#34;&gt;Dante.nvim&lt;/a&gt; for the beautifully simple diff implementation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/piersolenski/wtf.nvim&#34;&gt;Wtf.nvim&lt;/a&gt; for the LSP assistant action&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CopilotC-Nvim/CopilotChat.nvim&#34;&gt;CopilotChat.nvim&lt;/a&gt; for the rendering and usability of the chat buffer&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stevearc/aerial.nvim&#34;&gt;Aerial.nvim&lt;/a&gt; for the Tree-sitter parsing which inspired the symbols Slash Command&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- panvimdoc-ignore-end --&gt;</summary>
  </entry>
</feed>