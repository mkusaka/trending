<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-03T01:28:31Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>github-linguist/linguist</title>
    <updated>2024-10-03T01:28:31Z</updated>
    <id>tag:github.com,2024-10-03:/github-linguist/linguist</id>
    <link href="https://github.com/github-linguist/linguist" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Language Savant. If your repository&#39;s language is being reported incorrectly, send us a pull request!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Linguist&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/github/linguist/actions&#34;&gt;&lt;img src=&#34;https://github.com/github/linguist/workflows/Run%20Tests/badge.svg?sanitize=true&#34; alt=&#34;Actions Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://codespaces.new/github-linguist/linguist&#34;&gt;&lt;img src=&#34;https://github.com/codespaces/badge.svg?sanitize=true&#34; alt=&#34;Open in GitHub Codespaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This library is used on GitHub.com to detect blob languages, ignore binary or vendored files, suppress generated files in diffs, and generate language breakdown graphs.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/github-linguist/linguist/main/docs/how-linguist-works.md&#34;&gt;How Linguist works&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/github-linguist/linguist/main/docs/overrides.md&#34;&gt;Change Linguist&#39;s behaviour with overrides&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/github-linguist/linguist/main/docs/troubleshooting.md&#34;&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/github-linguist/linguist/main/CONTRIBUTING.md&#34;&gt;Contributing guidelines&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install the gem:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gem install github-linguist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Linguist is a Ruby library so you will need a recent version of Ruby installed. There are known problems with the macOS/Xcode supplied version of Ruby that causes problems installing some of the dependencies. Accordingly, we highly recommend you install a version of Ruby using Homebrew, &lt;code&gt;rbenv&lt;/code&gt;, &lt;code&gt;rvm&lt;/code&gt;, &lt;code&gt;ruby-build&lt;/code&gt;, &lt;code&gt;asdf&lt;/code&gt; or other packaging system, before attempting to install Linguist and the dependencies.&lt;/p&gt; &#xA;&lt;p&gt;Linguist uses &lt;a href=&#34;https://github.com/brianmario/charlock_holmes&#34;&gt;&lt;code&gt;charlock_holmes&lt;/code&gt;&lt;/a&gt; for character encoding and &lt;a href=&#34;https://github.com/libgit2/rugged&#34;&gt;&lt;code&gt;rugged&lt;/code&gt;&lt;/a&gt; for libgit2 bindings for Ruby. These components have their own dependencies.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;charlock_holmes &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;cmake&lt;/li&gt; &#xA;   &lt;li&gt;pkg-config&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;http://site.icu-project.org/&#34;&gt;ICU&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://zlib.net/&#34;&gt;zlib&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;rugged &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://curl.haxx.se/libcurl/&#34;&gt;libcurl&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.openssl.org&#34;&gt;OpenSSL&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You may need to install missing dependencies before you can install Linguist. For example, on macOS with &lt;a href=&#34;http://brew.sh/&#34;&gt;Homebrew&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install cmake pkg-config icu4c&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Ubuntu:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install build-essential cmake pkg-config libicu-dev zlib1g-dev libcurl4-openssl-dev libssl-dev ruby-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Application usage&lt;/h3&gt; &#xA;&lt;p&gt;Linguist can be used in your application as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#39;rugged&#39;&#xA;require &#39;linguist&#39;&#xA;&#xA;repo = Rugged::Repository.new(&#39;.&#39;)&#xA;project = Linguist::Repository.new(repo, repo.head.target_id)&#xA;project.language       #=&amp;gt; &#34;Ruby&#34;&#xA;project.languages      #=&amp;gt; { &#34;Ruby&#34; =&amp;gt; 119387 }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Command line usage&lt;/h3&gt; &#xA;&lt;h4&gt;Git Repository&lt;/h4&gt; &#xA;&lt;p&gt;A repository&#39;s languages stats can also be assessed from the command line using the &lt;code&gt;github-linguist&lt;/code&gt; executable. Without any options, &lt;code&gt;github-linguist&lt;/code&gt; will output the language breakdown by percentage and file size.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /path-to-repository&#xA;github-linguist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can try running &lt;code&gt;github-linguist&lt;/code&gt; on the root directory in this repository itself:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ github-linguist&#xA;66.84%  264519     Ruby&#xA;24.68%  97685      C&#xA;6.57%   25999      Go&#xA;1.29%   5098       Lex&#xA;0.32%   1257       Shell&#xA;0.31%   1212       Dockerfile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Additional options&lt;/h4&gt; &#xA;&lt;h5&gt;&lt;code&gt;--rev REV&lt;/code&gt;&lt;/h5&gt; &#xA;&lt;p&gt;The &lt;code&gt;--rev REV&lt;/code&gt; flag will change the git revision being analyzed to any &lt;a href=&#34;https://git-scm.com/docs/gitrevisions#_specifying_revisions&#34;&gt;gitrevisions(1)&lt;/a&gt; compatible revision you specify.&lt;/p&gt; &#xA;&lt;p&gt;This is useful to analyze the makeup of a repo as of a certain tag, or in a certain branch.&lt;/p&gt; &#xA;&lt;p&gt;For example, here is the popular &lt;a href=&#34;https://github.com/jekyll/jekyll&#34;&gt;Jekyll open source project&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ github-linguist jekyll&#xA;&#xA;70.64%  709959     Ruby&#xA;23.04%  231555     Gherkin&#xA;3.80%   38178      JavaScript&#xA;1.19%   11943      HTML&#xA;0.79%   7900       Shell&#xA;0.23%   2279       Dockerfile&#xA;0.13%   1344       Earthly&#xA;0.10%   1019       CSS&#xA;0.06%   606        SCSS&#xA;0.02%   234        CoffeeScript&#xA;0.01%   90         Hack&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And here is Jekyll&#39;s published website, from the gh-pages branch inside their repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ github-linguist jekyll --rev origin/gh-pages&#xA;100.00% 2568354    HTML&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;&lt;code&gt;--breakdown&lt;/code&gt;&lt;/h5&gt; &#xA;&lt;p&gt;The &lt;code&gt;--breakdown&lt;/code&gt; or &lt;code&gt;-b&lt;/code&gt; flag will additionally show the breakdown of files by language.&lt;/p&gt; &#xA;&lt;p&gt;You can try running &lt;code&gt;github-linguist&lt;/code&gt; on the root directory in this repository itself:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ github-linguist --breakdown&#xA;66.84%  264519     Ruby&#xA;24.68%  97685      C&#xA;6.57%   25999      Go&#xA;1.29%   5098       Lex&#xA;0.32%   1257       Shell&#xA;0.31%   1212       Dockerfile&#xA;&#xA;Ruby:&#xA;Gemfile&#xA;Rakefile&#xA;bin/git-linguist&#xA;bin/github-linguist&#xA;ext/linguist/extconf.rb&#xA;github-linguist.gemspec&#xA;lib/linguist.rb&#xA;â€¦&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;&lt;code&gt;--json&lt;/code&gt;&lt;/h5&gt; &#xA;&lt;p&gt;The &lt;code&gt;--json&lt;/code&gt; or &lt;code&gt;-j&lt;/code&gt; flag output the data into JSON format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ github-linguist --json&#xA;{&#34;Dockerfile&#34;:{&#34;size&#34;:1212,&#34;percentage&#34;:&#34;0.31&#34;},&#34;Ruby&#34;:{&#34;size&#34;:264519,&#34;percentage&#34;:&#34;66.84&#34;},&#34;C&#34;:{&#34;size&#34;:97685,&#34;percentage&#34;:&#34;24.68&#34;},&#34;Lex&#34;:{&#34;size&#34;:5098,&#34;percentage&#34;:&#34;1.29&#34;},&#34;Shell&#34;:{&#34;size&#34;:1257,&#34;percentage&#34;:&#34;0.32&#34;},&#34;Go&#34;:{&#34;size&#34;:25999,&#34;percentage&#34;:&#34;6.57&#34;}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This option can be used in conjunction with &lt;code&gt;--breakdown&lt;/code&gt; to get a full list of files along with the size and percentage data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ github-linguist --breakdown --json&#xA;{&#34;Dockerfile&#34;:{&#34;size&#34;:1212,&#34;percentage&#34;:&#34;0.31&#34;,&#34;files&#34;:[&#34;Dockerfile&#34;,&#34;tools/grammars/Dockerfile&#34;]},&#34;Ruby&#34;:{&#34;size&#34;:264519,&#34;percentage&#34;:&#34;66.84&#34;,&#34;files&#34;:[&#34;Gemfile&#34;,&#34;Rakefile&#34;,&#34;bin/git-linguist&#34;,&#34;bin/github-linguist&#34;,&#34;ext/linguist/extconf.rb&#34;,&#34;github-linguist.gemspec&#34;,&#34;lib/linguist.rb&#34;,...]}}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Single file&lt;/h4&gt; &#xA;&lt;p&gt;Alternatively you can find stats for a single file using the &lt;code&gt;github-linguist&lt;/code&gt; executable.&lt;/p&gt; &#xA;&lt;p&gt;You can try running &lt;code&gt;github-linguist&lt;/code&gt; on files in this repository itself:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ github-linguist grammars.yml&#xA;grammars.yml: 884 lines (884 sloc)&#xA;  type:      Text&#xA;  mime type: text/x-yaml&#xA;  language:  YAML&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Docker&lt;/h4&gt; &#xA;&lt;p&gt;If you have Docker installed you can build an image and run Linguist within a container:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ docker build -t linguist .&#xA;$ docker run --rm -v $(pwd):$(pwd) -w $(pwd) -t linguist&#xA;66.84%  264519     Ruby&#xA;24.68%  97685      C&#xA;6.57%   25999      Go&#xA;1.29%   5098       Lex&#xA;0.32%   1257       Shell&#xA;0.31%   1212       Dockerfile&#xA;$ docker run --rm -v $(pwd):$(pwd) -w $(pwd) -t linguist github-linguist --breakdown&#xA;66.84%  264519     Ruby&#xA;24.68%  97685      C&#xA;6.57%   25999      Go&#xA;1.29%   5098       Lex&#xA;0.32%   1257       Shell&#xA;0.31%   1212       Dockerfile&#xA;&#xA;Ruby:&#xA;Gemfile&#xA;Rakefile&#xA;bin/git-linguist&#xA;bin/github-linguist&#xA;ext/linguist/extconf.rb&#xA;github-linguist.gemspec&#xA;lib/linguist.rb&#xA;â€¦&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please check out our &lt;a href=&#34;https://raw.githubusercontent.com/github-linguist/linguist/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The language grammars included in this gem are covered by their repositories&#39; respective licenses. &lt;a href=&#34;https://raw.githubusercontent.com/github-linguist/linguist/main/vendor/README.md&#34;&gt;&lt;code&gt;vendor/README.md&lt;/code&gt;&lt;/a&gt; lists the repository for each grammar.&lt;/p&gt; &#xA;&lt;p&gt;All other files are covered by the MIT license, see &lt;a href=&#34;https://raw.githubusercontent.com/github-linguist/linguist/main/LICENSE&#34;&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/ao</title>
    <updated>2024-10-03T01:28:31Z</updated>
    <id>tag:github.com,2024-10-03:/pytorch/ao</id>
    <link href="https://github.com/pytorch/ao" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch native quantization and sparsity for training and inference&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;torchao: PyTorch Architecture Optimization&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/gpumode&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/gpumode?style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#introduction&#34;&gt;Introduction&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#inference&#34;&gt;Inference&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#training&#34;&gt;Training&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#composability&#34;&gt;Composability&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#custom-kernels&#34;&gt;Custom Kernels&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#alpha-features&#34;&gt;Alpha Features&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#installation&#34;&gt;Installation&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#integrations&#34;&gt;Integrations&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#videos&#34;&gt;Videos&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/#license&#34;&gt;License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;torchao: PyTorch library for custom data types &amp;amp; optimizations. Quantize and sparsify weights, gradients, optimizers &amp;amp; activations for inference and training.&lt;/p&gt; &#xA;&lt;p&gt;From the team that brought you the fast series&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;9.5x speedups for Image segmentation models with &lt;a href=&#34;https://pytorch.org/blog/accelerating-generative-ai&#34;&gt;sam-fast&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;10x speedups for Language models with &lt;a href=&#34;https://pytorch.org/blog/accelerating-generative-ai-2&#34;&gt;gpt-fast&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;3x speedup for Diffusion models with &lt;a href=&#34;https://pytorch.org/blog/accelerating-generative-ai-3&#34;&gt;sd-fast&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;torchao just works with &lt;code&gt;torch.compile()&lt;/code&gt; and &lt;code&gt;FSDP2&lt;/code&gt; over most PyTorch models on Huggingface out of the box.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Post Training Quantization&lt;/h3&gt; &#xA;&lt;p&gt;Quantizing and Sparsifying your models is a 1 liner that should work on any model with an &lt;code&gt;nn.Linear&lt;/code&gt; including your favorite HuggingFace model. You can find a more comprehensive usage instructions &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/quantization/&#34;&gt;here&lt;/a&gt;, sparsity &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/_models/sam/README.md&#34;&gt;here&lt;/a&gt; and a HuggingFace inference example &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/scripts/hf_eval.py&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For inference, we have the option of&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Quantize only the weights: works best for memory bound models&lt;/li&gt; &#xA; &lt;li&gt;Quantize the weights and activations: works best for compute bound models&lt;/li&gt; &#xA; &lt;li&gt;Quantize the activations and weights and sparsify the weight&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torchao.quantization.quant_api import (&#xA;    quantize_,&#xA;    int8_dynamic_activation_int8_weight,&#xA;    int4_weight_only,&#xA;    int8_weight_only&#xA;)&#xA;quantize_(m, int4_weight_only())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For gpt-fast &lt;code&gt;int4_weight_only()&lt;/code&gt; is the best option at bs=1 as it &lt;strong&gt;2x the tok/s and reduces the VRAM requirements by about 65%&lt;/strong&gt; over a torch.compiled baseline.&lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t have enough VRAM to quantize your entire model on GPU and you find CPU quantization to be too slow then you can use the device argument like so &lt;code&gt;quantize_(model, int8_weight_only(), device=&#34;cuda&#34;)&lt;/code&gt; which will send and quantize each layer individually to your GPU.&lt;/p&gt; &#xA;&lt;p&gt;If you see slowdowns with any of these techniques or you&#39;re unsure which option to use, consider using &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/quantization/README.md#autoquantization&#34;&gt;autoquant&lt;/a&gt; which will automatically profile layers and pick the best way to quantize each layer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = torchao.autoquant(torch.compile(model, mode=&#39;max-autotune&#39;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide a developer facing API so you can implement your own quantization algorithms so please use the excellent &lt;a href=&#34;https://github.com/pytorch/ao/tree/main/torchao/prototype/hqq&#34;&gt;HQQ&lt;/a&gt; algorithm as a motivating example.&lt;/p&gt; &#xA;&lt;h3&gt;KV Cache Quantization&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ve added kv cache quantization and other features in order to enable long context length (and necessarily memory efficient) inference.&lt;/p&gt; &#xA;&lt;p&gt;In practice these features alongside int4 weight only quantization allow us to &lt;strong&gt;reduce peak memory by ~55%&lt;/strong&gt;, meaning we can Llama3.1-8B inference with a &lt;strong&gt;130k context length with only 18.9 GB of peak memory.&lt;/strong&gt; More details can be found &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/_models/llama/README.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Quantization Aware Training&lt;/h3&gt; &#xA;&lt;p&gt;Post-training quantization can result in a fast and compact model, but may also lead to accuracy degradation. We recommend exploring Quantization Aware Training (QAT) to overcome this limitation. In collaboration with Torchtune, we&#39;ve developed a QAT recipe that demonstrates significant accuracy improvements over traditional PTQ, recovering &lt;strong&gt;96% of the accuracy degradation on hellaswag and 68% of the perplexity degradation on wikitext&lt;/strong&gt; for Llama3 compared to post-training quantization (PTQ). And we&#39;ve provided a full recipe &lt;a href=&#34;https://pytorch.org/blog/quantization-aware-training/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torchao.quantization.prototype.qat import Int8DynActInt4WeightQATQuantizer&#xA;&#xA;qat_quantizer = Int8DynActInt4WeightQATQuantizer()&#xA;&#xA;# Insert &#34;fake quantize&#34; operations into linear layers.&#xA;# These operations simulate quantization numerics&#xA;model = qat_quantizer.prepare(model)&#xA;&#xA;# Run Training...&#xA;&#xA;# Convert fake quantize to actual quantize operations&#xA;model = qat_quantizer.convert(model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Float8&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/float8&#34;&gt;torchao.float8&lt;/a&gt; implements training recipes with the scaled float8 dtypes, as laid out in &lt;a href=&#34;https://arxiv.org/abs/2209.05433&#34;&gt;https://arxiv.org/abs/2209.05433&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;With &lt;code&gt;torch.compile&lt;/code&gt; on, current results show throughput speedups of up to &lt;strong&gt;1.5x on 128 H100 GPU LLaMa 3 70B pretraining jobs&lt;/strong&gt; (&lt;a href=&#34;https://dev-discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359&#34;&gt;details&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torchao.float8 import convert_to_float8_training&#xA;convert_to_float8_training(m, module_filter_fn=...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And for an end-to-minimal training recipe of pretraining with float8, you can check out &lt;a href=&#34;https://github.com/pytorch/torchtitan/raw/main/docs/float8.md&#34;&gt;torchtitan&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Sparse Training&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ve added support for semi-structured 2:4 sparsity with &lt;strong&gt;6% end-to-end speedups on ViT-L&lt;/strong&gt;. Full blog &lt;a href=&#34;https://pytorch.org/blog/accelerating-neural-network-training/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The code change is a 1 liner with the full example available &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/sparsity/training/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;swap_linear_with_semi_sparse_linear(model, {&#34;seq.0&#34;: SemiSparseLinear})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Memory-efficient optimizers&lt;/h3&gt; &#xA;&lt;p&gt;ADAM takes 2x as much memory as the model params so we can quantize the optimizer state to either 8 or 4 bit effectively reducing the optimizer VRAM requirements by 2x or 4x respectively over an fp16 baseline&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torchao.prototype.low_bit_optim import AdamW8bit, AdamW4bit, AdamWFp8&#xA;optim = AdamW8bit(model.parameters()) # replace with Adam4bit and AdamFp8 for the 4 / fp8 versions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In practice, we are a tiny bit slower than expertly written kernels but the implementations for these optimizers were written in a &lt;strong&gt;few hundred lines of PyTorch code&lt;/strong&gt; and compiled so please use them or copy-paste them for your quantized optimizers. Benchmarks &lt;a href=&#34;https://github.com/pytorch/ao/tree/main/torchao/prototype/low_bit_optim&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We also have support for &lt;a href=&#34;https://github.com/pytorch/ao/tree/main/torchao/prototype/low_bit_optim#optimizer-cpu-offload&#34;&gt;single GPU CPU offloading&lt;/a&gt; where both the gradients (same size as weights) and the optimizers will be efficiently sent to the CPU. This alone can &lt;strong&gt;reduce your VRAM requirements by 60%&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;optim = CPUOffloadOptimizer(model.parameters(), torch.optim.AdamW, fused=True)&#xA;optim.load_state_dict(ckpt[&#34;optim&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Composability&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;torch.compile&lt;/code&gt;: A key design principle for us is composability as in any new dtype or layout we provide needs to work with our compiler. It shouldn&#39;t matter if the kernels are written in pure PyTorch, CUDA, C++, or Triton - things should just work! So we write the dtype, layout, or bit packing logic in pure PyTorch and code-generate efficient kernels.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/torchtitan/raw/main/docs/fsdp.md&#34;&gt;FSDP2&lt;/a&gt;: Historically most quantization has been done for inference, there is now a thriving area of research combining distributed algorithms and quantization.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The best example we have combining the composability of lower bit dtype with compile and fsdp is &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/dtypes/nf4tensor.py&#34;&gt;NF4&lt;/a&gt; which we used to implement the &lt;a href=&#34;https://www.youtube.com/watch?v=UvRl4ansfCg&#34;&gt;QLoRA&lt;/a&gt; algorithm. So if you&#39;re doing research at the intersection of this area we&#39;d love to hear from you.&lt;/p&gt; &#xA;&lt;h2&gt;Custom Kernels&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve added support for authoring and releasing &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/csrc/&#34;&gt;custom ops&lt;/a&gt; that do not graph break with &lt;code&gt;torch.compile()&lt;/code&gt; so if you love writing kernels but hate packaging them so they work all operating systems and cuda versions, we&#39;d love to accept contributions for your custom ops. We have a few examples you can follow&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/dtypes/floatx&#34;&gt;fp6&lt;/a&gt; for 2x faster inference over fp16 with an easy to use API &lt;code&gt;quantize_(model, fpx_weight_only(3, 2))&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/ao/pull/733&#34;&gt;2:4 Sparse Marlin GEMM&lt;/a&gt; 2x speedups for FP16xINT4 kernels even at batch sizes up to 256&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/ao/pull/415&#34;&gt;int4 tinygemm unpacker&lt;/a&gt; which makes it easier to switch quantized backends for inference&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you believe there&#39;s other CUDA kernels we should be taking a closer look at please leave a comment on &lt;a href=&#34;https://github.com/pytorch/ao/issues/697&#34;&gt;this issue&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Alpha features&lt;/h2&gt; &#xA;&lt;p&gt;Things we&#39;re excited about but need more time to cook in the oven&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/ao/main/torchao/prototype/mx_formats&#34;&gt;MX&lt;/a&gt; training and inference support with tensors using the &lt;a href=&#34;https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf&#34;&gt;OCP MX spec&lt;/a&gt; data types, which can be described as groupwise scaled float8/float6/float4/int8, with the scales being constrained to powers of two. This work is prototype as the hardware support is not available yet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/ao/tree/main/torchao/prototype/quantized_training&#34;&gt;Int8 Quantized Training&lt;/a&gt;: We&#39;re trying out full int8 training. This is easy to use with &lt;code&gt;quantize_(model, int8_weight_only_quantized_training())&lt;/code&gt;. This work is prototype as the memory benchmarks are not compelling yet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/ao/tree/main/torchao/dtypes/uintx&#34;&gt;IntX&lt;/a&gt;: We&#39;ve managed to support all the ints by doing some clever bitpacking in pure PyTorch and then compiling it. This work is prototype as unfortunately without some more investment in either the compiler or low-bit kernels, int4 is more compelling than any smaller dtype&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/ao/raw/main/torchao/prototype/dtypes/bitnet.py&#34;&gt;Bitnet&lt;/a&gt;: Mostly this is very cool to people on the team. This is prototype because how useful these kernels are is highly dependent on better hardware and kernel support.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;torchao&lt;/code&gt; makes liberal use of several new features in Pytorch, it&#39;s recommended to use it with the current nightly or latest stable version of PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;Stable release from Pypi which will default to CUDA 12.1&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;pip install torchao&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Stable Release from the PyTorch index&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;pip install torchao --extra-index-url https://download.pytorch.org/whl/cu121 # full options are cpu/cu118/cu121/cu124&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Nightly Release&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;pip install --pre torchao --index-url https://download.pytorch.org/whl/nightly/cu121 # full options are cpu/cu118/cu121/cu124&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;em&gt;most&lt;/em&gt; developers you probably want to skip building custom C++/CUDA extensions for faster iteration&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;USE_CPP=0 pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Integrations&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re also fortunate to be integrated into some of the leading open-source libraries including&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Hugging Face transformers with a &lt;a href=&#34;https://huggingface.co/docs/transformers/main/quantization/torchao&#34;&gt;builtin inference backend&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/transformers/pull/31865&#34;&gt;low bit optimizers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face diffusers best practices with torch.compile and torchao &lt;a href=&#34;https://github.com/sayakpaul/diffusers-torchao&#34;&gt;standalone repo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mobius HQQ backend leveraged our int4 kernels to get &lt;a href=&#34;https://github.com/mobiusml/hqq#faster-inference&#34;&gt;195 tok/s on a 4090&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Videos&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=UvRl4ansfCg&#34;&gt;Slaying OOMs at the Mastering LLM&#39;s course&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/1u9xUK3G4VM?si=4JcPlw2w8chPXW8J&#34;&gt;Advanced Quantization at CUDA MODE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/live/v_q2JTIqE20?si=mf7HeZ63rS-uYpS6&#34;&gt;Chip Huyen&#39;s GPU Optimization Workshop&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=lVgrE36ZUw0&#34;&gt;Cohere for AI community talk&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;torchao&lt;/code&gt; is released under the &lt;a href=&#34;https://github.com/pytorch-labs/ao/raw/main/LICENSE&#34;&gt;BSD 3&lt;/a&gt; license.&lt;/p&gt;</summary>
  </entry>
</feed>