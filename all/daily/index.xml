<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-03T01:24:13Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>FuelLabs/fuels-rs</title>
    <updated>2024-03-03T01:24:13Z</updated>
    <id>tag:github.com,2024-03-03:/FuelLabs/fuels-rs</id>
    <link href="https://github.com/FuelLabs/fuels-rs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fuel Network Rust SDK&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;fuels-rs&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/FuelLabs/fuels-rs/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/FuelLabs/fuels-rs/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crates.io/crates/fuels&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/fuels?label=latest&#34; alt=&#34;crates.io&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.rs/fuels&#34;&gt;&lt;img src=&#34;https://docs.rs/fuels/badge.svg?sanitize=true&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/xfpK4Pe&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat%20on-discord-orange?&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=7389D8&amp;amp;labelColor=6A7EC2&#34; alt=&#34;discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Rust SDK for Fuel. It can be used for a variety of things, including but not limited to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Compiling, deploying, and testing &lt;a href=&#34;https://github.com/FuelLabs/sway&#34;&gt;Sway&lt;/a&gt; contracts;&lt;/li&gt; &#xA; &lt;li&gt;Launching a local Fuel network;&lt;/li&gt; &#xA; &lt;li&gt;Crafting and signing transactions with hand-crafted scripts or contract calls;&lt;/li&gt; &#xA; &lt;li&gt;Generating type-safe Rust bindings of contract methods;&lt;/li&gt; &#xA; &lt;li&gt;And more, &lt;code&gt;fuels-rs&lt;/code&gt; is still in active development.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://fuellabs.github.io/fuels-rs/latest/&#34;&gt;the &lt;code&gt;fuels-rs&lt;/code&gt; book&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Launch Fuel nodes&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Deploy contracts&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Interact with deployed contracts&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Type-safe Sway contracts bindings code generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Run Sway scripts&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; CLI for common operations&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Local test wallets&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Wallet integration&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Events querying/monitoring&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;What dependencies do I need?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.fuel.network/guides/installation/#installing-rust&#34;&gt;The latest &lt;code&gt;stable&lt;/code&gt; Rust toolchain&lt;/a&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.fuel.network/guides/installation/#installing-the-fuel-toolchain-using-fuelup&#34;&gt;&lt;code&gt;forc&lt;/code&gt; and &lt;code&gt;fuel-core&lt;/code&gt; binaries&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How can I run the SDK tests?&lt;/h3&gt; &#xA;&lt;p&gt;First, build the test projects using &lt;code&gt;forc&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;forc build --path packages/fuels&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can run the SDK tests with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also run specific tests. The following example will run all integration tests in &lt;code&gt;types.rs&lt;/code&gt; whose names contain &lt;code&gt;in_vector&lt;/code&gt; and show their outputs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo test --test types in_vector -- --show-output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to run WASM tests?&lt;/h3&gt; &#xA;&lt;p&gt;You need to have wasm32 as a target, if you don&#39;t already:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt; rustup target add wasm32-unknown-unknown&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You also need &lt;code&gt;wasm-pack&lt;/code&gt;, if you don&#39;t already:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo install wasm-pack&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Navigate to &lt;code&gt;packages/wasm-tests&lt;/code&gt; and run &lt;code&gt;wasm-pack test&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;What to do if my tests are failing on &lt;code&gt;master&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Before doing anything else, try all these commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo clean&#xA;rm Cargo.lock&#xA;forc build --path packages/fuels&#xA;cargo test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Why is the prefix &lt;code&gt;fuels&lt;/code&gt; and not &lt;code&gt;fuel&lt;/code&gt;?&lt;/h3&gt; &#xA;&lt;p&gt;In order to make the SDK for Fuel feel familiar with those coming from the &lt;a href=&#34;https://github.com/ethers-io/ethers.js&#34;&gt;ethers.js&lt;/a&gt; ecosystem, this project opted for an &lt;code&gt;s&lt;/code&gt; at the end. The &lt;code&gt;fuels-*&lt;/code&gt; family of SDKs is inspired by The Ethers Project.&lt;/p&gt; &#xA;&lt;h3&gt;How can I run the docs locally?&lt;/h3&gt; &#xA;&lt;p&gt;Install &lt;code&gt;mdbook&lt;/code&gt; by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo install mdbook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, navigate to the &lt;code&gt;docs&lt;/code&gt; folder and run the command below to start a local server and open a new tab in you browser.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mdbook serve --open&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can build the book by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mdbook build&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>kyegomez/BitNet</title>
    <updated>2024-03-03T01:24:13Z</updated>
    <id>tag:github.com,2024-03-03:/kyegomez/BitNet</id>
    <link href="https://github.com/kyegomez/BitNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of &#34;BitNet: Scaling 1-bit Transformers for Large Language Models&#34; in pytorch&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://discord.gg/qUtxnK2NMf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kyegomez/BitNet/main/agorabanner.png&#34; alt=&#34;Multi-Modality&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;BitNet&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kyegomez/BitNet/main/bitnet.png&#34; alt=&#34;bitnet&#34;&gt; PyTorch Implementation of the linear methods and model from the paper &#34;BitNet: Scaling 1-bit Transformers for Large Language Models&#34;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.11453.pdf&#34;&gt;Paper link:&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;BitLinear = tensor -&amp;gt; layernorm -&amp;gt; Binarize -&amp;gt; abs max quantization -&amp;gt; dequant&lt;/p&gt; &#xA;&lt;p&gt;&#34;The implementation of the BitNet architecture is quite simple, requiring only the replacement of linear projections (i.e., nn.Linear in PyTorch) in the Transformer. &#34; -- BitNet is really easy to implement just swap out the linears with the BitLinear modules!&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;NEWS&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;BitNet Transformer has been trained using the &lt;code&gt;train.py&lt;/code&gt; file that trains on enwiki8 a small 1gb dataset of wikipedia: &lt;a href=&#34;https://drive.google.com/file/d/1gBuZRFBqMV3cVD902LXA_hmZl4e0dLyY/view&#34;&gt;HERE IS THE LINK&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;New Iteration&lt;/strong&gt; ðŸ”¥ There is an all-new iteration from the paper &#34;&lt;a href=&#34;https://arxiv.org/abs/2402.17764&#34;&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&#34;, we&#39;re implementing it now. Join the Agora discord and contribute! &lt;a href=&#34;https://discord.gg/hFzevCjG8c&#34;&gt;Join Here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;New Optimizations&lt;/strong&gt; The first &lt;code&gt;BitLinear&lt;/code&gt; has been optimized and we now have a Bit Attention &lt;code&gt;BitMGQA&lt;/code&gt; That implements BitLinear into the attention mechanism. Multi Grouped Query Attention is also widely recognized as the best attention for its fast decoding and long context handling, thanks to Frank for his easy to use implementation!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Appreciation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dimitry, Nullonix for analysis and code review and revision&lt;/li&gt; &#xA; &lt;li&gt;Vyom, for providing 4080 to train!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install bitnet&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage:&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;BitLinear&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of the BitLinear layer which is the main innovation of the paper!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;from bitnet import BitLinear&#xA;&#xA;# Input&#xA;x = torch.randn(10, 512)&#xA;&#xA;# BitLinear layer&#xA;layer = BitLinear(512, 400)&#xA;&#xA;# Output&#xA;y = layer(x)&#xA;&#xA;print(y)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;code&gt;BitNetTransformer&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fully implemented Transformer as described in the diagram with MHA, and BitFeedforwards&lt;/li&gt; &#xA; &lt;li&gt;Can be utilized not just for text but for images and maybe even video or audio processing&lt;/li&gt; &#xA; &lt;li&gt;Complete with residuals and skip connections for gradient flow&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import the necessary libraries&#xA;import torch&#xA;from bitnet import BitNetTransformer&#xA;&#xA;# Create a random tensor of integers&#xA;x = torch.randint(0, 20000, (1, 1024))&#xA;&#xA;# Initialize the BitNetTransformer model&#xA;bitnet = BitNetTransformer(&#xA;    num_tokens=20000,  # Number of unique tokens in the input&#xA;    dim=1024,  # Dimension of the input and output embeddings&#xA;    depth=6,  # Number of transformer layers&#xA;    heads=8,  # Number of attention heads&#xA;    ff_mult=4,  # Multiplier for the hidden dimension in the feed-forward network&#xA;)&#xA;&#xA;# Pass the tensor through the transformer model&#xA;logits = bitnet(x)&#xA;&#xA;# Print the shape of the output&#xA;print(logits)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;BitAttention&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This Attention has been modified to use BitLinear instead of the default linear projection. It&#39;s also using Multi-Grouped Query Attention instead of regular multi-head attention for faster decoding and longer context handling.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from bitnet import BitMGQA&#xA;&#xA;# Create a random tensor of shape (1, 10, 512)&#xA;x = torch.randn(1, 10, 512)&#xA;&#xA;# Create an instance of the BitMGQA model with input size 512, 8 attention heads, and 4 layers&#xA;gqa = BitMGQA(512, 8, 4)&#xA;&#xA;# Pass the input tensor through the BitMGQA model and get the output and attention weights&#xA;out, _ = gqa(x, x, x, need_weights=True)&#xA;&#xA;# Print the shapes of the output tensor and attention tensor&#xA;print(out)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;BitFeedForward&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Feedforward as shown in the diagram with BitLinear and a GELU:&lt;/li&gt; &#xA; &lt;li&gt;Linear -&amp;gt; GELU -&amp;gt; Linear&lt;/li&gt; &#xA; &lt;li&gt;You can add dropouts, or layernorms, or other layers for a better ffn&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from bitnet import BitFeedForward&#xA;&#xA;# Create a random input tensor of shape (10, 512)&#xA;x = torch.randn(10, 512)&#xA;&#xA;# Create an instance of the BitFeedForward class with the following parameters:&#xA;# - input_dim: 512&#xA;# - hidden_dim: 512&#xA;# - num_layers: 4&#xA;# - swish: True (use Swish activation function)&#xA;# - post_act_ln: True (apply Layer Normalization after each activation)&#xA;# - dropout: 0.1 (apply dropout with a probability of 0.1)&#xA;ff = BitFeedForward(512, 512, 4, swish=True, post_act_ln=True, dropout=0.1)&#xA;&#xA;# Apply the BitFeedForward network to the input tensor x&#xA;y = ff(x)&#xA;&#xA;# Print the shape of the output tensor y&#xA;print(y)  # torch.Size([10, 512])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bitnet import BitNetInference&#xA;&#xA;bitnet = BitNetInference()&#xA;bitnet.load_model(&#34;../model_checkpoint.pth&#34;)  # Download model&#xA;output_str = bitnet.generate(&#34;The dog jumped over the &#34;, 512)&#xA;print(output_str)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Huggingface Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoModelForSequenceClassification, AutoTokenizer&#xA;&#xA;from bitnet import replace_linears_in_hf&#xA;&#xA;# Load a model from Hugging Face&#39;s Transformers&#xA;model_name = &#34;bert-base-uncased&#34;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;model = AutoModelForSequenceClassification.from_pretrained(model_name)&#xA;&#xA;# Replace Linear layers with BitLinear&#xA;replace_linears_in_hf(model)&#xA;&#xA;# Example text to classify&#xA;text = &#34;Replace this with your text&#34;&#xA;inputs = tokenizer(&#xA;    text, return_tensors=&#34;pt&#34;, padding=True, truncation=True, max_length=512&#xA;)&#xA;&#xA;# Perform inference&#xA;model.eval()  # Set the model to evaluation mode&#xA;with torch.no_grad():&#xA;    outputs = model(**inputs)&#xA;    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)&#xA;    print(predictions)&#xA;&#xA;# Process predictions&#xA;predicted_class_id = predictions.argmax().item()&#xA;print(f&#34;Predicted class ID: {predicted_class_id}&#34;)&#xA;&#xA;# Optionally, map the predicted class ID to a label, if you know the classification labels&#xA;# labels = [&#34;Label 1&#34;, &#34;Label 2&#34;, ...]  # Define your labels corresponding to the model&#39;s classes&#xA;# print(f&#34;Predicted label: {labels[predicted_class_id]}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{2310.11453,&#xA;Author = {Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei},&#xA;Title = {BitNet: Scaling 1-bit Transformers for Large Language Models},&#xA;Year = {2023},&#xA;Eprint = {arXiv:2310.11453},&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Todo&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Double check BitLinear implementation and make sure it works exactly as in paper&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Implement training script for &lt;code&gt;BitNetTransformer&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Train on Enwiki8, copy and past code and data from Lucidrains repos&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Benchmark performance&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Look into Straight Through Estimator for non-differentiable backprop&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Implement BitFeedForward&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Clean up codebase&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add unit tests for each module&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement the new BitNet1.5b from the &lt;a href=&#34;https://arxiv.org/abs/2402.17764&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement the BitNet15b in Cuda&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>memorysafety/river</title>
    <updated>2024-03-03T01:24:13Z</updated>
    <id>tag:github.com,2024-03-03:/memorysafety/river</id>
    <link href="https://github.com/memorysafety/river" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository is the future home of the River reverse proxy application, based on the pingora library from Cloudflare.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;River&lt;/h1&gt; &#xA;&lt;p&gt;This repository is the future home of the &lt;code&gt;river&lt;/code&gt; Reverse Proxy Application, based on the &lt;code&gt;pingora&lt;/code&gt; library from Cloudflare.&lt;/p&gt; &#xA;&lt;p&gt;This repository currently only contains design and requirements documents.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0: (&lt;a href=&#34;https://raw.githubusercontent.com/memorysafety/river/main/LICENSE-APACHE&#34;&gt;LICENSE-APACHE&lt;/a&gt; or &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Contribution&lt;/h3&gt; &#xA;&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;</summary>
  </entry>
</feed>