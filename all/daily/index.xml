<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-22T01:26:44Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>danieldonda/Cybersecurity101</title>
    <updated>2023-10-22T01:26:44Z</updated>
    <id>tag:github.com,2023-10-22:/danieldonda/Cybersecurity101</id>
    <link href="https://github.com/danieldonda/Cybersecurity101" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Um guia abrangente para iniciantes na √°rea de ciberseguran√ßa.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üì£ &lt;strong&gt;COMECE POR AQUI!&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Um guia abrangente para iniciantes na √°rea de ciberseguran√ßa.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/danieldonda/Cybersecurity101/assets/16530643/33aa3883-9e3c-482c-b821-df63ce22ec1b&#34; width=&#34;300&#34;&gt; &#xA;&lt;h2&gt;üìñ Conte√∫do&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#trilha-de-estudo&#34;&gt;Trilha de Estudo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#-guia-de-certifica%C3%A7%C3%B5es&#34;&gt;Guia de Certifica√ß√µes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#-fundamentos-de-ciberseguran%C3%A7a&#34;&gt;Fundamentos de Ciberseguran√ßa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#soft-skills&#34;&gt;Soft skills&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#-livros-recomendados-para-iniciar&#34;&gt;Livros recomendados&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/danieldonda/1stStepCyberSec/raw/main/README.md#-eventos-no-brasil&#34;&gt;Eventos no Brasil&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#laborat%C3%B3rios-virtuais&#34;&gt;Laborat√≥rios Virtuais&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#%EF%B8%8Fdesafios-ctf-capture-the-flag&#34;&gt;Desafios CTF (Capture The Flag)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#-sites-e-blogs-recomendados&#34;&gt;Sites e Blogs Recomendados:&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#-podcasts-sobre-ciberseguran%C3%A7a&#34;&gt;Podcasts sobre Ciberseguran√ßa:&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#-ferramentas-essenciais&#34;&gt;Ferramentas Essenciais:&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#-reddit-subreddits-para-discuss%C3%B5es&#34;&gt;Reddit: Subreddits para discuss√µes.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#-youtube-canais-sobre-ciberseguran%C3%A7a-brasileiros&#34;&gt;Youtube: Canais sobre Ciberseguran√ßa Brasileiros.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#-youtube-usa&#34;&gt;Youtube: USA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#awesome-github--reposit%C3%B3rios-para-ciberseguran%C3%A7a&#34;&gt;Awesome Github ‚Äì Reposit√≥rios para ciberseguran√ßa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danieldonda/Cybersecurity101/main/#-frameworks-e-melhores-pr%C3%A1ticas&#34;&gt;Frameworks e Melhores Pr√°ticas&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üìöTrilha de Estudo&lt;/h2&gt; &#xA;&lt;h3&gt;Eu devo fazer Faculdade ou uma certifica√ß√£o para come√ßar a trabalhar em Ciberseguran√ßa ü§î ?&lt;/h3&gt; &#xA;&lt;p&gt;O mercado de trabalho em ciberseguran√ßa √© semelhante aos outros: quanto mais qualifica√ß√µes voc√™ tiver, melhor. Afinal, o mercado est√° se tornando competitivo, e voc√™ deve sempre se destacar para conquistar uma oportunidade.&lt;/p&gt; &#xA;&lt;h3&gt;üéìFaculdade Pr√≥s e Contras&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pr√≥s:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Base S√≥lida&lt;/strong&gt;: Forma√ß√£o te√≥rica e pr√°tica abrangente sobre fundamentos de TI e ciberseguran√ßa.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ampla Cobertura&lt;/strong&gt;: Abordagem de diversos t√≥picos para vis√£o hol√≠stica da √°rea.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Networking&lt;/strong&gt;: Oportunidades para conectar-se com colegas, professores e profissionais.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reconhecimento&lt;/strong&gt;: Muitas vezes, √© um requisito ou diferencial para certos cargos.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contras:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tempo e Custo&lt;/strong&gt;: Geralmente leva anos e pode ser caro.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conte√∫do Desatualizado&lt;/strong&gt;: A r√°pida evolu√ß√£o da ciberseguran√ßa pode superar alguns curr√≠culos.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üíº Certifica√ß√µes Pr√≥s e Contras&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pr√≥s:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Especializa√ß√£o&lt;/strong&gt;: Foco em habilidades e t√≥picos espec√≠ficos.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Atualiza√ß√£o&lt;/strong&gt;: Reflete tend√™ncias e tecnologias atuais.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reconhecimento da Ind√∫stria&lt;/strong&gt;: Valorizado por empregadores.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexibilidade&lt;/strong&gt;: Pode ser obtido mais rapidamente e com menor custo.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contras:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Falta de Base Ampla&lt;/strong&gt;: Pode n√£o fornecer uma base t√£o ampla quanto um diploma universit√°rio.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Validade&lt;/strong&gt;: Algumas certifica√ß√µes precisam ser renovadas.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Lembrando, o mais importante √© a curiosidade e a vontade de aprender. A ciberseguran√ßa √© um campo vasto, e sempre haver√° algo novo a ser descoberto. Comece com o b√°sico, construa uma base s√≥lida e avance gradualmente para t√≥picos mais complexos.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üìù Guia de Certifica√ß√µes&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paul Jerimy&lt;/strong&gt; compilou &lt;strong&gt;436 certifica√ß√µes&lt;/strong&gt; (abril de 2022) e criou um &lt;strong&gt;&lt;a href=&#34;https://pauljerimy.com/security-certification-roadmap/&#34;&gt;roadmap&lt;/a&gt;&lt;/strong&gt; de certifica√ß√£o de seguran√ßa para iniciantes, intermedi√°rios e experts. &lt;img src=&#34;https://danieldonda.com/wp-content/uploads/2022/06/image-9-1024x505.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;üö® Acesse &lt;em&gt;&lt;a href=&#34;https://danieldonda.com/roteiro-de-certificacao-de-seguranca/&#34;&gt;Roteiro de certifica√ß√£o de seguran√ßa&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;üîí Fundamentos de Ciberseguran√ßa&lt;/h3&gt; &#xA;&lt;p&gt;A trilha de aprendizado &lt;strong&gt;pode variar com base nos objetivos individuais&lt;/strong&gt; e nas especialidades desejadas, mas aqui est√£o os fundamentos que, em geral, todos os profissionais de ciberseguran√ßa devem considerar.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kit B√°sico necess√°rio de conhecimento&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Windows / Windows Server + Active Directory&lt;/li&gt; &#xA;   &lt;li&gt;Redes de computadores / TCP/IP / Portas e protocolos&lt;/li&gt; &#xA;   &lt;li&gt;Linux&lt;/li&gt; &#xA;   &lt;li&gt;B√°sico de programa√ß√£o&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Princ√≠pios B√°sicos de Tecnologia da Informa√ß√£o&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Entendimento de &lt;strong&gt;sistemas operacionais&lt;/strong&gt; (Windows, Linux, macOS). No√ß√µes b√°sicas sobre &lt;strong&gt;redes&lt;/strong&gt; (TCP/IP, sub-redes, VLANs, roteadores, switches). Familiaridade com &lt;strong&gt;bancos de dados&lt;/strong&gt; e armazenamento.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conceitos de Seguran√ßa&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Compreens√£o do tri√¢ngulo CIA: &lt;strong&gt;Confidencialidade&lt;/strong&gt;, &lt;strong&gt;Integridade&lt;/strong&gt; e &lt;strong&gt;Disponibilidade&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Conhecimento sobre tipos de amea√ßas: malware, phishing, ataques man-in-the-middle, etc.&lt;/li&gt; &#xA; &lt;li&gt;Familiaridade com a diferen√ßa entre vulnerabilidades, amea√ßas e riscos.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Criptografia&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Entender os conceitos b√°sicos de criptografia sim√©trica e assim√©trica.&lt;/li&gt; &#xA; &lt;li&gt;Familiaridade com protocolos de criptografia comuns e uso de certificados.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Redes e Monitoramento&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Familiaridade com ferramentas como Wireshark e Nmap.&lt;/li&gt; &#xA; &lt;li&gt;Compreens√£o de firewalls, IDS/IPS, e VPNs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Seguran√ßa F√≠sica&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A import√¢ncia de controlar o acesso f√≠sico a √°reas e dispositivos cr√≠ticos.&lt;/li&gt; &#xA; &lt;li&gt;Prote√ß√£o contra amea√ßas f√≠sicas, como tailgating e dumpster diving.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gest√£o de Identidade e Acesso&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Autentica√ß√£o, autoriza√ß√£o e contabilidade (AAA).&lt;/li&gt; &#xA; &lt;li&gt;Controle de acesso baseado em fun√ß√£o (RBAC) e controle de acesso discricion√°rio (DAC).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Resposta a Incidentes&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Etapas b√°sicas para responder a um incidente de seguran√ßa.&lt;/li&gt; &#xA; &lt;li&gt;Constru√ß√£o e manuten√ß√£o de um plano de resposta a incidentes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Legisla√ß√£o e √âtica&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Leis e regulamentos relacionados √† ciberseguran√ßa (por exemplo, GDPR, LGPD, CCPA).&lt;/li&gt; &#xA; &lt;li&gt;C√≥digo de √©tica em ciberseguran√ßa e hacking √©tico.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Desenvolvimento Seguro&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pr√°ticas b√°sicas de codifica√ß√£o segura.&lt;/li&gt; &#xA; &lt;li&gt;Compreens√£o das vulnerabilidades comuns de desenvolvimento, como as listadas no OWASP Top 10.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cloud e Virtualiza√ß√£o&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Seguran√ßa em ambientes de nuvem (IaaS, PaaS, SaaS).&lt;/li&gt; &#xA; &lt;li&gt;Conceitos b√°sicos de virtualiza√ß√£o e seus desafios de seguran√ßa.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üì£Soft skills&lt;/h2&gt; &#xA;&lt;p&gt;Segundo a &lt;a href=&#34;https://br.psicologia-online.com/autor/sara-sanchis-24.html&#34;&gt;Sara Sanchis&lt;/a&gt;, Psic√≥loga especializada em Crescimento Pessoal, Hard skills s√£o habilidades que &lt;strong&gt;podem ser medidas&lt;/strong&gt;, como saber programar, operar um sistema operacional, fazer pentest. J√° as soft skills s√£o &lt;strong&gt;tra√ßos&lt;/strong&gt; que fazem de voc√™ um &lt;strong&gt;bom profissional&lt;/strong&gt;, como etiqueta, comunica√ß√£o etc.&lt;/p&gt; &#xA;&lt;p&gt;Pensando em ciberseguran√ßa e nas empresas no qual eu trabalhei, notei que os soft skill mais requeridos eram:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Resolu√ß√£o de problemas&lt;/li&gt; &#xA; &lt;li&gt;Comunica√ß√£o&lt;/li&gt; &#xA; &lt;li&gt;Capacidade de organiza√ß√£o&lt;/li&gt; &#xA; &lt;li&gt;Pensamento cr√≠tico&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Leia mais sobre o tema em &lt;a href=&#34;https://br.psicologia-online.com/soft-skills-o-que-sao-quais-sao-e-exemplos-440.html&#34;&gt;Psicologia-Online&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üìö Livros recomendados para iniciar&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://amzn.to/48WyP0N&#34;&gt;An√°lise de Tr√°fego em Redes TCP/IP: Utilize Tcpdump na An√°lise de Tr√°fegos em Qualquer Sistema Operacional &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://amzn.to/48W5JP8&#34;&gt;Testes de Invas√£o: uma Introdu√ß√£o Pr√°tica ao Hacking &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://amzn.to/400wIow&#34;&gt;T√©cnicas de Invas√£o: Aprenda as t√©cnicas usadas por hackers em invas√µes reais&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://amzn.to/3Q06VbG&#34;&gt;Black Hat Python: Programa√ß√£o Python Para Hackers e Pentesters&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìö Livros para pesquisa e entendimento&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://amzn.to/46Mhkym&#34;&gt;Redes de Computadores&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://amzn.to/3rYOmwq&#34;&gt;Per√≠cia forense digital: Guia pr√°tico com uso do sistema operacional Windows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://amzn.to/48SXYtn&#34;&gt;Programa√ß√£o Shell Linux: Refer√™ncia Definitiva da Linguagem Shell&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìö Livros de cybersecurity em ingl√™s&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://amzn.to/3rPELYT&#34;&gt;Linux Basics for Hackers: Getting Started with Networking, Scripting, and Security in Kali&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üó∫ Eventos no Brasil&lt;/h2&gt; &#xA;&lt;p&gt;Melhor maneira de aprender e fazer um excelente network que pode gerar √≥timas oportunidades.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ysts.org/&#34;&gt;You Shot The Sheriff&lt;/a&gt; (S√£o Paulo)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mindthesec.com.br/&#34;&gt;Mind the Sec&lt;/a&gt; (S√£o Paulo e Rio de Janeiro)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://roadsec.com.br/&#34;&gt;RoadSec&lt;/a&gt; (V√°rios estados do Brasil)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cnasi.com.br/home-df&#34;&gt;CNASI&lt;/a&gt; (V√°rios estados do Brasil)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nullbyte-con.org/&#34;&gt;Nullbyte&lt;/a&gt; (Salvador)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bhack.com.br/&#34;&gt;BHACK&lt;/a&gt; (Belo Horizonte)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://sp13.securitybsides.com.br/&#34;&gt;BSidesSP&lt;/a&gt; (S√£o Paulo)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.h2hc.com.br/h2hc/pt/&#34;&gt;H2HC&lt;/a&gt; (S√£o Paulo)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.securityleaders.com.br/&#34;&gt;Congresso Security Leaders&lt;/a&gt; (V√°rios estados do Brasil)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://sacicon.com.br/&#34;&gt;SACICON&lt;/a&gt; (S√£o Paulo)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cybersecuritybrazil.com.br/&#34;&gt;Cyber Security Brazil&lt;/a&gt; (S√£o Paulo)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cybersecuritysummit.com.br/&#34;&gt;Cyber Security Summit&lt;/a&gt; (S√£o Paulo)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.sbc.org.br/2-uncategorised/1816-sbseg-simposio-brasileiro-de-seguranca-da-informacao-e-sistemas-computacionais&#34;&gt;SBSeg&lt;/a&gt; (Bras√≠lia)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hackbahia.github.io/&#34;&gt;HACKBAHIA&lt;/a&gt; (Bahia)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://caos.ufrj.br/&#34;&gt;CAOS&lt;/a&gt; (Rio de Janeiro)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://workshop.seginfo.com.br/&#34;&gt;SegInfo&lt;/a&gt; (S√£o Paulo e Rio de Janeiro)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://hackerday.com.br/&#34;&gt;Hacker Day&lt;/a&gt; (Paran√°)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.bwcon.com.br/&#34;&gt;BWCON&lt;/a&gt; (Recife)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.gartner.com/events/pt/la/security&#34;&gt;Gartner Security &amp;amp; Risk Management Summit&lt;/a&gt; (S√£o Paulo)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üíªLaborat√≥rios Virtuais&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Laborat√≥rios Virtuais&lt;/strong&gt;: S√£o ambientes controlados onde se pode praticar e testar habilidades de ciberseguran√ßa.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pentesterlab.com/&#34;&gt;PentesterLab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.vulnhub.com/&#34;&gt;VulnHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.hackthebox.eu/&#34;&gt;Hack The Box&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üè¥‚Äç‚ò†Ô∏èDesafios CTF (Capture The Flag)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://picoctf.org/&#34;&gt;picoCTF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.hackthebox.eu/&#34;&gt;Hack The Box&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://overthewire.org/wargames/&#34;&gt;OverTheWire&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.root-me.org/&#34;&gt;Root Me&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ctftime.org/&#34;&gt;CTFtime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.defcon.org/&#34;&gt;DEF CON CTF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://capturetheflag.withgoogle.com/&#34;&gt;Google CTF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://csaw.engineering.nyu.edu/&#34;&gt;CSAW CTF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ringzer0ctf.com/&#34;&gt;RingZer0 CTF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ctf.hacker101.com/&#34;&gt;Hacker101 CTF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîç Sites e Blogs Recomendados:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://danieldonda.com&#34;&gt;Daniel Donda&lt;/a&gt; Site onde eu compartilho artigos e recursos de ciberseguran√ßa.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://garoa.net.br/wiki/P%C3%A1gina_principal&#34;&gt;Garoa Hacker Clube&lt;/a&gt;: Coletivo hacker paulistano com eventos e workshops frequentes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://seginfo.com.br/&#34;&gt;SegInfo&lt;/a&gt; Blog com not√≠cias, artigos e informa√ß√µes sobre seguran√ßa da informa√ß√£o.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://segurancalegal.com/&#34;&gt;Seguran√ßa Legal&lt;/a&gt; Blog focado em ciberseguran√ßa e aspectos legais da tecnologia. -&lt;a href=&#34;https://mindthesec.com.br&#34;&gt;Mind The Sec&lt;/a&gt; Confer√™ncia de seguran√ßa da informa√ß√£o que tamb√©m conta com um blog com not√≠cias e an√°lises.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Em ingl√™s:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://krebsonsecurity.com/&#34;&gt;Krebs on Security&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://thehackernews.com/&#34;&gt;The Hacker News&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.darkreading.com/&#34;&gt;Dark Reading&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://threatpost.com/&#34;&gt;Threatpost&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.infosecurity-magazine.com/&#34;&gt;Infosecurity Magazine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zdnet.com/topic/security/&#34;&gt;ZDNet Security&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cyberscoop.com/&#34;&gt;CyberScoop&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.securityweek.com/&#34;&gt;SecurityWeek&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.scmagazine.com/&#34;&gt;SC Magazine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wired.com/category/security/&#34;&gt;Wired - Security Section&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üéß Podcasts sobre Ciberseguran√ßa:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Darknet Diaries&lt;/strong&gt; Hist√≥rias reais de ataques cibern√©ticos.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CyberWire&lt;/strong&gt; Resumos di√°rios sobre not√≠cias.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Smashing Security&lt;/strong&gt; Not√≠cias com um toque humor√≠stico.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cyber Morning Call&lt;/strong&gt; Produzido pela Tempest com epis√≥dios di√°rios&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîê Ferramentas Essenciais:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Nmap&lt;/strong&gt;: Ferramenta de escaneamento de portas e descoberta de rede e scan de vuln.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wireshark&lt;/strong&gt;: Analisador de protocolo de rede.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Metasploit&lt;/strong&gt;: Framework de teste de penetra√ß√£o.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Burp Suite&lt;/strong&gt;: Ferramenta de teste de seguran√ßa para aplica√ß√µes web.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Aircrack-ng&lt;/strong&gt;: Conjunto de ferramentas para auditoria de redes sem fio.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;John the Ripper&lt;/strong&gt;: Cracker de senha.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hydra&lt;/strong&gt;: Ferramenta de for√ßa bruta para autentica√ß√£o.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Nessus&lt;/strong&gt;: Scanner de vulnerabilidades.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenVAS&lt;/strong&gt;: Sistema de gerenciamento de vulnerabilidades.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Snort&lt;/strong&gt;: Sistema de detec√ß√£o/preven√ß√£o de intrus√µes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Suricata&lt;/strong&gt;: Motor de detec√ß√£o de amea√ßas de alto desempenho.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tails&lt;/strong&gt;: Sistema operacional focado em privacidade.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TrueCrypt/VeraCrypt&lt;/strong&gt;: Software de criptografia de disco.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kali Linux&lt;/strong&gt;: Distribui√ß√£o Linux para teste de penetra√ß√£o.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parrot Security OS&lt;/strong&gt;: Distribui√ß√£o Linux focada em seguran√ßa e forense digital.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ghidra&lt;/strong&gt;: Desmontador e decompilador.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sqlmap&lt;/strong&gt;: Ferramenta de detec√ß√£o e explora√ß√£o de inje√ß√£o SQL.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OWASP ZAP&lt;/strong&gt;: Ferramenta de teste de penetra√ß√£o para aplica√ß√µes web.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mimikatz&lt;/strong&gt;: Ferramenta de extra√ß√£o de credenciais.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cuckoo Sandbox&lt;/strong&gt;: An√°lise automatizada de malware.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Yara&lt;/strong&gt;: Ferramenta para identificar e classificar malware.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Shodan&lt;/strong&gt;: Motor de busca para dispositivos conectados √† Internet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Binary Ninja&lt;/strong&gt;: Plataforma de an√°lise de c√≥digo bin√°rio.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Radare2&lt;/strong&gt;: Framework de engenharia reversa.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Steganography Toolkit&lt;/strong&gt;: Conjunto de ferramentas para esteganografia.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hashcat&lt;/strong&gt;: Ferramenta de cracking de senha.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Empire&lt;/strong&gt;: Framework p√≥s-explora√ß√£o.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Responder&lt;/strong&gt;: Ferramenta para an√°lise de protocolo NetBIOS/LLMNR.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cobalt Strike&lt;/strong&gt;: Ferramenta de avalia√ß√£o de amea√ßas.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recon-ng&lt;/strong&gt;: Framework de reconhecimento da web.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SIFT&lt;/strong&gt;: Kit de ferramentas forenses.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Volatility&lt;/strong&gt;: An√°lise de mem√≥ria forense.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ &lt;strong&gt;Reddit&lt;/strong&gt;: Subreddits para discuss√µes.&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;r/netsec&lt;/strong&gt; - Este subreddit √© dedicado a discuss√µes de alto n√≠vel sobre seguran√ßa da informa√ß√£o.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;r/AskNetsec&lt;/strong&gt; - Um lugar para perguntar a profissionais de seguran√ßa quest√µes relacionadas ao campo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;r/cybersecurity&lt;/strong&gt; - Discuss√£o geral sobre ciberseguran√ßa.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;r/ReverseEngineering&lt;/strong&gt; - Um subreddit para os interessados em reverse engineering.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;r/malware&lt;/strong&gt; - Discuss√µes sobre malwares e artefatos relacionados.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;r/pentest&lt;/strong&gt; - Focado em testes de penetra√ß√£o e ferramentas/t√©cnicas relacionadas.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;r/Hacking_Tutorials&lt;/strong&gt; - Focado em tutoriais hackers&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;r/hacking&lt;/strong&gt; - Um lugar para hackers e aspirantes a hackers aprenderem e compartilharem.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;r/ComputerForensics&lt;/strong&gt; - Discuss√£o sobre forense digital e t√≥picos relacionados.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;r/HowToHack&lt;/strong&gt; - Comunidade Hacker.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;r/CompTIA&lt;/strong&gt; - Comunidade sobre certifica√ß√µes da CompTIA&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì∫ &lt;strong&gt;Youtube&lt;/strong&gt;: Canais sobre Ciberseguran√ßa Brasileiros.&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/PapoBin%C3%A1rio&#34;&gt;Papo Bin√°rio&lt;/a&gt; ‚ÄìEntrevistas, tutoriais e dicas voltada a TI e ciberseguran√ßa&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@RicardoLongatto&#34;&gt;Ricardo Longatto&lt;/a&gt; ‚Äì Seguran√ßa da informa√ß√£o, pentest, t√©cnicas de invas√£o&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/adsecf&#34;&gt;Guia An√¥nima&lt;/a&gt; ‚Äì Metodologias, t√©cnicas, entrevistas e tutoriais&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@FabioSobiecki&#34;&gt;Fabio Sobiecki&lt;/a&gt; ‚Äì Ajuda a profissionais a conquistarem sua primeira vaga em Seguran√ßa da Informa√ß√£o.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCpIYXOF825aX8qq77xnTCLQ&#34;&gt;prog.shell.linux&lt;/a&gt; ‚Äì Curso de Shell Linux com o Prof. Julio Cezar Neves&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/kretcheu2001&#34;&gt;Paulo Kretcheu&lt;/a&gt; ‚Äì Curso de Redes, Curso GNU/Linux ‚Äì Tutoriais ‚Äì Solu√ß√£o de problemas&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/securitycast&#34;&gt;SecurityCast&lt;/a&gt; ‚Äì Programas quinzenais sobre Seguran√ßa da Informa√ß√£o&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@videos100security/about&#34;&gt;100SECURITY&lt;/a&gt; ‚Äì Profissionais de Seguran√ßa da Informa√ß√£o&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@GabrielPato/&#34;&gt;Gabriel Pato&lt;/a&gt; ‚Äì Tecnologia e Hacking&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/@danieldonda&#34;&gt;Daniel Donda&lt;/a&gt; ‚Äì O melhor canal do youtube&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/AcademiadeForenseDigital&#34;&gt;Academia de Forense Digital&lt;/a&gt; ‚Äì Ensino e desenvolvimento de Forense Digital no Brasil&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì∫ &lt;strong&gt;Youtube&lt;/strong&gt;: USA&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/CyberSecurityHub&#34;&gt;Cyber Security Hub&lt;/a&gt; - Oferece not√≠cias, entrevistas e an√°lises sobre as mais recentes amea√ßas e solu√ß√µes de seguran√ßa.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/irongeek&#34;&gt;Adrian Crenshaw (IronGeek)&lt;/a&gt; - Compila e compartilha v√°rias palestras e tutoriais de confer√™ncias de seguran√ßa.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/WebDevSimplified&#34;&gt;WebDevSimplified&lt;/a&gt; - Embora seja mais voltado para o desenvolvimento web, frequentemente aborda t√≥picos de seguran√ßa web, como Cross-Site Scripting (XSS) e Cross-Site Request Forgery (CSRF).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/NetworkChuck&#34;&gt;NetworkChuck&lt;/a&gt; - Enquanto este canal aborda uma ampla gama de t√≥picos de TI, frequentemente explora aspectos de ciberseguran√ßa, especialmente relacionados a redes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC3s0BtrBJpwNDaflRSoiieQ&#34;&gt;Cyber Weapons Lab&lt;/a&gt; - Uma s√©rie que explora hacking √©tico, ferramentas de seguran√ßa e t√©cnicas.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/professormesser&#34;&gt;Professor Messer&lt;/a&gt; - Conhecido por seus cursos de certifica√ß√£o, incluindo CompTIA Security+.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/MalwareTech&#34;&gt;MalwareTech&lt;/a&gt; - Como o nome sugere, este canal se concentra principalmente na an√°lise de malware e no estudo de botnets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCJUFUScDG1t0Z_5mndVGfAg&#34;&gt;Jenny Radcliffe - The People Hacker&lt;/a&gt; - Jenny √© uma especialista em engenharia social e frequentemente compartilha insights sobre o lado humano da ciberseguran√ßa.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/CyberSecLabs&#34;&gt;CyberSecLabs&lt;/a&gt; - Oferece tutoriais pr√°ticos e walkthroughs de boxes de pentesting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/Detectify&#34;&gt;DETECTIFY LABS&lt;/a&gt; - Detectify √© uma empresa de seguran√ßa que frequentemente compartilha descobertas, vulnerabilidades e dicas sobre seguran√ßa web.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/PwnFunction&#34;&gt;PwnFunction&lt;/a&gt; - Este canal explora vulnerabilidades, com uma √™nfase particular em JavaScript e seguran√ßa web.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/Hak5Darren&#34;&gt;Hak5&lt;/a&gt; - Um dos canais mais antigos sobre hacking e seguran√ßa cibern√©tica, apresentando uma variedade de conte√∫dos, desde ferramentas a tutoriais.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/LiveOverflowCTF&#34;&gt;LiveOverflow&lt;/a&gt; - Oferece uma s√©rie de v√≠deos que se aprofundam em diversos t√≥picos de seguran√ßa, especialmente relacionados a CTFs (Capture The Flag) e vulnerabilidades.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/ToolsWatch&#34;&gt;Black Hat&lt;/a&gt; - O canal oficial do Black Hat, uma das mais renomadas confer√™ncias de seguran√ßa. Cont√©m palestras e apresenta√ß√µes dos eventos.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/DEFCONConference&#34;&gt;DefconConference&lt;/a&gt; - Semelhante ao Black Hat, √© o canal oficial da DEFCON, uma das maiores e mais antigas confer√™ncias de hackers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/TheCyberMentor&#34;&gt;The Cyber Mentor&lt;/a&gt; - Oferece tutoriais pr√°ticos e cursos sobre ethical hacking e seguran√ßa cibern√©tica.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/NullByteWht&#34;&gt;Null Byte&lt;/a&gt; - Foca em tutoriais sobre hacking √©tico e ferramentas populares, ideal para iniciantes e intermedi√°rios.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/stokfredrik&#34;&gt;ST√ñK&lt;/a&gt; - Oferece insights sobre a vida de um hacker √©tico, com dicas e truques sobre pentesting e seguran√ßa cibern√©tica.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/JohnHammond010&#34;&gt;John Hammond&lt;/a&gt; - Oferece tutoriais, desafios de CTF e dicas sobre seguran√ßa cibern√©tica e ethical hacking.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üíéAwesome Github ‚Äì Reposit√≥rios para ciberseguran√ßa&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Hack-with-Github/Awesome-Hacking&#34;&gt;Awesome Hacking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vitalysim/Awesome-Hacking-Resources&#34;&gt;Awesome Hacking Resources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sbilly/awesome-security&#34;&gt;Awesome Security&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jivoi/awesome-osint&#34;&gt;Awesome OSINT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/enaqx/awesome-pentest&#34;&gt;Awesome Penetration Testing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gwen001/pentest-tools&#34;&gt;Awesome Pentest Tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ivbeg/awesome-forensicstools&#34;&gt;Awesome Forensic Tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/djadmin/awesome-bug-bounty&#34;&gt;Awesome Bug Bounty&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîê Frameworks e Melhores Pr√°ticas&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://attack.mitre.org/&#34;&gt;Framework MITRE Adversarial Tactics Techniques e Common Knowledge¬Æ (MITRE ATT&amp;amp;CK¬Æ)&lt;/a&gt; ‚Äì A estrutura MITRE ATT&amp;amp;CK √© uma base de conhecimento com curadoria e um modelo para o comportamento do advers√°rio cibern√©tico, refletindo as v√°rias fases do ciclo de vida do ataque de um advers√°rio e as plataformas que eles visam.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nist.gov/&#34;&gt;National Institute of Standards and Technology¬Æ (NIST¬Æ)&lt;/a&gt; ‚Äì O &lt;strong&gt;National Institute of Standards and Technology,&lt;/strong&gt; anteriormente conhecido como The National Bureau of Standards, √© uma ag√™ncia governamental n√£o regulat√≥ria da administra√ß√£o de tecnologia do Departamento de Com√©rcio dos Estados Unidos. &lt;strong&gt;Possui um excelente framework de ciberseguran√ßa.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloudsecurityalliance.org/&#34;&gt;Cloud Security Alliance (CSA)&lt;/a&gt; ‚Äì A &lt;strong&gt;Cloud Security Alliance&lt;/strong&gt; √© uma organiza√ß√£o sem fins lucrativos com a miss√£o de ‚Äúpromover o uso das &lt;strong&gt;melhores pr√°ticas para fornecer garantia de seguran√ßa na computa√ß√£o em nuvem&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://safecode.org/&#34;&gt;Software Assurance Forum for Excellence in Code (SAFECode)&lt;/a&gt; ‚Äì O SAFECode √© um esfor√ßo global liderado pela ind√∫stria para identificar e promover as &lt;strong&gt;melhores pr√°ticas para desenvolver e fornecer software, hardware e servi√ßos mais seguros e confi√°veis.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://owasp.org/&#34;&gt;Open Web Application Security Project¬Æ (OWASP¬Æ)&lt;/a&gt; ‚Äì O &lt;strong&gt;Open Web Application Security Project (OWASP)&lt;/strong&gt; √© uma funda√ß√£o sem fins lucrativos que fornece orienta√ß√£o sobre como desenvolver, comprar e manter aplicativos de software confi√°veis e seguros. &lt;strong&gt;OWASP √© conhecido por sua popular lista Top 10 de vulnerabilidades de seguran√ßa de aplicativos da web.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.itgovernance.asia/iso27000-family&#34;&gt;International Organization for Standardization (ISO)&lt;/a&gt; ‚Äì Normas ISO foram criadas para fornecer orienta√ß√£o, coordena√ß√£o, simplifica√ß√£o e unifica√ß√£o de crit√©rios para empresas e organiza√ß√µes. A fam√≠lia &lt;strong&gt;ISO 27000&lt;/strong&gt; √© um conjunto de certifica√ß√µes de seguran√ßa da informa√ß√£o e prote√ß√£o de dados. Elas servem como base para a cria√ß√£o de um &lt;strong&gt;Sistema de Gest√£o de Seguran√ßa da Informa√ß√£o (SGSI)&lt;/strong&gt; em organiza√ß√µes de pequeno, m√©dio e grande porte.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cisecurity.org/controls&#34;&gt;CIS Critical Security Controls&lt;/a&gt; ‚Äì &lt;strong&gt;CIS Controls&lt;/strong&gt; √© uma publica√ß√£o de diretrizes de melhores pr√°ticas para seguran√ßa.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/TensorRT-LLM</title>
    <updated>2023-10-22T01:26:44Z</updated>
    <id>tag:github.com,2023-10-22:/NVIDIA/TensorRT-LLM</id>
    <link href="https://github.com/NVIDIA/TensorRT-LLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;TensorRT-LLM&lt;/h1&gt; &#xA; &lt;h4&gt; A TensorRT Toolbox for Large Language Models &lt;/h4&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://nvidia.github.io/TensorRT-LLM/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-31012/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.10.12-green&#34; alt=&#34;python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/cuda-12.2-green&#34; alt=&#34;cuda&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://developer.nvidia.com/tensorrt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TRT-9.1-green&#34; alt=&#34;trt&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/setup.py&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/release-0.5.0-green&#34; alt=&#34;version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-blue&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/architecture.md&#34;&gt;Architecture&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/performance.md&#34;&gt;Results&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/&#34;&gt;Examples&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;div align=&#34;left&#34;&gt; &#xA;  &lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#tensorrt-llm-overview&#34;&gt;TensorRT-LLM Overview&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#support-matrix&#34;&gt;Support Matrix&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#performance&#34;&gt;Performance&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#advanced-topics&#34;&gt;Advanced Topics&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#quantization&#34;&gt;Quantization&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#in-flight-batching&#34;&gt;In-flight Batching&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#attention&#34;&gt;Attention&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#graph-rewriting&#34;&gt;Graph Rewriting&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#benchmarking&#34;&gt;Benchmarking&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#troubleshooting&#34;&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#release-notes&#34;&gt;Release Notes&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#changelog&#34;&gt;Changelog&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#known-issues&#34;&gt;Known issues&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;h2&gt;TensorRT-LLM Overview&lt;/h2&gt; &#xA;  &lt;p&gt;TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build &lt;a href=&#34;https://developer.nvidia.com/tensorrt&#34;&gt;TensorRT&lt;/a&gt; engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. It also includes a &lt;a href=&#34;https://github.com/triton-inference-server/tensorrtllm_backend&#34;&gt;backend&lt;/a&gt; for integration with the &lt;a href=&#34;https://developer.nvidia.com/nvidia-triton-inference-server&#34;&gt;NVIDIA Triton Inference Server&lt;/a&gt;; a production-quality system to serve LLMs. Models built with TensorRT-LLM can be executed on a wide range of configurations going from a single GPU to multiple nodes with multiple GPUs (using &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/parallelisms.html#tensor-parallelism&#34;&gt;Tensor Parallelism&lt;/a&gt; and/or &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/parallelisms.html#pipeline-parallelism&#34;&gt;Pipeline Parallelism&lt;/a&gt;).&lt;/p&gt; &#xA;  &lt;p&gt;The Python API of TensorRT-LLM is architectured to look similar to the &lt;a href=&#34;https://pytorch.org&#34;&gt;PyTorch&lt;/a&gt; API. It provides users with a &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/tensorrt_llm/functional.py&#34;&gt;functional&lt;/a&gt; module containing functions like &lt;code&gt;einsum&lt;/code&gt;, &lt;code&gt;softmax&lt;/code&gt;, &lt;code&gt;matmul&lt;/code&gt; or &lt;code&gt;view&lt;/code&gt;. The &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/tensorrt_llm/layers&#34;&gt;layers&lt;/a&gt; module bundles useful building blocks to assemble LLMs; like an &lt;code&gt;Attention&lt;/code&gt; block, a &lt;code&gt;MLP&lt;/code&gt; or the entire &lt;code&gt;Transformer&lt;/code&gt; layer. Model-specific components, like &lt;code&gt;GPTAttention&lt;/code&gt; or &lt;code&gt;BertAttention&lt;/code&gt;, can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/tensorrt_llm/models&#34;&gt;models&lt;/a&gt; module.&lt;/p&gt; &#xA;  &lt;p&gt;TensorRT-LLM comes with several popular models pre-defined. They can easily be modified and extended to fit custom needs. See below for a list of supported &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#Models&#34;&gt;models&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;p&gt;To maximize performance and reduce memory footprint, TensorRT-LLM allows the models to be executed using different quantization modes (see &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/gpt&#34;&gt;&lt;code&gt;examples/gpt&lt;/code&gt;&lt;/a&gt; for concrete examples). TensorRT-LLM supports INT4 or INT8 weights (and FP16 activations; a.k.a. INT4/INT8 weight-only) as well as a complete implementation of the &lt;a href=&#34;https://arxiv.org/abs/2211.10438&#34;&gt;SmoothQuant&lt;/a&gt; technique.&lt;/p&gt; &#xA;  &lt;p&gt;For a more detailed presentation of the software architecture and the key concepts used in TensorRT-LLM, we recommend you to read the following &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/architecture.md&#34;&gt;document&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;h2&gt;Installation&lt;/h2&gt; &#xA;  &lt;p&gt;&lt;em&gt;For Windows installation, see &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/windows/&#34;&gt;&lt;code&gt;Windows/&lt;/code&gt;&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;  &lt;p&gt;TensorRT-LLM must be built from source, instructions can be found &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/installation.md&#34;&gt;here&lt;/a&gt;. An image of a Docker container with TensorRT-LLM and its Triton Inference Server Backend will be made available soon.&lt;/p&gt; &#xA;  &lt;p&gt;The remaining commands in that document must be executed from the TensorRT-LLM container.&lt;/p&gt; &#xA;  &lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;  &lt;p&gt;To create a TensorRT engine for an existing model, there are 3 steps:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Download pre-trained weights,&lt;/li&gt; &#xA;   &lt;li&gt;Build a fully-optimized engine of the model,&lt;/li&gt; &#xA;   &lt;li&gt;Deploy the engine.&lt;/li&gt; &#xA;  &lt;/ol&gt; &#xA;  &lt;p&gt;The following sections show how to use TensorRT-LLM to run the &lt;a href=&#34;https://huggingface.co/bigscience/bloom-560m&#34;&gt;BLOOM-560m&lt;/a&gt; model.&lt;/p&gt; &#xA;  &lt;p&gt;&lt;em&gt;&lt;strong&gt;0. In the BLOOM folder&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;  &lt;p&gt;Inside the Docker container, you have to install the requirements:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r examples/bloom/requirements.txt&#xA;git lfs install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;&lt;em&gt;&lt;strong&gt;1. Download the model weights from HuggingFace&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;  &lt;p&gt;From the BLOOM example folder, you must download the weights of the model.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd examples/bloom&#xA;rm -rf ./bloom/560M&#xA;mkdir -p ./bloom/560M &amp;amp;&amp;amp; git clone https://huggingface.co/bigscience/bloom-560m ./bloom/560M&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;&lt;em&gt;&lt;strong&gt;2. Build the engine&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Single GPU on BLOOM 560M&#xA;python build.py --model_dir ./bloom/560M/ \&#xA;                --dtype float16 \&#xA;                --use_gemm_plugin float16 \&#xA;                --use_gpt_attention_plugin float16 \&#xA;                --output_dir ./bloom/560M/trt_engines/fp16/1-gpu/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;See the BLOOM &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/bloom&#34;&gt;example&lt;/a&gt; for more details and options regarding the &lt;code&gt;build.py&lt;/code&gt; script.&lt;/p&gt; &#xA;  &lt;p&gt;&lt;em&gt;&lt;strong&gt;3. Run&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;  &lt;p&gt;The &lt;code&gt;summarize.py&lt;/code&gt; script can be used to perform the summarization of articles from the CNN Daily dataset:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python summarize.py --test_trt_llm \&#xA;                    --hf_model_location ./bloom/560M/ \&#xA;                    --data_type fp16 \&#xA;                    --engine_dir ./bloom/560M/trt_engines/fp16/1-gpu/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;More details about the script and how to run the BLOOM model can be found in the example &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/bloom&#34;&gt;folder&lt;/a&gt;. Many more &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#models&#34;&gt;models&lt;/a&gt; than BLOOM are implemented in TensorRT-LLM. They can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;  &lt;h2&gt;Support Matrix&lt;/h2&gt; &#xA;  &lt;p&gt;TensorRT-LLM optimizes the performance of a range of well-known models on NVIDIA GPUs. The following sections provide a list of supported GPU architectures as well as important features implemented in TensorRT-LLM.&lt;/p&gt; &#xA;  &lt;h3&gt;Devices&lt;/h3&gt; &#xA;  &lt;p&gt;TensorRT-LLM is rigorously tested on the following GPUs:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/data-center/h100/&#34;&gt;H100&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/data-center/l40s/&#34;&gt;L40S&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/data-center/a100/&#34;&gt;A100&lt;/a&gt;/&lt;a href=&#34;https://www.nvidia.com/en-us/data-center/products/a30-gpu/&#34;&gt;A30&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/data-center/v100/&#34;&gt;V100&lt;/a&gt; (experimental)&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;p&gt;If a GPU is not listed above, it is important to note that TensorRT-LLM is expected to work on GPUs based on the Volta, Turing, Ampere, Hopper and Ada Lovelace architectures. Certain limitations may, however, apply.&lt;/p&gt; &#xA;  &lt;h3&gt;Precision&lt;/h3&gt; &#xA;  &lt;p&gt;Various numerical precisions are supported in TensorRT-LLM. The support for some of those numerical features require specific architectures:&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;FP32&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;FP16&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;BF16&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;FP8&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;INT8&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;INT4&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Volta (SM70)&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;N&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;N&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Turing (SM75)&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;N&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;N&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Ampere (SM80, SM86)&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;N&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Ada-Lovelace (SM89)&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Hopper (SM90)&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;p&gt;In this release of TensorRT-LLM, the support for FP8 and quantized data types (INT8 or INT4) is not implemented for all the models. See the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/precision.md&#34;&gt;precision&lt;/a&gt; document and the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/&#34;&gt;examples&lt;/a&gt; folder for additional details.&lt;/p&gt; &#xA;  &lt;h3&gt;Key Features&lt;/h3&gt; &#xA;  &lt;p&gt;TensorRT-LLM contains examples that implement the following features.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Multi-head Attention(&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;MHA&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Multi-query Attention (&lt;a href=&#34;https://arxiv.org/abs/1911.02150&#34;&gt;MQA&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Group-query Attention(&lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;GQA&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;In-flight Batching&lt;/li&gt; &#xA;   &lt;li&gt;Paged KV Cache for the Attention&lt;/li&gt; &#xA;   &lt;li&gt;Tensor Parallelism&lt;/li&gt; &#xA;   &lt;li&gt;Pipeline Parallelism&lt;/li&gt; &#xA;   &lt;li&gt;INT4/INT8 Weight-Only Quantization (W4A16 &amp;amp; W8A16)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.10438&#34;&gt;SmoothQuant&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.05433&#34;&gt;FP8&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Greedy-search&lt;/li&gt; &#xA;   &lt;li&gt;Beam-search&lt;/li&gt; &#xA;   &lt;li&gt;RoPE&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;p&gt;In this release of TensorRT-LLM, some of the features are not enabled for all the models listed in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/&#34;&gt;examples&lt;/a&gt; folder.&lt;/p&gt; &#xA;  &lt;h3&gt;Models&lt;/h3&gt; &#xA;  &lt;p&gt;The list of supported models is:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/baichuan&#34;&gt;Baichuan&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/bert&#34;&gt;Bert&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/blip2&#34;&gt;Blip2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/bloom&#34;&gt;BLOOM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/chatglm6b&#34;&gt;ChatGLM-6B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/chatglm2-6b/&#34;&gt;ChatGLM2-6B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/falcon&#34;&gt;Falcon&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/gpt&#34;&gt;GPT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/gptj&#34;&gt;GPT-J&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/gpt&#34;&gt;GPT-Nemo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/gptneox&#34;&gt;GPT-NeoX&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/llama&#34;&gt;LLaMA&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/llama&#34;&gt;LLaMA-v2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/mpt&#34;&gt;MPT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/opt&#34;&gt;OPT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/gpt&#34;&gt;SantaCoder&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/gpt&#34;&gt;StarCoder&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;h2&gt;Performance&lt;/h2&gt; &#xA;  &lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/performance.md&#34;&gt;performance&lt;/a&gt; page for performance numbers. That page contains measured numbers for four variants of popular models (GPT-J, LLAMA-7B, LLAMA-70B, Falcon-180B), measured on the H100, L40S and A100 GPU(s).&lt;/p&gt; &#xA;  &lt;h2&gt;Advanced Topics&lt;/h2&gt; &#xA;  &lt;h3&gt;Quantization&lt;/h3&gt; &#xA;  &lt;p&gt;This &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/precision.md&#34;&gt;document&lt;/a&gt; describes the different quantization methods implemented in TensorRT-LLM and contains a support matrix for the different models.&lt;/p&gt; &#xA;  &lt;h3&gt;In-flight Batching&lt;/h3&gt; &#xA;  &lt;p&gt;TensorRT-LLM supports in-flight batching of requests (also known as continuous batching or iteration-level batching). It&#39;s a &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/batch_manager.md&#34;&gt;technique&lt;/a&gt; that aims at reducing wait times in queues, eliminating the need for padding requests and allowing for higher GPU utilization.&lt;/p&gt; &#xA;  &lt;h3&gt;Attention&lt;/h3&gt; &#xA;  &lt;p&gt;TensorRT-LLM implements several variants of the Attention mechanism that appears in most the Large Language Models. This &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/gpt_attention.md&#34;&gt;document&lt;/a&gt; summarizes those implementations and how they are optimized in TensorRT-LLM.&lt;/p&gt; &#xA;  &lt;h3&gt;Graph Rewriting&lt;/h3&gt; &#xA;  &lt;p&gt;TensorRT-LLM uses a declarative approach to define neural networks and contains techniques to optimize the underlying graph. For more details, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/graph-rewriting.md&#34;&gt;doc&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;h3&gt;Benchmark&lt;/h3&gt; &#xA;  &lt;p&gt;TensorRT-LLM provides &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/benchmarks/cpp/README.md&#34;&gt;C++&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/benchmarks/python/README.md&#34;&gt;Python&lt;/a&gt; tools to perform benchmarking. Note, however, that it is recommended to use the C++ version.&lt;/p&gt; &#xA;  &lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;It&#39;s recommended to add options &lt;code&gt;‚Äìshm-size=1g ‚Äìulimit memlock=-1&lt;/code&gt; to the docker or nvidia-docker run command. Otherwise you may see NCCL errors when running multiple GPU inferences. See &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#errors&#34;&gt;https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#errors&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;When building models, memory-related issues such as&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;pre&gt;&lt;code&gt;[09/23/2023-03:13:00] [TRT] [E] 9: GPTLMHeadModel/layers/0/attention/qkv/PLUGIN_V2_Gemm_0: could not find any supported formats consistent with input/output data types&#xA;[09/23/2023-03:13:00] [TRT] [E] 9: [pluginV2Builder.cpp::reportPluginError::24] Error Code 9: Internal Error (GPTLMHeadModel/layers/0/attention/qkv/PLUGIN_V2_Gemm_0: could not find any supported formats consistent with input/output data types)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;may happen. One possible solution is to reduce the amount of memory needed by reducing the maximum batch size, input and output lengths. Another option is to enable plugins, for example: &lt;code&gt;--use_gpt_attention_plugin&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;h2&gt;Release notes&lt;/h2&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;TensorRT-LLM requires TensorRT 9.1.0.4 and 23.08 containers.&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;h3&gt;Change Log&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;TensorRT-LLM v0.5.0 is the first public release.&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;h3&gt;Known Issues&lt;/h3&gt; &#xA;  &lt;h3&gt;Report Issues&lt;/h3&gt; &#xA;  &lt;p&gt;You can use GitHub issues to report issues with TensorRT-LLM.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>neuralmagic/deepsparse</title>
    <updated>2023-10-22T01:26:44Z</updated>
    <id>tag:github.com,2023-10-22:/neuralmagic/deepsparse</id>
    <link href="https://github.com/neuralmagic/deepsparse" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Sparsity-aware deep learning inference runtime for CPUs&lt;/p&gt;&lt;hr&gt;&lt;div style=&#34;display: flex; flex-direction: column; align-items: center;&#34;&gt; &#xA; &lt;h1&gt; &lt;img alt=&#34;tool icon&#34; src=&#34;https://raw.githubusercontent.com/neuralmagic/deepsparse/main/docs/old/source/icon-deepsparse.png&#34;&gt; &amp;nbsp;&amp;nbsp;DeepSparse &lt;/h1&gt; &#xA; &lt;h4&gt;Sparsity-aware deep learning inference runtime for CPUs&lt;/h4&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://docs.neuralmagic.com/deepsparse/&#34;&gt; &lt;img alt=&#34;Documentation&#34; src=&#34;https://img.shields.io/badge/documentation-darkred?&amp;amp;style=for-the-badge&amp;amp;logo=read-the-docs&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ/&#34;&gt; &lt;img alt=&#34;Slack&#34; src=&#34;https://img.shields.io/badge/slack-purple?style=for-the-badge&amp;amp;logo=slack&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://github.com/neuralmagic/deepsparse/issues/&#34;&gt; &lt;img alt=&#34;Support&#34; src=&#34;https://img.shields.io/badge/support%20forums-navy?style=for-the-badge&amp;amp;logo=github&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://www.youtube.com/channel/UCo8dO_WMGYbWCRnj_Dxr4EA&#34;&gt; &lt;img alt=&#34;YouTube&#34; src=&#34;https://img.shields.io/badge/-YouTube-red?&amp;amp;style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://twitter.com/neuralmagic&#34;&gt; &lt;img alt=&#34;Twitter&#34; src=&#34;https://img.shields.io/twitter/follow/neuralmagic?color=darkgreen&amp;amp;label=Follow&amp;amp;style=social&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse&#34;&gt;DeepSparse&lt;/a&gt; is a CPU inference runtime that takes advantage of sparsity to accelerate neural network inference. Coupled with &lt;a href=&#34;https://github.com/neuralmagic/sparseml&#34;&gt;SparseML&lt;/a&gt;, our optimization library for pruning and quantizing your models, DeepSparse delivers exceptional inference performance on CPU hardware.&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;NM Flow&#34; src=&#34;https://github.com/neuralmagic/deepsparse/assets/3195154/51e62fe7-9d9a-4fa5-a774-877158da1e29&#34; width=&#34;60%&#34;&gt; &lt;/p&gt; &#xA; &lt;h2&gt;‚ú®NEW‚ú® DeepSparse LLMs&lt;/h2&gt; &#xA; &lt;p&gt;Neural Magic is excited to announce initial support for performant LLM inference in DeepSparse with:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;sparse kernels for speedups and memory savings from unstructured sparse weights.&lt;/li&gt; &#xA;  &lt;li&gt;8-bit weight and activation quantization support.&lt;/li&gt; &#xA;  &lt;li&gt;efficient usage of cached attention keys and values for minimal memory movement.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/neuralmagic/deepsparse/assets/3195154/ccf39323-4603-4489-8462-7b103872aeb3&#34; alt=&#34;mpt-chat-comparison&#34;&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Try It Now&lt;/h3&gt; &#xA; &lt;p&gt;Install (requires Linux):&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U deepsparse-nightly[llm]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Run inference:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepsparse import TextGeneration&#xA;pipeline = TextGeneration(model=&#34;zoo:mpt-7b-dolly_mpt_pretrain-pruned50_quantized&#34;)&#xA;&#xA;prompt=&#34;&#34;&#34;&#xA;Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: what is sparsity? ### Response:&#xA;&#34;&#34;&#34;&#xA;print(pipeline(prompt, max_new_tokens=75).generations[0].text)&#xA;&#xA;# Sparsity is the property of a matrix or other data structure in which a large number of elements are zero and a smaller number of elements are non-zero. In the context of machine learning, sparsity can be used to improve the efficiency of training and prediction.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/raw/main/docs/llms/text-generation-pipeline.md&#34;&gt;Check out the &lt;code&gt;TextGeneration&lt;/code&gt; documentation for usage details.&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;h3&gt;Sparsity &lt;span&gt;ü§ù&lt;/span&gt; Performance&lt;/h3&gt; &#xA; &lt;p&gt;Developed in collaboration with IST Austria, &lt;a href=&#34;https://arxiv.org/abs/2310.06927&#34;&gt;our recent paper&lt;/a&gt; details a new technique called &lt;strong&gt;Sparse Fine-Tuning&lt;/strong&gt;, which allows us to prune MPT-7B to 60% sparsity during fine-tuning without drop in accuracy. With our new support for LLMs, DeepSparse accelerates the sparse-quantized model 7x over the dense baseline:&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://github.com/neuralmagic/deepsparse/assets/3195154/8687401c-f479-4999-ba6b-e01c747dace9&#34; width=&#34;60%&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/raw/main/research/mpt#sparse-finetuned-llms-with-deepsparse&#34;&gt;Learn more about our Sparse Fine-Tuning research.&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/neuralmagic/sparse-mpt-7b-gsm8k&#34;&gt;Check out the model running live on Hugging Face.&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;h3&gt;LLM Roadmap&lt;/h3&gt; &#xA; &lt;p&gt;Following this initial launch, we are rapidly expanding our support for LLMs, including:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Productizing Sparse Fine-Tuning: Enable external users to apply sparse fine-tuning to their datasets via SparseML.&lt;/li&gt; &#xA;  &lt;li&gt;Expanding model support: Apply our sparse fine-tuning results to Llama 2 and Mistral models.&lt;/li&gt; &#xA;  &lt;li&gt;Pushing for higher sparsity: Improving our pruning algorithms to reach even higher sparsity.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;h2&gt;Computer Vision and NLP Models&lt;/h2&gt; &#xA; &lt;p&gt;In addition to LLMs, DeepSparse supports many variants of CNNs and Transformer models, such as BERT, ViT, ResNet, EfficientNet, YOLOv5/8, and many more! Take a look at the &lt;a href=&#34;https://sparsezoo.neuralmagic.com/?modelSet=computer_vision&#34;&gt;Computer Vision&lt;/a&gt; and &lt;a href=&#34;https://sparsezoo.neuralmagic.com/?modelSet=natural_language_processing&#34;&gt;Natural Language Processing&lt;/a&gt; domains of &lt;a href=&#34;https://sparsezoo.neuralmagic.com/&#34;&gt;SparseZoo&lt;/a&gt;, our home for optimized models.&lt;/p&gt; &#xA; &lt;h3&gt;Installation&lt;/h3&gt; &#xA; &lt;p&gt;Install via &lt;a href=&#34;https://pypi.org/project/deepsparse/&#34;&gt;PyPI&lt;/a&gt; (&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/installation.md&#34;&gt;optional dependencies detailed here&lt;/a&gt;):&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install deepsparse &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To experiment with the latest features, there is a nightly build available using &lt;code&gt;pip install deepsparse-nightly&lt;/code&gt; or you can clone and install from source using &lt;code&gt;pip install -e path/to/deepsparse&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;h4&gt;System Requirements&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Hardware: &lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/hardware-support.md&#34;&gt;x86 AVX2, AVX-512, AVX-512 VNNI and ARM v8.2+&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Operating System: Linux&lt;/li&gt; &#xA;  &lt;li&gt;Python: 3.8-3.11&lt;/li&gt; &#xA;  &lt;li&gt;ONNX versions 1.5.0-1.15.0, ONNX opset version 11 or higher&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;For those using Mac or Windows, we recommend using Linux containers with Docker.&lt;/p&gt; &#xA; &lt;h2&gt;Deployment APIs&lt;/h2&gt; &#xA; &lt;p&gt;DeepSparse includes three deployment APIs:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Engine&lt;/strong&gt; is the lowest-level API. With Engine, you compile an ONNX model, pass tensors as input, and receive the raw outputs.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Pipeline&lt;/strong&gt; wraps the Engine with pre- and post-processing. With Pipeline, you pass raw data and receive the prediction.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Server&lt;/strong&gt; wraps Pipelines with a REST API using FastAPI. With Server, you send raw data over HTTP and receive the prediction.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Engine&lt;/h3&gt; &#xA; &lt;p&gt;The example below downloads a 90% pruned-quantized BERT model for sentiment analysis in ONNX format from SparseZoo, compiles the model, and runs inference on randomly generated input. Users can provide their own ONNX models, whether dense or sparse.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepsparse import Engine&#xA;&#xA;# download onnx, compile&#xA;zoo_stub = &#34;zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none&#34;&#xA;compiled_model = Engine(model=zoo_stub, batch_size=1)&#xA;&#xA;# run inference (input is raw numpy tensors, output is raw scores)&#xA;inputs = compiled_model.generate_random_inputs()&#xA;output = compiled_model(inputs)&#xA;print(output)&#xA;&#xA;# &amp;gt; [array([[-0.3380675 ,  0.09602544]], dtype=float32)] &amp;lt;&amp;lt; raw scores&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Pipeline&lt;/h3&gt; &#xA; &lt;p&gt;Pipelines wrap Engine with pre- and post-processing, enabling you to pass raw data and receive the post-processed prediction. The example below downloads a 90% pruned-quantized BERT model for sentiment analysis in ONNX format from SparseZoo, sets up a pipeline, and runs inference on sample data.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepsparse import Pipeline&#xA;&#xA;# download onnx, set up pipeline&#xA;zoo_stub = &#34;zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none&#34;  &#xA;sentiment_analysis_pipeline = Pipeline.create(&#xA;  task=&#34;sentiment-analysis&#34;,    # name of the task&#xA;  model_path=zoo_stub,          # zoo stub or path to local onnx file&#xA;)&#xA;&#xA;# run inference (input is a sentence, output is the prediction)&#xA;prediction = sentiment_analysis_pipeline(&#34;I love using DeepSparse Pipelines&#34;)&#xA;print(prediction)&#xA;# &amp;gt; labels=[&#39;positive&#39;] scores=[0.9954759478569031]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Server&lt;/h3&gt; &#xA; &lt;p&gt;Server wraps Pipelines with REST APIs, enabling you to set up a model-serving endpoint running DeepSparse. This enables you to send raw data to DeepSparse over HTTP and receive the post-processed predictions. DeepSparse Server is launched from the command line and configured via arguments or a server configuration file. The following downloads a 90% pruned-quantized BERT model for sentiment analysis in ONNX format from SparseZoo and launches a sentiment analysis endpoint:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;deepsparse.server \&#xA;  --task sentiment-analysis \&#xA;  --model_path zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Sending a request:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests&#xA;&#xA;url = &#34;http://localhost:5543/v2/models/sentiment_analysis/infer&#34; # Server&#39;s port default to 5543&#xA;obj = {&#34;sequences&#34;: &#34;Snorlax loves my Tesla!&#34;}&#xA;&#xA;response = requests.post(url, json=obj)&#xA;print(response.text)&#xA;# {&#34;labels&#34;:[&#34;positive&#34;],&#34;scores&#34;:[0.9965094327926636]}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Additional Resources&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/docs/use-cases&#34;&gt;Use Cases Page&lt;/a&gt; for more details on supported tasks&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/deepsparse-pipelines.md&#34;&gt;Pipelines User Guide&lt;/a&gt; for Pipeline documentation&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/deepsparse-server.md&#34;&gt;Server User Guide&lt;/a&gt; for Server documentation&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/deepsparse-benchmarking.md&#34;&gt;Benchmarking User Guide&lt;/a&gt; for benchmarking documentation&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/examples/&#34;&gt;Cloud Deployments and Demos&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide&#34;&gt;User Guide&lt;/a&gt; for more detailed documentation&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h2&gt;Product Usage Analytics&lt;/h2&gt; &#xA; &lt;p&gt;DeepSparse gathers basic usage telemetry, including, but not limited to, Invocations, Package, Version, and IP Address, for Product Usage Analytics purposes. Review Neural Magic&#39;s &lt;a href=&#34;https://neuralmagic.com/legal/&#34;&gt;Products Privacy Policy&lt;/a&gt; for further details on how we process this data.&lt;/p&gt; &#xA; &lt;p&gt;To disable Product Usage Analytics, run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export NM_DISABLE_ANALYTICS=True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Confirm that telemetry is shut off through info logs streamed with engine invocation by looking for the phrase &#34;Skipping Neural Magic&#39;s latest package version check.&#34;&lt;/p&gt; &#xA; &lt;h2&gt;Community&lt;/h2&gt; &#xA; &lt;h3&gt;Get In Touch&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/raw/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ&#34;&gt;Community Slack&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/neuralmagic/deepsparse/issues&#34;&gt;GitHub Issue Queue&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://neuralmagic.com/subscribe/&#34;&gt;Subscribe To Our Newsletter&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.neuralmagic.com/blog/&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;For more general questions about Neural Magic, &lt;a href=&#34;http://neuralmagic.com/contact/&#34;&gt;complete this form.&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;License&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;DeepSparse Community&lt;/strong&gt; is licensed under the &lt;a href=&#34;https://github.com/neuralmagic/deepsparse/raw/main/LICENSE-NEURALMAGIC&#34;&gt;Neural Magic DeepSparse Community License.&lt;/a&gt; Some source code, example files, and scripts included in the DeepSparse GitHub repository or directory are licensed under the &lt;a href=&#34;https://github.com/neuralmagic/deepsparse/raw/main/LICENSE&#34;&gt;Apache License Version 2.0&lt;/a&gt; as noted.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;DeepSparse Enterprise&lt;/strong&gt; requires a Trial License or &lt;a href=&#34;https://neuralmagic.com/legal/master-software-license-and-service-agreement/&#34;&gt;can be fully licensed&lt;/a&gt; for production, commercial applications.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Cite&lt;/h3&gt; &#xA; &lt;p&gt;Find this project useful in your research or other communications? Please consider citing:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{kurtic2023sparse,&#xA;      title={Sparse Fine-Tuning for Inference Acceleration of Large Language Models}, &#xA;      author={Eldar Kurtic and Denis Kuznedelev and Elias Frantar and Michael Goin and Dan Alistarh},&#xA;      year={2023},&#xA;      url={https://arxiv.org/abs/2310.06927},&#xA;      eprint={2310.06927},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&#xA;@misc{kurtic2022optimal,&#xA;      title={The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models}, &#xA;      author={Eldar Kurtic and Daniel Campos and Tuan Nguyen and Elias Frantar and Mark Kurtz and Benjamin Fineran and Michael Goin and Dan Alistarh},&#xA;      year={2022},&#xA;      url={https://arxiv.org/abs/2203.07259},&#xA;      eprint={2203.07259},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&#xA;@InProceedings{&#xA;    pmlr-v119-kurtz20a, &#xA;    title = {Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks}, &#xA;    author = {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Nell, Bill and Shavit, Nir and Alistarh, Dan}, &#xA;    booktitle = {Proceedings of the 37th International Conference on Machine Learning}, &#xA;    pages = {5533--5543}, &#xA;    year = {2020}, &#xA;    editor = {Hal Daum√© III and Aarti Singh}, &#xA;    volume = {119}, &#xA;    series = {Proceedings of Machine Learning Research}, &#xA;    address = {Virtual}, &#xA;    month = {13--18 Jul}, &#xA;    publisher = {PMLR}, &#xA;    pdf = {http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf},&#xA;    url = {http://proceedings.mlr.press/v119/kurtz20a.html}&#xA;}&#xA;&#xA;@article{DBLP:journals/corr/abs-2111-13445,&#xA;  author    = {Eugenia Iofinova and Alexandra Peste and Mark Kurtz and Dan Alistarh},&#xA;  title     = {How Well Do Sparse Imagenet Models Transfer?},&#xA;  journal   = {CoRR},&#xA;  volume    = {abs/2111.13445},&#xA;  year      = {2021},&#xA;  url       = {https://arxiv.org/abs/2111.13445},&#xA;  eprinttype = {arXiv},&#xA;  eprint    = {2111.13445},&#xA;  timestamp = {Wed, 01 Dec 2021 15:16:43 +0100},&#xA;  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-13445.bib},&#xA;  bibsource = {dblp computer science bibliography, https://dblp.org}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>