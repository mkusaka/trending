<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-02T01:23:52Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>roboflow/multimodal-maestro</title>
    <updated>2023-12-02T01:23:52Z</updated>
    <id>tag:github.com,2023-12-02:/roboflow/multimodal-maestro</id>
    <link href="https://github.com/roboflow/multimodal-maestro" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Effective prompting for Large Multimodal Models like GPT-4 Vision or LLaVA. 🔥&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;multimodal-maestro&lt;/h1&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/maestro&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/maestro.svg?sanitize=true&#34; alt=&#34;version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/roboflow/multimodal-maestro/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/maestro&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/maestro&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/maestro&#34; alt=&#34;python-version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Roboflow/SoM&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Gradio&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/roboflow/multimodal-maestro/blob/develop/cookbooks/multimodal_maestro_gpt_4_vision.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;👋 hello&lt;/h2&gt; &#xA;&lt;p&gt;Multimodal-Maestro gives you more control over large multimodal models to get the outputs you want. With more effective prompting tactics, you can get multimodal models to do tasks you didn&#39;t know (or think!) were possible. Curious how it works? Try our &lt;a href=&#34;https://huggingface.co/spaces/Roboflow/SoM&#34;&gt;HF space&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;💻 install&lt;/h2&gt; &#xA;&lt;p&gt;⚠️ Our package has been renamed to &lt;code&gt;maestro&lt;/code&gt;. Install the package in a &lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;strong&gt;3.11&amp;gt;=Python&amp;gt;=3.8&lt;/strong&gt;&lt;/a&gt; environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install maestro&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🔌 API&lt;/h2&gt; &#xA;&lt;p&gt;🚧 The project is still under construction. The redesigned API is coming soon.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/roboflow/multimodal-maestro/assets/26109316/a787b7c0-527e-465a-9ca9-d46f4d63ea53&#34; alt=&#34;maestro-docs-Snap&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🚀 examples&lt;/h2&gt; &#xA;&lt;h3&gt;GPT-4 Vision&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;Find dog.&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; The dog is prominently featured in the center of the image with the label [9].&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details close&gt; &#xA; &lt;summary&gt;👉 read more&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;load image&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;&#xA;image = cv2.imread(&#34;...&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;create and refine marks&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import maestro&#xA;&#xA;generator = maestro.SegmentAnythingMarkGenerator(device=&#39;cuda&#39;)&#xA;marks = generator.generate(image=image)&#xA;marks = maestro.refine_marks(marks=marks)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;visualize marks&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mark_visualizer = maestro.MarkVisualizer()&#xA;marked_image = mark_visualizer.visualize(image=image, marks=marks)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/roboflow/multimodal-maestro/assets/26109316/92951ed2-65c0-475a-9279-6fd344757092&#34; alt=&#34;image-vs-marked-image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;prompt&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = &#34;Find dog.&#34;&#xA;&#xA;response = maestro.prompt_image(api_key=api_key, image=marked_image, prompt=prompt)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; &#34;The dog is prominently featured in the center of the image with the label [9].&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;extract related marks&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;masks = maestro.extract_relevant_masks(text=response, detections=refined_marks)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; {&#39;6&#39;: array([&#xA;...     [False, False, False, ..., False, False, False],&#xA;...     [False, False, False, ..., False, False, False],&#xA;...     [False, False, False, ..., False, False, False],&#xA;...     ...,&#xA;...     [ True,  True,  True, ..., False, False, False],&#xA;...     [ True,  True,  True, ..., False, False, False],&#xA;...     [ True,  True,  True, ..., False, False, False]])&#xA;... }&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/roboflow/multimodal-maestro/assets/26109316/c04f2b18-2a1d-4535-9582-e5d3ec0a926e&#34; alt=&#34;multimodal-maestro&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🚧 roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Rewriting the &lt;code&gt;maestro&lt;/code&gt; API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update &lt;a href=&#34;https://huggingface.co/spaces/Roboflow/SoM&#34;&gt;HF space&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Documentation page.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add GroundingDINO prompting strategy.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; CovVLM demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Qwen-VL demo.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;💜 acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.11441&#34;&gt;Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V&lt;/a&gt; by Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, Jianfeng Gao.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.17421&#34;&gt;The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)&lt;/a&gt; by Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, Lijuan Wang&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🦸 contribution&lt;/h2&gt; &#xA;&lt;p&gt;We would love your help in making this repository even better! If you noticed any bug, or if you have any suggestions for improvement, feel free to open an &lt;a href=&#34;https://github.com/roboflow/multimodal-maestro/issues&#34;&gt;issue&lt;/a&gt; or submit a &lt;a href=&#34;https://github.com/roboflow/multimodal-maestro/pulls&#34;&gt;pull request&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>notmahi/dobb-e</title>
    <updated>2023-12-02T01:23:52Z</updated>
    <id>tag:github.com,2023-12-02:/notmahi/dobb-e</id>
    <link href="https://github.com/notmahi/dobb-e" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Dobb·E: An open-source, general framework for learning household robotic manipulation&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/notmahi/dobb-e/assets/3000253/341faa2f-285a-4152-91f6-73bec2811a97&#34; alt=&#34;preview&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Dobb·E&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.16098&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2311.16098-163144.svg?style=for-the-badge&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/license/notmahi/bet?color=873a7e&amp;amp;style=for-the-badge&#34; alt=&#34;License&#34;&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20Style-Black-262626?style=for-the-badge&#34; alt=&#34;Code Style: Black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PyTorch-2.1.1-db6a4b.svg?style=for-the-badge&amp;amp;logo=pytorch&#34; alt=&#34;PyTorch&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dobb-e.com&#34;&gt;Project webpage&lt;/a&gt; · &lt;a href=&#34;https://docs.dobb-e.com&#34;&gt;Documentation (gitbooks)&lt;/a&gt; · &lt;a href=&#34;https://arxiv.org/abs/2311.16098&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: &lt;a href=&#34;https://mahis.life&#34;&gt;Mahi Shafiullah*&lt;/a&gt;, &lt;a href=&#34;https://raianant.github.io/&#34;&gt;Anant Rai*&lt;/a&gt;, &lt;a href=&#34;https://haritheja.com/&#34;&gt;Haritheja Etukuru&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/eva-liu-ba90a5209/&#34;&gt;Yiqian Liu&lt;/a&gt;, &lt;a href=&#34;https://imisra.github.io/&#34;&gt;Ishan Misra&lt;/a&gt;, &lt;a href=&#34;https://soumith.ch&#34;&gt;Soumith Chintala&lt;/a&gt;, &lt;a href=&#34;https://lerrelpinto.com&#34;&gt;Lerrel Pinto&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Open-source repository of the hardware and software components of &lt;a href=&#34;https://dobb-e.com&#34;&gt;Dobb·E&lt;/a&gt; and the associated paper, &lt;a href=&#34;https://arxiv.org/abs/2311.16098&#34;&gt;On Bringing Robots Home&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/notmahi/dobb-e/assets/3000253/d332fa42-d351-4ad0-a305-4b6664bd7170&#34;&gt;https://github.com/notmahi/dobb-e/assets/3000253/d332fa42-d351-4ad0-a305-4b6664bd7170&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2&gt;Abstract&lt;/h2&gt;&lt;/summary&gt; Throughout history, we have successfully integrated various machines into our homes - dishwashers, laundry machines, stand mixers, and robot vacuums are a few of the latest examples. However, these machines excel at performing a single task effectively. The concept of a “generalist machine” in homes - a domestic assistant that can adapt and learn from our needs, all while remaining cost-effective has long been a northstar in robotics that has been steadily pursued for decades. In this work, we initiate a large-scale effort towards this goal by introducing Dobb·E, an affordable yet versatile general-purpose system for learning robotic manipulation within household settings. Dobb·E can learn a new task with only five minutes of a user showing it how to, thanks to a demonstration collection tool (“The Stick”) we built out of cheap parts and iPhones. We use the Stick to collect 13 hours of data in 22 homes of New York City, and train Home Pretrained Representations (HPR). Then, in a novel home environment, with five minutes of demonstrations and fifteen minutes of adapting the HPR model, we show that Dobb·E can reliably solve the task on the Stretch, a mobile robot readily available in the market. Across roughly 30 days of experimentation in homes of New York City and surrounding areas, we test our system in 10 homes, with a total of 109 tasks in different environments, and finally achieve a success rate of 81%. Beyond success percentages, our experiments reveala plethora of unique challenges absent or ignored in lab-robotics, ranging fromeffects of strong shadows, to demonstration quality by non-expert users. With the hope of accelerating research on home robots, and eventually seeing robot butlers in every home, we open-source Dobb·E software stack and models, our data, and our hardware designs. &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;What&#39;s on this repo&lt;/h2&gt; &#xA;&lt;p&gt;Dobb·E is made out of four major components:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A hardware tool, called &lt;a href=&#34;https://dobb-e.com/#hardware&#34;&gt;The Stick&lt;/a&gt;, to comfortably collect robotic demonstrations in homes.&lt;/li&gt; &#xA; &lt;li&gt;A dataset, called &lt;a href=&#34;https://dobb-e.com/#dataset&#34;&gt;Homes of New York (HoNY)&lt;/a&gt;, with 1.5 million RGB-D frames. collected with the Stick across 22 homes and 216 environments of New York City.&lt;/li&gt; &#xA; &lt;li&gt;A pretrained lightweight foundational vision model called &lt;a href=&#34;https://dobb-e.com/#models&#34;&gt;Home Pretrained Representations (HPR)&lt;/a&gt;, trained on the HoNY dataset.&lt;/li&gt; &#xA; &lt;li&gt;Finally, the platform to tie it all together to &lt;a href=&#34;https://dobb-e.com/#videos&#34;&gt;deploy it in novel homes&lt;/a&gt;, where with only five minutes of training data and 15 minutes of fine-tuning HPR, Dobb·E can solve many simple household tasks.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Reflecting this structure, there are four folders in this repo, where:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/notmahi/dobb-e/main/hardware&#34;&gt;&lt;code&gt;hardware&lt;/code&gt;&lt;/a&gt; contains our 3D printable STL files, as well as instructions on how to set up the Stick.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/notmahi/dobb-e/main/stick-data-collection&#34;&gt;&lt;code&gt;stick-data-collection&lt;/code&gt;&lt;/a&gt; contains all the necessary software for processing any data you collect on the Stick.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/notmahi/dobb-e/main/imitation-in-homes&#34;&gt;&lt;code&gt;imitation-in-homes&lt;/code&gt;&lt;/a&gt; contains our code for training a policy on your collected data using our pretrained models, and also the code to pretrain a new model yourself.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/notmahi/dobb-e/main/robot-server&#34;&gt;&lt;code&gt;robot-server&lt;/code&gt;&lt;/a&gt; contains the code to be run on the robot to deploy the learned policies.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The primary documentation source is gitbooks at &lt;a href=&#34;https://docs.dobb-e.com&#34;&gt;https://docs.dobb-e.com&lt;/a&gt;. There are also associated documentations inside each folder&#39;s READMEs.&lt;/p&gt; &#xA;&lt;h2&gt;Paper&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/notmahi/dobb-e/assets/3000253/0190c36b-da84-4b77-9979-762062c3b2b7&#34; alt=&#34;paper_preview&#34;&gt; Get it from &lt;a href=&#34;https://arxiv.org/abs/2311.16098&#34;&gt;ArXiv&lt;/a&gt; or our &lt;a href=&#34;https://dobb-e.com/#paper&#34;&gt;website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find any of our work useful, please cite us!&lt;/p&gt; &#xA;&lt;pre&gt;&#xA;@misc{shafiullah2023dobbe,&#xA;      title={On Bringing Robots Home}, &#xA;      author={Nur Muhammad Mahi Shafiullah and Anant Rai and Haritheja Etukuru and Yiqian Liu and Ishan Misra and Soumith Chintala and Lerrel Pinto},&#xA;      year={2023},&#xA;      eprint={2311.16098},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.RO}&#xA;}&#xA;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>autonomousvision/mip-splatting</title>
    <updated>2023-12-02T01:23:52Z</updated>
    <id>tag:github.com,2023-12-02:/autonomousvision/mip-splatting</id>
    <link href="https://github.com/autonomousvision/mip-splatting" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mip-Splatting: Alias-free 3D Gaussian Splatting&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;Mip-Splatting: Alias-free 3D Gaussian Splatting&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://niujinshuchong.github.io/&#34;&gt;Zehao Yu&lt;/a&gt; · &lt;a href=&#34;https://apchenstu.github.io/&#34;&gt;Anpei Chen&lt;/a&gt; · &lt;a href=&#34;https://github.com/hbb1&#34;&gt;Binbin Huang&lt;/a&gt; · &lt;a href=&#34;https://tsattler.github.io/&#34;&gt;Torsten Sattler&lt;/a&gt; · &lt;a href=&#34;http://www.cvlibs.net/&#34;&gt;Andreas Geiger&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1Q7KgGbynzcIEyFJV1I17HgrYz6xrOwRJ/view?usp=sharing&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/pdf/2311.16493.pdf&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://niujinshuchong.github.io/mip-splatting/&#34;&gt;Project Page&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/autonomousvision/mip-splatting/main/media/bicycle_3dgs_vs_ours.gif&#34; alt=&#34;Logo&#34; width=&#34;95%&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; We introduce a 3D smoothing filter and a 2D Mip filter for 3D Gaussian Splatting (3DGS), eliminating multiple artifacts and achieving alias-free renderings. &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Clone the repository and create an anaconda environment using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:autonomousvision/mip-splatting.git&#xA;cd mip-splatting&#xA;&#xA;conda create -y -n mip-splatting python=3.8&#xA;conda activate mip-splatting&#xA;&#xA;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html&#xA;conda install cudatoolkit-dev=11.3 -c conda-forge&#xA;&#xA;pip install -r requirements.txt&#xA;&#xA;pip install submodules/diff-gaussian-rasterization&#xA;pip install submodules/simple-knn/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Dataset&lt;/h1&gt; &#xA;&lt;h2&gt;Blender Dataset&lt;/h2&gt; &#xA;&lt;p&gt;Please download and unzip nerf_synthetic.zip from the &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;NeRF&#39;s official Google Drive&lt;/a&gt;. Then generate multi-scale blender dataset with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python convert_blender_data.py --blender_dir nerf_synthetic/ --out_dir multi-scale&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Mip-NeRF 360 Dataset&lt;/h2&gt; &#xA;&lt;p&gt;Please download the data from the &lt;a href=&#34;https://jonbarron.info/mipnerf360/&#34;&gt;Mip-NeRF 360&lt;/a&gt; and request the authors for the treehill and flowers scenes.&lt;/p&gt; &#xA;&lt;h1&gt;Training and evaluation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;# single-scale training and single-scale testing on NeRF-synthetic dataset&#xA;python scripts/run_nerf_synthetic_stmt.py &#xA;&#xA;# multi-scale training and multi-scale testing on NeRF-synthetic dataset&#xA;python scripts/run_nerf_synthetic_mtmt.py &#xA;&#xA;# single-scale training and single-scale testing on the mip-nerf 360 dataset&#xA;python scripts/run_mipnerf360.py &#xA;&#xA;# single-scale training and multi-scale testing on the mip-nerf 360 dataset&#xA;python scripts/run_mipnerf360_stmt.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;This project is built upon &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;3DGS&lt;/a&gt;. Please follow the license of 3DGS. We thank all the authors for their great work and repos.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you find our code or paper useful, please cite&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Yu2023MipSplatting,&#xA;  author    = {Yu, Zehao and Chen, Anpei and Huang, Binbin and Sattler, Torsten and Geiger, Andreas},&#xA;  title     = {Mip-Splatting: Alias-free 3D Gaussian Splatting},&#xA;  journal   = {arXiv:2311.16493},&#xA;  year      = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>