<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-30T01:28:30Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>airtai/faststream</title>
    <updated>2024-10-30T01:28:30Z</updated>
    <id>tag:github.com,2024-10-30:/airtai/faststream</id>
    <link href="https://github.com/airtai/faststream" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FastStream is a powerful and easy-to-use Python framework for building asynchronous services interacting with event streams such as Apache Kafka, RabbitMQ, NATS and Redis.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FastStream&lt;/h1&gt; &#xA;&lt;p&gt;&lt;b&gt;Effortless event stream integration for your services&lt;/b&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/airtai/faststream/actions/workflows/pr_tests.yaml&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/airtai/faststream/actions/workflows/pr_tests.yaml/badge.svg?branch=main&#34; alt=&#34;Test Passing&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://coverage-badge.samuelcolvin.workers.dev/redirect/airtai/faststream&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://coverage-badge.samuelcolvin.workers.dev/airtai/faststream.svg?sanitize=true&#34; alt=&#34;Coverage&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.pepy.tech/projects/faststream&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://static.pepy.tech/personalized-badge/faststream?period=month&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=green&amp;amp;left_text=downloads/month&#34; alt=&#34;Downloads&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/faststream&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/faststream?label=PyPI&#34; alt=&#34;Package version&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/faststream&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/faststream.svg?sanitize=true&#34; alt=&#34;Supported Python versions&#34;&gt; &lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/airtai/faststream/actions/workflows/pr_codeql.yaml&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/airtai/faststream/actions/workflows/pr_codeql.yaml/badge.svg?sanitize=true&#34; alt=&#34;CodeQL&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/airtai/faststream/actions/workflows/pr_dependency-review.yaml&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/airtai/faststream/actions/workflows/pr_dependency-review.yaml/badge.svg?sanitize=true&#34; alt=&#34;Dependency Review&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/airtai/faststream/raw/main/LICENSE&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/airtai/faststream.png&#34; alt=&#34;License&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/airtai/faststream/raw/main/CODE_OF_CONDUCT.md&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg?sanitize=true&#34; alt=&#34;Code of Conduct&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/qFm6aSqq59&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/1085457301214855171?logo=discord&amp;amp;label=EN&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://t.me/python_faststream&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Telegram&#34; src=&#34;https://img.shields.io/badge/-telegram-black?color=blue&amp;amp;logo=telegram&amp;amp;label=RU&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://faststream.airt.ai/latest/&#34;&gt;&lt;strong&gt;FastStream&lt;/strong&gt;&lt;/a&gt; simplifies the process of writing producers and consumers for message queues, handling all the parsing, networking and documentation generation automatically.&lt;/p&gt; &#xA;&lt;p&gt;Making streaming microservices has never been easier. Designed with junior developers in mind, &lt;strong&gt;FastStream&lt;/strong&gt; simplifies your work while keeping the door open for more advanced use cases. Here&#39;s a look at the core features that make &lt;strong&gt;FastStream&lt;/strong&gt; a go-to framework for modern, data-centric microservices.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multiple Brokers&lt;/strong&gt;: &lt;strong&gt;FastStream&lt;/strong&gt; provides a unified API to work across multiple message brokers (&lt;a href=&#34;https://kafka.apache.org/&#34;&gt;&lt;strong&gt;Kafka&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.rabbitmq.com/&#34;&gt;&lt;strong&gt;RabbitMQ&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://nats.io/&#34;&gt;&lt;strong&gt;NATS&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://redis.io/&#34;&gt;&lt;strong&gt;Redis&lt;/strong&gt;&lt;/a&gt; support)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airtai/faststream/main/#writing-app-code&#34;&gt;&lt;strong&gt;Pydantic Validation&lt;/strong&gt;&lt;/a&gt;: Leverage &lt;a href=&#34;https://docs.pydantic.dev/&#34;&gt;&lt;strong&gt;Pydantic&#39;s&lt;/strong&gt;&lt;/a&gt; validation capabilities to serialize and validate incoming messages&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airtai/faststream/main/#project-documentation&#34;&gt;&lt;strong&gt;Automatic Docs&lt;/strong&gt;&lt;/a&gt;: Stay ahead with automatic &lt;a href=&#34;https://www.asyncapi.com/&#34;&gt;&lt;strong&gt;AsyncAPI&lt;/strong&gt;&lt;/a&gt; documentation&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Intuitive&lt;/strong&gt;: Full-typed editor support makes your development experience smooth, catching errors before they reach runtime&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airtai/faststream/main/#dependencies&#34;&gt;&lt;strong&gt;Powerful Dependency Injection System&lt;/strong&gt;&lt;/a&gt;: Manage your service dependencies efficiently with &lt;strong&gt;FastStream&lt;/strong&gt;&#39;s built-in DI system&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airtai/faststream/main/#testing-the-service&#34;&gt;&lt;strong&gt;Testable&lt;/strong&gt;&lt;/a&gt;: Supports in-memory tests, making your CI/CD pipeline faster and more reliable&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: Use extensions for lifespans, custom serialization and middleware&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airtai/faststream/main/#any-framework&#34;&gt;&lt;strong&gt;Integrations&lt;/strong&gt;&lt;/a&gt;: &lt;strong&gt;FastStream&lt;/strong&gt; is fully compatible with any HTTP framework you want (&lt;a href=&#34;https://raw.githubusercontent.com/airtai/faststream/main/#fastapi-plugin&#34;&gt;&lt;strong&gt;FastAPI&lt;/strong&gt;&lt;/a&gt; especially)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;That&#39;s &lt;strong&gt;FastStream&lt;/strong&gt; in a nutshell—easy, efficient, and powerful. Whether you&#39;re just starting with streaming microservices or looking to scale, &lt;strong&gt;FastStream&lt;/strong&gt; has got you covered.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href=&#34;https://faststream.airt.ai/latest/&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://faststream.airt.ai/latest/&#34;&gt;https://faststream.airt.ai/latest/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FastStream&lt;/strong&gt; is a new package based on the ideas and experiences gained from &lt;a href=&#34;https://github.com/airtai/fastkafka&#34;&gt;&lt;strong&gt;FastKafka&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/lancetnik/propan&#34;&gt;&lt;strong&gt;Propan&lt;/strong&gt;&lt;/a&gt;. By joining our forces, we picked up the best from both packages and created a unified way to write services capable of processing streamed data regardless of the underlying protocol. We&#39;ll continue to maintain both packages, but new development will be in this project. If you are starting a new service, this package is the recommended way to do it.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FastStream&lt;/strong&gt; works on &lt;strong&gt;Linux&lt;/strong&gt;, &lt;strong&gt;macOS&lt;/strong&gt;, &lt;strong&gt;Windows&lt;/strong&gt; and most &lt;strong&gt;Unix&lt;/strong&gt;-style operating systems. You can install it with &lt;code&gt;pip&lt;/code&gt; as usual:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install faststream[kafka]&#xA;# or&#xA;pip install faststream[rabbit]&#xA;# or&#xA;pip install faststream[nats]&#xA;# or&#xA;pip install faststream[redis]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default &lt;strong&gt;FastStream&lt;/strong&gt; uses &lt;strong&gt;PydanticV2&lt;/strong&gt; written in &lt;strong&gt;Rust&lt;/strong&gt;, but you can downgrade it manually, if your platform has no &lt;strong&gt;Rust&lt;/strong&gt; support - &lt;strong&gt;FastStream&lt;/strong&gt; will work correctly with &lt;strong&gt;PydanticV1&lt;/strong&gt; as well.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Writing app code&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FastStream&lt;/strong&gt; brokers provide convenient function decorators &lt;code&gt;@broker.subscriber&lt;/code&gt; and &lt;code&gt;@broker.publisher&lt;/code&gt; to allow you to delegate the actual process of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;consuming and producing data to Event queues, and&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;decoding and encoding JSON-encoded messages&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These decorators make it easy to specify the processing logic for your consumers and producers, allowing you to focus on the core business logic of your application without worrying about the underlying integration.&lt;/p&gt; &#xA;&lt;p&gt;Also, &lt;strong&gt;FastStream&lt;/strong&gt; uses &lt;a href=&#34;https://docs.pydantic.dev/&#34;&gt;&lt;strong&gt;Pydantic&lt;/strong&gt;&lt;/a&gt; to parse input JSON-encoded data into Python objects, making it easy to work with structured data in your applications, so you can serialize your input messages just using type annotations.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example Python app using &lt;strong&gt;FastStream&lt;/strong&gt; that consumes data from an incoming data stream and outputs the data to another one:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from faststream import FastStream&#xA;from faststream.kafka import KafkaBroker&#xA;# from faststream.rabbit import RabbitBroker&#xA;# from faststream.nats import NatsBroker&#xA;# from faststream.redis import RedisBroker&#xA;&#xA;broker = KafkaBroker(&#34;localhost:9092&#34;)&#xA;# broker = RabbitBroker(&#34;amqp://guest:guest@localhost:5672/&#34;)&#xA;# broker = NatsBroker(&#34;nats://localhost:4222/&#34;)&#xA;# broker = RedisBroker(&#34;redis://localhost:6379/&#34;)&#xA;&#xA;app = FastStream(broker)&#xA;&#xA;@broker.subscriber(&#34;in&#34;)&#xA;@broker.publisher(&#34;out&#34;)&#xA;async def handle_msg(user: str, user_id: int) -&amp;gt; str:&#xA;    return f&#34;User: {user_id} - {user} registered&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also, &lt;strong&gt;Pydantic&lt;/strong&gt;’s &lt;a href=&#34;https://docs.pydantic.dev/usage/models/&#34;&gt;&lt;code&gt;BaseModel&lt;/code&gt;&lt;/a&gt; class allows you to define messages using a declarative syntax, making it easy to specify the fields and types of your messages.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pydantic import BaseModel, Field, PositiveInt&#xA;from faststream import FastStream&#xA;from faststream.kafka import KafkaBroker&#xA;&#xA;broker = KafkaBroker(&#34;localhost:9092&#34;)&#xA;app = FastStream(broker)&#xA;&#xA;class User(BaseModel):&#xA;    user: str = Field(..., examples=[&#34;John&#34;])&#xA;    user_id: PositiveInt = Field(..., examples=[&#34;1&#34;])&#xA;&#xA;@broker.subscriber(&#34;in&#34;)&#xA;@broker.publisher(&#34;out&#34;)&#xA;async def handle_msg(data: User) -&amp;gt; str:&#xA;    return f&#34;User: {data.user} - {data.user_id} registered&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Testing the service&lt;/h2&gt; &#xA;&lt;p&gt;The service can be tested using the &lt;code&gt;TestBroker&lt;/code&gt; context managers, which, by default, puts the Broker into &#34;testing mode&#34;.&lt;/p&gt; &#xA;&lt;p&gt;The Tester will redirect your &lt;code&gt;subscriber&lt;/code&gt; and &lt;code&gt;publisher&lt;/code&gt; decorated functions to the InMemory brokers, allowing you to quickly test your app without the need for a running broker and all its dependencies.&lt;/p&gt; &#xA;&lt;p&gt;Using pytest, the test for our service would look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Code above omitted 👆&#xA;&#xA;import pytest&#xA;import pydantic&#xA;from faststream.kafka import TestKafkaBroker&#xA;&#xA;&#xA;@pytest.mark.asyncio&#xA;async def test_correct():&#xA;    async with TestKafkaBroker(broker) as br:&#xA;        await br.publish({&#xA;            &#34;user&#34;: &#34;John&#34;,&#xA;            &#34;user_id&#34;: 1,&#xA;        }, &#34;in&#34;)&#xA;&#xA;@pytest.mark.asyncio&#xA;async def test_invalid():&#xA;    async with TestKafkaBroker(broker) as br:&#xA;        with pytest.raises(pydantic.ValidationError):&#xA;            await br.publish(&#34;wrong message&#34;, &#34;in&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running the application&lt;/h2&gt; &#xA;&lt;p&gt;The application can be started using built-in &lt;strong&gt;FastStream&lt;/strong&gt; CLI command.&lt;/p&gt; &#xA;&lt;p&gt;Before running the service, install &lt;strong&gt;FastStream CLI&lt;/strong&gt; using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install &#34;faststream[cli]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the service, use the &lt;strong&gt;FastStream CLI&lt;/strong&gt; command and pass the module (in this case, the file where the app implementation is located) and the app symbol to the command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;faststream run basic:app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running the command, you should see the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;INFO     - FastStream app starting...&#xA;INFO     - input_data |            - `HandleMsg` waiting for messages&#xA;INFO     - FastStream app started successfully! To exit press CTRL+C&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also, &lt;strong&gt;FastStream&lt;/strong&gt; provides you with a great hot reload feature to improve your Development Experience&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;faststream run basic:app --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And multiprocessing horizontal scaling feature as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;faststream run basic:app --workers 3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can learn more about &lt;strong&gt;CLI&lt;/strong&gt; features &lt;a href=&#34;https://faststream.airt.ai/latest/getting-started/cli/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Project Documentation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FastStream&lt;/strong&gt; automatically generates documentation for your project according to the &lt;a href=&#34;https://www.asyncapi.com/&#34;&gt;&lt;strong&gt;AsyncAPI&lt;/strong&gt;&lt;/a&gt; specification. You can work with both generated artifacts and place a web view of your documentation on resources available to related teams.&lt;/p&gt; &#xA;&lt;p&gt;The availability of such documentation significantly simplifies the integration of services: you can immediately see what channels and message formats the application works with. And most importantly, it won&#39;t cost anything - &lt;strong&gt;FastStream&lt;/strong&gt; has already created the docs for you!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/airtai/faststream/raw/main/docs/docs/assets/img/AsyncAPI-basic-html-short.png?raw=true&#34; alt=&#34;HTML-page&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FastStream&lt;/strong&gt; (thanks to &lt;a href=&#34;https://lancetnik.github.io/FastDepends/&#34;&gt;&lt;strong&gt;FastDepends&lt;/strong&gt;&lt;/a&gt;) has a dependency management system similar to &lt;code&gt;pytest fixtures&lt;/code&gt; and &lt;code&gt;FastAPI Depends&lt;/code&gt; at the same time. Function arguments declare which dependencies you want are needed, and a special decorator delivers them from the global Context object.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from faststream import Depends, Logger&#xA;&#xA;async def base_dep(user_id: int) -&amp;gt; bool:&#xA;    return True&#xA;&#xA;@broker.subscriber(&#34;in-test&#34;)&#xA;async def base_handler(user: str,&#xA;                       logger: Logger,&#xA;                       dep: bool = Depends(base_dep)):&#xA;    assert dep is True&#xA;    logger.info(user)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;HTTP Frameworks integrations&lt;/h2&gt; &#xA;&lt;h3&gt;Any Framework&lt;/h3&gt; &#xA;&lt;p&gt;You can use &lt;strong&gt;FastStream&lt;/strong&gt; &lt;code&gt;MQBrokers&lt;/code&gt; without a &lt;code&gt;FastStream&lt;/code&gt; application. Just &lt;em&gt;start&lt;/em&gt; and &lt;em&gt;stop&lt;/em&gt; them according to your application&#39;s lifespan.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from aiohttp import web&#xA;&#xA;from faststream.kafka import KafkaBroker&#xA;&#xA;broker = KafkaBroker(&#34;localhost:9092&#34;)&#xA;&#xA;@broker.subscriber(&#34;test&#34;)&#xA;async def base_handler(body):&#xA;    print(body)&#xA;&#xA;async def start_broker(app):&#xA;    await broker.start()&#xA;&#xA;async def stop_broker(app):&#xA;    await broker.close()&#xA;&#xA;async def hello(request):&#xA;    return web.Response(text=&#34;Hello, world&#34;)&#xA;&#xA;app = web.Application()&#xA;app.add_routes([web.get(&#34;/&#34;, hello)])&#xA;app.on_startup.append(start_broker)&#xA;app.on_cleanup.append(stop_broker)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    web.run_app(app)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;strong&gt;FastAPI&lt;/strong&gt; Plugin&lt;/h3&gt; &#xA;&lt;p&gt;Also, &lt;strong&gt;FastStream&lt;/strong&gt; can be used as part of &lt;strong&gt;FastAPI&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Just import a &lt;strong&gt;StreamRouter&lt;/strong&gt; you need and declare the message handler with the same &lt;code&gt;@router.subscriber(...)&lt;/code&gt; and &lt;code&gt;@router.publisher(...)&lt;/code&gt; decorators.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastapi import FastAPI&#xA;from pydantic import BaseModel&#xA;&#xA;from faststream.kafka.fastapi import KafkaRouter&#xA;&#xA;router = KafkaRouter(&#34;localhost:9092&#34;)&#xA;&#xA;class Incoming(BaseModel):&#xA;    m: dict&#xA;&#xA;@router.subscriber(&#34;test&#34;)&#xA;@router.publisher(&#34;response&#34;)&#xA;async def hello(m: Incoming):&#xA;    return {&#34;response&#34;: &#34;Hello, world!&#34;}&#xA;&#xA;app = FastAPI()&#xA;app.include_router(router)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More integration features can be found &lt;a href=&#34;https://faststream.airt.ai/latest/getting-started/integrations/fastapi/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Stay in touch&lt;/h2&gt; &#xA;&lt;p&gt;Please show your support and stay in touch by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;giving our &lt;a href=&#34;https://github.com/airtai/faststream/&#34;&gt;GitHub repository&lt;/a&gt; a star, and&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;joining our &lt;a href=&#34;https://discord.gg/qFm6aSqq59&#34;&gt;EN Discord server&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;joining our &lt;a href=&#34;https://t.me/python_faststream&#34;&gt;RU Telegram group&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Your support helps us to stay in touch with you and encourages us to continue developing and improving the framework. Thank you for your support!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to all of these amazing people who made the project better!&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/airtai/faststream/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=airtai/faststream&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>naver/mast3r</title>
    <updated>2024-10-30T01:28:30Z</updated>
    <id>tag:github.com,2024-10-30:/naver/mast3r</id>
    <link href="https://github.com/naver/mast3r" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Grounding Image Matching in 3D with MASt3R&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naver/mast3r/main/assets/mast3r.jpg&#34; alt=&#34;banner&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Official implementation of &lt;code&gt;Grounding Image Matching in 3D with MASt3R&lt;/code&gt;&lt;br&gt; [&lt;a href=&#34;https://europe.naverlabs.com/blog/mast3r-matching-and-stereo-3d-reconstruction/&#34;&gt;Project page&lt;/a&gt;], [&lt;a href=&#34;https://arxiv.org/abs/2406.09756&#34;&gt;MASt3R arxiv&lt;/a&gt;], [&lt;a href=&#34;https://arxiv.org/abs/2312.14132&#34;&gt;DUSt3R arxiv&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naver/mast3r/main/assets/examples.jpg&#34; alt=&#34;Example of matching results obtained from MASt3R&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naver/mast3r/main/assets/mast3r_archi.jpg&#34; alt=&#34;High level overview of MASt3R&#39;s architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{mast3r_arxiv24,&#xA;      title={Grounding Image Matching in 3D with MASt3R}, &#xA;      author={Vincent Leroy and Yohann Cabon and Jerome Revaud},&#xA;      year={2024},&#xA;      eprint={2406.09756},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;@inproceedings{dust3r_cvpr24,&#xA;      title={DUSt3R: Geometric 3D Vision Made Easy}, &#xA;      author={Shuzhe Wang and Vincent Leroy and Yohann Cabon and Boris Chidlovskii and Jerome Revaud},&#xA;      booktitle = {CVPR},&#xA;      year = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#get-started&#34;&gt;Get Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#checkpoints&#34;&gt;Checkpoints&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#interactive-demo&#34;&gt;Interactive demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#interactive-demo-with-docker&#34;&gt;Interactive demo with docker&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#training&#34;&gt;Training&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#our-hyperparameters&#34;&gt;Our Hyperparameters&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#visual-localization&#34;&gt;Visual Localization&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#dataset-preparation&#34;&gt;Dataset Preparation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#example-commands&#34;&gt;Example Commands&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code is distributed under the CC BY-NC-SA 4.0 License. See &lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Copyright (C) 2024-present Naver Corporation. All rights reserved.&#xA;# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone MASt3R.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/naver/mast3r&#xA;cd mast3r&#xA;# if you have already cloned mast3r:&#xA;# git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create the environment, here we show an example using conda.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n mast3r python=3.11 cmake=3.14.0&#xA;conda activate mast3r &#xA;conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia  # use the correct version of cuda for your system&#xA;pip install -r requirements.txt&#xA;pip install -r dust3r/requirements.txt&#xA;# Optional: you can also install additional packages to:&#xA;# - add support for HEIC images&#xA;# - add required packages for visloc.py&#xA;pip install -r dust3r/requirements_optional.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Optional, compile the cuda kernels for RoPE (as in CroCo v2).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# DUST3R relies on RoPE positional embeddings for which you can compile some cuda kernels for faster runtime.&#xA;cd dust3r/croco/models/curope/&#xA;python setup.py build_ext --inplace&#xA;cd ../../../../&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;You can obtain the checkpoints by two ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;You can use our huggingface_hub integration: the models will be downloaded automatically.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Otherwise, We provide several pre-trained models:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Modelname&lt;/th&gt; &#xA;   &lt;th&gt;Training resolutions&lt;/th&gt; &#xA;   &lt;th&gt;Head&lt;/th&gt; &#xA;   &lt;th&gt;Encoder&lt;/th&gt; &#xA;   &lt;th&gt;Decoder&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://download.europe.naverlabs.com/ComputerVision/MASt3R/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth&#34;&gt;&lt;code&gt;MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;512x384, 512x336, 512x288, 512x256, 512x160&lt;/td&gt; &#xA;   &lt;td&gt;CatMLP+DPT&lt;/td&gt; &#xA;   &lt;td&gt;ViT-L&lt;/td&gt; &#xA;   &lt;td&gt;ViT-B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can check the hyperparameters we used to train these models in the &lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#our-hyperparameters&#34;&gt;section: Our Hyperparameters&lt;/a&gt; Make sure to check license of the datasets we used.&lt;/p&gt; &#xA;&lt;p&gt;To download a specific model, for example &lt;code&gt;MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p checkpoints/&#xA;wget https://download.europe.naverlabs.com/ComputerVision/MASt3R/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth -P checkpoints/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For these checkpoints, make sure to agree to the license of all the training datasets we used, in addition to CC-BY-NC-SA 4.0. The mapfree dataset license in particular is very restrictive. For more information, check &lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/CHECKPOINTS_NOTICE&#34;&gt;CHECKPOINTS_NOTICE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Interactive demo&lt;/h3&gt; &#xA;&lt;p&gt;We made one huggingface space running the new sparse global alignment in a simplified demo for small scenes: &lt;a href=&#34;https://huggingface.co/spaces/naver/MASt3R&#34;&gt;naver/MASt3R&lt;/a&gt; There are two demos available to run locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;demo.py is the updated demo for MASt3R. It uses our new sparse global alignment method that allows you to reconstruct larger scenes&#xA;&#xA;python3 demo.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric&#xA;&#xA;# Use --weights to load a checkpoint from a local file, eg --weights checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth&#xA;# Use --local_network to make it accessible on the local network, or --server_name to specify the url manually&#xA;# Use --server_port to change the port, by default it will search for an available port starting at 7860&#xA;# Use --device to use a different device, by default it&#39;s &#34;cuda&#34;&#xA;&#xA;demo_dust3r_ga.py is the same demo as in dust3r (+ compatibility for MASt3R models)&#xA;see https://github.com/naver/dust3r?tab=readme-ov-file#interactive-demo for details&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Interactive demo with docker&lt;/h3&gt; &#xA;&lt;p&gt;To run MASt3R using Docker, including with NVIDIA CUDA support, follow these instructions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Docker&lt;/strong&gt;: If not already installed, download and install &lt;code&gt;docker&lt;/code&gt; and &lt;code&gt;docker compose&lt;/code&gt; from the &lt;a href=&#34;https://www.docker.com/get-started&#34;&gt;Docker website&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install NVIDIA Docker Toolkit&lt;/strong&gt;: For GPU support, install the NVIDIA Docker toolkit from the &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#34;&gt;Nvidia website&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Build the Docker image and run it&lt;/strong&gt;: &lt;code&gt;cd&lt;/code&gt; into the &lt;code&gt;./docker&lt;/code&gt; directory and run the following commands:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docker&#xA;bash run.sh --with-cuda --model_name=&#34;MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if you want to run the demo without CUDA support, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docker&#xA;bash run.sh --model_name=&#34;MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, &lt;code&gt;demo.py&lt;/code&gt; is launched with the option &lt;code&gt;--local_network&lt;/code&gt;.&lt;br&gt; Visit &lt;code&gt;http://localhost:7860/&lt;/code&gt; to access the web UI (or replace &lt;code&gt;localhost&lt;/code&gt; with the machine&#39;s name to access it from the network).&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;run.sh&lt;/code&gt; will launch docker-compose using either the &lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/docker/docker-compose-cuda.yml&#34;&gt;docker-compose-cuda.yml&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/docker/docker-compose-cpu.yml&#34;&gt;docker-compose-cpu.ym&lt;/a&gt; config file, then it starts the demo using &lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/docker/files/entrypoint.sh&#34;&gt;entrypoint.sh&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naver/mast3r/main/assets/demo.jpg&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mast3r.model import AsymmetricMASt3R&#xA;from mast3r.fast_nn import fast_reciprocal_NNs&#xA;&#xA;import mast3r.utils.path_to_dust3r&#xA;from dust3r.inference import inference&#xA;from dust3r.utils.image import load_images&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    device = &#39;cuda&#39;&#xA;    schedule = &#39;cosine&#39;&#xA;    lr = 0.01&#xA;    niter = 300&#xA;&#xA;    model_name = &#34;naver/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric&#34;&#xA;    # you can put the path to a local checkpoint in model_name if needed&#xA;    model = AsymmetricMASt3R.from_pretrained(model_name).to(device)&#xA;    images = load_images([&#39;dust3r/croco/assets/Chateau1.png&#39;, &#39;dust3r/croco/assets/Chateau2.png&#39;], size=512)&#xA;    output = inference([tuple(images)], model, device, batch_size=1, verbose=False)&#xA;&#xA;    # at this stage, you have the raw dust3r predictions&#xA;    view1, pred1 = output[&#39;view1&#39;], output[&#39;pred1&#39;]&#xA;    view2, pred2 = output[&#39;view2&#39;], output[&#39;pred2&#39;]&#xA;&#xA;    desc1, desc2 = pred1[&#39;desc&#39;].squeeze(0).detach(), pred2[&#39;desc&#39;].squeeze(0).detach()&#xA;&#xA;    # find 2D-2D matches between the two images&#xA;    matches_im0, matches_im1 = fast_reciprocal_NNs(desc1, desc2, subsample_or_initxy1=8,&#xA;                                                   device=device, dist=&#39;dot&#39;, block_size=2**13)&#xA;&#xA;    # ignore small border around the edge&#xA;    H0, W0 = view1[&#39;true_shape&#39;][0]&#xA;    valid_matches_im0 = (matches_im0[:, 0] &amp;gt;= 3) &amp;amp; (matches_im0[:, 0] &amp;lt; int(W0) - 3) &amp;amp; (&#xA;        matches_im0[:, 1] &amp;gt;= 3) &amp;amp; (matches_im0[:, 1] &amp;lt; int(H0) - 3)&#xA;&#xA;    H1, W1 = view2[&#39;true_shape&#39;][0]&#xA;    valid_matches_im1 = (matches_im1[:, 0] &amp;gt;= 3) &amp;amp; (matches_im1[:, 0] &amp;lt; int(W1) - 3) &amp;amp; (&#xA;        matches_im1[:, 1] &amp;gt;= 3) &amp;amp; (matches_im1[:, 1] &amp;lt; int(H1) - 3)&#xA;&#xA;    valid_matches = valid_matches_im0 &amp;amp; valid_matches_im1&#xA;    matches_im0, matches_im1 = matches_im0[valid_matches], matches_im1[valid_matches]&#xA;&#xA;    # visualize a few matches&#xA;    import numpy as np&#xA;    import torch&#xA;    import torchvision.transforms.functional&#xA;    from matplotlib import pyplot as pl&#xA;&#xA;    n_viz = 20&#xA;    num_matches = matches_im0.shape[0]&#xA;    match_idx_to_viz = np.round(np.linspace(0, num_matches - 1, n_viz)).astype(int)&#xA;    viz_matches_im0, viz_matches_im1 = matches_im0[match_idx_to_viz], matches_im1[match_idx_to_viz]&#xA;&#xA;    image_mean = torch.as_tensor([0.5, 0.5, 0.5], device=&#39;cpu&#39;).reshape(1, 3, 1, 1)&#xA;    image_std = torch.as_tensor([0.5, 0.5, 0.5], device=&#39;cpu&#39;).reshape(1, 3, 1, 1)&#xA;&#xA;    viz_imgs = []&#xA;    for i, view in enumerate([view1, view2]):&#xA;        rgb_tensor = view[&#39;img&#39;] * image_std + image_mean&#xA;        viz_imgs.append(rgb_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy())&#xA;&#xA;    H0, W0, H1, W1 = *viz_imgs[0].shape[:2], *viz_imgs[1].shape[:2]&#xA;    img0 = np.pad(viz_imgs[0], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)), &#39;constant&#39;, constant_values=0)&#xA;    img1 = np.pad(viz_imgs[1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)), &#39;constant&#39;, constant_values=0)&#xA;    img = np.concatenate((img0, img1), axis=1)&#xA;    pl.figure()&#xA;    pl.imshow(img)&#xA;    cmap = pl.get_cmap(&#39;jet&#39;)&#xA;    for i in range(n_viz):&#xA;        (x0, y0), (x1, y1) = viz_matches_im0[i].T, viz_matches_im1[i].T&#xA;        pl.plot([x0, x1 + W0], [y0, y1], &#39;-+&#39;, color=cmap(i / (n_viz - 1)), scalex=False, scaley=False)&#xA;    pl.show(block=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naver/mast3r/main/assets/matching.jpg&#34; alt=&#34;matching example on croco pair&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;In this section, we present a short demonstration to get started with training MASt3R.&lt;/p&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/naver/dust3r?tab=readme-ov-file#datasets&#34;&gt;Datasets section in DUSt3R&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;p&gt;Like for the DUSt3R training demo, we&#39;re going to download and prepare the same subset of &lt;a href=&#34;https://github.com/facebookresearch/co3d&#34;&gt;CO3Dv2&lt;/a&gt; - &lt;a href=&#34;https://github.com/facebookresearch/co3d/raw/main/LICENSE&#34;&gt;Creative Commons Attribution-NonCommercial 4.0 International&lt;/a&gt; and launch the training code on it. It is the exact same process as DUSt3R. The demo model will be trained for a few epochs on a very small dataset. It will not be very good.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download and prepare the co3d subset&#xA;mkdir -p data/co3d_subset&#xA;cd data/co3d_subset&#xA;git clone https://github.com/facebookresearch/co3d&#xA;cd co3d&#xA;python3 ./co3d/download_dataset.py --download_folder ../ --single_sequence_subset&#xA;rm ../*.zip&#xA;cd ../../..&#xA;&#xA;python3 datasets_preprocess/preprocess_co3d.py --co3d_dir data/co3d_subset --output_dir data/co3d_subset_processed  --single_sequence_subset&#xA;&#xA;# download the pretrained dust3r checkpoint&#xA;mkdir -p checkpoints/&#xA;wget https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth -P checkpoints/&#xA;&#xA;# for this example we&#39;ll do fewer epochs, for the actual hyperparameters we used in the paper, see the next section: &#34;Our Hyperparameters&#34;&#xA;torchrun --nproc_per_node=4 train.py \&#xA;    --train_dataset &#34;1000 @ Co3d(split=&#39;train&#39;, ROOT=&#39;data/co3d_subset_processed&#39;, aug_crop=&#39;auto&#39;, aug_monocular=0.005, aug_rot90=&#39;diff&#39;, mask_bg=&#39;rand&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], n_corres=8192, nneg=0.5, transform=ColorJitter)&#34; \&#xA;    --test_dataset &#34;100 @ Co3d(split=&#39;test&#39;, ROOT=&#39;data/co3d_subset_processed&#39;, resolution=(512,384), n_corres=1024, seed=777)&#34; \&#xA;    --model &#34;AsymmetricMASt3R(pos_embed=&#39;RoPE100&#39;, patch_embed_cls=&#39;ManyAR_PatchEmbed&#39;, img_size=(512, 512), head_type=&#39;catmlp+dpt&#39;, output_mode=&#39;pts3d+desc24&#39;, depth_mode=(&#39;exp&#39;, -inf, inf), conf_mode=(&#39;exp&#39;, 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12, two_confs=True)&#34; \&#xA;    --train_criterion &#34;ConfLoss(Regr3D(L21, norm_mode=&#39;?avg_dis&#39;), alpha=0.2) + 0.075*ConfMatchingLoss(MatchingLoss(InfoNCE(mode=&#39;proper&#39;, temperature=0.05), negatives_padding=0, blocksize=8192), alpha=10.0, confmode=&#39;mean&#39;)&#34; \&#xA;    --test_criterion &#34;Regr3D_ScaleShiftInv(L21, norm_mode=&#39;?avg_dis&#39;, gt_scale=True, sky_loss_value=0) + -1.*MatchingLoss(APLoss(nq=&#39;torch&#39;, fp=torch.float16), negatives_padding=12288)&#34; \&#xA;    --pretrained &#34;checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth&#34; \&#xA;    --lr 0.0001 --min_lr 1e-06 --warmup_epochs 1 --epochs 10 --batch_size 4 --accum_iter 4 \&#xA;    --save_freq 1 --keep_freq 5 --eval_freq 1 --disable_cudnn_benchmark \&#xA;    --output_dir &#34;checkpoints/mast3r_demo&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Our Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;We didn&#39;t release all the training datasets, but here are the commands we used for training our models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric - train mast3r with metric regression and matching loss&#xA;# we used cosxl to generate variations of DL3DV: &#34;foggy&#34;, &#34;night&#34;, &#34;rainy&#34;, &#34;snow&#34;, &#34;sunny&#34; but we were not convinced by it.&#xA;&#xA;torchrun --nproc_per_node=8 train.py \&#xA;    --train_dataset &#34;57_000 @ Habitat512(1_000_000, split=&#39;train&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 68_400 @ BlendedMVS(split=&#39;train&#39;, mask_sky=True, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 68_400 @ MegaDepth(split=&#39;train&#39;, mask_sky=True, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 45_600 @ ARKitScenes(split=&#39;train&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 22_800 @ Co3d(split=&#39;train&#39;, mask_bg=&#39;rand&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 22_800 @ StaticThings3D(mask_bg=&#39;rand&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 45_600 @ ScanNetpp(split=&#39;train&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 45_600 @ TartanAir(pairs_subset=&#39;&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 4_560 @ UnrealStereo4K(resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 1_140 @ VirtualKitti(optical_center_is_centered=True, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 22_800 @ WildRgbd(split=&#39;train&#39;, mask_bg=&#39;rand&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 145_920 @ NianticMapFree(split=&#39;train&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 57_000 @ DL3DV(split=&#39;nlight&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 57_000 @ DL3DV(split=&#39;not-nlight&#39;, cosxl_augmentations=None, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 34_200 @ InternalUnreleasedDataset(resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5)&#34; \&#xA;    --test_dataset &#34;Habitat512(1_000, split=&#39;val&#39;, resolution=(512,384), seed=777, n_corres=1024) + 1_000 @ BlendedMVS(split=&#39;val&#39;, resolution=(512,384), mask_sky=True, seed=777, n_corres=1024) + 1_000 @ ARKitScenes(split=&#39;test&#39;, resolution=(512,384), seed=777, n_corres=1024) + 1_000 @ MegaDepth(split=&#39;val&#39;, mask_sky=True, resolution=(512,336), seed=777, n_corres=1024) + 1_000 @ Co3d(split=&#39;test&#39;, resolution=(512,384), mask_bg=&#39;rand&#39;, seed=777, n_corres=1024)&#34; \&#xA;    --model &#34;AsymmetricMASt3R(pos_embed=&#39;RoPE100&#39;, patch_embed_cls=&#39;ManyAR_PatchEmbed&#39;, img_size=(512, 512), head_type=&#39;catmlp+dpt&#39;, output_mode=&#39;pts3d+desc24&#39;, depth_mode=(&#39;exp&#39;, -inf, inf), conf_mode=(&#39;exp&#39;, 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12, two_confs=True, desc_conf_mode=(&#39;exp&#39;, 0, inf))&#34; \&#xA;    --train_criterion &#34;ConfLoss(Regr3D(L21, norm_mode=&#39;?avg_dis&#39;), alpha=0.2, loss_in_log=False) + 0.075*ConfMatchingLoss(MatchingLoss(InfoNCE(mode=&#39;proper&#39;, temperature=0.05), negatives_padding=0, blocksize=8192), alpha=10.0, confmode=&#39;mean&#39;)&#34; \&#xA;    --test_criterion &#34;Regr3D(L21, norm_mode=&#39;?avg_dis&#39;, gt_scale=True, sky_loss_value=0) + -1.*MatchingLoss(APLoss(nq=&#39;torch&#39;, fp=torch.float16), negatives_padding=12288)&#34; \&#xA;    --pretrained &#34;checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth&#34; \&#xA;    --lr 0.0001 --min_lr 1e-06 --warmup_epochs 8 --epochs 50 --batch_size 4 --accum_iter 2 \&#xA;    --save_freq 1 --keep_freq 5 --eval_freq 1 --print_freq=10 --disable_cudnn_benchmark \&#xA;    --output_dir &#34;checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Visual Localization&lt;/h2&gt; &#xA;&lt;h3&gt;Dataset preparation&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/naver/dust3r/raw/main/dust3r_visloc/README.md#dataset-preparation&#34;&gt;Visloc section in DUSt3R&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Example Commands&lt;/h3&gt; &#xA;&lt;p&gt;With &lt;code&gt;visloc.py&lt;/code&gt; you can run our visual localization experiments on Aachen-Day-Night, InLoc, Cambridge Landmarks and 7 Scenes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Aachen-Day-Night-v1.1:&#xA;# scene in &#39;day&#39; &#39;night&#39;&#xA;# scene can also be &#39;all&#39;&#xA;python3 visloc.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric --dataset &#34;VislocAachenDayNight(&#39;/path/to/prepared/Aachen-Day-Night-v1.1/&#39;, subscene=&#39;${scene}&#39;, pairsfile=&#39;fire_top50&#39;, topk=20)&#34; --pixel_tol 5 --pnp_mode poselib --reprojection_error_diag_ratio 0.008 --output_dir /path/to/output/Aachen-Day-Night-v1.1/${scene}/loc&#xA;&#xA;# or with coarse to fine:&#xA;&#xA;python3 visloc.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric --dataset &#34;VislocAachenDayNight(&#39;/path/to/prepared/Aachen-Day-Night-v1.1/&#39;, subscene=&#39;${scene}&#39;, pairsfile=&#39;fire_top50&#39;, topk=20)&#34; --pixel_tol 5 --pnp_mode poselib --reprojection_error_diag_ratio 0.008 --output_dir /path/to/output/Aachen-Day-Night-v1.1/${scene}/loc --coarse_to_fine --max_batch_size 48 --c2f_crop_with_homography&#xA;&#xA;# InLoc&#xA;python3 visloc.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric --dataset &#34;VislocInLoc(&#39;/path/to/prepared/InLoc/&#39;, pairsfile=&#39;pairs-query-netvlad40-temporal&#39;, topk=20)&#34; --pixel_tol 5 --pnp_mode poselib --reprojection_error_diag_ratio 0.008 --output_dir /path/to/output/InLoc/loc&#xA;&#xA;# or with coarse to fine:&#xA;&#xA;python3 visloc.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric --dataset &#34;VislocInLoc(&#39;/path/to/prepared/InLoc/&#39;, pairsfile=&#39;pairs-query-netvlad40-temporal&#39;, topk=20)&#34; --pixel_tol 5 --pnp_mode poselib --reprojection_error_diag_ratio 0.008 --output_dir /path/to/output/InLoc/loc --coarse_to_fine --max_image_size 1200 --max_batch_size 48 --c2f_crop_with_homography&#xA;&#xA;# 7-scenes:&#xA;# scene in &#39;chess&#39; &#39;fire&#39; &#39;heads&#39; &#39;office&#39; &#39;pumpkin&#39; &#39;redkitchen&#39; &#39;stairs&#39;&#xA;python3 visloc.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric --dataset &#34;VislocSevenScenes(&#39;/path/to/prepared/7-scenes/&#39;, subscene=&#39;${scene}&#39;, pairsfile=&#39;APGeM-LM18_top20&#39;, topk=1)&#34; --pixel_tol 5 --pnp_mode poselib --reprojection_error_diag_ratio 0.008 --output_dir /path/to/output/7-scenes/${scene}/loc&#xA;&#xA;# Cambridge Landmarks:&#xA;# scene in &#39;ShopFacade&#39; &#39;GreatCourt&#39; &#39;KingsCollege&#39; &#39;OldHospital&#39; &#39;StMarysChurch&#39;&#xA;python3 visloc.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric --dataset &#34;VislocCambridgeLandmarks(&#39;/path/to/prepared/Cambridge_Landmarks/&#39;, subscene=&#39;${scene}&#39;, pairsfile=&#39;APGeM-LM18_top50&#39;, topk=20)&#34; --pixel_tol 5 --pnp_mode poselib --reprojection_error_diag_ratio 0.008 --output_dir /path/to/output/Cambridge_Landmarks/${scene}/loc&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Marker-Inc-Korea/AutoRAG</title>
    <updated>2024-10-30T01:28:30Z</updated>
    <id>tag:github.com,2024-10-30:/Marker-Inc-Korea/AutoRAG</id>
    <link href="https://github.com/Marker-Inc-Korea/AutoRAG" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AutoML tool for RAG&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AutoRAG&lt;/h1&gt; &#xA;&lt;p&gt;RAG AutoML tool for automatically finding an optimal RAG pipeline for your data.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/6bab243d-a4b3-431a-8ac0-fe17336ab4de&#34; alt=&#34;Thumbnail&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/discord/1204010535272587264&#34; alt=&#34;Discord&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/dm/AutoRAG&#34; alt=&#34;PyPI - Downloads&#34;&gt; &lt;a href=&#34;https://www.linkedin.com/company/104375108/admin/dashboard/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/LinkedIn-Connect-blue?style=flat-square&amp;amp;logo=linkedin&#34; alt=&#34;LinkedIn&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/AutoRAG_HQ&#34; alt=&#34;X (formerly Twitter) Follow&#34;&gt; &lt;a href=&#34;https://huggingface.co/AutoRAG&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hugging%20Face-Follow-orange?style=flat-square&amp;amp;logo=huggingface&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/orgs/Auto-RAG/projects/1/views/2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Roadmap-5D3FD3&#34; alt=&#34;Static Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are many RAG pipelines and modules out there, but you don’t know what pipeline is great for “your own data” and &#34;your own use-case.&#34; Making and evaluating all RAG modules is very time-consuming and hard to do. But without it, you will never know which RAG pipeline is the best for your own use-case.&lt;/p&gt; &#xA;&lt;p&gt;AutoRAG is a tool for finding the optimal RAG pipeline for “your data.” You can evaluate various RAG modules automatically with your own evaluation data and find the best RAG pipeline for your own use-case.&lt;/p&gt; &#xA;&lt;p&gt;AutoRAG supports a simple way to evaluate many RAG module combinations. Try now and find the best RAG pipeline for your own use-case.&lt;/p&gt; &#xA;&lt;p&gt;Explore our 📖 &lt;a href=&#34;https://docs.auto-rag.com&#34;&gt;Document&lt;/a&gt;!!&lt;/p&gt; &#xA;&lt;p&gt;Plus, join our 📞 &lt;a href=&#34;https://discord.gg/P4DYXfmSAs&#34;&gt;Discord&lt;/a&gt; Community.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Do you have any difficulties in optimizing your RAG pipeline? Or is it hard to set up things to use AutoRAG? Try &lt;a href=&#34;https://tally.so/r/n0jOrZ&#34;&gt;&lt;strong&gt;AutoRAG Cloud&lt;/strong&gt;&lt;/a&gt; beta. We will help you to run AutoRAG and optimize. Plus, we can help you to build RAG evaluation dataset.&lt;/p&gt; &#xA;&lt;p&gt;Starts with 9.99$ per optimization.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;YouTube Tutorial&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Marker-Inc-Korea/AutoRAG/assets/96727832/c0d23896-40c0-479f-a17b-aa2ec3183a26&#34;&gt;https://github.com/Marker-Inc-Korea/AutoRAG/assets/96727832/c0d23896-40c0-479f-a17b-aa2ec3183a26&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Muted by default, enable sound for voice-over&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can see on &lt;a href=&#34;https://youtu.be/2ojK8xjyXAU?feature=shared&#34;&gt;YouTube&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Use AutoRAG in HuggingFace Space 🚀&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/AutoRAG/Naive-RAG-chatbot&#34;&gt;💬 Naive RAG Chatbot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/AutoRAG/AutoRAG-data-creation&#34;&gt;✏️ AutoRAG Data Creation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/AutoRAG/AutoRAG-optimization&#34;&gt;🚀 AutoRAG RAG Pipeline Optimization&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Colab Tutorial&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/19OEQXO_pHN6gnn2WdfPd4hjnS-4GurVd?usp=sharing&#34;&gt;Step 1: Basic of AutoRAG | Optimizing your RAG pipeline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1BOdzMndYgMY_iqhwKcCCS7ezHbZ4Oz5X?usp=sharing&#34;&gt;Step 2: Data Creation | Create your own Data for RAG Optimization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/12VpWcSTSOsLSyW0BKb-kPoEzK22ACxvS?usp=sharing&#34;&gt;Step 3: Use Custom LLM &amp;amp; Embedding Model | Use Custom Model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Index&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#quick-install&#34;&gt;Quick Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#-autorag-docker-guide&#34;&gt;🐳 AutoRAG Docker Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#data-creation&#34;&gt;Data Creation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#1-parsing&#34;&gt;Parsing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#2-chunking&#34;&gt;Chunking&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#3-qa-creation&#34;&gt;QA Creation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#rag-optimization&#34;&gt;RAG Optimization&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#1-set-yaml-file&#34;&gt;Set YAML File&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#2-run-autorag&#34;&gt;Run AutoRAG&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#3-run-dashboard&#34;&gt;Run Dashboard&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#4-deploy-your-optimal-rag-pipeline-for-testing&#34;&gt;Deploy your optimal RAG pipeline (for testing)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#-supporting-data-creation-modules&#34;&gt;Supporting Data Creation Modules&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#supporting-rag-optimization-nodes--modules&#34;&gt;Supporting RAG Optimization Nodes &amp;amp; modules&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#supporting-evaluation-metrics&#34;&gt;Supporting Evaluation Metrics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#-faq&#34;&gt;FaQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Quick Install&lt;/h1&gt; &#xA;&lt;p&gt;We recommend using Python version 3.10 or higher for AutoRAG.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install AutoRAG&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use the local models, you need to install gpu version.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;AutoRAG[gpu]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or for parsing, you can use the parsing version.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;AutoRAG[gpu,parse]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Data Creation&lt;/h1&gt; &#xA;&lt;a href=&#34;https://huggingface.co/spaces/AutoRAG/AutoRAG-data-creation&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/8c6e4b02-3938-4560-b817-c95764965b50&#34; alt=&#34;Hugging Face Sticker&#34; style=&#34;width:200px;height:auto;&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/6079f696-207c-4221-8d28-5561a203dfe2&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;RAG Optimization requires two types of data: QA dataset and Corpus dataset.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;QA&lt;/strong&gt; dataset file (qa.parquet)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Corpus&lt;/strong&gt; dataset file (corpus.parquet)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;QA&lt;/strong&gt; dataset is important for accurate and reliable evaluation and optimization.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Corpus&lt;/strong&gt; dataset is critical to the performance of RAGs. This is because RAG uses the corpus to retrieve documents and generate answers using it.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;1. Parsing&lt;/h3&gt; &#xA;&lt;h4&gt;Set YAML File&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;modules:&#xA;  - module_type: langchain_parse&#xA;    parse_method: pdfminer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use multiple Parse modules at once. However, in this case, you&#39;ll need to return a new process for each parsed result.&lt;/p&gt; &#xA;&lt;h4&gt;Start Parsing&lt;/h4&gt; &#xA;&lt;p&gt;You can parse your raw documents with just a few lines of code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from autorag.parser import Parser&#xA;&#xA;parser = Parser(data_path_glob=&#34;your/data/path/*&#34;)&#xA;parser.start_parsing(&#34;your/path/to/parse_config.yaml&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Chunking&lt;/h3&gt; &#xA;&lt;h4&gt;Set YAML File&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;modules:&#xA;  - module_type: llama_index_chunk&#xA;    chunk_method: Token&#xA;    chunk_size: 1024&#xA;    chunk_overlap: 24&#xA;    add_file_name: en&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use multiple Chunk modules at once. In this case, you need to use one corpus to create QA and then map the rest of the corpus to QA Data. If the chunk method is different, the retrieval_gt will be different, so we need to remap it to the QA dataset.&lt;/p&gt; &#xA;&lt;h4&gt;Start Chunking&lt;/h4&gt; &#xA;&lt;p&gt;You can chunk your parsed results with just a few lines of code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from autorag.chunker import Chunker&#xA;&#xA;chunker = Chunker.from_parquet(parsed_data_path=&#34;your/parsed/data/path&#34;)&#xA;chunker.start_chunking(&#34;your/path/to/chunk_config.yaml&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. QA Creation&lt;/h3&gt; &#xA;&lt;p&gt;You can create QA dataset with just a few lines of code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd&#xA;from llama_index.llms.openai import OpenAI&#xA;&#xA;from autorag.data.qa.filter.dontknow import dontknow_filter_rule_based&#xA;from autorag.data.qa.generation_gt.llama_index_gen_gt import (&#xA;    make_basic_gen_gt,&#xA;    make_concise_gen_gt,&#xA;)&#xA;from autorag.data.qa.schema import Raw, Corpus&#xA;from autorag.data.qa.query.llama_gen_query import factoid_query_gen&#xA;from autorag.data.qa.sample import random_single_hop&#xA;&#xA;llm = OpenAI()&#xA;raw_df = pd.read_parquet(&#34;your/path/to/parsed.parquet&#34;)&#xA;raw_instance = Raw(raw_df)&#xA;&#xA;corpus_df = pd.read_parquet(&#34;your/path/to/corpus.parquet&#34;)&#xA;corpus_instance = Corpus(corpus_df, raw_instance)&#xA;&#xA;initial_qa = (&#xA;    corpus_instance.sample(random_single_hop, n=3)&#xA;    .map(&#xA;        lambda df: df.reset_index(drop=True),&#xA;    )&#xA;    .make_retrieval_gt_contents()&#xA;    .batch_apply(&#xA;        factoid_query_gen,  # query generation&#xA;        llm=llm,&#xA;    )&#xA;    .batch_apply(&#xA;        make_basic_gen_gt,  # answer generation (basic)&#xA;        llm=llm,&#xA;    )&#xA;    .batch_apply(&#xA;        make_concise_gen_gt,  # answer generation (concise)&#xA;        llm=llm,&#xA;    )&#xA;    .filter(&#xA;        dontknow_filter_rule_based,  # filter don&#39;t know&#xA;        lang=&#34;en&#34;,&#xA;    )&#xA;)&#xA;&#xA;initial_qa.to_parquet(&#39;./qa.parquet&#39;, &#39;./corpus.parquet&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;RAG Optimization&lt;/h1&gt; &#xA;&lt;a href=&#34;https://huggingface.co/spaces/AutoRAG/AutoRAG-optimization&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/8c6e4b02-3938-4560-b817-c95764965b50&#34; alt=&#34;Hugging Face Sticker&#34; style=&#34;width:200px;height:auto;&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/214d842e-fc67-4113-9c24-c94158b00c23&#34; alt=&#34;rag&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How AutoRAG optimizes RAG pipeline?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/55bd09cd-8420-4f6d-bc7d-0a66af288317&#34; alt=&#34;rag_opt_gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🐳 AutoRAG Docker Guide&lt;/h2&gt; &#xA;&lt;p&gt;This guide provides a quick overview of building and running the AutoRAG Docker container for production, with instructions on setting up the environment for evaluation using your configuration and data paths.&lt;/p&gt; &#xA;&lt;h3&gt;🚀 Building the Docker Image&lt;/h3&gt; &#xA;&lt;p&gt;Tip: If you want to build an image for a gpu version, you can use &lt;code&gt;autoraghq/autorag:gpu&lt;/code&gt; or &lt;code&gt;autoraghq/autorag:gpu-parsing&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.Download dataset for &lt;a href=&#34;https://colab.research.google.com/drive/19OEQXO_pHN6gnn2WdfPd4hjnS-4GurVd?usp=sharing&#34;&gt;Tutorial Step 1&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python sample_dataset/eli5/load_eli5_dataset.py --save_path projects/tutorial_1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Run &lt;code&gt;evaluate&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This step may take a long time to complete and involves OpenAI API calls, which may cost approximately $0.30.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --rm -it \&#xA;  -v ~/.cache/huggingface:/root/.cache/huggingface \&#xA;  -v $(pwd)/projects:/usr/src/app/projects \&#xA;  -e OPENAI_API_KEY=${OPENAI_API_KEY} \&#xA;  autoraghq/autorag:api evaluate \&#xA;  --config /usr/src/app/projects/tutorial_1/config.yaml \&#xA;  --qa_data_path /usr/src/app/projects/tutorial_1/qa_test.parquet \&#xA;  --corpus_data_path /usr/src/app/projects/tutorial_1/corpus.parquet \&#xA;  --project_dir /usr/src/app/projects/tutorial_1/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Run validate&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --rm -it \&#xA;  -v ~/.cache/huggingface:/root/.cache/huggingface \&#xA;  -v $(pwd)/projects:/usr/src/app/projects \&#xA;  -e OPENAI_API_KEY=${OPENAI_API_KEY} \&#xA;  autoraghq/autorag:api validate \&#xA;  --config /usr/src/app/projects/tutorial_1/config.yaml \&#xA;  --qa_data_path /usr/src/app/projects/tutorial_1/qa_test.parquet \&#xA;  --corpus_data_path /usr/src/app/projects/tutorial_1/corpus.parquet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;4. Run &lt;code&gt;dashboard&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --rm -it \&#xA;  -v ~/.cache/huggingface:/root/.cache/huggingface \&#xA;  -v $(pwd)/projects:/usr/src/app/projects \&#xA;  -e OPENAI_API_KEY=${OPENAI_API_KEY} \&#xA;  -p 8502:8502 \&#xA;  autoraghq/autorag:api dashboard \&#xA;    --trial_dir /usr/src/app/projects/tutorial_1/0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;4. Run &lt;code&gt;run_web&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --rm -it \&#xA;  -v ~/.cache/huggingface:/root/.cache/huggingface \&#xA;  -v $(pwd)/projects:/usr/src/app/projects \&#xA;  -e OPENAI_API_KEY=${OPENAI_API_KEY} \&#xA;  -p 8501:8501 \&#xA;  autoraghq/autorag:api run_web --trial_path ./projects/tutorial_1/0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Key Points :&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;-v ~/.cache/huggingface:/cache/huggingface&lt;/code&gt;&lt;/strong&gt;: Mounts the host machine’s Hugging Face cache to &lt;code&gt;/cache/huggingface&lt;/code&gt; in the container, enabling access to pre-downloaded models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;-e OPENAI_API_KEY: ${OPENAI_API_KEY}&lt;/code&gt;&lt;/strong&gt;: Passes the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; from your host environment.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more detailed instructions, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/docs/source/install.md#1-build-the-docker-image&#34;&gt;Docker Installation Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;1. Set YAML File&lt;/h3&gt; &#xA;&lt;p&gt;First, you need to set the config YAML file for your RAG optimization.&lt;/p&gt; &#xA;&lt;p&gt;You can get various config YAML files at &lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/sample_config&#34;&gt;here&lt;/a&gt;. We highly recommend using pre-made config YAML files for starter.&lt;/p&gt; &#xA;&lt;p&gt;If you want to make your own config YAML files, check out the &lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#-create-your-own-config-yaml-file&#34;&gt;Config YAML file&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of the config YAML file to use &lt;code&gt;retrieval&lt;/code&gt;, &lt;code&gt;prompt_maker&lt;/code&gt;, and &lt;code&gt;generator&lt;/code&gt; nodes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;node_lines:&#xA;- node_line_name: retrieve_node_line  # Set Node Line (Arbitrary Name)&#xA;  nodes:&#xA;    - node_type: retrieval  # Set Retrieval Node&#xA;      strategy:&#xA;        metrics: [retrieval_f1, retrieval_recall, retrieval_ndcg, retrieval_mrr]  # Set Retrieval Metrics&#xA;      top_k: 3&#xA;      modules:&#xA;        - module_type: vectordb&#xA;          vectordb: default&#xA;        - module_type: bm25&#xA;        - module_type: hybrid_rrf&#xA;          weight_range: (4,80)&#xA;- node_line_name: post_retrieve_node_line  # Set Node Line (Arbitrary Name)&#xA;  nodes:&#xA;    - node_type: prompt_maker  # Set Prompt Maker Node&#xA;      strategy:&#xA;        metrics:   # Set Generation Metrics&#xA;          - metric_name: meteor&#xA;          - metric_name: rouge&#xA;          - metric_name: sem_score&#xA;            embedding_model: openai&#xA;      modules:&#xA;        - module_type: fstring&#xA;          prompt: &#34;Read the passages and answer the given question. \n Question: {query} \n Passage: {retrieved_contents} \n Answer : &#34;&#xA;    - node_type: generator  # Set Generator Node&#xA;      strategy:&#xA;        metrics:  # Set Generation Metrics&#xA;          - metric_name: meteor&#xA;          - metric_name: rouge&#xA;          - metric_name: sem_score&#xA;            embedding_model: openai&#xA;      modules:&#xA;        - module_type: openai_llm&#xA;          llm: gpt-4o-mini&#xA;          batch: 16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Run AutoRAG&lt;/h3&gt; &#xA;&lt;p&gt;You can evaluate your RAG pipeline with just a few lines of code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from autorag.evaluator import Evaluator&#xA;&#xA;evaluator = Evaluator(qa_data_path=&#39;your/path/to/qa.parquet&#39;, corpus_data_path=&#39;your/path/to/corpus.parquet&#39;)&#xA;evaluator.start_trial(&#39;your/path/to/config.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or you can use the command line interface&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;autorag evaluate --config your/path/to/default_config.yaml --qa_data_path your/path/to/qa.parquet --corpus_data_path your/path/to/corpus.parquet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once it is done, you can see several files and folders created in your current directory. At the trial folder named to numbers (like 0), you can check &lt;code&gt;summary.csv&lt;/code&gt; file that summarizes the evaluation results and the best RAG pipeline for your data.&lt;/p&gt; &#xA;&lt;p&gt;For more details, you can check out how the folder structure looks like at &lt;a href=&#34;https://docs.auto-rag.com/optimization/folder_structure.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;3. Run Dashboard&lt;/h3&gt; &#xA;&lt;p&gt;You can run a dashboard to easily see the result.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;autorag dashboard --trial_dir /your/path/to/trial_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;sample dashboard&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Marker-Inc-Korea/AutoRAG/assets/96727832/3798827d-31d7-4c4e-a9b1-54340b964e53&#34; alt=&#34;dashboard&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;4. Deploy your optimal RAG pipeline (for testing)&lt;/h3&gt; &#xA;&lt;h3&gt;4-1. Run as a Code&lt;/h3&gt; &#xA;&lt;p&gt;You can use an optimal RAG pipeline right away from the trial folder. The trial folder is the directory used in the running dashboard. (like 0, 1, 2, ...)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from autorag.deploy import Runner&#xA;&#xA;runner = Runner.from_trial_folder(&#39;/your/path/to/trial_dir&#39;)&#xA;runner.run(&#39;your question&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4-2. Run as an API server&lt;/h3&gt; &#xA;&lt;p&gt;You can run this pipeline as an API server.&lt;/p&gt; &#xA;&lt;p&gt;Check out the API endpoint at &lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/docs/source/deploy/api_endpoint.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import nest_asyncio&#xA;from autorag.deploy import ApiRunner&#xA;&#xA;nest_asyncio.apply()&#xA;&#xA;runner = ApiRunner.from_trial_folder(&#39;/your/path/to/trial_dir&#39;)&#xA;runner.run_api_server()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;autorag run_api --trial_dir your/path/to/trial_dir --host 0.0.0.0 --port 8000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The cli command uses extracted config YAML file. If you want to know it more, check out &lt;a href=&#34;https://docs.auto-rag.com/tutorial.html#extract-pipeline-and-evaluate-test-dataset&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;4-3. Run as a Web Interface&lt;/h3&gt; &#xA;&lt;p&gt;you can run this pipeline as a web interface.&lt;/p&gt; &#xA;&lt;p&gt;Check out the web interface at &lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/deploy/web.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;autorag run_web --trial_path your/path/to/trial_path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;sample web interface&lt;/h4&gt; &#xA;&lt;img width=&#34;1491&#34; alt=&#34;web_interface&#34; src=&#34;https://github.com/Marker-Inc-Korea/AutoRAG/assets/96727832/f6b00353-f6bb-4d8f-8740-1c264c0acbb8&#34;&gt; &#xA;&lt;h3&gt;Use advanced web interface&lt;/h3&gt; &#xA;&lt;p&gt;You can deploy the advanced web interface featured by &lt;a href=&#34;https://github.com/Cinnamon/kotaemon&#34;&gt;Kotaemon&lt;/a&gt; to the fly.io. Go &lt;a href=&#34;https://github.com/vkehfdl1/AutoRAG-web-kotaemon&#34;&gt;here&lt;/a&gt; to use it and deploy to the fly.io.&lt;/p&gt; &#xA;&lt;p&gt;Example :&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://velog.velcdn.com/images/autorag/post/5e71b8d9-3e59-4e63-9191-355a1a5aa3a0/image.png&#34; alt=&#34;Kotaemon Example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📌 Supporting Data Creation Modules&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/0e5872de-2892-46b4-9ecd-e395671e324c&#34; alt=&#34;Data Creation&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can check our all Parsing Modules at &lt;a href=&#34;https://edai.notion.site/Supporting-Parse-Modules-e0b7579c7c0e4fb2963e408eeccddd75?pvs=4&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can check our all Chunk Modules at &lt;a href=&#34;https://edai.notion.site/Supporting-Chunk-Modules-8db803dba2ec4cd0a8789659106e86a3?pvs=4&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;❗Supporting RAG Optimization Nodes &amp;amp; modules&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/c2e4c7d2-0f46-4b13-bb21-0ffc19cc9492&#34; alt=&#34;module_1&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/e013f04e-f69d-4dd7-96da-06e6b5921c3d&#34; alt=&#34;module_2&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/72490b97-81a1-4620-b9bd-5c5e0ead79a7&#34; alt=&#34;module_3&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/7a8ee260-9c60-4a27-b708-5286d1c37851&#34; alt=&#34;module_4&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can check our all supporting Nodes &amp;amp; modules at &lt;a href=&#34;https://edai.notion.site/Supporting-Nodes-modules-0ebc7810649f4e41aead472a92976be4?pvs=4&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;❗Supporting Evaluation Metrics&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/fab551cd-8892-4bda-acae-eff22ece94b3&#34; alt=&#34;Metrics&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can check our all supporting Evaluation Metrics at &lt;a href=&#34;https://edai.notion.site/Supporting-metrics-867d71caefd7401c9264dd91ba406043?pvs=4&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://edai.notion.site/Retrieval-Metrics-dde3d9fa1d9547cdb8b31b94060d21e7?pvs=4&#34;&gt;Retrieval Metrics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://edai.notion.site/Retrieval-Token-Metrics-c3e2d83358e04510a34b80429ebb543f?pvs=4&#34;&gt;Retrieval Token Metrics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/7d4a3069-9186-4854-885d-ca0f7bcc17e8&#34;&gt;Generation Metrics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;☎️ FaQ&lt;/h2&gt; &#xA;&lt;p&gt;🛣️ &lt;a href=&#34;https://github.com/orgs/Auto-RAG/projects/1/views/2&#34;&gt;Roadmap&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;💻 &lt;a href=&#34;https://edai.notion.site/Hardware-specs-28cefcf2a26246ffadc91e2f3dc3d61c?pvs=4&#34;&gt;Hardware Specs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;⭐ &lt;a href=&#34;https://edai.notion.site/About-running-AutoRAG-44a8058307af42068fc218a073ee480b?pvs=4&#34;&gt;Running AutoRAG&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;🍯 &lt;a href=&#34;https://edai.notion.site/Tips-Tricks-10708a0e36ff461cb8a5d4fb3279ff15?pvs=4&#34;&gt;Tips/Tricks&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;☎️ &lt;a href=&#34;https://medium.com/@autorag/autorag-troubleshooting-5cf872b100e3&#34;&gt;TroubleShooting&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;💬 Talk with Founders&lt;/h2&gt; &#xA;&lt;p&gt;Talk with us! We are always open to talk with you.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;🎤 &lt;a href=&#34;https://zcal.co/autorag-jeffrey/autorag-demo-15min&#34;&gt;Talk with Jeffrey&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🦜 &lt;a href=&#34;https://zcal.co/i/tcuLtmq5&#34;&gt;Talk with Bwook&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;✨ Contributors ✨&lt;/h1&gt; &#xA;&lt;p&gt;Thanks go to these wonderful people:&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/Marker-Inc-Korea/AutoRAG/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=Marker-Inc-Korea/AutoRAG&#34;&gt; &lt;/a&gt; &#xA;&lt;h1&gt;Contribution&lt;/h1&gt; &#xA;&lt;p&gt;We are developing AutoRAG as open-source.&lt;/p&gt; &#xA;&lt;p&gt;So this project welcomes contributions and suggestions. Feel free to contribute to this project.&lt;/p&gt; &#xA;&lt;p&gt;Plus, check out our detailed documentation at &lt;a href=&#34;https://docs.auto-rag.com/index.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>