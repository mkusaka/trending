<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-28T01:31:22Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>haoel/haoel.github.io</title>
    <updated>2023-03-28T01:31:22Z</updated>
    <id>tag:github.com,2023-03-28:/haoel/haoel.github.io</id>
    <link href="https://github.com/haoel/haoel.github.io" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;科学上网&lt;/h1&gt; &#xA;&lt;p&gt;作者：左耳朵 &lt;a href=&#34;http://coolshell.cn&#34;&gt;http://coolshell.cn&lt;/a&gt; 更新时间：2023-02-02&lt;/p&gt; &#xA;&lt;p&gt;这篇文章可以写的更好，欢迎到 &lt;a href=&#34;https://github.com/haoel/haoel.github.io&#34;&gt;https://github.com/haoel/haoel.github.io&lt;/a&gt; 更新&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/images/cover.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91&#34;&gt;科学上网&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#0-%E5%BA%8F&#34;&gt;0. 序&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#1-%E8%8B%B1%E6%96%87%E8%83%BD%E5%8A%9B&#34;&gt;1. 英文能力&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#2-%E8%B4%AD%E4%B9%B0vps&#34;&gt;2. 购买VPS&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#21-%E5%B8%B8%E8%A7%84vps&#34;&gt;2.1 常规VPS&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#22-cn2-%E7%BA%BF%E8%B7%AF&#34;&gt;2.2 CN2 线路&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#23-ncp-%E7%BA%BF%E8%B7%AF&#34;&gt;2.3 NCP 线路&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#3-%E6%90%AD%E5%BB%BA%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1&#34;&gt;3. 搭建相关代理服务&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#31-%E8%AE%BE%E7%BD%AEdocker%E6%9C%8D%E5%8A%A1&#34;&gt;3.1 设置Docker服务&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#32-%E5%BC%80%E5%90%AF-tcp-bbr-%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95&#34;&gt;3.2 开启 TCP BBR 拥塞控制算法&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#33-%E7%94%A8-gost-%E8%AE%BE%E7%BD%AE-https-%E6%9C%8D%E5%8A%A1&#34;&gt;3.3 用 Gost 设置 HTTPS 服务&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#34-%E8%AE%BE%E7%BD%AE-shadowsocks-%E6%9C%8D%E5%8A%A1&#34;&gt;3.4 设置 ShadowSocks 服务&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#35-%E8%AE%BE%E7%BD%AEl2tpipsec%E6%9C%8D%E5%8A%A1&#34;&gt;3.5 设置L2TP/IPSec服务&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#36-%E8%AE%BE%E7%BD%AE-pptp-%E6%9C%8D%E5%8A%A1&#34;&gt;3.6 设置 PPTP 服务&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#4-%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%AE%BE%E7%BD%AE&#34;&gt;4. 客户端设置&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#41-gost-%E5%AE%A2%E6%88%B7%E7%AB%AF&#34;&gt;4.1 gost 客户端&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#42-shadowsocks-%E5%AE%A2%E6%88%B7%E7%AB%AF&#34;&gt;4.2 Shadowsocks 客户端&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#43-vpn-%E5%AE%A2%E6%88%B7%E7%AB%AF&#34;&gt;4.3 VPN 客户端&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#5-%E6%B5%81%E9%87%8F%E4%BC%AA%E8%A3%85%E5%92%8C%E5%85%B6%E5%AE%83%E6%96%B9%E5%BC%8F&#34;&gt;5. 流量伪装和其它方式&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#51-v2ray&#34;&gt;5.1 V2Ray&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#52-brook&#34;&gt;5.2 Brook&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#6-%E9%92%88%E5%AF%B9-ip-%E8%A2%AB%E5%B0%81%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88&#34;&gt;6. 针对 IP 被封的解决方案&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#61-cloudflare&#34;&gt;6.1 Cloudflare&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#62-v2ray&#34;&gt;6.2 V2Ray&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#63-%E8%A1%A5%E5%85%85&#34;&gt;6.3 补充&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#7-%E5%AE%B6%E7%94%A8%E9%80%8F%E6%98%8E%E7%BD%91%E5%85%B3&#34;&gt;7. 家用透明网关&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#71-openwrt-%E8%B7%AF%E7%94%B1%E5%99%A8&#34;&gt;7.1 OpenWRT 路由器&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#72-%E9%80%9A%E8%BF%87%E6%A0%91%E8%8E%93%E6%B4%BE%E5%81%9A%E6%97%81%E8%B7%AF%E7%BD%91%E5%85%B3&#34;&gt;7.2 通过树莓派做旁路网关&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#73-%E5%AE%89%E8%A3%85-clash&#34;&gt;7.3 安装 Clash&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#74-%E8%AE%BE%E7%BD%AE-iptables-%E8%BD%AC%E5%8F%91&#34;&gt;7.4 设置 iptables 转发&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#8-%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E9%80%8F%E6%98%8E%E7%BD%91%E5%85%B3&#34;&gt;8. 数据中心透明网关&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#81-aws-%E7%BD%91%E7%BB%9C%E6%9E%84%E5%BB%BA&#34;&gt;8.1 AWS 网络构建&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#82-%E5%AE%89%E8%A3%85-clash&#34;&gt;8.2 安装 Clash&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#83-%E9%85%8D%E7%BD%AE%E7%A7%81%E6%9C%89%E5%AD%90%E7%BD%91%E4%B8%AD%E7%9A%84-ec2&#34;&gt;8.3 配置私有子网中的 EC2&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#84-%E7%A7%81%E6%9C%89%E5%AD%90%E7%BD%91%E4%B8%AD%E7%9A%84-kubernetes&#34;&gt;8.4 私有子网中的 Kubernetes&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#9-%E4%BB%A3%E7%90%86%E6%8A%80%E5%B7%A7&#34;&gt;9. 代理技巧&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#91-http-%E9%9A%A7%E9%81%93&#34;&gt;9.1 HTTP 隧道&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#92-ssh-%E9%9A%A7%E9%81%93&#34;&gt;9.2 SSH 隧道&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#93-github--git-ssh-%E4%BB%A3%E7%90%86&#34;&gt;9.3 Github / Git SSH 代理&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#94-cloudflare-warp-%E5%8E%9F%E7%94%9F-ip&#34;&gt;9.4 Cloudflare Warp 原生 IP&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#941-%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85&#34;&gt;9.4.1 脚本安装&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#942-%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85&#34;&gt;9.4.2 手动安装&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#10-%E5%85%B6%E5%AE%83&#34;&gt;10. 其它&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#101-%E5%85%B6%E5%AE%83%E6%96%B9%E5%BC%8F&#34;&gt;10.1 其它方式&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#102-%E6%90%AD%E5%BB%BA%E8%84%9A%E6%9C%AC&#34;&gt;10.2 搭建脚本&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;0. 序&lt;/h2&gt; &#xA;&lt;p&gt;首先，我们先明确一下，我科学上网的目的主要是为了学习、工作、交友、查资料、和丰富自己的眼界，不是其它的事。&lt;/p&gt; &#xA;&lt;p&gt;对我来说，科学上网很重要，下面罗列一下需要科学上网，我才能真正学习工作和生活的网站：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Youtube 和 Vimeo 上的各种大会和教学视频，除了我自己要学，我的孩子也要学。&lt;/li&gt; &#xA; &lt;li&gt;Wikipedia 维基百科是我目前唯一信得过的百科全书，我在上面可以比较系统地翻阅各种词条。&lt;/li&gt; &#xA; &lt;li&gt;SlideShare 上有很多的技术文档和资料的PPT，是我的知识学习的地方。&lt;/li&gt; &#xA; &lt;li&gt;Quora 问答网站，在上面有很多有趣的问答。&lt;/li&gt; &#xA; &lt;li&gt;博客和论文，很多博客和论文站点都被墙了，比如：Blogspot 和 Medium。&lt;/li&gt; &#xA; &lt;li&gt;Google 的各种服务，比如：Gmail, Map, Docs，Driver，照片，图片搜索，Voices，论文搜索……包括Google官方的各种技术文档……&lt;/li&gt; &#xA; &lt;li&gt;一些云服务，比如：Dropbox，IFTTT，Imgur，archive.org……&lt;/li&gt; &#xA; &lt;li&gt;Twitter 上 Follow 一些牛人和一些官方账号，比如：AWS、Docker……&lt;/li&gt; &#xA; &lt;li&gt;社交 Facebook, Telegram, Whatsapp, Slack……，有一些我在国外的亲戚和朋友……&lt;/li&gt; &#xA; &lt;li&gt;Reddit 是一个聚合网站，一个新闻和文章的集散地，你可以认为是各种频道的今日头条……&lt;/li&gt; &#xA; &lt;li&gt;Pinterest 和 Instagram 上面有很多不错的图片和视频新闻，是我减压力的地方……&lt;/li&gt; &#xA; &lt;li&gt;新闻，如BBC。 BBC是全球比较出众的媒体，有太多的有价值资源和内容了，比如纪录片、学英文……&lt;/li&gt; &#xA; &lt;li&gt;编程，有很多编程的场景需要翻墙，比如，Go语言编程时的 go get 中的很多库是放在 Google的服务器上， 然而Google是全部被墙，包括 Android 和其它一些文档和资源也是一样。包括 SourceForge 的某些项目也需要科学上网，Docker Registry也有部分被墙，还有偶尔抽风的Github，以及不能访问的gist……&lt;/li&gt; &#xA; &lt;li&gt;……等等&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;是的，我的互联网不是——全是骗子的百度、充满广告的微信朋友圈、质量低下的公众号、娱乐至死的新浪微博、只有抖机灵和“怎么看XX”的知乎、毫无营养的今日头条…… 在这样的网络空间里，我真的无法生存…… 这根本不是互联网，不是为我服务的互联网，而是在消费我的互联网，是让我变傻变笨的互联网…… 我不能忍，因为它影响到了我的生存……&lt;/p&gt; &#xA;&lt;h2&gt;1. 英文能力&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;首先，你应该对英文读写没什么问题!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;为什么这么说？&lt;strong&gt;这主要是针对计算机相关的知识，逻辑是这样的，如果你上了Google还是在用中文关键词，那么你好不容易出来了，结果又回去了，所以没什么意义。&lt;/strong&gt; 换言之，科学上网的目的是为了进入广阔的世界范围与全世界的人交流，所以，英文是必备的，如果你英文有问题，VPN过去的用处也不大。&lt;/p&gt; &#xA;&lt;p&gt;所以，我把这个前提条件放在第一的位置，就是说—— &lt;strong&gt;真正的墙不是GFW，而是人的大脑！&lt;/strong&gt; 意思是，屏蔽你获得信息能力的不是墙，而很大一部分则是我们自己的语言能力！&lt;/p&gt; &#xA;&lt;h2&gt;2. 购买VPS&lt;/h2&gt; &#xA;&lt;p&gt;然后，你需要一个VPS。 在这里，强烈建议通过自建的方式，可能成本会比托管的“机场”要高一些，而且还很麻烦，但是，在安全性方面会比较好一些。自己动手，自力更生，让人有更多的安全感。&lt;/p&gt; &#xA;&lt;p&gt;（注：&lt;em&gt;当然，你也可以直接购买一些科学上网的服务，但我这里不推荐了，一方面是广告，另一方面通常这样的服务非常的不稳定，而且也容易被代理方做中间人攻击&lt;/em&gt;）&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;现在你买一台VPS也不贵了，也就是一个月10美金左右（70元），我个人觉得一个月花70元钱不算奢侈的事，而且会让你的生活质量得得改善。当然，线路好的得需要多花一些钱。&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;p&gt;（注：&lt;em&gt;我现在每个月投入在科学上网上的成本大概在不到500元人民币左右，常备3-5个不同国家的VPS，因为国内的网络路由经常性的变化，所以，为了确保总是有一条快的，所以，得多备几个&lt;/em&gt;）。&lt;/p&gt; &#xA;&lt;h3&gt;2.1 常规VPS&lt;/h3&gt; &#xA;&lt;p&gt;对于 VPS，下面是一些常规选项。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightsail.aws.amazon.com/&#34;&gt;AWS LightSail&lt;/a&gt; 是一个非常便宜好用的服务，最低配置一个月 $3.5 美金，流量不限，目前的Zone不多，推荐使用日本，新加坡或美国俄勒冈（支持银联卡）。现对 2021/8/7 之后使用 Lightsail 的用户提供3个月的免费试用。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/cn/&#34;&gt;AWS EC2&lt;/a&gt;香港、日本或韩国申请个免费试用一年的EC2 VPS （支持银联卡）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.google.com/&#34;&gt;Google Cloud Platform&lt;/a&gt;提供免费试用，赠送300刀赠金（需要国际信用卡）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linode.com&#34;&gt;Linode&lt;/a&gt;买个一月USD5刀的VPS&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.conoha.jp/zh/&#34;&gt;Conoha&lt;/a&gt;上买一个日本的VPS，一个月900日元 （可以支付宝）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.vultr.com&#34;&gt;Vultr&lt;/a&gt;上买一个日本的VPS，一个月5刀 （可以支付宝）(注：据说被墙的IP太多）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.oracle.com/cloud/free/&#34;&gt;Oracle Cloud&lt;/a&gt;两台VPS无限期使用，可选美日韩等地（需要国际信用卡）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;在中国，因为有太多的网络提供商，所以，国内的网络也是很奇葩的，可以看到的是，不同的地方，不同的网络，到不同的国家完全不一样，而且还经常性地调整路由，所以，经常性地有时候快有时候慢，简直就是随机的。所以，像我这样要求比较高的人，一般会备3-5个不同国家地区的VPS，以保障上网的速度。&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;香港网速应该是比较好的，但是香港的成本也是比较高的。台湾的网速也是不错的，日本的网速其次，新加坡再次之，然后是美国的东海岸。但是，因为线路的问题，如果没有为中国区优化的线路，丢包率是非常大的，日本区 ping 值虽然很低，但是经常性的丢包，好的线路的美国的 ping 值虽然大，但是也会飞快。&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;日本区的网络质量并不一定很好，有时候快的飞快，但有时候会有很大的丢包率（不同的网络不一样），有时候会很慢。上述的这几个VPS服务商中，AWS韩国和日本会好点，然后是 &lt;a href=&#34;https://www.linode.com/&#34;&gt;Linode&lt;/a&gt;，最后是 &lt;a href=&#34;https://www.vultr.com/&#34;&gt;Vultr&lt;/a&gt;（如果你有更好的，请推荐）&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Google Cloud Platform - GCP 的香港和台湾节点也是很快的。但是你要能买GCP的主机，你还得先翻墙，所以，感觉有点死锁了。所以，你可能先用 &lt;a href=&#34;https://www.vultr.com/&#34;&gt;Vultr&lt;/a&gt;（按时付费）翻墙，然后再到GCP上购买。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;2.2 CN2 线路&lt;/h3&gt; &#xA;&lt;p&gt;如果你需要更好更高速的网络服务（比如你要看 Youtube 的 1080P），那么，你需要下面的这些服务器资源了（价格也会高一些）&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;CN2&lt;/code&gt; 和 &lt;code&gt;GIA&lt;/code&gt; 是两个关键词。&lt;strong&gt;CN2 GIA&lt;/strong&gt; 全称 China telecom Next Carrier Network- Global Internet Access 电信国际精品网络，特征是路由线路上骨干节点均为59.43开头的IP。如果想要寻找接入CN2线路的国外VPS提供商，建议使用 &lt;code&gt;Next Carrier Network&lt;/code&gt; 或者 &lt;code&gt;CN2&lt;/code&gt; 这个关键词搜索即可。&lt;/p&gt; &#xA;&lt;p&gt;多说一句， CN2本身又分为两种类型：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CN2 GT&lt;/strong&gt;: CN2 里属于Global Transit的产品(又名GIS-Global Internet Service)，在CN2里等级低，省级/出国节点为 &lt;code&gt;202.97&lt;/code&gt; 开头，国际骨干节点有2～4个 &lt;code&gt;59.43&lt;/code&gt; 开头的CN2节点。在出国线路上拥堵程度一般，相对于163骨干网的稍强，相比CN2 GIA，性价比也较高。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CN2 GIA&lt;/strong&gt;: CN2 里属于Global Internet Access的产品，等级最高，省级/出国/国际骨干节点都以&lt;code&gt;59.43&lt;/code&gt;开头，全程没有&lt;code&gt;202.97&lt;/code&gt;开头的节点。在出国线路上表现最好，很少拥堵，理论上速度最快最稳定，当然，价格也相对 CN2 GT 偏高。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;关于 &lt;code&gt;CN2&lt;/code&gt; 线路的主机提供商，好些都不靠谱，只推荐下面两个，首推搬瓦工。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bwh8.net/aff.php?aff=39384&#34;&gt;搬瓦工&lt;/a&gt; 这应该是美区最好的一个用来科学上网的VPS提供商了，实测飞快。购买时你需要注意VPS规格上的 &lt;code&gt;CN2&lt;/code&gt; 和 &lt;code&gt;GIA&lt;/code&gt; 的描述。（注：点击主页右上角的 &lt;code&gt;regisiter&lt;/code&gt; 以后，你可以看到页面上方有两个导航条，在下面的导航条上点 &lt;code&gt;Services&lt;/code&gt; -&amp;gt; &lt;code&gt;Order New Services&lt;/code&gt; 就可以看到所有的列表了。买完后，你可能需要重装一下操作系统，装成64位带BBR的 ）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bill.hostdare.com/aff.php?aff=3397&#34;&gt;Hostdare&lt;/a&gt; 的 CN2 GIA 产品也是三网直连，KVM 和 OpenVZ 两种架构，KVM 产品长期缺货&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;更多的可以参考这篇文章《&lt;a href=&#34;https://wzfou.com/cn2-gia-vps/&#34;&gt;CN2 GIA VPS主机收集整理汇总-电信,联通,移动三网CN2 GIA线路VPS主机&lt;/a&gt;》（注：随时间推移，这篇文章的内容可能会失效）&lt;/p&gt; &#xA;&lt;p&gt;重点说一下，&lt;strong&gt;CN2 GIA + 香港机房&lt;/strong&gt;，你会得到巨快无比的上网速度（无论你在中国的哪个位置，无论使用哪家运营商，CN2 GIA都是最优的），然而，香港地区的VPS的确是有点贵了。在Youtube.com上看 4K 的视频毫无压力。虽然阿里云和腾讯的也有，但是被查到的风险基本上是100%，不建议使用，被抓了别怪我没警告过你。&lt;/p&gt; &#xA;&lt;h3&gt;2.3 NCP 线路&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;NCP&lt;/strong&gt; 全称 New Cross Pacific（新跨太平洋海底光缆系统）。 2018年11月底，中国到美国之间的海底光缆新开通了NCP线路，并且容量更大（系统设计容量超过80Tbps），路由更少（中国上海到美国中间路由节点只有11个，ping值110ms）。&lt;/p&gt; &#xA;&lt;p&gt;NCP线路全长13,000公里，连接美国俄勒冈州希尔斯伯勒，连接崇明（中国大陆），南汇（中国大陆），临港（中国大陆），釜山（韩国），头城（台湾），和丸山（日本）。&lt;/p&gt; &#xA;&lt;p&gt;相对于第二条中美直达海底光缆系统（跨太平洋快线，TPE），现阶段NCP线路的网络流量更少更稳定。特征是华东/中地区流量会经过NCP直达路由节点，IP地址为202.97.95.201/202。&lt;/p&gt; &#xA;&lt;p&gt;关于 &lt;code&gt;NCP&lt;/code&gt; 线路的主机提供商，下面罗列两个（欢迎补充）&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.50kvm.com&#34;&gt;50KVM VPS&lt;/a&gt; 截止2018年12月2日KVM 产品最低价格￥81.60/月。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://t667.com/&#34;&gt;OLVPS&lt;/a&gt; 截止2018年12月2日KVM 产品最低价格¥22/月。（&lt;strong&gt;特别注意&lt;/strong&gt; ： 在 OLVPS 上的《&lt;a href=&#34;https://olvps.com/index.php?rp=/knowledgebase/1/TOS.html&#34;&gt;服务条款&lt;/a&gt;》 中有一条说明：“&lt;strong&gt;禁止OpenV&lt;em&gt;P&lt;/em&gt;N/Socks5/PPTP/L2TP等软件、公共代理&lt;/strong&gt;”，所以，可能OLPVS并不太适合）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;3. 搭建相关代理服务&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;注：如下的搭建和安装脚本可参看本库的 scripts 目录下的脚本，如： &lt;a href=&#34;https://github.com/haoel/haoel.github.io/raw/master/scripts/install.ubuntu.18.04.sh&#34;&gt;Ubuntu 18.04 Installation Script&lt;/a&gt; （感谢网友 &lt;a href=&#34;https://github.com/gongzili456&#34;&gt;@gongzili456&lt;/a&gt; 开发）&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;3.1 设置Docker服务&lt;/h3&gt; &#xA;&lt;p&gt;首先，你要安装一个Docker CE 服务，这里你要去看一下docker官方的安装文档：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/install/linux/docker-ce/centos/&#34;&gt;CentOS 上的 Docker CE 安装&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/install/linux/docker-ce/ubuntu/&#34;&gt;Ubuntu 上的 Docker CE 安装&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;然后开始设置你的VPN/SS服务&lt;/p&gt; &#xA;&lt;h3&gt;3.2 开启 TCP BBR 拥塞控制算法&lt;/h3&gt; &#xA;&lt;p&gt;TCP BBR（Bottleneck Bandwidth and Round-trip propagation time）是由Google设计，于2016年发布的拥塞算法。以往大部分拥塞算法是基于丢包来作为降低传输速率的信号，而BBR则基于模型主动探测。该算法使用网络最近出站数据分组当时的最大带宽和往返时间来创建网络的显式模型。数据包传输的每个累积或选择性确认用于生成记录在数据包传输过程和确认返回期间的时间内所传送数据量的采样率。该算法认为随着网络接口控制器逐渐进入千兆速度时，分组丢失不应该被认为是识别拥塞的主要决定因素，所以基于模型的拥塞控制算法能有更高的吞吐量和更低的延迟，可以用BBR来替代其他流行的拥塞算法，例如CUBIC。Google在YouTube上应用该算法，将全球平均的YouTube网络吞吐量提高了4%，在一些国家超过了14%。&lt;/p&gt; &#xA;&lt;p&gt;BBR之后移植入Linux内核4.9版本，并且对于QUIC可用。&lt;/p&gt; &#xA;&lt;p&gt;如果开启，请参看 《&lt;a href=&#34;https://github.com/iMeiji/shadowsocks_install/wiki/%E5%BC%80%E5%90%AF-TCP-BBR-%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95&#34;&gt;开启TCP BBR拥塞控制算法&lt;/a&gt; 》&lt;/p&gt; &#xA;&lt;h3&gt;3.3 用 Gost 设置 HTTPS 服务&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ginuerzh/gost&#34;&gt;gost&lt;/a&gt; 是一个非常强的代理服务，它可以设置成 HTTPS 代理，然后把你的服务伪装成一个Web服务器，&lt;strong&gt;我感觉这比其它的流量伪装更好，也更隐蔽。这也是这里强烈推荐的一个方式&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;p&gt;为了更为的隐蔽，你需要一个域名（可以上 GoDaddy，但一定要使用美国版），然后使用 &lt;a href=&#34;https://letsencrypt.org&#34;&gt;Let&#39;s Encrypt&lt;/a&gt; 来签 一个证书。使用 Let&#39;s Encrypt 证书你需要在服务器上安装一个 &lt;a href=&#34;https://certbot.eff.org/instructions&#34;&gt;certbot&lt;/a&gt;，点击 &lt;a href=&#34;https://certbot.eff.org/instructions&#34;&gt;certbot&lt;/a&gt; 这个链接，你可以选择你的服务器，操作系统，然后就跟着指令走吧。&lt;/p&gt; &#xA;&lt;p&gt;接下来，你需要申请一个证书（我们使用standalone的方式，然后，你需要输入你的电子邮件和你的域名）：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ sudo certbot certonly --standalone&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;证书默认生成在 &lt;code&gt;/etc/letsencrypt/live/&amp;lt;YOUR.DOMAIN.COM/&amp;gt;&lt;/code&gt; 目录下，这个证书90天后就过期了，所以，需要使用一个 cron job 来定期更新（稍后给出）&lt;/p&gt; &#xA;&lt;p&gt;接下来就是启动 gost 服务了，我们这里还是使用 Docker 的方式建立 gost 服务器。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/bin/bash&#xA;&#xA;# 下面的四个参数需要改成你的&#xA;DOMAIN=&#34;YOU.DOMAIN.NAME&#34;&#xA;USER=&#34;username&#34;&#xA;PASS=&#34;password&#34;&#xA;PORT=443&#xA;&#xA;BIND_IP=0.0.0.0&#xA;CERT_DIR=/etc/letsencrypt&#xA;CERT=${CERT_DIR}/live/${DOMAIN}/fullchain.pem&#xA;KEY=${CERT_DIR}/live/${DOMAIN}/privkey.pem&#xA;sudo docker run -d --name gost \&#xA;    -v ${CERT_DIR}:${CERT_DIR}:ro \&#xA;    --net=host ginuerzh/gost \&#xA;    -L &#34;http2://${USER}:${PASS}@${BIND_IP}:${PORT}?cert=${CERT}&amp;amp;key=${KEY}&amp;amp;probe_resist=code:404&amp;amp;knock=www.google.com&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;上面这个脚本，你需要配置：域名(&lt;code&gt;DOMAIN&lt;/code&gt;), 用户名 (&lt;code&gt;USER&lt;/code&gt;), 密码 (&lt;code&gt;PASS&lt;/code&gt;) 和 端口号(&lt;code&gt;PORT&lt;/code&gt;) 这几个变量。&lt;/p&gt; &#xA;&lt;p&gt;关于 gost 的参数， 你可以参看其文档：&lt;a href=&#34;https://docs.ginuerzh.xyz/gost/&#34;&gt;Gost Wiki&lt;/a&gt;，上面我设置一个参数 &lt;code&gt;probe_resist=code:404&lt;/code&gt; 意思是，如果服务器被探测，或是用浏览器来访问，返回404错误，也可以返回一个网页（如：&lt;code&gt;probe_resist=file:/path/to/file.txt&lt;/code&gt; 或其它网站 &lt;code&gt;probe_resist=web:example.com/page.html&lt;/code&gt;）&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：开启了探测防御功能后，当认证失败时服务器默认不会响应 &lt;code&gt;407 Proxy Authentication Required&lt;/code&gt;，但某些情况下客户端需要服务器告知代理是否需要认证(例如Chrome中的 SwitchyOmega 插件)。通过knock参数设置服务器才会发送407响应。对于上面的例子，我们的&lt;code&gt;knock&lt;/code&gt;参数配置的是&lt;code&gt;www.google.com&lt;/code&gt;，所以，你需要先访问一下 &lt;code&gt;https://www.google.com&lt;/code&gt; 让服务端返回一个 &lt;code&gt;407&lt;/code&gt; 后，SwitchyOmega 才能正常工作。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：如果认证信息（也就是用户名和密码）中包含特殊字符，则可以（应该是必须！否则客户端一侧会有很多不兼容）通过auth参数来设置，下面是使用 &lt;code&gt;auth&lt;/code&gt; 参数的例子（注意，需要 gost 在 2.9.2+ 以上版本）：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;DOMAIN=&#34;YOU.DOMAIN.NAME&#34;&#xA;USER=&#34;username&#34;&#xA;PASS=&#34;password&#34;&#xA;PORT=443&#xA;AUTH=$(echo -n ${USER}:${PASS} | base64)&#xA;&#xA;BIND_IP=0.0.0.0&#xA;CERT_DIR=/etc/letsencrypt&#xA;CERT=${CERT_DIR}/live/${DOMAIN}/fullchain.pem&#xA;KEY=${CERT_DIR}/live/${DOMAIN}/privkey.pem&#xA;sudo docker run -d --name gost \&#xA;    -v ${CERT_DIR}:${CERT_DIR}:ro \&#xA;    --net=host ginuerzh/gost \&#xA;    -L &#34;http2://${BIND_IP}:${PORT}?auth=${AUTH}&amp;amp;cert=${CERT}&amp;amp;key=${KEY}&amp;amp;probe_resist=code:404&amp;amp;knock=www.google.com&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;如无意外，你的服务就启起来了。你可以使用下面的命令验证你的 gost 服务是否正常。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl -v &#34;https://www.google.com&#34; --proxy &#34;https://DOMAIN&#34; --proxy-user &#39;USER:PASS&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;接下来就是证书的自动化更新。&lt;/p&gt; &#xA;&lt;p&gt;可以使用命令 &lt;code&gt;crontab -e&lt;/code&gt; 来编辑定时任务：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;0 0 1 * * /usr/bin/certbot renew --force-renewal&#xA;5 0 1 * * /usr/bin/docker restart gost&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;这样，服务器就配置完成了。客户端请移动后面的客户端章节。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;使用 Cloudflare 的注意事项&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;上述的方法并不支持 Cloudflare CDN，如果你想使用了 Cloudflare CDN，你需要使用 WebSocket 协议，如下所示，你需要使用 &lt;code&gt;mwss&lt;/code&gt; 协议。&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;gost -L=mwss://username:password@:443?cert=/path/to/your/cert/file\&amp;amp;key=/path/to/your/key/file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;在 CloudFlare 上，请将TLS/SSL设置为 &lt;strong&gt;完全&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;如果你的客户端只能使用 socks 协议，你还要在客户端这边转一下：&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;gost -L socks://:YourLocalPort -F mwss://username:password@example.com:443&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;然后在其他软件中设置socks5代理即可&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;3.4 设置 ShadowSocks 服务&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;（注：ShadowSocks 被查的机率非常大，不推荐使用）&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;（如果有隧道转发，可以使用）&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;ShadowSocks 的 Docker 启动脚本 （其中的 &lt;code&gt;SS_PORT&lt;/code&gt; 和 &lt;code&gt;SS_PASSWD&lt;/code&gt; 需要重新定义一下）&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/bin/bash&#xA;&#xA;SS_PORT=1984&#xA;SS_PASSWD=MyPasswd&#xA;&#xA;sudo docker run -dt --name ss \&#xA;   -p ${SS_PORT}:${SS_PORT} mritd/shadowsocks \&#xA;   -s &#34;-s 0.0.0.0 -p ${SS_PORT} -m aes-256-cfb -k ${SS_PASSWD} --fast-open&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3.5 设置L2TP/IPSec服务&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;（注：VPN方式被查的机率非常大，不推荐使用）&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;L2TP/IPSec 的启动脚本，其中的三个环境变量 &lt;code&gt;USER&lt;/code&gt;， &lt;code&gt;PASS&lt;/code&gt; 和 &lt;code&gt;PSK&lt;/code&gt; 需要替换一下。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/bin/bash&#xA;&#xA;USER=someone&#xA;PASS=password&#xA;PSK=psk_key&#xA;&#xA;sudo docker run -d  --privileged \&#xA;    -e PSK=${PSK} \&#xA;    -e USERNAME=${USER} -e PASSWORD=${PASS} \&#xA;    -p 500:500/udp \&#xA;    -p 4500:4500/udp \&#xA;    -p 1701:1701/tcp \&#xA;    -p 1194:1194/udp  \&#xA;    siomiz/softethervpn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3.6 设置 PPTP 服务&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;（注：PPTP 不安全，请不要使用）&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo docker run -d --privileged --net=host&#xA;                -v {/path_to_file/chap-secrets}:/etc/ppp/chap-secrets \&#xA;                mobtitude/vpn-pptp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PPTP 使用 &lt;code&gt;/etc/ppp/chap-secrets&lt;/code&gt; 文件设置用户名和密码，所以你需要给docker容器提供这个文件，下面是这个文件的示例：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-conf&#34;&gt;# Secrets for authentication using PAP&#xA;# client    server      secret           acceptable local IP addresses&#xA;  fuckgfw   *           whosyourdaddy    *&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;4. 客户端设置&lt;/h2&gt; &#xA;&lt;h3&gt;4.1 gost 客户端&lt;/h3&gt; &#xA;&lt;p&gt;大多数的代理服务都支持 https 的代理，但是我们需要智能代理（也就是该翻的时候翻，不用翻的时候不翻），那么我们可以重用 ShadowSocks 的客户端。&lt;/p&gt; &#xA;&lt;p&gt;对于电脑来说，你同样可以 &lt;a href=&#34;https://github.com/ginuerzh/gost/releases&#34;&gt;下载 gost 程序&lt;/a&gt;，然后使用下面的命令行：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gost -L ss://aes-128-cfb:passcode@:1984 -F &#39;https://USER:PASS@DOMAIN:443&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;这样用 gost 在你的本机启动了一个 &lt;code&gt;ShadowSocks&lt;/code&gt; 的服务，然后，把请求转到你在上面配置的 HTTPS服务器上，这样就完成转接。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;┌─────────────┐  ┌─────────────┐            ┌─────────────┐&#xA;│ ShadowSocks │  │             │            │             │&#xA;│    Client   ├──► Gost Client ├────────────► Gost Server │&#xA;│ (PAC Auto)  │  │             │            │             │&#xA;└─────────────┘  └─────────────┘            └─────────────┘&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;ShadowSocks Client 主要完成：自动设置操作系统代理服务器的 pac （自动设置翻墙或是不翻墙的路由）&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;这样，你的ShadowSocks客户端只需要简单的配置一个本机的 SS 配置就好了。&lt;/p&gt; &#xA;&lt;p&gt;对于手机端&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iPhone，可以考虑使用 &lt;code&gt;ShadowRocket&lt;/code&gt; （需要付费），其中使用 HTTPS 的代理，配置上就好了。&lt;/li&gt; &#xA; &lt;li&gt;Android，可以考虑使用这个Plugin - &lt;a href=&#34;https://github.com/xausky/ShadowsocksGostPlugin&#34;&gt;ShadowsocksGostPlugin&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;注明&lt;/strong&gt;：如果你之前使用了Chrome插件 SwitchyOmega，如果无法直接配置HTTPS代理，具体原因可能是因为你设置了&lt;code&gt;probe_resist&lt;/code&gt;以开启探测防御功能。这里，你需要在服务器端设置 &lt;code&gt;knock&lt;/code&gt; 参数（参看 &lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#33-%E7%94%A8-gost-%E8%AE%BE%E7%BD%AE-https-%E6%9C%8D%E5%8A%A1&#34;&gt;用 Gost 设置 HTTPS 服务&lt;/a&gt; 中的“注意”一节 ）&lt;/p&gt; &#xA;&lt;p&gt;或是，干脆使用gost客户端在本机启动一个 SOCKS5的代理服务用来代替（&lt;code&gt;gost -L socks5://:1080 -F &#39;https://USER:PASS@DOMAIN:443&#39;&lt;/code&gt;），然后在 SwitchyOmega 配置代理为&#39;127.0.0.1:1080&#39;即可。比如:&lt;/p&gt; &#xA;&lt;h3&gt;4.2 Shadowsocks 客户端&lt;/h3&gt; &#xA;&lt;p&gt;对于 Shadowsocks 客户端，可以到这里查看 &lt;a href=&#34;https://shadowsocks.org/en/download/clients.html&#34;&gt;Shadowsocks Clients&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MacOS 上你可以下载 &lt;a href=&#34;https://github.com/shadowsocks/ShadowsocksX-NG/releases&#34;&gt;ShadowsocksX-NG&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Windows上你可以下载 &lt;a href=&#34;https://github.com/shadowsocks/shadowsocks-windows/releases&#34;&gt;Shadowsocks-Windows&lt;/a&gt;，需要先安装 &lt;a href=&#34;https://dotnet.microsoft.com/download/dotnet-framework-runtime&#34;&gt;.NET Framework&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Android的客户端，你可以用手机访问并下载 &lt;a href=&#34;https://github.com/shadowsocks/shadowsocks-android/releases&#34;&gt;Shadowsocks-Android&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;iPhone 端就比较麻烦了。因为国内全都被下架了。 &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;你需要注册一个美国的苹果ID.&lt;/li&gt; &#xA;   &lt;li&gt;然后 iTunes/App Store 用这个美区的ID登录（不是退出iCloud ，而是退出App Store）&lt;/li&gt; &#xA;   &lt;li&gt;然后搜索 &lt;code&gt;Potatso Lite&lt;/code&gt; ，&lt;code&gt;ShadowRocket&lt;/code&gt;, &lt;code&gt;Wingy&lt;/code&gt;, &lt;code&gt;Quantumult&lt;/code&gt; 等。（我使用前两个）&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;关于如何注册美区Apple ID账号，你可以参看如下的这几篇文章（我不保证这些文章可不可用，但是你可以自行Google）。 &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/36574047&#34;&gt;5分钟注册美国区Apple ID（18年亲测有效）&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://www.jianshu.com/p/b32da641e849&#34;&gt;2018年6月亲测：注册美国地区苹果apple ID帐号终极教程&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/ziyuzhiye/article/details/82769129&#34;&gt;iOS开发之注册美国Apple Id不需要绑定信用卡，亲测可用&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;4.3 VPN 客户端&lt;/h3&gt; &#xA;&lt;p&gt;对于L2TP/IPSec，几乎所有的客户端操作系统（无论是Windows/Mac/Linux的电脑，还是iPhone/Android）都支持，你可以自行Google。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.it-shit.com/74.html&#34;&gt;Mac OS X PPTP/L2TP设置教程&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nic.upc.edu.cn/2016/0928/c7809a132077/page.htm&#34;&gt;Windows 7操作系统配置L2TP VPN方法&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;5. 流量伪装和其它方式&lt;/h2&gt; &#xA;&lt;p&gt;无论你用VPN，SS，SSR，都有可能被识别，&lt;strong&gt;只有使用 HTTP over TLS 的样子，才会跟正常的流量混在一起，很难被识别&lt;/strong&gt;，所以，目前来说，V2Ray客户端 + Nginx + V2Ray服务端的方式，或是gost的HTTPS的方式，基本上来说，在网络四层上看到的都是TLS的包，很难被识别。这种代理服务我觉得只能做探测，或是得到更多的算力来做统计学分析。所以，V2Ray 和 gost 的服务器端用 nginx 再挡一道，那么就很难被发现了。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt; 说句老实话，我其时并不想害怕别人知道自己的上什么样的网站，因为我觉得我访问的都是合法的网站，但是就今天这个局势我也没办法——为什么要让像我这样的光明正大的良民搞得跟偷鸡摸狗之徒一样……&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;5.1 V2Ray&lt;/h3&gt; &#xA;&lt;p&gt;V2Ray 可以配置成一个非常隐蔽的代理软件。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;V2Ray 用户手册：&lt;a href=&#34;https://www.v2fly.org&#34;&gt;https://www.v2fly.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;V2Ray 项目地址：&lt;a href=&#34;https://github.com/v2fly/v2ray-core&#34;&gt;https://github.com/v2fly/v2ray-core&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;一般来说，祼用 V2Ray 不是一个很好的方式，现在比较流行的是使用nginx来代理，也就是 V2Ray + WebSocket + TLS + Nginx，可以参看这篇文章《&lt;a href=&#34;https://guide.v2fly.org/advanced/wss_and_web.html&#34;&gt;V2Ray+WebSocket+TLS+Nginx配置与使用教程&lt;/a&gt;》（需要翻墙）。&lt;/p&gt; &#xA;&lt;p&gt;我个人觉得，配置起来比较复杂，而且环节太多，不如直接用 &lt;code&gt;gost&lt;/code&gt; 的 https/http2 的方式配置起来简单，所以，没有放在前面。&lt;/p&gt; &#xA;&lt;h3&gt;5.2 Brook&lt;/h3&gt; &#xA;&lt;p&gt;Brook是一个由 Go语言编写的跨平台代理软件，支持 Linux/MacOS/Windows/Android/iOS 各个平台。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Brook Github项目：&lt;a href=&#34;https://github.com/txthinking/brook&#34;&gt;https://github.com/txthinking/brook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Github Wiki教程：&lt;a href=&#34;https://github.com/txthinking/brook/wiki/%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E(%E4%B8%AD%E6%96%87)&#34;&gt;https://github.com/txthinking/brook/wiki/使用说明(中文)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;服务器一行命令安装：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/brook.sh &amp;amp;&amp;amp; chmod +x brook.sh &amp;amp;&amp;amp; bash brook.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;运行 &lt;code&gt;brook.sh&lt;/code&gt; 会出菜单项，你可以按菜单项来，主要就是设置端口号，密码。很简单的，我这里就不截图了，因为这个脚本运行起来中文菜单式的。&lt;/p&gt; &#xA;&lt;p&gt;然后你可以在 Brook 项目的 Github 首页上下载不同平台的客户端。设置起来也很简单！&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;注意: 如果运行出现下载错误，可能是因为brook的下载文件名问题，你需要自己修改一下脚本：&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Download_brook(){&#xA; &#x9;[[ ! -e ${file} ]] &amp;amp;&amp;amp; mkdir ${file}&#xA; &#x9;cd ${file}&#xA; &#x9;if [[ ${bit} == &#34;x86_64&#34; ]]; then&#xA;-&#x9;&#x9;wget --no-check-certificate -N &#34;https://github.com/txthinking/brook/releases/download/${brook_new_ver}/brook&#34;&#xA;+&#x9;&#x9;wget --no-check-certificate -N &#34;https://github.com/txthinking/brook/releases/download/${brook_new_ver}/brook_linux_amd64&#34;&#xA;+&#x9;&#x9;mv brook_linux_amd64 brook&#xA; &#x9;else&#xA; &#x9;&#x9;wget --no-check-certificate -N &#34;https://github.com/txthinking/brook/releases/download/${brook_new_ver}/brook_linux_386&#34;&#xA; &#x9;&#x9;mv brook_linux_386 brook&#xA; &#x9;fi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;6. 针对 IP 被封的解决方案&lt;/h2&gt; &#xA;&lt;p&gt;花钱购买的 VPS 即便做了流量伪装依然有很大的几率 IP 被封锁，大多 VPS 服务商并不提供更换 IP 的服务，使用 CDN 可以让被封锁的 VPS 继续发挥翻墙功能。&lt;/p&gt; &#xA;&lt;h3&gt;6.1 Cloudflare&lt;/h3&gt; &#xA;&lt;p&gt;Cloudflare 是一个 CDN 服务商，目前国内依然能正常的访问，可以作为跳板来实现翻墙。&lt;/p&gt; &#xA;&lt;p&gt;注册 Cloudflare 帐号，并有一个空闲域名（三级域名即可），交给 Cloudflare 托管并将域名指向被封的 VPS IP，注意开启 Proxied 并且 SSL-TLS 使用 Flexible 选项。&lt;/p&gt; &#xA;&lt;p&gt;Cloudflare 只需免费方案足以，不必花钱。&lt;/p&gt; &#xA;&lt;p&gt;关于优选IP，可以手动更改本地hosts文件指向最佳IP。&lt;/p&gt; &#xA;&lt;h3&gt;6.2 V2Ray&lt;/h3&gt; &#xA;&lt;p&gt;VPS 上正常安装并配置好 V2Ray，注意两点:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;传输协议必须要使用 ws&lt;/li&gt; &#xA; &lt;li&gt;要使用 80 或者 8080 端口&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;如果端口有其他用途，那么用 Nginx/Caddy 之类软件，做一个 WebSocket proxy 到 V2Ray 即可。&lt;/p&gt; &#xA;&lt;h3&gt;6.3 补充&lt;/h3&gt; &#xA;&lt;p&gt;客户端注意使用网址来连接。&lt;/p&gt; &#xA;&lt;p&gt;目前支持 WebSocket 的免费 CDN 似乎只有 Cloudflare 一家，国内 CDN 服务商既不支持也不安全，不要考虑了。如果有更好的服务商欢迎补充。&lt;/p&gt; &#xA;&lt;p&gt;网络延迟比直连增加不少，如果是频繁操作会很痛苦。网络带宽如果运气好可能比直连还优化了，用来看 Youtube 搞不好更流畅。&lt;/p&gt; &#xA;&lt;h2&gt;7. 家用透明网关&lt;/h2&gt; &#xA;&lt;h3&gt;7.1 OpenWRT 路由器&lt;/h3&gt; &#xA;&lt;p&gt;所谓透明网关的意思是，一切都交给网关来做。最好的方式是你需要一个 OpenWRT 的路由器，推荐使用华硕的路由器，贵是贵一些，但是这几年用下来，非常不错。我用的是 &lt;strong&gt;华硕（ASUS） RT-AC68U 1900M AC 双频智能无线路由路&lt;/strong&gt; 。&lt;/p&gt; &#xA;&lt;p&gt;路由器买来后，要刷一下固件。首先 Asuswrt 是华硕公司为他的路由器所开发的固件。Asuswrt-merlin是一个对Asuswrt固件二次开发进行各种改进和修正的项目。源代码在这里：&lt;a href=&#34;https://github.com/RMerl/asuswrt-merlin&#34;&gt;https://github.com/RMerl/asuswrt-merlin&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;不必担心把路由器刷废了，华硕的路由器可以让你一键重置回来&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1）下载固件&lt;/strong&gt;。先到 &lt;a href=&#34;https://asuswrt.lostrealm.ca/download&#34;&gt;https://asuswrt.lostrealm.ca/download&lt;/a&gt; 下载相应的固件，并解压。（我下载的是 &lt;code&gt;RT-AC68U_380.61_0.zip&lt;/code&gt; ）&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2）升级固件&lt;/strong&gt;。登录到你的路由器后台 &lt;code&gt;http://192.168.1.1/&lt;/code&gt; ，在 &lt;code&gt;系统管理&lt;/code&gt; -&amp;gt; &lt;code&gt;固件升级&lt;/code&gt; 中上传固件文件（我上传的是：&lt;code&gt;RT-AC68U_380.61_0.trx&lt;/code&gt;）&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3）打开 JFFS 分区&lt;/strong&gt;。&lt;code&gt;系统管理&lt;/code&gt; -&amp;gt; &lt;code&gt;系统设置&lt;/code&gt; -&amp;gt; &lt;code&gt;Persistent JFFS2 partition&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Format JFFS partition at next boot&lt;/code&gt; - &lt;code&gt;否&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Enable JFFS custom scripts and configs&lt;/code&gt; - &lt;code&gt;是&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;4）打开 ssh 登录&lt;/strong&gt;。 &lt;code&gt;系统管理&lt;/code&gt; -&amp;gt; &lt;code&gt;系统设置&lt;/code&gt; -&amp;gt; &lt;code&gt;SSH Daemon&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Allow SSH password login&lt;/code&gt; - &lt;code&gt;是&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;接下来，在 WiFi 路由器上安装 Clash，就可以了。&lt;/p&gt; &#xA;&lt;p&gt;大概的示意图如下所示。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;        Phone/PC/Pad （无需设置）&#xA;             │&#xA;             │&#xA;             │ 1&#xA;             │&#xA;    ┌────────▼──────┐&#xA;    │               │&#xA;    │  WiFi Router  │ （安装 Clash 网关）&#xA;    │               │&#xA;    └─────┬────┬────┘&#xA;          │    │&#xA;          │    │ 2&#xA;          │    └────────► 墙内 - China LAN&#xA;       3  │&#xA;    ┌─────▼──────┐&#xA;    │    VPS     │&#xA;    │   Proxy    │&#xA;    └─────┬──────┘&#xA;          │&#xA;          │&#xA;          ▼&#xA;        墙外 - Internet WAN&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;7.2 通过树莓派做旁路网关&lt;/h3&gt; &#xA;&lt;p&gt;如果你的路由器不能刷 OpenWRT，也就是没法通过SSH登录上去装软件，你就用一个别的设备。比如用一个树莓派。我正好有一个很老旧的树莓派，刷了一个老旧的 Debian 7.5的操作系统。&lt;/p&gt; &#xA;&lt;p&gt;把它连上你的路由器上，然后，&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;你需要把你设备上的IP地址、网关和DNS服务器都要手动设置到这个树莓派上。&lt;/li&gt; &#xA; &lt;li&gt;于是，所有的路由就会通过路由器转到树莓派上，再由树莓派决定是否要走代理。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;大概的示意图如下所示。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1 --&amp;gt; 2 是设备把所有的请求都发给树莓派。&lt;/li&gt; &#xA; &lt;li&gt;3 --&amp;gt; 3.1 或 3.2 是由树莓派来决走是否翻墙。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;        Phone/PC/Pad （设置&#34;网关&#34;和&#34;DNS&#34;为树莓派）&#xA;             │&#xA;             │&#xA;             │ 1&#xA;             │                    （安装 Clash 网关）&#xA;    ┌────────▼──────┐      2       ┌───────────┐&#xA;    │               ├──────────────►           │&#xA;    │  WiFi Router  │              │   树莓派   │&#xA;    │               ◄──────────────┤           │&#xA;    └─────┬────┬────┘      3       └───────────┘&#xA;          │    │&#xA;          │    │ 3.2&#xA;          │    └────────► 墙内 - China LAN&#xA;      3.1 │&#xA;    ┌─────▼──────┐&#xA;    │    VPS     │&#xA;    │   Proxy    │&#xA;    └─────┬──────┘&#xA;          │&#xA;          │&#xA;          ▼&#xA;        墙外 - Internet WAN&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;7.3 安装 Clash&lt;/h3&gt; &#xA;&lt;p&gt;Clash 的 Github项目是：&lt;a href=&#34;https://github.com/Dreamacro/clash&#34;&gt;Dreamacro/clash&lt;/a&gt; ，在它的 Release 页面上，你可以找到相关的下载。（注：在本文更新的时候，如果你需要支持 Tun，你需要下载 Clash 的 &lt;a href=&#34;https://github.com/Dreamacro/clash/releases/tag/premium&#34;&gt;Premium 版本&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Clash 支持很多翻墙协议：ShadowSocks(R), Vmess, Socks5, HTTP(s)，Snell，Trojan。&lt;/p&gt; &#xA;&lt;p&gt;在你的 OpenWRT 或 树莓派 下用 &lt;code&gt;uname -m&lt;/code&gt; 查看一下你的硬件架构是什么的，比如，我的是华硕和树莓派都是 &lt;code&gt;armv7l&lt;/code&gt; 的，所以，需要下载 &lt;code&gt;clash-linux-armv7-....&lt;/code&gt;的版本（注：根据 clash 官方仓库 &lt;a href=&#34;https://github.com/Dreamacro/clash/issues/189&#34;&gt;Dreamacro/clash#189&lt;/a&gt; 系列固件不适用 armv7l 架构的 AC68U，需选择 armv5）。 下载完解压后，加个可执行权限 &lt;code&gt;chmod +x clash&lt;/code&gt; 就可以运行了，不过，还差一个界面和两个配置文件，它们的目录关系如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;├── clash                &amp;lt;- 建一个 clash 的目录&#xA;│&amp;nbsp;&amp;nbsp; ├── clash            &amp;lt;- 运行文件&#xA;│&amp;nbsp;&amp;nbsp; ├── config.yaml      &amp;lt;- 配置文件&#xA;│&amp;nbsp;&amp;nbsp; ├── Country.mmdb     &amp;lt;- IP地址库&#xA;│&amp;nbsp;&amp;nbsp; └── ui               &amp;lt;- Clash 的 UI&#xA;│&amp;nbsp;&amp;nbsp;     ├── index.html&#xA;│&amp;nbsp;&amp;nbsp;     ├── ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;UI界面可以到 &lt;a href=&#34;https://github.com/haishanh/yacd&#34;&gt;haishah/yacd&lt;/a&gt; 下载。放到clash的配置目录下 &lt;code&gt;ui&lt;/code&gt; 目录下&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;一个是 &lt;code&gt;Country.mmdb&lt;/code&gt; 这是IP地址的在哪个国家的数据库。你需要到这里下载 - &lt;a href=&#34;https://github.com/Dreamacro/maxmind-geoip/releases/latest/download/Country.mmdb&#34;&gt;Country.mmdb&lt;/a&gt; （当然，clash启动时，会自动下载，我这里给你一个手动下载的链接）&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;另一个是 &lt;code&gt;config.yaml&lt;/code&gt; 文件，这个文件详细解释可参看 - &lt;a href=&#34;https://github.com/Dreamacro/clash/wiki/configuration&#34;&gt;官方Wiki&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;下面是个示例：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;port: 7890&#xA;socks-port: 7891&#xA;redir-port: 7892&#xA;mixed-port: 7893&#xA;ipv6: false&#xA;allow-lan: true&#xA;mode: Rule&#xA;log-level: info&#xA;external-controller: &#39;0.0.0.0:9090&#39;&#xA;external-ui: ui&#xA;secret: &#39;&#39;&#xA;tun:&#xA;  enable: true&#xA;  stack: system&#xA;  dns-hijack:&#xA;    - tcp://8.8.8.8:53&#xA;    - udp://8.8.8.8:53&#xA;dns:&#xA;  enable: true&#xA;  ipv6: false&#xA;  listen: 0.0.0.0:53&#xA;  default-nameserver:&#xA;    - 114.114.114.114&#xA;  #enhanced-mode: redir-host&#xA;  enhanced-mode: fake-ip #如果要玩netflix，需要使用fake-ip&#xA;  fake-ip-range: 198.18.0.1/16&#xA;  nameserver:&#xA;    - 114.114.114.114&#xA;    - 223.5.5.5&#xA;    - tls://8.8.8.8:853&#xA;  fallback:&#xA;    - tls://8.8.8.8:853&#xA;&#xA;# 两个代理服务器&#xA;proxies:&#xA;  # http&#xA;  - name: &#34;https01&#34;&#xA;    type: http&#xA;    server: https.server.domain&#xA;    port: 443&#xA;    username: user&#xA;    password: &#34;password&#34;&#xA;    tls: true # https&#xA;    skip-cert-verify: true&#xA;  - name: &#34;https01&#34;&#xA;    type: http&#xA;    server: https.server.domain&#xA;    port: 443&#xA;    username: user&#xA;    password: &#34;passowrd&#34;&#xA;    tls: true # https&#xA;    skip-cert-verify: true&#xA;&#xA;# 配置 Group&#xA;proxy-groups:&#xA;  # 自动切换&#xA;  - name: &#34;auto&#34;&#xA;    type: url-test&#xA;    proxies:&#xA;      - us01_https&#xA;      #- us02_https&#xA;      #- hk_https&#xA;    # tolerance: 150&#xA;    url: &#39;https://www.google.com/&#39;&#xA;    interval: 300&#xA;  # 按需选择 - 可以在UI上选择&#xA;  - name: &#34;netflix&#34;&#xA;    type: select&#xA;    proxies:&#xA;      - us01_https&#xA;      - us02_https&#xA;      - hk_https&#xA;&#xA;rules:&#xA;# LAN&#xA;  - DOMAIN-SUFFIX,local,DIRECT&#xA;  - IP-CIDR,127.0.0.0/8,DIRECT&#xA;  - IP-CIDR,172.16.0.0/12,DIRECT&#xA;  - IP-CIDR,192.168.0.0/16,DIRECT&#xA;  - IP-CIDR,10.0.0.0/8,DIRECT&#xA;&#xA;# Netflix&#xA;  - DOMAIN-SUFFIX,fast.com,netflix&#xA;  - DOMAIN-SUFFIX,api-global.netflix.com,netflix&#xA;  - DOMAIN-SUFFIX,netflix.com,netflix&#xA;  - DOMAIN-SUFFIX,netflix.net,netflix&#xA;  - DOMAIN-SUFFIX,nflxext.com,netflix&#xA;  - DOMAIN-SUFFIX,nflximg.com,netflix&#xA;  - DOMAIN-SUFFIX,nflximg.net,netflix&#xA;  - DOMAIN-SUFFIX,nflxso.net,netflix&#xA;  - DOMAIN-SUFFIX,nflxvideo.net,netflix&#xA;&#xA;# 最终规则（除了中国区的IP之外的，全部翻墙）&#xA;  - GEOIP,CN,DIRECT&#xA;  - MATCH,auto&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;更多的规则网上可以找到很多，也可以参看这里：&lt;a href=&#34;https://github.com/Hackl0us/SS-Rule-Snippet/raw/master/LAZY_RULES/clash.yaml&#34;&gt;SS-Rule-Snippet/LAZY_RULES/clash.yaml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;这个时候你就可以启动 clash 了：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;/path/to/clash/cash -d /path/to/clash &amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;然后，你就可以把你的上网设备上的 路由网关 和 DNS 服务器都手动地配置成这个网关就好了（OpenWRT应该不用配置了，树莓派的方式需要手动配置一下）&lt;/p&gt; &#xA;&lt;h3&gt;7.4 设置 iptables 转发&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;iptables -t nat -N CLASH&#xA;iptables -t nat -A CLASH -d 10.0.0.0/8 -j RETURN&#xA;iptables -t nat -A CLASH -d 127.0.0.0/8 -j RETURN&#xA;iptables -t nat -A CLASH -d 169.254.0.0/16 -j RETURN&#xA;iptables -t nat -A CLASH -d 172.16.0.0/12 -j RETURN&#xA;iptables -t nat -A CLASH -d 192.168.0.0/16 -j RETURN&#xA;iptables -t nat -A CLASH -d 224.0.0.0/4 -j RETURN&#xA;iptables -t nat -A CLASH -d 240.0.0.0/4 -j RETURN&#xA;iptables -t nat -A CLASH -p tcp -j REDIRECT --to-ports 7892&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;然后，你可以保存一下这些 iptables 的规则&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt; iptables-save &amp;gt; /etc/iptables.up.rules&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;编辑 &lt;code&gt;/etc/network/if-pre-up.d/iptables&lt;/code&gt;，在网卡启动的时候加载这些规则&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/bin/sh&#xA;/sbin/iptables-restore &amp;lt; /etc/iptables.up.rules&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;然后，再 &lt;code&gt;chmod +x /etc/network/if-pre-up.d/iptables&lt;/code&gt; 加上可执行权限就好了。&lt;/p&gt; &#xA;&lt;h2&gt;8. 数据中心透明网关&lt;/h2&gt; &#xA;&lt;p&gt;这里仅针对 AWS 进行说明，其它云平台应该大同小异，大家可以补充。&lt;/p&gt; &#xA;&lt;h3&gt;8.1 AWS 网络构建&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;构建一个 &lt;code&gt;172.20.0.0/16&lt;/code&gt; 的 VPC，分成两个子网：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;有公网IP的公有子网 - &lt;code&gt;172.20.1.0/24&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;无公网IP的私有子网 - &lt;code&gt;172.20.2.0/24&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;在公有子网里创建 &lt;a href=&#34;https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/VPC_NAT_Instance.html&#34;&gt;EC2 NAT Instance&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;创建时，指定私网IP为 &lt;code&gt;172.20.1.1&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;（Option）为该实例分配弹性IP，可成为外网访问内网的跳板机&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;建立路由规则&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;创建“互联网网关”，并把“互联网网关”添加到公有子网 &lt;code&gt;172.20.1.0/24&lt;/code&gt; 的路由表中&lt;/li&gt; &#xA;   &lt;li&gt;把 EC2 NAT Instance &lt;code&gt;172.20.1.1&lt;/code&gt; 添加到私有子网&lt;code&gt;172.20.2.0/24&lt;/code&gt;的路由表中。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;于是整个网络就如下所示。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;                                    ┌──────────┐&#xA;                                    │          │&#xA;                                    │          │&#xA;                                    └──────────┘&#xA;          弹性IP                      互联网网关&#xA;        ┌───────────────┐                ▲&#xA;        │xxx.xxx.xxx.xxx├─┐              │&#xA;        └───────────────┘ │  ┌───────────┘&#xA;                          │  │&#xA;                  ┌───────┼──┼────────┐       ┌───────────────────┐&#xA;                  │       │  │        │       │                   │&#xA;                  │     ┌─┴──▼──┐     │       │  ┌─┐ ┌─┐ ┌─┐ ┌─┐  │&#xA;  Public Network  │     │       │◄────┼───┬───┼─►└─┘ └─┘ └─┘ └─┘  │  Private Network&#xA;                  │     └───────┘     │   │   │                   │&#xA;                  │  EC2 NAT Instance │   │   │  ┌─┐ ┌─┐ ┌─┐ ┌─┐  │&#xA;                  │    172.20.1.1     │   ├───┼─►└─┘ └─┘ └─┘ └─┘  │&#xA;                  │                   │   │   │                   │&#xA;                  │   (NAT Instance)  │   │   │    ┌─┐ ┌─┐ ┌─┐    │&#xA;                  │                   │   └───┼─►  └─┘ └─┘ └─┘    │&#xA;                  │                   │       │                   │&#xA;                  └───────────────────┘       └───────────────────┘&#xA;&#xA;                      172.20.1.0/24              172.20.2.0/24&#xA;                           ▲                            ▲&#xA;                    subnet │                            │ subnet&#xA;                           │                            │&#xA;                           └──────────  VPC  ───────────┘&#xA;                                   172.20.0.0/16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注：你需要认真的按照 &lt;a href=&#34;https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/VPC_NAT_Instance.html&#34;&gt;EC2 NAT Instance&lt;/a&gt; 的文档进行设置这个NAT实例。尤其需要设置下面几项：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo sysctl -w net.ipv4.ip_forward=1&#xA;sudo iptables -A FORWARD -i eth0 -j ACCEPT&#xA;sudo iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;顺便科普一下：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;net.ipv4.ip_forward&lt;/code&gt; 是内核参数，主要是用来把Linux当成路由器来用的参数。一般来说，一个路由器至少要有两个网络接口，一个是WAN，的一个是LAN的，为了让LAN和WAN的流量相通，需要进行内核上路由。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;iptables -A FORWARD -i eth0 -j ACCEPT&lt;/code&gt; 通行所有需要转发的包，只有机器成为一个路由器时，需要在两个网卡间进行网络包转发时，才需要配置这条规则。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE&lt;/code&gt; 关键字 &lt;code&gt;MASQUERADE&lt;/code&gt; 意思是“伪装“，NAT的工作原理是就像是一个宿舍收发室对学生宿舍一样，学生宿舍的地址外部不可见，邮递员只看得见整栋宿舍收发室的地址，邮递员把快递交给收发室，收发室再把快递转给学习宿舍（反之，如果学生要对外寄邮件，也是先到收发室，收发室传给邮局）。现在的问题是，所有的学生宿舍如何才能参与到任何快递的通信中，如果把学生宿舍地址发到外部，则没人能把信送回来。如果这个收发室是个自动化的机器人，他要干的事就是，把学生宿舍的地址换成收发室地址。这就是 &lt;code&gt;MASQUERADE&lt;/code&gt; 的意思——&lt;strong&gt;来自具有接收方 IP 地址的本地网络到达 Internet 某处的数据包必须进行修改，也就是让发送方的地址等于路由器的地&lt;/strong&gt;址。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;8.2 安装 Clash&lt;/h3&gt; &#xA;&lt;p&gt;在 EC2 NAT Instance 上安装 clash 透明网关，安装配置参看 &lt;a href=&#34;https://raw.githubusercontent.com/haoel/haoel.github.io/master/#73-%E5%AE%89%E8%A3%85-clash&#34;&gt;7.3 安装 Clash&lt;/a&gt; ，基本一致。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;注：在实际操作中，没有设置 &lt;code&gt;iptables&lt;/code&gt; 转发规则&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;8.3 配置私有子网中的 EC2&lt;/h3&gt; &#xA;&lt;p&gt;只需要配置 &lt;code&gt;/etc/resolv.conf&lt;/code&gt; 文件，把 EC2 NAT Instance 加入其中。如：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-conf&#34;&gt;# /etc/resolv.conf&#xA;nameserver 172.20.1.1  #&amp;lt;--- 透明网关 EC2 NAT 实例&#xA;nameserver 172.20.0.2  #&amp;lt;--- AWS 的 DNS 服务&#xA;search [zone].compute.internal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note&lt;/p&gt; &#xA; &lt;p&gt;新版的 Ubuntu 已经把 DNS Resolver 托管给了 systemd 的 &lt;code&gt;systemd-resolved&lt;/code&gt; 服务，所以需要把 &lt;code&gt;/etc/resolv.conf&lt;/code&gt; 文件改成软链接，指向 &lt;code&gt;systemd-resolved&lt;/code&gt; 的配置文件，如：&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;然后，在 &lt;code&gt;/etc/systemd/resolved.conf&lt;/code&gt; 文件中，把 &lt;code&gt;DNS&lt;/code&gt; 和 &lt;code&gt;Domains&lt;/code&gt; 配置项加上，如：&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-conf&#34;&gt;DNS=172.20.0.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;8.4 私有子网中的 Kubernetes&lt;/h3&gt; &#xA;&lt;p&gt;K8s 里有两组 CoreDNS 部署和配置，一组是边缘的（或是叫本地的），一组是中心的。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;边缘的 Pod 名叫 &lt;code&gt;nodelocaldns&lt;/code&gt;，侦听在本机。如：&lt;code&gt;169.254.25.10:53&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;中心的 Pod 名叫 &lt;code&gt;coredns&lt;/code&gt;，侦听在 cluster IP 上，如：&lt;code&gt;10.233.0.3:53&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;边缘的规则会把k8s的域名 &lt;code&gt;cluster.local&lt;/code&gt;, &lt;code&gt;in-addr.arp&lt;/code&gt; &lt;code&gt;ip6.arpa&lt;/code&gt; 转给中心的 CoreDNS 处理，其它的交给本地的 &lt;code&gt;/etc/resolv.conf&lt;/code&gt; 处理。&lt;/p&gt; &#xA;&lt;p&gt;Kubernetes 会把如下内容打到 Pod 里的 &lt;code&gt;/etc/resolv.conf&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-conf&#34;&gt;nameserver 169.254.25.10&#xA;search default.svc.cluster.local svc.cluster.local cluster.local cn-northwest-1.compute.internal&#xA;options ndots:5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;查看一下 &lt;code&gt;nodelocaldns&lt;/code&gt; 的配置：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl get cm nodelocaldns -n kube-system -o yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;我们可以看到，除了 K8s 自己的域名外，其它的都交给了本机的 &lt;code&gt;/etc/resolv.conf&lt;/code&gt;，如下所示：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;.:53 {&#xA;    errors&#xA;    cache 30&#xA;    reload&#xA;    loop&#xA;    bind 169.254.25.10&#xA;    forward . /etc/resolv.conf  # &amp;lt;--- 注意这条语句&#xA;    prometheus :9253&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;然而，本机的 &lt;code&gt;/etc/resolv.conf&lt;/code&gt; 里有两个 DNS，一个是我们的透明网关，一个是AWS的。而 CoreDNS 的 &lt;code&gt;forward&lt;/code&gt; 策略是随机挑选，所以，这样的会导致，时而交给AWS处理，时而交给我们自己的clash处理。最终导致IP解析紊乱。&lt;/p&gt; &#xA;&lt;p&gt;通过以下命令进行修改：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl edit cm nodelocaldns -n kube-system&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;修改如下：（AWS的归 172.20.0.2， 其它的走我们自己的网关）&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;+    compute.internal:53 {&#xA;+        errors&#xA;+        cache 30&#xA;+        reload&#xA;+        loop&#xA;+        bind 169.254.25.10&#xA;+        forward . 172.20.0.2&#xA;+        prometheus :9253&#xA;+    }&#xA;     .:53 {&#xA;         errors&#xA;         cache 30&#xA;         reload&#xA;         loop&#xA;         bind 169.254.25.10&#xA;-        forward . /etc/resolv.conf&#xA;+        forward . /etc/resolv.conf {&#xA;+            policy sequential&#xA;+        }&#xA;         prometheus: 9253&#xA;     }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;退出保存后，等大约30秒左右配置就会生效。&lt;/p&gt; &#xA;&lt;h2&gt;9. 代理技巧&lt;/h2&gt; &#xA;&lt;p&gt;看到这里，相信已经能够按照上面的教程搭建好自己的上网环境，但是灵活的应用网络，你还需要了解一技巧，比如 SOCKS 协议, http 隧道 和 ssh 网络隧道等。&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zh.m.wikipedia.org/zh-hans/SOCKS&#34;&gt;SOCKS 协议&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zh.m.wikipedia.org/zh-hans/HTTP%E9%9A%A7%E9%81%93&#34;&gt;HTTP 隧道&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;9.1 HTTP 隧道&lt;/h3&gt; &#xA;&lt;p&gt;常见的软件 curl , git, wget 都能通过设置 &lt;code&gt;HTTP_PROXY&lt;/code&gt;,&lt;code&gt;HTTPS_PROXY&lt;/code&gt;，&lt;code&gt;NO_PROXY&lt;/code&gt; 来配置一个网络代理，&lt;code&gt;NO_PROXY&lt;/code&gt;用来配置不需要代理的主机(多个用逗号隔开), 那么我们就可以编写一个 &lt;code&gt;bash &lt;/code&gt; 函数来运行需要走代理的命令:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;with_proxy(){&#xA;   HTTPS_PROXY=http://127.0.0.1:7890 HTTP_PROXY=http://127.0.0.1:7890 &#34;$@&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;把上面的 &lt;code&gt;127.0.0.1:7890&lt;/code&gt; 改成你自己的网络代理, 将上面脚本写入到 &lt;code&gt;~/.bashrc&lt;/code&gt; 中， &lt;code&gt;source ~/.bashrc&lt;/code&gt; 后就能使用 &lt;code&gt;with_proxy&lt;/code&gt; 这个函数了，比如我想要使用代理网络下载一个文件 &lt;code&gt;with_proxy wget https://....&lt;/code&gt;, 想要使用代理网络从 &lt;code&gt;github&lt;/code&gt; clone 一个项目 &lt;code&gt;with_proxy git clone https://...&lt;/code&gt;, 当我们不用 &lt;code&gt;with_proxy&lt;/code&gt; 这个函数的时候命令是不会走代理的，如果在 &lt;code&gt;windows&lt;/code&gt; 上你也想要使用这样的功能，可以使用这个项目&lt;a href=&#34;https://github.com/hellojukay/with-env&#34;&gt;with-env&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;另外，你也可以使用如下的两个 alias:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;SOCKS=&#34;socks5://127.0.0.1:1085&#34;&#xA;alias proxy=&#34;export http_proxy=${SOCKS} https_proxy=${SOCKS} all_proxy=${SOCKS}&#34;&#xA;alias unproxy=&#39;unset all_proxy http_proxy https_proxy&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;这样，你就可以在需要代理的时候输入 &lt;code&gt;proxy&lt;/code&gt;，不需要的时候输入 &lt;code&gt;unproxy&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;9.2 SSH 隧道&lt;/h3&gt; &#xA;&lt;p&gt;另外，我们可以使用 SSH Tunnel 来建立 SOCKS5 的代理（假设本地电脑无法访问，但是某台可以 SSH 的服务器能够访问外网，那么我们就可以使用如下的命令来建议翻墙代理：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ssh -D 1080 -qCN username@server:port&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;解释：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;-D&lt;/code&gt;：本机SOCKS 服务端口&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-q&lt;/code&gt; : quiet 模式，没有输出&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-C&lt;/code&gt; : 数据压缩，可以节约一些带宽&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-N&lt;/code&gt; : 不运行远程命令，只做端口转发&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;登录成功以后,本地 &lt;code&gt;1080&lt;/code&gt;端口会开启一个 &lt;code&gt;SOCKS5&lt;/code&gt; 协议的代理，只要配置好代理就能使用这个端口上网。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;with_proxy(){&#xA;   HTTPS_PROXY=socks5://127.0.0.1:1080 HTTP_PROXY=socks5://127.0.0.1:1080 &#34;$@&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;如果是浏览器，配置好&lt;code&gt;SwitchyOmega&lt;/code&gt;插件也能实现上外网。&lt;/p&gt; &#xA;&lt;h3&gt;9.3 Github / Git SSH 代理&lt;/h3&gt; &#xA;&lt;p&gt;现在访问 Github 速度很慢甚至不通，我们可以使用代理来加速，首先我们需要配置好代理，然后配置好 &lt;code&gt;git&lt;/code&gt; 的代理，这样就能加速 &lt;code&gt;git clone&lt;/code&gt; 和 &lt;code&gt;git push&lt;/code&gt; 了。&lt;/p&gt; &#xA;&lt;p&gt;当你使用 &lt;code&gt;git clone ssh://[user@]server/project.git&lt;/code&gt; 或是 &lt;code&gt;git clone [user@]server:project.git&lt;/code&gt; 的时候，你是在使用 SSH 协议。你需要配置 &lt;code&gt;~/.ssh/config&lt;/code&gt; 来使用代理，比如我的本地有一个Sock5代理，端口是 1085，那么我可以这样配置：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-s&#34;&gt;### github.com&#xA;Host github.com&#xA;    Hostname github.com&#xA;    ProxyCommand nc -x localhost:1085 %h %p&#xA;    # git-for-windows 下可以用 connect 代替 nc&#xA;    # ProxyCommand connect -S localhost:1085 %h %p&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;9.4 Cloudflare Warp 原生 IP&lt;/h3&gt; &#xA;&lt;p&gt;很多我们需要访问的网站都需要使用“原生 IP”，比如：&lt;a href=&#34;https://www.disneyplus.com/&#34;&gt;Disney+&lt;/a&gt;， &lt;a href=&#34;https://chat.openai.com&#34;&gt;ChatGPT&lt;/a&gt;，&lt;a href=&#34;https://www.bing.com/&#34;&gt;New Bing&lt;/a&gt; 等。&lt;/p&gt; &#xA;&lt;p&gt;所谓“原生 IP”就是指该网站的 IP 地址和其机房的 IP 地址是一致的，但是，很多 IDC 提供商的 IP 都是从其它国家调配来的，这导致我们就算是翻墙了，也是使用了美国的 VPS，但是还是访问不了相关的服务。所以，我们需要使用 Cloudflare Warp 来访问这些网站。&lt;/p&gt; &#xA;&lt;h4&gt;9.4.1 脚本安装&lt;/h4&gt; &#xA;&lt;p&gt;你可以使用这个一键安装的脚本来快速完成安装 &lt;a href=&#34;https://github.com/P3TERX/warp.sh&#34;&gt;https://github.com/P3TERX/warp.sh&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;下载脚本&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget git.io/warp.sh&#xA;chmod +x warp.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;运行脚本 中文菜单&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./warp.sh menu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;先后执行如下安装：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1 - 安装 Cloudflare WARP 官方客户端&lt;/li&gt; &#xA; &lt;li&gt;4 - 安装 WireGuard 相关组件&lt;/li&gt; &#xA; &lt;li&gt;7 - 自动配置 WARP WireGuard 双栈全局网络&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;如果没有 ipv6 网络，那么第 7 步可以换成第 5 步 自动配置 WARP WireGuard IPv4 网络，或是执行 &lt;code&gt;./warp.sh 4&lt;/code&gt;。&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;你需要备份一下你的帐号和配置文件，在 &lt;code&gt;/etc/warp/&lt;/code&gt; 目录下，主要是两个文件，一个是 &lt;code&gt;wgcf-account.toml&lt;/code&gt;，一个是 &lt;code&gt;wgcf-profile.conf&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;这个脚本会启动两个 Systemd 服务，一个是 &lt;code&gt;warp-svc&lt;/code&gt;，另一个是 &lt;code&gt;wg-quick@wgcf&lt;/code&gt;。第一个是 Cloudflare Warp 的服务，第二个是 WireGuard 的服务。&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;你可以使用 &lt;code&gt;./warp.sh status&lt;/code&gt; 来查看服务的状态，如果正常的话，会显示如下的信息。如果你的服务没有正常启动，那么会自动 &lt;code&gt;disable wg-quick@wgcf&lt;/code&gt;，如果这样的话，你可以使用 &lt;code&gt;./warp.sh d&lt;/code&gt;（ipv4 + ipv6 双栈网格）或是 &lt;code&gt;./warp.sh 4&lt;/code&gt;（ipv4 网络） 来重头走一遍所有的安装过程。&lt;/p&gt; &lt;pre&gt;&lt;code&gt;WireGuard&#x9;: Running&#xA;IPv4 Network&#x9;: WARP&#xA;IPv6 Network&#x9;: WARP&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;使用 &lt;code&gt;curl ipinfo.io&lt;/code&gt; 命令来检查你的 IP 地址，如果显示的是 Cloudflare 的 IP 地址，那么恭喜你，你已经成功了。如：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;ip&#34;: &#34;104.28.227.191&#34;,&#xA;  &#34;city&#34;: &#34;Los Angeles&#34;,&#xA;  &#34;region&#34;: &#34;California&#34;,&#xA;  &#34;country&#34;: &#34;US&#34;,&#xA;  &#34;loc&#34;: &#34;34.0522,-118.2437&#34;,&#xA;  &#34;org&#34;: &#34;AS13335 Cloudflare, Inc.&#34;,&#xA;  &#34;postal&#34;: &#34;90009&#34;,&#xA;  &#34;timezone&#34;: &#34;America/Los_Angeles&#34;,&#xA;  &#34;readme&#34;: &#34;https://ipinfo.io/missingauth&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;9.4.2 手动安装&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;最好还是通过上面的脚本来安装，但是如果你想手动安装，那么你可以参考下面的步骤。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;1） 安装软件包&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo  apt-get install net-tools openresolv wireguard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2） 安装 Cloudflare Warp&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ViRb3/wgcf&#34;&gt;ViRb3/wgcf&lt;/a&gt; 是一个 Cloudflare Warp 的非官方客户端，它可以帮助我们生成 Cloudflare Warp 的配置文件。&lt;/p&gt; &#xA;&lt;p&gt;注意替换下面命令行的中 Github Release 的版本号，最新的版本号可以在 &lt;a href=&#34;https://github.com/ViRb3/wgcf/releases&#34;&gt;ViRb3/wgcf Release&lt;/a&gt; 里查找。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# create and enter the folder&#xA;mkdir warp &amp;amp;&amp;amp; cd warp&#xA;&#xA;# install wgcf&#xA;wget -O wgcf https://github.com/ViRb3/wgcf/releases/download/v2.2.14/wgcf_2.2.14_linux_amd64&#xA;&#xA;# change permission&#xA;chmod +x wgcf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;3）注册 Cloudflare Warp&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./wgcf register&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;4）生成 wgcf 配置文件&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./wgcf generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;执行完当前目录会生成2个文件：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;WARP注册文件：&lt;code&gt;wgcf-account.toml&lt;/code&gt;；&lt;/li&gt; &#xA; &lt;li&gt;WireGuard 配置文件：&lt;code&gt;wgcf-profile.conf&lt;/code&gt;。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;5）复制 WireGuard 配置文件到 &lt;code&gt;/etc/wireguard/&lt;/code&gt; 目录&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo cp wgcf-profile.conf /etc/wireguard/wgcf.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;6） 修改 WireGuard 配置文件&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;你一定要修改 &lt;code&gt;wgcf-profile.conf&lt;/code&gt; 文件，你把你的 VPS 的公网 IP 加进去，否则，你的 VPS 会失连。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;PostUp = ip rule add from &amp;lt;服务器 Public IP&amp;gt; lookup main&#xA;PostDown = ip rule delete from &amp;lt;服务器 Public IP&amp;gt; lookup main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;下面是配置文件的示例：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Interface]&#xA;PrivateKey = XXXXXXXXXXXXXxxxxxxxxxxxxx=&#xA;Address = 172.16.0.2/32,2606:4700:110:8d9b:bc8f:16f:668:d891/128&#xA;DNS = 8.8.8.8,8.8.4.4,2001:4860:4860::8888,2001:4860:4860::8844&#xA;MTU = 1420&#xA;PostUp = ip -4 rule add from 10.10.10.10 lookup main prio 18&#xA;PostDown = ip -4 rule delete from 10.10.10.10 lookup main prio 18&#xA;&#xA;[Peer]&#xA;PublicKey = bmXOC+F1FxEMF9dyiK2H5/1SUtzH0JuVo51h2wPfgyo=&#xA;AllowedIPs = 0.0.0.0/0,::/0&#xA;Endpoint = 162.159.192.1:2408&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;10.10.10.10&lt;/code&gt; 只是一个示例，你要把它替换成你的 VPS 的公网 IP&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;162.159.192.1&lt;/code&gt; 是域名的 &lt;code&gt;engage.cloudflareclient.com&lt;/code&gt; 的 IP 地址，你可以使用 &lt;code&gt;nslookup engage.cloudflareclient.com&lt;/code&gt; 获得。&lt;/li&gt; &#xA; &lt;li&gt;配置文件中默认的 DNS 是 &lt;code&gt;1.1.1.1&lt;/code&gt; 这里使用了 Google 的 DNS，因为会快一些。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;7）启停 WireGuard&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;启用网络接口（命令中的 &lt;code&gt;wgcf&lt;/code&gt; 对应的是配置文件 &lt;code&gt;/etc/wireguard/wgcf.conf&lt;/code&gt; 的文件名前缀）&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo wg-quick up wgcf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;停止网络接口如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo wg-quick down wgcf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;8）设置开机自启动&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo systemctl enable wg-quick@wgcf&#xA;sudo systemctl start wg-quick@wgcf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;10. 其它&lt;/h2&gt; &#xA;&lt;h3&gt;10.1 其它方式&lt;/h3&gt; &#xA;&lt;p&gt;如下还有一些其它的方式（注：均由网友提供，我没有验证过）&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://getoutline.org/en/home&#34;&gt;Outline&lt;/a&gt; 是由 Google 旗下 &lt;a href=&#34;https://jigsaw.google.com/&#34;&gt;Jigsaw&lt;/a&gt; 团队开发的整套翻墙解决方案。Server 端使用 Shadowsocks，MacOS, Windows, iOS, Android 均有官方客户端。使用 Outline Manager 可以一键配置 DigitalOcean。其他平台例如 AWS, Google Cloud 也提供相应脚本。主要优点就是使用简单并且整个软件栈全部&lt;a href=&#34;https://github.com/Jigsaw-Code/?q=outline&#34;&gt;开源&lt;/a&gt;，有专业团队长期维护。&lt;/p&gt; &#xA;&lt;h3&gt;10.2 搭建脚本&lt;/h3&gt; &#xA;&lt;p&gt;上述的搭建和安装脚本可参看本库的 scripts 目录下的脚本（感谢网友 &lt;a href=&#34;https://github.com/gongzili456&#34;&gt;@gongzili456&lt;/a&gt; 开发）&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haoel/haoel.github.io/raw/master/scripts/install.ubuntu.18.04.sh&#34;&gt;Ubuntu 18.04 Installation Script&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;欢迎补充和改善！&lt;/p&gt; &#xA;&lt;p&gt;（全文完）&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Yidadaa/ChatGPT-Next-Web</title>
    <updated>2023-03-28T01:31:22Z</updated>
    <id>tag:github.com,2023-03-28:/Yidadaa/ChatGPT-Next-Web</id>
    <link href="https://github.com/Yidadaa/ChatGPT-Next-Web" rel="alternate"></link>
    <summary type="html">&lt;p&gt;一键拥有你自己的 ChatGPT 网页服务。 One-Click to deploy your own ChatGPT web UI.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Yidadaa/ChatGPT-Next-Web/main/static/icon.svg?sanitize=true&#34; alt=&#34;预览&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;ChatGPT Next Web&lt;/h1&gt; &#xA; &lt;p&gt;一键免费部署你的私人 ChatGPT 网页应用。&lt;/p&gt; &#xA; &lt;p&gt;One-Click to deploy your own ChatGPT web UI.&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://chat-gpt-next-web.vercel.app/&#34;&gt;演示 Demo&lt;/a&gt; / &lt;a href=&#34;https://github.com/Yidadaa/ChatGPT-Next-Web/issues&#34;&gt;反馈 Issues&lt;/a&gt; / &lt;a href=&#34;https://discord.gg/zrhvHCr79N&#34;&gt;加入 Discord&lt;/a&gt; / &lt;a href=&#34;https://user-images.githubusercontent.com/16968934/227772522-b3ba3713-9206-4c8d-a81f-22300b7c313a.jpg&#34;&gt;微信群&lt;/a&gt; / &lt;a href=&#34;https://user-images.githubusercontent.com/16968934/227772541-5bcd52d8-61b7-488c-a203-0330d8006e2b.jpg&#34;&gt;打赏开发者&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2FYidadaa%2FChatGPT-Next-Web&amp;amp;env=OPENAI_API_KEY&amp;amp;project-name=chatgpt-next-web&amp;amp;repository-name=ChatGPT-Next-Web&#34;&gt;&lt;img src=&#34;https://vercel.com/button&#34; alt=&#34;Deploy with Vercel&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://gitpod.io/#https://github.com/Yidadaa/ChatGPT-Next-Web&#34;&gt;&lt;img src=&#34;https://gitpod.io/button/open-in-gitpod.svg?sanitize=true&#34; alt=&#34;Open in Gitpod&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Yidadaa/ChatGPT-Next-Web/main/static/cover.png&#34; alt=&#34;主界面&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;主要功能&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;在 1 分钟内使用 Vercel &lt;strong&gt;免费一键部署&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;精心设计的 UI，响应式设计，支持深色模式&lt;/li&gt; &#xA; &lt;li&gt;极快的首屏加载速度（~85kb）&lt;/li&gt; &#xA; &lt;li&gt;自动压缩上下文聊天记录，在节省 Token 的同时支持超长对话&lt;/li&gt; &#xA; &lt;li&gt;一键导出聊天记录，完整的 Markdown 支持&lt;/li&gt; &#xA; &lt;li&gt;拥有自己的域名？好上加好，绑定后即可在任何地方&lt;strong&gt;无障碍&lt;/strong&gt;快速访问&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deploy for free with one-click&lt;/strong&gt; on Vercel in under 1 minute&lt;/li&gt; &#xA; &lt;li&gt;Responsive design, and dark mode&lt;/li&gt; &#xA; &lt;li&gt;Fast first screen loading speed (~85kb)&lt;/li&gt; &#xA; &lt;li&gt;Automatically compresses chat history to support long conversations while also saving your tokens&lt;/li&gt; &#xA; &lt;li&gt;One-click export all chat history with full Markdown support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;使用&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;准备好你的 &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;OpenAI API Key&lt;/a&gt;;&lt;/li&gt; &#xA; &lt;li&gt;点击右侧按钮开始部署： &lt;a href=&#34;https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2FYidadaa%2FChatGPT-Next-Web&amp;amp;env=OPENAI_API_KEY&amp;amp;project-name=chatgpt-next-web&amp;amp;repository-name=ChatGPT-Next-Web&#34;&gt;&lt;img src=&#34;https://vercel.com/button&#34; alt=&#34;Deploy with Vercel&#34;&gt;&lt;/a&gt;，直接使用 Github 账号登陆即可，记得在环境变量页填入 API Key；&lt;/li&gt; &#xA; &lt;li&gt;部署完毕后，即可开始使用；&lt;/li&gt; &#xA; &lt;li&gt;（可选）&lt;a href=&#34;https://vercel.com/docs/concepts/projects/domains/add-a-domain&#34;&gt;绑定自定义域名&lt;/a&gt;：Vercel 分配的域名 DNS 在某些区域被污染了，绑定自定义域名即可直连。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;OpenAI API Key&lt;/a&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;a href=&#34;https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2FYidadaa%2FChatGPT-Next-Web&amp;amp;env=OPENAI_API_KEY&amp;amp;project-name=chatgpt-next-web&amp;amp;repository-name=ChatGPT-Next-Web&#34;&gt;&lt;img src=&#34;https://vercel.com/button&#34; alt=&#34;Deploy with Vercel&#34;&gt;&lt;/a&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Enjoy :)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;保持更新 Keep Updated&lt;/h2&gt; &#xA;&lt;p&gt;如果你按照上述步骤一键部署了自己的项目，可能会发现总是提示“存在更新”的问题，这是由于 Vercel 会默认为你创建一个新项目而不是 fork 本项目，这会导致无法正确地检测更新。 推荐你按照下列步骤重新部署：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;删除掉原先的 repo；&lt;/li&gt; &#xA; &lt;li&gt;fork 本项目；&lt;/li&gt; &#xA; &lt;li&gt;前往 vercel 控制台，删除掉原先的 project，然后新建 project，选择你刚刚 fork 出来的项目重新进行部署即可；&lt;/li&gt; &#xA; &lt;li&gt;在重新部署的过程中，请手动添加名为 &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; 的环境变量，并填入你的 api key 作为值。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;本项目会持续更新，如果你想让代码库总是保持更新，可以查看 &lt;a href=&#34;https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork&#34;&gt;Github 的文档&lt;/a&gt; 了解如何让 fork 的项目与上游代码同步，建议定期进行同步操作以获得新功能。&lt;/p&gt; &#xA;&lt;p&gt;你可以 star/watch 本项目或者 follow 作者来及时获得新功能更新通知。&lt;/p&gt; &#xA;&lt;p&gt;If you have deployed your own project with just one click following the steps above, you may encounter the issue of &#34;Updates Available&#34; constantly showing up. This is because Vercel will create a new project for you by default instead of forking this project, resulting in the inability to detect updates correctly.&lt;/p&gt; &#xA;&lt;p&gt;We recommend that you follow the steps below to re-deploy:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Delete the original repo;&lt;/li&gt; &#xA; &lt;li&gt;Fork this project;&lt;/li&gt; &#xA; &lt;li&gt;Go to the Vercel dashboard, delete the original project, then create a new project and select the project you just forked to redeploy;&lt;/li&gt; &#xA; &lt;li&gt;Please manually add an environment variable named &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; and enter your API key as the value during the redeploy process.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This project will be continuously maintained. If you want to keep the code repository up to date, you can check out the &lt;a href=&#34;https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork&#34;&gt;Github documentation&lt;/a&gt; to learn how to synchronize a forked project with upstream code. It is recommended to perform synchronization operations regularly.&lt;/p&gt; &#xA;&lt;p&gt;You can star or watch this project or follow author to get release notifictions in time.&lt;/p&gt; &#xA;&lt;h2&gt;访问控制 Access Control&lt;/h2&gt; &#xA;&lt;p&gt;本项目提供有限的权限控制功能，请在环境变量页增加名为 &lt;code&gt;CODE&lt;/code&gt; 的环境变量，值为用英文逗号分隔的自定义控制码：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;code1,code2,code3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;增加或修改该环境变量后，请&lt;strong&gt;重新部署&lt;/strong&gt;项目使改动生效。&lt;/p&gt; &#xA;&lt;p&gt;This project provides limited access control. Please add an environment variable named &lt;code&gt;CODE&lt;/code&gt; on the environment variables page. The value should be a custom control code separated by comma like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;code1,code2,code3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After adding or modifying this environment variable, please redeploy the project for the changes to take effect.&lt;/p&gt; &#xA;&lt;h2&gt;开发 Development&lt;/h2&gt; &#xA;&lt;p&gt;点击下方按钮，开始二次开发：&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitpod.io/#https://github.com/Yidadaa/ChatGPT-Next-Web&#34;&gt;&lt;img src=&#34;https://gitpod.io/button/open-in-gitpod.svg?sanitize=true&#34; alt=&#34;Open in Gitpod&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;在开始写代码之前，需要在项目根目录新建一个 &lt;code&gt;.env.local&lt;/code&gt; 文件，里面填入环境变量：&lt;/p&gt; &#xA;&lt;p&gt;Before starting development, you must create a new &lt;code&gt;.env.local&lt;/code&gt; file at project root, and place your api key into it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&amp;lt;your api key here&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;本地开发 Local Development&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;如果你是中国大陆用户，不建议在本地进行开发，除非你能够独立解决 OpenAI API 本地代理问题。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;安装 nodejs 和 yarn，具体细节请询问 ChatGPT；&lt;/li&gt; &#xA; &lt;li&gt;执行 &lt;code&gt;yarn install &amp;amp;&amp;amp; yarn dev&lt;/code&gt; 即可。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;本地部署 Local Deployment&lt;/h3&gt; &#xA;&lt;p&gt;请直接询问 ChatGPT，使用下列 Prompt：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;如何使用 pm2 和 yarn 部署 nextjs 项目到 ubuntu 服务器上，项目编译命令为 yarn build，启动命令为 yarn start，启动时需要设置环境变量为 OPENAI_API_KEY，端口为 3000，使用 ngnix 做反向代理&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please ask ChatGPT with prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;how to deploy nextjs project with pm2 and yarn on my ubuntu server, the build command is `yarn build`, the start command is `yarn start`, the project must start with env var named `OPENAI_API_KEY`, the port is 3000, use ngnix&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;容器部署 Docker Deployment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker pull yidadaa/chatgpt-next-web&#xA;&#xA;docker run -d -p 3000:3000 -e OPENAI_API_KEY=&#34;&#34; -e CODE=&#34;&#34; yidadaa/chatgpt-next-web&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;截图 Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Yidadaa/ChatGPT-Next-Web/main/static/settings.png&#34; alt=&#34;设置 Settings&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Yidadaa/ChatGPT-Next-Web/main/static/more.png&#34; alt=&#34;更多展示 More&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;说明 Attention&lt;/h2&gt; &#xA;&lt;p&gt;本项目的演示地址所用的 OpenAI 账户的免费额度将于 2023-04-01 过期，届时将无法通过演示地址在线体验。&lt;/p&gt; &#xA;&lt;p&gt;如果你想贡献出自己的 API Key，可以通过作者主页的邮箱发送给作者，并标注过期时间。&lt;/p&gt; &#xA;&lt;p&gt;The free trial of the OpenAI account used by the demo will expire on April 1, 2023, and the demo will not be available at that time.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to contribute your API key, you can email it to the author and indicate the expiration date of the API key.&lt;/p&gt; &#xA;&lt;h2&gt;鸣谢 Special Thanks&lt;/h2&gt; &#xA;&lt;h3&gt;捐赠者 Sponsor&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mushan0x0&#34;&gt;@mushan0x0&lt;/a&gt; &lt;a href=&#34;https://github.com/ClarenceDan&#34;&gt;@ClarenceDan&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;贡献者 Contributor&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AprilNEA&#34;&gt;@AprilNEA&lt;/a&gt; &lt;a href=&#34;https://github.com/iSource&#34;&gt;@iSource&lt;/a&gt; &lt;a href=&#34;https://github.com/iFwu&#34;&gt;@iFwu&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kattgu7/Anti-996-License/raw/master/LICENSE_CN_EN&#34;&gt;Anti 996 License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>BlinkDL/RWKV-LM</title>
    <updated>2023-03-28T01:31:22Z</updated>
    <id>tag:github.com,2023-03-28:/BlinkDL/RWKV-LM</id>
    <link href="https://github.com/BlinkDL/RWKV-LM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it&#39;s combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, &#34;infinite&#34; ctx_len, and free sentence embedding.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The RWKV Language Model (and my LM tricks)&lt;/h1&gt; &#xA;&lt;h2&gt;RWKV: Parallelizable RNN with Transformer-level LLM Performance (pronounced as &#34;RwaKuv&#34;, from 4 major params: R W K V)&lt;/h2&gt; &#xA;&lt;p&gt;RWKV is an RNN with Transformer-level LLM performance, which can also be directly trained like a GPT transformer (parallelizable). And it&#39;s 100% attention-free. You only need the hidden state at position t to compute the state at position t+1. You can use the &#34;GPT&#34; mode to quickly compute the hidden state for the &#34;RNN&#34; mode.&lt;/p&gt; &#xA;&lt;p&gt;So it&#39;s combining the best of RNN and transformer - &lt;strong&gt;great performance, fast inference, saves VRAM, fast training, &#34;infinite&#34; ctx_len, and free sentence embedding&lt;/strong&gt; (using the final hidden state).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RWKV pip package&lt;/strong&gt;: &lt;a href=&#34;https://pypi.org/project/rwkv/&#34;&gt;https://pypi.org/project/rwkv/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;os.environ[&#34;RWKV_JIT_ON&#34;] = &#39;1&#39;&#xA;os.environ[&#34;RWKV_CUDA_ON&#34;] = &#39;0&#39; # if &#39;1&#39; then use CUDA kernel for seq mode (much faster)&#xA;from rwkv.model import RWKV                         # pip install rwkv&#xA;model = RWKV(model=&#39;/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-1b5/RWKV-4-Pile-1B5-20220903-8040&#39;, strategy=&#39;cuda fp16&#39;)&#xA;&#xA;out, state = model.forward([187, 510, 1563, 310, 247], None)   # use 20B_tokenizer.json&#xA;print(out.detach().cpu().numpy())                   # get logits&#xA;out, state = model.forward([187, 510], None)&#xA;out, state = model.forward([1563], state)           # RNN has state (use deepcopy if you want to clone it)&#xA;out, state = model.forward([310, 247], state)&#xA;print(out.detach().cpu().numpy())                   # same result as above&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Download RWKV-4 0.1/0.4/1.5/3/7/14B weights&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/BlinkDL&#34;&gt;https://huggingface.co/BlinkDL&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Join Our Discord: &lt;a href=&#34;https://discord.gg/bDSBUMeFpc&#34;&gt;https://discord.gg/bDSBUMeFpc&lt;/a&gt; (lots of developers)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Twitter&lt;/strong&gt;: &lt;a href=&#34;https://twitter.com/BlinkDL_AI&#34;&gt;https://twitter.com/BlinkDL_AI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RWKV in 150 lines&lt;/strong&gt; (model, inference, text generation): &lt;a href=&#34;https://github.com/BlinkDL/ChatRWKV/raw/main/RWKV_in_150_lines.py&#34;&gt;https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ChatRWKV v2:&lt;/strong&gt; with &#34;stream&#34; and &#34;split&#34; strategies and INT8. &lt;strong&gt;3G VRAM is enough to run RWKV 14B :)&lt;/strong&gt; &lt;a href=&#34;https://github.com/BlinkDL/ChatRWKV/tree/main/v2&#34;&gt;https://github.com/BlinkDL/ChatRWKV/tree/main/v2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-chat.png&#34; alt=&#34;RWKV-chat&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hugging Face space&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio&#34;&gt;https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You are welcome to join the RWKV discord &lt;a href=&#34;https://discord.gg/bDSBUMeFpc&#34;&gt;https://discord.gg/bDSBUMeFpc&lt;/a&gt; to build upon it. We have plenty of potential compute (A100 40Gs) now (thanks to Stability and EleutherAI), so if you have interesting ideas I can run them.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-eval2.png&#34; alt=&#34;RWKV-eval2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;RWKV [loss vs token position] for 10000 ctx4k+ documents in Pile. RWKV 1B5-4k is mostly flat after ctx1500, but 3B-4k and 7B-4k and 14B-4k have some slopes, and they are getting better. This debunks the old view that RNNs cannot model long ctxlens. We can predict that RWKV 100B will be great, and RWKV 1T is probably all you need :)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-ctxlen.png&#34; alt=&#34;RWKV-ctxlen&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;I believe RNN is a better candidate for fundamental models, because: (1) It&#39;s more friendly for ASICs (no kv cache). (2) It&#39;s more friendly for RL. (3) When we write, our brain is more similar to RNN. (4) The universe is like an RNN too (because of locality). Transformers are non-local models.&lt;/p&gt; &#xA;&lt;p&gt;RWKV-3 1.5B on A40 (tf32) = always 0.015 sec/token, tested using simple pytorch code (no CUDA), GPU utilization 45%, VRAM 7823M&lt;/p&gt; &#xA;&lt;p&gt;GPT2-XL 1.3B on A40 (tf32) = 0.032 sec/token (for ctxlen 1000), tested using HF, GPU utilization 45% too (interesting), VRAM 9655M&lt;/p&gt; &#xA;&lt;p&gt;Training speed: (new training code) RWKV-4 14B BF16 ctxlen4096 = 114K tokens/s on 8x8 A100 80G (ZERO2+CP). (old training code) RWKV-4 1.5B BF16 ctxlen1024 = 106K tokens/s on 8xA100 40G.&lt;/p&gt; &#xA;&lt;p&gt;I am doing image experiments too (For example: &lt;a href=&#34;https://huggingface.co/BlinkDL/clip-guided-binary-autoencoder&#34;&gt;https://huggingface.co/BlinkDL/clip-guided-binary-autoencoder&lt;/a&gt;) and RWKV will be able to do txt2img diffusion :) My idea: 256x256 rgb image -&amp;gt; 32x32x13bit latents -&amp;gt; apply RWKV to compute transition probability for each of the 32x32 grid -&amp;gt; pretend the grids are independent and &#34;diffuse&#34; using these probabilities.&lt;/p&gt; &#xA;&lt;p&gt;Smooth training - no loss spikes! (lr &amp;amp; bsz change around 15G tokens) &lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-loss.png&#34; alt=&#34;RWKV-loss&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-eval.png&#34; alt=&#34;RWKV-eval&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;All of the trained models will be open-source. Inference is very fast (only matrix-vector multiplications, no matrix-matrix multiplications) even on CPUs, so you can even run a LLM on your phone.&lt;/p&gt; &#xA;&lt;p&gt;How it works: RWKV gathers information to a number of channels, which are also decaying with different speeds as you move to the next token. It&#39;s very simple once you understand it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RWKV is parallelizable because the time-decay of each channel is data-independent (and trainable)&lt;/strong&gt;. For example, in usual RNN you can adjust the time-decay of a channel from say 0.8 to 0.5 (these are called &#34;gates&#34;), while in RWKV you simply move the information from a W-0.8-channel to a W-0.5-channel to achieve the same effect. Moreover, you can fine-tune RWKV into a non-parallelizable RNN (then you can use outputs of later layers of the previous token) if you want extra performance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-formula.png&#34; alt=&#34;RWKV-formula&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here are some of my TODOs. Let&#39;s work together :)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;HuggingFace integration (check &lt;a href=&#34;https://github.com/huggingface/transformers/issues/17230&#34;&gt;https://github.com/huggingface/transformers/issues/17230&lt;/a&gt; ), and optimized CPU &amp;amp; iOS &amp;amp; Android &amp;amp; WASM &amp;amp; WebGL inference. RWKV is a RNN and very friendly for edge devices. Let&#39;s make it possible to run a LLM on your phone.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Test it on bidirectional &amp;amp; MLM tasks, and image &amp;amp; audio &amp;amp; video tokens. I think RWKV can support Encoder-Decoder via this: for each decoder token, use a learned mixture of [decoder previous hidden state] &amp;amp; [encoder final hidden state]. Hence all decoder tokens will have access to the encoder output.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now training RWKV-4a with one single tiny extra attention (just a few extra lines comparing with RWKV-4) to further improve some difficult zeroshot tasks (such as LAMBADA) for smaller models. See &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM/commit/a268cd2e40351ee31c30c5f8a5d1266d35b41829&#34;&gt;https://github.com/BlinkDL/RWKV-LM/commit/a268cd2e40351ee31c30c5f8a5d1266d35b41829&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;User feedback:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;I&#39;ve so far toyed around the character-based model on our relatively small pre-training dataset (around 10GB of text), and the results are extremely good - similar ppl to models taking much, much longer to train.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;dear god rwkv is fast. i switched to another tab after starting training it from scratch &amp;amp; when i returned it was emitting plausible english &amp;amp; maori words, i left to go microwave some coffee &amp;amp; when i came back it was producing fully grammatically correct sentences.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Tweet from Sepp Hochreiter (thank you!): &lt;a href=&#34;https://twitter.com/HochreiterSepp/status/1524270961314484227&#34;&gt;https://twitter.com/HochreiterSepp/status/1524270961314484227&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can find me (BlinkDL) in the EleutherAI Discord too: &lt;a href=&#34;https://www.eleuther.ai/get-involved/&#34;&gt;https://www.eleuther.ai/get-involved/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-demo.png&#34; alt=&#34;RWKV-demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v4neo&#34;&gt;https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v4neo&lt;/a&gt; (latest code, compatible with v4).&lt;/p&gt; &#xA;&lt;p&gt;Here is a great prompt for testing Q&amp;amp;A of LLMs. Works for any model: (found by minimizing ChatGPT ppls for RWKV 1.5B)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = f&#39;\nQ &amp;amp; A\n\nQuestion:\n{qq}\n\nDetailed Expert Answer:\n&#39; # let the model generate after this&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cool Community RWKV Projects (check them!)&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/rwkvstic/&#34;&gt;https://pypi.org/project/rwkvstic/&lt;/a&gt; a pip package (with 8bit &amp;amp; offload for low VRAM GPUs)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/harrisonvanderbyl/rwkv_chatbot&#34;&gt;https://github.com/harrisonvanderbyl/rwkv_chatbot&lt;/a&gt; a chatbot&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hizkifw/WebChatRWKVstic&#34;&gt;https://github.com/hizkifw/WebChatRWKVstic&lt;/a&gt; WebUI (WIP)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gururise/rwkv_gradio&#34;&gt;https://github.com/gururise/rwkv_gradio&lt;/a&gt; RWKV Gradio&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cryscan/eloise&#34;&gt;https://github.com/cryscan/eloise&lt;/a&gt; RWKV QQ bot&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Blealtan/RWKV-LM-LoRA&#34;&gt;https://github.com/Blealtan/RWKV-LM-LoRA&lt;/a&gt; LoRA fine-tuning&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mrsteyk/RWKV-LM-jax&#34;&gt;https://github.com/mrsteyk/RWKV-LM-jax&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wozeparrot/tinyrwkv&#34;&gt;https://github.com/wozeparrot/tinyrwkv&lt;/a&gt; RWKV in tinygrad (nice simple DL framework)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/transformers/issues/17230&#34;&gt;https://github.com/huggingface/transformers/issues/17230&lt;/a&gt; RWKV HF package (WIP)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ArEnSc/Production-RWKV&#34;&gt;https://github.com/ArEnSc/Production-RWKV&lt;/a&gt; RWKV HF package source&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nlpodyssey/verbaflow&#34;&gt;https://github.com/nlpodyssey/verbaflow&lt;/a&gt; RWKV in Go&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nlpodyssey/rwkv&#34;&gt;https://github.com/nlpodyssey/rwkv&lt;/a&gt; RWKV in Go&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mrsteyk/rwkvk-rs&#34;&gt;https://github.com/mrsteyk/rwkvk-rs&lt;/a&gt; RWKV in Rust&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/josephrocca/rwkv-v4-web&#34;&gt;https://github.com/josephrocca/rwkv-v4-web&lt;/a&gt; RWKV in browser&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/imxcstar/CSharp-RWKV-V4&#34;&gt;https://github.com/imxcstar/CSharp-RWKV-V4&lt;/a&gt; RWKV in C#&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mrsteyk/RWKV-LM-deepspeed&#34;&gt;https://github.com/mrsteyk/RWKV-LM-deepspeed&lt;/a&gt; Another training fork&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/resloved/RWKV-notebooks&#34;&gt;https://github.com/resloved/RWKV-notebooks&lt;/a&gt; RWKV colab notebooks&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/harrisonvanderbyl/rwkvstic/blob/master/notebooks/chatbot.ipynb&#34;&gt;https://colab.research.google.com/github/harrisonvanderbyl/rwkvstic/blob/master/notebooks/chatbot.ipynb&lt;/a&gt; RWKV chatbot colab notebook&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Pathos14489/RWKVDistributedInference&#34;&gt;https://github.com/Pathos14489/RWKVDistributedInference&lt;/a&gt; RWKV Distributed Inference&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AXKuhta/rwkv-onnx-dml&#34;&gt;https://github.com/AXKuhta/rwkv-onnx-dml&lt;/a&gt; RWKV ONNX&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Run RWKV-4 Pile models:&lt;/strong&gt; Download models from &lt;a href=&#34;https://huggingface.co/BlinkDL&#34;&gt;https://huggingface.co/BlinkDL&lt;/a&gt;. Set TOKEN_MODE = &#39;pile&#39; in run.py and run it. It&#39;s fast even on CPU (the default mode).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Colab for RWKV-4 Pile 1.5B&lt;/strong&gt;: &lt;a href=&#34;https://colab.research.google.com/drive/1F7tZoPZaWJf1fsCmZ5tjw6sYHiFOYVWM&#34;&gt;https://colab.research.google.com/drive/1F7tZoPZaWJf1fsCmZ5tjw6sYHiFOYVWM&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run RWKV-4 Pile models in your browser (and onnx version): see this issue &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM/issues/7&#34;&gt;https://github.com/BlinkDL/RWKV-LM/issues/7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;RWKV-4 Web Demo: &lt;a href=&#34;https://josephrocca.github.io/rwkv-v4-web/demo/&#34;&gt;https://josephrocca.github.io/rwkv-v4-web/demo/&lt;/a&gt; (note: only greedy sampling for now)&lt;/p&gt; &#xA;&lt;p&gt;For the old RWKV-2: see the release here for a 27M params model on enwik8 with 0.72 BPC(dev). Run run.py in &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v2-RNN&#34;&gt;https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v2-RNN&lt;/a&gt;. You can even run it in your browser: &lt;a href=&#34;https://github.com/BlinkDL/AI-Writer/tree/main/docs/eng&#34;&gt;https://github.com/BlinkDL/AI-Writer/tree/main/docs/eng&lt;/a&gt; &lt;a href=&#34;https://blinkdl.github.io/AI-Writer/eng/&#34;&gt;https://blinkdl.github.io/AI-Writer/eng/&lt;/a&gt; (this is using tf.js WASM single-thread mode).&lt;/p&gt; &#xA;&lt;p&gt;I&#39;d like to build an almost-INT8 version of RWKV. A simple method to quantize a matrix with outliers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as npA&#xA;&#xA;# the original M, with outliers&#xA;M = np.array([[1,   2,   1,  2],[2,  100,    2, 10],[1,   2,   1, 2],[2,   1, 20, 1]])&#xA;&#xA;# the scaled M, without outliers&#xA;Q = np.array([[1, 0.2, 0.1,  2],[0.4,  2, 0.04, 2], [1, 0.2, 0.1, 2],[2, 0.1,  2, 1]])&#xA;# we can find optimal a &amp;amp; b to minimize inference error after quantization&#xA;a = np.array([1, 10, 10, 1])&#xA;b = np.array([1, 5, 1, 1])&#xA;&#xA;# test M.v with random v - the results will be the same&#xA;v = np.array([1.23, 5.44, 9.75, 2.98])&#xA;print(M.dot(v))&#xA;print(Q.dot(v * a) * b)&#xA;&#xA;# even better: decompose M.dot(v) as Q.dot(v * a + aa) * b + bb where aa &amp;amp; bb are vectors too&#xA;# and can apply more scaling to achieve W8A8 (example: https://arxiv.org/pdf/2211.10438.pdf)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training / Fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training RWKV-4 from scratch:&lt;/strong&gt; run train.py, which by default is using the enwik8 dataset (unzip &lt;a href=&#34;https://data.deepai.org/enwik8.zip&#34;&gt;https://data.deepai.org/enwik8.zip&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;You will be training the &#34;GPT&#34; version because it&#39;s paralleziable and faster to train. RWKV-4 can extrapolate, so training with ctxLen 1024 can work for ctxLen of 2500+. You can fine-tune the model with longer ctxLen and it can quickly adapt to longer ctxLens.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fine-tuning RWKV-4 Pile models:&lt;/strong&gt; use &#39;prepare-data.py&#39; in &lt;a href=&#34;https://github.com/BlinkDL/RWKV-v2-RNN-Pile/tree/main/RWKV-v3&#34;&gt;https://github.com/BlinkDL/RWKV-v2-RNN-Pile/tree/main/RWKV-v3&lt;/a&gt; to tokenize .txt into train.npy data. Then set EXPRESS_PILE_MODE to True in train.py, and run it.&lt;/p&gt; &#xA;&lt;p&gt;Read the inference code in src/model.py and try using the final hidden state（.xx .aa .bb) as a faithful sentence embedding for other tasks. Probably you should begin with .xx and .aa/.bb (.aa divided by .bb).&lt;/p&gt; &#xA;&lt;p&gt;Colab for fine-tuning RWKV-4 Pile models: &lt;a href=&#34;https://colab.research.google.com/github/resloved/RWKV-notebooks/blob/master/RWKV_v4_RNN_Pile_Fine_Tuning.ipynb&#34;&gt;https://colab.research.google.com/github/resloved/RWKV-notebooks/blob/master/RWKV_v4_RNN_Pile_Fine_Tuning.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Large corpus:&lt;/strong&gt; Use &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;https://github.com/EleutherAI/gpt-neox&lt;/a&gt; to convert .jsonl into .bin and .idx&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python tools/preprocess_data.py --input ./my_data.jsonl --output-prefix ./data/my_data --vocab ./20B_tokenizer.json --dataset-impl mmap --tokenizer-type HFTokenizer --append-eod&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The jsonl format sample (one line for each document):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#34;meta&#34;: {&#34;ID&#34;: 101}, &#34;text&#34;: &#34;This is the first document.&#34;}&#xA;{&#34;meta&#34;: {&#34;ID&#34;: 102}, &#34;text&#34;: &#34;Hello\nWorld&#34;}&#xA;{&#34;meta&#34;: {&#34;ID&#34;: 103}, &#34;text&#34;: &#34;1+1=2\n1+2=3\n2+2=4&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;generated by code like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ss = json.dumps({&#34;meta&#34;: meta, &#34;text&#34;: text}, ensure_ascii=False)&#xA;out.write(ss + &#34;\n&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Towards RWKV-5 (just to record some new ideas)&lt;/h2&gt; &#xA;&lt;h3&gt;Some ideas&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Now time decay is like 0.999^T (0.999 is learnable). Change it to something like (0.999^T + 0.1) where 0.1 is learnable too. The 0.1 part will be kept forever. Or, A^T + B^T + C = fast-decay + slow-decay + constant. Can even use different formulas (for example, K^2 instead of e^K for a decay component, or, without normalization).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use complex-valued decay (so, rotation instead of decay) in some channels.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Inject some trainable and extrapolatable positional encoding?&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Aside from 2d rotation, we can try other Lie groups such as 3d rotation ( SO(3) ). Non-abelian RWKV lol.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;RWKV might be great on analog devices (search for Analog Matrix-vector multiplication &amp;amp; Photonic Matrix-vector multiplication). The RNN mode is very hardware-friendly (processing-in-memory). Can be a SNN too (&lt;a href=&#34;https://github.com/ridgerchu/SpikeGPT&#34;&gt;https://github.com/ridgerchu/SpikeGPT&lt;/a&gt;). I wonder if it can be optimized for quantum computation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Trainable initial hidden state (xx aa bb pp xx).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Vision Tasks&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;I find it&#39;s good to add a 2d pos encoding:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;self.pos_emb_x = nn.Parameter(torch.zeros((1,args.my_pos_emb,args.n_embd)))&#xA;self.pos_emb_y = nn.Parameter(torch.zeros((args.my_pos_emb,1,args.n_embd)))&#xA;...&#xA;x = x + pos_emb_x + pos_emb_y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;In a BPE langauge model, it&#39;s the best to use [tokenShift of 1 token] (you can mix more tokens in a char-level English model). However you can try [tokenShift of N (or N-1) (or N+1) tokens] if the image size is N x N, because that will be like mixing [the token above the current positon (or the token above the to-be-predicted positon)] with [current token]. You can use try different tokenShift styles for &#34;ATT&#34; &amp;amp; &#34;FFN&#34;, or mixing different tokenShift styles - such as mixing [token A] with [token A-1] and [token A-(N-1)] etc.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Misc&lt;/h3&gt; &#xA;&lt;p&gt;I have an idea to improve tokenization. We can hardcode some channels to have meanings. Example:&lt;/p&gt; &#xA;&lt;p&gt;Channel 0 = &#34;space&#34;&lt;/p&gt; &#xA;&lt;p&gt;Channel 1 = &#34;capitalize first letter&#34;&lt;/p&gt; &#xA;&lt;p&gt;Channel 2 = &#34;capitalize all letters&#34;&lt;/p&gt; &#xA;&lt;p&gt;Therefore:&lt;/p&gt; &#xA;&lt;p&gt;Embedding of &#34;abc&#34;: [0, 0, 0, x0, x1, x2 , ..]&lt;/p&gt; &#xA;&lt;p&gt;Embedding of &#34; abc&#34;: [1, 0, 0, x0, x1, x2, ..]&lt;/p&gt; &#xA;&lt;p&gt;Embedding of &#34; Abc&#34;: [1, 1, 0, x0, x1, x2, ..]&lt;/p&gt; &#xA;&lt;p&gt;Embedding of &#34;ABC&#34;: [0, 0, 1, x0, x1, x2, ...]&lt;/p&gt; &#xA;&lt;p&gt;......&lt;/p&gt; &#xA;&lt;p&gt;so they will share most of the embedding. And we can rapidly compute the output probability of all variations of &#34;abc&#34;.&lt;/p&gt; &#xA;&lt;p&gt;Note: the above method is assuming that p(&#34; xyz&#34;) / p(&#34;xyz&#34;) is the same for any &#34;xyz&#34;, which can be wrong.&lt;/p&gt; &#xA;&lt;p&gt;Better: define emb_space emb_capitalize_first emb_capitalize_all to be a function of emb.&lt;/p&gt; &#xA;&lt;p&gt;Maybe the Best: let &#39;abc&#39; &#39; abc&#39; etc. to share the last 90% of their embeddings.&lt;/p&gt; &#xA;&lt;p&gt;At this moment, all our tokenizers spend too many items to represent all variations of &#39;abc&#39; &#39; abc&#39; &#39; Abc&#39; etc. Moreover the model cannot discover that these are actually similar if some of these variations are rare in the dataset. The method here can improve this. I plan to test this in a new version of RWKV.&lt;/p&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;RWKV is inspired by Apple&#39;s AFT (&lt;a href=&#34;https://arxiv.org/abs/2105.14103&#34;&gt;https://arxiv.org/abs/2105.14103&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Moreover it&#39;s using a number of my tricks, such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;SmallInitEmb: &lt;a href=&#34;https://github.com/BlinkDL/SmallInitEmb&#34;&gt;https://github.com/BlinkDL/SmallInitEmb&lt;/a&gt; (applicable to all transformers) which helps the embedding quality, and stabilizes Post-LN (which is what I am using).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Token-shift: &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM#token-shift-time-shift-mixing&#34;&gt;https://github.com/BlinkDL/RWKV-LM#token-shift-time-shift-mixing&lt;/a&gt; (applicable to all transformers), especially helpful for char-level models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Head-QK: &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM#the-head-qk-trick-learning-to-copy-and-avoid-tokens&#34;&gt;https://github.com/BlinkDL/RWKV-LM#the-head-qk-trick-learning-to-copy-and-avoid-tokens&lt;/a&gt; (applicable to all transformers). Note: it&#39;s helpful, but I disabled it in the Pile model to keep it 100% RNN.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Extra R-gate in the FFN (applicable to all transformers). I am also using reluSquared from Primer.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Better initilization: I init most of the matrices to ZERO (see RWKV_Init in &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM/raw/main/RWKV-v2-RNN/src/model.py&#34;&gt;https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v2-RNN/src/model.py&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can transfer some parameters from a small model to a large model (note: I sort &amp;amp; smooth them too), for faster and better convergence (see &lt;a href=&#34;https://www.reddit.com/r/MachineLearning/comments/umq908/r_rwkvv2rnn_a_parallelizable_rnn_with/&#34;&gt;https://www.reddit.com/r/MachineLearning/comments/umq908/r_rwkvv2rnn_a_parallelizable_rnn_with/&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;My CUDA kernel: &lt;a href=&#34;https://github.com/BlinkDL/RWKV-CUDA&#34;&gt;https://github.com/BlinkDL/RWKV-CUDA&lt;/a&gt; to speedup training.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;The pseudocode (execution from top to bottom):&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-v2-RNN.png&#34; alt=&#34;RWKV-v2-RNN&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The a b c d factors work together to build a time-decay curve: [X, 1, W, W^2, W^3, ...].&lt;/p&gt; &#xA;&lt;p&gt;Write out the formulas for &#34;token at pos 2&#34; and &#34;token at pos 3&#34; and you will get the idea:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a and b: EMAs of kv and k.&lt;/li&gt; &#xA; &lt;li&gt;c and d: these are a and b combined with &#34;self-attention&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;kv / k is the memory mechanism. The token with high k can be remembered for a long duration, if W is close to 1 in the channel.&lt;/p&gt; &#xA;&lt;p&gt;The R-gate is important for performance. k = info strength of this token (to be passed to future tokens). r = whether to apply the info to this token.&lt;/p&gt; &#xA;&lt;h2&gt;RWKV-3 improvements&lt;/h2&gt; &#xA;&lt;p&gt;Use different trainable TimeMix factors for R / K / V in SA and FF layers. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xx = self.time_shift(x)&#xA;xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)&#xA;xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)&#xA;xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use preLN instead of postLN (more stable &amp;amp; faster convergence):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if self.layer_id == 0:&#xA;&#x9;x = self.ln0(x)&#xA;x = x + self.att(self.ln1(x))&#xA;x = x + self.ffn(self.ln2(x))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Explaining the code for RWKV-3 GPT mode&lt;/h2&gt; &#xA;&lt;h3&gt;The GPT mode - overview&lt;/h3&gt; &#xA;&lt;p&gt;The building blocks of RWKV-3 GPT mode are similar to that of a usual preLN GPT.&lt;/p&gt; &#xA;&lt;p&gt;The only difference is an extra LN after embedding. Note you can absorb this LN into the embedding after finishing the training.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = self.emb(idx)  # input: idx = token indices&#xA;x = self.ln_emb(x) # extra LN after embedding&#xA;x = x + self.att_0(self.ln_att_0(x)) # preLN&#xA;x = x + self.ffn_0(self.ln_ffn_0(x))&#xA;...&#xA;x = x + self.att_n(self.ln_att_n(x))&#xA;x = x + self.ffn_n(self.ln_ffn_n(x))&#xA;x = self.ln_head(x) # final LN before projection&#xA;x = self.head(x)    # output: x = logits&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is important to initialize emb to tiny values, such as nn.init.uniform_(a=-1e-4, b=1e-4), to utilize my trick &lt;a href=&#34;https://github.com/BlinkDL/SmallInitEmb&#34;&gt;https://github.com/BlinkDL/SmallInitEmb&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For the 1.5B RWKV-3, I use Adam (no wd, no dropout) optimizer on 8 * A100 40G.&lt;/p&gt; &#xA;&lt;p&gt;batchSz = 32 * 896, ctxLen = 896. I am using tf32 so the batchSz is a bit small.&lt;/p&gt; &#xA;&lt;p&gt;For the first 15B tokens, LR is fixed at 3e-4, and beta=(0.9, 0.99).&lt;/p&gt; &#xA;&lt;p&gt;Then I set beta=(0.9, 0.999), and do an exponential decay of LR, reaching 1e-5 at 332B tokens.&lt;/p&gt; &#xA;&lt;h3&gt;The GPT mode - ATT block&lt;/h3&gt; &#xA;&lt;p&gt;The RWKV-3 does not have any attention in the usual sense, but we will call this block ATT anyway.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;B, T, C = x.size() # x = (Batch,Time,Channel)&#xA;&#xA;# Mix x with the previous timestep to produce xk, xv, xr&#xA;xx = self.time_shift(x) # self.time_shift = nn.ZeroPad2d((0,0,1,-1))&#xA;xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)&#xA;xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)&#xA;xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)&#xA;&#xA;# Use xk, xv, xr to produce k, v, r&#xA;k = self.key(xk).transpose(-1, -2)&#xA;v = self.value(xv).transpose(-1, -2)&#xA;r = self.receptance(xr)&#xA;k = torch.clamp(k, max=60) # clamp k to avoid overflow&#xA;k = torch.exp(k)&#xA;kv = k * v&#xA;&#xA;# Compute the W-curve = [e^(-n * e^time_decay), e^(-(n-1) * e^time_decay), ..., 1, e^(time_first)]&#xA;self.time_w = torch.cat([torch.exp(self.time_decay) * self.time_curve.to(x.device), self.time_first], dim=-1)&#xA;w = torch.exp(self.time_w)&#xA;&#xA;# Use W to mix kv and k respectively. Add K_EPS to wk to avoid divide-by-zero&#xA;if RUN_DEVICE == &#39;cuda&#39;:&#xA;    wkv = TimeX.apply(w, kv, B,C,T, 0)&#xA;    wk = TimeX.apply(w, k, B,C,T, K_EPS)&#xA;else:&#xA;    w = w[:,-T:].unsqueeze(1)&#xA;    wkv = F.conv1d(nn.ZeroPad2d((T-1, 0, 0, 0))(kv), w, groups=C)&#xA;    wk = F.conv1d(nn.ZeroPad2d((T-1, 0, 0, 0))(k), w, groups=C) + K_EPS&#xA;&#xA;# The RWKV formula&#xA;rwkv = torch.sigmoid(r) * (wkv / wk).transpose(-1, -2)&#xA;rwkv = self.output(rwkv) # final output projection&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The self.key, self.receptance, self.output matrices are all initialized to zero.&lt;/p&gt; &#xA;&lt;p&gt;The time_mix, time_decay, time_first vectors are transferred from a smaller trained model (note: I sort &amp;amp; smooth them too).&lt;/p&gt; &#xA;&lt;h3&gt;The GPT mode - FFN block&lt;/h3&gt; &#xA;&lt;p&gt;The FFN block has three tricks comparing with the usual GPT:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;My time_mix trick.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The sqReLU from the Primer paper.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;An extra receptance-gate (similar to the receptance-gate in ATT block).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Mix x with the previous timestep to produce xk, xr&#xA;xx = self.time_shift(x)&#xA;xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)&#xA;xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)&#xA;&#xA;# The usual FFN operation&#xA;k = self.key(xk)&#xA;k = torch.square(torch.relu(k)) # from the Primer paper&#xA;kv = self.value(k)&#xA;&#xA;# Apply an extra receptance-gate to kv&#xA;rkv = torch.sigmoid(self.receptance(xr)) * kv&#xA;return rkv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The self.value, self.receptance matrices are all initialized to zero.&lt;/p&gt; &#xA;&lt;h2&gt;RWKV-4 improvements&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-v3-plan.png&#34; alt=&#34;RWKV-v3-plan&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;From GPT to RWKV (the formulas)&lt;/h2&gt; &#xA;&lt;p&gt;Let F[t] be the system state at t.&lt;/p&gt; &#xA;&lt;p&gt;Let x[t] be the new external input at t.&lt;/p&gt; &#xA;&lt;p&gt;In GPT, predicting F[t+1] requires considering F[0], F[1], .. F[t]. So it takes O(T^2) to generate a length T sequence.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;simplified formula&lt;/strong&gt; for GPT:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B%5Cmathrm%7Bt%7D%2B1%5D%3D%5Cfrac%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D&#34; alt=&#34;F[\mathrm{t}+1]=\frac{\sum_{\mathrm{i}=0}^{\mathrm{t}} \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}]) \cdot(\mathbf{V}F[\mathrm{i}])}{\sum_{\mathrm{i}=0}^{\mathrm{t}} \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}])}&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s very capable in theory, however that &lt;strong&gt;does not mean we can fully utilize its capability with usual optimizers&lt;/strong&gt;. I suspect the loss landscape is too difficult for our current methods.&lt;/p&gt; &#xA;&lt;p&gt;Compare with the &lt;strong&gt;simplified formula&lt;/strong&gt; for RWKV (the parallel mode, looks similar to Apple&#39;s AFT):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B%5Cmathrm%7Bt%7D%2B1%5D%3D%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot+%5Cfrac%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D&#34; alt=&#34;F[\mathrm{t}+1]=\sigma(\mathbf{R}x[\mathrm{t}]) \cdot \frac{\sum_{\mathrm{i}=0}^{\mathrm{t}} \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K}F[\mathrm{i}]) \cdot(\mathbf{V}F[\mathrm{i}])}{\sum_{\mathrm{i}=0}^{\mathrm{t}} \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K }F[\mathrm{i}])}&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The R, K, V are trainable matrices, and W is a trainable vector (time-decay factor for each channel).&lt;/p&gt; &#xA;&lt;p&gt;In GPT, the contribution of F[i] to F[t+1] is weighted by &lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle++%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+&#34; alt=&#34; \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}]) &#34;&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In RWKV-2, the contribution of F[i] to F[t+1] is weighted by &lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+&#34; alt=&#34;\sigma(\mathbf{R}x[\mathrm{t}]) \cdot \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K}F[\mathrm{i}]) &#34;&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma&#34; alt=&#34;\sigma&#34;&gt; is a non-linearity and we can use sigmoid.&lt;/li&gt; &#xA; &lt;li&gt;Note &lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29&#34; alt=&#34;\sigma(\mathbf{R}x[\mathrm{t}])&#34;&gt; is not in the denominator, and I call R the &#34;receptance&#34;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29&#34; alt=&#34;\exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i}))&#34;&gt; is the time-decay factor. I proposed the same idea (scaling the attention by distance) in Aug 2020 and called it the &#34;time-weighting&#34; (check the commit history of &lt;a href=&#34;https://github.com/BlinkDL/minGPT-tuned&#34;&gt;https://github.com/BlinkDL/minGPT-tuned&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here comes the punchline: we can rewrite it into a RNN (recursive formula). Note:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B1%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5B0%5D%29+%5Ccdot+%5Cfrac%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B0%5D%29%7D%7B%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29%7D&#34; alt=&#34;F[1]=\sigma(\mathbf{R }x[0]) \cdot \frac{ \exp (\mathbf{K }F[0]) \cdot(\mathbf{V }F[0])}{\exp (\mathbf{K }F[0])}&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B2%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5B1%5D%29+%5Ccdot+%5Cfrac%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B1%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B1%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D+%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B0%5D%29%7D%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B1%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D+%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29%7D&#34; alt=&#34;F[2]=\sigma(\mathbf{R }x[1]) \cdot \frac{ \exp (\mathbf{K }F[1]) \cdot(\mathbf{V }F[1])+\exp (\mathbf{W} ) \cdot \exp (\mathbf{K }F[0]) \cdot(\mathbf{V }F[0])}{ \exp (\mathbf{K }F[1])+\exp (\mathbf{W} ) \cdot \exp (\mathbf{K }F[0])}&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Therefore it&#39;s straightforward to verify:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5Bt%2B1%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5Bt%5D%29+%5Ccdot+%5Cfrac%7B%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bt%7D%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D%29+%5Ccdot+A%5B%5Cmathrm%7Bt%7D%5D%7D%7B+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bt%7D%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D%29+%5Ccdot+B%5B%5Cmathrm%7Bt%7D%5D%7D&#34; alt=&#34;F[t+1]=\sigma(\mathbf{R }x[t]) \cdot \frac{\exp (\mathbf{K}F[\mathrm{t}]) \cdot(\mathbf{V}F[\mathrm{t}])+\exp (\mathbf{W}) \cdot A[\mathrm{t}]}{ \exp (\mathbf{K}F[\mathrm{t}])+\exp (\mathbf{W}) \cdot B[\mathrm{t}]}&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;where A[t] and B[t] are the numerator and denominator of the previous step, respectively.&lt;/p&gt; &#xA;&lt;p&gt;I believe RWKV is performant because W is like repeatedly applying a diagonal matrix. Note (P^{-1} D P)^n = P^{-1} D^n P, so it is similar to repeatedly applying a general diagonalizable matrix.&lt;/p&gt; &#xA;&lt;p&gt;Moreover it&#39;s possible to turn it into a continuous ODE (a bit similar to State Space Models). I will write about it later.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#BlinkDL/RWKV-LM&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=BlinkDL/RWKV-LM&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Multimodal ideas&lt;/h2&gt; &#xA;&lt;p&gt;I have an idea for [text --&amp;gt; 32x32 RGB image] using a LM (transformer, RWKV, etc.). Will test it soon.&lt;/p&gt; &#xA;&lt;p&gt;Firstly, LM loss (instead of L2 loss), so the image will not be blurry.&lt;/p&gt; &#xA;&lt;p&gt;Secondly, color quantization. For example, only allowing 8 levels for R/G/B. Then the image vocab size is 8x8x8 = 512 (for each pixel), instead of 2^24. Therefore, a 32x32 RGB image = a len1024 sequence of vocab512 (image tokens), which is a typical input for usual LMs. (Later we can use diffusion models to upsample and generate RGB888 images. We might be able to use a LM for this too.)&lt;/p&gt; &#xA;&lt;p&gt;Thirdly, 2D positional embeddings that are easy for the model to understand. For example, add one-hot X &amp;amp; Y coords to the first 64(=32+32) channels. Say if the pixel is at x=8, y=20, then we will add 1 to channel 8 and channel 52 (=32+20). Moreover probably we can add the float X &amp;amp; Y coords (normalized to 0~1 range) to another 2 channels. And other periodic pos. encoding might help too (will test).&lt;/p&gt; &#xA;&lt;p&gt;Finally, RandRound when doing the color quantization in the DataLoader. For example, if the float level is 4.578, then there is a 57.8% chance to use 5, and (1-57.8%) chance to use 4. And we can allow both 4 and 5 in the prediction, but the loss will be higher if the prediction is 4.&lt;/p&gt; &#xA;&lt;p&gt;Multi-task training might help too. I will try this dataset format: [TxtFirst] [Desc of Img (txt tokens)] [Img] [img tokens] and sometimes [ImgFirst] [img tokens] [Txt] [Desc of Img (txt tokens)] ... the order of the imgs should be randomized in the DataLoader, and [TxtFirst] [ImgFirst] [Img] [Txt] are special tokens and do random sampling of the full dataset. So sometimes the model will see the img tokens first and then the corresponding txt tokens, which is a [img -&amp;gt; txt] task. And the model will see some partial imgs and partial txts. I think a char-level LM might help the model to write correct text on images.&lt;/p&gt; &#xA;&lt;h2&gt;How to sample a large dataset (for training)&lt;/h2&gt; &#xA;&lt;p&gt;I am using a trick to sample the Pile deterministically yet randomly enough.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s say the pile has x chunks (a chunk = ctx_len tokens).&lt;/p&gt; &#xA;&lt;p&gt;pick a prime number p just less than x, and make sure p = 2 (mod 3).&lt;/p&gt; &#xA;&lt;p&gt;Use (step * step * step) mod p to sample it. Add some bias to step for extra randomness.&lt;/p&gt; &#xA;&lt;h2&gt;The top-p-x sampling method (for inference)&lt;/h2&gt; &#xA;&lt;p&gt;We propose a new sampling method called top-p-x:&lt;/p&gt; &#xA;&lt;p&gt;it&#39;s like top-p, and the only difference is you also keep all tokens whose prob &amp;gt; x.&lt;/p&gt; &#xA;&lt;p&gt;Try x = 0.01 first.&lt;/p&gt; &#xA;&lt;h2&gt;Better Learning Rate Schedule via Variantional Method of Loss Curve&lt;/h2&gt; &#xA;&lt;p&gt;I propose a simple new method to find better LR schedules. The method is cost-efficient and practical for large LMs. The takeaway is we can model the loss curve dynamics (phenomenology) w.r.t. the LR, and a nice closed-form LR curve can be directly computed from it using variantional method. Moreover we can predict the final loss with reasonable accuracy.&lt;/p&gt; &#xA;&lt;p&gt;UPDATE: In &#34;Conclusion 1.&#34;, use the best-fitting regime (ignore the initial steps where our approximations break down) to fit the parameters.&lt;/p&gt; &#xA;&lt;p&gt;Try this: fixed lr for 1 hr, then exponential decay to 0.2 * lr in 12 hrs, and choose the t=[1hr, 13hr] segment.&lt;/p&gt; &#xA;&lt;p&gt;In the last three plots, black = predicted loss curve of the new LR schedule, blue = original (unoptimized) real loss curve, orange = new LR schedule.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/Research/better_lr_schedule.png&#34; alt=&#34;better_lr_schedule&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;RWKV v1&lt;/h1&gt; &#xA;&lt;p&gt;We propose the RWKV language model, with alternating time-mix and channel-mix layers:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Balign%2A%7D%0A%5Ctext%7BTime-mix+%3A%7D+%26%26+%5Ctext%7BTM%7D_%7Bt%2Cc%7D+%26%26%3D%26%26%5Ctext%7Bsigmoid%7D%28%5Ctext%7BR%7D_%7Bt%2Cc%7D%29+%26%26%5Ccdot%26%26+%26%26%5Ctextstyle%5Csum_%7Bu%7D+%26%26%5Ctextbf%7BW%7D_%7Bt%2Cu%2Cc%7D+%26%26%5Ccdot%26%26+%5Ctext%7Bsoftmax%7D_t%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29+%26%26%5Ccdot%26%26+%5Ctext%7BV%7D_%7Bu%2Cc%7D%5C%5C%0A%5Ctext%7BChannel-mix+%3A%7D+%26%26+%5Ctext%7BCM%7D_%7Bt%2Cc%7D+%26%26%3D%26%26%5Ctext%7Bsigmoid%7D%28%5Ctext%7BR%7D_%7Bt%2Cc%7D%29+%26%26%5Ccdot%26%26+%26%26%5Ctextstyle%5Csum_d+%26%26%5Ctextbf%7BW%7D_%7Bc%2Cd%7D+%26%26%5Ccdot%26%26+%5Ctext%7Bgelu%7D%28%5Ctext%7BK%7D_%7Bt%2Cd%7D%29+%26%26%5Ccdot%26%26+%5Ctext%7BV%7D_%7Bt%2Cd%7D%0A%5Cend%7Balign%2A%7D%0A&#34; alt=&#34;\begin{align*}&#xA;\text{Time-mix :} &amp;amp;&amp;amp; \text{TM}_{t,c} &amp;amp;&amp;amp;=&amp;amp;&amp;amp;\text{sigmoid}(\text{R}_{t,c}) &amp;amp;&amp;amp;\cdot&amp;amp;&amp;amp; &amp;amp;&amp;amp;\textstyle\sum_{u} &amp;amp;&amp;amp;\textbf{W}_{t,u,c} &amp;amp;&amp;amp;\cdot&amp;amp;&amp;amp; \text{softmax}_t(\text{K}_{u,c}) &amp;amp;&amp;amp;\cdot&amp;amp;&amp;amp; \text{V}_{u,c}\\&#xA;\text{Channel-mix :} &amp;amp;&amp;amp; \text{CM}_{t,c} &amp;amp;&amp;amp;=&amp;amp;&amp;amp;\text{sigmoid}(\text{R}_{t,c}) &amp;amp;&amp;amp;\cdot&amp;amp;&amp;amp; &amp;amp;&amp;amp;\textstyle\sum_d &amp;amp;&amp;amp;\textbf{W}_{c,d} &amp;amp;&amp;amp;\cdot&amp;amp;&amp;amp; \text{gelu}(\text{K}_{t,d}) &amp;amp;&amp;amp;\cdot&amp;amp;&amp;amp; \text{V}_{t,d}&#xA;\end{align*}&#xA;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The R, K, V are generated by linear transforms of input, and W is parameter. The idea of RWKV is to decompose attention into R(target) * W(src, target) * K(src). So we can call R &#34;receptance&#34;, and sigmoid means it&#39;s in 0~1 range.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The Time-mix is similar to AFT (&lt;a href=&#34;https://arxiv.org/abs/2105.14103&#34;&gt;https://arxiv.org/abs/2105.14103&lt;/a&gt;). There are two differences.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;(1) We changed the normalization (denominator). For masked language models, we define:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext%7Bsoftmax%7D_t%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29+%3D+%5Cfrac%7B%5Cexp%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29%7D%7B%5Csum_%7Bv+%5Cleq+t%7D%5Cexp%28%5Ctext%7BK%7D_%7Bv%2Cc%7D%29%7D&#34; alt=&#34;\text{softmax}_t(\text{K}_{u,c}) = \frac{\exp(\text{K}_{u,c})}{\sum_{v \leq t}\exp(\text{K}_{v,c})}&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(UPDATE: We are using the original AFT normalization in v2)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Initialize K and R matrices (and the output projection matrix) to ZERO for fast &amp;amp; stable convergence.&lt;/p&gt; &#xA;&lt;p&gt;(2) We decompose W_{t,u,c} and introduce multi-head W (here h is the corresponding head of c):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+W_%7Bt%2Cu%2Cc%7D%3Df_h%28t-u%29%5Ccdot+%5Calpha_h%28u%29+%5Ccdot+%5Cbeta_h%28t%29&#34; alt=&#34;W_{t,u,c}=f_h(t-u)\cdot \alpha_h(u) \cdot \beta_h(t)&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Moreover we multiply the final output of Time-mix layer by γ(t). The reason for the α β γ factors, is because the context size is smaller when t is small, and this can be compensated using the α β γ factors.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(UPDATE: We remove α β γ factors in v2-RNN and restrict W to be of a simple form and hence able to rewrite it as RNN)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The Channel-mix is similar to GeGLU (&lt;a href=&#34;https://arxiv.org/abs/2002.05202&#34;&gt;https://arxiv.org/abs/2002.05202&lt;/a&gt;) with an extra R factor. Initialize R and W matrices to ZERO for fast &amp;amp; stable convergence.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Finally, we add extra token-shift (time-shift mixing) as in (&lt;a href=&#34;https://github.com/BlinkDL/minGPT-tuned&#34;&gt;https://github.com/BlinkDL/minGPT-tuned&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Token-shift (time-shift mixing)&lt;/h1&gt; &#xA;&lt;p&gt;The token-shift explicitly uses (half the channels of this token) &amp;amp; (half the channels of prev token) to generate all vectors (QKV, RWKV, ...).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;self.time_shift = nn.ZeroPad2d((0,0,1,-1))&#xA;&#xA;x = torch.cat([self.time_shift(x[:, :, :C//2]), x[:, :, C//2:]], dim = -1)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Dividing channels by 2 and shift-1 works great for char-level English and char-level Chinese LM.&lt;/p&gt; &#xA;&lt;p&gt;However for BPE-level English LM, it&#39;s only effective if your embedding is large enough (at least 1024 - so the usual small L12-D768 model is not enough).&lt;/p&gt; &#xA;&lt;p&gt;My theory on the effectiveness of token-shift:&lt;/p&gt; &#xA;&lt;p&gt;When we train a GPT, the hidden representation of a token has to accomplish two different objects:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Predict the next token. Sometimes this is easy (obvious next token).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Collect all previous context info, so later tokens can use it. This is always hard.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The shifted channels can focus on (2), so we have good propagation of info. It&#39;s like some kind of residual connection, or a small RNN inside the transformer.&lt;/p&gt; &#xA;&lt;p&gt;You can use token-shift in usual QKV self-attention too. I looked at the weights, and found V really likes the shifted channels, less so for Q. Makes sense if you think about it. I also found you may want to use less mixing in higher layers.&lt;/p&gt; &#xA;&lt;p&gt;p.s. There is a MHA_pro model in this repo with strong performance. Give it a try :)&lt;/p&gt; &#xA;&lt;h1&gt;The Head-QK Trick: learning to copy and avoid tokens&lt;/h1&gt; &#xA;&lt;p&gt;In usual transformer, a small model has difficulty copying tokens (such as person names) in the context. We add extra Q &amp;amp; K to the final output such that the model can directly copy (or avoid) tokens in the context. Afterwards the model will teach itself NER (named entity recognition) if you look at the learned weights.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;q = self.head_q(x)[:,:T,:] # projecting to 256-d&#xA;k = self.head_k(x)[:,:T,:] # projecting to 256-d&#xA;c = (q @ k.transpose(-2, -1)) * (1.0 / 256)&#xA;c = c.masked_fill(self.copy_mask[:T,:T] == 0, 0)&#xA;c = c @ F.one_hot(idx, num_classes = self.config.vocab_size).float()       &#xA;x = self.head(x) + c&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: when a token occurs multiple times in the context, it might be better to use max(prob) instead of sum(prob).&lt;/p&gt; &#xA;&lt;h1&gt;The top-a sampling method&lt;/h1&gt; &#xA;&lt;p&gt;We also propose a new sampling method called top-a (as in src/utils.py):&lt;/p&gt; &#xA;&lt;p&gt;(1) Find the max probability p_max after softmax.&lt;/p&gt; &#xA;&lt;p&gt;(2) Remove all entries whose probability is lower than 0.2 * pow(p_max, 2). So it&#39;s adaptive, hence &#34;top-a&#34;.&lt;/p&gt; &#xA;&lt;p&gt;(3) Feel free to tune the 0.2 and 2 factor. Tune 0.2 first.&lt;/p&gt; &#xA;&lt;p&gt;The idea of top-a:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If max_prob=0.9, then remove all tokens with prob &amp;lt; 0.162 (so, removing all alternatives)&lt;/li&gt; &#xA; &lt;li&gt;If max_prob=0.5, then remove all tokens with prob &amp;lt; 0.05 (so, allowing more choices)&lt;/li&gt; &#xA; &lt;li&gt;If max_prob=0.1, then remove all tokens with prob &amp;lt; 0.002 (so, allowing lots of possibilities)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;probs = F.softmax(logits, dim=-1)&#xA;&#xA;limit = torch.pow(torch.max(probs), 2) * 0.02&#xA;logits[probs &amp;lt; limit] = -float(&#39;Inf&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Performance&lt;/h1&gt; &#xA;&lt;p&gt;Character-level loss on simplebooks-92 dataset &lt;a href=&#34;https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip&#34;&gt;https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-vs-MHA.png&#34; alt=&#34;RWKV-vs-MHA&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Gray: usual MHA+Rotary+GeGLU - performance not as good. 17.2M params.&lt;/p&gt; &#xA;&lt;p&gt;Red: RWKV (&#34;linear&#34; attention) - VRAM friendly - quite faster when ctx window is long - good performance. 16.6M params.&lt;/p&gt; &#xA;&lt;p&gt;Green: MHA+Rotary+GeGLU+Token_shift. 17.2M params.&lt;/p&gt; &#xA;&lt;p&gt;Blue: MHA_pro (MHA with various tweaks &amp;amp; RWKV-type-FFN) - slow - needs more VRAM - good performance. 16.6M params.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{peng_bo_2021_5196578,&#xA;  author       = {PENG Bo},&#xA;  title        = {BlinkDL/RWKV-LM: 0.01},&#xA;  month        = aug,&#xA;  year         = 2021,&#xA;  publisher    = {Zenodo},&#xA;  version      = {0.01},&#xA;  doi          = {10.5281/zenodo.5196577},&#xA;  url          = {https://doi.org/10.5281/zenodo.5196577}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Initialization&lt;/h1&gt; &#xA;&lt;p&gt;We use careful initialization for RWKV to get fast convergence - orthogonal matrices with proper scaling, and special time_w curves. Check model.py for details.&lt;/p&gt; &#xA;&lt;p&gt;Some learned time_w examples:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/BlinkDL/RWKV-LM/main/RWKV-time-w.png&#34; alt=&#34;RWKV-time-w&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>