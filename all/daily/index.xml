<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-14T01:28:59Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>eumemic/ai-legion</title>
    <updated>2023-04-14T01:28:59Z</updated>
    <id>tag:github.com,2023-04-14:/eumemic/ai-legion</id>
    <link href="https://github.com/eumemic/ai-legion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An LLM-powered autonomous agent platform&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Legion: an LLM-powered autonomous agent platform&lt;/h1&gt; &#xA;&lt;p&gt;A framework for autonomous agents who can work together to accomplish tasks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/X9MkUEsEUC&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1095770840173383802?label=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;You will need at least Node 10.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Rename the &lt;code&gt;.env.template&lt;/code&gt; file at the root of the project to &lt;code&gt;.env&lt;/code&gt; and add your secrets to it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=... # obtain from https://platform.openai.com/account/api-keys&#xA;# the following are needed for the agent to be able to search the web:&#xA;GOOGLE_SEARCH_ENGINE_ID=... # create a custom search engine at https://cse.google.com/cse/all&#xA;GOOGLE_API_KEY=... # obtain from https://console.cloud.google.com/apis/credentials&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll also need to enable the Google Custom Search API for your Google Cloud account, e.g. &lt;a href=&#34;https://console.cloud.google.com/apis/library/customsearch.googleapis.com&#34;&gt;https://console.cloud.google.com/apis/library/customsearch.googleapis.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running&lt;/h2&gt; &#xA;&lt;p&gt;Start the program:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm run start [# of agents] [gpt-3.5-turbo|gpt-4]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Interact with the agents through the console. Anything you type will be sent as a message to all agents currently.&lt;/p&gt; &#xA;&lt;h2&gt;Action errors&lt;/h2&gt; &#xA;&lt;p&gt;After spinning up a new agent, you will often see them make some mistakes which generate errors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Trying to use an action before they&#39;ve asked for &lt;code&gt;help&lt;/code&gt; on it to know what its parameters are&lt;/li&gt; &#xA; &lt;li&gt;Trying to just use a raw text response instead of a correctly-formatted action (or raw text wrapping a code block which contains a valid action)&lt;/li&gt; &#xA; &lt;li&gt;Trying to use a multi-line parameter value without wrapping it in the multiline delimiter (&lt;code&gt;% ff9d7713-0bb0-40d4-823c-5a66de48761b&lt;/code&gt;) This is a normal period of adjustment as they learn to operate themselves. They generally will learn from these mistakes and recover, although &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; agents sometimes devolve into endless error loops and can&#39;t figure out what the problem is. It&#39;s highly advised never to leave an agent unattended, since such infinite loops can eat through your tokens quickly (especially if they&#39;re stuck on a high-token-count action such as &lt;code&gt;writeFile&lt;/code&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Agent state&lt;/h2&gt; &#xA;&lt;p&gt;Each agent stores its state under the &lt;code&gt;.store&lt;/code&gt; directory. Agent 1, for example, has&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.store/1/memory&#xA;.store/1/goals&#xA;.store/1/notes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can simply delete any of these things, or the whole agent folder (or the whole &lt;code&gt;.store&lt;/code&gt;) to selectively wipe whatever state you want between runs. Otherwise, agents will pick up where you left off on restart.&lt;/p&gt; &#xA;&lt;p&gt;A nice aspect of this is that when you want to debug a problem you ran into with a particular agent, you can delete the events in their memory subsequent to the point where the problem occurred, make changes to the code, and restart them to effectively replay that moment until you&#39;ve fixed the bug. You can also ask an agent to implement a feature, and once they&#39;ve done so you can restart, tell them that you&#39;ve loaded the feature, and ask them to try it out.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>biobootloader/wolverine</title>
    <updated>2023-04-14T01:28:59Z</updated>
    <id>tag:github.com,2023-04-14:/biobootloader/wolverine</id>
    <link href="https://github.com/biobootloader/wolverine" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Wolverine&lt;/h1&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;Give your python scripts regenerative healing abilities!&lt;/p&gt; &#xA;&lt;p&gt;Run your scripts with Wolverine and when they crash, GPT-4 edits them and explains what went wrong. Even if you have many bugs it will repeatedly rerun until it&#39;s fixed.&lt;/p&gt; &#xA;&lt;p&gt;For a quick demonstration see my &lt;a href=&#34;https://twitter.com/bio_bootloader/status/1636880208304431104&#34;&gt;demo video on twitter&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m venv venv&#xA;source venv/bin/activate&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add your openAI api key to &lt;code&gt;openai_key.txt&lt;/code&gt; - &lt;em&gt;warning!&lt;/em&gt; by default this uses GPT-4 and may make many repeated calls to the api.&lt;/p&gt; &#xA;&lt;h2&gt;Example Usage&lt;/h2&gt; &#xA;&lt;p&gt;To run with gpt-4 (the default, tested option):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python wolverine.py buggy_script.py &#34;subtract&#34; 20 3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also run with other models, but be warned they may not adhere to the edit format as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python wolverine.py --model=gpt-3.5-turbo buggy_script.py &#34;subtract&#34; 20 3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Future Plans&lt;/h2&gt; &#xA;&lt;p&gt;This is just a quick prototype I threw together in a few hours. There are many possible extensions and contributions are welcome:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;add flags to customize usage, such as asking for user confirmation before running changed code&lt;/li&gt; &#xA; &lt;li&gt;further iterations on the edit format that GPT responds in. Currently it struggles a bit with indentation, but I&#39;m sure that can be improved&lt;/li&gt; &#xA; &lt;li&gt;a suite of example buggy files that we can test prompts on to ensure reliablity and measure improvement&lt;/li&gt; &#xA; &lt;li&gt;multiple files / codebases: send GPT everything that appears in the stacktrace&lt;/li&gt; &#xA; &lt;li&gt;graceful handling of large files - should we just send GPT relevant classes / functions?&lt;/li&gt; &#xA; &lt;li&gt;extension to languages other than python&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ohmplatform/FreedomGPT</title>
    <updated>2023-04-14T01:28:59Z</updated>
    <id>tag:github.com,2023-04-14:/ohmplatform/FreedomGPT</id>
    <link href="https://github.com/ohmplatform/FreedomGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This codebase is for a React and Electron-based app that executes the FreedomGPT LLM locally (offline and private) on Mac and Windows using a chat-based interface (based on Alpaca Lora)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Freedom GPT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/license-GNU-blue.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/release/ohmplatform/freedom-gpt-electron-app.svg?sanitize=true&#34; alt=&#34;GitHub release&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ohmplatform/freedom-gpt-electron-app.svg?sanitize=true&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/ohmplatform/freedom-gpt-electron-app/total.svg?sanitize=true&#34; alt=&#34;GitHub All Releases&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This is the repository for the Freedom GPT application. This application is built using &lt;a href=&#34;https://www.electronjs.org/&#34;&gt;Electron&lt;/a&gt; and &lt;a href=&#34;https://reactjs.org/&#34;&gt;React&lt;/a&gt;. It is a desktop application that allows users to run alpaca models on their local machine.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nodejs.org/en/download/&#34;&gt;Node.js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://classic.yarnpkg.com/en/docs/install/#windows-stable&#34;&gt;Yarn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;Git&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;If you want to run the application directly&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/ohmplatform/FreedomGPT.git&#xA;cd FreedomGPT&#xA;yarn install&#xA;yarn start:prod&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;If you want to contribute to the project&lt;/h1&gt; &#xA;&lt;h2&gt;Working with the repository&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone --recursive https://github.com/ohmplatform/FreedomGPT.git&#xA;cd FreedomGPT&#xA;yarn install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Building the alpaca.cpp library&lt;/h1&gt; &#xA;&lt;h2&gt;Building from Source (MacOS/Linux)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd alpaca.cpp&#xA;make chat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building from Source (Windows)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download and install CMake: &lt;a href=&#34;https://cmake.org/download/&#34;&gt;https://cmake.org/download/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run the following commands one by one:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ps1&#34;&gt;cd alpaca.cpp&#xA;cmake .&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You should now have a &lt;code&gt;Release&lt;/code&gt; folder with a &lt;code&gt;chat.exe&lt;/code&gt; file inside it. You can run this file to test the chat client.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Changing the API URL&lt;/h2&gt; &#xA;&lt;p&gt;We are using &lt;code&gt;http://localhost:8889&lt;/code&gt; as the API URL, you can change it in the file &lt;code&gt;src/index.ts&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running the application&lt;/h2&gt; &#xA;&lt;p&gt;To run the application, run the following command in your terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;yarn start&#xA;&#xA;⦻ Make sure you are in the root directory of the project.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Credits&lt;/h1&gt; &#xA;&lt;p&gt;This project utilizes several open-source packages and libraries, without which this project would not have been possible:&lt;/p&gt; &#xA;&lt;p&gt;&#34;alpaca.cpp&#34; by antimatter15 - a C++ library for Alpaca API. &lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;https://github.com/antimatter15/alpaca.cpp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&#34;LLAMA&#34; by Facebook Research - a low-latency, large-scale approximate nearest neighbor search algorithm. &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;https://github.com/facebookresearch/llama&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&#34;Alpaca&#34; by Stanford CRFM - a framework for understanding and improving the efficiency and robustness of algorithms. &lt;a href=&#34;https://crfm.stanford.edu/2023/03/13/alpaca.html&#34;&gt;https://crfm.stanford.edu/2023/03/13/alpaca.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&#34;alpaca-lora&#34; by tloen - a Python library for working with LoRa radios and the Alpaca protocol. &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;https://github.com/tloen/alpaca-lora&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&#34;alpaca-lora-7b&#34; by Hugging Face - a pre-trained language model for the Alpaca protocol. &lt;a href=&#34;https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml/tree/main&#34;&gt;https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml/tree/main&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We would like to express our gratitude to the developers of these packages and their contributors for making their work available to the public under open source licenses. Their contributions have enabled us to build a more robust and efficient project.&lt;/p&gt; &#xA;&lt;h1&gt;LICENSE&lt;/h1&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/ohmplatform/FreedomGPT/main/LICENSE&#34;&gt; LICENSE &lt;/a&gt;file.&lt;/p&gt;</summary>
  </entry>
</feed>