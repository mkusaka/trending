<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-13T01:25:11Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ishan0102/vimGPT</title>
    <updated>2023-11-13T01:25:11Z</updated>
    <id>tag:github.com,2023-11-13:/ishan0102/vimGPT</id>
    <link href="https://github.com/ishan0102/vimGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Browse the web with GPT-4V and Vimium&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;vimGPT&lt;/h1&gt; &#xA;&lt;p&gt;Giving multimodal models an interface to play with.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ishan0102/vimGPT/assets/47067154/467be2ac-7e8d-47de-af89-5bb6f51c1c31&#34;&gt;https://github.com/ishan0102/vimGPT/assets/47067154/467be2ac-7e8d-47de-af89-5bb6f51c1c31&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;LLMs as a way to browse the web is being explored by numerous startups and open-source projects. With this project, I was interested in seeing if we could only use &lt;a href=&#34;https://openai.com/research/gpt-4v-system-card&#34;&gt;GPT-4V&lt;/a&gt;&#39;s vision capabilities for web browsing.&lt;/p&gt; &#xA;&lt;p&gt;The issue with this is it&#39;s hard to determine what the model wants to click on without giving it the browser DOM as text. &lt;a href=&#34;https://vimium.github.io/&#34;&gt;Vimium&lt;/a&gt; is a Chrome extension that lets you navigate the web with only your keyboard. I thought it would be interesting to see if we could use Vimium to give the model a way to interact with the web.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Install Python requirements&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download Vimium locally (have to load the extension manually when running Playwright)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Ideas&lt;/h2&gt; &#xA;&lt;p&gt;Feel free to collaborate with me on this, I have a number of ideas:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://platform.openai.com/docs/assistants/overview&#34;&gt;Assistant API&lt;/a&gt; once it&#39;s released for automatic context retrieval. The Assistant API will create a thread that we can add messages too, to keep the history of actions, but it doesn&#39;t support the Vision API yet.&lt;/li&gt; &#xA; &lt;li&gt;Vimium fork for overlaying elements. A specialized version of Vimium that selectively overlays elements based on context could be useful, effectively pruning based on the user query. Might be worth testing if different sized boxes/colors help.&lt;/li&gt; &#xA; &lt;li&gt;Use higher resolution images, as it seems to fail at low res. I noticed that below a certain threshold, the model wouldn&#39;t detect anything. This might be improved by using higher resolution images but that would require more tokens.&lt;/li&gt; &#xA; &lt;li&gt;Fine-tune &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVa&lt;/a&gt; or &lt;a href=&#34;https://github.com/THUDM/CogVLM&#34;&gt;CogVLM&lt;/a&gt; to do this or &lt;a href=&#34;https://www.adept.ai/blog/fuyu-8b&#34;&gt;Fuyu-8B&lt;/a&gt;. Could be faster/cheaper. CogVLM can accurately specify pixel coordinates which may be a good way to augment this.&lt;/li&gt; &#xA; &lt;li&gt;Use JSON mode once it&#39;s released for Vision API. Currently the Vision API doesn&#39;t support JSON mode or function calling, so we have to rely on more primitive prompting methods.&lt;/li&gt; &#xA; &lt;li&gt;Have the Vision API return general instructions, formalized by another call to the JSON mode version of the API. This is a workaround for the JSON mode issue but requires another LLM call, which is slower/more expensive.&lt;/li&gt; &#xA; &lt;li&gt;Add speech-to-text with Whisper or another model to eliminate text input and make this more accessible.&lt;/li&gt; &#xA; &lt;li&gt;Make this work for your own browser instead of spinning up an artificial one. I want to be able to order food with my credit card.&lt;/li&gt; &#xA; &lt;li&gt;Provide the frames with and without Vimium enabled in case the model can&#39;t see what&#39;s under the yellow square.&lt;/li&gt; &#xA; &lt;li&gt;Pass the Chrome accessibility tree in as input in addition to the image. This provides a layout of interactive elements that can be mapped to the Vimium bindings.&lt;/li&gt; &#xA; &lt;li&gt;Have it write longer things based on the context of the page or return information to the user based on the query. Examples are replying to an email, summarizing a news article, etc. Visual question answering.&lt;/li&gt; &#xA; &lt;li&gt;Make this a useful tool for blind people by adding voice mode and a key that creates an Assistant API for a given page. Something where you can &#34;speak to an agent&#34; about a page content in natural language.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Globe-Engineer/globot&#34;&gt;https://github.com/Globe-Engineer/globot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nat/natbot&#34;&gt;https://github.com/nat/natbot&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>chatwoot/chatwoot</title>
    <updated>2023-11-13T01:25:11Z</updated>
    <id>tag:github.com,2023-11-13:/chatwoot/chatwoot</id>
    <link href="https://github.com/chatwoot/chatwoot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open-source customer engagement suite, an alternative to Intercom, Zendesk, Salesforce Service Cloud etc. üî•üí¨&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/2246121/282256557-1570674b-d142-4198-9740-69404cc6a339.png#gh-light-mode-only&#34; width=&#34;100%&#34; alt=&#34;Chat dashboard dark mode&#34;&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/2246121/282256632-87f6a01b-6467-4e0e-8a93-7bbf66d03a17.png#gh-dark-mode-only&#34; width=&#34;100%&#34; alt=&#34;Chat dashboard&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Chatwoot&lt;/h1&gt; &#xA;&lt;p&gt;Customer engagement suite, an open-source alternative to Intercom, Zendesk, Salesforce Service Cloud etc.&lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://heroku.com/deploy?template=https://github.com/chatwoot/chatwoot/tree/master&#34; alt=&#34;Deploy to Heroku&#34;&gt; &lt;img width=&#34;150&#34; alt=&#34;Deploy&#34; src=&#34;https://www.herokucdn.com/deploy/button.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://marketplace.digitalocean.com/apps/chatwoot?refcode=f2238426a2a8&#34; alt=&#34;Deploy to DigitalOcean&#34;&gt; &lt;img width=&#34;200&#34; alt=&#34;Deploy to DO&#34; src=&#34;https://www.deploytodo.com/do-btn-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://codeclimate.com/github/chatwoot/chatwoot/maintainability&#34;&gt;&lt;img src=&#34;https://api.codeclimate.com/v1/badges/e6e3f66332c91e5a4c0c/maintainability&#34; alt=&#34;Maintainability&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/circleci/build/github/chatwoot/chatwoot&#34; alt=&#34;CircleCI Badge&#34;&gt; &lt;a href=&#34;https://hub.docker.com/r/chatwoot/chatwoot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/chatwoot/chatwoot&#34; alt=&#34;Docker Pull Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/chatwoot/chatwoot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/cloud/build/chatwoot/chatwoot&#34; alt=&#34;Docker Build Badge&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/commit-activity/m/chatwoot/chatwoot&#34; alt=&#34;Commits-per-month&#34;&gt; &lt;a title=&#34;Crowdin&#34; target=&#34;_self&#34; href=&#34;https://chatwoot.crowdin.com/chatwoot&#34;&gt;&lt;img src=&#34;https://badges.crowdin.net/e/37ced7eba411064bd792feb3b7a28b16/localized.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/cJXdrwS&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/647412545203994635&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huntr.dev/bounties/disclose&#34;&gt;&lt;img src=&#34;https://cdn.huntr.dev/huntr_security_badge_mono.svg?sanitize=true&#34; alt=&#34;Huntr&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://status.chatwoot.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fchatwoot%2Fstatus%2Fmaster%2Fapi%2Fchatwoot%2Fuptime.json&#34; alt=&#34;uptime&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://status.chatwoot.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fchatwoot%2Fstatus%2Fmaster%2Fapi%2Fchatwoot%2Fresponse-time.json&#34; alt=&#34;response time&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://artifacthub.io/packages/helm/chatwoot/chatwoot&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/artifact-hub&#34; alt=&#34;Artifact HUB&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/2246121/282255783-ee8a50c9-f42d-4752-8201-2d59965a663d.png#gh-light-mode-only&#34; width=&#34;100%&#34; alt=&#34;Chat dashboard dark mode&#34;&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/2246121/282255784-3d1994ec-d895-4ff5-ac68-d819987e1869.png#gh-dark-mode-only&#34; width=&#34;100%&#34; alt=&#34;Chat dashboard&#34;&gt; &#xA;&lt;p&gt;Chatwoot is an open-source, self-hosted customer engagement suite. Chatwoot lets you view and manage your customer data, communicate with them irrespective of which medium they use, and re-engage them based on their profile.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;Chatwoot supports the following conversation channels:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Website&lt;/strong&gt;: Talk to your customers using our live chat widget and make use of our SDK to identify a user and provide contextual support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Facebook&lt;/strong&gt;: Connect your Facebook pages and start replying to the direct messages to your page.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Instagram&lt;/strong&gt;: Connect your Instagram profile and start replying to the direct messages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Twitter&lt;/strong&gt;: Connect your Twitter profiles and reply to direct messages or the tweets where you are mentioned.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Telegram&lt;/strong&gt;: Connect your Telegram bot and reply to your customers right from a single dashboard.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;WhatsApp&lt;/strong&gt;: Connect your WhatsApp business account and manage the conversation in Chatwoot.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Line&lt;/strong&gt;: Connect your Line account and manage the conversations in Chatwoot.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SMS&lt;/strong&gt;: Connect your Twilio SMS account and reply to the SMS queries in Chatwoot.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;API Channel&lt;/strong&gt;: Build custom communication channels using our API channel.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: Forward all your email queries to Chatwoot and view it in our integrated dashboard.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And more.&lt;/p&gt; &#xA;&lt;p&gt;Other features include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;CRM&lt;/strong&gt;: Save all your customer information right inside Chatwoot, use contact notes to log emails, phone calls, or meeting notes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Custom Attributes&lt;/strong&gt;: Define custom attribute attributes to store information about a contact or a conversation and extend the product to match your workflow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Shared multi-brand inboxes&lt;/strong&gt;: Manage multiple brands or pages using a shared inbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Private notes&lt;/strong&gt;: Use @mentions and private notes to communicate internally about a conversation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Canned responses (Saved replies)&lt;/strong&gt;: Improve the response rate by adding saved replies for frequently asked questions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conversation Labels&lt;/strong&gt;: Use conversation labels to create custom workflows.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Auto assignment&lt;/strong&gt;: Chatwoot intelligently assigns a ticket to the agents who have access to the inbox depending on their availability and load.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conversation continuity&lt;/strong&gt;: If the user has provided an email address through the chat widget, Chatwoot will send an email to the customer under the agent name so that the user can continue the conversation over the email.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-lingual support&lt;/strong&gt;: Chatwoot supports 10+ languages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Powerful API &amp;amp; Webhooks&lt;/strong&gt;: Extend the capability of the software using Chatwoot‚Äôs webhooks and APIs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt;: Chatwoot natively integrates with Slack right now. Manage your conversations in Slack without logging into the dashboard.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Detailed documentation is available at &lt;a href=&#34;https://www.chatwoot.com/help-center&#34;&gt;chatwoot.com/help-center&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Translation process&lt;/h2&gt; &#xA;&lt;p&gt;The translation process for Chatwoot web and mobile app is managed at &lt;a href=&#34;https://translate.chatwoot.com&#34;&gt;https://translate.chatwoot.com&lt;/a&gt; using Crowdin. Please read the &lt;a href=&#34;https://www.chatwoot.com/docs/contributing/translating-chatwoot-to-your-language&#34;&gt;translation guide&lt;/a&gt; for contributing to Chatwoot.&lt;/p&gt; &#xA;&lt;h2&gt;Branching model&lt;/h2&gt; &#xA;&lt;p&gt;We use the &lt;a href=&#34;https://nvie.com/posts/a-successful-git-branching-model/&#34;&gt;git-flow&lt;/a&gt; branching model. The base branch is &lt;code&gt;develop&lt;/code&gt;. If you are looking for a stable version, please use the &lt;code&gt;master&lt;/code&gt; or tags labelled as &lt;code&gt;v1.x.x&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;h3&gt;Heroku one-click deploy&lt;/h3&gt; &#xA;&lt;p&gt;Deploying Chatwoot to Heroku is a breeze. It&#39;s as simple as clicking this button:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://heroku.com/deploy?template=https://github.com/chatwoot/chatwoot/tree/master&#34;&gt;&lt;img src=&#34;https://www.herokucdn.com/deploy/button.svg?sanitize=true&#34; alt=&#34;Deploy&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Follow this &lt;a href=&#34;https://www.chatwoot.com/docs/environment-variables&#34;&gt;link&lt;/a&gt; to understand setting the correct environment variables for the app to work with all the features. There might be breakages if you do not set the relevant environment variables.&lt;/p&gt; &#xA;&lt;h3&gt;DigitalOcean 1-Click Kubernetes deployment&lt;/h3&gt; &#xA;&lt;p&gt;Chatwoot now supports 1-Click deployment to DigitalOcean as a kubernetes app.&lt;/p&gt; &#xA;&lt;a href=&#34;https://marketplace.digitalocean.com/apps/chatwoot?refcode=f2238426a2a8&#34; alt=&#34;Deploy to DigitalOcean&#34;&gt; &lt;img width=&#34;200&#34; alt=&#34;Deploy to DO&#34; src=&#34;https://www.deploytodo.com/do-btn-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;&lt;h3&gt;Other deployment options&lt;/h3&gt; &#xA;&lt;p&gt;For other supported options, checkout our &lt;a href=&#34;https://chatwoot.com/deploy&#34;&gt;deployment page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;p&gt;Looking to report a vulnerability? Please refer our &lt;a href=&#34;https://raw.githubusercontent.com/chatwoot/chatwoot/develop/SECURITY.md&#34;&gt;SECURITY.md&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;Community? Questions? Support ?&lt;/h2&gt; &#xA;&lt;p&gt;If you need help or just want to hang out, come, say hi on our &lt;a href=&#34;https://discord.gg/cJXdrwS&#34;&gt;Discord&lt;/a&gt; server.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors ‚ú®&lt;/h2&gt; &#xA;&lt;p&gt;Thanks goes to all these &lt;a href=&#34;https://www.chatwoot.com/docs/contributors&#34;&gt;wonderful people&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/chatwoot/chatwoot/graphs/contributors&#34;&gt;&lt;img src=&#34;https://opencollective.com/chatwoot/contributors.svg?width=890&amp;amp;button=false&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Chatwoot&lt;/em&gt; ¬© 2017-2023, Chatwoot Inc - Released under the MIT License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/alignment-handbook</title>
    <updated>2023-11-13T01:25:11Z</updated>
    <id>tag:github.com,2023-11-13:/huggingface/alignment-handbook</id>
    <link href="https://github.com/huggingface/alignment-handbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Robust recipes for to align language models with human and AI preferences&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/alignment-handbook/main/assets/handbook.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/collections/alignment-handbook/handbook-v01-models-and-datasets-654e424d22e6880da5ebc015&#34; target=&#34;_blank&#34;&gt;Models &amp;amp; Datasets&lt;/a&gt; | üìÉ &lt;a href=&#34;https://arxiv.org/abs/2310.16944&#34; target=&#34;_blank&#34;&gt;Technical Report&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;The Alignment Handbook&lt;/h1&gt; &#xA;&lt;p&gt;Robust recipes to align language models with human and AI preferences.&lt;/p&gt; &#xA;&lt;h2&gt;What is this?&lt;/h2&gt; &#xA;&lt;p&gt;Just one year ago, chatbots were out of fashion and most people hadn&#39;t heard about techniques like Reinforcement Learning from Human Feedback (RLHF) to align language models with human preferences. Then, OpenAI broke the internet with ChatGPT and Meta followed suit by releasing the Llama series of language models which enabled the ML community to build their very own capable chatbots. This has led to a rich ecosystem of datasets and models that have mostly focused on teaching language models to follow instructions through supervised fine-tuning (SFT).&lt;/p&gt; &#xA;&lt;p&gt;However, we know from the &lt;a href=&#34;https://huggingface.co/papers/2203.02155&#34;&gt;InstructGPT&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/papers/2307.09288&#34;&gt;Llama2&lt;/a&gt; papers that significant gains in helpfulness and safety can be had by augmenting SFT with human (or AI) preferences. At the same time, aligning language models to a set of preferences is a fairly novel idea and there are few public resources available on how to train these models, what data to collect, and what metrics to measure for best downstream performance.&lt;/p&gt; &#xA;&lt;p&gt;The Alignment Handbook aims to fill that gap by providing the community with a series of robust training recipes that span the whole pipeline.&lt;/p&gt; &#xA;&lt;h2&gt;News üóûÔ∏è&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;November 10, 2023:&lt;/strong&gt; We release all the training code to replicate Zephyr-7b-Œ≤ ü™Å! We also release &lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceH4/no_robots&#34;&gt;No Robots&lt;/a&gt;, a brand new dataset of 10,000 instructions and demonstrations written entirely by skilled human annotators.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Links üîó&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/HuggingFaceH4/zephyr-7b-6538c6d6d5ddd1cbb1744a66&#34;&gt;Zephyr 7B models, datasets, and demos&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to navigate this project üß≠&lt;/h2&gt; &#xA;&lt;p&gt;This project is simple by design and mostly consists of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/alignment-handbook/main/scripts/&#34;&gt;&lt;code&gt;scripts&lt;/code&gt;&lt;/a&gt; to train and evaluate chat models. Each script supports distributed training of the full model weights with DeepSpeed ZeRO-3, or LoRA/QLoRA for parameter-efficient fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/alignment-handbook/main/recipes/&#34;&gt;&lt;code&gt;recipes&lt;/code&gt;&lt;/a&gt; to reproduce models like Zephyr 7B. Each recipe takes the form of a YAML file which contains all the parameters associated with a single training run.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are also working on a series of guides to explain how methods like direct preference optimization (DPO) work, along with lessons learned from gathering human preferences in practice. To get started, we recommend the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Follow the &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/alignment-handbook/main/#installation-instructions&#34;&gt;installation instructions&lt;/a&gt; to set up your environment etc.&lt;/li&gt; &#xA; &lt;li&gt;Replicate Zephyr-7b-Œ≤ by following the &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/alignment-handbook/main/recipes/zephyr-7b-beta/README.md&#34;&gt;recipe instructions&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you would like to train chat models on your own datasets, we recommend following the dataset formatting instructions &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/alignment-handbook/main/scripts/README.md#fine-tuning-on-your-datasets&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;p&gt;The initial release of the handbook will focus on the following techniques:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Supervised fine-tuning:&lt;/strong&gt; teach language models to follow instructions and tips on how to collect and curate your own training dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reward modeling:&lt;/strong&gt; teach language models to distinguish model responses according to human or AI preferences.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rejection sampling:&lt;/strong&gt; a simple, but powerful technique to boost the performance of your SFT model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Direct preference optimisation (DPO):&lt;/strong&gt; a powerful and promising alternative to PPO.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation instructions&lt;/h2&gt; &#xA;&lt;p&gt;To run the code in this project, first, create a Python virtual environment using e.g. Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n handbook python=3.10 &amp;amp;&amp;amp; conda activate handbook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, install PyTorch &lt;code&gt;v2.1.0&lt;/code&gt; - the precise version is important for reproducibility! Since this is hardware-dependent, we direct you to the &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch Installation Page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can then install the remaining package dependencies as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will also need Flash Attention 2 installed, which can be done by running: _Note: If your machine has less than 96GB of RAM and many CPU cores, reduce the MAX_JOBS., e.g. &lt;code&gt;MAX_JOBS=4 pip install flash-attn --no-build-isolation&lt;/code&gt; _&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m pip install flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, log into your Hugging Face account as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, install Git LFS so that you can push models to the Hugging Face Hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get install git-lfs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now check out the &lt;code&gt;scripts&lt;/code&gt; and &lt;code&gt;recipes&lt;/code&gt; directories for instructions on how to train some models ü™Å!&lt;/p&gt; &#xA;&lt;h2&gt;Project structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ LICENSE&#xA;‚îú‚îÄ‚îÄ Makefile                    &amp;lt;- Makefile with commands like `make style`&#xA;‚îú‚îÄ‚îÄ README.md                   &amp;lt;- The top-level README for developers using this project&#xA;‚îú‚îÄ‚îÄ chapters                    &amp;lt;- Educational content to render on hf.co/learn&#xA;‚îú‚îÄ‚îÄ recipes                     &amp;lt;- Recipe configs, accelerate configs, slurm scripts&#xA;‚îú‚îÄ‚îÄ scripts                     &amp;lt;- Scripts to train and evaluate chat models&#xA;‚îú‚îÄ‚îÄ setup.cfg                   &amp;lt;- Installation config (mostly used for configuring code quality &amp;amp; tests)&#xA;‚îú‚îÄ‚îÄ setup.py                    &amp;lt;- Makes project pip installable (pip install -e .) so `alignment` can be imported&#xA;‚îú‚îÄ‚îÄ src                         &amp;lt;- Source code for use in this project&#xA;‚îî‚îÄ‚îÄ tests                       &amp;lt;- Unit tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find the content of this repo useful in your work, please cite it as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{alignment_handbook2023,&#xA;  author = {Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Alexander M. Rush and Thomas Wolf},&#xA;  title = {The Alignment Handbook},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/huggingface/alignment-handbook}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>