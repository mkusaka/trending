<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-18T01:23:18Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ianand/spreadsheets-are-all-you-need</title>
    <updated>2024-03-18T01:23:18Z</updated>
    <id>tag:github.com,2024-03-18:/ianand/spreadsheets-are-all-you-need</id>
    <link href="https://github.com/ianand/spreadsheets-are-all-you-need" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spreadsheets-are-all-you-need&lt;/h1&gt; &#xA;&lt;p&gt;Spreadsheets-are-all-you-need implements the forward pass of GPT2 (an ancestor of ChatGPT) entirely in Excel using standard spreadsheet functions.&lt;/p&gt; &#xA;&lt;p&gt;By using a spreadsheet anyone (even non-developers) can explore and play directly with how a “real” transformer works under the hood with minimal abstractions to get in the way.&lt;/p&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://spreadsheets-are-all-you-need.ai&#34;&gt;spreadsheets-are-all-you-need.ai&lt;/a&gt; for more info&lt;/p&gt; &#xA;&lt;h1&gt;Download the sheet&lt;/h1&gt; &#xA;&lt;p&gt;The sheet is available as an xlsb (Excel binary) file in the &lt;a href=&#34;https://github.com/ianand/spreadsheets-are-all-you-need/releases/tag/v0.6.0&#34;&gt;Releases section&lt;/a&gt; of this repo. You should be able to download and run this file in Excel for Mac or PC.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NanmiCoder/MediaCrawler</title>
    <updated>2024-03-18T01:23:18Z</updated>
    <id>tag:github.com,2024-03-18:/NanmiCoder/MediaCrawler</id>
    <link href="https://github.com/NanmiCoder/MediaCrawler" rel="alternate"></link>
    <summary type="html">&lt;p&gt;小红书笔记 | 评论爬虫、抖音视频 | 评论爬虫、快手视频 | 评论爬虫、B 站视频 ｜ 评论爬虫、微博帖子 ｜ 评论爬虫&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;免责声明：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;本仓库的所有内容仅供学习和参考之用，禁止用于商业用途。任何人或组织不得将本仓库的内容用于非法用途或侵犯他人合法权益。本仓库所涉及的爬虫技术仅用于学习和研究，不得用于对其他平台进行大规模爬虫或其他非法行为。对于因使用本仓库内容而引起的任何法律责任，本仓库不承担任何责任。使用本仓库的内容即表示您同意本免责声明的所有条款和条件。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;仓库描述&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;小红书爬虫&lt;/strong&gt;，&lt;strong&gt;抖音爬虫&lt;/strong&gt;， &lt;strong&gt;快手爬虫&lt;/strong&gt;， &lt;strong&gt;B站爬虫&lt;/strong&gt;， &lt;strong&gt;微博爬虫&lt;/strong&gt;...。&lt;br&gt; 目前能抓取小红书、抖音、快手、B站、微博的视频、图片、评论、点赞、转发等信息。&lt;/p&gt; &#xA;&lt;p&gt;原理：利用&lt;a href=&#34;https://playwright.dev/&#34;&gt;playwright&lt;/a&gt;搭桥，保留登录成功后的上下文浏览器环境，通过执行JS表达式获取一些加密参数 通过使用此方式，免去了复现核心加密JS代码，逆向难度大大降低&lt;/p&gt; &#xA;&lt;p&gt;爬虫技术交流群：&lt;a href=&#34;http://qm.qq.com/cgi-bin/qm/qr?_wv=1027&amp;amp;k=NFz-oY7Pek3gpG5zbLJFHARlB8lKL94f&amp;amp;authKey=FlxIQK99Uu90wddNV5W%2FBga6T6lXU5BRqyTTc26f2P2ZK5OW%2BDhHp7MwviX%2BbrPa&amp;amp;noverify=0&amp;amp;group_code=949715256&#34;&gt;949715256&lt;/a&gt;，同时欢迎大家贡献代码提交PR&lt;/p&gt; &#xA;&lt;p&gt;视频配置教程：&lt;a href=&#34;https://space.bilibili.com/434377496/channel/series&#34;&gt;MediaCrawler视频入门教程&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;感谢下列Sponsors对本仓库赞助&lt;/h2&gt; &#xA;&lt;p&gt;成为赞助者，展示你的产品在这里，联系作者：&lt;a href=&#34;mailto:relakkes@gmail.com&#34;&gt;relakkes@gmail.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;功能列表&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;平台&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cookie 登录&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;二维码登录&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;指定创作者主页&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;关键词搜索&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;指定视频/帖子 ID 爬取&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;登录状态缓存&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;数据保存&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;IP 代理池&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;滑块验证码&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;小红书&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✕&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;抖音&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;快手&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✕&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;B 站&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✕&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;微博&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✕&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;使用方法&lt;/h2&gt; &#xA;&lt;h3&gt;创建并激活 python 虚拟环境&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 进入项目根目录&#xA;cd MediaCrawler&#xA;&#xA;# 创建虚拟环境&#xA;python -m venv venv&#xA;&#xA;# macos &amp;amp; linux 激活虚拟环境&#xA;source venv/bin/activate&#xA;&#xA;# windows 激活虚拟环境&#xA;venv\Scripts\activate&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;安装依赖库&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;安装 playwright浏览器驱动&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;playwright install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;运行爬虫程序&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 默认没有开启评论爬取模式，有需要请到配置文件中指定&#xA;# 从配置文件中读取关键词搜索相关的帖子并爬去帖子信息与评论&#xA;python main.py --platform xhs --lt qrcode --type search&#xA;&#xA;# 从配置文件中读取指定的帖子ID列表获取指定帖子的信息与评论信息&#xA;python main.py --platform xhs --lt qrcode --type detail&#xA;&#xA;# 打开对应APP扫二维码登录&#xA;  &#xA;# 其他平台爬虫使用示例, 执行下面的命令查看&#xA;python main.py --help    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;数据保存&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;支持保存到关系型数据库（Mysql、PgSQL等）&lt;/li&gt; &#xA; &lt;li&gt;支持保存到csv中（data/目录下）&lt;/li&gt; &#xA; &lt;li&gt;支持保存到json中（data/目录下）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;打赏&lt;/h2&gt; &#xA;&lt;p&gt;如果觉得项目不错的话可以打赏哦。您的支持就是我最大的动力！&lt;/p&gt; &#xA;&lt;p&gt;打赏时您可以备注名称，我会将您添加至打赏列表中。&lt;/p&gt; &#xA;&lt;p&gt; &lt;img alt=&#34;打赏-微信&#34; src=&#34;https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/static/images/wechat_pay.jpeg&#34; style=&#34;width: 200px;margin-right: 140px;&#34;&gt; &lt;img alt=&#34;打赏-支付宝&#34; src=&#34;https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/static/images/zfb_pay.jpeg&#34; style=&#34;width: 200px&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;捐赠信息&lt;/h2&gt; &#xA;&lt;p&gt;PS：如果打赏时请备注捐赠者，如有遗漏请联系我添加（有时候消息多可能会漏掉，十分抱歉）&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;捐赠者&lt;/th&gt; &#xA;   &lt;th&gt;捐赠金额&lt;/th&gt; &#xA;   &lt;th&gt;捐赠日期&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;*木&lt;/td&gt; &#xA;   &lt;td&gt;20 元&lt;/td&gt; &#xA;   &lt;td&gt;2024-03-17&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;*诚&lt;/td&gt; &#xA;   &lt;td&gt;20 元&lt;/td&gt; &#xA;   &lt;td&gt;2024-03-17&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Strem Gamer&lt;/td&gt; &#xA;   &lt;td&gt;20 元&lt;/td&gt; &#xA;   &lt;td&gt;2024-03-16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;*鑫&lt;/td&gt; &#xA;   &lt;td&gt;20 元&lt;/td&gt; &#xA;   &lt;td&gt;2024-03-14&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Yuzu&lt;/td&gt; &#xA;   &lt;td&gt;20 元&lt;/td&gt; &#xA;   &lt;td&gt;2024-03-07&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;**宁&lt;/td&gt; &#xA;   &lt;td&gt;100 元&lt;/td&gt; &#xA;   &lt;td&gt;2024-03-03&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;**媛&lt;/td&gt; &#xA;   &lt;td&gt;20 元&lt;/td&gt; &#xA;   &lt;td&gt;2024-03-03&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scarlett&lt;/td&gt; &#xA;   &lt;td&gt;20 元&lt;/td&gt; &#xA;   &lt;td&gt;2024-02-16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Asun&lt;/td&gt; &#xA;   &lt;td&gt;20 元&lt;/td&gt; &#xA;   &lt;td&gt;2024-01-30&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;何*&lt;/td&gt; &#xA;   &lt;td&gt;100 元&lt;/td&gt; &#xA;   &lt;td&gt;2024-01-21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;allen&lt;/td&gt; &#xA;   &lt;td&gt;20 元&lt;/td&gt; &#xA;   &lt;td&gt;2024-01-10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llllll&lt;/td&gt; &#xA;   &lt;td&gt;20 元&lt;/td&gt; &#xA;   &lt;td&gt;2024-01-07&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;邝*元&lt;/td&gt; &#xA;   &lt;td&gt;20 元&lt;/td&gt; &#xA;   &lt;td&gt;2023-12-29&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;50chen&lt;/td&gt; &#xA;   &lt;td&gt;50 元&lt;/td&gt; &#xA;   &lt;td&gt;2023-12-22&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;xiongot&lt;/td&gt; &#xA;   &lt;td&gt;20 元&lt;/td&gt; &#xA;   &lt;td&gt;2023-12-17&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;atom.hu&lt;/td&gt; &#xA;   &lt;td&gt;20 元&lt;/td&gt; &#xA;   &lt;td&gt;2023-12-16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;一呆&lt;/td&gt; &#xA;   &lt;td&gt;20 元&lt;/td&gt; &#xA;   &lt;td&gt;2023-12-01&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;坠落&lt;/td&gt; &#xA;   &lt;td&gt;50 元&lt;/td&gt; &#xA;   &lt;td&gt;2023-11-08&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;运行报错常见问题Q&amp;amp;A&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;遇到问题先自行搜索解决下，现在AI很火，用ChatGPT大多情况下能解决你的问题 &lt;a href=&#34;https://sider.ai/invited?c=8e03db1a973401fdf114ed9cf9f8c183&#34;&gt;免费的ChatGPT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;➡️➡️➡️ &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98.md&#34;&gt;常见问题&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;项目代码结构&lt;/h2&gt; &#xA;&lt;p&gt;➡️➡️➡️ &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/%E9%A1%B9%E7%9B%AE%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84.md&#34;&gt;项目代码结构说明&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;手机号登录说明&lt;/h2&gt; &#xA;&lt;p&gt;➡️➡️➡️ &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/%E6%89%8B%E6%9C%BA%E5%8F%B7%E7%99%BB%E5%BD%95%E8%AF%B4%E6%98%8E.md&#34;&gt;手机号登录说明&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;star 趋势图&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;如果该项目对你有帮助，star一下 ❤️❤️❤️&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#NanmiCoder/MediaCrawler&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=NanmiCoder/MediaCrawler&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;参考&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;xhs客户端 &lt;a href=&#34;https://github.com/ReaJason/xhs&#34;&gt;ReaJason的xhs仓库&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;短信转发 &lt;a href=&#34;https://github.com/pppscn/SmsForwarder&#34;&gt;参考仓库&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;内网穿透工具 &lt;a href=&#34;https://ngrok.com/docs/&#34;&gt;ngrok&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>amazon-science/chronos-forecasting</title>
    <updated>2024-03-18T01:23:18Z</updated>
    <id>tag:github.com,2024-03-18:/amazon-science/chronos-forecasting</id>
    <link href="https://github.com/amazon-science/chronos-forecasting" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chronos: Pretrained (Language) Models for Probabilistic Time Series Forecasting&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chronos: Learning the Language of Time Series&lt;/h1&gt; &#xA;&lt;p&gt;Chronos is a family of &lt;strong&gt;pretrained time series forecasting models&lt;/strong&gt; based on language model architectures. A time series is transformed into a sequence of tokens via scaling and quantization, and a language model is trained on these tokens using the cross-entropy loss. Once trained, probabilistic forecasts are obtained by sampling multiple future trajectories given the historical context. Chronos models have been trained on a large corpus of publicly available time series data, as well as synthetic data generated using Gaussian processes.&lt;/p&gt; &#xA;&lt;p&gt;For details on Chronos models, training data and procedures, and experimental results, please refer to the paper &lt;a href=&#34;https://arxiv.org/abs/2403.07815&#34;&gt;Chronos: Learning the Language of Time Series&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/amazon-science/chronos-forecasting/main/figures/main-figure.png&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;span&gt; Fig. 1: High-level depiction of Chronos. (&lt;b&gt;Left&lt;/b&gt;) The input time series is scaled and quantized to obtain a sequence of tokens. (&lt;b&gt;Center&lt;/b&gt;) The tokens are fed into a language model which may either be an encoder-decoder or a decoder-only model. The model is trained using the cross-entropy loss. (&lt;b&gt;Right&lt;/b&gt;) During inference, we autoregressively sample tokens from the model and map them back to numerical values. Multiple trajectories are sampled to obtain a predictive distribution. &lt;/span&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The models in this repository are based on the &lt;a href=&#34;https://arxiv.org/abs/1910.10683&#34;&gt;T5 architecture&lt;/a&gt;. The only difference is in the vocabulary size: Chronos-T5 models use 4096 different tokens, compared to 32128 of the original T5 models, resulting in fewer parameters.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Parameters&lt;/th&gt; &#xA;   &lt;th&gt;Based on&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/amazon/chronos-t5-tiny&#34;&gt;&lt;strong&gt;chronos-t5-tiny&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/google/t5-efficient-tiny&#34;&gt;t5-efficient-tiny&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/amazon/chronos-t5-mini&#34;&gt;&lt;strong&gt;chronos-t5-mini&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;20M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/google/t5-efficient-mini&#34;&gt;t5-efficient-mini&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/amazon/chronos-t5-small&#34;&gt;&lt;strong&gt;chronos-t5-small&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;46M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/google/t5-efficient-small&#34;&gt;t5-efficient-small&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/amazon/chronos-t5-base&#34;&gt;&lt;strong&gt;chronos-t5-base&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;200M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/google/t5-efficient-base&#34;&gt;t5-efficient-base&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/amazon/chronos-t5-large&#34;&gt;&lt;strong&gt;chronos-t5-large&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;710M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/google/t5-efficient-large&#34;&gt;t5-efficient-large&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To perform inference with Chronos models, install this package by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/amazon-science/chronos-forecasting.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A minimal example showing how to perform inference using Chronos models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt&#xA;import numpy as np&#xA;import pandas as pd&#xA;import torch&#xA;from chronos import ChronosPipeline&#xA;&#xA;pipeline = ChronosPipeline.from_pretrained(&#xA;  &#34;amazon/chronos-t5-small&#34;,&#xA;  device_map=&#34;cuda&#34;,&#xA;  torch_dtype=torch.bfloat16,&#xA;)&#xA;&#xA;df = pd.read_csv(&#34;https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv&#34;)&#xA;&#xA;# context must be either a 1D tensor, a list of 1D tensors,&#xA;# or a left-padded 2D tensor with batch as the first dimension&#xA;context = torch.tensor(df[&#34;#Passengers&#34;])&#xA;prediction_length = 12&#xA;forecast = pipeline.predict(context, prediction_length)  # shape [num_series, num_samples, prediction_length]&#xA;&#xA;# visualize the forecast&#xA;forecast_index = range(len(df), len(df) + prediction_length)&#xA;low, median, high = np.quantile(forecast[0].numpy(), [0.1, 0.5, 0.9], axis=0)&#xA;&#xA;plt.figure(figsize=(8, 4))&#xA;plt.plot(df[&#34;#Passengers&#34;], color=&#34;royalblue&#34;, label=&#34;historical data&#34;)&#xA;plt.plot(forecast_index, median, color=&#34;tomato&#34;, label=&#34;median forecast&#34;)&#xA;plt.fill_between(forecast_index, low, high, color=&#34;tomato&#34;, alpha=0.3, label=&#34;80% prediction interval&#34;)&#xA;plt.legend()&#xA;plt.grid()&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find Chronos models useful for your research, please consider citing the associated &lt;a href=&#34;https://arxiv.org/abs/2403.07815&#34;&gt;paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{ansari2024chronos,&#xA;  author  = {Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},&#xA;  title   = {Chronos: Learning the Language of Time Series},&#xA;  journal = {arXiv preprint arXiv:2403.07815},&#xA;  year    = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/amazon-science/chronos-forecasting/main/CONTRIBUTING.md#security-issue-notifications&#34;&gt;CONTRIBUTING&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache-2.0 License.&lt;/p&gt;</summary>
  </entry>
</feed>