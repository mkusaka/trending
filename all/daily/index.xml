<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-08T01:32:37Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>focus-creative-games/hybridclr</title>
    <updated>2022-07-08T01:32:37Z</updated>
    <id>tag:github.com,2022-07-08:/focus-creative-games/hybridclr</id>
    <link href="https://github.com/focus-creative-games/hybridclr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;HybridCLRæ˜¯ä¸€ä¸ªç‰¹æ€§å®Œæ•´ã€é›¶æˆæœ¬ã€é«˜æ€§èƒ½ã€ä½å†…å­˜çš„è¿‘ä¹å®Œç¾çš„Unityå…¨å¹³å°åŸç”Ÿc#çƒ­æ›´æ–¹æ¡ˆã€‚ HybridCLR is a fully featured, zero-cost, high-performance, low-memory solution for Unity&#39;s all-platform native c# hotfix&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;HybridCLR&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/focus-creative-games/HybridCLR/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;http://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;HybridCLRæ˜¯ä¸€ä¸ª&lt;strong&gt;ç‰¹æ€§å®Œæ•´ã€é›¶æˆæœ¬ã€é«˜æ€§èƒ½ã€ä½å†…å­˜&lt;/strong&gt;çš„&lt;strong&gt;è¿‘ä¹å®Œç¾&lt;/strong&gt;çš„Unityå…¨å¹³å°åŸç”Ÿc#çƒ­æ›´æ–¹æ¡ˆã€‚&lt;/p&gt; &#xA;&lt;p&gt;HybridCLRæ‰©å……äº†il2cppçš„ä»£ç ï¼Œä½¿å®ƒç”±çº¯&lt;a href=&#34;https://en.wikipedia.org/wiki/Ahead-of-time_compilation&#34;&gt;AOT&lt;/a&gt; runtimeå˜æˆâ€˜AOT+Interpreterâ€™ æ··åˆruntimeï¼Œè¿›è€ŒåŸç”Ÿæ”¯æŒåŠ¨æ€åŠ è½½assemblyï¼Œä½¿å¾—åŸºäºil2cpp backendæ‰“åŒ…çš„æ¸¸æˆä¸ä»…èƒ½åœ¨Androidå¹³å°ï¼Œä¹Ÿèƒ½åœ¨IOSã€Consolesç­‰é™åˆ¶äº†JITçš„å¹³å°ä¸Šé«˜æ•ˆåœ°ä»¥&lt;strong&gt;AOT+interpreter&lt;/strong&gt;æ··åˆæ¨¡å¼æ‰§è¡Œã€‚ä»åº•å±‚å½»åº•æ”¯æŒäº†çƒ­æ›´æ–°ã€‚&lt;/p&gt; &#xA;&lt;p&gt;HybridCLR&lt;strong&gt;å¼€åˆ›æ€§åœ°å®ç°äº† &lt;code&gt;differential hybrid dll&lt;/code&gt; æŠ€æœ¯&lt;/strong&gt;====ã€‚å³å¯ä»¥å¯¹AOT dllä»»æ„å¢åˆ æ”¹ï¼ŒHybridCLRä¼šæ™ºèƒ½åœ°è®©å˜åŒ–æˆ–è€…æ–°å¢çš„ç±»å’Œå‡½æ•°ä»¥interpreteræ¨¡å¼è¿è¡Œï¼Œä½†æœªæ”¹åŠ¨çš„ç±»å’Œå‡½æ•°ä»¥AOTæ–¹å¼è¿è¡Œï¼Œè®©çƒ­æ›´æ–°çš„æ¸¸æˆé€»è¾‘çš„è¿è¡Œæ€§èƒ½åŸºæœ¬è¾¾åˆ°åŸç”ŸAOTçš„æ°´å¹³ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;ç‰¹æ€§&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç‰¹æ€§å®Œæ•´ã€‚ è¿‘ä¹å®Œæ•´å®ç°äº†&lt;a href=&#34;https://www.ecma-international.org/publications-and-standards/standards/ecma-335/&#34;&gt;ECMA-335è§„èŒƒ&lt;/a&gt;ï¼Œé™¤äº† ä¸‹æ–‡ä¸­&#34;é™åˆ¶å’Œæ³¨æ„äº‹é¡¹&#34; ä¹‹å¤–çš„ç‰¹æ€§éƒ½æ”¯æŒã€‚&lt;/li&gt; &#xA; &lt;li&gt;é›¶å­¦ä¹ å’Œä½¿ç”¨æˆæœ¬ã€‚ HybridCLRå°†çº¯AOT runtimeå¢å¼ºä¸ºå®Œæ•´çš„runtimeï¼Œä½¿å¾—çƒ­æ›´æ–°ä»£ç ä¸AOTä»£ç æ— ç¼å·¥ä½œã€‚è„šæœ¬ç±»ä¸AOTç±»åœ¨åŒä¸€ä¸ªè¿è¡Œæ—¶å†…ï¼Œå¯ä»¥éšæ„å†™ç»§æ‰¿ã€åå°„ã€å¤šçº¿ç¨‹(volatileã€ThreadStaticã€Taskã€async)ä¹‹ç±»çš„ä»£ç ã€‚ä¸éœ€è¦é¢å¤–å†™ä»»ä½•ç‰¹æ®Šä»£ç ã€æ²¡æœ‰ä»£ç ç”Ÿæˆï¼Œä¹Ÿæ²¡æœ‰ä»€ä¹ˆç‰¹æ®Šé™åˆ¶ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æ‰§è¡Œé«˜æ•ˆã€‚å®ç°äº†ä¸€ä¸ªæå…¶é«˜æ•ˆçš„å¯„å­˜å™¨è§£é‡Šå™¨ï¼Œæ‰€æœ‰æŒ‡æ ‡éƒ½å¤§å¹…ä¼˜äºå…¶ä»–çƒ­æ›´æ–°æ–¹æ¡ˆã€‚&lt;a href=&#34;https://focus-creative-games.github.io/HybridCLR/performance/benchmark/#%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A&#34;&gt;æ€§èƒ½æµ‹è¯•æŠ¥å‘Š&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;å†…å­˜é«˜æ•ˆã€‚ çƒ­æ›´æ–°è„šæœ¬ä¸­å®šä¹‰çš„ç±»è·Ÿæ™®é€šc#ç±»å ç”¨ä¸€æ ·çš„å†…å­˜ç©ºé—´ï¼Œè¿œä¼˜äºå…¶ä»–çƒ­æ›´æ–°æ–¹æ¡ˆã€‚&lt;a href=&#34;https://focus-creative-games.github.io/HybridCLR/performance/benchmark/#%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E6%8A%A5%E5%91%8A&#34;&gt;å†…å­˜å ç”¨æŠ¥å‘Š&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;åŸç”Ÿæ”¯æŒhotfixä¿®å¤AOTéƒ¨åˆ†ä»£ç ã€‚å‡ ä¹ä¸å¢åŠ ä»»ä½•å¼€å‘å’Œè¿è¡Œå¼€é”€ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;å¼€åˆ›æ€§åœ°å®ç°äº† &lt;code&gt;differential hybrid dll&lt;/code&gt; æŠ€æœ¯&lt;/strong&gt;ã€‚å³å¯ä»¥å°†æŸä¸ªçƒ­æ›´æ–°dllå…ˆAOTå½¢å¼æ‰“åŒ…ï¼Œåé¢å¯ä»¥å¯¹è¯¥dllä»»æ„å¢åˆ æ”¹ï¼ŒHybridCLRä¼šæ™ºèƒ½åœ°è®©å˜åŒ–æˆ–è€…æ–°å¢çš„ç±»å’Œå‡½æ•°ä»¥interpreteræ¨¡å¼è¿è¡Œï¼Œä½†æœªæ”¹åŠ¨çš„ç±»å’Œå‡½æ•°ä»¥AOTæ–¹å¼è¿è¡Œã€‚è¿™æ„å‘³ç€çƒ­æ›´æ–°çš„æ¸¸æˆé€»è¾‘çš„è¿è¡Œæ€§èƒ½å°†æ¥è¿‘åŸç”ŸAOTçš„æ°´å¹³ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å·¥ä½œåŸç†&lt;/h2&gt; &#xA;&lt;p&gt;HybridCLRä»monoçš„&lt;a href=&#34;https://developpaper.com/new-net-interpreter-mono-has-arrived/&#34;&gt;hybrid mode execution&lt;/a&gt;æŠ€æœ¯ä¸­å¾—åˆ°å¯å‘ï¼Œä¸ºunityçš„il2cppä¹‹ç±»çš„AOT runtimeé¢å¤–æä¾›äº†interpreteræ¨¡å—ï¼Œå°†å®ƒä»¬ç”±çº¯AOTè¿è¡Œæ—¶æ”¹é€ ä¸º&#34;AOT + Interpreter&#34;æ··åˆè¿è¡Œæ–¹å¼ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/focus-creative-games/hybridclr/main/docs/images/architecture.png&#34; alt=&#34;icon&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;æ›´å…·ä½“åœ°è¯´ï¼ŒHybridCLRåšäº†ä»¥ä¸‹å‡ ç‚¹å·¥ä½œï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å®ç°äº†ä¸€ä¸ªé«˜æ•ˆçš„å…ƒæ•°æ®(dll)è§£æåº“&lt;/li&gt; &#xA; &lt;li&gt;æ”¹é€ äº†å…ƒæ•°æ®ç®¡ç†æ¨¡å—ï¼Œå®ç°äº†å…ƒæ•°æ®çš„åŠ¨æ€æ³¨å†Œ&lt;/li&gt; &#xA; &lt;li&gt;å®ç°äº†ä¸€ä¸ªILæŒ‡ä»¤é›†åˆ°è‡ªå®šä¹‰çš„å¯„å­˜å™¨æŒ‡ä»¤é›†çš„compiler&lt;/li&gt; &#xA; &lt;li&gt;å®ç°äº†ä¸€ä¸ªé«˜æ•ˆçš„å¯„å­˜å™¨è§£é‡Šå™¨&lt;/li&gt; &#xA; &lt;li&gt;é¢å¤–æä¾›å¤§é‡çš„instinctå‡½æ•°ï¼Œæå‡è§£é‡Šå™¨æ€§èƒ½&lt;/li&gt; &#xA; &lt;li&gt;æä¾›hotfix AOTçš„æ”¯æŒ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ä¸å…¶ä»–æµè¡Œçš„c#çƒ­æ›´æ–°æ–¹æ¡ˆçš„åŒºåˆ«&lt;/h2&gt; &#xA;&lt;h3&gt;æœ¬è´¨æ¯”è¾ƒ&lt;/h3&gt; &#xA;&lt;p&gt;HybridCLRæ˜¯åŸç”Ÿçš„c#çƒ­æ›´æ–°æ–¹æ¡ˆã€‚é€šä¿—åœ°è¯´ï¼Œil2cppç›¸å½“äºmonoçš„aotæ¨¡å—ï¼ŒHybridCLRç›¸å½“äºmonoçš„interpreteræ¨¡å—ï¼Œä¸¤è€…åˆä¸€æˆä¸ºå®Œæ•´monoã€‚HybridCLRä½¿å¾—il2cppå˜æˆä¸€ä¸ªå…¨åŠŸèƒ½çš„runtimeï¼ŒåŸç”Ÿï¼ˆå³é€šè¿‡System.Reflection.Assembly.Loadï¼‰æ”¯æŒåŠ¨æ€åŠ è½½dllï¼Œä»è€Œæ”¯æŒioså¹³å°çš„çƒ­æ›´æ–°ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æ­£å› ä¸ºHybridCLRæ˜¯åŸç”Ÿruntimeçº§åˆ«å®ç°ï¼Œçƒ­æ›´æ–°éƒ¨åˆ†çš„ç±»å‹ä¸ä¸»å·¥ç¨‹AOTéƒ¨åˆ†ç±»å‹æ˜¯å®Œå…¨ç­‰ä»·å¹¶ä¸”æ— ç¼ç»Ÿä¸€çš„ã€‚å¯ä»¥éšæ„è°ƒç”¨ã€ç»§æ‰¿ã€åå°„ã€å¤šçº¿ç¨‹ï¼Œä¸éœ€è¦ç”Ÿæˆä»£ç æˆ–è€…å†™é€‚é…å™¨ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å…¶ä»–çƒ­æ›´æ–°æ–¹æ¡ˆåˆ™æ˜¯ç‹¬ç«‹vmï¼Œä¸il2cppçš„å…³ç³»æœ¬è´¨ä¸Šç›¸å½“äºmonoä¸­åµŒå…¥luaçš„å…³ç³»ã€‚å› æ­¤ç±»å‹ç³»ç»Ÿä¸ç»Ÿä¸€ï¼Œä¸ºäº†è®©çƒ­æ›´æ–°ç±»å‹èƒ½å¤Ÿç»§æ‰¿AOTéƒ¨åˆ†ç±»å‹ï¼Œéœ€è¦å†™é€‚é…å™¨ï¼Œå¹¶ä¸”è§£é‡Šå™¨ä¸­çš„ç±»å‹ä¸èƒ½ä¸ºä¸»å·¥ç¨‹çš„ç±»å‹ç³»ç»Ÿæ‰€è¯†åˆ«ã€‚ç‰¹æ€§ä¸å®Œæ•´ã€å¼€å‘éº»çƒ¦ã€è¿è¡Œæ•ˆç‡ä½ä¸‹ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;å®é™…ä½¿ç”¨ä½“éªŒæˆ–è€…ç‰¹æ€§æ¯”è¾ƒ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;HybridCLRå­¦ä¹ å’Œä½¿ç”¨æˆæœ¬å‡ ä¹ä¸ºé›¶ã€‚HybridCLRè®©il2cppå˜æˆå…¨åŠŸèƒ½çš„runtimeï¼Œå­¦ä¹ å’Œä½¿ç”¨æˆæœ¬å‡ ä¹ä¸ºé›¶ï¼Œå‡ ä¹é›¶ä¾µå…¥æ€§ã€‚è€Œå…¶ä»–æ–¹æ¡ˆåˆ™æœ‰å¤§é‡çš„å‘å’Œéœ€è¦è§„é¿çš„è§„åˆ™ï¼Œå­¦ä¹ å’Œä½¿ç”¨æˆæœ¬ï¼Œéœ€è¦å¯¹åŸé¡¹ç›®ä½œå¤§é‡æ”¹é€ ã€‚&lt;/li&gt; &#xA; &lt;li&gt;HybridCLRå¯ä»¥ä½¿ç”¨æ‰€æœ‰c#çš„ç‰¹æ€§ã€‚è€Œå…¶ä»–æ–¹æ¡ˆå¾€å¾€æœ‰å¤§é‡çš„é™åˆ¶ã€‚&lt;/li&gt; &#xA; &lt;li&gt;HybridCLRä¸­å¯ä»¥ç›´æ¥æ”¯æŒä½¿ç”¨å’Œç»§æ‰¿ä¸»å·¥ç¨‹ä¸­çš„ç±»å‹ã€‚å…¶ä»–æ–¹æ¡ˆè¦å†™é€‚é…å™¨æˆ–è€…ç”Ÿæˆä»£ç ã€‚&lt;/li&gt; &#xA; &lt;li&gt;HybridCLRä¸­çƒ­æ›´æ–°éƒ¨åˆ†å…ƒæ•°æ®ä¸AOTå…ƒæ•°æ®æ— ç¼ç»Ÿä¸€ã€‚åƒåå°„ä»£ç èƒ½å¤Ÿæ­£å¸¸å·¥ä½œçš„ï¼ŒAOTéƒ¨åˆ†ä¹Ÿå¯ä»¥é€šè¿‡æ ‡å‡†Reflectionæ¥å£åˆ›å»ºå‡ºçƒ­æ›´æ–°å¯¹è±¡ã€‚å…¶ä»–æ–¹æ¡ˆåšä¸åˆ°ã€‚&lt;/li&gt; &#xA; &lt;li&gt;HybridCLRå¯¹å¤šçº¿ç¨‹æ”¯æŒè‰¯å¥½ã€‚åƒå¤šçº¿ç¨‹ã€ThreadStaticã€asyncç­‰ç­‰ç‰¹æ€§éƒ½æ˜¯HybridCLRç›´æ¥æ”¯æŒï¼Œå…¶ä»–æ–¹æ¡ˆé™¤äº†asyncç‰¹æ€§å¤–å‡éš¾ä»¥æ”¯æŒã€‚&lt;/li&gt; &#xA; &lt;li&gt;HybridCLRä¸­Unityå·¥ä½œæµä¸åŸç”Ÿå‡ ä¹å®Œå…¨ç›¸åŒã€‚HybridCLRä¸­çƒ­æ›´æ–°MonoBehaviourå¯ä»¥ç›´æ¥æŒ‚è½½åœ¨çƒ­æ›´æ–°èµ„æºä¸Šï¼Œå¹¶ä¸”æ­£ç¡®å·¥ä½œã€‚å…¶ä»–æ–¹æ¡ˆä¸è¡Œã€‚&lt;/li&gt; &#xA; &lt;li&gt;HybridCLRå…¼å®¹æ€§æé«˜ã€‚å„ç§ç¬¬ä¸‰æ–¹åº“åªè¦åœ¨il2cppä¸‹èƒ½å·¥ä½œï¼Œåœ¨HybridCLRä¸‹ä¹Ÿèƒ½æ­£å¸¸å·¥ä½œã€‚å…¶ä»–æ–¹æ¡ˆå¾€å¾€è¦å¤§é‡é­”æ”¹æºç ã€‚&lt;/li&gt; &#xA; &lt;li&gt;HybridCLRå†…å­˜æ•ˆç‡æé«˜ã€‚HybridCLRä¸­çƒ­æ›´æ–°ç±»å‹ä¸ä¸»å·¥ç¨‹çš„AOTç±»å‹å®Œå…¨ç­‰ä»·ï¼Œå ç”¨ä¸€æ ·å¤šçš„ç©ºé—´ã€‚å…¶ä»–æ–¹æ¡ˆçš„åŒç­‰ç±»å‹åˆ™æ˜¯å‡ç±»å‹ï¼Œä¸ä»…ä¸èƒ½è¢«runtimeè¯†åˆ«ï¼Œè¿˜å¤šå äº†æ•°å€ç©ºé—´ã€‚&lt;/li&gt; &#xA; &lt;li&gt;HybridCLRæ‰§è¡Œæ•ˆç‡é«˜ã€‚HybridCLRä¸­çƒ­æ›´æ–°éƒ¨åˆ†ä¸ä¸»å·¥ç¨‹AOTéƒ¨åˆ†äº¤äº’å±äºil2cppå†…éƒ¨äº¤äº’ï¼Œæ•ˆç‡æé«˜ã€‚è€Œå…¶ä»–æ–¹æ¡ˆåˆ™æ˜¯ç‹¬ç«‹è™šæ‹Ÿæœºä¸il2cppä¹‹é—´çš„æ•ˆç‡ï¼Œä¸ä»…äº¤äº’éº»çƒ¦è¿˜æ•ˆç‡ä½ä¸‹ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æ–‡æ¡£&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://focus-creative-games.github.io/&#34;&gt;æ–‡æ¡£ç«™&lt;/a&gt;ï¼Œ&lt;strong&gt;æ¨èä½¿ç”¨&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://focus-creative-games.github.io/HybridCLR/faq/&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://focus-creative-games.github.io/HybridCLR/performance/limit/&#34;&gt;é™åˆ¶å’Œæ³¨æ„äº‹é¡¹&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/focus-creative-games/HybridCLR_trial&#34;&gt;ç¤ºä¾‹é¡¹ç›®&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/column/c_1489549396035870720&#34;&gt;çŸ¥ä¹ä¸“æ &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://focus-creative-games.github.io/HybridCLR/donate/&#34;&gt;==&amp;gt;è‡´è°¢åå•&amp;lt;==&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç¨³å®šæ€§çŠ¶å†µ&lt;/h2&gt; &#xA;&lt;p&gt;=== &lt;strong&gt;åº†ç¥äº 2021.6.7 ç¬¬ä¸€æ¬¾ä½¿ç”¨HybridCLRçš„androidå’ŒiOSåŒç«¯ä¼‘é—²æ¸¸æˆæ­£å¼ä¸Šçº¿&lt;/strong&gt; ===ï¼Œ7æœˆä»½è¿˜æœ‰å‡ æ¬¾ä¸­é‡æ¸¸æˆä¸Šçº¿æˆ–è€…å¯¹å¤–æµ‹è¯•ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æŠ€æœ¯è¯„ä¼°ä¸Šç›®å‰ç¨³å®šæ€§å¤„äºBetaç‰ˆæœ¬ã€‚ç”±äºHybridCLRæŠ€æœ¯åŸç†çš„å…ˆè¿›æ€§ï¼Œbugæœ¬è´¨ä¸Šä¸å¤šï¼Œç¨³å®šå¾—éå¸¸å¿«ã€‚&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç›®å‰PCã€Androidã€iOS å·²è·‘é€šæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œå¯ç¨³å®šä½“éªŒä½¿ç”¨ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æµ‹è¯•äº†æ¸¸æˆå¸¸ç”¨åº“å’Œæ¡†æ¶çš„å…¼å®¹æ€§ï¼Œå…¼å®¹æ€§è‰¯å¥½ã€‚åªè¦èƒ½åœ¨il2cpp backendä¸‹å·¥ä½œçš„åº“éƒ½å¯ä»¥åœ¨HybridCLRä¸‹æ­£å¸¸å·¥ä½œã€‚ç”šè‡³é‚£äº›ä¸il2cppå› ä¸ºAOTé—®é¢˜ä¸å…¼å®¹çš„åº“ï¼Œç°åœ¨å› ä¸ºHybridCLRå¯¹il2cppçš„èƒ½åŠ›æ‰©å……ï¼Œåè€Œå¯ä»¥æ­£å¸¸è¿è¡Œäº†ã€‚å…·ä½“å‚è§ &lt;a href=&#34;https://focus-creative-games.github.io/HybridCLR/performance/compatible/&#34;&gt;å…¼å®¹æ€§æŠ¥å‘Š&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;å·²ç»æœ‰å‡ åä¸ªå¤§ä¸­å‹æ¸¸æˆé¡¹ç›®è¾ƒå®Œæ•´åœ°æ¥å…¥HybridCLRï¼Œå¹¶ä¸”å…¶ä¸­ä¸€äº›åœ¨ç´§é”£å¯†é¼“ä½œä¸Šçº¿å‰æµ‹è¯•ã€‚å…·ä½“å‚è§æ”¶é›†çš„ä¸€äº› &lt;a href=&#34;https://focus-creative-games.github.io/HybridCLR/ref_project/&#34;&gt;å®Œæ•´æ¥å…¥çš„å•†ä¸šé¡¹ç›®åˆ—è¡¨&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æ”¯æŒä¸è”ç³»&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å¼€å‘äº¤æµï¼Œæ¬¢è¿åŠ QQç¾¤æˆ–é‚®ä»¶è”ç³» &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;QQç¾¤ï¼š651188171 HybridCLRæŠ€æœ¯äº¤æµç¾¤ &lt;strong&gt;(å®˜æ–¹ä¸»ç¾¤)&lt;/strong&gt;ã€‚å¯ä»¥åé¦ˆbugï¼Œä½†&lt;strong&gt;ä¸è¦åœ¨ç¾¤é‡Œå’¨è¯¢åŸºç¡€ä½¿ç”¨é—®é¢˜&lt;/strong&gt;ã€‚&lt;/li&gt; &#xA;   &lt;li&gt;QQç¾¤ï¼š428404198 HybridCLRä½¿ç”¨ç–‘éš¾å’¨è¯¢ç¾¤ &lt;strong&gt;(æ–°æ‰‹ç¾¤)&lt;/strong&gt;ã€‚æ–°æ‰‹ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œéƒ½å¯ä»¥åœ¨ç¾¤é‡Œå’¨è¯¢ã€‚&lt;/li&gt; &#xA;   &lt;li&gt;é‚®ç®±ï¼štaojingjian#gmail.com&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;RoadMap&lt;/h2&gt; &#xA;&lt;p&gt;HybridCLRè™½ç„¶ä¸il2cppç›¸å…³ï¼Œä½†ç»å¤§å¤šæ•°æ ¸å¿ƒä»£ç ç‹¬ç«‹äºil2cppï¼Œå¾ˆå®¹æ˜“ç§»æ¤ï¼ˆé¢„è®¡ä¸€ä¸ªæœˆï¼‰åˆ°å…¶ä»–ä¸æ”¯æŒAOT+Interpreterçš„CLRå¹³å°ã€‚æ— è®ºunityå¦‚ä½•ç‰ˆæœ¬å˜è¿ï¼Œå“ªæ€•åºŸå¼ƒäº†il2cppæ”¹ç”¨.net 6+ï¼ŒHybridCLRä¼šæŒç»­è·Ÿè¿›ï¼Œç¨³å®šåœ°æä¾›è·¨å¹³å°çš„CLRçƒ­æ›´æ–°æœåŠ¡ï¼Œç›´è‡³æŸå¤©.netå®˜æ–¹ç›´æ¥æ”¯æŒAOT+Interpreterï¼Œåˆ™HybridCLRå®Œæˆå…¶å†å²ä½¿å‘½ã€‚&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ”¯æŒUnity 2019ã€2020å’Œ2021ç³»åˆ—ç‰ˆæœ¬ (2022.6 -)&lt;/li&gt; &#xA; &lt;li&gt;æ”¯æŒ32ä½ (2022.6 - 2022.7)&lt;/li&gt; &#xA; &lt;li&gt;æŒ‡ä»¤ä¼˜åŒ–ï¼Œç¼–è¯‘åæŒ‡ä»¤æ•°å‡å°‘åˆ°åŸæ¥1/4-1/2ï¼ŒåŸºç¡€æŒ‡ä»¤å’Œå¤§å¤šæ•°å¯¹è±¡æ¨¡å‹æŒ‡ä»¤æœ‰100%-300%çš„æ€§èƒ½æå‡ã€‚ (2022.7 -)&lt;/li&gt; &#xA; &lt;li&gt;æ”¯æŒå¢é‡å¼gc (2022.8 -)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;license&lt;/h2&gt; &#xA;&lt;p&gt;HybridCLR is licensed under the &lt;a href=&#34;https://github.com/focus-creative-games/HybridCLR/raw/main/LICENSE&#34;&gt;MIT&lt;/a&gt; license&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>babysor/MockingBird</title>
    <updated>2022-07-08T01:32:37Z</updated>
    <id>tag:github.com,2022-07-08:/babysor/MockingBird</id>
    <link href="https://github.com/babysor/MockingBird" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ğŸš€AIæ‹Ÿå£°: 5ç§’å†…å…‹éš†æ‚¨çš„å£°éŸ³å¹¶ç”Ÿæˆä»»æ„è¯­éŸ³å†…å®¹ Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg&#34; alt=&#34;mockingbird&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://choosealicense.com/licenses/mit/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?style=flat&#34; alt=&#34;MIT License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/babysor/MockingBird/main/README-CN.md&#34;&gt;ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;ğŸŒ &lt;strong&gt;Chinese&lt;/strong&gt; supported mandarin and tested with multiple datasets: aidatatang_200zh, magicdata, aishell3, data_aishell, and etc.&lt;/p&gt; &#xA;&lt;p&gt;ğŸ¤© &lt;strong&gt;PyTorch&lt;/strong&gt; worked for pytorch, tested in version of 1.9.0(latest in August 2021), with GPU Tesla T4 and GTX 2060&lt;/p&gt; &#xA;&lt;p&gt;ğŸŒ &lt;strong&gt;Windows + Linux&lt;/strong&gt; run in both Windows OS and linux OS (even in M1 MACOS)&lt;/p&gt; &#xA;&lt;p&gt;ğŸ¤© &lt;strong&gt;Easy &amp;amp; Awesome&lt;/strong&gt; effect with only newly-trained synthesizer, by reusing the pretrained encoder/vocoder&lt;/p&gt; &#xA;&lt;p&gt;ğŸŒ &lt;strong&gt;Webserver Ready&lt;/strong&gt; to serve your result with remote calling&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV17Q4y1B7mY/&#34;&gt;DEMO VIDEO&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;Ongoing Works(Helps Needed)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Major upgrade on GUI/Client and unifying web and toolbox [X] Init framework &lt;code&gt;./mkgui&lt;/code&gt; and &lt;a href=&#34;https://vaj2fgg8yn.feishu.cn/docs/doccnvotLWylBub8VJIjKzoEaee&#34;&gt;tech design&lt;/a&gt; [X] Add demo part of Voice Cloning and Conversion [X] Add preprocessing and training for Voice Conversion [ ] Add preprocessing and training for Encoder/Synthesizer/Vocoder&lt;/li&gt; &#xA; &lt;li&gt;Major upgrade on model backend based on ESPnet2(not yet started)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;1. Install Requirements&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Follow the original repo to test if you got all environment ready. **Python 3.7 or higher ** is needed to run the toolbox.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you get an &lt;code&gt;ERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu102 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2 )&lt;/code&gt; This error is probably due to a low version of python, try using 3.9 and it will install successfully&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://ffmpeg.org/download.html#get-packages&#34;&gt;ffmpeg&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to install the remaining necessary packages.&lt;/li&gt; &#xA; &lt;li&gt;Install webrtcvad &lt;code&gt;pip install webrtcvad-wheels&lt;/code&gt;(If you need)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note that we are using the pretrained encoder/vocoder but synthesizer, since the original model is incompatible with the Chinese sympols. It means the demo_cli is not working at this moment.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;2. Prepare your models&lt;/h3&gt; &#xA;&lt;p&gt;You can either train your models or use existing ones:&lt;/p&gt; &#xA;&lt;h4&gt;2.1 Train encoder with your dataset (Optional)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Preprocess with the audios and the mel spectrograms: &lt;code&gt;python encoder_preprocess.py &amp;lt;datasets_root&amp;gt;&lt;/code&gt; Allowing parameter &lt;code&gt;--dataset {dataset}&lt;/code&gt; to support the datasets you want to preprocess. Only the train set of these datasets will be used. Possible names: librispeech_other, voxceleb1, voxceleb2. Use comma to sperate multiple datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the encoder: &lt;code&gt;python encoder_train.py my_run &amp;lt;datasets_root&amp;gt;/SV2TTS/encoder&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For training, the encoder uses visdom. You can disable it with &lt;code&gt;--no_visdom&lt;/code&gt;, but it&#39;s nice to have. Run &#34;visdom&#34; in a separate CLI/process to start your visdom server.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2.2 Train synthesizer with your dataset&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download dataset and unzip: make sure you can access all .wav in folder&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Preprocess with the audios and the mel spectrograms: &lt;code&gt;python pre.py &amp;lt;datasets_root&amp;gt;&lt;/code&gt; Allowing parameter &lt;code&gt;--dataset {dataset}&lt;/code&gt; to support aidatatang_200zh, magicdata, aishell3, data_aishell, etc.If this parameter is not passed, the default dataset will be aidatatang_200zh.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the synthesizer: &lt;code&gt;python synthesizer_train.py mandarin &amp;lt;datasets_root&amp;gt;/SV2TTS/synthesizer&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Go to next step when you see attention line show and loss meet your need in training folder &lt;em&gt;synthesizer/saved_models/&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2.3 Use pretrained model of synthesizer&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Thanks to the community, some models will be shared:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;author&lt;/th&gt; &#xA;   &lt;th&gt;Download link&lt;/th&gt; &#xA;   &lt;th&gt;Preview Video&lt;/th&gt; &#xA;   &lt;th&gt;Info&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@author&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g&#34;&gt;https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g&#34;&gt;Baidu&lt;/a&gt; 4j5d&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;75k steps trained by multiple datasets&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@author&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw&#34;&gt;https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw&#34;&gt;Baidu&lt;/a&gt; codeï¼šom7f&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;25k steps trained by multiple datasets, only works under version 0.0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@FawenYo&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1H-YGOUHpmqKxJ9FRc6vAjPuqQki24UbC/view?usp=sharing&#34;&gt;https://drive.google.com/file/d/1H-YGOUHpmqKxJ9FRc6vAjPuqQki24UbC/view?usp=sharing&lt;/a&gt; &lt;a href=&#34;https://u.teknik.io/AYxWf.pt&#34;&gt;https://u.teknik.io/AYxWf.pt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/babysor/MockingBird/wiki/audio/self_test.mp3&#34;&gt;input&lt;/a&gt; &lt;a href=&#34;https://github.com/babysor/MockingBird/wiki/audio/export.wav&#34;&gt;output&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;200k steps with local accent of Taiwan, only works under version 0.0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@miven&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ&#34;&gt;https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ&lt;/a&gt; code: 2021 &lt;a href=&#34;https://www.aliyundrive.com/s/AwPsbo8mcSP&#34;&gt;https://www.aliyundrive.com/s/AwPsbo8mcSP&lt;/a&gt; code: z2m0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1uh411B7AD/&#34;&gt;https://www.bilibili.com/video/BV1uh411B7AD/&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;only works under version 0.0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;2.4 Train vocoder (Optional)&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;note: vocoder has little difference in effect, so you may not need to train a new one.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Preprocess the data: &lt;code&gt;python vocoder_preprocess.py &amp;lt;datasets_root&amp;gt; -m &amp;lt;synthesizer_model_path&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; replace with your dataset rootï¼Œ&lt;code&gt;&amp;lt;synthesizer_model_path&amp;gt;&lt;/code&gt;replace with directory of your best trained models of sythensizer, e.g. &lt;em&gt;sythensizer\saved_mode\xxx&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the wavernn vocoder: &lt;code&gt;python vocoder_train.py mandarin &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the hifigan vocoder &lt;code&gt;python vocoder_train.py mandarin &amp;lt;datasets_root&amp;gt; hifigan&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. Launch&lt;/h3&gt; &#xA;&lt;h4&gt;3.1 Using the web server&lt;/h4&gt; &#xA;&lt;p&gt;You can then try to run:&lt;code&gt;python web.py&lt;/code&gt; and open it in browser, default as &lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.2 Using the Toolbox&lt;/h4&gt; &#xA;&lt;p&gt;You can then try the toolbox: &lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.3 Using the command line&lt;/h4&gt; &#xA;&lt;p&gt;You can then try the command: &lt;code&gt;python gen_voice.py &amp;lt;text_file.txt&amp;gt; your_wav_file.wav&lt;/code&gt; you may need to install cn2an by &#34;pip install cn2an&#34; for better digital number result.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This repository is forked from &lt;a href=&#34;https://github.com/CorentinJ/Real-Time-Voice-Cloning&#34;&gt;Real-Time-Voice-Cloning&lt;/a&gt; which only support English.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;   &lt;th&gt;Designation&lt;/th&gt; &#xA;   &lt;th&gt;Title&lt;/th&gt; &#xA;   &lt;th&gt;Implementation source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1803.09017&#34;&gt;1803.09017&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GlobalStyleToken (synthesizer)&lt;/td&gt; &#xA;   &lt;td&gt;Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;2010.05646&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HiFi-GAN (vocoder)&lt;/td&gt; &#xA;   &lt;td&gt;Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.02297&#34;&gt;2106.02297&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fre-GAN (vocoder)&lt;/td&gt; &#xA;   &lt;td&gt;Fre-GAN: Adversarial Frequency-consistent Audio Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1806.04558.pdf&#34;&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1802.08435.pdf&#34;&gt;1802.08435&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WaveRNN (vocoder)&lt;/td&gt; &#xA;   &lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fatchord/WaveRNN&#34;&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.10135.pdf&#34;&gt;1703.10135&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron (synthesizer)&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fatchord/WaveRNN&#34;&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.10467.pdf&#34;&gt;1710.10467&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GE2E (encoder)&lt;/td&gt; &#xA;   &lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;F Q&amp;amp;A&lt;/h2&gt; &#xA;&lt;h4&gt;1.Where can I download the dataset?&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Original Source&lt;/th&gt; &#xA;   &lt;th&gt;Alternative Sources&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;aidatatang_200zh&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.openslr.org/62/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/110A11KZoVe7vy6kXlLb6zVPLb_J91I_t/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;magicdata&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.openslr.org/68/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1g5bWRUSNH68ycC6eNvtwh07nX3QhOOlo/view?usp=sharing&#34;&gt;Google Drive (Dev set)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;aishell3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.openslr.org/93/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1shYp_o4Z0X0cZSKQDtFirct2luFUwKzZ/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;data_aishell&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.openslr.org/33/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;After unzip aidatatang_200zh, you need to unzip all the files under &lt;code&gt;aidatatang_200zh\corpus\train&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2.What is&lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt;?&lt;/h4&gt; &#xA;&lt;p&gt;If the dataset path is &lt;code&gt;D:\data\aidatatang_200zh&lt;/code&gt;,then &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is&lt;code&gt;D:\data&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.Not enough VRAM&lt;/h4&gt; &#xA;&lt;p&gt;Train the synthesizerï¼šadjust the batch_size in &lt;code&gt;synthesizer/hparams.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;//Before&#xA;tts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule&#xA;                (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)&#xA;                (2,  2e-4,  80_000,  12),   #&#xA;                (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames&#xA;                (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)&#xA;                (2,  1e-5, 640_000,  12)],  # lr = learning rate&#xA;//After&#xA;tts_schedule = [(2,  1e-3,  20_000,  8),   # Progressive training schedule&#xA;                (2,  5e-4,  40_000,  8),   # (r, lr, step, batch_size)&#xA;                (2,  2e-4,  80_000,  8),   #&#xA;                (2,  1e-4, 160_000,  8),   # r = reduction factor (# of mel frames&#xA;                (2,  3e-5, 320_000,  8),   #     synthesized for each decoder iteration)&#xA;                (2,  1e-5, 640_000,  8)],  # lr = learning rate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train Vocoder-Preprocess the dataï¼šadjust the batch_size in &lt;code&gt;synthesizer/hparams.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;//Before&#xA;### Data Preprocessing&#xA;        max_mel_frames = 900,&#xA;        rescale = True,&#xA;        rescaling_max = 0.9,&#xA;        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.&#xA;//After&#xA;### Data Preprocessing&#xA;        max_mel_frames = 900,&#xA;        rescale = True,&#xA;        rescaling_max = 0.9,&#xA;        synthesis_batch_size = 8,                  # For vocoder preprocessing and inference.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train Vocoder-Train the vocoderï¼šadjust the batch_size in &lt;code&gt;vocoder/wavernn/hparams.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;//Before&#xA;# Training&#xA;voc_batch_size = 100&#xA;voc_lr = 1e-4&#xA;voc_gen_at_checkpoint = 5&#xA;voc_pad = 2&#xA;&#xA;//After&#xA;# Training&#xA;voc_batch_size = 6&#xA;voc_lr = 1e-4&#xA;voc_gen_at_checkpoint = 5&#xA;voc_pad =2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;4.If it happens &lt;code&gt;RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Please refer to issue &lt;a href=&#34;https://github.com/babysor/MockingBird/issues/37&#34;&gt;#37&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5. How to improve CPU and GPU occupancy rate?&lt;/h4&gt; &#xA;&lt;p&gt;Adjust the batch_size as appropriate to improve&lt;/p&gt; &#xA;&lt;h4&gt;6. What if it happens &lt;code&gt;the page file is too small to complete the operation&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Please refer to this &lt;a href=&#34;https://www.youtube.com/watch?v=Oh6dga-Oy10&amp;amp;ab_channel=CodeProf&#34;&gt;video&lt;/a&gt; and change the virtual memory to 100G (102400), for example : When the file is placed in the D disk, the virtual memory of the D disk is changed.&lt;/p&gt; &#xA;&lt;h4&gt;7. When should I stop during training?&lt;/h4&gt; &#xA;&lt;p&gt;FYI, my attention came after 18k steps and loss became lower than 0.4 after 50k steps. &lt;img src=&#34;https://user-images.githubusercontent.com/7423248/128587252-f669f05a-f411-4811-8784-222156ea5e9d.png&#34; alt=&#34;attention_step_20500_sample_1&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/7423248/128587255-4945faa0-5517-46ea-b173-928eff999330.png&#34; alt=&#34;step-135500-mel-spectrogram_sample_1&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>asdfugil/haxx</title>
    <updated>2022-07-08T01:32:37Z</updated>
    <id>tag:github.com,2022-07-08:/asdfugil/haxx</id>
    <link href="https://github.com/asdfugil/haxx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Untethered + Unsandboxed code execution haxx as root on iOS 14 - iOS 14.8.1.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;haxx&lt;/h1&gt; &#xA;&lt;p&gt;Untethered + Unsandboxed code execution haxx as root on iOS 14 - iOS 14.8.1.&lt;/p&gt; &#xA;&lt;p&gt;Based on &lt;a href=&#34;https://github.com/zhuowei/CoreTrustDemo&#34;&gt;CoreTrustDemo&lt;/a&gt;, also please note that certificates are not copyrightable.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Note: requires macOS + existing jailbreak&lt;/p&gt; &#xA;&lt;h3&gt;Get up and running&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;On your mac import dev_certificate.p12 into the keychain, and the password is &lt;code&gt;password&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Modify haxx.c to include your own code (if you need it).&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;make&lt;/code&gt; to build&lt;/li&gt; &#xA; &lt;li&gt;On the device, Copy &lt;code&gt;/System/Library/PrivateFrameworks/CoreAnalytics.framework/Support/analyticsd&lt;/code&gt; to &lt;code&gt;/System/Library/PrivateFrameworks/CoreAnalytics.framework/Support/analyticsd.back&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Then replace &lt;code&gt;/System/Library/PrivateFrameworks/CoreAnalytics.framework/Support/analyticsd&lt;/code&gt; with &lt;code&gt;/usr/bin/fileproviderctl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create the &lt;code&gt;/private/var/haxx&lt;/code&gt; directory, mode should be 0777&lt;/li&gt; &#xA; &lt;li&gt;Copy &lt;code&gt;fileproviderctl_internal&lt;/code&gt; and &lt;code&gt;haxx&lt;/code&gt; generated from the build to &lt;code&gt;/usr/local/bin&lt;/code&gt; on the device, mode should be 0755.&lt;/li&gt; &#xA; &lt;li&gt;Profit.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Fixing fileproviderctl&lt;/h3&gt; &#xA;&lt;p&gt;After doing the above steps, &lt;code&gt;fileproviderctl&lt;/code&gt; will be broken, to fix it do the following steps&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Grab a copy of &lt;code&gt;/usr/bin/fileproviderctl&lt;/code&gt; on your device to your mac&lt;/li&gt; &#xA; &lt;li&gt;Patch the binary with GNU sed: &lt;code&gt;gsed -i &#39;s|/usr/local/bin/fileproviderctl_internal|/usr/local/bin/fileproviderctl_XXXXXXXX|g&#39; fileproviderctl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Resign it: &lt;code&gt;codesign -s &#34;Worth Doing Badly iPhone OS Application Signing&#34; --preserve-metadata=entitlements --force fileproviderctl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Put the fixed binary back onto your device.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Removal&lt;/h3&gt; &#xA;&lt;p&gt;To remove the installation, do the following steps&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copy &lt;code&gt;/System/Library/PrivateFrameworks/CoreAnalytics.framework/Support/analyticsd&lt;/code&gt; to &lt;code&gt;/usr/bin/fileproviderctl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Move &lt;code&gt;/System/Library/PrivateFrameworks/CoreAnalytics.framework/Support/analyticsd.back&lt;/code&gt; to &lt;code&gt;/System/Library/PrivateFrameworks/CoreAnalytics.framework/Support/analyticsd&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Delete &lt;code&gt;/var/haxx&lt;/code&gt;, &lt;code&gt;/usr/local/bin/fileproviderctl_internal&lt;/code&gt; as well as &lt;code&gt;/usr/local/bin/haxx&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>