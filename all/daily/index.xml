<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-24T01:27:38Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>SJTU-IPADS/PowerInfer</title>
    <updated>2023-12-24T01:27:38Z</updated>
    <id>tag:github.com,2023-12-24:/SJTU-IPADS/PowerInfer</id>
    <link href="https://github.com/SJTU-IPADS/PowerInfer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High-speed Large Language Model Serving on PCs with Consumer-grade GPUs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU&lt;/h1&gt; &#xA;&lt;h2&gt;TL;DR&lt;/h2&gt; &#xA;&lt;p&gt;PowerInfer is a CPU/GPU LLM inference engine leveraging &lt;strong&gt;activation locality&lt;/strong&gt; for your device.&lt;/p&gt; &#xA;&lt;h2&gt;Demo 🔥&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SJTU-IPADS/PowerInfer/assets/34213478/fe441a42-5fce-448b-a3e5-ea4abb43ba23&#34;&gt;https://github.com/SJTU-IPADS/PowerInfer/assets/34213478/fe441a42-5fce-448b-a3e5-ea4abb43ba23&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PowerInfer v.s. llama.cpp on a single RTX 4090(24G) running Falcon(ReLU)-40B-FP16 with a 11x speedup!&lt;/p&gt; &#xA;&lt;p&gt;&lt;sub&gt;Both PowerInfer and llama.cpp were running on the same hardware and fully utilized VRAM on RTX 4090.&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;We introduce PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high &lt;strong&gt;locality&lt;/strong&gt; inherent in LLM inference, characterized by a power-law distribution in neuron activation.&lt;/p&gt; &#xA;&lt;p&gt;This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity.&lt;/p&gt; &#xA;&lt;p&gt;Evaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU, only 18% lower than that achieved by a top-tier server-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;PowerInfer is a high-speed and easy-to-use inference engine for deploying LLMs locally.&lt;/p&gt; &#xA;&lt;p&gt;PowerInfer is fast with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Locality-centric design&lt;/strong&gt;: Utilizes sparse activation and &#39;hot&#39;/&#39;cold&#39; neuron concept for efficient LLM inference, ensuring high speed with lower resource demands.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hybrid CPU/GPU Utilization&lt;/strong&gt;: Seamlessly integrates memory/computation capabilities of CPU and GPU for a balanced workload and faster processing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;PowerInfer is flexible and easy to use with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy Integration&lt;/strong&gt;: Compatible with popular &lt;a href=&#34;https://huggingface.co/SparseLLM&#34;&gt;ReLU-sparse models&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local Deployment Ease&lt;/strong&gt;: Designed and deeply optimized for local deployment on consumer-grade hardware, enabling low-latency LLM inference and serving on a single GPU.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Backward Compatibility&lt;/strong&gt;: While distinct from llama.cpp, you can make use of most of &lt;code&gt;examples/&lt;/code&gt; the same way as llama.cpp such as server and batched generation. PowerInfer also supports inference with llama.cpp&#39;s model weights for compatibility purposes, but there will be no performance gain.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can use these models with PowerInfer today:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Falcon-40B&lt;/li&gt; &#xA; &lt;li&gt;Llama2 family&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We have tested PowerInfer on the following platforms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;x86-64 CPU (with AVX2 instructions) on Linux&lt;/li&gt; &#xA; &lt;li&gt;x86-64 CPU and NVIDIA GPU on Linux&lt;/li&gt; &#xA; &lt;li&gt;Apple M Chips on macOS (As we do not optimize for Mac, the performance improvement is not significant now.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And new features coming soon:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mistral-7B model&lt;/li&gt; &#xA; &lt;li&gt;Metal backend for sparse inference on macOS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SJTU-IPADS/PowerInfer/main/#setup-and-installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SJTU-IPADS/PowerInfer/main/#model-weights&#34;&gt;Model Weights&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup and Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Get the Code&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/SJTU-IPADS/PowerInfer&#xA;cd PowerInfer&#xA;pip install -r requirements.txt # install Python helpers&#39; dependencies&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;p&gt;In order to build PowerInfer you have two different options. These commands are supposed to be run from the root directory of the project.&lt;/p&gt; &#xA;&lt;p&gt;Using &lt;code&gt;CMake&lt;/code&gt;(3.13+) on Linux or macOS:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have an NVIDIA GPU:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake -S . -B build -DLLAMA_CUBLAS=ON&#xA;cmake --build build --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you just CPU:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake -S . -B build&#xA;cmake --build build --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model Weights&lt;/h2&gt; &#xA;&lt;p&gt;PowerInfer models are stored in a special format called &lt;em&gt;PowerInfer GGUF&lt;/em&gt; based on GGUF format, consisting of both LLM weights and predictor weights.&lt;/p&gt; &#xA;&lt;h3&gt;Download PowerInfer GGUF via Hugging Face&lt;/h3&gt; &#xA;&lt;p&gt;You can obtain PowerInfer GGUF weights at &lt;code&gt;*.powerinfer.gguf&lt;/code&gt; as well as profiled model activation statistics for &#39;hot&#39;-neuron offloading from each Hugging Face repo below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Base Model&lt;/th&gt; &#xA;   &lt;th&gt;PowerInfer GGUF&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA(ReLU)-2-7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF&#34;&gt;PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA(ReLU)-2-13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluLLaMA-13B-PowerInfer-GGUF&#34;&gt;PowerInfer/ReluLLaMA-13B-PowerInfer-GGUF&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Falcon(ReLU)-40B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluFalcon-40B-PowerInfer-GGUF&#34;&gt;PowerInfer/ReluFalcon-40B-PowerInfer-GGUF&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA(ReLU)-2-70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluLLaMA-70B-PowerInfer-GGUF&#34;&gt;PowerInfer/ReluLLaMA-70B-PowerInfer-GGUF&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We suggest downloading/cloning the whole repo so PowerInfer can automatically make use of such directory structure for feature-complete model offloading:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;├── *.powerinfer.gguf (Unquantized PowerInfer model)&#xA;├── *.q4.powerinfer.gguf (INT4 quantized PowerInfer model, if available)&#xA;├── activation (Profiled activation statistics for fine-grained FFN offloading)&#xA;│   ├── activation_x.pt (Profiled activation statistics for layer x)&#xA;│   └── ...&#xA;├── *.[q4].powerinfer.gguf.generated.gpuidx (Generated GPU index at runtime for corresponding model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Convert from Original Model Weights + Predictor Weights&lt;/h3&gt; &#xA;&lt;p&gt;Hugging Face limits single model weight to 50GiB. For unquantized models &amp;gt;= 40B, you can convert PowerInfer GGUF from the original model weights and predictor weights obtained from Hugging Face.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Base Model&lt;/th&gt; &#xA;   &lt;th&gt;Original Model&lt;/th&gt; &#xA;   &lt;th&gt;Predictor&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA(ReLU)-2-7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/SparseLLM/ReluLLaMA-7B&#34;&gt;SparseLLM/ReluLLaMA-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluLLaMA-7B-Predictor&#34;&gt;PowerInfer/ReluLLaMA-7B-Predictor&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA(ReLU)-2-13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/SparseLLM/ReluLLaMA-13B&#34;&gt;SparseLLM/ReluLLaMA-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluLLaMA-13B-Predictor&#34;&gt;PowerInfer/ReluLLaMA-13B-Predictor&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Falcon(ReLU)-40B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/SparseLLM/ReluFalcon-40B&#34;&gt;SparseLLM/ReluFalcon-40B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluFalcon-40B-Predictor&#34;&gt;PowerInfer/ReluFalcon-40B-Predictor&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA(ReLU)-2-70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/SparseLLM/ReluLLaMA-70B&#34;&gt;SparseLLM/ReluLLaMA-70B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/PowerInfer/ReluLLaMA-70B-Predictor&#34;&gt;PowerInfer/ReluLLaMA-70B-Predictor&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can use the following command to convert the original model weights and predictor weights to PowerInfer GGUF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# make sure that you have done `pip install -r requirements.txt`&#xA;python convert.py --outfile /PATH/TO/POWERINFER/GGUF/REPO/MODELNAME.powerinfer.gguf /PATH/TO/ORIGINAL/MODEL /PATH/TO/PREDICTOR&#xA;# python convert.py --outfile ./ReluLLaMA-70B-PowerInfer-GGUF/llama-70b-relu.powerinfer.gguf ./SparseLLM/ReluLLaMA-70B ./PowerInfer/ReluLLaMA-70B-Predictor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the same reason, we suggest keeping the same directory structure as PowerInfer GGUF repos after conversion.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;For CPU-only and CPU-GPU hybrid inference with all available VRAM, you can use the following instructions to run PowerInfer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/bin/main -m /PATH/TO/MODEL -n $output_token_count -t $thread_num -p $prompt&#xA;# ./build/bin/main -m ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.q4.powerinfer.gguf -n 128 -t 8 -p &#34;Once upon a time&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to limit the VRAM usage of GPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/bin/main -m /PATH/TO/MODEL -n $output_token_count -t $thread_num -p $prompt --vram-budget $vram_gb&#xA;# ./build/bin/main -m ./ReluLLaMA-7B-PowerInfer-GGUF/llama-7b-relu.powerinfer.gguf -n 128 -t 8 -p &#34;Once upon a time&#34; --vram-budget 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Under CPU-GPU hybrid inference, PowerInfer will automatically offload all dense activation blocks to GPU, then split FFN and offload to GPU if possible.&lt;/p&gt; &#xA;&lt;h2&gt;Quantization&lt;/h2&gt; &#xA;&lt;p&gt;PowerInfer has optimized quantization support for INT4(&lt;code&gt;Q4_0&lt;/code&gt;) models. You can use the following instructions to quantize PowerInfer GGUF model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/bin/quantize /PATH/TO/MODEL /PATH/TO/OUTPUT/QUANTIZED/MODEL Q4_0&#xA;# ./build/bin/quantize ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.powerinfer.gguf ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.q4.powerinfer.gguf Q4_0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can use the quantized model for inference with PowerInfer with the same instructions as above.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We evaluated PowerInfer vs. llama.cpp on a single RTX 4090(24G) with a series of FP16 ReLU models under inputs of length 64, and the results are shown below. PowerInfer achieves up to 11x speedup on Falcon 40B and up to 3x speedup on Llama 2 70B.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/SJTU-IPADS/PowerInfer/assets/34213478/d700fa6c-77ba-462f-a2fc-3fd21c898f33&#34; alt=&#34;github-eval-4090&#34;&gt; &lt;sub&gt;The X axis indicates the output length, and the Y axis represents the speedup compared with llama.cpp. The number above each bar indicates the end-to-end generation speed (total prompting + generation time / total tokens generated, in tokens/s).&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;p&gt;We also evaluated PowerInfer on a single RTX 2080Ti(11G) with INT4 ReLU models under inputs of length 8, and the results are illustrated in the same way as above. PowerInfer achieves up to 8x speedup on Falcon 40B and up to 3x speedup on Llama 2 70B.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/SJTU-IPADS/PowerInfer/assets/34213478/0fc1bfc4-aafc-4e82-a865-bec0143aff1a&#34; alt=&#34;github-eval-2080ti-q4&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf&#34;&gt;paper&lt;/a&gt; for more evaluation details.&lt;/p&gt; &#xA;&lt;h2&gt;FAQs&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;What if I encountered &lt;code&gt;CUDA_ERROR_OUT_OF_MEMORY&lt;/code&gt;?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can try to run with &lt;code&gt;--reset-gpu-index&lt;/code&gt; argument to rebuild the GPU index for this model to avoid any stale cache.&lt;/li&gt; &#xA;   &lt;li&gt;Due to our current implementation, model offloading might not be as accurate as expected. You can try with &lt;code&gt;--vram-budget&lt;/code&gt; with a slightly lower value or &lt;code&gt;--disable-gpu-index&lt;/code&gt; to disable FFN offloading.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Does PowerInfer support mistral, original llama, Qwen, ...?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Now we only support models with ReLU/ReGLU/Squared ReLU activation function. So we do not support these models now. It&#39;s worth mentioning that a &lt;a href=&#34;https://arxiv.org/pdf/2310.04564.pdf&#34;&gt;paper&lt;/a&gt; has demonstrated that using the ReLU/ReGLU activation function has a negligible impact on convergence and performance.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Why is there a noticeable downgrade in the performance metrics of our current ReLU model, particularly the 70B model?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;In contrast to the typical requirement of around 2T tokens for LLM training, our model&#39;s fine-tuning was conducted with only 5B tokens. This insufficient retraining has resulted in the model&#39;s inability to regain its original performance. We are actively working on updating to a more capable model, so please stay tuned.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;What if...&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Issues are welcomed! Please feel free to open an issue and attach your running environment and running parameters. We will try our best to help you.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;TODOs&lt;/h2&gt; &#xA;&lt;p&gt;We will release the code and data in the following order, please stay tuned!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release core code of PowerInfer, supporting Llama-2, Falcon-40B.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support Mistral-7B&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support Windows&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support text-generation-webui&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release perplexity evaluation code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support Metal for Mac&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release code for OPT models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release predictor training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support online split for FFN network&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support Multi-GPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Paper and Citation&lt;/h2&gt; &#xA;&lt;p&gt;More technical details can be found in our &lt;a href=&#34;https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you find PowerInfer useful or relevant to your project and research, please kindly cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{song2023powerinfer,&#xA;      title={PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU}, &#xA;      author={Yixin Song and Zeyu Mi and Haotong Xie and Haibo Chen},&#xA;      year={2023},&#xA;      eprint={2312.12456},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We are thankful for the easily modifiable operator library &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt; and execution runtime provided by &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;. We also extend our gratitude to &lt;a href=&#34;https://nlp.csai.tsinghua.edu.cn/&#34;&gt;THUNLP&lt;/a&gt; for their support of ReLU-based sparse models. We also appreciate the research of &lt;a href=&#34;https://proceedings.mlr.press/v202/liu23am.html&#34;&gt;Deja Vu&lt;/a&gt;, which inspires PowerInfer.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>electric-capital/crypto-ecosystems</title>
    <updated>2023-12-24T01:27:38Z</updated>
    <id>tag:github.com,2023-12-24:/electric-capital/crypto-ecosystems</id>
    <link href="https://github.com/electric-capital/crypto-ecosystems" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A taxonomy for open source cryptocurrency, blockchain, and decentralized ecosystems&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;V1.1 UPDATE [12/20/23]:&lt;/strong&gt; Read about the Crypto Ecosystems taxonomy&#39;s update to Version 1.1 &lt;a href=&#34;https://github.com/electric-capital/crypto-ecosystems/releases/tag/1.1&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Crypto Ecosystems&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/electric-capital/crypto-ecosystems/raw/master/LICENSE&#34;&gt;MIT license with attribution&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;🌲 Crypto Ecosystems is a taxonomy for sharing data around open source blockchain, Web3, cryptocurrency, and decentralized ecosystems and tying them to GitHub organizations and code repositories. All of the ecosystems are specified in &lt;a href=&#34;https://github.com/toml-lang/toml&#34;&gt;TOML&lt;/a&gt; configuration files.&lt;/p&gt; &#xA;&lt;p&gt;This repository is not complete, and hopefully it never is as there are new ecosystems and repositories created everyday.&lt;/p&gt; &#xA;&lt;h2&gt;How to Contribute&lt;/h2&gt; &#xA;&lt;p&gt;There&#39;s a couple of ways you can help grow this initiative.&lt;/p&gt; &#xA;&lt;h3&gt;Option 1: Opening a Pull Request&lt;/h3&gt; &#xA;&lt;p&gt;You can make any .toml file for an ecosystem under the &lt;code&gt;/data/ecosystems&lt;/code&gt; directory or edit an existing one to help improve data around an ecosystem.&lt;/p&gt; &#xA;&lt;p&gt;You can fork this repository and open a PR from the forked repo to this repo. If you are not sure how to do that, you can follow the tutorial &lt;a href=&#34;https://www.loom.com/share/f23aab8c675940a9998b228ea1e179b7&#34;&gt;in this video&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Data Format&lt;/h4&gt; &#xA;&lt;p&gt;An example configuration file for the Bitcoin ecosystem looks like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Ecosystem Level Information&#xA;title = &#34;Bitcoin&#34;&#xA;&#xA;# Sub Ecosystems&#xA;# These are the titles of other ecosystems in different .toml files in the /data/ecosystems directory&#xA;sub_ecosystems = [ &#34;Lightning&#34;, &#34;RSK Smart Bitcoin&#34;, &#34;ZeroNet&#34;]&#xA;&#xA;# GitHub Organizations&#xA;# This is a list of links to associated GitHub organizations.&#xA;github_organizations = [&#34;https://github.com/bitcoin&#34;, &#34;https://github.com/bitcoin-core&#34;, &#34;https://github.com/bitcoinj&#34;, &#34;https://github.com&#xA;/btcsuite&#34;, &#34;https://github.com/libbitcoin&#34;, &#34;https://github.com/rust-bitcoin&#34;]&#xA;&#xA;# Repositories&#xA;# These are structs including a url and tags for a git repository. These URLs do not necessarily have to be on GitHub.&#xA;[[repo]]&#xA;url = &#34;https://github.com/bitcoin/bitcoin&#34;&#xA;tags = [ &#34;Protocol&#34;]&#xA;&#xA;[[repo]]&#xA;url = &#34;https://github.com/bitcoinbook/bitcoinbook&#34;&#xA;tags = [ &#34;Documentation&#34;]&#xA;&#xA;[[repo]]&#xA;url = &#34;https://github.com/bitcoin-wallet/bitcoin-wallet&#34;&#xA;tags = [ &#34;Wallet&#34;]&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By specifying the data as evolving config files in git, we benefit from a long term, auditable database that is both human and machine readable.&lt;/p&gt; &#xA;&lt;h3&gt;Option 2: Complete the Ecosystem Submission form&lt;/h3&gt; &#xA;&lt;p&gt;If you are not a developer or you find making a commit too difficult, you can use this Airtable based alternative below.&lt;/p&gt; &#xA;&lt;p&gt;You can &lt;a href=&#34;https://airtable.com/shrN4vZMlBLm3Dap8&#34;&gt;visit the form here&lt;/a&gt;, fill it, submit it and we&#39;ll take care of the rest :)&lt;/p&gt; &#xA;&lt;h2&gt;How to Give Attribution For Usage of the Electric Capital Crypto Ecosystems&lt;/h2&gt; &#xA;&lt;p&gt;To use the Electric Capital Crypto Ecosystems Map, you will need an attribution.&lt;/p&gt; &#xA;&lt;p&gt;Attribution needs to have 3 components:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Source: “Electric Capital Crypto Ecosystems Mapping”&lt;/li&gt; &#xA; &lt;li&gt;Link: &lt;a href=&#34;https://github.com/electric-capital/crypto-ecosystems&#34;&gt;https://github.com/electric-capital/crypto-ecosystems&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Logo: &lt;a href=&#34;https://drive.google.com/file/d/1DAX6wmcbtia7kaP5AaUWyg6t-ZEW9z22/view?usp=sharing&#34;&gt;Link to logo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Optional: Everyone in the crypto ecosystem benefits from additions to this repository. It is a help to everyone to include an ask to contribute next to your attribution.&lt;/p&gt; &#xA;&lt;p&gt;Sample request language: &#34;If you’re working in open source crypto, submit your repository here to be counted.&#34;&lt;/p&gt; &#xA;&lt;p&gt;&lt;ins&gt;Sample attribution&lt;/ins&gt;&lt;/p&gt; &#xA;&lt;p&gt;Data Source: &lt;a href=&#34;https://github.com/electric-capital/crypto-ecosystems&#34;&gt;Electric Capital Crypto Ecosystems Mapping&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you’re working in open source crypto, submit your repository &lt;a href=&#34;https://github.com/electric-capital/crypto-ecosystems&#34;&gt;here&lt;/a&gt; to be counted.&lt;/p&gt; &#xA;&lt;h2&gt;How to Contribute (Step-by-Step Guide)&lt;/h2&gt; &#xA;&lt;p&gt;There are three types of contributions you can make to this data set:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Adding a new ecosystem (e.g. a new layer 1 blockchain)&lt;/li&gt; &#xA; &lt;li&gt;Adding a new sub-ecosystem (e.g. a big organisation that has multiple repos within the above ecosystem)&lt;/li&gt; &#xA; &lt;li&gt;Adding a new repo (e.g. an individual project within the ecosystem/sub-ecosystem) or organization&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This may sound confusing. It is perhaps even more confusing because whilst there are these different data sources/structures, all of them sit within one directory (data/ecosystems) as &lt;code&gt;.toml&lt;/code&gt; files.&lt;/p&gt; &#xA;&lt;p&gt;To make things easier, we&#39;ve made the following roadmap for you to follow depending on which of the above 3 types of contributions you&#39;re trying to make.&lt;/p&gt; &#xA;&lt;h3&gt;Option 1: Adding a new ecosystem (e.g. blockchain)&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re adding a totally new ecosystem that has no parents (e.g. Cosmos/Ethereum), then follow this path. You&#39;re most likely adding a new L1 blockchain, so let&#39;s take the fictitious example of a new chain called &lt;code&gt;EasyA Chain&lt;/code&gt;. Follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go to the &lt;code&gt;data/ecosystems&lt;/code&gt; directory&lt;/li&gt; &#xA; &lt;li&gt;Find the folder named the first letter of the ecosystem you&#39;re adding. Here, it&#39;s the letter &lt;code&gt;E&lt;/code&gt; because our L1 is called &lt;code&gt;EasyA Chain&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Open the folder&lt;/li&gt; &#xA; &lt;li&gt;Inside the folder, create a new &lt;code&gt;.toml&lt;/code&gt; file named after your L1 in kebab-case. Here, it will be called &lt;code&gt;easya-chain.toml&lt;/code&gt;. The full path will now be &lt;code&gt;data/ecosystems/e/easya-chain.toml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Add the following 2 required fields:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Required field number 1: Name of the ecosystem&#xA;title = &#34;EasyA Chain&#34;&#xA;&#xA;# Required field number 2: List of associated GitHub organizations&#xA;github_organizations = [&#34;https://github.com/EasyA-Tech&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Make your PR! ✅&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Remember that this is a hierarchy. If you&#39;re adding a Cosmos appchain, therefore, you should be following Option 2 below (since it will be a sub-ecosystem of Cosmos).&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s important to note also that you &lt;strong&gt;do not&lt;/strong&gt; need to add all the repos within your GitHub organizations to the &lt;code&gt;.toml&lt;/code&gt; file as individual repos, because the system automatically fetches all repos within the organization.&lt;/p&gt; &#xA;&lt;p&gt;You may see other ecosystems that have done this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;title = &#34;EasyA Chain&#34;&#xA;&#xA;github_organizations = [&#34;https://github.com/EasyA-Tech&#34;]&#xA;&#xA;[[repo]]&#xA;url = &#34;https://github.com/EasyA-Tech/Chain&#34; # ❌ Don&#39;t do this ❌&#xA;tags = [ &#34;Protocol&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is unnecessary. It adds clutter and makes it harder for reviewers to approve your PR. We will explain below when and why you should add repos to an ecosystem.&lt;/p&gt; &#xA;&lt;h3&gt;Option 2: Adding a new sub-ecosystem&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re adding a new sub-ecosystem (in other words, it has a parent, like a blockchain or a layer 0), then follow these steps. Again, we&#39;ll be using the fictitious &lt;code&gt;EasyA Chain&lt;/code&gt; L1 blockchain as an example. However, this time, we&#39;ll be adding the new &lt;code&gt;EasyA Community Wallet&lt;/code&gt; sub-ecosystem to it.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go to the &lt;code&gt;data/ecosystems&lt;/code&gt; directory&lt;/li&gt; &#xA; &lt;li&gt;Find the folder named the first letter of the name of the ecosystem which the project you&#39;re adding is part of. Here, it&#39;s the letter &lt;code&gt;E&lt;/code&gt; because our L1 is called &lt;code&gt;EasyA Chain&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Open the folder. Here, it&#39;s the &lt;code&gt;E&lt;/code&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;Inside the folder, find the &lt;code&gt;.toml&lt;/code&gt; file that has the ecosystem&#39;s name. Here, following our &lt;code&gt;EasyA Chain&lt;/code&gt; example, it will be &lt;code&gt;easya-chain.toml&lt;/code&gt;. The full path to the ecosystem will be &lt;code&gt;data/ecosystems/e/easya-chain.toml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Open this file. Inside the ecosystem file, you will see something that looks like this:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;title = &#34;EasyA Chain&#34;&#xA;&#xA;github_organizations = [&#34;https://github.com/EasyA-Tech&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;You will then need to do one of two things.&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;If there are no sub-ecosystems yet, add your sub-ecosystem by adding the following line:&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;sub_ecosystems = [&#34;EasyA Community Wallet&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ol start=&#34;2&#34;&gt; &#xA;   &lt;li&gt;If you see a line starting with &lt;code&gt;sub_ecosystems&lt;/code&gt; already, then simple add your sub-ecosystem to the list:&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;sub_ecosystems = [&#34;Pre-existing Sub-Ecosystem&#34;, &#34;EasyA Community Wallet&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Overall, your file should then look something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;title = &#34;EasyA Chain&#34;&#xA;&#xA;sub_ecosystems = [&#34;EasyA Community Wallet&#34;] # This is the line we changed&#xA;&#xA;github_organizations = [&#34;https://github.com/EasyA-Tech&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;Once you&#39;ve added your sub-ecosystem&#39;s name to the parent ecosystem file, go back to the &lt;code&gt;data/ecosystems&lt;/code&gt; directory.&lt;/li&gt; &#xA; &lt;li&gt;This time, find the folder that is the first letter of the name of the sub-ecosystem you&#39;re adding. Here, it also happens to be the letter &lt;code&gt;E&lt;/code&gt; because our sub-ecosystem is called &lt;code&gt;EasyA Community Wallet&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Open the folder. Here, it&#39;s the &lt;code&gt;E&lt;/code&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;Inside the folder, create the &lt;code&gt;.toml&lt;/code&gt; file that has the sub-ecosystem&#39;s name. Here, following our &lt;code&gt;EasyA Community Wallet&lt;/code&gt; example, it will be &lt;code&gt;easya-community-wallet.toml&lt;/code&gt;. The full path to the ecosystem will be &lt;code&gt;data/ecosystems/e/easya-community-wallet.toml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Add the following 2 required fields:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Required field number 1: Name of the sub-ecosystem&#xA;title = &#34;EasyA Community Wallet&#34;&#xA;&#xA;# Required field number 2: List of associated GitHub organizations&#xA;github_organizations = [&#34;https://github.com/EasyA-Community-Wallet&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;12&#34;&gt; &#xA; &lt;li&gt;Make your PR! ✅&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Please remember here too that the same note as in Option 1 applies: the system automatically pulls in the repos from your sub-ecosystem GitHub organization, so you don&#39;t need to list them all out individually.&lt;/p&gt; &#xA;&lt;p&gt;If you prefer videos, you can also see the above steps done live &lt;a href=&#34;https://www.loom.com/share/f23aab8c675940a9998b228ea1e179b7&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;ve been following along closely, you&#39;ll have noticed that the steps after adding the sub-ecosystem to the parent ecosystem are exactly the same a Option 1 (adding a totally new ecosystem that has no parents). That&#39;s because this taxonomy is based on ancestry. Any sub-ecosystem is basically just an ecosystem in its own right (it&#39;s not like a sub-ecosystem is any less valuable). The ecosystem and sub-ecosystem dichotomy is merely there so we can see the relationship between different ecosystems. You can keep adding sub-ecoystems to sub-ecosystems ad infinitum (forever).&lt;/p&gt; &#xA;&lt;h3&gt;Option 3: Adding a new repo or organization&lt;/h3&gt; &#xA;&lt;p&gt;The system automatically pulls in all repos listed under a GitHub organization within an ecosystem. For example, when the system sees the below ecosystem, it will automatically account for all the repos under the &lt;code&gt;EasyA-Tech&lt;/code&gt; GitHub organization.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;title = &#34;EasyA Chain&#34;&#xA;&#xA;github_organizations = [&#34;https://github.com/EasyA-Tech&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;So don&#39;t worry! You don&#39;t need to add every single repo if it&#39;s already part of an organization that&#39;s in the data set.&lt;/p&gt; &#xA;&lt;p&gt;To add a new organization, simply append its full GitHub URL to the list of organizations in the associated ecosystem. Let&#39;s take the example of adding an organization with the URL &lt;code&gt;https://github.com/EasyA-Community&lt;/code&gt; as part of the &lt;code&gt;EasyA Chain&lt;/code&gt; ecosystem.&lt;/p&gt; &#xA;&lt;p&gt;You would follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go to the &lt;code&gt;data/ecosystems&lt;/code&gt; directory&lt;/li&gt; &#xA; &lt;li&gt;Find the folder named the first letter of the name of the ecosystem which the organization you&#39;re adding is part of. Here, it&#39;s the letter &lt;code&gt;E&lt;/code&gt; because our ecosystem is called &lt;code&gt;EasyA Chain&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Open the folder. Here, it&#39;s the &lt;code&gt;E&lt;/code&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;Inside the folder, find the &lt;code&gt;.toml&lt;/code&gt; file that has the ecosystem&#39;s name. Here, following our &lt;code&gt;EasyA Chain&lt;/code&gt; example, it will be &lt;code&gt;easya-chain.toml&lt;/code&gt;. The full path to the ecosystem will be &lt;code&gt;data/ecosystems/e/easya-chain.toml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Open this file. Inside the ecosystem file, you will see something that looks like this:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;title = &#34;EasyA Chain&#34;&#xA;&#xA;github_organizations = [&#34;https://github.com/EasyA-Tech&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Simply add your GitHub organization URL to the list. Here, ours is &lt;code&gt;https://github.com/EasyA-Community&lt;/code&gt; so we&#39;ll add that:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;title = &#34;EasyA Chain&#34;&#xA;&#xA;github_organizations = [&#34;https://github.com/EasyA-Tech&#34;, &#34;https://github.com/EasyA-Community&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;Make your PR! ✅&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;When, then, should you add repos? You only need to add a repo directly to an ecosystem if:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;✅ It is not owned by a GitHub organization already listed in an ecosystem file (those &lt;code&gt;.toml&lt;/code&gt; files)&lt;/li&gt; &#xA; &lt;li&gt;✅ It is not itself an ecosystem/sub-ecosystem (in which case you&#39;d be adding it as an ecosystem)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The types of projects that will commonly get added as individual repos are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Documentation&lt;/li&gt; &#xA; &lt;li&gt;Wallets&lt;/li&gt; &#xA; &lt;li&gt;Utility Libraries&lt;/li&gt; &#xA; &lt;li&gt;Smaller protocols&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Usually these will be repos created by the community (so not already accounted for under the ecosystem/sub-ecosystem GitHub organization). Use that as a rough heuristic here. If the repo you&#39;re adding is actually one of many repos all in the same ecosystem, and in fact the organization only contributes to that one ecosystem, then you should almost certainly be adding your organization instead.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re happy that you should be adding this repo, then here&#39;s how to do it. Let&#39;s take the example of a community contributor with the GitHub handle &lt;code&gt;Platonicsocrates&lt;/code&gt; who&#39;s created a helper library for the &lt;code&gt;EasyA Chain&lt;/code&gt; but also contributes to other projects (so we shouldn&#39;t add their whole organization/profile). Their repo URL &lt;code&gt;https://github.com/platonicsocrates/easya-helpers&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You would follow these steps to add it:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go to the &lt;code&gt;data/ecosystems&lt;/code&gt; directory&lt;/li&gt; &#xA; &lt;li&gt;Find the folder named the first letter of the name of the ecosystem which the repo you&#39;re adding is part of. Here, it&#39;s the letter &lt;code&gt;E&lt;/code&gt; because our ecosystem is called &lt;code&gt;EasyA Chain&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Open the folder. Here, it&#39;s the &lt;code&gt;E&lt;/code&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;Inside the folder, find the &lt;code&gt;.toml&lt;/code&gt; file that has the ecosystem&#39;s name. Here, following our &lt;code&gt;EasyA Chain&lt;/code&gt; example, it will be &lt;code&gt;easya-chain.toml&lt;/code&gt;. The full path to the ecosystem will be &lt;code&gt;data/ecosystems/e/easya-chain.toml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Open this file. Inside the ecosystem file, you will see something that looks like this:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;title = &#34;EasyA Chain&#34;&#xA;&#xA;github_organizations = [&#34;https://github.com/EasyA-Tech&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Simply add the following three lines at the end of the &lt;code&gt;.toml&lt;/code&gt; file:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[[repo]]&#xA;url = &#34;https://github.com/platonicsocrates/easya-helpers&#34; # Replace this URL with your repo url&#xA;tags = [ &#34;Library&#34;] # This line is optional&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If there are already other repos in the ecosystem, just add the above as new lines (unlike adding organizations or sub-ecosystems, these aren&#39;t lists). For example, if the ecosystem already has a repo, we will just add it below as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;&#xA;# Repo that&#39;s already been added&#xA;[[repo]]&#xA;url = &#34;https://github.com/platonicsocrates/easya-js&#34;&#xA;tags = [ &#34;Library&#34;] &#xA;&#xA;# Our new repo&#xA;[[repo]]&#xA;url = &#34;https://github.com/platonicsocrates/easya-helpers&#34; # Replace this URL with your repo url&#xA;tags = [ &#34;Library&#34;] # This line is optional&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;Make your PR! ✅&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Thank you for contributing and for reading the contribution guide! ❤️&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>serverless/serverless</title>
    <updated>2023-12-24T01:27:38Z</updated>
    <id>tag:github.com,2023-12-24:/serverless/serverless</id>
    <link href="https://github.com/serverless/serverless" rel="alternate"></link>
    <summary type="html">&lt;p&gt;⚡ Serverless Framework – Build web, mobile and IoT applications with serverless architectures using AWS Lambda, Azure Functions, Google CloudFunctions &amp; more! –&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://serverless.com&#34;&gt;&lt;img src=&#34;https://s3.amazonaws.com/assets.github.serverless/readme-serverless-framework.gif&#34; alt=&#34;Serverless Application Framework AWS Lambda API Gateway&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.serverless.com&#34;&gt;&lt;img src=&#34;http://public.serverless.com/badges/v3.svg?sanitize=true&#34; alt=&#34;serverless&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/serverless/serverless/actions?query=workflow%3AIntegrate&#34;&gt;&lt;img src=&#34;https://github.com/serverless/serverless/workflows/Integrate/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/js/serverless&#34;&gt;&lt;img src=&#34;https://badge.fury.io/js/serverless.svg?sanitize=true&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/serverless/serverless&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/serverless/serverless/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/serverless/serverless&#34;&gt;&lt;img src=&#34;https://img.shields.io/gitter/room/serverless/serverless.svg?sanitize=true&#34; alt=&#34;gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://snyk.io/test/github/serverless/serverless&#34;&gt;&lt;img src=&#34;https://snyk.io/test/github/serverless/serverless/badge.svg?sanitize=true&#34; alt=&#34;Known Vulnerabilities&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/serverless&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/l/serverless.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.serverless.com&#34;&gt;Website&lt;/a&gt; • &lt;a href=&#34;https://serverless.com/framework/docs/&#34;&gt;Docs&lt;/a&gt; • &lt;a href=&#34;https://serverless.com/slack&#34;&gt;Community Slack&lt;/a&gt; • &lt;a href=&#34;http://forum.serverless.com&#34;&gt;Forum&lt;/a&gt; • &lt;a href=&#34;https://twitter.com/goserverless&#34;&gt;Twitter&lt;/a&gt; • &lt;a href=&#34;https://www.meetup.com/pro/serverless/&#34;&gt;Meetups&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Serverless Framework&lt;/strong&gt; – Build applications on AWS Lambda and other next-gen cloud services, that auto-scale and only charge you when they run. This lowers the total cost of running and operating your apps, enabling you to build more and manage less.&lt;/p&gt; &#xA;&lt;p&gt;The Serverless Framework is a command-line tool with an easy and approachable YAML syntax to deploy both your code and cloud infrastructure needed to make tons of serverless application use-cases. It&#39;s a multi-language framework that supports Node.js, Typescript, Python, Go, Java, and more. It&#39;s also completely extensible via over 1,000 plugins which add more serverless use-cases and workflows to the Framework.&lt;/p&gt; &#xA;&lt;p&gt;Actively maintained by &lt;a href=&#34;https://www.serverless.com&#34;&gt;Serverless Inc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/serverless/serverless/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/serverless/serverless/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/serverless/plugins&#34;&gt;Plugins&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/serverless/serverless/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/serverless/serverless/main/#community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/serverless/serverless/main/#licensing&#34;&gt;Licensing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;features&#34;&gt;&lt;/a&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hyper-Productive&lt;/strong&gt; - Build more and manage less with serverless architectures.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiple Use-Cases&lt;/strong&gt; - Choose from tons of efficient serverless use-cases (APIs, Scheduled Tasks, Event Handlers, Streaming Data Pipelines, Web Sockets &amp;amp; more).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Infra &amp;amp; Code&lt;/strong&gt; - Deploys both code and infrastructure together, resulting in out-of-the-box serverless apps.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy&lt;/strong&gt; - Enjoy simple syntax to safely deploy deploy AWS Lambda functions, event sources and more without being a cloud expert.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Language&lt;/strong&gt; - Supports Node.js, Python, Java, Go, C#, Ruby, Swift, Kotlin, PHP, Scala, &amp;amp; F#&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Full Lifecycle&lt;/strong&gt; - Manages the lifecycle of your serverless architecture (build, deploy, update, monitor, troubleshoot).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Domains&lt;/strong&gt; - Group domains into Serverless Services for easy management of code, resources &amp;amp; processes, across large projects &amp;amp; teams.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Environments&lt;/strong&gt; - Built-in support for multiple stages (e.g. development, staging, production).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Guardrails&lt;/strong&gt; - Loaded with automation, optimization and best practices.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt; - Extend or modify the Framework and its operations via Plugins.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Plugin Ecosystem&lt;/strong&gt; - Extend or modify the Framework and its operations via Plugins.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Welcoming&lt;/strong&gt; - A passionate and welcoming community!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;quick-start&#34;&gt;&lt;/a&gt;Quick Start&lt;/h1&gt; &#xA;&lt;p&gt;Here&#39;s how to get started quickly, as well as some recommended development workflows.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install &lt;code&gt;serverless&lt;/code&gt; module via NPM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install -g serverless&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;If you don’t already have Node.js on your machine, &lt;a href=&#34;https://nodejs.org/&#34;&gt;install it first&lt;/a&gt;. If you don&#39;t want to install Node or NPM, you can &lt;a href=&#34;https://www.serverless.com/framework/docs/install-standalone&#34;&gt;install &lt;strong&gt;serverless&lt;/strong&gt; as a standalone binary&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Creating A Service&lt;/h2&gt; &#xA;&lt;p&gt;To create your first project (known as a Serverless Framework &#34;Service&#34;), run the &lt;code&gt;serverless&lt;/code&gt; command below, then follow the prompts.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create a new serverless project&#xA;serverless&#xA;&#xA;# Move into the newly created directory&#xA;cd your-service-name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;serverless&lt;/code&gt; command will guide you to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a new project&lt;/li&gt; &#xA; &lt;li&gt;Configure your &lt;a href=&#34;https://serverless.com/framework/docs/providers/aws/guide/credentials/&#34;&gt;AWS credentials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Optionally set up a free Serverless Framework account with additional features.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Your new serverless project will contain a &lt;code&gt;serverless.yml&lt;/code&gt; file. This file features simple syntax for deploying infrastructure to AWS, such as AWS Lambda functions, infrastructure that triggers those functions with events, and additional infrastructure your AWS Lambda functions may need for various use-cases. You can learn more about this in the &lt;a href=&#34;https://www.serverless.com/framework/docs/providers/aws/guide/intro&#34;&gt;Core Concepts documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;serverless&lt;/code&gt; command will give you a variety of templates to choose from. If those do not fit your needs, check out the &lt;a href=&#34;https://github.com/serverless/examples&#34;&gt;project examples from Serverless Inc. and our community&lt;/a&gt;. You can install any example by passing a GitHub URL using the &lt;code&gt;--template-url&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-base&#34;&gt;serverless --template-url=https://github.com/serverless/examples/tree/v3/...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that you can use &lt;code&gt;serverless&lt;/code&gt; or &lt;code&gt;sls&lt;/code&gt; to run Serverless Framework commands.&lt;/p&gt; &#xA;&lt;h2&gt;Deploying&lt;/h2&gt; &#xA;&lt;p&gt;If you haven&#39;t done so already within the &lt;code&gt;serverless&lt;/code&gt; command, you can deploy the project at any time by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sls deploy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The deployed AWS Lambda functions and other essential information such as API Endpoint URLs will be displayed in the command output.&lt;/p&gt; &#xA;&lt;p&gt;More details on deploying can be found &lt;a href=&#34;https://www.serverless.com/framework/docs/providers/aws/guide/deploying&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Developing On The Cloud&lt;/h2&gt; &#xA;&lt;p&gt;Many Serverless Framework users choose to develop on the cloud, since it matches reality and emulating Lambda locally can be complex. To develop on the cloud quickly, without sacrificing speed, we recommend the following workflow...&lt;/p&gt; &#xA;&lt;p&gt;To deploy code changes quickly, skip the &lt;code&gt;serverless deploy&lt;/code&gt; command which is much slower since it triggers a full AWS CloudFormation update. Instead, deploy code and configuration changes to individual AWS Lambda functions in seconds via the &lt;code&gt;deploy function&lt;/code&gt; command, with &lt;code&gt;-f [function name in serverless.yml]&lt;/code&gt; set to the function you want to deploy.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sls deploy function -f my-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More details on the &lt;code&gt;deploy function&lt;/code&gt; command can be found &lt;a href=&#34;https://www.serverless.com/framework/docs/providers/aws/cli-reference/deploy-function&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To invoke your AWS Lambda function on the cloud, you can find URLs for your functions w/ API endpoints in the &lt;code&gt;serverless deploy&lt;/code&gt; output, or retrieve them via &lt;code&gt;serverless info&lt;/code&gt;. If your functions do not have API endpoints, you can use the &lt;code&gt;invoke&lt;/code&gt; command, like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sls invoke -f hello&#xA;&#xA;# Invoke and display logs:&#xA;serverless invoke -f hello --log&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More details on the &lt;code&gt;invoke&lt;/code&gt; command can be found &lt;a href=&#34;https://www.serverless.com/framework/docs/providers/aws/cli-reference/invoke&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To stream your logs while you work, use the &lt;code&gt;sls logs&lt;/code&gt; command in a separate terminal window:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sls logs -f [Function name in serverless.yml] -t&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Target a specific function via the &lt;code&gt;-f&lt;/code&gt; option and enable streaming via the &lt;code&gt;-t&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;h2&gt;Developing Locally&lt;/h2&gt; &#xA;&lt;p&gt;Many Serverless Framework users rely on local emulation to develop more quickly. Please note, emulating AWS Lambda and other cloud services is never accurate and the process can be complex. We recommend the following workflow to develop locally...&lt;/p&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;invoke local&lt;/code&gt; command to invoke your function locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sls invoke local -f my-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also pass data to this local invocation via a variety of ways. Here&#39;s one of them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;serverless invoke local --function functionName --data &#39;{&#34;a&#34;:&#34;bar&#34;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More details on the &lt;code&gt;invoke local&lt;/code&gt; command can be found &lt;a href=&#34;https://www.serverless.com/framework/docs/providers/aws/cli-reference/invoke-local&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Serverless Framework also has a great plugin that allows you to run a server locally and emulate AWS API Gateway. This is the &lt;code&gt;serverless-offline&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;p&gt;More details on the &lt;strong&gt;serverless-offline&lt;/strong&gt; plugins command can be found &lt;a href=&#34;https://github.com/dherault/serverless-offline&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Monitoring, Secrets &amp;amp; Collaboration&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re looking for easy, out-of-the-box monitoring, secrets management and collaboration features, sign into the Serverless Framework Dashboard. It&#39;s free!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sls login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Remove your service&lt;/h2&gt; &#xA;&lt;p&gt;If you want to delete your service, run &lt;code&gt;remove&lt;/code&gt;. This will delete all the AWS resources created by your project and ensure that you don&#39;t incur any unexpected charges. It will also remove the service from Serverless Dashboard.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sls remove&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More details on the &lt;code&gt;remove&lt;/code&gt; command can be found &lt;a href=&#34;https://www.serverless.com/framework/docs/providers/aws/cli-reference/remove&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s Next&lt;/h2&gt; &#xA;&lt;p&gt;Here are some helpful resources for continuing with the Serverless Framework:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.serverless.com/framework/docs/providers/aws/guide/intro&#34;&gt;Study Serverless Framework&#39;s core concepts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/serverless/examples&#34;&gt;Get inspiration from these Serverless Framework templates&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.serverless.com/framework/docs/providers/aws/guide/events&#34;&gt;Discover all of the events that can trigger Lambda functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.serverless.com/framework/docs/providers/aws/guide/serverless.yml&#34;&gt;Bookmark Serverless Framework&#39;s &lt;code&gt;serverless.yml&lt;/code&gt; guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.serverless.com/plugins&#34;&gt;Search the plugins registry to extend Serverless Framework&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;contributing&#34;&gt;&lt;/a&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We love our contributors! Please read our &lt;a href=&#34;https://raw.githubusercontent.com/serverless/serverless/main/CONTRIBUTING.md&#34;&gt;Contributing Document&lt;/a&gt; to learn how you can start working on the Framework yourself.&lt;/p&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://github.com/serverless/serverless/labels/help%20wanted&#34;&gt;help wanted&lt;/a&gt; or &lt;a href=&#34;https://github.com/serverless/serverless/labels/good%20first%20issue&#34;&gt;good first issue&lt;/a&gt; labels to find issues we want to move forward on with your help.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;community&#34;&gt;&lt;/a&gt;Community&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/goserverless&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://serverless.com/slack&#34;&gt;Community Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.meetup.com/serverless/&#34;&gt;Serverless Meetups&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://stackoverflow.com/questions/tagged/serverless-framework&#34;&gt;Stackoverflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.facebook.com/serverless&#34;&gt;Facebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;mailto:hello@serverless.com&#34;&gt;Contact Us&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;licensing&#34;&gt;&lt;/a&gt;Licensing&lt;/h1&gt; &#xA;&lt;p&gt;Serverless is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/serverless/serverless/main/LICENSE.txt&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All files located in the node_modules and external directories are externally maintained libraries used by this software which have their own licenses; we recommend you read them, as their terms may differ from the terms in the MIT License.&lt;/p&gt;</summary>
  </entry>
</feed>