<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-06T01:30:35Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kamranahmedse/developer-roadmap</title>
    <updated>2022-08-06T01:30:35Z</updated>
    <id>tag:github.com,2022-08-06:/kamranahmedse/developer-roadmap</id>
    <link href="https://github.com/kamranahmedse/developer-roadmap" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Roadmap to becoming a developer in 2022&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/kamranahmedse/developer-roadmap/master/public/brand.png&#34; height=&#34;128&#34;&gt; &lt;/p&gt;&#xA;&lt;h2 align=&#34;center&#34;&gt;roadmap.sh&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Community driven roadmaps, articles and resources for developers&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://roadmap.sh/roadmaps&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-Roadmaps%20-0a0a0a.svg?style=flat&amp;amp;colorA=0a0a0a&#34; alt=&#34;roadmaps&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://youtube.com/theroadmap?sub_confirmation=1&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-Videos-0a0a0a.svg?style=flat&amp;amp;colorA=0a0a0a&#34; alt=&#34;videos&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/kamranahmedse/developer-roadmap/tree/0471d44c8fae58b6a36a7c57bba12253916d0249/translations&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-Translations-0a0a0a.svg?style=flat&amp;amp;colorA=0a0a0a&#34; alt=&#34;videos&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/channel/UCA0H2KIWgWTwpTFjSxp0now?sub_confirmation=1&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%E2%9D%A4-YouTube%20Channel-0a0a0a.svg?style=flat&amp;amp;colorA=0a0a0a&#34; alt=&#34;roadmaps&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/waxVImv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Roadmaps are being made interactive and have been moved to website.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://roadmap.sh&#34;&gt;View all Roadmaps&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/waxVImv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here is the list of available roadmaps with more being actively worked upon.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/frontend&#34;&gt;Frontend Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/backend&#34;&gt;Backend Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/devops&#34;&gt;DevOps Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/react&#34;&gt;React Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/angular&#34;&gt;Angular Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/android&#34;&gt;Android Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/python&#34;&gt;Python Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/golang&#34;&gt;Go Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/java&#34;&gt;Java Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/postgresql-dba&#34;&gt;DBA Roadmap&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/waxVImv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you think that these can be improved in any way, please do suggest. Also, if you would like to contribute to existing roadmaps or add a new roadmap, please open an issue or reach out to &lt;a href=&#34;https://twitter.com/kamranahmedse&#34;&gt;@kamranahmedse&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repository, install the dependencies and start the application&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:kamranahmedse/developer-roadmap.git&#xA;npm install&#xA;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Have a look at &lt;a href=&#34;https://raw.githubusercontent.com/kamranahmedse/developer-roadmap/master/contributing&#34;&gt;contribution docs&lt;/a&gt; for how to update any of the roadmaps&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Suggest changes to existing roadmaps&lt;/li&gt; &#xA; &lt;li&gt;Improve the site&#39;s codebase&lt;/li&gt; &#xA; &lt;li&gt;Add new Roadmap&lt;/li&gt; &#xA; &lt;li&gt;Write tests&lt;/li&gt; &#xA; &lt;li&gt;Discuss ideas in issues&lt;/li&gt; &#xA; &lt;li&gt;Spread the word&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Have a look at the &lt;a href=&#34;https://raw.githubusercontent.com/kamranahmedse/developer-roadmap/master/license&#34;&gt;license file&lt;/a&gt; for details&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>moyix/fauxpilot</title>
    <updated>2022-08-06T01:30:35Z</updated>
    <id>tag:github.com,2022-08-06:/moyix/fauxpilot</id>
    <link href="https://github.com/moyix/fauxpilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FauxPilot - an open-source GitHub Copilot server&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FauxPilot&lt;/h1&gt; &#xA;&lt;p&gt;This is an attempt to build a locally hosted version of &lt;a href=&#34;https://copilot.github.com/&#34;&gt;GitHub Copilot&lt;/a&gt;. It uses the &lt;a href=&#34;https://github.com/salesforce/CodeGen&#34;&gt;SalesForce CodeGen&lt;/a&gt; models inside of NVIDIA&#39;s &lt;a href=&#34;https://developer.nvidia.com/nvidia-triton-inference-server&#34;&gt;Triton Inference Server&lt;/a&gt; with the &lt;a href=&#34;https://github.com/triton-inference-server/fastertransformer_backend/&#34;&gt;FasterTransformer backend&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;You&#39;ll need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;docker-compose&lt;/code&gt; &amp;gt;= 1.28&lt;/li&gt; &#xA; &lt;li&gt;An NVIDIA GPU with enough VRAM to run the model you want.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker&#34;&gt;&lt;code&gt;nvidia-docker&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that the VRAM requirements listed by &lt;code&gt;setup.sh&lt;/code&gt; are &lt;em&gt;total&lt;/em&gt; -- if you have multiple GPUs, you can split the model across them. So, if you have two NVIDIA RTX 3080 GPUs, you &lt;em&gt;should&lt;/em&gt; be able to run the 6B model by putting half on each GPU.&lt;/p&gt; &#xA;&lt;h2&gt;Support and Warranty&lt;/h2&gt; &#xA;&lt;p&gt;lmao&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Run the setup script to choose a model to use. This will download the model from Huggingface and then convert it for use with FasterTransformer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./setup.sh &#xA;Models available:&#xA;[1] codegen-350M-mono (2GB total VRAM required; Python-only)&#xA;[2] codegen-350M-multi (2GB total VRAM required; multi-language)&#xA;[3] codegen-2B-mono (7GB total VRAM required; Python-only)&#xA;[4] codegen-2B-multi (7GB total VRAM required; multi-language)&#xA;[5] codegen-6B-mono (13GB total VRAM required; Python-only)&#xA;[6] codegen-6B-multi (13GB total VRAM required; multi-language)&#xA;[7] codegen-16B-mono (32GB total VRAM required; Python-only)&#xA;[8] codegen-16B-multi (32GB total VRAM required; multi-language)&#xA;Enter your choice [6]: 2&#xA;Enter number of GPUs [1]: 1&#xA;Where do you want to save the model [/home/moyix/git/fauxpilot/models]? /fastdata/mymodels&#xA;Downloading and converting the model, this will take a while...&#xA;Converting model codegen-350M-multi with 1 GPUs&#xA;Loading CodeGen model&#xA;Downloading config.json: 100%|██████████| 996/996 [00:00&amp;lt;00:00, 1.25MB/s]&#xA;Downloading pytorch_model.bin: 100%|██████████| 760M/760M [00:11&amp;lt;00:00, 68.3MB/s] &#xA;Creating empty GPTJ model&#xA;Converting...&#xA;Conversion complete.&#xA;Saving model to codegen-350M-multi-hf...&#xA;&#xA;=============== Argument ===============&#xA;saved_dir: /models/codegen-350M-multi-1gpu/fastertransformer/1&#xA;in_file: codegen-350M-multi-hf&#xA;trained_gpu_num: 1&#xA;infer_gpu_num: 1&#xA;processes: 4&#xA;weight_data_type: fp32&#xA;========================================&#xA;transformer.wte.weight&#xA;transformer.h.0.ln_1.weight&#xA;[... more conversion output trimmed ...]&#xA;transformer.ln_f.weight&#xA;transformer.ln_f.bias&#xA;lm_head.weight&#xA;lm_head.bias&#xA;Done! Now run ./launch.sh to start the FauxPilot server.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can just run &lt;code&gt;./launch.sh&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./launch.sh &#xA;[+] Running 2/0&#xA; ⠿ Container fauxpilot-triton-1         Created                                                                                                                                                                                                                                                                                             0.0s&#xA; ⠿ Container fauxpilot-copilot_proxy-1  Created                                                                                                                                                                                                                                                                                             0.0s&#xA;Attaching to fauxpilot-copilot_proxy-1, fauxpilot-triton-1&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | =============================&#xA;fauxpilot-triton-1         | == Triton Inference Server ==&#xA;fauxpilot-triton-1         | =============================&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | NVIDIA Release 22.06 (build 39726160)&#xA;fauxpilot-triton-1         | Triton Server Version 2.23.0&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | Copyright (c) 2018-2022, NVIDIA CORPORATION &amp;amp; AFFILIATES.  All rights reserved.&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | Various files include modifications (c) NVIDIA CORPORATION &amp;amp; AFFILIATES.  All rights reserved.&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | This container image and its contents are governed by the NVIDIA Deep Learning Container License.&#xA;fauxpilot-triton-1         | By pulling and using the container, you accept the terms and conditions of this license:&#xA;fauxpilot-triton-1         | https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license&#xA;fauxpilot-copilot_proxy-1  | WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.&#xA;fauxpilot-copilot_proxy-1  |  * Debug mode: off&#xA;fauxpilot-copilot_proxy-1  |  * Running on all addresses (0.0.0.0)&#xA;fauxpilot-copilot_proxy-1  |    WARNING: This is a development server. Do not use it in a production deployment.&#xA;fauxpilot-copilot_proxy-1  |  * Running on http://127.0.0.1:5000&#xA;fauxpilot-copilot_proxy-1  |  * Running on http://172.18.0.3:5000 (Press CTRL+C to quit)&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | ERROR: This container was built for NVIDIA Driver Release 515.48 or later, but&#xA;fauxpilot-triton-1         |        version  was detected and compatibility mode is UNAVAILABLE.&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         |        [[]]&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:02.690042 93 pinned_memory_manager.cc:240] Pinned memory pool is created at &#39;0x7f6104000000&#39; with size 268435456&#xA;fauxpilot-triton-1         | I0803 01:51:02.690461 93 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864&#xA;fauxpilot-triton-1         | I0803 01:51:02.692434 93 model_repository_manager.cc:1191] loading: fastertransformer:1&#xA;fauxpilot-triton-1         | I0803 01:51:02.936798 93 libfastertransformer.cc:1226] TRITONBACKEND_Initialize: fastertransformer&#xA;fauxpilot-triton-1         | I0803 01:51:02.936818 93 libfastertransformer.cc:1236] Triton TRITONBACKEND API version: 1.10&#xA;fauxpilot-triton-1         | I0803 01:51:02.936821 93 libfastertransformer.cc:1242] &#39;fastertransformer&#39; TRITONBACKEND API version: 1.10&#xA;fauxpilot-triton-1         | I0803 01:51:02.936850 93 libfastertransformer.cc:1274] TRITONBACKEND_ModelInitialize: fastertransformer (version 1)&#xA;fauxpilot-triton-1         | W0803 01:51:02.937855 93 libfastertransformer.cc:149] model configuration:&#xA;fauxpilot-triton-1         | {&#xA;[... lots more output trimmed ...]&#xA;fauxpilot-triton-1         | I0803 01:51:04.711929 93 libfastertransformer.cc:321] After Loading Model:&#xA;fauxpilot-triton-1         | I0803 01:51:04.712427 93 libfastertransformer.cc:537] Model instance is created on GPU NVIDIA RTX A6000&#xA;fauxpilot-triton-1         | I0803 01:51:04.712694 93 model_repository_manager.cc:1345] successfully loaded &#39;fastertransformer&#39; version 1&#xA;fauxpilot-triton-1         | I0803 01:51:04.712841 93 server.cc:556] &#xA;fauxpilot-triton-1         | +------------------+------+&#xA;fauxpilot-triton-1         | | Repository Agent | Path |&#xA;fauxpilot-triton-1         | +------------------+------+&#xA;fauxpilot-triton-1         | +------------------+------+&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:04.712916 93 server.cc:583] &#xA;fauxpilot-triton-1         | +-------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | | Backend           | Path                                                                        | Config                                                                                                                                                         |&#xA;fauxpilot-triton-1         | +-------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | | fastertransformer | /opt/tritonserver/backends/fastertransformer/libtriton_fastertransformer.so | {&#34;cmdline&#34;:{&#34;auto-complete-config&#34;:&#34;false&#34;,&#34;min-compute-capability&#34;:&#34;6.000000&#34;,&#34;backend-directory&#34;:&#34;/opt/tritonserver/backends&#34;,&#34;default-max-batch-size&#34;:&#34;4&#34;}} |&#xA;fauxpilot-triton-1         | +-------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:04.712959 93 server.cc:626] &#xA;fauxpilot-triton-1         | +-------------------+---------+--------+&#xA;fauxpilot-triton-1         | | Model             | Version | Status |&#xA;fauxpilot-triton-1         | +-------------------+---------+--------+&#xA;fauxpilot-triton-1         | | fastertransformer | 1       | READY  |&#xA;fauxpilot-triton-1         | +-------------------+---------+--------+&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:04.738989 93 metrics.cc:650] Collecting metrics for GPU 0: NVIDIA RTX A6000&#xA;fauxpilot-triton-1         | I0803 01:51:04.739373 93 tritonserver.cc:2159] &#xA;fauxpilot-triton-1         | +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | | Option                           | Value                                                                                                                                                                                        |&#xA;fauxpilot-triton-1         | +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | | server_id                        | triton                                                                                                                                                                                       |&#xA;fauxpilot-triton-1         | | server_version                   | 2.23.0                                                                                                                                                                                       |&#xA;fauxpilot-triton-1         | | server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |&#xA;fauxpilot-triton-1         | | model_repository_path[0]         | /model                                                                                                                                                                                       |&#xA;fauxpilot-triton-1         | | model_control_mode               | MODE_NONE                                                                                                                                                                                    |&#xA;fauxpilot-triton-1         | | strict_model_config              | 1                                                                                                                                                                                            |&#xA;fauxpilot-triton-1         | | rate_limit                       | OFF                                                                                                                                                                                          |&#xA;fauxpilot-triton-1         | | pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |&#xA;fauxpilot-triton-1         | | cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                     |&#xA;fauxpilot-triton-1         | | response_cache_byte_size         | 0                                                                                                                                                                                            |&#xA;fauxpilot-triton-1         | | min_supported_compute_capability | 6.0                                                                                                                                                                                          |&#xA;fauxpilot-triton-1         | | strict_readiness                 | 1                                                                                                                                                                                            |&#xA;fauxpilot-triton-1         | | exit_timeout                     | 30                                                                                                                                                                                           |&#xA;fauxpilot-triton-1         | +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:04.740423 93 grpc_server.cc:4587] Started GRPCInferenceService at 0.0.0.0:8001&#xA;fauxpilot-triton-1         | I0803 01:51:04.740608 93 http_server.cc:3303] Started HTTPService at 0.0.0.0:8000&#xA;fauxpilot-triton-1         | I0803 01:51:04.781561 93 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;API&lt;/h2&gt; &#xA;&lt;p&gt;Once everything is up and running, you should have a server listening for requests on &lt;code&gt;http://localhost:5000&lt;/code&gt;. You can now talk to it using the standard &lt;a href=&#34;https://beta.openai.com/docs/api-reference/&#34;&gt;OpenAI API&lt;/a&gt; (although the full API isn&#39;t implemented yet). For example, from Python, using the &lt;a href=&#34;https://github.com/openai/openai-python&#34;&gt;OpenAI Python bindings&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;$ ipython&#xA;Python 3.8.10 (default, Mar 15 2022, 12:22:08) &#xA;Type &#39;copyright&#39;, &#39;credits&#39; or &#39;license&#39; for more information&#xA;IPython 8.2.0 -- An enhanced Interactive Python. Type &#39;?&#39; for help.&#xA;&#xA;In [1]: import openai&#xA;&#xA;In [2]: openai.api_key = &#39;dummy&#39;&#xA;&#xA;In [3]: openai.api_base = &#39;http://127.0.0.1:5000/v1&#39;&#xA;&#xA;In [4]: result = openai.Completion.create(engine=&#39;codegen&#39;, prompt=&#39;def hello&#39;, max_tokens=16, temperature=0.1, stop=[&#34;\n\n&#34;])&#xA;&#xA;In [5]: result&#xA;Out[5]: &#xA;&amp;lt;OpenAIObject text_completion id=cmpl-6hqu8Rcaq25078IHNJNVooU4xLY6w at 0x7f602c3d2f40&amp;gt; JSON: {&#xA;  &#34;choices&#34;: [&#xA;    {&#xA;      &#34;finish_reason&#34;: &#34;stop&#34;,&#xA;      &#34;index&#34;: 0,&#xA;      &#34;logprobs&#34;: null,&#xA;      &#34;text&#34;: &#34;() {\n    return \&#34;Hello, World!\&#34;;\n}&#34;&#xA;    }&#xA;  ],&#xA;  &#34;created&#34;: 1659492191,&#xA;  &#34;id&#34;: &#34;cmpl-6hqu8Rcaq25078IHNJNVooU4xLY6w&#34;,&#xA;  &#34;model&#34;: &#34;codegen&#34;,&#xA;  &#34;object&#34;: &#34;text_completion&#34;,&#xA;  &#34;usage&#34;: {&#xA;    &#34;completion_tokens&#34;: 15,&#xA;    &#34;prompt_tokens&#34;: 2,&#xA;    &#34;total_tokens&#34;: 17&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Copilot Plugin&lt;/h2&gt; &#xA;&lt;p&gt;Perhaps more excitingly, you can configure the official &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=GitHub.copilot&#34;&gt;VSCode Copilot plugin&lt;/a&gt; to use your local server. Just edit your &lt;code&gt;settings.json&lt;/code&gt; to add:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;    &#34;github.copilot.advanced&#34;: {&#xA;        &#34;debug.overrideEngine&#34;: &#34;codegen&#34;,&#xA;        &#34;debug.testOverrideProxyUrl&#34;: &#34;http://localhost:5000&#34;,&#xA;        &#34;debug.overrideProxyUrl&#34;: &#34;http://localhost:5000&#34;&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And you should be able to use Copilot with your own locally hosted suggestions! Of course, probably a lot of stuff is subtly broken. In particular, the probabilities returned by the server are partly fake. Fixing this would require changing FasterTransformer so that it can return log-probabilities for the top k tokens rather that just the chosen token.&lt;/p&gt; &#xA;&lt;p&gt;Have fun!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>YDHCUI/manjusaka</title>
    <updated>2022-08-06T01:30:35Z</updated>
    <id>tag:github.com,2022-08-06:/YDHCUI/manjusaka</id>
    <link href="https://github.com/YDHCUI/manjusaka" rel="alternate"></link>
    <summary type="html">&lt;p&gt;牛屎花 一款C2远控&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CowExcrementFlower&lt;/h1&gt; &#xA;&lt;p&gt;牛屎花 一款基于WEB界面的仿CobaltStrike C2远控&lt;/p&gt; &#xA;&lt;p&gt;##系统架构： &lt;img src=&#34;https://user-images.githubusercontent.com/46884495/159195361-cc3b75f1-ab5e-425b-a3b3-65d65878c048.jpg&#34; alt=&#34;164759109&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;##操作截图：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/46884495/161952357-d9a86804-0b52-4866-b1f9-2148746f744d.png&#34; alt=&#34;微信截图_20220406181015&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/46884495/159195383-348e6fb1-3516-40be-9522-5c562e626d36.png&#34; alt=&#34;微信截图_20220318162038&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/46884495/159195398-bf7b2cd1-cbae-4d23-a101-fc8311c24949.png&#34; alt=&#34;微信截图_20220318162241&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/46884495/161950618-aadf6240-5672-4756-b103-f8be08f55747.png&#34; alt=&#34;微信截图_20220406180114&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;使用方法&lt;/h2&gt; &#xA;&lt;p&gt;./manjusaka -h&lt;/p&gt; &#xA;&lt;p&gt;默认 ./manjusaka -h vpsip&lt;/p&gt; &#xA;&lt;h2&gt;更新&lt;/h2&gt; &#xA;&lt;h3&gt;v0.3&lt;/h3&gt; &#xA;&lt;p&gt;1、实现截屏、密码获取功能。(仅window)&lt;/p&gt; &#xA;&lt;p&gt;2、修复cmd界面不能黏贴的bug。&lt;/p&gt; &#xA;&lt;p&gt;3、修复项目不能暂停的bug。&lt;/p&gt; &#xA;&lt;p&gt;4、自动创建data文件夹。&lt;/p&gt; &#xA;&lt;h3&gt;v0.2&lt;/h3&gt; &#xA;&lt;p&gt;1、修改网络协议使流量加密。&lt;/p&gt; &#xA;&lt;p&gt;2、加入本地文件上传下载功能。&lt;/p&gt; &#xA;&lt;p&gt;3、修复shell界面位移bug。&lt;/p&gt; &#xA;&lt;h3&gt;v0.1&lt;/h3&gt; &#xA;&lt;p&gt;1、实现基础远控功能。&lt;/p&gt; &#xA;&lt;h2&gt;交流建议：&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/46884495/163297490-65edf3fb-6f87-407f-bc6f-161e404acdc1.png&#34; alt=&#34;1649900653(1)&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>