<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-04T01:29:14Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>karpathy/nanoGPT</title>
    <updated>2023-01-04T01:29:14Z</updated>
    <id>tag:github.com,2023-01-04:/karpathy/nanoGPT</id>
    <link href="https://github.com/karpathy/nanoGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nanoGPT&lt;/h1&gt; &#xA;&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs. It&#39;s a re-write of &lt;a href=&#34;https://github.com/karpathy/minGPT&#34;&gt;minGPT&lt;/a&gt;, which I think became too complicated, and which I am hesitant to now touch. Still under active development, currently working to reproduce GPT-2 on OpenWebText dataset. The code itself aims by design to be plain and readable: &lt;code&gt;train.py&lt;/code&gt; is a ~300-line boilerplate training loop and &lt;code&gt;model.py&lt;/code&gt; a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That&#39;s it.&lt;/p&gt; &#xA;&lt;h2&gt;install&lt;/h2&gt; &#xA;&lt;p&gt;Dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org&#34;&gt;pytorch&lt;/a&gt; &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://numpy.org/install/&#34;&gt;numpy&lt;/a&gt; &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install datasets&lt;/code&gt; for huggingface datasets &amp;lt;3 (if you want to download + preprocess OpenWebText)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install tiktoken&lt;/code&gt; for OpenAI&#39;s fast bpe code &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install wandb&lt;/code&gt; for optional logging &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install tqdm&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install networkx&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;usage&lt;/h2&gt; &#xA;&lt;p&gt;To render a dataset we first tokenize some documents into one simple long 1D array of indices. E.g. for OpenWebText see:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd data/openwebtext&#xA;$ python prepare.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To download and tokenize the &lt;a href=&#34;https://huggingface.co/datasets/openwebtext&#34;&gt;OpenWebText&lt;/a&gt; dataset. This will create a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; which holds the GPT2 BPE token ids in one sequence, stored as raw uint16 bytes. Then we&#39;re ready to kick off training. The training script currently by default tries to reproduce the smallest GPT-2 released by OpenAI, i.e. the 124M version of GPT-2. We can demo train as follows on a single device, though I encourage you to read the code and see all of the settings and paths up top in the file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train using PyTorch Distributed Data Parallel (DDP) run the script with torchrun. For example to train on a node with 4 GPUs run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ torchrun --standalone --nproc_per_node=4 train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To my knowledge, running this with the current script with the GPT-2 hyperparameters should reproduce the GPT-2 result, provided that OpenWebText ~= WebText. I&#39;d like to make the code more efficient before attempting to go there. Once some checkpoints are written to the output directory (e.g. &lt;code&gt;./out&lt;/code&gt; by default), we can sample from the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python sample.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training on 1 A100 40GB GPU overnight currently gets loss ~3.74, training on 4 gets ~3.60. Random chance at init is -ln(1/50257) = 10.82. Which brings us to baselines:&lt;/p&gt; &#xA;&lt;h2&gt;finetuning&lt;/h2&gt; &#xA;&lt;p&gt;For an example of how to finetune a GPT on new text go to &lt;code&gt;data/shakespeare&lt;/code&gt; and look at &lt;code&gt;prepare.py&lt;/code&gt; to download the tiny shakespeare dataset and render it into a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt;. Unlike OpenWebText this will run in seconds. Finetuning takes very little time, e.g. on a single GPT just a few minutes. Run an example finetuning like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python train.py finetune_shakespeare&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will load the config parameter overrides in &lt;code&gt;config/finetune_shakespeare.py&lt;/code&gt; (I didn&#39;t tune them much though). Basically, we initialize from a GPT2 checkpoint with &lt;code&gt;init_from&lt;/code&gt; and train as normal, except shorter and with a small learning rate. The best checkpoint (lowest validation loss) will be in the &lt;code&gt;out_dir&lt;/code&gt; directory, e.g. in &lt;code&gt;out-shakespeare&lt;/code&gt; by default, per the config file. You can then run the code in &lt;code&gt;sample.py&lt;/code&gt; to generate infinite Shakespeare. Note that you&#39;ll have to edit it to point to the correct &lt;code&gt;out_dir&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;baselines&lt;/h2&gt; &#xA;&lt;p&gt;OpenAI GPT-2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python train.py eval_gpt2&#xA;$ python train.py eval_gpt2_medium&#xA;$ python train.py eval_gpt2_large&#xA;$ python train.py eval_gpt2_xl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and observe the following losses on train and val:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;params&lt;/th&gt; &#xA;   &lt;th&gt;train loss&lt;/th&gt; &#xA;   &lt;th&gt;val loss&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2&lt;/td&gt; &#xA;   &lt;td&gt;124M&lt;/td&gt; &#xA;   &lt;td&gt;3.11&lt;/td&gt; &#xA;   &lt;td&gt;3.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2-medium&lt;/td&gt; &#xA;   &lt;td&gt;350M&lt;/td&gt; &#xA;   &lt;td&gt;2.85&lt;/td&gt; &#xA;   &lt;td&gt;2.84&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2-large&lt;/td&gt; &#xA;   &lt;td&gt;774M&lt;/td&gt; &#xA;   &lt;td&gt;2.66&lt;/td&gt; &#xA;   &lt;td&gt;2.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2-xl&lt;/td&gt; &#xA;   &lt;td&gt;1558M&lt;/td&gt; &#xA;   &lt;td&gt;2.56&lt;/td&gt; &#xA;   &lt;td&gt;2.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;I briefly tried finetuning gpt2 a bit more on our OWT and didn&#39;t notice dramatic improvements, suggesting that OWT is not much much different from WT in terms of the data distribution, but this needs a bit more thorough attempt once the code is in a better place.&lt;/p&gt; &#xA;&lt;h2&gt;benchmarking&lt;/h2&gt; &#xA;&lt;p&gt;For model benchmarking &lt;code&gt;bench.py&lt;/code&gt; might be useful. It&#39;s identical what happens in the meat of the training loop of &lt;code&gt;train.py&lt;/code&gt;, but omits much of the other complexities.&lt;/p&gt; &#xA;&lt;h2&gt;efficiency notes&lt;/h2&gt; &#xA;&lt;p&gt;Code by default now uses &lt;a href=&#34;https://pytorch.org/get-started/pytorch-2.0/&#34;&gt;PyTorch 2.0&lt;/a&gt;. At the time of writing (Dec 29, 2022) this makes &lt;code&gt;torch.compile()&lt;/code&gt; available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!&lt;/p&gt; &#xA;&lt;h2&gt;todos&lt;/h2&gt; &#xA;&lt;p&gt;A few todos I&#39;m aware of:&lt;/p&gt; &#xA;&lt;p&gt;Optimizations&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Additional optimizations to the running time&lt;/li&gt; &#xA; &lt;li&gt;Investigate need for an actual Data Loader with a dedicated worker process for data&lt;/li&gt; &#xA; &lt;li&gt;Look into more efficient fused optimizers (e.g. apex)&lt;/li&gt; &#xA; &lt;li&gt;Re-evaluate use of flash attention (previously I wasn&#39;t able to get the forward pass to match up so I took it out)&lt;/li&gt; &#xA; &lt;li&gt;CUDA Graphs?&lt;/li&gt; &#xA; &lt;li&gt;Investigate potential speedups from Lightning or huggingface Accelerate&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Features / APIs&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add back fp16 support? (would need to also add back gradient scaler)&lt;/li&gt; &#xA; &lt;li&gt;Add CPU support&lt;/li&gt; &#xA; &lt;li&gt;Finetune the finetuning script, I think the hyperparams are not great&lt;/li&gt; &#xA; &lt;li&gt;Replace poor man&#39;s configurator, and make sample.py configurable...&lt;/li&gt; &#xA; &lt;li&gt;Report and track other metrics e.g. perplexity, num_tokens, MFU, ...&lt;/li&gt; &#xA; &lt;li&gt;Eval zero-shot perplexities on PTB, WikiText, other related benchmarks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Suspiciousness&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Current initialization (PyTorch default) departs from GPT-2. In a very quick experiment I found it to be superior to the one suggested in the papers, but that can&#39;t be right?&lt;/li&gt; &#xA; &lt;li&gt;I don&#39;t currently seem to need gradient clipping but it is very often used (?). Nothing is exploding so far at these scales but maybe I&#39;m laeving performance on the table. Evaluate with/without.&lt;/li&gt; &#xA; &lt;li&gt;I am still not 100% confident that my GPT-2 small reproduction hyperparameters are good, if someone has reproduced GPT-2 I&#39;d be eager to exchange notes ty&lt;/li&gt; &#xA; &lt;li&gt;I keep seeing different values cited for weight decay and AdamW betas, look into&lt;/li&gt; &#xA; &lt;li&gt;I can&#39;t exactly reproduce Chinchilla paper results, see &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nanoGPT/master/scaling_laws.ipynb&#34;&gt;scaling_laws.ipynb&lt;/a&gt; notebook&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Results&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Actually reproduce GPT-2 results and have clean configs that reproduce the result. It was estimated ~3 years ago that the training cost of 1.5B model was ~$50K (?). Sounds a bit too high.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Thank you &lt;a href=&#34;https://lambdalabs.com&#34;&gt;Lambda labs&lt;/a&gt; for supporting the training costs of nanoGPT experiments.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>godly-devotion/MochiDiffusion</title>
    <updated>2023-01-04T01:29:14Z</updated>
    <id>tag:github.com,2023-01-04:/godly-devotion/MochiDiffusion</id>
    <link href="https://github.com/godly-devotion/MochiDiffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run Stable Diffusion on Apple Silicon Macs natively&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img height=&#34;256&#34; src=&#34;https://github.com/godly-devotion/MochiDiffusion/raw/main/Mochi Diffusion/Assets.xcassets/AppIcon.appiconset/AppIcon.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Mochi Diffusion&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Run Stable Diffusion on Apple Silicon Macs natively&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/godly-devotion/MochiDiffusion/raw/main/README.md&#34;&gt;English&lt;/a&gt;, &lt;a href=&#34;https://github.com/godly-devotion/MochiDiffusion/raw/main/README.ko.md&#34;&gt;한국어&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/godly-devotion/MochiDiffusion/main/.github/images/screenshot.png&#34; alt=&#34;Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;This app uses &lt;a href=&#34;https://github.com/apple/ml-stable-diffusion&#34;&gt;Apple&#39;s Core ML Stable Diffusion implementation&lt;/a&gt; to achieve maximum performance and speed on Apple Silicon based Macs while reducing memory requirements.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extremely fast and memory efficient (~150MB with Neural Engine)&lt;/li&gt; &#xA; &lt;li&gt;Runs well on all Apple Silicon Macs by fully utilizing Neural Engine&lt;/li&gt; &#xA; &lt;li&gt;Generate images locally and completely offline&lt;/li&gt; &#xA; &lt;li&gt;Generated images are saved with prompt info inside EXIF metadata&lt;/li&gt; &#xA; &lt;li&gt;Convert generated images to high resolution (using RealESRGAN)&lt;/li&gt; &#xA; &lt;li&gt;Use custom Stable Diffusion Core ML models&lt;/li&gt; &#xA; &lt;li&gt;No worries about pickled models&lt;/li&gt; &#xA; &lt;li&gt;macOS native app using SwiftUI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;Download the latest version from the &lt;a href=&#34;https://github.com/godly-devotion/MochiDiffusion/releases&#34;&gt;releases&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h2&gt;Running&lt;/h2&gt; &#xA;&lt;p&gt;When using a model for the very first time, it may take up to 30 seconds for the Neural Engine to compile a cached version. Afterwards, subsequent generations will be much faster.&lt;/p&gt; &#xA;&lt;h2&gt;Compute Unit&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;CPU &amp;amp; Neural Engine&lt;/code&gt; provides a good balance between speed and low memory usage&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CPU &amp;amp; GPU&lt;/code&gt; may be faster on M1 Max, Ultra and later but will use more memory&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Depending on the option chosen, you will need to use the correct model version (see Models section for details).&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;You will need to convert or download Core ML models in order to use Mochi Diffusion.&lt;/p&gt; &#xA;&lt;p&gt;A few models have been converted and uploaded &lt;a href=&#34;https://huggingface.co/godly-devotion&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/apple/ml-stable-diffusion#-converting-models-to-core-ml&#34;&gt;Convert&lt;/a&gt; or download Core ML models &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;split_einsum&lt;/code&gt; version is compatible with all compute unit options including Neural Engine&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;original&lt;/code&gt; version is only compatible with &lt;code&gt;CPU &amp;amp; GPU&lt;/code&gt; option&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;By default, the app&#39;s working directory will be created under the Documents folder. This location can be customized under Settings&lt;/li&gt; &#xA; &lt;li&gt;In the working folder, create a new folder with the name you&#39;d like displayed in the app then move or extract the converted models here&lt;/li&gt; &#xA; &lt;li&gt;Your directory should look like this: &lt;code&gt;~/Documents/MochiDiffusion/models/[Model Folder Name]/[Model&#39;s Files]&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apple Silicon (M1 and later)&lt;/li&gt; &#xA; &lt;li&gt;macOS Ventura 13.1 and later&lt;/li&gt; &#xA; &lt;li&gt;Xcode 14.2 (to build)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Privacy&lt;/h2&gt; &#xA;&lt;p&gt;All generation happens locally and absolutely nothing is sent to the cloud.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Mochi Diffusion is always looking for contributions, whether it&#39;s through bug reports, code, or new translations.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If you find a bug, or would like to suggest a new feature or enhancement, try &lt;a href=&#34;https://github.com/godly-devotion/MochiDiffusion/discussions&#34;&gt;searching for your problem first&lt;/a&gt; as it helps avoid duplicates. If you can&#39;t find your issue, feel free to &lt;a href=&#34;https://github.com/godly-devotion/MochiDiffusion/discussions/new&#34;&gt;create a new discussion&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you&#39;re looking to contribute code, feel free to &lt;a href=&#34;https://github.com/godly-devotion/MochiDiffusion/pulls&#34;&gt;open a Pull Request&lt;/a&gt; or &lt;a href=&#34;https://github.com/godly-devotion/MochiDiffusion/discussions&#34;&gt;create a new discussion&lt;/a&gt; to talk about it first.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/apple/ml-stable-diffusion&#34;&gt;Apple&#39;s Core ML Stable Diffusion implementation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/swift-coreml-diffusers&#34;&gt;HuggingFace&#39;s Swift UI sample implementation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;App Icon by &lt;a href=&#34;https://github.com/Zabriskije&#34;&gt;Zabriskije&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>gluon-framework/gluon</title>
    <updated>2023-01-04T01:29:14Z</updated>
    <id>tag:github.com,2023-01-04:/gluon-framework/gluon</id>
    <link href="https://github.com/gluon-framework/gluon" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A framework for creating &#34;desktop apps&#34; from websites, using system installed browsers (not webviews) and NodeJS&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gluon&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://choosealicense.com/licenses/mit/l&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/CanadaHonk&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/sponsors/CanadaHonk?label=Sponsors&amp;amp;logo=github&#34; alt=&#34;GitHub Sponsors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/RFtUCA8fST&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1051940602704564244.svg?label=&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=7389D8&amp;amp;labelColor=6A7EC2&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gluon is a framework for creating &#34;desktop apps&#34; from websites&lt;/strong&gt;, using &lt;strong&gt;system installed browsers&lt;/strong&gt; &lt;em&gt;(not webviews)&lt;/em&gt; and NodeJS, differing a lot from other existing active projects - opening up innovation and allowing some major advantages. Instead of other similar frameworks bundling a browser like Chromium or using webviews (like Edge Webview2 on Windows), &lt;strong&gt;Gluon just uses system installed browsers&lt;/strong&gt; like Chrome, Edge, Firefox, etc. Gluon supports Chromium &lt;em&gt;&lt;strong&gt;and Firefox&lt;/strong&gt;&lt;/em&gt; based browsers as the frontend, while Gluon&#39;s backend uses NodeJS to be versatile and easy to develop (also allowing easy learning from other popular frameworks like Electron by using the same-ish stack).&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Uses normal system installed browsers&lt;/strong&gt; - allows user choice by &lt;strong&gt;supporting most Chromium &lt;em&gt;and Firefox&lt;/em&gt;&lt;/strong&gt; based browsers, no webviews&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tiny bundle sizes&lt;/strong&gt; - &amp;lt;1MB using system Node, &amp;lt;10MB when bundling it&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chromium &lt;em&gt;and Firefox&lt;/em&gt; supported as browser engine&lt;/strong&gt;, unlike any other active framework&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Minimal and easy to use&lt;/strong&gt; - Gluon has a simple yet powerful API to make apps with a Node backend&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast build times&lt;/strong&gt; - Gluon has build times under 1 second on most machines for small projects&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Actively developed&lt;/strong&gt; and &lt;strong&gt;listening to feedback&lt;/strong&gt; - new updates are coming around weekly, quickly adding unplanned requested features if liked by the community (like Firefox support!)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lower memory usage&lt;/strong&gt; - compared to most other frameworks Gluon should have a slightly lower average memory usage by using browser flags to squeeze out more performance&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- - **No forks needed** - Gluon doesn&#39;t need forks of Node or Chromium/etc to use them, it just uses normal versions --&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/19228318/210174682-b261dba0-8b3c-4ca0-8093-aeeb9fdbc52d.png&#34; alt=&#34;Gluworld Screenshot showing Chrome Canary and Firefox Nightly being used at once.&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Trying Gluon&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone &lt;a href=&#34;https://github.com/gluon-framework/examples&#34;&gt;the Gluon examples repo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Inside of &lt;code&gt;gluworld&lt;/code&gt;, run &lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Now do &lt;code&gt;node .&lt;/code&gt; to run it!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Shell example&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ git clone https://github.com/gluon-framework/examples.git&#xA;$ cd examples&#xA;$ cd gluworld&#xA;$ npm install&#xA;...&#xA;$ node .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;Gluon is currently &lt;strong&gt;barely a month old&lt;/strong&gt;, so is still in an &lt;strong&gt;early and experimental state&lt;/strong&gt;. But it works and shows (in my opinion) potential! I am open to opinions, suggestions, feedback, ideas, etc. Currently you cannot easily test it yourself. If you&#39;re interested and want to talk to me and others about Gluon, you can &lt;a href=&#34;https://discord.gg/RFtUCA8fST&#34;&gt;join our Discord server&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Specific feature statuses&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Feature&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Using Chromium based browsers&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stable&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Using Firefox based browsers&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Experimental&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Web-Node IPC&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stable&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Idle API&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Experimental&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Using other JS runtimes (&lt;a href=&#34;https://github.com/gluon-framework/gluon/tree/deno&#34;&gt;Deno&lt;/a&gt;/&lt;a href=&#34;https://github.com/gluon-framework/gluon/tree/bun&#34;&gt;Bun&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Experimental&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Ecosystem&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gluon-framework/gluon&#34;&gt;Gluon&lt;/a&gt;: the Gluon framework (NodeJS)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gluon-framework/glugun&#34;&gt;Glugun&lt;/a&gt;: builds Gluon apps into releasable builds with optional bundling (soon)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Apps&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gluon-framework/examples/tree/main/gluworld&#34;&gt;Gluworld&lt;/a&gt;: Hello World demo app with version info shown&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gluon-framework/examples/tree/main/gludoom&#34;&gt;Gludoom&lt;/a&gt;: Doom running as WASM, made into a native looking app with Gluon&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gluon-framework/examples/tree/main/glucord&#34;&gt;Glucord&lt;/a&gt;: minimal desktop Discord client loading official webapp (demo/example)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;IPC API&lt;/h2&gt; &#xA;&lt;p&gt;Gluon has an easy to use, but powerful asynchronous IPC API. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;// In your website&#39;s JS&#xA;const reply = await Gluon.ipc.send(&#39;my type&#39;, { more: &#39;data&#39; });&#xA;console.log(reply); // { give: &#39;back&#39;, different: &#39;stuff&#39; }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;// In your Node backend&#xA;import * as Gluon from &#39;@gluon-framework/gluon&#39;;&#xA;const Window = await Gluon.open(...);&#xA;&#xA;Window.ipc.on(&#39;my type&#39;, data =&amp;gt; { // { more: &#39;data&#39; }&#xA;  return { give: &#39;back&#39;, different: &#39;stuff&#39; };&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Comparisons&lt;/h2&gt; &#xA;&lt;h3&gt;Internals&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Part&lt;/th&gt; &#xA;   &lt;th&gt;Gluon&lt;/th&gt; &#xA;   &lt;th&gt;Electron&lt;/th&gt; &#xA;   &lt;th&gt;Tauri&lt;/th&gt; &#xA;   &lt;th&gt;Neutralinojs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Frontend&lt;/td&gt; &#xA;   &lt;td&gt;System installed Chromium &lt;em&gt;or Firefox&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Self-contained Chromium&lt;/td&gt; &#xA;   &lt;td&gt;System installed webview&lt;/td&gt; &#xA;   &lt;td&gt;System installed webview&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Backend&lt;/td&gt; &#xA;   &lt;td&gt;System installed &lt;em&gt;or bundled&lt;/em&gt; Node.JS&lt;/td&gt; &#xA;   &lt;td&gt;Self-contained Node.JS&lt;/td&gt; &#xA;   &lt;td&gt;Native (Rust)&lt;/td&gt; &#xA;   &lt;td&gt;Native (Any)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IPC&lt;/td&gt; &#xA;   &lt;td&gt;Window object&lt;/td&gt; &#xA;   &lt;td&gt;Preload&lt;/td&gt; &#xA;   &lt;td&gt;Window object&lt;/td&gt; &#xA;   &lt;td&gt;Window object&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Status&lt;/td&gt; &#xA;   &lt;td&gt;Early in development&lt;/td&gt; &#xA;   &lt;td&gt;Production ready&lt;/td&gt; &#xA;   &lt;td&gt;Usable&lt;/td&gt; &#xA;   &lt;td&gt;Usable&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ecosystem&lt;/td&gt; &#xA;   &lt;td&gt;Integrated&lt;/td&gt; &#xA;   &lt;td&gt;Distributed&lt;/td&gt; &#xA;   &lt;td&gt;Integrated&lt;/td&gt; &#xA;   &lt;td&gt;Integrated&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Benchmark / Stats&lt;/h3&gt; &#xA;&lt;p&gt;Basic (plain HTML) Hello World demo, measured on up to date Windows 10, on my machine (your experience will probably differ). Used latest stable versions of all frameworks as of 9th Dec 2022. (You shouldn&#39;t actually use random stats in benchmarks to compare frameworks, this is more so you know what Gluon is like compared to other similar projects.)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Stat&lt;/th&gt; &#xA;   &lt;th&gt;Gluon&lt;/th&gt; &#xA;   &lt;th&gt;Electron&lt;/th&gt; &#xA;   &lt;th&gt;Tauri&lt;/th&gt; &#xA;   &lt;th&gt;Neutralinojs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Build Size&lt;/td&gt; &#xA;   &lt;td&gt;&amp;lt;1MB[^system][^gluon][^1]&lt;/td&gt; &#xA;   &lt;td&gt;~220MB&lt;/td&gt; &#xA;   &lt;td&gt;~1.8MB[^system]&lt;/td&gt; &#xA;   &lt;td&gt;~2.6MB[^system]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Memory Usage&lt;/td&gt; &#xA;   &lt;td&gt;~80MB[^gluon]&lt;/td&gt; &#xA;   &lt;td&gt;~100MB&lt;/td&gt; &#xA;   &lt;td&gt;~90MB&lt;/td&gt; &#xA;   &lt;td&gt;~90MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Backend[^2] Memory Usage&lt;/td&gt; &#xA;   &lt;td&gt;~13MB[^gluon] (Node)&lt;/td&gt; &#xA;   &lt;td&gt;~22MB (Node)&lt;/td&gt; &#xA;   &lt;td&gt;~3MB (Native)&lt;/td&gt; &#xA;   &lt;td&gt;~3MB (Native)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Build Time&lt;/td&gt; &#xA;   &lt;td&gt;~0.7s[^3]&lt;/td&gt; &#xA;   &lt;td&gt;~20s[^4]&lt;/td&gt; &#xA;   &lt;td&gt;~120s[^5]&lt;/td&gt; &#xA;   &lt;td&gt;~2s[^3][^6]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Extra info: All HTML/CSS/JS is unminified (including Gluon). Built in release configuration. All binaries were left as compiled with common size optimizations enabled for that language, no stripping/packing done.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;[^system]: Does not include system installed components. [^gluon]: Using Chrome as system browser. Early/WIP data, may change in future.&lt;/p&gt; &#xA;&lt;p&gt;[^1]: &lt;em&gt;How is Gluon so small?&lt;/em&gt; Since NodeJS is expected as a system installed component, it is &#34;just&#34; bundled and minified Node code. [^2]: Backend like non-Web (not Chromium/WebView2/etc). [^3]: Includes Node.JS spinup time. [^4]: Built for win32 zip (not Squirrel) as a fairer comparison. [^5]: Cold build (includes deps compiling) in release mode. [^6]: Using &lt;code&gt;neu build -r&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>