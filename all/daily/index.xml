<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-06T01:29:01Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>GoogleCloudPlatform/kubectl-ai</title>
    <updated>2025-05-06T01:29:01Z</updated>
    <id>tag:github.com,2025-05-06:/GoogleCloudPlatform/kubectl-ai</id>
    <link href="https://github.com/GoogleCloudPlatform/kubectl-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI powered Kubernetes Assistant&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;kubectl-ai&lt;/h1&gt; &#xA;&lt;p&gt;kubectl-ai is an AI powered kubernetes agent that runs in your terminal.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/kubectl-ai/main/.github/kubectl-ai.gif&#34; alt=&#34;kubectl-ai demo GIF using: kubectl-ai &amp;quot;how&#39;s nginx app doing in my cluster&amp;quot;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;First, ensure that kubectl is installed and configured.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the latest release from the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubectl-ai/releases/latest&#34;&gt;releases page&lt;/a&gt; for your target machine.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Untar the release, make the binary executable and move it to a directory in your $PATH (as shown below).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ tar -zxvf kubectl-ai_Darwin_arm64.tar.gz&#xA;$ chmod a+x kubectl-ai&#xA;$ sudo mv kubectl-ai /usr/local/bin/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;h4&gt;Using Gemini (Default)&lt;/h4&gt; &#xA;&lt;p&gt;Set your Gemini API key as an environment variable. If you don&#39;t have a key, get one from &lt;a href=&#34;https://aistudio.google.com&#34;&gt;Google AI Studio&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export GEMINI_API_KEY=your_api_key_here&#xA;kubectl-ai&#xA;&#xA;# Use different gemini model&#xA;kubectl-ai --model gemini-2.5-pro-exp-03-25&#xA;&#xA;# Use 2.5 flash (faster) model&#xA;kubectl-ai --quiet --model gemini-2.5-flash-preview-04-17 &#34;check logs for nginx app in hello namespace&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using AI models running locally (ollama or llamacpp)&lt;/h4&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;kubectl-ai&lt;/code&gt; with AI models running locally. &lt;code&gt;kubectl-ai&lt;/code&gt; supports &lt;a href=&#34;https://ollama.com/&#34;&gt;ollama&lt;/a&gt; and &lt;a href=&#34;https://github.com/ggml-org/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; to use the AI models running locally.&lt;/p&gt; &#xA;&lt;p&gt;An example of using Google&#39;s &lt;code&gt;gemma3&lt;/code&gt; model with &lt;code&gt;ollama&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# assuming ollama is already running and you have pulled one of the gemma models&#xA;# ollama pull gemma3:12b-it-qat&#xA;&#xA;# enable-tool-use-shim because models require special prompting to enable tool calling&#xA;kubectl-ai --llm-provider ollama --model gemma3:12b-it-qat --enable-tool-use-shim&#xA;&#xA;# you can use `models` command to discover the locally available models&#xA;&amp;gt;&amp;gt; models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using Azure OpenAI&lt;/h4&gt; &#xA;&lt;p&gt;You can also use Azure OpenAI deployment by setting your OpenAI API key and specifying the provider:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export AZURE_OPENAI_API_KEY=your_azure_openai_api_key_here&#xA;export AZURE_OPENAI_ENDPOINT=https://your_azure_openai_endpoint_here&#xA;kubectl-ai --llm-provider=azopenai --model=your_azure_openai_deployment_name_here&#xA;# or&#xA;az login&#xA;kubectl-ai --llm-provider=openai://your_azure_openai_endpoint_here --model=your_azure_openai_deployment_name_here&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using OpenAI&lt;/h4&gt; &#xA;&lt;p&gt;You can also use OpenAI models by setting your OpenAI API key and specifying the provider:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=your_openai_api_key_here&#xA;kubectl-ai --llm-provider=openai --model=gpt-4.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note: &lt;code&gt;kubectl-ai&lt;/code&gt; supports AI models from &lt;code&gt;gemini&lt;/code&gt;, &lt;code&gt;vertexai&lt;/code&gt;, &lt;code&gt;azopenai&lt;/code&gt;, &lt;code&gt;openai&lt;/code&gt; and local LLM providers such as &lt;code&gt;ollama&lt;/code&gt; and &lt;code&gt;llamacpp&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Run interactively:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl-ai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The interactive mode allows you to have a chat with &lt;code&gt;kubectl-ai&lt;/code&gt;, asking multiple questions in sequence while maintaining context from previous interactions. Simply type your queries and press Enter to receive responses. To exit the interactive shell, type &lt;code&gt;exit&lt;/code&gt; or press Ctrl+C.&lt;/p&gt; &#xA;&lt;p&gt;Or, run with a task as input:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl-ai -quiet &#34;fetch logs for nginx app in hello namespace&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Combine it with other unix commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl-ai &amp;lt; query.txt&#xA;# OR&#xA;echo &#34;list pods in the default namespace&#34; | kubectl-ai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can even combine a positional argument with stdin input. The positional argument will be used as a prefix to the stdin content:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cat error.log | kubectl-ai &#34;explain the error&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Extras&lt;/h2&gt; &#xA;&lt;p&gt;You can use the following special keywords for specific actions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: Display the currently selected model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;models&lt;/code&gt;: List all available models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;version&lt;/code&gt;: Display the &lt;code&gt;kubectl-ai&lt;/code&gt; version.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;reset&lt;/code&gt;: Clear the conversational context.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;clear&lt;/code&gt;: Clear the terminal screen.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;exit&lt;/code&gt; or &lt;code&gt;quit&lt;/code&gt;: Terminate the interactive shell (Ctrl+C also works).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Invoking as kubectl plugin&lt;/h3&gt; &#xA;&lt;p&gt;Use it via the &lt;code&gt;kubectl&lt;/code&gt; plug interface like this: &lt;code&gt;kubectl ai&lt;/code&gt;. kubectl will find &lt;code&gt;kubectl-ai&lt;/code&gt; as long as it&#39;s in your PATH. For more information about plugins please see: &lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/&#34;&gt;https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Get information about pods in the default namespace&#xA;kubectl-ai -quiet &#34;show me all pods in the default namespace&#34;&#xA;&#xA;# Create a new deployment&#xA;kubectl-ai -quiet &#34;create a deployment named nginx with 3 replicas using the nginx:latest image&#34;&#xA;&#xA;# Troubleshoot issues&#xA;kubectl-ai -quiet &#34;double the capacity for the nginx app&#34;&#xA;&#xA;# Using Azure OpenAI instead of Gemini&#xA;kubectl-ai --llm-provider=azopenai --model=your_azure_openai_deployment_name_here -quiet &#34;scale the nginx deployment to 5 replicas&#34;&#xA;&#xA;# Using OpenAI instead of Gemini&#xA;kubectl-ai --llm-provider=openai --model=gpt-4.1 -quiet &#34;scale the nginx deployment to 5 replicas&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;kubectl-ai&lt;/code&gt; will process your query, execute the appropriate kubectl commands, and provide you with the results and explanations.&lt;/p&gt; &#xA;&lt;h2&gt;k8s-bench&lt;/h2&gt; &#xA;&lt;p&gt;kubectl-ai project includes &lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/kubectl-ai/main/k8s-bench/README.md&#34;&gt;k8s-bench&lt;/a&gt; - a benchmark to evaluate performance of different LLM models on kubernetes related tasks. Here is a summary from our last run:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Success&lt;/th&gt; &#xA;   &lt;th&gt;Fail&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gemini-2.5-flash-preview-04-17&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gemini-2.5-pro-preview-03-25&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gemma-3-27b-it&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;28&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/kubectl-ai/main/k8s-bench.md&#34;&gt;full report&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: This is not an officially supported Google product. This project is not eligible for the &lt;a href=&#34;https://bughunters.google.com/open-source-security&#34;&gt;Google Open Source Software Vulnerability Rewards Program&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>CapSoftware/Cap</title>
    <updated>2025-05-06T01:29:01Z</updated>
    <id>tag:github.com,2025-05-06:/CapSoftware/Cap</id>
    <link href="https://github.com/CapSoftware/Cap" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open source Loom alternative. Beautiful, shareable screen recordings.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;150&#34; height=&#34;150&#34; src=&#34;https://github.com/CapSoftware/Cap/raw/main/apps/desktop/src-tauri/icons/Square310x310Logo.png&#34; alt=&#34;Logo&#34;&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;&lt;b&gt;Cap&lt;/b&gt;&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; The open source Loom alternative. &lt;br&gt; &lt;a href=&#34;https://cap.so&#34;&gt;&lt;strong&gt;Cap.so »&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;b&gt;Downloads for &lt;/b&gt; &lt;a href=&#34;https://cap.so/download&#34;&gt;macOS &amp;amp; Windows&lt;/a&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://console.algora.io/org/CapSoftware/bounties?status=open&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https%3A%2F%2Fconsole.algora.io%2Fapi%2Fshields%2FCapSoftware%2Fbounties%3Fstatus%3Dopen&#34; alt=&#34;Open Bounties&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Cap is the open source alternative to Loom. It&#39;s a video messaging tool that allows you to record, edit and share videos in seconds.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/CapSoftware/Cap/refs/heads/main/apps/web/public/landing-cover.png&#34;&gt; &#xA;&lt;h1&gt;Cap Self Hosting&lt;/h1&gt; &#xA;&lt;p&gt;We&#39;re working on a self-hosting guide for Cap. This will include one-click deployment buttons for Vercel and Render, as well as an option to self host with Docker. Join the &lt;a href=&#34;https://discord.gg/y8gdQ3WRN3&#34;&gt;Cap Discord&lt;/a&gt; if you want to help contribute to this particular project.&lt;/p&gt; &#xA;&lt;h1&gt;Monorepo App Architecture&lt;/h1&gt; &#xA;&lt;p&gt;We use a combination of Rust, React (Next.js), TypeScript, Tauri, Drizzle (ORM), MySQL, TailwindCSS throughout this Turborepo powered monorepo.&lt;/p&gt; &#xA;&lt;h3&gt;Apps:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;desktop&lt;/code&gt;: A &lt;a href=&#34;https://tauri.app&#34;&gt;Tauri&lt;/a&gt; (Rust) app, using &lt;a href=&#34;https://start.solidjs.com&#34;&gt;SolidStart&lt;/a&gt; on the frontend.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;web&lt;/code&gt;: A &lt;a href=&#34;https://nextjs.org&#34;&gt;Next.js&lt;/a&gt; web app.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Packages:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;ui&lt;/code&gt;: A &lt;a href=&#34;https://reactjs.org&#34;&gt;React&lt;/a&gt; Shared component library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;utils&lt;/code&gt;: A &lt;a href=&#34;https://reactjs.org&#34;&gt;React&lt;/a&gt; Shared utility library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tsconfig&lt;/code&gt;: Shared &lt;code&gt;tsconfig&lt;/code&gt; configurations used throughout the monorepo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;database&lt;/code&gt;: A &lt;a href=&#34;https://reactjs.org&#34;&gt;React&lt;/a&gt; and &lt;a href=&#34;https://orm.drizzle.team/&#34;&gt;Drizzle ORM&lt;/a&gt; Shared database library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;config&lt;/code&gt;: &lt;code&gt;eslint&lt;/code&gt; configurations (includes &lt;code&gt;eslint-config-next&lt;/code&gt;, &lt;code&gt;eslint-config-prettier&lt;/code&gt; other configs used throughout the monorepo).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/CapSoftware/Cap/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more information. This guide is a work in progress, and is updated regularly as the app matures.&lt;/p&gt;</summary>
  </entry>
</feed>