<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-31T01:29:38Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mattermost-community/focalboard</title>
    <updated>2025-07-31T01:29:38Z</updated>
    <id>tag:github.com,2025-07-31:/mattermost-community/focalboard</id>
    <link href="https://github.com/mattermost-community/focalboard" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Focalboard is an open source, self-hosted alternative to Trello, Notion, and Asana.&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] This repository is currently not maintained. If you&#39;re interested in becoming a maintainer please &lt;a href=&#34;https://github.com/mattermost-community/focalboard/issues/5038&#34;&gt;let us know here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;This repository only contains standalone Focalboard. If you&#39;re looking for the Mattermost plugin please see &lt;a href=&#34;https://github.com/mattermost/mattermost-plugin-boards&#34;&gt;mattermost/mattermost-plugin-boards&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Focalboard&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/mattermost/focalboard/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;CI Status&#34;&gt; &lt;img src=&#34;https://github.com/mattermost/focalboard/actions/workflows/codeql-analysis.yml/badge.svg?sanitize=true&#34; alt=&#34;CodeQL&#34;&gt; &lt;img src=&#34;https://github.com/mattermost/focalboard/actions/workflows/dev-release.yml/badge.svg?sanitize=true&#34; alt=&#34;Dev Release&#34;&gt; &lt;img src=&#34;https://github.com/mattermost/focalboard/actions/workflows/prod-release.yml/badge.svg?sanitize=true&#34; alt=&#34;Prod Release&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mattermost-community/focalboard/main/website/site/static/img/hero.jpg&#34; alt=&#34;Focalboard&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Focalboard is an open source, multilingual, self-hosted project management tool that&#39;s an alternative to Trello, Notion, and Asana.&lt;/p&gt; &#xA;&lt;p&gt;It helps define, organize, track and manage work across individuals and teams. Focalboard comes in two editions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.focalboard.com/docs/personal-edition/desktop/&#34;&gt;Personal Desktop&lt;/a&gt;&lt;/strong&gt;: A standalone, single-user &lt;a href=&#34;https://apps.apple.com/app/apple-store/id1556908618?pt=2114704&amp;amp;ct=website&amp;amp;mt=8&#34;&gt;macOS&lt;/a&gt;, &lt;a href=&#34;https://www.microsoft.com/store/apps/9NLN2T0SX9VF?cid=website&#34;&gt;Windows&lt;/a&gt;, or &lt;a href=&#34;https://www.focalboard.com/download/personal-edition/desktop/#linux-desktop&#34;&gt;Linux&lt;/a&gt; desktop app for your own todos and personal projects.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.focalboard.com/download/personal-edition/ubuntu/&#34;&gt;Personal Server&lt;/a&gt;&lt;/strong&gt;: A standalone, multi-user server for development and personal use.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Try Focalboard&lt;/h2&gt; &#xA;&lt;h3&gt;Personal Desktop (Windows, Mac or Linux Desktop)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Download from the &lt;a href=&#34;https://www.microsoft.com/store/productId/9NLN2T0SX9VF&#34;&gt;Windows App Store&lt;/a&gt; or download &lt;code&gt;focalboard-win.zip&lt;/code&gt; from the &lt;a href=&#34;https://github.com/mattermost/focalboard/releases&#34;&gt;latest release&lt;/a&gt;, unpack, and run &lt;code&gt;Focalboard.exe&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mac&lt;/strong&gt;: Download from the &lt;a href=&#34;https://apps.apple.com/us/app/focalboard-insiders/id1556908618?mt=12&#34;&gt;Mac App Store&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linux Desktop&lt;/strong&gt;: Download &lt;code&gt;focalboard-linux.tar.gz&lt;/code&gt; from the &lt;a href=&#34;https://github.com/mattermost/focalboard/releases&#34;&gt;latest release&lt;/a&gt;, unpack, and open &lt;code&gt;focalboard-app&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Personal Server&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ubuntu&lt;/strong&gt;: You can download and run the compiled Focalboard &lt;strong&gt;Personal Server&lt;/strong&gt; on Ubuntu by following &lt;a href=&#34;https://www.focalboard.com/download/personal-edition/ubuntu/&#34;&gt;our latest install guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;API Docs&lt;/h3&gt; &#xA;&lt;p&gt;Boards API docs can be found over at &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/mattermost/focalboard/raw/main/server/swagger/docs/html/index.html&#34;&gt;https://htmlpreview.github.io/?https://github.com/mattermost/focalboard/blob/main/server/swagger/docs/html/index.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Getting started&lt;/h3&gt; &#xA;&lt;p&gt;Our &lt;a href=&#34;https://developers.mattermost.com/contribute/focalboard/personal-server-setup-guide&#34;&gt;developer guide&lt;/a&gt; has detailed instructions on how to set up your development environment for the &lt;strong&gt;Personal Server&lt;/strong&gt;. You can also join the &lt;a href=&#34;https://community.mattermost.com/core/channels/focalboard&#34;&gt;~Focalboard community channel&lt;/a&gt; to connect with other developers.&lt;/p&gt; &#xA;&lt;p&gt;Create an &lt;code&gt;.env&lt;/code&gt; file in the focalboard directory that contains:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;EXCLUDE_ENTERPRISE=&#34;1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To build the server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make prebuild&#xA;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; ./bin/focalboard-server&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then navigate your browser to &lt;a href=&#34;http://localhost:8000&#34;&gt;&lt;code&gt;http://localhost:8000&lt;/code&gt;&lt;/a&gt; to access your Focalboard server. The port is configured in &lt;code&gt;config.json&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once the server is running, you can rebuild just the web app via &lt;code&gt;make webapp&lt;/code&gt; in a separate terminal window. Reload your browser to see the changes.&lt;/p&gt; &#xA;&lt;h3&gt;Building and running standalone desktop apps&lt;/h3&gt; &#xA;&lt;p&gt;You can build standalone apps that package the server to run locally against SQLite:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;Requires Windows 10, &lt;a href=&#34;https://developer.microsoft.com/en-us/windows/downloads/sdk-archive/&#34;&gt;Windows 10 SDK&lt;/a&gt; 10.0.19041.0, and .NET 4.8 developer pack&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Open a &lt;code&gt;git-bash&lt;/code&gt; prompt.&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;make prebuild&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;The above prebuild step needs to be run only when you make changes to or want to install your npm dependencies, etc.&lt;/li&gt; &#xA;   &lt;li&gt;Once the prebuild is completed, you can keep repeating the below steps to build the app &amp;amp; see the changes.&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;make win-wpf-app&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;cd win-wpf/msix &amp;amp;&amp;amp; focalboard.exe&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mac&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;Requires macOS 11.3+ and Xcode 13.2.1+&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;make prebuild&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;The above prebuild step needs to be run only when you make changes to or want to install your npm dependencies, etc.&lt;/li&gt; &#xA;   &lt;li&gt;Once the prebuild is completed, you can keep repeating the below steps to build the app &amp;amp; see the changes.&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;make mac-app&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;open mac/dist/Focalboard.app&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;Tested on Ubuntu 18.04&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Install &lt;code&gt;webgtk&lt;/code&gt; dependencies &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Run &lt;code&gt;sudo apt-get install libgtk-3-dev&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Run &lt;code&gt;sudo apt-get install libwebkit2gtk-4.0-dev&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;make prebuild&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;The above prebuild step needs to be run only when you make changes to or want to install your npm dependencies, etc.&lt;/li&gt; &#xA;   &lt;li&gt;Once the prebuild is completed, you can keep repeating the below steps to build the app &amp;amp; see the changes.&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;make linux-app&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Uncompress &lt;code&gt;linux/dist/focalboard-linux.tar.gz&lt;/code&gt; to a directory of your choice&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;focalboard-app&lt;/code&gt; from the directory you have chosen&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;To run it locally from offical image: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;docker run -it -p 80:8000 mattermost/focalboard&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;To build it for your current architecture: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;docker build -f docker/Dockerfile .&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;To build it for a custom architecture (experimental): &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;docker build -f docker/Dockerfile --platform linux/arm64 .&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cross-compilation currently isn&#39;t fully supported, so please build on the appropriate platform. Refer to the GitHub Actions workflows (&lt;code&gt;build-mac.yml&lt;/code&gt;, &lt;code&gt;build-win.yml&lt;/code&gt;, &lt;code&gt;build-ubuntu.yml&lt;/code&gt;) for the detailed list of steps on each platform.&lt;/p&gt; &#xA;&lt;h3&gt;Unit testing&lt;/h3&gt; &#xA;&lt;p&gt;Before checking in commits, run &lt;code&gt;make ci&lt;/code&gt;, which is similar to the &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; workflow and includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Server unit tests&lt;/strong&gt;: &lt;code&gt;make server-test&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Web app ESLint&lt;/strong&gt;: &lt;code&gt;cd webapp; npm run check&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Web app unit tests&lt;/strong&gt;: &lt;code&gt;cd webapp; npm run test&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Web app UI tests&lt;/strong&gt;: &lt;code&gt;cd webapp; npm run cypress:ci&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Staying informed&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Changes&lt;/strong&gt;: See the &lt;a href=&#34;https://raw.githubusercontent.com/mattermost-community/focalboard/main/CHANGELOG.md&#34;&gt;CHANGELOG&lt;/a&gt; for the latest updates&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bug Reports&lt;/strong&gt;: &lt;a href=&#34;https://github.com/mattermost/focalboard/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=&#34;&gt;File a bug report&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;: Join the &lt;a href=&#34;https://community.mattermost.com/core/channels/focalboard&#34;&gt;~Focalboard community channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>LMCache/LMCache</title>
    <updated>2025-07-31T01:29:38Z</updated>
    <id>tag:github.com,2025-07-31:/LMCache/LMCache</id>
    <link href="https://github.com/LMCache/LMCache" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Supercharge Your LLM with the Fastest KV Cache Layer&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LMCache/LMCache/dev/asset/logo.png&#34; width=&#34;720&#34; alt=&#34;lmcache logo&#34;&gt; &lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://docs.lmcache.ai/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-live-brightgreen&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lmcache/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/lmcache&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lmcache/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/lmcache&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://buildkite.com/lmcache/lmcache-unittests&#34;&gt;&lt;img src=&#34;https://badge.buildkite.com/ce25f1819a274b7966273bfa54f0e02f092c3de0d7563c5c9d.svg?sanitize=true&#34; alt=&#34;Unit Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LMCache/LMCache/actions/workflows/code_quality_checks.yml&#34;&gt;&lt;img src=&#34;https://github.com/lmcache/lmcache/actions/workflows/code_quality_checks.yml/badge.svg?branch=dev&amp;amp;label=tests&#34; alt=&#34;Code Quality&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://buildkite.com/lmcache/lmcache-vllm-integration-tests&#34;&gt;&lt;img src=&#34;https://badge.buildkite.com/108ddd4ab482a2480999dec8c62a640a3315ed4e6c4e86798e.svg?sanitize=true&#34; alt=&#34;Integration Tests&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.bestpractices.dev/projects/10841&#34;&gt;&lt;img src=&#34;https://www.bestpractices.dev/projects/10841/badge&#34; alt=&#34;OpenSSF Best Practices&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://scorecard.dev/viewer/?uri=github.com/LMCache/LMCache&#34;&gt;&lt;img src=&#34;https://api.scorecard.dev/projects/github.com/LMCache/LMCache/badge&#34; alt=&#34;OpenSSF Scorecard&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://deepwiki.com/LMCache/LMCache/&#34;&gt;&lt;img src=&#34;https://deepwiki.com/badge.svg?sanitize=true&#34; alt=&#34;Ask DeepWiki&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LMCache/LMCache/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/w/LMCache/LMCache&#34; alt=&#34;GitHub commit activity&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lmcache/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/lmcache&#34; alt=&#34;PyPI - Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/channel/UC58zMz55n70rtf1Ak2PULJA&#34;&gt;&lt;img src=&#34;https://img.shields.io/youtube/channel/views/UC58zMz55n70rtf1Ak2PULJA&#34; alt=&#34;YouTube Channel Views&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;| &lt;a href=&#34;https://blog.lmcache.ai/&#34;&gt;&lt;strong&gt;Blog&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://docs.lmcache.ai/&#34;&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-36x1m765z-8FgDA_73vcXtlZ_4XvpE6Q&#34;&gt;&lt;strong&gt;Join Slack&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://forms.gle/MHwLiYDU6kcW3dLj7&#34;&gt;&lt;strong&gt;Interest Form&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/LMCache/LMCache/issues/574&#34;&gt;&lt;strong&gt;Roadmap&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;🔥 &lt;strong&gt;NEW: For enterprise-scale deployment of LMCache and vLLM, please check out vLLM &lt;a href=&#34;https://github.com/vllm-project/production-stack&#34;&gt;Production Stack&lt;/a&gt;. LMCache is also officially supported in &lt;a href=&#34;https://github.com/llm-d/llm-d/&#34;&gt;llm-d&lt;/a&gt; and &lt;a href=&#34;https://github.com/kserve/kserve&#34;&gt;KServe&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Summary&lt;/h2&gt; &#xA;&lt;p&gt;LMCache is an &lt;strong&gt;LLM&lt;/strong&gt; serving engine extension to &lt;strong&gt;reduce TTFT&lt;/strong&gt; and &lt;strong&gt;increase throughput&lt;/strong&gt;, especially under long-context scenarios. By storing the KV caches of reusable texts across various locations, including (GPU, CPU DRAM, Local Disk), LMCache reuses the KV caches of &lt;strong&gt;&lt;em&gt;any&lt;/em&gt;&lt;/strong&gt; reused text (not necessarily prefix) in &lt;strong&gt;&lt;em&gt;any&lt;/em&gt;&lt;/strong&gt; serving engine instance. Thus, LMCache saves precious GPU cycles and reduces user response delay.&lt;/p&gt; &#xA;&lt;p&gt;By combining LMCache with vLLM, developers achieve 3-10x delay savings and GPU cycle reduction in many LLM use cases, including multi-round QA and RAG.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/86137f17-f216-41a0-96a7-e537764f7a4c&#34; alt=&#34;performance&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 🔥 Integration with vLLM v1 with the following features: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;High performance CPU KVCache offloading&lt;/li&gt; &#xA;   &lt;li&gt;Disaggregated prefill&lt;/li&gt; &#xA;   &lt;li&gt;P2P KVCache sharing&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LMCache is supported in the &lt;a href=&#34;https://github.com/vllm-project/production-stack/&#34;&gt;vLLM production stack&lt;/a&gt;, &lt;a href=&#34;https://github.com/llm-d/llm-d/&#34;&gt;llm-d&lt;/a&gt;, and &lt;a href=&#34;https://github.com/kserve/kserve&#34;&gt;KServe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Stable support for non-prefix KV caches&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Storage support as follows: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;CPU&lt;/li&gt; &#xA;   &lt;li&gt;Disk&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/ai-dynamo/nixl&#34;&gt;NIXL&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Installation support through pip and latest vLLM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To use LMCache, simply install &lt;code&gt;lmcache&lt;/code&gt; from your package manager, e.g. pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install lmcache&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Works on Linux NVIDIA GPU platform.&lt;/p&gt; &#xA;&lt;p&gt;More &lt;a href=&#34;https://docs.lmcache.ai/getting_started/installation&#34;&gt;detailed installation instructions&lt;/a&gt; are available in the docs.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;The best way to get started is to checkout the &lt;a href=&#34;https://docs.lmcache.ai/getting_started/quickstart/&#34;&gt;Quickstart Examples&lt;/a&gt; in the docs.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Check out the LMCache &lt;a href=&#34;https://docs.lmcache.ai/&#34;&gt;documentation&lt;/a&gt; which is available online.&lt;/p&gt; &#xA;&lt;p&gt;We also post regularly in &lt;a href=&#34;https://blog.lmcache.ai/&#34;&gt;LMCache blogs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Go hands-on with our &lt;a href=&#34;https://github.com/LMCache/LMCache/tree/dev/examples&#34;&gt;examples&lt;/a&gt;, demonstrating how to address different use cases with LMCache.&lt;/p&gt; &#xA;&lt;h2&gt;Interested in Connecting?&lt;/h2&gt; &#xA;&lt;p&gt;Fill out the &lt;a href=&#34;https://forms.gle/mQfQDUXbKfp2St1z7&#34;&gt;interest form&lt;/a&gt;, &lt;a href=&#34;https://mailchi.mp/tensormesh/lmcache-sign-up-newsletter&#34;&gt;sign up for our newsletter&lt;/a&gt;, &lt;a href=&#34;https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-2viziwhue-5Amprc9k5hcIdXT7XevTaQ&#34;&gt;join LMCache slack&lt;/a&gt;, &lt;a href=&#34;https://lmcache.ai/&#34;&gt;check out LMCache website&lt;/a&gt;, or &lt;a href=&#34;https://raw.githubusercontent.com/LMCache/LMCache/dev/contact@lmcache.ai&#34;&gt;drop an email&lt;/a&gt;, and our team will reach out to you!&lt;/p&gt; &#xA;&lt;h2&gt;Community meeting&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://uchicago.zoom.us/j/6603596916?pwd=Z1E5MDRWUSt2am5XbEt4dTFkNGx6QT09&#34;&gt;community meeting&lt;/a&gt; for LMCache is hosted bi-weekly. All are welcome to join!&lt;/p&gt; &#xA;&lt;p&gt;Meetings are held bi-weekly on: Tuesdays at 9:00 AM PT – &lt;a href=&#34;https://drive.usercontent.google.com/u/0/uc?id=1f5EXbooGcwNwzIpTgn5u4PHqXgfypMtu&amp;amp;export=download&#34;&gt;Add to Calendar&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We keep notes from each meeting on this &lt;a href=&#34;https://docs.google.com/document/d/1_Fl3vLtERFa3vTH00cezri78NihNBtSClK-_1tSrcow&#34;&gt;document&lt;/a&gt; for summaries of standups, discussion, and action items.&lt;/p&gt; &#xA;&lt;p&gt;Recordings of meetings are available on the &lt;a href=&#34;https://www.youtube.com/channel/UC58zMz55n70rtf1Ak2PULJA&#34;&gt;YouTube LMCache channel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome and value all contributions and collaborations. Please check out &lt;a href=&#34;https://raw.githubusercontent.com/LMCache/LMCache/dev/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt; on how to contribute.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use LMCache for your research, please cite our papers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{liu2024cachegen,&#xA;  title={Cachegen: Kv cache compression and streaming for fast large language model serving},&#xA;  author={Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},&#xA;  booktitle={Proceedings of the ACM SIGCOMM 2024 Conference},&#xA;  pages={38--56},&#xA;  year={2024}&#xA;}&#xA;&#xA;@article{cheng2024large,&#xA;  title={Do Large Language Models Need a Content Delivery Network?},&#xA;  author={Cheng, Yihua and Du, Kuntai and Yao, Jiayi and Jiang, Junchen},&#xA;  journal={arXiv preprint arXiv:2409.13761},&#xA;  year={2024}&#xA;}&#xA;&#xA;@inproceedings{10.1145/3689031.3696098,&#xA;  author = {Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},&#xA;  title = {CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion},&#xA;  year = {2025},&#xA;  url = {https://doi.org/10.1145/3689031.3696098},&#xA;  doi = {10.1145/3689031.3696098},&#xA;  booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},&#xA;  pages = {94–109},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Socials&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/company/lmcache-lab/?viewAsMember=true&#34;&gt;Linkedin&lt;/a&gt; | &lt;a href=&#34;https://x.com/lmcache&#34;&gt;Twitter&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/@LMCacheTeam&#34;&gt;Youtube&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The LMCache codebase is licensed under Apache License 2.0. See the &lt;a href=&#34;https://raw.githubusercontent.com/LMCache/LMCache/dev/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
</feed>