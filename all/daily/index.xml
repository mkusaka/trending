<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-29T01:29:07Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>litanlitudan/skyagi</title>
    <updated>2023-04-29T01:29:07Z</updated>
    <id>tag:github.com,2023-04-29:/litanlitudan/skyagi</id>
    <link href="https://github.com/litanlitudan/skyagi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SkyAGI: Emerging human-behavior simulation capability in LLM&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/litanlitudan/skyagi/main/images/background.png&#34; height=&#34;600&#34; alt=&#34;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;SkyAGI: Emerging human-behavior simulation capability in LLM&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/skyagi/&#34;&gt; &lt;img alt=&#34;PyPI&#34; src=&#34;https://img.shields.io/pypi/v/skyagi?color=gree&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;SkyAGI&lt;/code&gt; is a python package that demonstrates LLM&#39;s emerging capability in simulating believable human behaviors. Specifically, &lt;code&gt;SkyAGI&lt;/code&gt; implements the idea of &lt;a href=&#34;https://arxiv.org/abs/2304.03442&#34;&gt;Generative Agents&lt;/a&gt; and delivers a role-playing game that creates a very interesting user experience.&lt;/p&gt; &#xA;&lt;p&gt;Different from previous AI based NPC systems, &lt;code&gt;SkyAGI&lt;/code&gt;&#39;s NPC generates very believable human responses. The interesting observations in this demo show a huge potential for rethinking game development in many aspects, such as NPC script writing.&lt;/p&gt; &#xA;&lt;p&gt;To demonstrate this, &lt;code&gt;SkyAGI&lt;/code&gt; provides example characters from &lt;code&gt;The Big Bang Theory&lt;/code&gt; and &lt;code&gt;The Avengers&lt;/code&gt; as a starting point. Users could also define customized characters by creating config json files like &lt;a href=&#34;https://github.com/litanlitudan/skyagi/raw/main/examples/example_agent.json&#34;&gt;customized_character.json&lt;/a&gt; For details about the interesting observations, refer to the &lt;a href=&#34;https://github.com/litanlitudan/skyagi/#interesting-observations-in-this-demo&#34;&gt;observations section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Installation&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install --upgrade skyagi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;How to run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export OPENAI_API_KEY=&#34;...&#34;&#xA;skyagi&#xA;# or&#xA;OPENAI_API_KEY=&#34;...&#34; skyagi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example if the OpenAI key is &lt;code&gt;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&lt;/code&gt;, then the exact command would be the following&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# make sure no quote around the token&#xA;export OPENAI_API_KEY=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&#xA;skyagi&#xA;# or&#xA;OPENAI_API_KEY=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 skyagi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use example agent configs, download it from here: &lt;a href=&#34;https://github.com/litanlitudan/skyagi/tree/main/examples&#34;&gt;https://github.com/litanlitudan/skyagi/tree/main/examples&lt;/a&gt; (pip install doesn&#39;t contain the agent configuration)&lt;/p&gt; &#xA;&lt;p&gt;An example agent configuration (Sheldon) looks something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;name&#34;: &#34;Sheldon&#34;,&#xA;    &#34;age&#34;: 27,&#xA;    &#34;personality&#34;: &#34;Intelligent, rigid, socially challenged, quirky, and arrogant.&#34;,&#xA;    &#34;memories&#34;: [&#xA;        &#34;Sheldon is a theoretical physicist who works at Caltech.&#34;,&#xA;        &#34;Sheldon has an eidetic memory and is highly intelligent, but struggles with social skills and sarcasm.&#34;,&#xA;        ...&#xA;        &#34;Knock, knock, knock, Penny - This is the specific knock that Sheldon uses when he visits Penny&#39;s apartment, which he repeats three times.&#34;,&#xA;        &#34;Bazinga! - This is Sheldon&#39;s catchphrase that he uses to indicate he was joking or playing a prank on someone.&#34;&#xA;    ],&#xA;    &#34;current_status&#34;: &#34;Sheldon is at the Cheesecake Factory&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interesting observations in this demo&lt;/h2&gt; &#xA;&lt;p&gt;Here is a screenshot of a live demo using The Big Bang Theory example. &lt;img src=&#34;https://raw.githubusercontent.com/litanlitudan/skyagi/main/images/demo.png&#34; alt=&#34;demo&#34;&gt; From the conversation, we can observe three interesting points that have not been widely seen in previous systems:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Leonard remembered that Penny had asked him to persuade Sheldon to go for a hike, which shows the capability of some kind of memory.&lt;/li&gt; &#xA; &lt;li&gt;Leonard changed his mind after Sheldon whispered to him and even tried to convince Penny to join the scientific effort, which shows that the agents had meaningful progress in the story even without human intervention.&lt;/li&gt; &#xA; &lt;li&gt;All the responses are quite human-like. As a user, it&#39;s quite hard to tell whether it&#39;s actually an AI behind the responses.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.03442&#34;&gt;https://arxiv.org/abs/2304.03442&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/en/latest/use_cases/agent_simulations/characters.html#create-a-generative-character&#34;&gt;https://python.langchain.com/en/latest/use_cases/agent_simulations/characters.html#create-a-generative-character&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>gaomingqi/Track-Anything</title>
    <updated>2023-04-29T01:29:07Z</updated>
    <id>tag:github.com,2023-04-29:/gaomingqi/Track-Anything</id>
    <link href="https://github.com/gaomingqi/Track-Anything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Track-Anything is a flexible and interactive tool for video object tracking and segmentation, based on Segment Anything, XMem, and E2FGVI.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/gaomingqi/Track-Anything/master/assets/track-anything-logo.jpg&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a src=&#34;https://img.shields.io/badge/%F0%9F%93%96-Open_in_Spaces-informational.svg?style=flat-square&#34; href=&#34;https://arxiv.org/abs/2304.11968&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%F0%9F%93%96-Arxiv_2304.11968-red.svg?style=flat-square&#34;&gt; &lt;/a&gt; &#xA; &lt;a src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open_in_Spaces-informational.svg?style=flat-square&#34; href=&#34;https://huggingface.co/spaces/watchtowerss/Track-Anything?duplicate=true&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face_Space-informational.svg?style=flat-square&#34;&gt; &lt;/a&gt; &#xA; &lt;a src=&#34;https://img.shields.io/badge/%F0%9F%9A%80-SUSTech_VIP_Lab-important.svg?style=flat-square&#34; href=&#34;https://zhengfenglab.com/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%F0%9F%9A%80-SUSTech_VIP_Lab-important.svg?style=flat-square&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Track-Anything&lt;/strong&gt;&lt;/em&gt; is a flexible and interactive tool for video object tracking and segmentation. It is developed upon &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt;, can specify anything to track and segment via user clicks only. During tracking, users can flexibly change the objects they wanna track or correct the region of interest if there are any ambiguities. These characteristics enable &lt;em&gt;&lt;strong&gt;Track-Anything&lt;/strong&gt;&lt;/em&gt; to be suitable for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Video object tracking and segmentation with shot changes.&lt;/li&gt; &#xA; &lt;li&gt;Visualized development and data annnotation for video object tracking and segmentation.&lt;/li&gt; &#xA; &lt;li&gt;Object-centric downstream video tasks, such as video inpainting and editing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/gaomingqi/Track-Anything/master/assets/avengers.gif&#34; width=&#34;81%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- ![avengers]() --&gt; &#xA;&lt;h2&gt;&lt;span&gt;üöÄ&lt;/span&gt; Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;2023/04/29: We improved inpainting by decoupling GPU memory usage and video length. Now Track-Anything can inpaint videos with any length! &lt;span&gt;üò∫&lt;/span&gt; Check &lt;a href=&#34;https://github.com/gaomingqi/Track-Anything/issues/4#issuecomment-1528198165&#34;&gt;HERE&lt;/a&gt; for our GPU memory requirements.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2023/04/25: We are delighted to introduce &lt;a href=&#34;https://github.com/ttengwang/Caption-Anything&#34;&gt;Caption-Anything&lt;/a&gt; &lt;span&gt;‚úç&lt;/span&gt;, an inventive project from our lab that combines the capabilities of Segment Anything, Visual Captioning, and ChatGPT.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2023/04/20: We deployed &lt;a href=&#34;https://huggingface.co/spaces/watchtowerss/Track-Anything?duplicate=trueg&#34;&gt;[DEMO]&lt;/a&gt; on Hugging Face &lt;span&gt;ü§ó&lt;/span&gt;!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2023/04/14: We made Track-Anything public!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üó∫&lt;/span&gt; Video Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/30309970/234902447-a4c59718-fcfe-443a-bd18-2f3f775cfc13.mp4&#34;&gt;https://user-images.githubusercontent.com/30309970/234902447-a4c59718-fcfe-443a-bd18-2f3f775cfc13.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;span&gt;üïπ&lt;/span&gt; Example - Multiple Object Tracking and Segmentation (with &lt;a href=&#34;https://github.com/hkchengrex/XMem&#34;&gt;XMem&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/39208339/233035206-0a151004-6461-4deb-b782-d1dbfe691493.mp4&#34;&gt;https://user-images.githubusercontent.com/39208339/233035206-0a151004-6461-4deb-b782-d1dbfe691493.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;span&gt;üïπ&lt;/span&gt; Example - Video Object Tracking and Segmentation with Shot Changes (with &lt;a href=&#34;https://github.com/hkchengrex/XMem&#34;&gt;XMem&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/30309970/232848349-f5e29e71-2ea4-4529-ac9a-94b9ca1e7055.mp4&#34;&gt;https://user-images.githubusercontent.com/30309970/232848349-f5e29e71-2ea4-4529-ac9a-94b9ca1e7055.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;span&gt;üïπ&lt;/span&gt; Example - Video Inpainting (with &lt;a href=&#34;https://github.com/MCG-NKU/E2FGVI&#34;&gt;E2FGVI&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/28050374/232959816-07f2826f-d267-4dda-8ae5-a5132173b8f4.mp4&#34;&gt;https://user-images.githubusercontent.com/28050374/232959816-07f2826f-d267-4dda-8ae5-a5132173b8f4.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üíª&lt;/span&gt; Get Started&lt;/h2&gt; &#xA;&lt;h4&gt;Linux &amp;amp; Windows&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Clone the repository:&#xA;git clone https://github.com/gaomingqi/Track-Anything.git&#xA;cd Track-Anything&#xA;&#xA;# Install dependencies: &#xA;pip install -r requirements.txt&#xA;&#xA;# Run the Track-Anything gradio demo.&#xA;python app.py --device cuda:0&#xA;# python app.py --device cuda:0 --sam_model_type vit_b # for lower memory usage&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìñ&lt;/span&gt; Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this work useful for your research or applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{yang2023track,&#xA;      title={Track Anything: Segment Anything Meets Videos}, &#xA;      author={Jinyu Yang and Mingqi Gao and Zhe Li and Shang Gao and Fangjing Wang and Feng Zheng},&#xA;      year={2023},&#xA;      eprint={2304.11968},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üëè&lt;/span&gt; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The project is based on &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt;, &lt;a href=&#34;https://github.com/hkchengrex/XMem&#34;&gt;XMem&lt;/a&gt;, and &lt;a href=&#34;https://github.com/MCG-NKU/E2FGVI&#34;&gt;E2FGVI&lt;/a&gt;. Thanks for the authors for their efforts.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>UX-Decoder/Segment-Everything-Everywhere-All-At-Once</title>
    <updated>2023-04-29T01:29:07Z</updated>
    <id>tag:github.com,2023-04-29:/UX-Decoder/Segment-Everything-Everywhere-All-At-Once</id>
    <link href="https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of the paper &#34;Segment Everything Everywhere All at Once&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üëÄ&lt;em&gt;SEEM:&lt;/em&gt; Segment Everything Everywhere All at Once&lt;/h1&gt; &#xA;&lt;p&gt;We introduce &lt;strong&gt;SEEM&lt;/strong&gt; that can &lt;strong&gt;S&lt;/strong&gt;egment &lt;strong&gt;E&lt;/strong&gt;verything &lt;strong&gt;E&lt;/strong&gt;verywhere with &lt;strong&gt;M&lt;/strong&gt;ulti-modal prompts all at once. SEEM allows users to easily segment an image using prompts of different types including visual prompts (points, marks, boxes, scribbles and image segments) and language prompts (text and audio), etc. It can also work with any combinations of prompts or generalize to custom prompts!&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üçá&lt;/span&gt; [&lt;a href=&#34;https://arxiv.org/pdf/2304.06718.pdf&#34;&gt;Read our arXiv Paper&lt;/a&gt;] &amp;nbsp; &lt;span&gt;üçé&lt;/span&gt; [&lt;a href=&#34;https://huggingface.co/spaces/xdecoder/SEEM&#34;&gt;Try Hugging Face Demo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üëâ&lt;/span&gt; &lt;em&gt;[New]&lt;/em&gt; &lt;strong&gt;One-Line Getting Started with Linux:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone git@github.com:UX-Decoder/Segment-Everything-Everywhere-All-At-Once.git &amp;amp;&amp;amp; cd Segment-Everything-Everywhere-All-At-Once/demo_code &amp;amp;&amp;amp; sh run_demo.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;span&gt;üî•&lt;/span&gt; &lt;strong&gt;Related projects:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/FocalNet&#34;&gt;FocalNet&lt;/a&gt; : Focal Modulation Networks; &lt;strong&gt;We used FocalNet as the vision backbone&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/UniCL&#34;&gt;UniCL&lt;/a&gt; : Unified Contrastive Learning; &lt;strong&gt;We used this technique for image-text contrastive learning&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/X-Decoder&#34;&gt;X-Decoder&lt;/a&gt; : Generic decoder that can do multiple tasks with one model onlyÔºõ&lt;strong&gt;We built SEEM based on X-Decoder&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;üî•&lt;/span&gt; &lt;strong&gt;Other projects you may find interesting:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/OpenSeeD&#34;&gt;OpenSeed&lt;/a&gt; : Strong open-set segmentation methods.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounding SAM&lt;/a&gt; : Combining Grounding DINO and Segment Anything; &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt;: A strong open-set detection model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/X-Decoder/tree/xgpt&#34;&gt;X-GPT&lt;/a&gt; : Conversational Visual Agent supported by X-Decoder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt; : Large Language and Vision Assistant.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üöÄ&lt;/span&gt; Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.04.28]&lt;/strong&gt; We have updated the &lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/raw/main/SEEM_arXiv.pdf&#34;&gt;Paper&lt;/a&gt; that shows &lt;em&gt;better interactive segmentation results than SAM&lt;/em&gt;, which trained on x50 more data than us!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.04.26]&lt;/strong&gt; We have released the &lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/tree/main/demo_code&#34;&gt;Demo Code&lt;/a&gt; and &lt;a href=&#34;https://projects4jw.blob.core.windows.net/x-decoder/release/seem_focalt_v1.pt&#34;&gt;SEEM-Tiny Checkpoint&lt;/a&gt;! Please try the One-Line Started!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.04.20]&lt;/strong&gt; SEEM Referring Video Segmentation is out! Please try the &lt;a href=&#34;https://huggingface.co/spaces/xdecoder/SEEM&#34;&gt;Video Demo&lt;/a&gt; and take a look at the &lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once#tulip-nerf-examples&#34;&gt;NERF examples&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11957155/233255289-35c0c1e2-35f7-48e4-a7e9-68da50c839d3.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11957155/233526415-a0a44963-19a3-4e56-965a-afaa598e6127.gif&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üí°&lt;/span&gt; Highlights&lt;/h2&gt; &#xA;&lt;p&gt;Inspired by the appealing universal interface in LLMs, we are advocating a universal, interactive multi-modal interface for any type of segmentation with &lt;strong&gt;ONE SINGLE MODEL&lt;/strong&gt;. We emphasize &lt;strong&gt;4&lt;/strong&gt; important features of &lt;strong&gt;SEEM&lt;/strong&gt; below.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Versatility&lt;/strong&gt;: work with various types of prompts, for example, clicks, boxes, polygons, scribbles, texts, and referring image;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Compositionaliy&lt;/strong&gt;: deal with any compositions of prompts;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Interactivity&lt;/strong&gt;: interact with user in multi-rounds, thanks to the memory prompt of &lt;strong&gt;SEEM&lt;/strong&gt; to store the session history;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Semantic awareness&lt;/strong&gt;: give a semantic label to any predicted mask;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/teaser_new.png?raw=true&#34; alt=&#34;SEEM design&#34;&gt; A brief introduction of all the generic and interactive segmentation tasks we can do.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;ü¶Ñ&lt;/span&gt; How to use the demo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Try our default examples first;&lt;/li&gt; &#xA; &lt;li&gt;Upload an image;&lt;/li&gt; &#xA; &lt;li&gt;Select at least one type of prompt of your choice (If you want to use referred region of another image please check &#34;Example&#34; and upload another image in referring image panel);&lt;/li&gt; &#xA; &lt;li&gt;Remember to provide the actual prompt for each prompt type you select, otherwise you will meet an error (e.g., remember to draw on the referring image);&lt;/li&gt; &#xA; &lt;li&gt;Our model by default support the &lt;strong&gt;vocabulary&lt;/strong&gt; of COCO 80 categories, others will be classified to &#39;others&#39; or misclassified. If you want to segment using open-vocabulary labels, include the text label in &#39;text&#39; button after drawing scribbles.&lt;/li&gt; &#xA; &lt;li&gt;Click &#34;Submit&#34; and wait for a few seconds.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üåã&lt;/span&gt; An interesting example&lt;/h2&gt; &#xA;&lt;p&gt;An example of Transformers. The referred image is the truck form of Optimus Prime. Our model can always segment Optimus Prime in target images no matter which form it is in. Thanks Hongyang Li for this fun example.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/transformers_gh.png&#34; width=&#34;700&#34; alt=&#34;assets/transformers_gh.png&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;&lt;span&gt;üå∑&lt;/span&gt; NERF Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inspired by the example in SA3D, we tried SEEM on NERF Examples and works well :)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11957155/234230320-2189056d-1c89-4f0c-88da-851d12e8323c.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11957155/234231284-0adc4bae-ef90-41d3-9883-41f6407a883b.gif&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üèï&lt;/span&gt; Click, scribble to mask&lt;/h2&gt; &#xA;&lt;p&gt;With a simple click or stoke from the user, we can generate the masks and the corresponding category labels for it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/click.png?raw=true&#34; alt=&#34;SEEM design&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üèî&lt;/span&gt; Text to mask&lt;/h2&gt; &#xA;&lt;p&gt;SEEM can generate the mask with text input from the user, providing multi-modality interaction with human.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/text.png?raw=true&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;!-- &#xA;&lt;div  align=&#34;center&#34;&gt;    &#xA;&lt;img src=&#34;assets/text.png&#34; width = &#34;700&#34; alt=&#34;assets/text.png&#34; align=center /&gt;&#xA;&lt;/div&gt; --&gt; &#xA;&lt;h2&gt;&lt;span&gt;üïå&lt;/span&gt; Referring image to mask&lt;/h2&gt; &#xA;&lt;p&gt;With a simple click or stroke on the referring image, the model is able to segment the objects with similar semantics on the target images. &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/ref_seg.png?raw=true&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;SEEM understands the spatial relationship very well. Look at the three zebras! The segmented zebras have similar positions with the referred zebras. For example, when the leftmost zebra is referred on the upper row, the leftmost zebra on the bottom row is segmented. &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/spatial_relation.png?raw=true&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üåº&lt;/span&gt; Referring image to video mask&lt;/h2&gt; &#xA;&lt;p&gt;No training on video data needed, SEEM works perfectly for you to segment videos with whatever queries you specify! &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/referring_video_visualize.png?raw=true&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üåª&lt;/span&gt; Audio to mask&lt;/h2&gt; &#xA;&lt;p&gt;We use Whisper to turn audio into text prompt to segment the object. Try it in our demo!&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/audio.png&#34; width=&#34;900&#34; alt=&#34;assets/audio.png&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- ## üî• Combination of different prompts to mask --&gt; &#xA;&lt;h2&gt;&lt;span&gt;üå≥&lt;/span&gt; Examples of different styles&lt;/h2&gt; &#xA;&lt;p&gt;An example of segmenting a meme.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/emoj.png&#34; width=&#34;500&#34; alt=&#34;assets/emoj.png&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;An example of segmenting trees in cartoon style.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/trees_text.png&#34; width=&#34;700&#34; alt=&#34;assets/trees_text.png&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;An example of segmenting a Minecraft image.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/minecraft.png&#34; width=&#34;700&#34; alt=&#34;assets/minecraft.png&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- ![example](assets/minecraft.png?raw=true) --&gt; An example of using referring image on a popular teddy bear. &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/fox_v2.png?raw=true&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/model.png?raw=true&#34; alt=&#34;SEEM design&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Comparison with SAM&lt;/h2&gt; &#xA;&lt;p&gt;In the following figure, we compare the levels of interaction and semantics of three segmentation tasks (edge detection, open-set, and interactive segmentation). Open-set Segmentation usually requires a high level of semantics and does not require interaction. Compared with &lt;a href=&#34;https://arxiv.org/abs/2304.02643&#34;&gt;SAM&lt;/a&gt;, SEEM covers a wider range of interaction and semantics levels. For example, SAM only supports limited interaction types like points and boxes, while misses high-semantic tasks since it does not output semantic labels itself. The reasons are: First, SEEM has a unified prompt encoder that encodes all visual and language prompts into a joint representation space. In consequence, SEEM can support more general usages. It has potential to extend to custom prompts. Second, SEEM works very well on text to mask (grounding segmentation) and outputs semantic-aware predictions.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/compare.jpg&#34; width=&#34;500&#34; alt=&#34;assets/compare.jpg&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- This figure shows a comparison of our model with concurrent work SAM on the level of interactions and semantics. The x-axis and y-axis denote the level of interaction and semantics, respectively. Three segmentation tasks are shown, including Open-set Segmentation, Edge detection, and Interactive Segmentation. These tasks have different levels of interactions and semantics. For example, Open-set Segmentation usually requires a high level of semantics and does not require interaction. Compared with SAM, our model covers a wider range of interaction and semantics levels. For example, SAM only supports limited interaction types like points and boxes, while misses high-semantic tasks since it does not output semantic labels itself. Note that although we do not report edge detection results, our model can support it by simply converting masks to edges. --&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìë&lt;/span&gt; Catelog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; SEEM Demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference and Installation Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; (Soon) Evaluation Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; (TBD When) Training Code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üíò&lt;/span&gt; Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We appreciate hugging face for the GPU support on demo!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- ## Citation (update when paper is available on arxiv)&#xA;If you find this project helpful for your research, please consider citing the following BibTeX entry.&#xA;```BibTex&#xA;&#xA;``` --&gt;</summary>
  </entry>
</feed>