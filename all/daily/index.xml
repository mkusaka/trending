<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-28T01:28:37Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>allenai/olmocr</title>
    <updated>2025-02-28T01:28:37Z</updated>
    <id>tag:github.com,2025-02-28:/allenai/olmocr</id>
    <link href="https://github.com/allenai/olmocr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Toolkit for linearizing PDFs for LLM datasets/training&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;img src=&#34;https://github.com/allenai/OLMo/assets/8812459/774ac485-a535-4768-8f7c-db7be20f5cc3&#34; width=&#34;300&#34;/&gt; --&gt; &#xA; &lt;img src=&#34;https://github.com/user-attachments/assets/d70c8644-3e64-4230-98c3-c52fddaeccb6&#34; alt=&#34;olmOCR Logo&#34; width=&#34;300&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;h1&gt;olmOCR&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;GitHub License&#34; src=&#34;https://img.shields.io/github/license/allenai/OLMo&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/olmocr/releases&#34;&gt; &lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/github/release/allenai/olmocr.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://olmocr.allenai.org/papers/olmocr.pdf&#34;&gt; &lt;img alt=&#34;Tech Report&#34; src=&#34;https://img.shields.io/badge/Paper-olmOCR-blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://olmocr.allenai.org&#34;&gt; &lt;img alt=&#34;Demo&#34; src=&#34;https://img.shields.io/badge/Ai2-Demo-F0529C&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/sZq3jTNVNG&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/badge/Discord%20-%20blue?style=flat&amp;amp;logo=discord&amp;amp;label=Ai2&amp;amp;color=%235B65E9&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;A toolkit for training language models to work with PDF documents in the wild.&lt;/p&gt; &#xA;&lt;p&gt;Try the online demo: &lt;a href=&#34;https://olmocr.allenai.org/&#34;&gt;https://olmocr.allenai.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;What is included:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A prompting strategy to get really good natural text parsing using ChatGPT 4o - &lt;a href=&#34;https://github.com/allenai/olmocr/raw/main/olmocr/data/buildsilver.py&#34;&gt;buildsilver.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;An side-by-side eval toolkit for comparing different pipeline versions - &lt;a href=&#34;https://github.com/allenai/olmocr/raw/main/olmocr/eval/runeval.py&#34;&gt;runeval.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Basic filtering by language and SEO spam removal - &lt;a href=&#34;https://github.com/allenai/olmocr/raw/main/olmocr/filter/filter.py&#34;&gt;filter.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Finetuning code for Qwen2-VL and Molmo-O - &lt;a href=&#34;https://github.com/allenai/olmocr/raw/main/olmocr/train/train.py&#34;&gt;train.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Processing millions of PDFs through a finetuned model using Sglang - &lt;a href=&#34;https://github.com/allenai/olmocr/raw/main/olmocr/pipeline.py&#34;&gt;pipeline.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Viewing &lt;a href=&#34;https://github.com/allenai/dolma&#34;&gt;Dolma docs&lt;/a&gt; created from PDFs - &lt;a href=&#34;https://github.com/allenai/olmocr/raw/main/olmocr/viewer/dolmaviewer.py&#34;&gt;dolmaviewer.py&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Recent NVIDIA GPU (tested on RTX 4090, L40S, A100, H100)&lt;/li&gt; &#xA; &lt;li&gt;30GB of free disk space&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You will need to install poppler-utils and additional fonts for rendering PDF images.&lt;/p&gt; &#xA;&lt;p&gt;Install dependencies (Ubuntu/Debian)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get update&#xA;sudo apt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set up a conda environment and install olmocr&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n olmocr python=3.11&#xA;conda activate olmocr&#xA;&#xA;git clone https://github.com/allenai/olmocr.git&#xA;cd olmocr&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install sglang with &lt;a href=&#34;https://github.com/flashinfer-ai/flashinfer&#34;&gt;flashinfer&lt;/a&gt; if you want to run inference on GPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install sgl-kernel==0.0.3.post1 --force-reinstall --no-deps&#xA;pip install &#34;sglang[all]==0.4.2&#34; --find-links https://flashinfer.ai/whl/cu124/torch2.4/flashinfer/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Local Usage Example&lt;/h3&gt; &#xA;&lt;p&gt;For quick testing, try the &lt;a href=&#34;https://olmocr.allen.ai/&#34;&gt;web demo&lt;/a&gt;. To run locally, a GPU is required, as inference is powered by &lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;sglang&lt;/a&gt; under the hood. Convert a Single PDF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.pipeline ./localworkspace --pdfs tests/gnarly_pdfs/horribleocr.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Convert Multiple PDFs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.pipeline ./localworkspace --pdfs tests/gnarly_pdfs/*.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Results will be stored as JSON in &lt;code&gt;./localworkspace&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Viewing Results&lt;/h4&gt; &#xA;&lt;p&gt;Extracted text is stored as &lt;a href=&#34;https://github.com/allenai/dolma&#34;&gt;Dolma&lt;/a&gt;-style JSONL inside of the &lt;code&gt;./localworkspace/results&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat localworkspace/results/output_*.jsonl  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;View results side-by-side with the original PDFs (uses &lt;code&gt;dolmaviewer&lt;/code&gt; command):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.viewer.dolmaviewer localworkspace/results/output_*.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now open &lt;code&gt;./dolma_previews/tests_gnarly_pdfs_horribleocr_pdf.html&lt;/code&gt; in your favorite browser.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/128922d1-63e6-4d34-84f2-d7901237da1f&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Multi-node / Cluster Usage&lt;/h3&gt; &#xA;&lt;p&gt;If you want to convert millions of PDFs, using multiple nodes running in parallel, then olmOCR supports reading your PDFs from AWS S3, and coordinating work using an AWS S3 output bucket.&lt;/p&gt; &#xA;&lt;p&gt;For example, you can start this command on your first worker node, and it will set up a simple work queue in your AWS bucket and start converting PDFs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now on any subsequent nodes, just run this and they will start grabbing items from the same workspace queue.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are at Ai2 and want to linearize millions of PDFs efficiently using &lt;a href=&#34;https://www.beaker.org&#34;&gt;beaker&lt;/a&gt;, just add the &lt;code&gt;--beaker&lt;/code&gt; flag. This will prepare the workspace on your local machine, and then launch N GPU workers in the cluster to start converting PDFs.&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf --beaker --beaker_gpus 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Full documentation for the pipeline&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m olmocr.pipeline --help&#xA;usage: pipeline.py [-h] [--pdfs PDFS] [--workspace_profile WORKSPACE_PROFILE] [--pdf_profile PDF_PROFILE] [--pages_per_group PAGES_PER_GROUP]&#xA;                   [--max_page_retries MAX_PAGE_RETRIES] [--max_page_error_rate MAX_PAGE_ERROR_RATE] [--workers WORKERS] [--apply_filter] [--stats] [--model MODEL]&#xA;                   [--model_max_context MODEL_MAX_CONTEXT] [--model_chat_template MODEL_CHAT_TEMPLATE] [--target_longest_image_dim TARGET_LONGEST_IMAGE_DIM]&#xA;                   [--target_anchor_text_len TARGET_ANCHOR_TEXT_LEN] [--beaker] [--beaker_workspace BEAKER_WORKSPACE] [--beaker_cluster BEAKER_CLUSTER]&#xA;                   [--beaker_gpus BEAKER_GPUS] [--beaker_priority BEAKER_PRIORITY]&#xA;                   workspace&#xA;&#xA;Manager for running millions of PDFs through a batch inference pipeline&#xA;&#xA;positional arguments:&#xA;  workspace             The filesystem path where work will be stored, can be a local folder, or an s3 path if coordinating work with many workers, s3://bucket/prefix/&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  --pdfs PDFS           Path to add pdfs stored in s3 to the workspace, can be a glob path s3://bucket/prefix/*.pdf or path to file containing list of pdf paths&#xA;  --workspace_profile WORKSPACE_PROFILE&#xA;                        S3 configuration profile for accessing the workspace&#xA;  --pdf_profile PDF_PROFILE&#xA;                        S3 configuration profile for accessing the raw pdf documents&#xA;  --pages_per_group PAGES_PER_GROUP&#xA;                        Aiming for this many pdf pages per work item group&#xA;  --max_page_retries MAX_PAGE_RETRIES&#xA;                        Max number of times we will retry rendering a page&#xA;  --max_page_error_rate MAX_PAGE_ERROR_RATE&#xA;                        Rate of allowable failed pages in a document, 1/250 by default&#xA;  --workers WORKERS     Number of workers to run at a time&#xA;  --apply_filter        Apply basic filtering to English pdfs which are not forms, and not likely seo spam&#xA;  --stats               Instead of running any job, reports some statistics about the current workspace&#xA;  --model MODEL         List of paths where you can find the model to convert this pdf. You can specify several different paths here, and the script will try to use the&#xA;                        one which is fastest to access&#xA;  --model_max_context MODEL_MAX_CONTEXT&#xA;                        Maximum context length that the model was fine tuned under&#xA;  --model_chat_template MODEL_CHAT_TEMPLATE&#xA;                        Chat template to pass to sglang server&#xA;  --target_longest_image_dim TARGET_LONGEST_IMAGE_DIM&#xA;                        Dimension on longest side to use for rendering the pdf pages&#xA;  --target_anchor_text_len TARGET_ANCHOR_TEXT_LEN&#xA;                        Maximum amount of anchor text to use (characters)&#xA;  --beaker              Submit this job to beaker instead of running locally&#xA;  --beaker_workspace BEAKER_WORKSPACE&#xA;                        Beaker workspace to submit to&#xA;  --beaker_cluster BEAKER_CLUSTER&#xA;                        Beaker clusters you want to run on&#xA;  --beaker_gpus BEAKER_GPUS&#xA;                        Number of gpu replicas to run&#xA;  --beaker_priority BEAKER_PRIORITY&#xA;                        Beaker priority level for the job&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Team&lt;/h2&gt; &#xA;&lt;!-- start team --&gt; &#xA;&lt;p&gt;&lt;strong&gt;olmOCR&lt;/strong&gt; is developed and maintained by the AllenNLP team, backed by &lt;a href=&#34;https://allenai.org/&#34;&gt;the Allen Institute for Artificial Intelligence (AI2)&lt;/a&gt;. AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. To learn more about who specifically contributed to this codebase, see &lt;a href=&#34;https://github.com/allenai/olmocr/graphs/contributors&#34;&gt;our contributors&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;!-- end team --&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;!-- start license --&gt; &#xA;&lt;p&gt;&lt;strong&gt;olmOCR&lt;/strong&gt; is licensed under &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache 2.0&lt;/a&gt;. A full copy of the license can be found &lt;a href=&#34;https://github.com/allenai/olmocr/raw/main/LICENSE&#34;&gt;on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- end license --&gt;</summary>
  </entry>
  <entry>
    <title>freddyaboulton/fastrtc</title>
    <updated>2025-02-28T01:28:37Z</updated>
    <id>tag:github.com,2025-02-28:/freddyaboulton/fastrtc</id>
    <link href="https://github.com/freddyaboulton/fastrtc" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The python library for real-time communication&lt;/p&gt;&lt;hr&gt;&lt;div style=&#34;text-align: center; margin-bottom: 1rem; display: flex; justify-content: center; align-items: center;&#34;&gt; &#xA; &lt;h1 style=&#34;color: white; margin: 0;&#34;&gt;FastRTC&lt;/h1&gt; &#xA; &lt;img src=&#34;https://huggingface.co/datasets/freddyaboulton/bucket/resolve/main/fastrtc_logo_small.png&#34; alt=&#34;FastRTC Logo&#34; style=&#34;margin-right: 10px;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div style=&#34;display: flex; flex-direction: row; justify-content: center&#34;&gt; &#xA; &lt;img style=&#34;display: block; padding-right: 5px; height: 20px;&#34; alt=&#34;Static Badge&#34; src=&#34;https://img.shields.io/pypi/v/fastrtc&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/freddyaboulton/fastrtc&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Static Badge&#34; src=&#34;https://img.shields.io/badge/github-white?logo=github&amp;amp;logoColor=black&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3 style=&#34;text-align: center&#34;&gt; The Real-Time Communication Library for Python. &lt;/h3&gt; &#xA;&lt;p&gt;Turn any python function into a real-time audio and video stream over WebRTC or WebSockets.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install fastrtc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to use built-in pause detection (see &lt;a href=&#34;https://fastrtc.org/&#34;&gt;ReplyOnPause&lt;/a&gt;), and text to speech (see &lt;a href=&#34;https://fastrtc.org/userguide/audio/#text-to-speech&#34;&gt;Text To Speech&lt;/a&gt;), install the &lt;code&gt;vad&lt;/code&gt; and &lt;code&gt;tts&lt;/code&gt; extras:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install fastrtc[vad, tts]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üó£Ô∏è Automatic Voice Detection and Turn Taking built-in, only worry about the logic for responding to the user.&lt;/li&gt; &#xA; &lt;li&gt;üíª Automatic UI - Use the &lt;code&gt;.ui.launch()&lt;/code&gt; method to launch the webRTC-enabled built-in Gradio UI.&lt;/li&gt; &#xA; &lt;li&gt;üîå Automatic WebRTC Support - Use the &lt;code&gt;.mount(app)&lt;/code&gt; method to mount the stream on a FastAPI app and get a webRTC endpoint for your own frontend!&lt;/li&gt; &#xA; &lt;li&gt;‚ö°Ô∏è Websocket Support - Use the &lt;code&gt;.mount(app)&lt;/code&gt; method to mount the stream on a FastAPI app and get a websocket endpoint for your own frontend!&lt;/li&gt; &#xA; &lt;li&gt;üìû Automatic Telephone Support - Use the &lt;code&gt;fastphone()&lt;/code&gt; method of the stream to launch the application and get a free temporary phone number!&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ Completely customizable backend - A &lt;code&gt;Stream&lt;/code&gt; can easily be mounted on a FastAPI app so you can easily extend it to fit your production application. See the &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/talk-to-claude&#34;&gt;Talk To Claude&lt;/a&gt; demo for an example on how to serve a custom JS frontend.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://fastrtc.org&#34;&gt;https://fastrtc.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://fastrtc.org/pr-preview/pr-60/cookbook/&#34;&gt;Cookbook&lt;/a&gt; for examples of how to use the library.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üó£Ô∏èüëÄ Gemini Audio Video Chat&lt;/h3&gt; &lt;p&gt;Stream BOTH your webcam video and audio feeds to Google Gemini. You can also upload images to augment your conversation!&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/9636dc97-4fee-46bb-abb8-b92e69c08c71&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/freddyaboulton/gemini-audio-video-chat&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/freddyaboulton/gemini-audio-video-chat/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üó£Ô∏è Google Gemini Real Time Voice API&lt;/h3&gt; &lt;p&gt;Talk to Gemini in real time using Google&#39;s voice API.&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/ea6d18cb-8589-422b-9bba-56332d9f61de&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/talk-to-gemini&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/talk-to-gemini/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üó£Ô∏è OpenAI Real Time Voice API&lt;/h3&gt; &lt;p&gt;Talk to ChatGPT in real time using OpenAI&#39;s voice API.&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/178bdadc-f17b-461a-8d26-e915c632ff80&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/talk-to-openai&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/talk-to-openai/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;ü§ñ Hello Computer&lt;/h3&gt; &lt;p&gt;Say computer before asking your question!&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/afb2a3ef-c1ab-4cfb-872d-578f895a10d5&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/hello-computer&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/hello-computer/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;ü§ñ Llama Code Editor&lt;/h3&gt; &lt;p&gt;Create and edit HTML pages with just your voice! Powered by SambaNova systems.&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/98523cf3-dac8-4127-9649-d91a997e3ef5&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/llama-code-editor&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/llama-code-editor/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üó£Ô∏è Talk to Claude&lt;/h3&gt; &lt;p&gt;Use the Anthropic and Play.Ht APIs to have an audio conversation with Claude.&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/fb6ef07f-3ccd-444a-997b-9bc9bdc035d3&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/talk-to-claude&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/talk-to-claude/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üéµ Whisper Transcription&lt;/h3&gt; &lt;p&gt;Have whisper transcribe your speech in real time!&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/87603053-acdc-4c8a-810f-f618c49caafb&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/whisper-realtime&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/whisper-realtime/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üì∑ Yolov10 Object Detection&lt;/h3&gt; &lt;p&gt;Run the Yolov10 model on a user webcam stream in real time!&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/f82feb74-a071-4e81-9110-a01989447ceb&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/object-detection&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/fastrtc/object-detection/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üó£Ô∏è Kyutai Moshi&lt;/h3&gt; &lt;p&gt;Kyutai&#39;s moshi is a novel speech-to-speech model for modeling human conversations.&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/becc7a13-9e89-4a19-9df2-5fb1467a0137&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/freddyaboulton/talk-to-moshi&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/freddyaboulton/talk-to-moshi/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt; &lt;h3&gt;üó£Ô∏è Hello Llama: Stop Word Detection&lt;/h3&gt; &lt;p&gt;A code editor built with Llama 3.3 70b that is triggered by the phrase &#34;Hello Llama&#34;. Build a Siri-like coding assistant in 100 lines of code!&lt;/p&gt; &#xA;    &lt;video width=&#34;100%&#34; src=&#34;https://github.com/user-attachments/assets/3e10cb15-ff1b-4b17-b141-ff0ad852e613&#34; controls&gt;&lt;/video&gt; &lt;p&gt; &lt;a href=&#34;https://huggingface.co/spaces/freddyaboulton/hey-llama-code-editor&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/freddyaboulton/hey-llama-code-editor/blob/main/app.py&#34;&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;This is an shortened version of the official &lt;a href=&#34;https://freddyaboulton.github.io/gradio-webrtc/user-guide/&#34;&gt;usage guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;.ui.launch()&lt;/code&gt;: Launch a built-in UI for easily testing and sharing your stream. Built with &lt;a href=&#34;https://www.gradio.app/&#34;&gt;Gradio&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;.fastphone()&lt;/code&gt;: Get a free temporary phone number to call into your stream. Hugging Face token required.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;.mount(app)&lt;/code&gt;: Mount the stream on a &lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI&lt;/a&gt; app. Perfect for integrating with your already existing production system.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Echo Audio&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastrtc import Stream, ReplyOnPause&#xA;import numpy as np&#xA;&#xA;def echo(audio: tuple[int, np.ndarray]):&#xA;    # The function will be passed the audio until the user pauses&#xA;    # Implement any iterator that yields audio&#xA;    # See &#34;LLM Voice Chat&#34; for a more complete example&#xA;    yield audio&#xA;&#xA;stream = Stream(&#xA;    handler=ReplyOnPause(detection),&#xA;    modality=&#34;audio&#34;, &#xA;    mode=&#34;send-receive&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LLM Voice Chat&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from fastrtc import (&#xA;    ReplyOnPause, AdditionalOutputs, Stream,&#xA;    audio_to_bytes, aggregate_bytes_to_16bit&#xA;)&#xA;import gradio as gr&#xA;from groq import Groq&#xA;import anthropic&#xA;from elevenlabs import ElevenLabs&#xA;&#xA;groq_client = Groq()&#xA;claude_client = anthropic.Anthropic()&#xA;tts_client = ElevenLabs()&#xA;&#xA;&#xA;# See &#34;Talk to Claude&#34; in Cookbook for an example of how to keep &#xA;# track of the chat history.&#xA;def response(&#xA;    audio: tuple[int, np.ndarray],&#xA;):&#xA;    prompt = groq_client.audio.transcriptions.create(&#xA;        file=(&#34;audio-file.mp3&#34;, audio_to_bytes(audio)),&#xA;        model=&#34;whisper-large-v3-turbo&#34;,&#xA;        response_format=&#34;verbose_json&#34;,&#xA;    ).text&#xA;    response = claude_client.messages.create(&#xA;        model=&#34;claude-3-5-haiku-20241022&#34;,&#xA;        max_tokens=512,&#xA;        messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}],&#xA;    )&#xA;    response_text = &#34; &#34;.join(&#xA;        block.text&#xA;        for block in response.content&#xA;        if getattr(block, &#34;type&#34;, None) == &#34;text&#34;&#xA;    )&#xA;    iterator = tts_client.text_to_speech.convert_as_stream(&#xA;        text=response_text,&#xA;        voice_id=&#34;JBFqnCBsd6RMkjVDRZzb&#34;,&#xA;        model_id=&#34;eleven_multilingual_v2&#34;,&#xA;        output_format=&#34;pcm_24000&#34;&#xA;        &#xA;    )&#xA;    for chunk in aggregate_bytes_to_16bit(iterator):&#xA;        audio_array = np.frombuffer(chunk, dtype=np.int16).reshape(1, -1)&#xA;        yield (24000, audio_array)&#xA;&#xA;stream = Stream(&#xA;    modality=&#34;audio&#34;,&#xA;    mode=&#34;send-receive&#34;,&#xA;    handler=ReplyOnPause(response),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Webcam Stream&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastrtc import Stream&#xA;import numpy as np&#xA;&#xA;&#xA;def flip_vertically(image):&#xA;    return np.flip(image, axis=0)&#xA;&#xA;&#xA;stream = Stream(&#xA;    handler=flip_vertically,&#xA;    modality=&#34;video&#34;,&#xA;    mode=&#34;send-receive&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Object Detection&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastrtc import Stream&#xA;import gradio as gr&#xA;import cv2&#xA;from huggingface_hub import hf_hub_download&#xA;from .inference import YOLOv10&#xA;&#xA;model_file = hf_hub_download(&#xA;    repo_id=&#34;onnx-community/yolov10n&#34;, filename=&#34;onnx/model.onnx&#34;&#xA;)&#xA;&#xA;# git clone https://huggingface.co/spaces/fastrtc/object-detection&#xA;# for YOLOv10 implementation&#xA;model = YOLOv10(model_file)&#xA;&#xA;def detection(image, conf_threshold=0.3):&#xA;    image = cv2.resize(image, (model.input_width, model.input_height))&#xA;    new_image = model.detect_objects(image, conf_threshold)&#xA;    return cv2.resize(new_image, (500, 500))&#xA;&#xA;stream = Stream(&#xA;    handler=detection,&#xA;    modality=&#34;video&#34;, &#xA;    mode=&#34;send-receive&#34;,&#xA;    additional_inputs=[&#xA;        gr.Slider(minimum=0, maximum=1, step=0.01, value=0.3)&#xA;    ]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running the Stream&lt;/h2&gt; &#xA;&lt;p&gt;Run:&lt;/p&gt; &#xA;&lt;h3&gt;Gradio&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;stream.ui.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Telephone (Audio Only)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;```py&#xA;stream.fastphone()&#xA;```&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;FastAPI&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;app = FastAPI()&#xA;stream.mount(app)&#xA;&#xA;# Optional: Add routes&#xA;@app.get(&#34;/&#34;)&#xA;async def _():&#xA;    return HTMLResponse(content=open(&#34;index.html&#34;).read())&#xA;&#xA;# uvicorn app:app --host 0.0.0.0 --port 8000&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>