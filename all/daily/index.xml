<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-06T01:30:14Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Nutlope/roomGPT</title>
    <updated>2023-03-06T01:30:14Z</updated>
    <id>tag:github.com,2023-03-06:/Nutlope/roomGPT</id>
    <link href="https://github.com/Nutlope/roomGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Upload a photo of your room to generate your dream room with AI.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://roomGPT.io&#34;&gt;roomGPT.io&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This project generates designs of your room with AI.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://roomGPT.io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Nutlope/roomGPT/main/public/screenshot.png&#34; alt=&#34;Room GPT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;It uses an ML model called &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt; to generate variations of rooms. This application gives you the ability to upload a photo of any room, which will send it through this ML Model using a Next.js API route, and return your generated room. The ML Model is hosted on &lt;a href=&#34;https://replicate.com&#34;&gt;Replicate&lt;/a&gt; and &lt;a href=&#34;https://upload.io&#34;&gt;Upload&lt;/a&gt; is used for image storage.&lt;/p&gt; &#xA;&lt;h2&gt;Running Locally&lt;/h2&gt; &#xA;&lt;h3&gt;Cloning the repository the local machine.&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Nutlope/roomGPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Creating a account on Replicate to get an API key.&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go to &lt;a href=&#34;https://replicate.com/&#34;&gt;Replicate&lt;/a&gt; to make an account.&lt;/li&gt; &#xA; &lt;li&gt;Click on your profile picture in the top right corner, and click on &#34;Dashboard&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Click on &#34;Account&#34; in the navbar. And, here you can find your API token, copy it.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Storing the API keys in .env&lt;/h3&gt; &#xA;&lt;p&gt;Create a file in root directory of project with env. And store your API key in it, as shown in the .example.env file.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d also like to do rate limiting, create an account on UpStash, create a Redis database, and populate the two environment variables in &lt;code&gt;.env&lt;/code&gt; as well. If you don&#39;t want to do rate limiting, you don&#39;t need to make any changes.&lt;/p&gt; &#xA;&lt;h3&gt;Installing the dependencies.&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running the application.&lt;/h3&gt; &#xA;&lt;p&gt;Then, run the application in the command line and it will be available at &lt;code&gt;http://localhost:3000&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;One-Click Deploy&lt;/h2&gt; &#xA;&lt;p&gt;Deploy the example using &lt;a href=&#34;https://vercel.com?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=vercel-examples&#34;&gt;Vercel&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://vercel.com/new/clone?repository-url=https://github.com/Nutlope/roomGPT&amp;amp;env=REPLICATE_API_KEY&amp;amp;project-name=room-GPT&amp;amp;repo-name=roomGPT&#34;&gt;&lt;img src=&#34;https://vercel.com/button&#34; alt=&#34;Deploy with Vercel&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yu-takagi/StableDiffusionReconstruction</title>
    <updated>2023-03-06T01:30:14Z</updated>
    <id>tag:github.com,2023-03-06:/yu-takagi/StableDiffusionReconstruction</id>
    <link href="https://github.com/yu-takagi/StableDiffusionReconstruction" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Takagi and Nishimoto, CVPR 2023&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;High-resolution image reconstruction with latent diffusion models from human brain activity&lt;/h1&gt; &#xA;&lt;p&gt;Takagi and Nishimoto, CVPR 2023&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2022.11.18.517004v2&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/stablediffusion-with-brain/&#34;&gt;Project Page&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h1&gt;Overview&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/yu-takagi/StableDiffusionReconstruction/main/visual_summary.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Abstract&lt;/h1&gt; &#xA;&lt;p&gt;Reconstructing visual experiences from human brain activity offers a unique way to understand how the brain represents the world, and to interpret the connection between computer vision models and our visual system. While deep generative models have recently been employed for this task, reconstructing realistic images with high semantic fidelity is still a challenging problem. Here, we propose a new method based on a diffusion model (DM) to reconstruct images from human brain activity obtained via functional magnetic resonance imaging (fMRI). More specifically, we rely on a latent diffusion model (LDM) termed Stable Diffusion. This model reduces the computational cost of DMs, while preserving their high generative performance. We also characterize the inner mechanisms of the LDM by studying how its different components (such as the latent vector Z, conditioning inputs C, and different elements of the denoising U-Net) relate to distinct brain functions. We show that our proposed method can reconstruct high-resolution images with high fidelity in straightforward fashion, without the need for any additional training and fine-tuning of complex deep-learning models. We also provide a quantitative interpretation of different LDM components from a neuroscientific perspective. Overall, our study proposes a promising method for reconstructing images from human brain activity, and provides a new framework for understanding DMs.&lt;/p&gt; &#xA;&lt;h1&gt;Others&lt;/h1&gt; &#xA;&lt;p&gt;Coming soon&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>innnky/so-vits-svc</title>
    <updated>2023-03-06T01:30:14Z</updated>
    <id>tag:github.com,2023-03-06:/innnky/so-vits-svc</id>
    <link href="https://github.com/innnky/so-vits-svc" rel="alternate"></link>
    <summary type="html">&lt;p&gt;基于vits与softvc的歌声音色转换模型&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SoftVC VITS Singing Voice Conversion&lt;/h1&gt; &#xA;&lt;h2&gt;强调！！！！！！！！！！！！&lt;/h2&gt; &#xA;&lt;p&gt;SoVits是语音转换 (说话人转换)，作用是将一个音频中语音的音色转化为目标说话人的音色，并不是TTS (文本转语音)，SoVits虽然基于Vits开发，但两者是两个不同的项目，请不要搞混，要训练TTS请前往 &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;使用规约&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;请自行解决数据集的授权问题，任何由于使用非授权数据集进行训练造成的问题，需自行承担全部责任和一切后果，与sovits无关！&lt;/li&gt; &#xA; &lt;li&gt;任何发布到视频平台的基于sovits制作的视频，都必须要在简介明确指明用于变声器转换的输入源歌声、音频，例如：使用他人发布的视频/音频，通过分离的人声作为输入源进行转换的，必须要给出明确的原视频、音乐链接；若使用是自己的人声，或是使用其他歌声合成引擎合成的声音作为输入源进行转换的，也必须在简介加以说明。&lt;/li&gt; &#xA; &lt;li&gt;由输入源造成的侵权问题需自行承担全部责任和一切后果。使用其他商用歌声合成软件作为输入源时，请确保遵守该软件的使用条例，注意，许多歌声合成引擎使用条例中明确指明不可用于输入源进行转换！&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;English docs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/Eng_docs.md&#34;&gt;Check here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;4.0模型及colab脚本已更新&lt;/strong&gt;：在&lt;a href=&#34;https://github.com/innnky/so-vits-svc/tree/4.0&#34;&gt;4.0分支&lt;/a&gt; 统一采样率使用44100hz（但推理显存占用比3.0的32khz还小），更换特征提取为contentvec， 目前稳定性还没有经过广泛测试&lt;/p&gt; &#xA; &lt;p&gt;据不完全统计，多说话人似乎会导致&lt;strong&gt;音色泄漏加重&lt;/strong&gt;，不建议训练超过5人的模型，目前的建议是如果想炼出来更像目标音色，&lt;strong&gt;尽可能炼单说话人的&lt;/strong&gt;&lt;br&gt; 断音问题已解决，音质提升了不少&lt;br&gt; 2.0版本已经移至 sovits_2.0分支&lt;br&gt; 3.0版本使用FreeVC的代码结构，与旧版本不通用&lt;br&gt; 与&lt;a href=&#34;https://github.com/prophesier/diff-svc&#34;&gt;DiffSVC&lt;/a&gt; 相比，在训练数据质量非常高时diffsvc有着更好的表现，对于质量差一些的数据集，本仓库可能会有更好的表现，此外，本仓库推理速度上比diffsvc快很多&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;模型简介&lt;/h2&gt; &#xA;&lt;p&gt;歌声音色转换模型，通过SoftVC内容编码器提取源音频语音特征，与F0同时输入VITS替换原本的文本输入达到歌声转换的效果。同时，更换声码器为 &lt;a href=&#34;https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan&#34;&gt;NSF HiFiGAN&lt;/a&gt; 解决断音问题&lt;/p&gt; &#xA;&lt;h2&gt;注意&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;当前分支是32khz版本的分支，32khz模型推理更快，显存占用大幅减小，数据集所占硬盘空间也大幅降低，推荐训练该版本模型&lt;/li&gt; &#xA; &lt;li&gt;如果要训练48khz的模型请切换到&lt;a href=&#34;https://github.com/innnky/so-vits-svc/tree/main&#34;&gt;main分支&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;预先下载的模型文件&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;soft vc hubert：&lt;a href=&#34;https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt&#34;&gt;hubert-soft-0d54a1f4.pt&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;放在&lt;code&gt;hubert&lt;/code&gt;目录下&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;预训练底模文件 &lt;a href=&#34;https://huggingface.co/innnky/sovits_pretrained/resolve/main/G_0.pth&#34;&gt;G_0.pth&lt;/a&gt; 与 &lt;a href=&#34;https://huggingface.co/innnky/sovits_pretrained/resolve/main/D_0.pth&#34;&gt;D_0.pth&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;放在&lt;code&gt;logs/32k&lt;/code&gt;目录下&lt;/li&gt; &#xA;   &lt;li&gt;预训练底模为必选项，因为据测试从零开始训练有概率不收敛，同时底模也能加快训练速度&lt;/li&gt; &#xA;   &lt;li&gt;预训练底模训练数据集包含云灏 即霜 辉宇·星AI 派蒙 绫地宁宁，覆盖男女生常见音域，可以认为是相对通用的底模&lt;/li&gt; &#xA;   &lt;li&gt;底模删除了&lt;code&gt;optimizer speaker_embedding&lt;/code&gt;等无关权重, 只可以用于初始化训练，无法用于推理&lt;/li&gt; &#xA;   &lt;li&gt;该底模和48khz底模通用&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 一键下载&#xA;# hubert&#xA;wget -P hubert/ https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt&#xA;# G与D预训练模型&#xA;wget -P logs/32k/ https://huggingface.co/innnky/sovits_pretrained/resolve/main/G_0.pth&#xA;wget -P logs/32k/ https://huggingface.co/innnky/sovits_pretrained/resolve/main/D_0.pth&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;colab一键数据集制作、训练脚本&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1_-gh9i-wCPNlRZw6pYF-9UufetcVrGBX?usp=sharing&#34;&gt;一键colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;数据集准备&lt;/h2&gt; &#xA;&lt;p&gt;仅需要以以下文件结构将数据集放入dataset_raw目录即可&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;dataset_raw&#xA;├───speaker0&#xA;│   ├───xxx1-xxx1.wav&#xA;│   ├───...&#xA;│   └───Lxx-0xx8.wav&#xA;└───speaker1&#xA;    ├───xx2-0xxx2.wav&#xA;    ├───...&#xA;    └───xxx7-xxx007.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;数据预处理&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;重采样至 32khz&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python resample.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;自动划分训练集 验证集 测试集 以及自动生成配置文件&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python preprocess_flist_config.py&#xA;# 注意&#xA;# 自动生成的配置文件中，说话人数量n_speakers会自动按照数据集中的人数而定&#xA;# 为了给之后添加说话人留下一定空间，n_speakers自动设置为 当前数据集人数乘2&#xA;# 如果想多留一些空位可以在此步骤后 自行修改生成的config.json中n_speakers数量&#xA;# 一旦模型开始训练后此项不可再更改&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;生成hubert与f0&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python preprocess_hubert_f0.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;执行完以上步骤后 dataset 目录便是预处理完成的数据，可以删除dataset_raw文件夹了&lt;/p&gt; &#xA;&lt;h2&gt;训练&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py -c configs/config.json -m 32k&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;推理&lt;/h2&gt; &#xA;&lt;p&gt;使用 &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/inference_main.py&#34;&gt;inference_main.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;更改&lt;code&gt;model_path&lt;/code&gt;为你自己训练的最新模型记录点&lt;/li&gt; &#xA; &lt;li&gt;将待转换的音频放在&lt;code&gt;raw&lt;/code&gt;文件夹下&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;clean_names&lt;/code&gt; 写待转换的音频名称&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;trans&lt;/code&gt; 填写变调半音数量&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;spk_list&lt;/code&gt; 填写合成的说话人名称&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Onnx导出&lt;/h2&gt; &#xA;&lt;h3&gt;重要的事情说三遍：导出Onnx时，请重新克隆整个仓库！！！导出Onnx时，请重新克隆整个仓库！！！导出Onnx时，请重新克隆整个仓库！！！&lt;/h3&gt; &#xA;&lt;p&gt;使用 &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/onnx_export.py&#34;&gt;onnx_export.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;新建文件夹：&lt;code&gt;checkpoints&lt;/code&gt; 并打开&lt;/li&gt; &#xA; &lt;li&gt;在&lt;code&gt;checkpoints&lt;/code&gt;文件夹中新建一个文件夹作为项目文件夹，文件夹名为你的项目名称，比如&lt;code&gt;aziplayer&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;将你的模型更名为&lt;code&gt;model.pth&lt;/code&gt;，配置文件更名为&lt;code&gt;config.json&lt;/code&gt;，并放置到刚才创建的&lt;code&gt;aziplayer&lt;/code&gt;文件夹下&lt;/li&gt; &#xA; &lt;li&gt;将 &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/onnx_export.py&#34;&gt;onnx_export.py&lt;/a&gt; 中&lt;code&gt;path = &#34;NyaruTaffy&#34;&lt;/code&gt; 的 &lt;code&gt;&#34;NyaruTaffy&#34;&lt;/code&gt; 修改为你的项目名称，&lt;code&gt;path = &#34;aziplayer&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;运行 &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/onnx_export.py&#34;&gt;onnx_export.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;等待执行完毕，在你的项目文件夹下会生成一个&lt;code&gt;model.onnx&lt;/code&gt;，即为导出的模型&lt;/li&gt; &#xA; &lt;li&gt;注意：若想导出48K模型，请按照以下步骤修改文件，或者直接使用&lt;code&gt;model_onnx_48k.py&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;请打开&lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/model_onnx.py&#34;&gt;model_onnx.py&lt;/a&gt;，将其中最后一个class&lt;code&gt;SynthesizerTrn&lt;/code&gt;的hps中&lt;code&gt;sampling_rate&lt;/code&gt;32000改为48000&lt;/li&gt; &#xA;   &lt;li&gt;请打开&lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/vdecoder/hifigan/nvSTFT.py&#34;&gt;nvSTFT&lt;/a&gt;，将其中所有32000改为48000&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;h3&gt;Onnx模型支持的UI&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/NaruseMioShirakana/MoeSS&#34;&gt;MoeSS&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;我去除了所有的训练用函数和一切复杂的转置，一行都没有保留，因为我认为只有去除了这些东西，才知道你用的是Onnx&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Gradio（WebUI）&lt;/h2&gt; &#xA;&lt;p&gt;使用 &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/sovits_gradio.py&#34;&gt;sovits_gradio.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;新建文件夹：checkpoints 并打开&lt;/li&gt; &#xA; &lt;li&gt;在checkpoints文件夹中新建一个文件夹作为项目文件夹，文件夹名为你的项目名称&lt;/li&gt; &#xA; &lt;li&gt;将你的模型更名为model.pth，配置文件更名为config.json，并放置到刚才创建的文件夹下&lt;/li&gt; &#xA; &lt;li&gt;运行 &lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/sovits_gradio.py&#34;&gt;sovits_gradio.py&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>