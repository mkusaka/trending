<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-27T01:28:17Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs</title>
    <updated>2024-05-27T01:28:17Z</updated>
    <id>tag:github.com,2024-05-27:/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs</id>
    <link href="https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The codes about &#34;Uni-MoE: Scaling Unified Multimodal Models with Mixture of Experts&#34;&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://s21.ax1x.com/2024/05/14/pkmtDSS.png&#34; width=&#34;250&#34; style=&#34;margin-bottom: 0.2;&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2 align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/&#34;&gt;Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h5 align=&#34;center&#34;&gt; If you appreciate our project, please consider giving us a star ‚≠ê on GitHub to stay updated with the latest developments. &lt;/h5&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;p&gt;üöÄ Welcome to the repo of &lt;strong&gt;Uni-MOE&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;Uni-MoE is a MoE-based unified multimodal model and can handle diverse modalities including audio, speech, image, text, and video.&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://uni-moe.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-Uni_MoE-blue&#34; alt=&#34;Project Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/master/todo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-todo-orange&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2405.11273&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-arxiv-yellow&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://yunxinli.github.io&#34;&gt;Yunxin Li&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/master/URL&#34;&gt;Shenyuan Jiang&lt;/a&gt;, &lt;a href=&#34;https://faculty.hitsz.edu.cn/hubaotian&#34;&gt;Baotian Hu&lt;/a&gt;, &lt;a href=&#34;http://www.longyuewang.com/&#34;&gt;Longyue Wang&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/master/URL&#34;&gt;Wanqi Zhong&lt;/a&gt;, &lt;a href=&#34;https://whluo.github.io/&#34;&gt;Wenhan Luo&lt;/a&gt;, &lt;a href=&#34;https://forestlinma.com/&#34;&gt;Lin Ma&lt;/a&gt;, &lt;a href=&#34;https://faculty.hitsz.edu.cn/MinZhang&#34;&gt;Min Zhang&lt;/a&gt;&lt;/p&gt; &lt;/h4&gt; &#xA;&lt;h2&gt;üî• News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[4/28] üî• We have upgraded the Uni-MoE codebase to facilitate training across multiple Nodes and GPUs. Explore this enhanced functionality in our revamped &lt;a href=&#34;https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/raw/master/Uni_MoE/finetune_speech_dp.sh&#34;&gt;fine-tuning script&lt;/a&gt;. Additionally, we have introduced a version that integrates distributed MoE modules. This enhancement allows for training our model with parallel processing at both the expert and modality levels, enhancing efficiency and scalability. For more details, please refer to the &lt;a href=&#34;https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/tree/master/Uni_MoE_v2&#34;&gt;Uni_MoE_v2&lt;/a&gt; documentation.&lt;/li&gt; &#xA; &lt;li&gt;[3/7] üî• We released &lt;strong&gt;Uni-MOE: Scaling Unified Multimodal LLMs with Mixture of Experts&lt;/strong&gt;. We proposed the development of a unified Multimodal LLM (MLLM) utilizing the MoE framework, which can process diverse modalities, including audio, image, text, and video. Checkout the &lt;a href=&#34;https://raw.githubusercontent.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/master/TODO&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/master/TODO&#34;&gt;demo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage and License Notices&lt;/strong&gt;: The data and checkpoint are intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA and Vicuna. The dataset and models trained using the dataset should not be used outside of research purposes.&lt;/p&gt; &#xA;&lt;h2&gt;üé® Case Show&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/raw/master/case_figure.png&#34; height=&#34;100%&#34; width=&#34;75%&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üìÄ Demo Video&lt;/h2&gt; &#xA;&lt;p&gt;Demo 2 contains the real-time understanding of speech (Starting from 30S).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://private-user-images.githubusercontent.com/45393746/331798338-dfc848a2-1fd2-4f8d-9274-f21f7118ecd9.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTYwMzUwOTUsIm5iZiI6MTcxNjAzNDc5NSwicGF0aCI6Ii80NTM5Mzc0Ni8zMzE3OTgzMzgtZGZjODQ4YTItMWZkMi00ZjhkLTkyNzQtZjIxZjcxMThlY2Q5Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MTglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTE4VDEyMTk1NVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYzYTNmZDNlM2FhOGE3MmM1MzM0Mzk4YTdlYTg3NTgzOTBmNzMyMjM4OTljYTA0ODQ0YmEzZDVlYmFhOWUwMzUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.vrqxhusaq3J_ULQKbeGOxEJH3wry6GjXLxwrFrP0jao&#34;&gt;https://private-user-images.githubusercontent.com/45393746/331798338-dfc848a2-1fd2-4f8d-9274-f21f7118ecd9.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTYwMzUwOTUsIm5iZiI6MTcxNjAzNDc5NSwicGF0aCI6Ii80NTM5Mzc0Ni8zMzE3OTgzMzgtZGZjODQ4YTItMWZkMi00ZjhkLTkyNzQtZjIxZjcxMThlY2Q5Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MTglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTE4VDEyMTk1NVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYzYTNmZDNlM2FhOGE3MmM1MzM0Mzk4YTdlYTg3NTgzOTBmNzMyMjM4OTljYTA0ODQ0YmEzZDVlYmFhOWUwMzUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.vrqxhusaq3J_ULQKbeGOxEJH3wry6GjXLxwrFrP0jao&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://private-user-images.githubusercontent.com/45393746/331798343-fcd3eb7e-3dfa-4470-a2e6-b9b140efe0fa.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTYwMzUyNTEsIm5iZiI6MTcxNjAzNDk1MSwicGF0aCI6Ii80NTM5Mzc0Ni8zMzE3OTgzNDMtZmNkM2ViN2UtM2RmYS00NDcwLWEyZTYtYjliMTQwZWZlMGZhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MTglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTE4VDEyMjIzMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIyNWU5OTM0NjM1MTgzMWIxNWI4MDllYzU5NWNlOTUxMGI1NzQ5MzkyNmRlNDFlMTY0YzYzMTJmZjk4ZjJmMWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Uz3PBfKbEjl5ZOfUSXrAaQQLrvKwCFK2uNPTjtKG3dU&#34;&gt;https://private-user-images.githubusercontent.com/45393746/331798343-fcd3eb7e-3dfa-4470-a2e6-b9b140efe0fa.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTYwMzUyNTEsIm5iZiI6MTcxNjAzNDk1MSwicGF0aCI6Ii80NTM5Mzc0Ni8zMzE3OTgzNDMtZmNkM2ViN2UtM2RmYS00NDcwLWEyZTYtYjliMTQwZWZlMGZhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MTglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTE4VDEyMjIzMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIyNWU5OTM0NjM1MTgzMWIxNWI4MDllYzU5NWNlOTUxMGI1NzQ5MzkyNmRlNDFlMTY0YzYzMTJmZjk4ZjJmMWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Uz3PBfKbEjl5ZOfUSXrAaQQLrvKwCFK2uNPTjtKG3dU&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üåü Structure&lt;/h2&gt; &#xA;&lt;p&gt;The model architecture of Uni-MoE is shown below. Three training stages contain: 1) Utilize pairs from different modalities and languages to build connectors that map these elements to a unified language space, establishing a foundation for multimodal understanding; 2) Develop modality-specific experts using cross-modal data to ensure deep understanding, preparing for a cohesive multi-expert model; 3) Incorporate multiple trained experts into LLMs and refine the unified multimodal model using the LoRA technique on mixed multimodal data.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/raw/master/model.png&#34; height=&#34;100%&#34; width=&#34;75%&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;‚ö°Ô∏è Install&lt;/h2&gt; &#xA;&lt;p&gt;The following instructions are for Linux installation. We would like to recommend the requirements as follows.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python == 3.9.16&lt;/li&gt; &#xA; &lt;li&gt;CUDA Version &amp;gt;= 11.7&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository and navigate to the Uni-MoE folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs.git&#xA;cd UMOE-Scaling-Unified-Multimodal-LLMs/Uni_MoE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Package&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n unimoe python==3.9.16&#xA;conda activate unimoe&#xA;pip install -r env.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Replace all the absolute pathnames &#39;/path/to/&#39; with your specific path to the Uni-MoE file&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;‚ö°Ô∏è Uni-MOE Weights&lt;/h2&gt; &#xA;&lt;p&gt;To use our model, all weights should be downloaded.&lt;/p&gt; &#xA;&lt;p&gt;After downloading all of them, organize the weights as follows in &#39;Uni_MoE/checkpoint&#39; folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îî‚îÄ‚îÄ checkpoint&#xA;    ‚îú‚îÄ‚îÄ Uni-MoE-audio-base&#xA;    ‚îú‚îÄ‚îÄ Uni-MoE-audio-e2&#xA;    ‚îú‚îÄ‚îÄ Uni-MoE-speech-base&#xA;    ‚îú‚îÄ‚îÄ Uni-MoE-speech-e2&#xA;    ‚îú‚îÄ‚îÄ Uni-MoE-speech-base-interval&#xA;    ‚îú‚îÄ‚îÄ Uni-MoE-speech-v1.5&#xA;    ‚îú‚îÄ‚îÄ clip-vit-large-patch14-336&#xA;    ‚îú‚îÄ‚îÄ whisper-small&#xA;    ‚îî‚îÄ‚îÄ BEATs_iter3_plus_AS2M.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;vision encoder&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai/clip-vit-large-patch14-336/tree/main&#34;&gt;CLIP ViT-L/14 336px&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;speech encoder&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openai/whisper-small/tree/main&#34;&gt;whisper small&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;audio encoder&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://valle.blob.core.windows.net/share/BEATs/BEATs_iter3_plus_AS2M.pt?sv=2020-08-04&amp;amp;st=2023-03-01T07%3A51%3A05Z&amp;amp;se=2033-03-02T07%3A51%3A00Z&amp;amp;sr=c&amp;amp;sp=rl&amp;amp;sig=QJXmSJG9DbMKf48UDIU1MfzIro8HQOf3sqlNXiflY1I%3D&#34;&gt;Fine-tuned BEATs_iter3+ (AS2M)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Uni-MoE-audio-base-model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/VictorJsy/Uni-MoE-audio-base/tree/main&#34;&gt;Uni-MoE/Uni-MoE-audio-base&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Uni-MoE-audio-fine-tuned-chekpoint&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/VictorJsy/Uni-MoE-audio-e2/tree/main&#34;&gt;Uni-MoE/Uni-MoE-audio-e2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Uni-MoE-speech-base-model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/VictorJsy/Uni-MoE-speech-base/tree/main&#34;&gt;Uni-MoE/Uni-MoE-speech-base&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Uni-MoE-speech-fine-tuned-chekpoint&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/VictorJsy/Uni-MoE-speech-e2/tree/main&#34;&gt;Uni-MoE/Uni-MoE-speech-e2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Uni-MoE-speech-base-interval&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/VictorJsy/Uni-MoE-speech-base-interval&#34;&gt;Uni-MoE/Uni-MoE-speech-base-interval&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Uni-MoE-speech-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/VictorJsy/Uni-MoE-speech-v1.5&#34;&gt;Uni-MoE/Uni-MoE-speech-v1.5&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Uni-MoE-speech refers to the MOE-Task2 and Uni-MoE-audio refers to the MOE-Task3 in our paper.&lt;/li&gt; &#xA; &lt;li&gt;&#39;Uni-MoE-base&#39; is the backbone containing LLMs and trained parameters gained from Training Stage 2: Training Modality-Specific Expert.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üóùÔ∏è Dataset&lt;/h2&gt; &#xA;&lt;h3&gt;Training Data&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;DataSet&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K&#34;&gt;LLaVA-Instruct-150K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://images.cocodataset.org/zips/train2014.zip&#34;&gt;image(train2014)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mbzuai-oryx/Video-ChatGPT&#34;&gt;Video-Instruct-Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/&#34;&gt;video(from youtube)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/cvssp/WavCaps/tree/main/json_files&#34;&gt;WavCaps&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/cvssp/WavCaps/tree/main/Zip_files&#34;&gt;audio&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://audiocaps.github.io/&#34;&gt;AudioCaps&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://audiocaps.github.io/&#34;&gt;audio(Cap)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zenodo.org/records/6473207&#34;&gt;ClothoAQA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zenodo.org/records/6473207&#34;&gt;audio(QA)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zenodo.org/records/3490684&#34;&gt;ClothoV1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zenodo.org/records/3490684&#34;&gt;audio(Cap)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://affective-meld.github.io/&#34;&gt;MELD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://affective-meld.github.io/&#34;&gt;audio(Music)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/race/tree/main&#34;&gt;RACE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~glai1/data/race/&#34;&gt;Speech(TTS)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.openslr.org/12&#34;&gt;LibriSpeech&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.openslr.org/12&#34;&gt;Speech(Long)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We use TTS technical to convert long text to speech to construct long speech understanding data.&lt;/p&gt; &#xA;&lt;p&gt;Overall, all training tasks (&lt;em&gt;16 comparative experiments covering models with single-expert and MoE configurations&lt;/em&gt;) are as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Training Tasks&lt;/th&gt; &#xA;   &lt;th&gt;Data Types&lt;/th&gt; &#xA;   &lt;th&gt;Data Size&lt;/th&gt; &#xA;   &lt;th&gt;Epochs&lt;/th&gt; &#xA;   &lt;th&gt;Trainable Modules&lt;/th&gt; &#xA;   &lt;th&gt;Pretraining tasks&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Audio-Language Pretraining&lt;/td&gt; &#xA;   &lt;td&gt;WaveCaps*, Audiocap*, MELD, ClothoV1&lt;/td&gt; &#xA;   &lt;td&gt;194K&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;Audio Q-former, Audio projection layer&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speech-Language Pretraining&lt;/td&gt; &#xA;   &lt;td&gt;Common Voice (Short Speech)&lt;/td&gt; &#xA;   &lt;td&gt;1.7M&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;Speech Q-former, Speech projection layer&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Single-Modality-Expert-Task1&lt;/td&gt; &#xA;   &lt;td&gt;LLaVA-Instruction-150K(I-A)&lt;/td&gt; &#xA;   &lt;td&gt;150K&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;LoRA, Speech projection layer&lt;/td&gt; &#xA;   &lt;td&gt;Speech-pretrain-task&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Single-Modality-Expert-Task2&lt;/td&gt; &#xA;   &lt;td&gt;LLaVA-Instruction-150K(T-I)&lt;/td&gt; &#xA;   &lt;td&gt;150K&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;LoRA, Image projection layer&lt;/td&gt; &#xA;   &lt;td&gt;Speech-pretrain-task&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Single-Modality-Expert-Task3&lt;/td&gt; &#xA;   &lt;td&gt;LLaVA-Instruction-150K(I-A)&lt;/td&gt; &#xA;   &lt;td&gt;150K&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;LoRA, Speech Q-former, Speech and Image projection layer&lt;/td&gt; &#xA;   &lt;td&gt;Speech-pretrain-task&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Single-Modality-Expert-Task4&lt;/td&gt; &#xA;   &lt;td&gt;LLaVA-Instruction-150K(I-A), RACE(T-A), LibriSpeech&lt;/td&gt; &#xA;   &lt;td&gt;271K&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;LoRA, Speech &amp;amp; Image projection&lt;/td&gt; &#xA;   &lt;td&gt;Speech-pretrain-task&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Single-Modality-Expert-Task5&lt;/td&gt; &#xA;   &lt;td&gt;LLaVA-Instruction-150K(T-I), RACE(T-A), LibriSpeech&lt;/td&gt; &#xA;   &lt;td&gt;271K&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;LoRA, Speech &amp;amp; Image projection&lt;/td&gt; &#xA;   &lt;td&gt;Speech-pretrain-task&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Single-Modality-Expert-Task6&lt;/td&gt; &#xA;   &lt;td&gt;LLaVA-Instruction-150K(I-A), LLaVA-Instruction-150K(T-I), RACE(T-A), LibriSpeech&lt;/td&gt; &#xA;   &lt;td&gt;421K&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;LoRA, Speech &amp;amp; Image projection&lt;/td&gt; &#xA;   &lt;td&gt;Speech-pretrain-task&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Single-Modality-Expert-Task7&lt;/td&gt; &#xA;   &lt;td&gt;RACE(T-A), LibriSpeech, RACE(T-A)-MC&lt;/td&gt; &#xA;   &lt;td&gt;209K&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;LoRA, Speech projection layer&lt;/td&gt; &#xA;   &lt;td&gt;Speech-pretrain-task&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Single-Modality-Expert-Task8&lt;/td&gt; &#xA;   &lt;td&gt;WaveCaps*, Audiocap*, MELD, ClothoAQA, ClothoV1&lt;/td&gt; &#xA;   &lt;td&gt;203K&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;LoRA, Audio projection layer&lt;/td&gt; &#xA;   &lt;td&gt;Audio-pretrain-task&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MoE-Task1&lt;/td&gt; &#xA;   &lt;td&gt;LLaVA-Instruction-Dataset(T-I), LLaVA-Instruction-150K(I-A), RACE(T-A), LibriSpeech, RACE(T-A)-MC&lt;/td&gt; &#xA;   &lt;td&gt;509K&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;LoRA, Router, speech &amp;amp; image projection layer&lt;/td&gt; &#xA;   &lt;td&gt;LLava-V1.5-LoRA, Single-Modality-Expert-Tasks 2/3/7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MoE-Task1-short-speech&lt;/td&gt; &#xA;   &lt;td&gt;LLaVA-Instruction-Dataset(T-I), LLaVA-Instruction-150K(I-A)&lt;/td&gt; &#xA;   &lt;td&gt;300K&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;LoRA, Router, speech &amp;amp; image projection layer&lt;/td&gt; &#xA;   &lt;td&gt;LLava-V1.5-LoRA, Single-Modality-Expert-Tasks 2/3/7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MoE-Task2&lt;/td&gt; &#xA;   &lt;td&gt;Video-Instruction-150K, LLaVA-Instruction-Dataset(T-I), RACE(T-A), LibriSpeech, RACE(T-A)-MC&lt;/td&gt; &#xA;   &lt;td&gt;459K&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;LoRA, Router, speech &amp;amp; image projection layer&lt;/td&gt; &#xA;   &lt;td&gt;Llava-v1.5-LoRA, Single-Modality-Expert-Tasks 2/3/7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MoE-Task3&lt;/td&gt; &#xA;   &lt;td&gt;Video-Instruction-150K, LLaVA-Instruction-Dataset(T-I), WaveCaps*, Audiocap*, MELD, ClothoAQA, ClothoV1&lt;/td&gt; &#xA;   &lt;td&gt;453K&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;LoRA, Router, audio &amp;amp; image projection layer&lt;/td&gt; &#xA;   &lt;td&gt;LLava-V1.5-LoRA, Single-Modality-Expert-Tasks 2/3/8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pure-MoE-Task1&lt;/td&gt; &#xA;   &lt;td&gt;Video-Instruction-Dataset, LLaVA-Instruction-Dataset(T-I), WaveCaps*, Audiocap*, MELD, ClothoAQA, ClothoV1&lt;/td&gt; &#xA;   &lt;td&gt;453K&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;LoRA, Router, audio &amp;amp; image projection layer&lt;/td&gt; &#xA;   &lt;td&gt;LLava-V1.5-LoRA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pure-MoE-Task2&lt;/td&gt; &#xA;   &lt;td&gt;Video-Instruction-Dataset, LLaVA-Instruction-Dataset(T-I), WaveCaps*, Audiocap*, MELD, ClothoAQA, ClothoV1&lt;/td&gt; &#xA;   &lt;td&gt;453K&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;LoRA, Router, audio &amp;amp; image projection layer&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;code&gt;*&lt;/code&gt; refers to the fact that the dataset we use is only a subset. &lt;code&gt;MC&lt;/code&gt; represents the multi-choice setting. &lt;code&gt;I-A&lt;/code&gt; means image-audio pairs, which convert the question into the corresponding speech. &lt;code&gt;T-I&lt;/code&gt; shows the original text-image pairs. &lt;code&gt;T-A&lt;/code&gt; indicates the contextual paragraph of the RACE dataset is transferred into the long speech. &lt;code&gt;Pretraining task&lt;/code&gt; represents the tasks included in the previous training stage.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation Data&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;DataSet&lt;/th&gt; &#xA;   &lt;th&gt;Input Type&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://allenai.org/project/a-okvqa/home&#34;&gt;AOKVQA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://okvqa.allenai.org/&#34;&gt;OKVQA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://visualqa.org/&#34;&gt;VQAv2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zenodo.org/records/6473207&#34;&gt;ClothoAQA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Audio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zenodo.org/records/3490684&#34;&gt;ClothoV1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Audio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zenodo.org/records/3490684&#34;&gt;ClothoV2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Audio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/RUCAIBox/POPE&#34;&gt;POPE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://textvqa.org/dataset/&#34;&gt;TextVQA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yuweihao/MM-Vet&#34;&gt;MM-Vet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ailab-cvc/seed-bench?tab=readme-ov-file&#34;&gt;SEEDBench(Image)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mmbench.opencompass.org.cn/home&#34;&gt;MMBench&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mmbench.opencompass.org.cn/home&#34;&gt;MMBench-Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Image-Speech(Long)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/VictorJsy/College-Entrance-English-Examination-Listening-Part/tree/main&#34;&gt;English-High-School-Listening&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Speech(Long)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/race/tree/main&#34;&gt;RACE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Speech(Long)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.cs.utexas.edu/users/ml/clamp/videoDescription/&#34;&gt;MSVD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Video-Audio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MILVLG/activitynet-qa&#34;&gt;Activitynet-QA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-Video-Audio&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;College Entrance English Examination Listening Part&lt;/h4&gt; &#xA;&lt;p&gt;We build a real speech understanding dataset to check the practical long speech recognition capabilities: &lt;a href=&#34;https://huggingface.co/datasets/VictorJsy/College-Entrance-English-Examination-Listening-Part/tree/main&#34;&gt;English-High-School-Listening&lt;/a&gt; It comprises 150 questions related to long audio segments with an average length of 109 seconds, and 50 questions about short audio segments with an average length of 14 seconds.&lt;/p&gt; &#xA;&lt;h2&gt;üåà How to infer and deploy your demo&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure that all the weights are downloaded and the running environment is set correctly.&lt;/li&gt; &#xA; &lt;li&gt;run inference scripts &lt;a href=&#34;https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/raw/master/Uni_MoE/inference_audio.sh&#34;&gt;&lt;code&gt;inference_audio.sh&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/raw/master/Uni_MoE/inference_speech.sh&#34;&gt;&lt;code&gt;inference_speech.sh&lt;/code&gt;&lt;/a&gt; using &lt;code&gt;bash inference_audio.sh&lt;/code&gt; &lt;code&gt;bash inference_speech.sh&lt;/code&gt; or run the following commands to inference:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /path/to/Uni_MoE&#xA;conda activate unimoe&#xA;python Uni_MoE_audio/inference_all.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /path/to/Uni_MoE&#xA;conda activate unimoe&#xA;python Uni_MoE_speech/inference_all.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To launch the online demo ( It is highly recommended to launch the demo with &lt;a href=&#34;https://huggingface.co/VictorJsy/Uni-MoE-speech-v1.5&#34;&gt;Uni-MoE-speech-v1.5&lt;/a&gt; that need the basic parameters of &lt;a href=&#34;https://huggingface.co/VictorJsy/Uni-MoE-speech-base-interval&#34;&gt;Uni-MoE-speech-base-interval&lt;/a&gt;), run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /path/to/Uni_MoE&#xA;conda activate unimoe&#xA;python demo/demo.py&#xA;python demo/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üåà How to train and evaluate on datasets&lt;/h2&gt; &#xA;&lt;p&gt;Training:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure that all the weights are downloaded and the environment is set correctly, especially for the base model.&lt;/li&gt; &#xA; &lt;li&gt;Our training data can be downloaded from &lt;a href=&#34;https://raw.githubusercontent.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/master/url&#34;&gt;UMOE-Speech-453k.json&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/master/url&#34;&gt;UMOE-Cap-453k.json&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Relevant vision and audio files: &lt;a href=&#34;https://raw.githubusercontent.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/master/#Training-Data&#34;&gt;Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run training scripts: &lt;a href=&#34;https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/raw/master/Uni_MoE/finetune_audio.sh&#34;&gt;&lt;code&gt;finetune_audio.sh&lt;/code&gt;&lt;/a&gt; or &lt;a href=&#34;https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/raw/master/Uni_MoE/finetune_speech.sh&#34;&gt;&lt;code&gt;finetune_speech.sh&lt;/code&gt;&lt;/a&gt; using &lt;code&gt;bash finetune_audio.sh&lt;/code&gt; &lt;code&gt;bash finetune_speech.sh&lt;/code&gt;, remember to modify the training set with your own preference.&lt;/li&gt; &#xA; &lt;li&gt;For multiple GPUs training, run training scripts: &lt;a href=&#34;https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/raw/master/Uni_MoE/finetune_speech_dp.sh&#34;&gt;&lt;code&gt;finetune_speech_dp.sh&lt;/code&gt;&lt;/a&gt; using &lt;code&gt;bash finetune_speech_dp.sh&lt;/code&gt;, remember to modify the training set with your own preference.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Evaluation:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prepare the evaluation set using the form as &lt;a href=&#34;https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/raw/master/Uni_MoE/data_sample/samples.json&#34;&gt;&lt;code&gt;samples.json&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run evaluation scripts: &lt;a href=&#34;https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/raw/master/Uni_MoE/eval_audio.sh&#34;&gt;&lt;code&gt;eval_audio.sh&lt;/code&gt;&lt;/a&gt; or &lt;a href=&#34;https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs/raw/master/Uni_MoE/eval_speech.sh&#34;&gt;&lt;code&gt;eval_speech.sh&lt;/code&gt;&lt;/a&gt; using &lt;code&gt;bash eval_audio.sh&lt;/code&gt; &lt;code&gt;bash eval_speech.sh&lt;/code&gt; or run the following commands to eval:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /path/to/Uni_MoE&#xA;conda activate unimoe&#xA;python Uni_MoE_audio/eval.py\&#xA; --data_path /path/to/clotho.json\&#xA; --data_type clothov1\&#xA; --output test.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /path/to/Uni_MoE&#xA;conda activate unimoe&#xA;python Uni_MoE_speech/eval.py\&#xA; --data_path /path/to/vqa_eval.json\&#xA; --data_type vqa\&#xA; --output test.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend using 80GB GPU RAM to run all experiments.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find Uni-MoE useful for your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;&#xA;@article{li2024uni,&#xA;  title={Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts},&#xA;  author={Li, Yunxin and Jiang, Shenyuan and Hu, Baotian and Wang, Longyue and Zhong, Wanqi and Luo, Wenhan and Ma, Lin and Zhang, Min},&#xA;  journal={arXiv preprint arXiv:2405.11273},&#xA;  year={2024}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>dream-num/univer</title>
    <updated>2024-05-27T01:28:17Z</updated>
    <id>tag:github.com,2024-05-27:/dream-num/univer</id>
    <link href="https://github.com/dream-num/univer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Univer is an open-source alternative to Google Sheets, Slides, and Docs&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;./docs/img/banner-light.png&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/banner-dark.png&#34; alt=&#34;Univer&#34; width=&#34;400&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/LICENSE.txt&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/dream-num/univer&#34; alt=&#34;GitHub License&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/dream-num/univer/actions/workflows/build.yml&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/dream-num/univer/build.yml&#34; alt=&#34;GitHub Workflow Status&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/dream-num/univer&#34;&gt; &lt;img src=&#34;https://codecov.io/gh/dream-num/univer/graph/badge.svg?token=aPfyW2pIMN&#34; alt=&#34;codecov&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.codefactor.io/repository/github/dream-num/univer/overview/dev&#34;&gt; &lt;img src=&#34;https://www.codefactor.io/repository/github/dream-num/univer/badge/dev&#34; alt=&#34;CodeFactor&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/z3NKNT6D2f&#34;&gt; &lt;img src=&#34;https://img.shields.io/discord/1136129819961217077?logo=discord&amp;amp;logoColor=FFFFFF&amp;amp;label=discord&amp;amp;color=5865F2&#34; alt=&#34;Discord&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; English | &lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/README-zh.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/README-ja.md&#34;&gt;Êó•Êú¨Ë™û&lt;/a&gt; &lt;/p&gt; &#xA;&lt;!-- An introduction photo here. --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üöß This project is still in heavy development. Please note that there are likely to be major API changes. Please submit issues and suggestions to us.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Univer is an open-source alternative to Google Sheets, Slides, and Docs.&lt;/p&gt; &#xA;&lt;p&gt;Highlights:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìà Univer is designed to support both &lt;strong&gt;spreadsheets&lt;/strong&gt; and &lt;strong&gt;documents&lt;/strong&gt;. &lt;strong&gt;Slides&lt;/strong&gt; will be supported as well in the future.&lt;/li&gt; &#xA; &lt;li&gt;‚öôÔ∏è Univer is easily &lt;strong&gt;embeddable&lt;/strong&gt;, allowing seamless integration into your applications.&lt;/li&gt; &#xA; &lt;li&gt;üéá Univer is &lt;strong&gt;powerful&lt;/strong&gt;, offering a wide range of features including &lt;strong&gt;formulas&lt;/strong&gt;, &lt;strong&gt;conditional formatting&lt;/strong&gt;, &lt;strong&gt;data validation&lt;/strong&gt;, &lt;strong&gt;filtering&lt;/strong&gt;, &lt;strong&gt;collaborative editing&lt;/strong&gt;, &lt;strong&gt;printing&lt;/strong&gt;, &lt;strong&gt;import &amp;amp; export&lt;/strong&gt; and more features on the horizon.&lt;/li&gt; &#xA; &lt;li&gt;üîå Univer is &lt;strong&gt;highly extensible&lt;/strong&gt;, thanks to its &lt;em&gt;plug-in architecture&lt;/em&gt; and &lt;em&gt;Facade API&lt;/em&gt; that makes it a delight for developers to implement their unique requirements on the top of Univer.&lt;/li&gt; &#xA; &lt;li&gt;üíÑ Univer is &lt;strong&gt;highly customizable&lt;/strong&gt;, allowing you to personalize its appearance using &lt;em&gt;themes&lt;/em&gt;. It also provides support for internationalization (i18n).&lt;/li&gt; &#xA; &lt;li&gt;‚ö° Univer in &lt;strong&gt;performant&lt;/strong&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;‚úèÔ∏è Univer boasts an efficient &lt;em&gt;rendering engine&lt;/em&gt; based on canvas, capable of rendering various document types flawlessly. The rendering engines supports advanced typesetting features such as &lt;em&gt;punctuation squeezing&lt;/em&gt;, &lt;em&gt;text and image layout&lt;/em&gt; and &lt;em&gt;scroll buffering&lt;/em&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;üßÆ Univer incorporates a lightning-fast &lt;em&gt;formula engine&lt;/em&gt; that can operate in Web Workers or even on the server side.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;üåå Univer is a &lt;strong&gt;highly integrated&lt;/strong&gt; system. Documents, spreadsheets and slides can interoperate with each others and even rendered on the same canvas, allowing information and data flow within Univer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&amp;nbsp;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìä Sheets&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìä Sheets Multi&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìä Sheets Uniscript&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/examples/sheets/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/examples-sheets.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/examples/sheets-multi/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/examples-sheets-multi.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/examples/sheets-uniscript/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/examples-sheets-uniscript.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìä Sheets Big Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìä Sheets Collaboration (Pro)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìä Sheets Collaboration Playground (Pro)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/examples/sheets-big-data/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/examples-sheets-big-data.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/pro/examples/sheets-collaboration/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/pro-examples-sheets-collaboration.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/pro/examples/sheets-collaboration-playground/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/pro-examples-sheets-collaboration-playground.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìä Sheets Import/Export (Pro)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìä Sheets Print (Pro)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìù Docs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/pro/examples/sheets-exchange/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/pro-examples-sheets-exchange.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/pro/examples/sheets-print/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/pro-examples-sheets-print.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/examples/docs/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/examples-docs.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìù Docs Multi&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìù Docs Uniscript&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìù Docs Big Data&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/examples/docs-multi/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/examples-docs-multi.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/examples/docs-uniscript/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/examples-docs-uniscript.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/examples/docs-big-data/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/examples-docs-big-data.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìù Docs Collaboration (Pro)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìù Docs Collaboration Playground (Pro)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìΩÔ∏è Slides&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/pro/examples/docs-collaboration/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/pro-examples-docs-collaboration.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/pro/examples/docs-collaboration-playground/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/pro-examples-docs-collaboration-playground.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/examples/slides/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/examples-slides.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üìä Zen Mode&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Univer Workspace (SaaS version)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&amp;nbsp;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://univer.ai/zh-CN/guides/sheet/tutorials/zen-editor/#%E6%BC%94%E7%A4%BA&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/zen-mode.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://youtu.be/kpV0MvQuFZA&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/docs/img/univer-workspace-drag-chart.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&amp;nbsp;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;We recommend to import Univer as a npm package. Please checkout the &lt;a href=&#34;https://univer.ai/guides/sheet/getting-started/quickstart&#34;&gt;Quick Start&lt;/a&gt; section on the documentation website. We also have an &lt;a href=&#34;https://univer.ai/playground/&#34;&gt;online playground&lt;/a&gt; which can help you preview Univer without setting up the development environment.&lt;/p&gt; &#xA;&lt;p&gt;Univer bases on a plugin architecture. You can install the following packages to enhance the functionality of Univer.&lt;/p&gt; &#xA;&lt;h3&gt;Packages&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Name&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/core&#34;&gt;core&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Implements plugin system and architecture of Univer. It also provides basic services and models of different types of documents.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/core&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/core&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/data-validation&#34;&gt;data-validation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Implements data validation features in Univer.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/data-validation&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/data-validation&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/design&#34;&gt;design&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Implements the design system on Univer. It provides CSS and a component kit based on React.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/design&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/design&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/docs&#34;&gt;docs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Implements basic logics of rich text editing features. It also facilitates text editing in other types of documents.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/docs&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/docs&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/docs-ui&#34;&gt;docs-ui&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Provides user interface of Univer Documents&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/docs-ui&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/docs-ui&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/engine-formula&#34;&gt;engine-formula&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;It implements a rendering engine based on Canvas and is extensible for&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/engine-formula&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/engine-formula&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/engine-numfmt&#34;&gt;engine-numfmt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;It implements a number format engine.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/engine-numfmt&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/engine-numfmt&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/engine-render&#34;&gt;engine-render&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;It implements a rendering engine based on canvas context2d.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/engine-render&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/engine-render&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/facade&#34;&gt;facade&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;It serves as an API layer to make it easier to use Univer&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/facade&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/facade&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/find-replace&#34;&gt;find-replace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;It implements find and replace features in Univer.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/find-replace&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/find-replace&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/network&#34;&gt;network&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;It implements network services based on WebSocket and HTTP.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/network&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/network&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/rpc&#34;&gt;rpc&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;It implements a RPC mechanism and methods to sync data between different replicas of Univer documents.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/rpc&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/rpc&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/sheets&#34;&gt;sheets&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Basic logics of spreadsheet features.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/sheets&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/sheets&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/sheets-conditional-formatting&#34;&gt;sheets-conditional-formatting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;It implements conditional formatting in Univer Spreadsheets.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/sheets-conditional-formatting&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/sheets-conditional-formatting&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/sheets-conditional-formatting-ui&#34;&gt;sheets-conditional-formatting-ui&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;It implements conditional formatting in Univer Spreadsheets.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/sheets-conditional-formatting-ui&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/sheets-conditional-formatting-ui&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/data-validation&#34;&gt;sheets-data-validation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;It implements data validation in Univer Spreadsheets.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/sheets-data-validation&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/sheets-data-validation&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/sheets-find-replace&#34;&gt;sheets-find-replace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;It implements find and replace features in Univer Spreadsheets.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/sheets-find-replace&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/sheets-find-replace&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/sheets-formula&#34;&gt;sheets-formula&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;It implements formula in spreadsheets.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/sheets-formula&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/sheets-formula&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/sheets-numfmt&#34;&gt;sheets-numfmt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;It implements number format in spreadsheets.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/sheets-numfmt&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/sheets-numfmt&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/sheets-zen-editor&#34;&gt;sheets-zen-editor&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;It implements Zen editing mode in spreadsheets.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/sheets-zen-editor&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/sheets-zen-editor&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/sheets-ui&#34;&gt;sheets-ui&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Provides user interface of Univer Spreadsheets&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/sheets-ui&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/sheets-ui&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/ui&#34;&gt;ui&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Implements basic user interactions with Univer and workbench layout based on React.&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/ui&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/ui&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/packages/uniscript&#34;&gt;uniscript&lt;/a&gt; (experimental)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Implements a DSL based on Typescript that empowers users to accomplish more sophisticated tasks&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://npmjs.org/package/@univerjs/uniscript&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@univerjs/uniscript&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate any kinds of contributing. You can submit &lt;a href=&#34;https://github.com/dream-num/univer/issues&#34;&gt;issues or feature requests&lt;/a&gt; to us. Please read our &lt;a href=&#34;https://raw.githubusercontent.com/dream-num/univer/dev/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; first.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to contribute code to Univer, please refer to the contributing guide as well. It would guide you through the process of setting up the development environment and submitting a pull request.&lt;/p&gt; &#xA;&lt;h2&gt;Sponsors&lt;/h2&gt; &#xA;&lt;p&gt;The growth and development of the Univer project rely on the support of its backers and sponsors. If you are interested in supporting our project, we kindly invite you to consider becoming a sponsor. You can sponsor us through &lt;a href=&#34;https://opencollective.com/univer&#34;&gt;Open Collective&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to our sponsors, just part of them are listed here because of the space limit, ranking is no particular order:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opencollective.com/univer/sponsor/0/website&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/univer/sponsor/0/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/univer/sponsor/1/website&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/univer/sponsor/1/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/univer/sponsor/2/website&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/univer/sponsor/2/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/univer/sponsor/3/website&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/univer/sponsor/3/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/univer/sponsor/4/website&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/univer/sponsor/4/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/univer/sponsor/5/website&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/univer/sponsor/5/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/univer/sponsor/6/website&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/univer/sponsor/6/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opencollective.com/univer/backer/0/website&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/univer/backer/0/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/univer/backer/1/website&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/univer/backer/1/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/univer/backer/2/website&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/univer/backer/2/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/univer/backer/3/website&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/univer/backer/3/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/univer/backer/4/website&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/univer/backer/4/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/univer/backer/5/website&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/univer/backer/5/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/univer/backer/6/website&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/univer/backer/6/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Stargazers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/dream-num/univer/stargazers&#34;&gt;&lt;img src=&#34;https://bytecrank.com/nastyox/reporoster/php/stargazersSVG.php?user=dream-num&amp;amp;repo=univer&#34; alt=&#34;Stargazers repo roster for @dream-num/univer&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://univer.ai/guides/sheet/introduction&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://univer.ai/playground/&#34;&gt;Online Playground&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://univer.ai&#34;&gt;Official Website&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Community&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/z3NKNT6D2f&#34;&gt;Discord community&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Univer is distributed under the terms of the Apache-2.0 license.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Copyright ¬© 2019-2024 Shanghai DreamNum Technology Co., Ltd. All rights reserved&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ashishps1/awesome-low-level-design</title>
    <updated>2024-05-27T01:28:17Z</updated>
    <id>tag:github.com,2024-05-27:/ashishps1/awesome-low-level-design</id>
    <link href="https://github.com/ashishps1/awesome-low-level-design" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn Low Level Design (LLD) and prepare for interviews using free resources.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/images/lld-repo-logo.png&#34; width=&#34;350&#34; height=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.linkedin.com/in/ashishps1/&#34;&gt;LinkedIn&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/@ashishps_1/videos&#34;&gt;YouTube&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/ashishps_1&#34;&gt;X&lt;/a&gt; | &lt;a href=&#34;https://newsletter.ashishps.com/&#34;&gt;Newsletter&lt;/a&gt; &lt;/p&gt; Learn Low Level Design (LLD) and prepare for interviews using free resources. &#xA;&lt;h2&gt;Fundamental Concepts&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://newsletter.ashishps.com/p/basic-oop-concepts-explained-with-code&#34;&gt;Basics OOP Concepts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/backticks-tildes/the-s-o-l-i-d-principles-in-pictures-b34ce2f1e898&#34;&gt;SOLID Principles with Pictures&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://newsletter.ashishps.com/p/solid-principles-explained-with-code&#34;&gt;SOLID Principles with Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://newsletter.ashishps.com/p/082450d8-0e7b-4447-a8dc-b7308e45f048&#34;&gt;DRY Principle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://newsletter.ashishps.com/p/8c3c7da7-885b-4a9c-a6e4-70ee02de4772&#34;&gt;YAGNI Principle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://newsletter.ashishps.com/p/21b57678-b351-4ed4-b390-3b6308af2f7d&#34;&gt;KISS Principle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/object-oriented-design&#34;&gt;Coursera - Object-Oriented Design&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Design Patterns&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/catalog&#34;&gt;Refactoring Guru - Catalog of Design Patterns&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=tv-_1er1mWI&amp;amp;pp=ygUPZGVzaWduIHBhdHRlcm5z&#34;&gt;Fireship - 10 Design Patterns in 10 Minutes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=v9ejT8FO-7I&amp;amp;list=PLrhzvIcii6GNjpARdnO4ueTUAVR9eMBpc&#34;&gt;Design Pattern Playlist&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/design-patterns&#34;&gt;Coursera - Design Patterns&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DovAmir/awesome-design-patterns&#34;&gt;Github - Awesome Design Patterns&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;UML&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLGLfVvz_LVvQ5G-LdJ8RLqe-ndo7QITYc&#34;&gt;Derek Banas - UML Playlist&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.visual-paradigm.com/guide/uml-unified-modeling-language/uml-class-diagram-tutorial/&#34;&gt;UML Class Diagram Tutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.visual-paradigm.com/what-are-the-six-types-of-relationships-in-uml-class-diagrams/&#34;&gt;Relationships in UML&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Low Level Design Interview Problems&lt;/h2&gt; &#xA;&lt;h3&gt;Easy&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/parking-lot.md&#34;&gt;Design Parking Lot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/vending-machine.md&#34;&gt;Design a Vending Machine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/stack-overflow.md&#34;&gt;Design Stack Overflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/logging-framework.md&#34;&gt;Design Logging Framework&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/coffee-vending-machine.md&#34;&gt;Design Coffee Vending Machine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/traffic-signal.md&#34;&gt;Design Traffic Signal Control System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/task-management-system.md&#34;&gt;Design a Task Management System&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Medium&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/pub-sub-system.md&#34;&gt;Design Pub Sub System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/tic-tac-toe.md&#34;&gt;Design Tic Tac Toe Game&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/car-rental-system.md&#34;&gt;Design Car Rental System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/atm.md&#34;&gt;Design an ATM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/hotel-management-system.md&#34;&gt;Design Hotel Management System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/linkedin.md&#34;&gt;Design LinkedIn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/social-networking-service.md&#34;&gt;Design a Social Network like Facebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/elevator-system.md&#34;&gt;Design an Elevator System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/library-management-system.md&#34;&gt;Design a Library Management System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/restaurant-management-system.md&#34;&gt;Design Restaurant Management System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/airline-management-system.md&#34;&gt;Design Airline Management System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/digital-wallet-system.md&#34;&gt;Design a Digital Wallet System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/online-auction-system.md&#34;&gt;Design an Online Auction System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/concert-ticketing-system.md&#34;&gt;Design a Concert Ticket Booking System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/lru-cache.md&#34;&gt;Design a Cache using LRU Eviction Policy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Hard&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/movie-ticket-booking-system.md&#34;&gt;Design Movie Ticket Booking System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/splitwise.md&#34;&gt;Design Splitwise&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/snake-and-ladder.md&#34;&gt;Design a Snake and Ladder game&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/online-shopping-service.md&#34;&gt;Design Online Shopping System like Amazon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/online-stock-brokerage-system.md&#34;&gt;Design Online Stock Brokerage System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/cricinfo.md&#34;&gt;Design CricInfo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/chess-game.md&#34;&gt;Design Chess Game&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/ride-sharing-service.md&#34;&gt;Design Ride-Sharing Service (like Uber)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/food-delivery-service.md&#34;&gt;Design Online Food Delivery Service (like Swiggy)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/music-streaming-service.md&#34;&gt;Design Music Streaming Service (like Spotify)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/course-registration-system.md&#34;&gt;Design University Course Registration System&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Books&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Head-First-Design-Patterns-Object-Oriented/dp/149207800X/&#34;&gt;Head First Design Patterns&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/B08X8ZXT15&#34;&gt;Clean Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Refactoring-Improving-Existing-Addison-Wesley-Signature/dp/0134757599/&#34;&gt;Refactoring: Improving the Design of Existing Code&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>