<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-09T01:29:18Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Alibaba-NLP/WebAgent</title>
    <updated>2025-07-09T01:29:18Z</updated>
    <id>tag:github.com,2025-07-09:/Alibaba-NLP/WebAgent</id>
    <link href="https://github.com/Alibaba-NLP/WebAgent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üåê WebAgent for Information Seeking bulit by Tongyi Lab: WebWalker &amp; WebDancer &amp; WebSailor https://arxiv.org/pdf/2507.02592&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;WebAgent for Information Seeking bulit by Tongyi Lab, Alibaba Group &lt;img src=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/tongyi.png&#34; width=&#34;30px&#34; style=&#34;display:inline;&#34;&gt;&lt;/h2&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://trendshift.io/repositories/14217&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/14217&#34; alt=&#34;Alibaba-NLP%2FWebAgent | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/Alibaba-NLP/WebSailor&#34; target=&#34;_blank&#34;&gt;WebSailor&lt;/a&gt; ÔΩú ü§ó &lt;a href=&#34;https://huggingface.co/Alibaba-NLP/WebDancer-32B&#34; target=&#34;_blank&#34;&gt;WebDancer-QwQ-32B&lt;/a&gt; | &lt;img src=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/tongyi.png&#34; width=&#34;14px&#34; style=&#34;display:inline;&#34;&gt; &lt;a href=&#34;https://modelscope.cn/models/iic/WebDancer-32B&#34; target=&#34;_blank&#34;&gt;ModelScope WebDancer-QwQ-32B&lt;/a&gt; | ü§ó &lt;a href=&#34;https://huggingface.co/datasets/callanwu/WebWalkerQA&#34; target=&#34;_blank&#34;&gt;WebWalkerQA&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/roadmap.png&#34; width=&#34;100%&#34; height=&#34;400%&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can check the paper of &lt;a href=&#34;https://arxiv.org/pdf/2505.22648&#34;&gt;WebDancer&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/2501.07572&#34;&gt;WebWalker&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/2507.02592&#34;&gt;WebSailor&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üí• üí• üí• Stay tuned for more updates! We are working on building native agentic model based on the Browser and more open-domain environments!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebSailor&#34;&gt;&lt;strong&gt;WebSailor&lt;/strong&gt;&lt;/a&gt; (Preprint 2025) - WebSailor: Navigating Super-human Reasoning for Web Agent&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebDancer&#34;&gt;&lt;strong&gt;WebDancer&lt;/strong&gt;&lt;/a&gt; (Preprint 2025) - WebDancer: Towards Autonomous Information Seeking Agency&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebWalker&#34;&gt;&lt;strong&gt;WebWalker&lt;/strong&gt;&lt;/a&gt; (ACL 2025) - WebWalker: Benchmarking LLMs in Web Traversal&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì∞ News and Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.07.03&lt;/code&gt; üî•üî•üî•We release &lt;strong&gt;WebSailor&lt;/strong&gt;, an agentic search model specialized in performing extremely complex information seeking tasks, achieving open-source SOTA on some of the most difficult browsing benchmarks. &lt;strong&gt;WebSailor&lt;/strong&gt; topped the HuggingFace &lt;a href=&#34;https://huggingface.co/papers/2507.02592&#34;&gt;daily papers&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.06.23&lt;/code&gt; üî•üî•üî•The model, interactive demo, and some of the data of &lt;strong&gt;WebDancer&lt;/strong&gt; have been open-sourced. You&#39;re welcome to try them out!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.05.29&lt;/code&gt; üî•üî•üî•We release &lt;strong&gt;WebDancer&lt;/strong&gt;, a native agentic search model towards autonomous information seeking agency and &lt;em&gt;Deep Research&lt;/em&gt;-like model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.05.15&lt;/code&gt; &lt;strong&gt;WebWalker&lt;/strong&gt; is accepted by ACL 2025 main conference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025.01.14&lt;/code&gt; We release &lt;strong&gt;WebWalker&lt;/strong&gt;, a benchmark for LLMs in web traversal and a multi-agent framework for information seeking.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üíé Results Showcase&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/result.png&#34; width=&#34;800%&#34; height=&#34;400%&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;‚õµÔ∏è Features for WebSailor&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A complete post-training methodology enabling models to engage in extended thinking and information seeking, ultimately allowing them to successfully complete extremely complex tasks previously considered unsolvable.&lt;/li&gt; &#xA; &lt;li&gt;Introduces &lt;strong&gt;SailorFog-QA&lt;/strong&gt;, a scalable QA benchmark with high uncertainty and difficulty, curated with a novel data synthesis method through graph sampling and information obfuscation. Example SailorFog-QA data samples can be found at: &lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebSailor/dataset/sailorfog-QA.jsonl&#34;&gt;&lt;code&gt;WebSailor/dataset/sailorfog-QA.jsonl&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Effective post-training pipeline consisting of (1) high-quality reconstruction of concise reasoning from expert trajectories for clean supervision, (2) a two-stage training process involving an RFT cold start stage, followed by &lt;strong&gt;Duplicating Sampling Policy Optimization (DUPO)&lt;/strong&gt;, an efficient agentic RL algorithm excelling in effectiveness and efficiency.&lt;/li&gt; &#xA; &lt;li&gt;WebSailor-72B significantly outperforms all open-source agents and frameworks while closing the performance gap with leading proprietary systems, achieving a score of &lt;strong&gt;12.0%&lt;/strong&gt; on BrowseComp-en, &lt;strong&gt;30.1%&lt;/strong&gt; on BrowseComp-zh, and &lt;strong&gt;55.4%&lt;/strong&gt; on GAIA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The checkpoint is coming soon.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üåê Features for WebDancer&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Native agentic search reasoning model using ReAct framework towards autonomous information seeking agency and &lt;em&gt;Deep Research&lt;/em&gt;-like model.&lt;/li&gt; &#xA; &lt;li&gt;We introduce a four-stage training paradigm comprising &lt;strong&gt;browsing data construction, trajectory sampling, supervised fine-tuning for effective cold start, and reinforcement learning for improved generalization&lt;/strong&gt;, enabling the agent to autonomously acquire autonomous search and reasoning skills.&lt;/li&gt; &#xA; &lt;li&gt;Our data-centric approach integrates trajectory-level supervision fine-tuning and reinforcement learning (DAPO) to develop a scalable pipeline for &lt;strong&gt;training agentic systems&lt;/strong&gt; via SFT or RL.&lt;/li&gt; &#xA; &lt;li&gt;WebDancer achieves a Pass@3 score of 64.1% on GAIA and 62.0% on WebWalkerQA.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;You need to enter the &lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebDancer&#34;&gt;&lt;code&gt;WebDancer&lt;/code&gt;&lt;/a&gt; folder for the following commands.&lt;/p&gt; &#xA;&lt;h3&gt;Step 0: Set Up the Environment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n webdancer python=3.12&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 1: Deploy the Model&lt;/h3&gt; &#xA;&lt;p&gt;Download the WebDancer model from &lt;a href=&#34;https://huggingface.co/Alibaba-NLP/WebDancer-32B&#34;&gt;ü§ó HuggingFace&lt;/a&gt; and deploy it using the provided scripts with &lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;sglang&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd scripts&#xA;bash deploy_model.sh WebDancer_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Replace &lt;code&gt;WebDancer_PATH&lt;/code&gt; with the actual path to the downloaded model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Step 2: Run the Demo&lt;/h3&gt; &#xA;&lt;p&gt;Edit the following keys in &lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebDancer/scripts/run_demo.sh&#34;&gt;&lt;code&gt;WebDancer/scripts/run_demo.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;GOOGLE_SEARCH_KEY&lt;/code&gt;, you can get it from &lt;a href=&#34;https://serpapi.com/&#34;&gt;serpapi&lt;/a&gt; or &lt;a href=&#34;https://serper.dev/&#34;&gt;serper&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;JINA_API_KEY&lt;/code&gt;, you can get it from &lt;a href=&#34;https://jina.ai/api-dashboard/&#34;&gt;jina&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DASHSCOPE_API_KEY&lt;/code&gt;, you can get it from &lt;a href=&#34;https://dashscope.aliyun.com/&#34;&gt;dashscope&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then, launch the demo with Gradio to interact with the WebDancer model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd scripts&#xA;bash run_demo.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üé• WebSailor Demos&lt;/h2&gt; &#xA;&lt;p&gt;We provide demos for BrowseComp-en, BrowseComp-zh and Daily Use. Our model can complete highly difficult and uncertain tasks requiring massive information acquisition and complex reasoning.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;BrowseComp-en&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/2dc0b03a-c241-4f70-bf11-92fda28020fa&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;BrowseComp-zh&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/f9aed746-ffc8-4b76-b135-715ec0eab544&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;Daily Use&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/1299c5a8-cee3-4a70-b68b-c5d227cf8055&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üé• WebDancer Demos&lt;/h2&gt; &#xA;&lt;p&gt;We provide demos for WebWalkerQA, GAIA and Daily Use. Our model can execute the long-horizon tasks with &lt;strong&gt;multiple steps&lt;/strong&gt; and &lt;strong&gt;complex reasoning&lt;/strong&gt;, such as web traversal, information seeking and question answering.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;WebWalkerQA&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/0bbaf55b-897e-4c57-967d-a6e8bbd2167e&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;GAIA&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/935c668e-6169-4712-9c04-ac80f0531872&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h3&gt;Daily Use&lt;/h3&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/d1d5b533-4009-478b-bd87-96b86389327d&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üìÉ License&lt;/h2&gt; &#xA;&lt;p&gt;The content of this project itself is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üö© Citation&lt;/h2&gt; &#xA;&lt;p&gt;If this work is helpful, please kindly cite as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bigquery&#34;&gt;@misc{li2025websailor,&#xA;      title={WebSailor: Navigating Super-human Reasoning for Web Agent},&#xA;      author={Kuan Li and Zhongwang Zhang and Huifeng Yin and Liwen Zhang and Litu Ou and Jialong Wu and Wenbiao Yin and Baixuan Li and Zhengwei Tao and Xinyu Wang and Weizhou Shen and Junkai Zhang and Dingchu Zhang and Xixi Wu and Yong Jiang and Ming Yan and Pengjun Xie and Fei Huang and Jingren Zhou},&#xA;      year={2025},&#xA;      eprint={2507.02592},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2507.02592},&#xA;}&#xA;@misc{wu2025webdancer,&#xA;      title={WebDancer: Towards Autonomous Information Seeking Agency},&#xA;      author={Jialong Wu and Baixuan Li and Runnan Fang and Wenbiao Yin and Liwen Zhang and Zhengwei Tao and Dingchu Zhang and Zekun Xi and Yong Jiang and Pengjun Xie and Fei Huang and Jingren Zhou},&#xA;      year={2025},&#xA;      eprint={2505.22648},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2505.22648},&#xA;}&#xA;@misc{wu2025webwalker,&#xA;      title={WebWalker: Benchmarking LLMs in Web Traversal},&#xA;      author={Jialong Wu and Wenbiao Yin and Yong Jiang and Zhenglin Wang and Zekun Xi and Runnan Fang and Deyu Zhou and Pengjun Xie and Fei Huang},&#xA;      year={2025},&#xA;      eprint={2501.07572},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2501.07572},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The repo is contributed by &lt;a href=&#34;https://callanwu.github.io/&#34;&gt;Jialong Wu&lt;/a&gt;. If you have any questions, please feel free to contact via &lt;a href=&#34;mailto:wujialongml@gmail.com&#34;&gt;wujialongml@gmail.com&lt;/a&gt; or create an issue.&lt;/p&gt; &#xA;&lt;h2&gt;üåü Misc&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.star-history.com/#Alibaba-NLP/WebAgent&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Alibaba-NLP/WebAgent&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üö© Talent Recruitment&lt;/h2&gt; &#xA;&lt;p&gt;üî•üî•üî• We are hiring! Research intern positions are open (based in Hangzhou„ÄÅBeijing„ÄÅShanghai)&lt;/p&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;Research Area&lt;/strong&gt;ÔºöWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG&lt;/p&gt; &#xA;&lt;p&gt;‚òéÔ∏è &lt;strong&gt;Contact&lt;/strong&gt;Ôºö&lt;a href=&#34;&#34;&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>HandsOnLLM/Hands-On-Large-Language-Models</title>
    <updated>2025-07-09T01:29:18Z</updated>
    <id>tag:github.com,2025-07-09:/HandsOnLLM/Hands-On-Large-Language-Models</id>
    <link href="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official code repo for the O&#39;Reilly Book - &#34;Hands-On Large Language Models&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Hands-On Large Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/jalammar/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Follow%20Jay-blue.svg?logo=linkedin&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/in/mgrootendorst/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Follow%20Maarten-blue.svg?logo=linkedin&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.deeplearning.ai/short-courses/how-transformer-llms-work/?utm_campaign=handsonllm-launch&amp;amp;utm_medium=partner&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/DeepLearning.AI%20Course-NEW!-&amp;amp;labelColor=black&amp;amp;color=red.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAuMDAwMzY1MjgxIC0wLjAwMDE0MDE0MiAzMy4yOSAzMy4xNSI+Cgk8cGF0aCBkPSJNMTYuNjQzIDMzLjE0NWMtMy4yOTIgMC02LjUxLS45NzItOS4yNDYtMi43OTNhMTYuNTg4IDE2LjU4OCAwIDAxLTYuMTMtNy40MzhBMTYuNTA3IDE2LjUwNyAwIDAxLjMyIDEzLjM0YTE2LjU1IDE2LjU1IDAgMDE0LjU1NS04LjQ4NUExNi42NjUgMTYuNjY1IDAgMDExMy4zOTYuMzE4YTE2LjcxIDE2LjcxIDAgMDE5LjYxNi45NDQgMTYuNjI4IDE2LjYyOCAwIDAxNy40NyA2LjEwMyAxNi41MjIgMTYuNTIyIDAgMDEyLjgwNCA5LjIwN2MwIDQuMzk2LTEuNzUzIDguNjEtNC44NzQgMTEuNzE5YTE2LjY4IDE2LjY4IDAgMDEtMTEuNzY5IDQuODU0em0uMTI1LTYuNjI4YzYuOTA2IDAgMTIuNTE3LTUuNjk4IDEyLjUxNy0xMi43MyAwLTcuMDMtNS42MS0xMi43MjUtMTIuNTE3LTEyLjcyNS02LjkwNiAwLTEyLjUxNyA1LjY5OC0xMi41MTcgMTIuNzI1IDAgNy4wMjcgNS42MTEgMTIuNzMgMTIuNTE3IDEyLjczem0tLjEyNS0yLjkxOGMtNi4yODkgMC0xMS4zODYtNC45MjUtMTEuMzg2LTExLjAwMkM1LjI1NyA2LjUyIDEwLjM2IDEuNTkgMTYuNjQzIDEuNTljNi4yODQgMCAxMS4zODYgNC45MyAxMS4zODYgMTEuMDA3cy01LjA5NyAxMS4wMDItMTEuMzg2IDExLjAwMnptLS4yNDItNC41MDhjNC43NyAwIDguNjMzLTMuNjc5IDguNjMzLTguMjE4IDAtNC41MzgtMy44ODUtOC4yMjEtOC42MzMtOC4yMjEtNC43NDcgMC04LjYzMiAzLjY3OS04LjYzMiA4LjIyMSAwIDQuNTQzIDMuODg1IDguMjE4IDguNjMyIDguMjE4eiIgZmlsbD0iI0ZENEE2MSIvPgo8L3N2Zz4=&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome! In this repository you will find the code for all examples throughout the book &lt;a href=&#34;https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961&#34;&gt;Hands-On Large Language Models&lt;/a&gt; written by &lt;a href=&#34;https://www.linkedin.com/in/jalammar/&#34;&gt;Jay Alammar&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/mgrootendorst/&#34;&gt;Maarten Grootendorst&lt;/a&gt; which we playfully dubbed: &lt;br&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;&#34;The Illustrated LLM Book&#34;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &#xA;&lt;p&gt;Through the visually educational nature of this book and with &lt;strong&gt;almost 300 custom made figures&lt;/strong&gt;, learn the practical tools and concepts you need to use Large Language Models today!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png&#34; width=&#34;50%&#34; height=&#34;50%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;The book is available on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961&#34;&gt;Amazon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.shroffpublishers.com/books/computer-science/large-language-models/9789355425522/&#34;&gt;Shroff Publishers (India)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/&#34;&gt;O&#39;Reilly&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Hands-Large-Language-Models-Alammar-ebook/dp/B0DGZ46G88/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;amp;qid=&amp;amp;sr=&#34;&gt;Kindle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.barnesandnoble.com/w/hands-on-large-language-models-jay-alammar/1145185960&#34;&gt;Barnes and Noble&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.goodreads.com/book/show/210408850-hands-on-large-language-models&#34;&gt;Goodreads&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;p&gt;We advise to run all examples through Google Colab for the easiest setup. Google Colab allows you to use a T4 GPU with 16GB of VRAM for free. All examples were mainly built and tested using Google Colab, so it should be the most stable platform. However, any other cloud provider should work.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Chapter&lt;/th&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chapter 1: Introduction to Language Models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter01/Chapter%201%20-%20Introduction%20to%20Language%20Models.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chapter 2: Tokens and Embeddings&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter02/Chapter%202%20-%20Tokens%20and%20Token%20Embeddings.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chapter 3: Looking Inside Transformer LLMs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter03/Chapter%203%20-%20Looking%20Inside%20LLMs.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chapter 4: Text Classification&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter04/Chapter%204%20-%20Text%20Classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chapter 5: Text Clustering and Topic Modeling&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter05/Chapter%205%20-%20Text%20Clustering%20and%20Topic%20Modeling.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chapter 6: Prompt Engineering&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter06/Chapter%206%20-%20Prompt%20Engineering.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chapter 7: Advanced Text Generation Techniques and Tools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter07/Chapter%207%20-%20Advanced%20Text%20Generation%20Techniques%20and%20Tools.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chapter 8: Semantic Search and Retrieval-Augmented Generation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter08/Chapter%208%20-%20Semantic%20Search.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chapter 9: Multimodal Large Language Models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter09/Chapter%209%20-%20Multimodal%20Large%20Language%20Models.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chapter 10: Creating Text Embedding Models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter10/Chapter%2010%20-%20Creating%20Text%20Embedding%20Models.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chapter 11: Fine-tuning Representation Models for Classification&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter11/Chapter%2011%20-%20Fine-Tuning%20BERT.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chapter 12: Fine-tuning Generation Models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter12/Chapter%2012%20-%20Fine-tuning%20Generation%20Models.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] You can check the &lt;a href=&#34;https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/.setup/&#34;&gt;setup&lt;/a&gt; folder for a quick-start guide to install all packages locally and you can check the &lt;a href=&#34;https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/.setup/conda/&#34;&gt;conda&lt;/a&gt; folder for a complete guide on how to setup your environment, including conda and PyTorch installation. Note that the depending on your OS, Python version, and dependencies your results might be slightly differ. However, they should this be similar to the examples in the book.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Reviews&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;&lt;em&gt;Jay and Maarten have continued their tradition of providing beautifully illustrated and insightful descriptions of complex topics in their new book. Bolstered with working code, timelines, and references to key papers, their book is a valuable resource for anyone looking to understand the main techniques behind how Large Language Models are built.&lt;/em&gt;&#34;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Andrew Ng&lt;/strong&gt; - founder of &lt;a href=&#34;https://www.deeplearning.ai/&#34;&gt;DeepLearning.AI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;&lt;em&gt;This is an exceptional guide to the world of language models and their practical applications in industry. Its highly-visual coverage of generative, representational, and retrieval applications of language models empowers readers to quickly understand, use, and refine LLMs. Highly recommended!&lt;/em&gt;&#34;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Nils Reimers&lt;/strong&gt; - Director of Machine Learning at Cohere | creator of &lt;a href=&#34;https://github.com/UKPLab/sentence-transformers&#34;&gt;sentence-transformers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;&lt;em&gt;I can‚Äôt think of another book that is more important to read right now. On every single page, I learned something that is critical to success in this era of language models.&lt;/em&gt;&#34;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Josh Starmer&lt;/strong&gt; - &lt;a href=&#34;https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw&#34;&gt;StatQuest&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;&lt;em&gt;If you‚Äôre looking to get up to speed in everything regarding LLMs, look no further! In this wonderful book, Jay and Maarten will take you from zero to expert in the history and latest advances in large language models. With very intuitive explanations, great real-life examples, clear illustrations, and comprehensive code labs, this book lifts the curtain on the complexities of transformer models, tokenizers, semantic search, RAG, and many other cutting-edge technologies. A must read for anyone interested in the latest AI technology!&lt;/em&gt;&#34;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Luis Serrano, PhD&lt;/strong&gt; - Founder and CEO of &lt;a href=&#34;https://www.youtube.com/@SerranoAcademy&#34;&gt;Serrano Academy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;&lt;em&gt;Hands-On Large Language Models brings clarity and practical examples to cut through the hype of AI. It provides a wealth of great diagrams and visual aids to supplement the clear explanations. The worked examples and code make concrete what other books leave abstract. The book starts with simple introductory beginnings, and steadily builds in scope. By the final chapters, you will be fine-tuning and building your own large language models with confidence.&lt;/em&gt;&#34;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Leland McInnes&lt;/strong&gt; - Researcher at the Tutte Institute for Mathematics and Computing | creator of &lt;a href=&#34;https://github.com/lmcinnes/umap&#34;&gt;UMAP&lt;/a&gt; and &lt;a href=&#34;https://github.com/scikit-learn-contrib/hdbscan&#34;&gt;HDBSCAN&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/bonus/&#34;&gt;Bonus content!&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We attempted to put as much information into the book without it being overwhelming. However, even with a 400-page book there is still much to discover!&lt;/p&gt; &#xA;&lt;p&gt;We continue to create more guides that compliment the book and go more in-depth into new and &lt;a href=&#34;https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/(bonus/)&#34;&gt;exciting topics&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state&#34;&gt;A Visual Guide to Mamba&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization&#34;&gt;A Visual Guide to Quantization&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-stable-diffusion/&#34;&gt;The Illustrated Stable Diffusion&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/mamba.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/quant.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/diffusion.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts&#34;&gt;A Visual Guide to Mixture of Experts&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms&#34;&gt;A Visual Guide to Reasoning LLMs&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1&#34;&gt;The Illustrated DeepSeek-R1&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/moe.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/reasoning.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/deepseek.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please consider citing the book if you consider it useful for your research:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@book{hands-on-llms-book,&#xA;  author       = {Jay Alammar and Maarten Grootendorst},&#xA;  title        = {Hands-On Large Language Models},&#xA;  publisher    = {O&#39;Reilly},&#xA;  year         = {2024},&#xA;  isbn         = {978-1098150969},&#xA;  url          = {https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/},&#xA;  github       = {https://github.com/HandsOnLLM/Hands-On-Large-Language-Models}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>jbhuang0604/awesome-computer-vision</title>
    <updated>2025-07-09T01:29:18Z</updated>
    <id>tag:github.com,2025-07-09:/jbhuang0604/awesome-computer-vision</id>
    <link href="https://github.com/jbhuang0604/awesome-computer-vision" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A curated list of awesome computer vision resources&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Computer Vision: &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;A curated list of awesome computer vision resources, inspired by &lt;a href=&#34;https://github.com/ziadoz/awesome-php&#34;&gt;awesome-php&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a list people in computer vision listed with their academic genealogy, please visit &lt;a href=&#34;https://github.com/jbhuang0604/awesome-computer-vision/raw/master/people.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please feel free to send me &lt;a href=&#34;https://github.com/jbhuang0604/awesome-computer-vision/pulls&#34;&gt;pull requests&lt;/a&gt; or email (&lt;a href=&#34;mailto:jbhuang@vt.edu&#34;&gt;jbhuang@vt.edu&lt;/a&gt;) to add links.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jbhuang0604/awesome-computer-vision/master/#awesome-lists&#34;&gt;Awesome Lists&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jbhuang0604/awesome-computer-vision/master/#books&#34;&gt;Books&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jbhuang0604/awesome-computer-vision/master/#courses&#34;&gt;Courses&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jbhuang0604/awesome-computer-vision/master/#papers&#34;&gt;Papers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jbhuang0604/awesome-computer-vision/master/#software&#34;&gt;Software&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jbhuang0604/awesome-computer-vision/master/#datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jbhuang0604/awesome-computer-vision/master/#Pre-trained-Computer-Vision-Models&#34;&gt;Pre-trained Computer Vision Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jbhuang0604/awesome-computer-vision/master/#tutorials-and-talks&#34;&gt;Tutorials and Talks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jbhuang0604/awesome-computer-vision/master/#resources-for-students&#34;&gt;Resources for students&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jbhuang0604/awesome-computer-vision/master/#blogs&#34;&gt;Blogs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jbhuang0604/awesome-computer-vision/master/#links&#34;&gt;Links&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jbhuang0604/awesome-computer-vision/master/#songs&#34;&gt;Songs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Awesome Lists&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/josephmisiti/awesome-machine-learning&#34;&gt;Awesome Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kjw0612/awesome-deep-vision&#34;&gt;Awesome Deep Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhaoxin94/awesome-domain-adaptation&#34;&gt;Awesome Domain Adaptation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/amusi/awesome-object-detection&#34;&gt;Awesome Object Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/timzhang642/3D-Machine-Learning&#34;&gt;Awesome 3D Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jinwchoi/awesome-action-recognition&#34;&gt;Awesome Action Recognition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bertjiazheng/awesome-scene-understanding&#34;&gt;Awesome Scene Understanding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yenchenlin/awesome-adversarial-machine-learning&#34;&gt;Awesome Adversarial Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chbrian/awesome-adversarial-examples-dl&#34;&gt;Awesome Adversarial Deep Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/polarisZhao/awesome-face&#34;&gt;Awesome Face&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ChanChiChoi/awesome-Face_Recognition&#34;&gt;Awesome Face Recognition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wangzheallen/awesome-human-pose-estimation&#34;&gt;Awesome Human Pose Estimation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fepegar/awesome-medical-imaging&#34;&gt;Awesome medical imaging&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/heyalexej/awesome-images&#34;&gt;Awesome Images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ericjang/awesome-graphics&#34;&gt;Awesome Graphics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF&#34;&gt;Awesome Neural Radiance Fields&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vsitzmann/awesome-implicit-representations&#34;&gt;Awesome Implicit Neural Representations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/weihaox/awesome-neural-rendering&#34;&gt;Awesome Neural Rendering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awesomedata/awesome-public-datasets&#34;&gt;Awesome Public Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jsbroks/awesome-dataset-tools&#34;&gt;Awesome Dataset Tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sunglok/awesome-robotics-datasets&#34;&gt;Awesome Robotics Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fritzlabs/Awesome-Mobile-Machine-Learning&#34;&gt;Awesome Mobile Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wangyongjie-ntu/Awesome-explainable-AI&#34;&gt;Awesome Explainable AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datamllab/awesome-fairness-in-ai&#34;&gt;Awesome Fairness in AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jphall663/awesome-machine-learning-interpretability&#34;&gt;Awesome Machine Learning Interpretability&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EthicalML/awesome-production-machine-learning&#34;&gt;Awesome Production Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/danieljf24/awesome-video-text-retrieval&#34;&gt;Awesome Video Text Retrieval&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/weihaox/awesome-image-translation&#34;&gt;Awesome Image-to-Image Translation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/1900zyh/Awesome-Image-Inpainting&#34;&gt;Awesome Image Inpainting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vinthony/awesome-deep-hdr&#34;&gt;Awesome Deep HDR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/matthewvowels1/Awesome-Video-Generation&#34;&gt;Awesome Video Generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nashory/gans-awesome-applications&#34;&gt;Awesome GAN applications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhoubolei/awesome-generative-modeling&#34;&gt;Awesome Generative Modeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/weiaicunzai/awesome-image-classification&#34;&gt;Awesome Image Classification&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ChristosChristofidis/awesome-deep-learning&#34;&gt;Awesome Deep Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/XindiWu/Awesome-Machine-Learning-in-Biomedical-Healthcare-Imaging&#34;&gt;Awesome Machine Learning in Biomedical(Healthcare) Imaging&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/abhineet123/Deep-Learning-for-Tracking-and-Detection&#34;&gt;Awesome Deep Learning for Tracking and Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wangzheallen/awesome-human-pose-estimation&#34;&gt;Awesome Human Pose Estimation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HuaizhengZhang/Awsome-Deep-Learning-for-Video-Analysis&#34;&gt;Awesome Deep Learning for Video Analysis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers&#34;&gt;Awesome Vision + Language&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kiloreux/awesome-robotics&#34;&gt;Awesome Robotics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dk-liang/Awesome-Visual-Transformer&#34;&gt;Awesome Visual Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ChanganVR/awesome-embodied-vision&#34;&gt;Awesome Embodied Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hoya012/awesome-anomaly-detection&#34;&gt;Awesome Anomaly Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thaoshibe/awesome-makeup-transfer&#34;&gt;Awesome Makeup Transfer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise&#34;&gt;Awesome Learning with Label Noise&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/subeeshvasu/Awesome-Deblurring&#34;&gt;Awesome Deblurring&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/subeeshvasu/Awsome_Deep_Geometry_Learning&#34;&gt;Awsome Deep Geometry Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/subeeshvasu/Awesome-Image-Distortion-Correction&#34;&gt;Awesome Image Distortion Correction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/subeeshvasu/Awesome-Neuron-Segmentation-in-EM-Images&#34;&gt;Awesome Neuron Segmentation in EM Images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/subeeshvasu/Awsome_Delineation&#34;&gt;Awsome Delineation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/subeeshvasu/Awesome-ImageHarmonization&#34;&gt;Awesome ImageHarmonization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/subeeshvasu/Awsome-GAN-Training&#34;&gt;Awsome GAN Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tstanislawek/awesome-document-understanding&#34;&gt;Awesome Document Understanding&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Books&lt;/h2&gt; &#xA;&lt;h4&gt;Computer Vision&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.computervisionmodels.com/&#34;&gt;Computer Vision: Models, Learning, and Inference&lt;/a&gt; - Simon J. D. Prince 2012&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://szeliski.org/Book/&#34;&gt;Computer Vision: Theory and Application&lt;/a&gt; - Rick Szeliski 2010&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.amazon.com/Computer-Vision-Modern-Approach-2nd/dp/013608592X/ref=dp_ob_title_bk&#34;&gt;Computer Vision: A Modern Approach (2nd edition)&lt;/a&gt; - David Forsyth and Jean Ponce 2011&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.robots.ox.ac.uk/~vgg/hzbook/&#34;&gt;Multiple View Geometry in Computer Vision&lt;/a&gt; - Richard Hartley and Andrew Zisserman 2004&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.amazon.com/Computer-Vision-Linda-G-Shapiro/dp/0130307963&#34;&gt;Computer Vision&lt;/a&gt; - Linda G. Shapiro 2001&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.amazon.com/Vision-Science-Phenomenology-Stephen-Palmer/dp/0262161834/&#34;&gt;Vision Science: Photons to Phenomenology&lt;/a&gt; - Stephen E. Palmer 1999&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.morganclaypool.com/doi/abs/10.2200/S00332ED1V01Y201103AIM011&#34;&gt;Visual Object Recognition synthesis lecture&lt;/a&gt; - Kristen Grauman and Bastian Leibe 2011&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cvfxbook.com/&#34;&gt;Computer Vision for Visual Effects&lt;/a&gt; - Richard J. Radke, 2012&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.amazon.com/High-Dynamic-Range-Imaging-Second/dp/012374914X&#34;&gt;High dynamic range imaging: acquisition, display, and image-based lighting&lt;/a&gt; - Reinhard, E., Heidrich, W., Debevec, P., Pattanaik, S., Ward, G., Myszkowski, K 2010&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://people.csail.mit.edu/jsolomon/share/book/numerical_book.pdf&#34;&gt;Numerical Algorithms: Methods for Computer Vision, Machine Learning, and Graphics&lt;/a&gt; - Justin Solomon 2015&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Processing-Analysis-Activate-Learning-Engineering/dp/1285179528&#34;&gt;Image Processing and Analysis&lt;/a&gt; - Stan Birchfield 2018&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs231a/&#34;&gt;Computer Vision, From 3D Reconstruction to Recognition&lt;/a&gt; - Silvio Savarese 2018&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;OpenCV Programming&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.amazon.com/Learning-OpenCV-Computer-Vision-Library/dp/0596516134&#34;&gt;Learning OpenCV: Computer Vision with the OpenCV Library&lt;/a&gt; - Gary Bradski and Adrian Kaehler&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pyimagesearch.com/practical-python-opencv/&#34;&gt;Practical Python and OpenCV&lt;/a&gt; - Adrian Rosebrock&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.amazon.com/OpenCV-Essentials-Oscar-Deniz-Suarez/dp/1783984244/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1424594237&amp;amp;sr=1-1&amp;amp;keywords=opencv+essentials#&#34;&gt;OpenCV Essentials&lt;/a&gt; - Oscar Deniz Suarez, M¬™ del Milagro Fernandez Carrobles, Noelia Vallez Enano, Gloria Bueno Garcia, Ismael Serrano Gracia&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Machine Learning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://research.microsoft.com/en-us/um/people/cmbishop/prml/index.htm&#34;&gt;Pattern Recognition and Machine Learning&lt;/a&gt; - Christopher M. Bishop 2007&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.engineering.upm.ro/master-ie/sacpi/mat_did/info068/docum/Neural%20Networks%20for%20Pattern%20Recognition.pdf&#34;&gt;Neural Networks for Pattern Recognition&lt;/a&gt; - Christopher M. Bishop 1995&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://pgm.stanford.edu/&#34;&gt;Probabilistic Graphical Models: Principles and Techniques&lt;/a&gt; - Daphne Koller and Nir Friedman 2009&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.amazon.com/Pattern-Classification-2nd-Richard-Duda/dp/0471056693&#34;&gt;Pattern Classification&lt;/a&gt; - Peter E. Hart, David G. Stork, and Richard O. Duda 2000&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.amazon.com/Machine-Learning-Tom-M-Mitchell/dp/0070428077/&#34;&gt;Machine Learning&lt;/a&gt; - Tom M. Mitchell 1997&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.gaussianprocess.org/gpml/&#34;&gt;Gaussian processes for machine learning&lt;/a&gt; - Carl Edward Rasmussen and Christopher K. I. Williams 2005&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://work.caltech.edu/telecourse.html&#34;&gt;Learning From Data&lt;/a&gt;- Yaser S. Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin 2012&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://neuralnetworksanddeeplearning.com/&#34;&gt;Neural Networks and Deep Learning&lt;/a&gt; - Michael Nielsen 2014&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.ucl.ac.uk/staff/d.barber/brml/&#34;&gt;Bayesian Reasoning and Machine Learning&lt;/a&gt; - David Barber, Cambridge University Press, 2012&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Fundamentals&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.amazon.com/Linear-Algebra-Its-Applications-4th/dp/0030105676/ref=sr_1_4?ie=UTF8&amp;amp;qid=1421433773&amp;amp;sr=8-4&amp;amp;keywords=Linear+Algebra+and+Its+Applications&#34;&gt;Linear Algebra and Its Applications&lt;/a&gt; - Gilbert Strang 1995&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Courses&lt;/h2&gt; &#xA;&lt;h4&gt;Computer Vision&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://inside.mines.edu/~whoff/courses/EENG512/&#34;&gt;EENG 512 / CSCI 512 - Computer Vision&lt;/a&gt; - William Hoff (Colorado School of Mines)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/ucbcs29443/&#34;&gt;Visual Object and Activity Recognition&lt;/a&gt; - Alexei A. Efros and Trevor Darrell (UC Berkeley)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://courses.cs.washington.edu/courses/cse455/12wi/&#34;&gt;Computer Vision&lt;/a&gt; - Steve Seitz (University of Washington)&lt;/li&gt; &#xA; &lt;li&gt;Visual Recognition &lt;a href=&#34;http://vision.cs.utexas.edu/381V-spring2016/&#34;&gt;Spring 2016&lt;/a&gt;, &lt;a href=&#34;http://vision.cs.utexas.edu/381V-fall2016/&#34;&gt;Fall 2016&lt;/a&gt; - Kristen Grauman (UT Austin)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.tamaraberg.com/teaching/Spring_15/&#34;&gt;Language and Vision&lt;/a&gt; - Tamara Berg (UNC Chapel Hill)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://vision.stanford.edu/teaching/cs231n/&#34;&gt;Convolutional Neural Networks for Visual Recognition&lt;/a&gt; - Fei-Fei Li and Andrej Karpathy (Stanford University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cs.nyu.edu/~fergus/teaching/vision/index.html&#34;&gt;Computer Vision&lt;/a&gt; - Rob Fergus (NYU)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://courses.engr.illinois.edu/cs543/sp2015/&#34;&gt;Computer Vision&lt;/a&gt; - Derek Hoiem (UIUC)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://vision.stanford.edu/teaching/cs131_fall1415/index.html&#34;&gt;Computer Vision: Foundations and Applications&lt;/a&gt; - Kalanit Grill-Spector and Fei-Fei Li (Stanford University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://vision.stanford.edu/teaching/cs431_spring1314/&#34;&gt;High-Level Vision: Behaviors, Neurons and Computational Models&lt;/a&gt; - Fei-Fei Li (Stanford University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://6.869.csail.mit.edu/fa15/&#34;&gt;Advances in Computer Vision&lt;/a&gt; - Antonio Torralba and Bill Freeman (MIT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.vision.rwth-aachen.de/course/11/&#34;&gt;Computer Vision&lt;/a&gt; - Bastian Leibe (RWTH Aachen University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.vision.rwth-aachen.de/course/9/&#34;&gt;Computer Vision 2&lt;/a&gt; - Bastian Leibe (RWTH Aachen University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://klewel.com/conferences/epfl-computer-vision/&#34;&gt;Computer Vision&lt;/a&gt; Pascal Fua (EPFL):&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cvlab-dresden.de/courses/computer-vision-1/&#34;&gt;Computer Vision 1&lt;/a&gt; Carsten Rother (TU Dresden):&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cvlab-dresden.de/courses/CV2/&#34;&gt;Computer Vision 2&lt;/a&gt; Carsten Rother (TU Dresden):&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/RDkwklFGMfo?list=PLTBdjV_4f-EJn6udZ34tht9EVIW7lbeo4&#34;&gt;Multiple View Geometry&lt;/a&gt; Daniel Cremers (TU Munich):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Computational Photography&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://inst.eecs.berkeley.edu/~cs194-26/fa14/&#34;&gt;Image Manipulation and Computational Photography&lt;/a&gt; - Alexei A. Efros (UC Berkeley)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://graphics.cs.cmu.edu/courses/15-463/2012_fall/463.html&#34;&gt;Computational Photography&lt;/a&gt; - Alexei A. Efros (CMU)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://courses.engr.illinois.edu/cs498dh3/&#34;&gt;Computational Photography&lt;/a&gt; - Derek Hoiem (UIUC)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cs.brown.edu/courses/csci1290/&#34;&gt;Computational Photography&lt;/a&gt; - James Hays (Brown University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://stellar.mit.edu/S/course/6/sp12/6.815/&#34;&gt;Digital &amp;amp; Computational Photography&lt;/a&gt; - Fredo Durand (MIT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ocw.mit.edu/courses/media-arts-and-sciences/mas-531-computational-camera-and-photography-fall-2009/&#34;&gt;Computational Camera and Photography&lt;/a&gt; - Ramesh Raskar (MIT Media Lab)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/computational-photography--ud955&#34;&gt;Computational Photography&lt;/a&gt; - Irfan Essa (Georgia Tech)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://graphics.stanford.edu/courses/&#34;&gt;Courses in Graphics&lt;/a&gt; - Stanford University&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cs.nyu.edu/~fergus/teaching/comp_photo/index.html&#34;&gt;Computational Photography&lt;/a&gt; - Rob Fergus (NYU)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.toronto.edu/~kyros/courses/320/&#34;&gt;Introduction to Visual Computing&lt;/a&gt; - Kyros Kutulakos (University of Toronto)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.toronto.edu/~kyros/courses/2530/&#34;&gt;Computational Photography&lt;/a&gt; - Kyros Kutulakos (University of Toronto)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ecse.rpi.edu/~rjradke/cvfxcourse.html&#34;&gt;Computer Vision for Visual Effects&lt;/a&gt; - Rich Radke (Rensselaer Polytechnic Institute)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ecse.rpi.edu/~rjradke/improccourse.html&#34;&gt;Introduction to Image Processing&lt;/a&gt; - Rich Radke (Rensselaer Polytechnic Institute)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Machine Learning and Statistical Learning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34;&gt;Machine Learning&lt;/a&gt; - Andrew Ng (Stanford University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://work.caltech.edu/telecourse.html&#34;&gt;Learning from Data&lt;/a&gt; - Yaser S. Abu-Mostafa (Caltech)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://class.stanford.edu/courses/HumanitiesandScience/StatLearning/Winter2015/about&#34;&gt;Statistical Learning&lt;/a&gt; - Trevor Hastie and Rob Tibshirani (Stanford University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.mit.edu/~9.520/fall14/&#34;&gt;Statistical Learning Theory and Applications&lt;/a&gt; - Tomaso Poggio, Lorenzo Rosasco, Carlo Ciliberto, Charlie Frogner, Georgios Evangelopoulos, Ben Deen (MIT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.stat.rice.edu/~gallen/stat640.html&#34;&gt;Statistical Learning&lt;/a&gt; - Genevera Allen (Rice University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.berkeley.edu/~jordan/courses/294-fall09/&#34;&gt;Practical Machine Learning&lt;/a&gt; - Michael Jordan (UC Berkeley)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/course_information_theory_pattern_recognition/&#34;&gt;Course on Information Theory, Pattern Recognition, and Neural Networks&lt;/a&gt; - David MacKay (University of Cambridge)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/~lmackey/stats306b/&#34;&gt;Methods for Applied Statistics: Unsupervised Learning&lt;/a&gt; - Lester Mackey (Stanford)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.robots.ox.ac.uk/~az/lectures/ml/index.html&#34;&gt;Machine Learning&lt;/a&gt; - Andrew Zisserman (University of Oxford)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning--ud120&#34;&gt;Intro to Machine Learning&lt;/a&gt; - Sebastian Thrun (Stanford University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/machine-learning--ud262&#34;&gt;Machine Learning&lt;/a&gt; - Charles Isbell, Michael Littman (Georgia Tech)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cs231n.github.io/&#34;&gt;(Convolutional) Neural Networks for Visual Recognition&lt;/a&gt; - Fei-Fei Li, Andrej Karphaty, Justin Johnson (Stanford University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/QZmZFeZxEKI?list=PLTBdjV_4f-EIiongKlS9OKrBEp8QR47Wl&#34;&gt;Machine Learning for Computer Vision&lt;/a&gt; - Rudolph Triebel (TU Munich)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Optimization&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://stanford.edu/class/ee364a/&#34;&gt;Convex Optimization I&lt;/a&gt; - Stephen Boyd (Stanford University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://stanford.edu/class/ee364b/&#34;&gt;Convex Optimization II&lt;/a&gt; - Stephen Boyd (Stanford University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://class.stanford.edu/courses/Engineering/CVX101/Winter2014/about&#34;&gt;Convex Optimization&lt;/a&gt; - Stephen Boyd (Stanford University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://optimization.mit.edu/classes.php&#34;&gt;Optimization at MIT&lt;/a&gt; - (MIT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.stat.cmu.edu/~ryantibs/convexopt/&#34;&gt;Convex Optimization&lt;/a&gt; - Ryan Tibshirani (CMU)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;h4&gt;Conference papers on the web&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cvpapers.com/&#34;&gt;CVPapers&lt;/a&gt; - Computer vision papers on the web&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://kesen.realtimerendering.com/&#34;&gt;SIGGRAPH Paper on the web&lt;/a&gt; - Graphics papers on the web&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/&#34;&gt;NIPS Proceedings&lt;/a&gt; - NIPS papers on the web&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cv-foundation.org/openaccess/menu.py&#34;&gt;Computer Vision Foundation open access&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://iris.usc.edu/Vision-Notes/bibliography/contents.html&#34;&gt;Annotated Computer Vision Bibliography&lt;/a&gt; - Keith Price (USC)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://iris.usc.edu/Information/Iris-Conferences.html&#34;&gt;Calendar of Computer Image Analysis, Computer Vision Conferences&lt;/a&gt; - (USC)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Survey Papers&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://surveys.visionbib.com/index.html&#34;&gt;Visionbib Survey Paper List&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.nowpublishers.com/CGV&#34;&gt;Foundations and Trends¬Æ in Computer Graphics and Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://link.springer.com/book/10.1007/978-0-387-31439-6&#34;&gt;Computer Vision: A Reference Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pre-trained Computer Vision Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/shubham-shahh/Open-Source-Models&#34;&gt;List of Computer Vision models&lt;/a&gt; These models are trained on custom objects&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tutorials and talks&lt;/h2&gt; &#xA;&lt;h4&gt;Computer Vision&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.computervisiontalks.com/&#34;&gt;Computer Vision Talks&lt;/a&gt; - Lectures, keynotes, panel discussions on computer vision&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Mqg6eorYRIQ&#34;&gt;The Three R&#39;s of Computer Vision&lt;/a&gt; - Jitendra Malik (UC Berkeley) 2013&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/epsrcws08_blake_amv/&#34;&gt;Applications to Machine Vision&lt;/a&gt; - Andrew Blake (Microsoft Research) 2008&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/kdd08_malik_fis/?q=image&#34;&gt;The Future of Image Search&lt;/a&gt; - Jitendra Malik (UC Berkeley) 2008&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=M17oGxh3Ny8&#34;&gt;Should I do a PhD in Computer Vision?&lt;/a&gt; - Fatih Porikli (Australian National University)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.ipam.ucla.edu/programs/summer-schools/graduate-summer-school-computer-vision/?tab=schedule&#34;&gt;Graduate Summer School 2013: Computer Vision&lt;/a&gt; - IPAM, 2013&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Recent Conference Talks&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.pamitc.org/cvpr15/&#34;&gt;CVPR 2015&lt;/a&gt; - Jun 2015&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/eccv2014_zurich/&#34;&gt;ECCV 2014&lt;/a&gt; - Sep 2014&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://techtalks.tv/cvpr-2014-oral-talks/&#34;&gt;CVPR 2014&lt;/a&gt; - Jun 2014&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://techtalks.tv/iccv2013/&#34;&gt;ICCV 2013&lt;/a&gt; - Dec 2013&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://techtalks.tv/icml/2013/&#34;&gt;ICML 2013&lt;/a&gt; - Jul 2013&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://techtalks.tv/cvpr2013/&#34;&gt;CVPR 2013&lt;/a&gt; - Jun 2013&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/eccv2012_firenze/&#34;&gt;ECCV 2012&lt;/a&gt; - Oct 2012&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://techtalks.tv/icml/2012/orals/&#34;&gt;ICML 2012&lt;/a&gt; - Jun 2012&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://techtalks.tv/cvpr2012webcast/&#34;&gt;CVPR 2012&lt;/a&gt; - Jun 2012&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;3D Computer Vision&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kyIzMr917Rc&#34;&gt;3D Computer Vision: Past, Present, and Future&lt;/a&gt; - Steve Seitz (University of Washington) 2011&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=04Kgg3QEXFI&#34;&gt;Reconstructing the World from Photos on the Internet&lt;/a&gt; - Steve Seitz (University of Washington) 2013&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Internet Vision&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.technologyreview.com/video/426265/meet-2011-tr35-winner-noah-snavely/&#34;&gt;The Distributed Camera&lt;/a&gt; - Noah Snavely (Cornell University) 2011&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=UHkCa9-Z1Ps&#34;&gt;Planet-Scale Visual Understanding&lt;/a&gt; - Noah Snavely (Cornell University) 2014&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=6MWEfpKUfRc&#34;&gt;A Trillion Photos&lt;/a&gt; - Steve Seitz (University of Washington) 2013&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Computational Photography&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=j90_0Ndk7XM&#34;&gt;Reflections on Image-Based Modeling and Rendering&lt;/a&gt; - Richard Szeliski (Microsoft Research) 2013&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ZvPaHZZVPRk&#34;&gt;Photographing Events over Time&lt;/a&gt; - William T. Freeman (MIT) 2011&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/nipsworkshops2011_weiss_deconvolution/&#34;&gt;Old and New algorithm for Blind Deconvolution&lt;/a&gt; - Yair Weiss (The Hebrew University of Jerusalem) 2011&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/nipsworkshops2010_milanfar_tmi/&#34;&gt;A Tour of Modern &#34;Image Processing&#34;&lt;/a&gt; - Peyman Milanfar (UC Santa Cruz/Google) 2010&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/mlss07_blake_tiivp/&#34;&gt;Topics in image and video processing&lt;/a&gt; Andrew Blake (Microsoft Research) 2007&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=HJVNI0mkmqk&#34;&gt;Computational Photography&lt;/a&gt; - William T. Freeman (MIT) 2012&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=_BWnIQY_X98&#34;&gt;Revealing the Invisible&lt;/a&gt; - Fr√©do Durand (MIT) 2012&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=rE-hVtytT-I&#34;&gt;Overview of Computer Vision and Visual Effects&lt;/a&gt; - Rich Radke (Rensselaer Polytechnic Institute) 2014&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Learning and Vision&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/colt2011_freeman_help/?q=computer%20vision&#34;&gt;Where machine vision needs help from machine learning&lt;/a&gt; - William T. Freeman (MIT) 2011&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/mlss08au_lucey_linv/&#34;&gt;Learning in Computer Vision&lt;/a&gt; - Simon Lucey (CMU) 2008&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/nips09_weiss_lil/?q=computer%20vision&#34;&gt;Learning and Inference in Low-Level Vision&lt;/a&gt; - Yair Weiss (The Hebrew University of Jerusalem) 2009&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Object Recognition&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://research.microsoft.com/apps/video/dl.aspx?id=231358&#34;&gt;Object Recognition&lt;/a&gt; - Larry Zitnick (Microsoft Research)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/mlas06_li_gmvoo/?q=Fei-Fei%20Li&#34;&gt;Generative Models for Visual Objects and Object Recognition via Bayesian Inference&lt;/a&gt; - Fei-Fei Li (Stanford University)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Graphical Models&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/uai2012_felzenszwalb_computer_vision/?q=computer%20vision&#34;&gt;Graphical Models for Computer Vision&lt;/a&gt; - Pedro Felzenszwalb (Brown University) 2012&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/mlss09uk_ghahramani_gm/&#34;&gt;Graphical Models&lt;/a&gt; - Zoubin Ghahramani (University of Cambridge) 2009&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/mlss06tw_roweis_mlpgm/&#34;&gt;Machine Learning, Probability and Graphical Models&lt;/a&gt; - Sam Roweis (NYU) 2006&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/mlss09us_weiss_gma/?q=Graphical%20Models&#34;&gt;Graphical Models and Applications&lt;/a&gt; - Yair Weiss (The Hebrew University of Jerusalem) 2009&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Machine Learning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nikola-rt.ee.washington.edu/people/bulyko/papers/em.pdf&#34;&gt;A Gentle Tutorial of the EM Algorithm&lt;/a&gt; - Jeff A. Bilmes (UC Berkeley) 1998&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/mlss09uk_bishop_ibi/&#34;&gt;Introduction To Bayesian Inference&lt;/a&gt; - Christopher Bishop (Microsoft Research) 2009&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/mlss06tw_lin_svm/&#34;&gt;Support Vector Machines&lt;/a&gt; - Chih-Jen Lin (National Taiwan University) 2006&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/mlss09uk_jordan_bfway/&#34;&gt;Bayesian or Frequentist, Which Are You? &lt;/a&gt; - Michael I. Jordan (UC Berkeley)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Optimization&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/nips2010_wright_oaml/&#34;&gt;Optimization Algorithms in Machine Learning&lt;/a&gt; - Stephen J. Wright (University of Wisconsin-Madison)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/mlss07_vandenberghe_copt/?q=convex%20optimization&#34;&gt;Convex Optimization&lt;/a&gt; - Lieven Vandenberghe (University of California, Los Angeles)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oZqoWozVDVg&#34;&gt;Continuous Optimization in Computer Vision&lt;/a&gt; - Andrew Fitzgibbon (Microsoft Research)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/sahd2014_bach_stochastic_gradient/&#34;&gt;Beyond stochastic gradient descent for large-scale machine learning&lt;/a&gt; - Francis Bach (INRIA)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLTBdjV_4f-EJ7A2iIH5L5ztqqrWYjP2RI&#34;&gt;Variational Methods for Computer Vision&lt;/a&gt; - Daniel Cremers (Technische Universit√§t M√ºnchen) (&lt;a href=&#34;https://www.youtube.com/watch?v=GgcbVPNd3SI&#34;&gt;lecture 18 missing from playlist&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Deep Learning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/jul09_hinton_deeplearn/&#34;&gt;A tutorial on Deep Learning&lt;/a&gt; - Geoffrey E. Hinton (University of Toronto)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/kdd2014_salakhutdinov_deep_learning/?q=Hidden%20Markov%20model#&#34;&gt;Deep Learning&lt;/a&gt; - Ruslan Salakhutdinov (University of Toronto)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/kdd2014_bengio_deep_learning/&#34;&gt;Scaling up Deep Learning&lt;/a&gt; - Yoshua Bengio (University of Montreal)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/machine_krizhevsky_imagenet_classification/?q=deep%20learning&#34;&gt;ImageNet Classification with Deep Convolutional Neural Networks&lt;/a&gt; - Alex Krizhevsky (University of Toronto)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/sahd2014_lecun_deep_learning/&#34;&gt;The Unreasonable Effectivness Of Deep Learning&lt;/a&gt; Yann LeCun (NYU/Facebook Research) 2014&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qgx57X0fBdA&#34;&gt;Deep Learning for Computer Vision&lt;/a&gt; - Rob Fergus (NYU/Facebook Research)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://videolectures.net/sahd2014_mallat_dimensional_learning/&#34;&gt;High-dimensional learning with deep network contractions&lt;/a&gt; - St√©phane Mallat (Ecole Normale Superieure)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.ipam.ucla.edu/programs/summer-schools/graduate-summer-school-deep-learning-feature-learning/?tab=schedule&#34;&gt;Graduate Summer School 2012: Deep Learning, Feature Learning&lt;/a&gt; - IPAM, 2012&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.fields.utoronto.ca/programs/scientific/14-15/bigdata/machine/&#34;&gt;Workshop on Big Data and Statistical Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC3ywjSv5OsDiDAnOP8C1NiQ&#34;&gt;Machine Learning Summer School&lt;/a&gt; - Reykjavik, Iceland 2014 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=JuimBuvEWBg&#34;&gt;Deep Learning Session 1&lt;/a&gt; - Yoshua Bengio (Universtiy of Montreal)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Fl-W7_z3w3o&#34;&gt;Deep Learning Session 2&lt;/a&gt; - Yoshua Bengio (University of Montreal)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=_cohR7LAgWA&#34;&gt;Deep Learning Session 3&lt;/a&gt; - Yoshua Bengio (University of Montreal)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Software&lt;/h2&gt; &#xA;&lt;h4&gt;Annotation tools&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://commacoloring.herokuapp.com/&#34;&gt;Comma Coloring&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://annotorious.github.io/&#34;&gt;Annotorious&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://labelme.csail.mit.edu/Release3.0/&#34;&gt;LabelME&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sanko-shoko/gtmaker&#34;&gt;gtmaker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;External Resource Links&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/jbhuang0604/resources/vision&#34;&gt;Computer Vision Resources&lt;/a&gt; - Jia-Bin Huang (UIUC)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cvpapers.com/rr.html&#34;&gt;Computer Vision Algorithm Implementations&lt;/a&gt; - CVPapers&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.csee.wvu.edu/~xinl/reproducible_research.html&#34;&gt;Source Code Collection for Reproducible Research&lt;/a&gt; - Xin Li (West Virginia University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/v-source.html&#34;&gt;CMU Computer Vision Page&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;General Purpose Computer Vision Library&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://opencv.org/&#34;&gt;Open CV&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://kyamagu.github.io/mexopencv/&#34;&gt;mexopencv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://simplecv.org/&#34;&gt;SimpleCV&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jesolem/PCV&#34;&gt;Open source Python module for computer vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liuliu/ccv&#34;&gt;ccv: A Modern Computer Vision Library&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.vlfeat.org/&#34;&gt;VLFeat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.mathworks.com/products/computer-vision/&#34;&gt;Matlab Computer Vision System Toolbox&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://vision.ucsd.edu/~pdollar/toolbox/doc/index.html&#34;&gt;Piotr&#39;s Computer Vision Matlab Toolbox&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://pointclouds.org/&#34;&gt;PCL: Point Cloud Library&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gitorious.org/imageutilities&#34;&gt;ImageUtilities&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Multiple-view Computer Vision&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.robots.ox.ac.uk/~vgg/hzbook/code/&#34;&gt;MATLAB Functions for Multiple View Geometry&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://staffhome.ecm.uwa.edu.au/~00011811/Research/MatlabFns/index.html&#34;&gt;Peter Kovesi&#39;s Matlab Functions for Computer Vision and Image Analysis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://laurentkneip.github.io/opengv/&#34;&gt;OpenGV &lt;/a&gt; - geometric computer vision algorithms&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cmp.felk.cvut.cz/mini/&#34;&gt;MinimalSolvers&lt;/a&gt; - Minimal problems solver&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.gcc.tu-darmstadt.de/home/proj/mve/&#34;&gt;Multi-View Environment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ccwu.me/vsfm/&#34;&gt;Visual SFM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.cornell.edu/~snavely/bundler/&#34;&gt;Bundler SFM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://imagine.enpc.fr/~moulonp/openMVG/&#34;&gt;openMVG: open Multiple View Geometry&lt;/a&gt; - Multiple View Geometry; Structure from Motion library &amp;amp; softwares&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.di.ens.fr/pmvs/&#34;&gt;Patch-based Multi-view Stereo V2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.di.ens.fr/cmvs/&#34;&gt;Clustering Views for Multi-view Stereo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.gris.informatik.tu-darmstadt.de/projects/floating-scale-surface-recon/&#34;&gt;Floating Scale Surface Reconstruction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.gcc.tu-darmstadt.de/home/proj/texrecon/&#34;&gt;Large-Scale Texturing of 3D Reconstructions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openMVG/awesome_3DReconstruction_list&#34;&gt;Awesome 3D reconstruction list&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Feature Detection and Extraction&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.vlfeat.org/&#34;&gt;VLFeat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.ubc.ca/~lowe/keypoints/&#34;&gt;SIFT&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;David G. Lowe, &#34;Distinctive image features from scale-invariant keypoints,&#34; International Journal of Computer Vision, 60, 2 (2004), pp. 91-110.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.robots.ox.ac.uk/~vedaldi/code/siftpp.html&#34;&gt;SIFT++&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.asl.ethz.ch/people/lestefan/personal/BRISK&#34;&gt;BRISK&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Stefan Leutenegger, Margarita Chli and Roland Siegwart, &#34;BRISK: Binary Robust Invariant Scalable Keypoints&#34;, ICCV 2011&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.vision.ee.ethz.ch/~surf/&#34;&gt;SURF&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, &#34;SURF: Speeded Up Robust Features&#34;, Computer Vision and Image Understanding (CVIU), Vol. 110, No. 3, pp. 346--359, 2008&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.ivpe.com/freak.htm&#34;&gt;FREAK&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A. Alahi, R. Ortiz, and P. Vandergheynst, &#34;FREAK: Fast Retina Keypoint&#34;, CVPR 2012&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.robesafe.com/personal/pablo.alcantarilla/kaze.html&#34;&gt;AKAZE&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Pablo F. Alcantarilla, Adrien Bartoli and Andrew J. Davison, &#34;KAZE Features&#34;, ECCV 2012&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nourani/LBP&#34;&gt;Local Binary Patterns&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;High Dynamic Range Imaging&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/banterle/HDR_Toolbox&#34;&gt;HDR_Toolbox&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Semantic Segmentation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.it-caesar.com/list-of-contemporary-semantic-segmentation-datasets/&#34;&gt;List of Semantic Segmentation algorithms&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Low-level Vision&lt;/h4&gt; &#xA;&lt;h6&gt;Stereo Vision&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://vision.middlebury.edu/stereo/&#34;&gt;Middlebury Stereo Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stero&#34;&gt;The KITTI Vision Benchmark Suite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cvlibs.net/software/libelas/&#34;&gt;LIBELAS: Library for Efficient Large-scale Stereo Matching&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.6d-vision.com/ground-truth-stixel-dataset&#34;&gt;Ground Truth Stixel Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Optical Flow&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://vision.middlebury.edu/flow/&#34;&gt;Middlebury Optical Flow Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://sintel.is.tue.mpg.de/&#34;&gt;MPI-Sintel Optical Flow Dataset and Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow&#34;&gt;The KITTI Vision Benchmark Suite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://hci.iwr.uni-heidelberg.de/Benchmarks/document/Challenging_Data_for_Stereo_and_Optical_Flow/&#34;&gt;HCI Challenge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/celiu/OpticalFlow/&#34;&gt;Coarse2Fine Optical Flow&lt;/a&gt; - Ce Liu (MIT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cs.brown.edu/~dqsun/code/cvpr10_flow_code.zip&#34;&gt;Secrets of Optical Flow Estimation and Their Principles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/celiu/OpticalFlow/&#34;&gt;C++/MatLab Optical Flow by C. Liu (based on Brox et al. and Bruhn et al.)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.ctim.es/research_works/parallel_robust_optical_flow/&#34;&gt;Parallel Robust Optical Flow by S√°nchez P√©rez et al.&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Image Denoising&lt;/h6&gt; &#xA;&lt;p&gt;BM3D, KSVD,&lt;/p&gt; &#xA;&lt;h6&gt;Super-resolution&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.robots.ox.ac.uk/~vgg/software/SR/&#34;&gt;Multi-frame image super-resolution&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Pickup, L. C. Machine Learning in Multi-frame Image Super-resolution, PhD thesis 2008&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/billf/project%20pages/sresCode/Markov%20Random%20Fields%20for%20Super-Resolution.html&#34;&gt;Markov Random Fields for Super-Resolution&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;W. T Freeman and C. Liu. Markov Random Fields for Super-resolution and Texture Synthesis. In A. Blake, P. Kohli, and C. Rother, eds., Advances in Markov Random Fields for Vision and Image Processing, Chapter 10. MIT Press, 2011&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://people.mpi-inf.mpg.de/~kkim/supres/supres.htm&#34;&gt;Sparse regression and natural image prior&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;K. I. Kim and Y. Kwon, &#34;Single-image super-resolution using sparse regression and natural image prior&#34;, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 32, no. 6, pp. 1127-1133, 2010.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.technion.ac.il/~elad/Various/SingleImageSR_TIP14_Box.zip&#34;&gt;Single-Image Super Resolution via a Statistical Model&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;T. Peleg and M. Elad, A Statistical Prediction Model Based on Sparse Representations for Single Image Super-Resolution, IEEE Transactions on Image Processing, Vol. 23, No. 6, Pages 2569-2582, June 2014&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.technion.ac.il/~elad/Various/Single_Image_SR.zip&#34;&gt;Sparse Coding for Super-Resolution&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;R. Zeyde, M. Elad, and M. Protter On Single Image Scale-Up using Sparse-Representations, Curves &amp;amp; Surfaces, Avignon-France, June 24-30, 2010 (appears also in Lecture-Notes-on-Computer-Science - LNCS).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.ifp.illinois.edu/~jyang29/ScSR.htm&#34;&gt;Patch-wise Sparse Recovery&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Jianchao Yang, John Wright, Thomas Huang, and Yi Ma. Image super-resolution via sparse representation. IEEE Transactions on Image Processing (TIP), vol. 19, issue 11, 2010.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.jdl.ac.cn/user/hchang/doc/code.rar&#34;&gt;Neighbor embedding&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;H. Chang, D.Y. Yeung, Y. Xiong. Super-resolution through neighbor embedding. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), vol.1, pp.275-282, Washington, DC, USA, 27 June - 2 July 2004.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/yuzhushome/single-image-super-resolution-using-deformable-patches&#34;&gt;Deformable Patches&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yu Zhu, Yanning Zhang and Alan Yuille, Single Image Super-resolution using Deformable Patches, CVPR 2014&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html&#34;&gt;SRCNN&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Learning a Deep Convolutional Network for Image Super-Resolution, in ECCV 2014&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.vision.ee.ethz.ch/~timofter/ACCV2014_ID820_SUPPLEMENTARY/index.html&#34;&gt;A+: Adjusted Anchored Neighborhood Regression&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted Anchored Neighborhood Regression for Fast Super-Resolution, ACCV 2014&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/jbhuang0604/publications/struct_sr&#34;&gt;Transformed Self-Exemplars&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja, Single Image Super-Resolution using Transformed Self-Exemplars, IEEE Conference on Computer Vision and Pattern Recognition, 2015&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Image Deblurring&lt;/h6&gt; &#xA;&lt;p&gt;Non-blind deconvolution&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://homes.cs.washington.edu/~shanqi/work/spvdeconv/&#34;&gt;Spatially variant non-blind deconvolution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cg.postech.ac.kr/research/deconv_outliers/&#34;&gt;Handling Outliers in Non-blind Image Deconvolution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cs.nyu.edu/~dilip/research/fast-deconvolution/&#34;&gt;Hyper-Laplacian Priors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/danielzoran/epllcode.zip&#34;&gt;From Learning Models of Natural Image Patches to Whole Image Restoration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://lxu.me/projects/dcnn/&#34;&gt;Deep Convolutional Neural Network for Image Deconvolution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://webdav.is.mpg.de/pixel/neural_deconvolution/&#34;&gt;Neural Deconvolution&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Blind deconvolution&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.nyu.edu/~fergus/research/deblur.html&#34;&gt;Removing Camera Shake From A Single Photograph&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cse.cuhk.edu.hk/leojia/projects/motion_deblurring/&#34;&gt;High-quality motion deblurring from a single image&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cse.cuhk.edu.hk/leojia/projects/robust_deblur/&#34;&gt;Two-Phase Kernel Estimation for Robust Motion Deblurring&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/taegsang/Documents/RadonDeblurringCode.zip&#34;&gt;Blur kernel estimation using the radon transform&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cg.postech.ac.kr/research/fast_motion_deblurring/&#34;&gt;Fast motion deblurring&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cs.nyu.edu//~dilip/research/blind-deconvolution/&#34;&gt;Blind Deconvolution Using a Normalized Sparsity Measure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.huji.ac.il/~raananf/projects/deblur/&#34;&gt;Blur-kernel estimation from spectral irregularities&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.wisdom.weizmann.ac.il/~levina/papers/LevinEtalCVPR2011Code.zip&#34;&gt;Efficient marginal likelihood optimization in blind deconvolution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cse.cuhk.edu.hk/leojia/projects/l0deblur/&#34;&gt;Unnatural L0 Sparse Representation for Natural Image Deblurring&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cs.brown.edu/~lbsun/deblur2013/deblur2013iccp.html&#34;&gt;Edge-based Blur Kernel Estimation Using Patch Priors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.wisdom.weizmann.ac.il/~vision/BlindDeblur.html&#34;&gt;Blind Deblurring Using Internal Patch Recurrence&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Non-uniform Deblurring&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.di.ens.fr/willow/research/deblurring/&#34;&gt;Non-uniform Deblurring for Shaken Images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://grail.cs.washington.edu/projects/mdf_deblurring/&#34;&gt;Single Image Deblurring Using Motion Density Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://research.microsoft.com/en-us/um/redmond/groups/ivm/imudeblurring/&#34;&gt;Image Deblurring using Inertial Measurement Sensors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://webdav.is.mpg.de/pixel/fast_removal_of_camera_shake/&#34;&gt;Fast Removal of Non-uniform Camera Shake&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Image Completion&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://registry.gimp.org/node/27986&#34;&gt;GIMP Resynthesizer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://lafarren.com/image-completer/&#34;&gt;Priority BP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.ece.ucsb.edu/~psen/melding&#34;&gt;ImageMelding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/jbhuang0604/publications/struct_completion&#34;&gt;PlanarStructureCompletion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Image Retargeting&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/mrub/retargetme/&#34;&gt;RetargetMe&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Alpha Matting&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.alphamatting.com/&#34;&gt;Alpha Matting Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/alevin/matting.tar.gz&#34;&gt;Closed-form image matting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.vision.huji.ac.il/SpectralMatting/&#34;&gt;Spectral Matting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.mathworks.com/matlabcentral/fileexchange/31412-learning-based-digital-matting&#34;&gt;Learning-based Matting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.alphamatting.com/ImprovingMattingComprehensiveSamplingSets_CVPR2013.zip&#34;&gt;Improving Image Matting using Comprehensive Sampling Sets&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Image Pyramid&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cns.nyu.edu/~eero/steerpyr/&#34;&gt;The Steerable Pyramid&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.curvelet.org/&#34;&gt;CurveLab&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Edge-preserving image processing&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/sparis/bf/&#34;&gt;Fast Bilateral Filter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.cityu.edu.hk/~qiyang/publications/code/qx.cvpr09.ctbf.zip&#34;&gt;O(1) Bilateral Filter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.cityu.edu.hk/~qiyang/publications/eccv-12/&#34;&gt;Recursive Bilateral Filtering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cse.cuhk.edu.hk/leojia/projects/rollguidance/&#34;&gt;Rolling Guidance Filter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cse.cuhk.edu.hk/leojia/projects/texturesep/index.html&#34;&gt;Relative Total Variation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cse.cuhk.edu.hk/leojia/projects/L0smoothing/index.html&#34;&gt;L0 Gradient Optimization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.inf.ufrgs.br/~eslgastal/DomainTransform/&#34;&gt;Domain Transform&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://inf.ufrgs.br/~eslgastal/AdaptiveManifolds/&#34;&gt;Adaptive Manifold&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://research.microsoft.com/en-us/um/people/kahe/eccv10/&#34;&gt;Guided image filtering&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Intrinsic Images&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.tuebingen.mpg.de/mkiefel/projects/intrinsic/&#34;&gt;Recovering Intrinsic Images with a global Sparsity Prior on Reflectance&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://giga.cps.unizar.es/~elenag/projects/EGSR2012_intrinsic/&#34;&gt;Intrinsic Images by Clustering&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Contour Detection and Image Segmentation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://coewww.rutgers.edu/riul/research/code/EDISON/&#34;&gt;Mean Shift Segmentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cs.brown.edu/~pff/segment/&#34;&gt;Graph-based Segmentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cis.upenn.edu/~jshi/software/&#34;&gt;Normalized Cut&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://grabcut.weebly.com/background--algorithm.html&#34;&gt;Grab Cut&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html&#34;&gt;Contour Detection and Image Segmentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://research.microsoft.com/en-us/downloads/389109f6-b4e8-404c-84bf-239f7cbf4e3d/&#34;&gt;Structured Edge Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://web.mit.edu/phillipi/pmi-boundaries/&#34;&gt;Pointwise Mutual Information&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ivrl.epfl.ch/research/superpixels&#34;&gt;SLIC Super-pixel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.vlfeat.org/overview/quickshift.html&#34;&gt;QuickShift&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.toronto.edu/~babalex/research.html&#34;&gt;TurboPixels&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mingyuliu.net/&#34;&gt;Entropy Rate Superpixel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.vsi.cs.uni-frankfurt.de/research/current-projects/research/superpixel-segmentation/&#34;&gt;Contour Relaxed Superpixels&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.mvdblive.org/seeds/&#34;&gt;SEEDS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/davidstutz/seeds-revised&#34;&gt;SEEDS Revised&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/&#34;&gt;Multiscale Combinatorial Grouping&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pdollar/edges&#34;&gt;Fast Edge Detection Using Structured Forests&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Interactive Image Segmentation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cns.bu.edu/~lgrady/software.html&#34;&gt;Random Walker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.tc.umn.edu/~baixx015/&#34;&gt;Geodesic Segmentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://research.microsoft.com/apps/pubs/default.aspx?id=69040&#34;&gt;Lazy Snapping&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://powerwatershed.sourceforge.net/&#34;&gt;Power Watershed&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.adobe.com/technology/people/san-jose/brian-price.html&#34;&gt;Geodesic Graph Cut&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~olivierd/&#34;&gt;Segmentation by Transduction&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Video Segmentation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/image-and-video-segmentation/video-segmentation-with-superpixels/&#34;&gt;Video Segmentation with Superpixels&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cc.gatech.edu/cpl/projects/videosegmentation/&#34;&gt;Efficient hierarchical graph-based video segmentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://lmb.informatik.uni-freiburg.de/Publications/2011/OB11/&#34;&gt;Object segmentation in video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cse.buffalo.edu/~jcorso/r/supervoxels/&#34;&gt;Streaming hierarchical video segmentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Camera calibration&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.vision.caltech.edu/bouguetj/calib_doc/&#34;&gt;Camera Calibration Toolbox for Matlab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://docs.opencv.org/trunk/doc/tutorials/calib3d/camera_calibration/camera_calibration.html#&#34;&gt;Camera calibration With OpenCV&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/prclibo/toolbox&#34;&gt;Multiple Camera Calibration Toolbox&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Simultaneous localization and mapping&lt;/h4&gt; &#xA;&lt;h6&gt;SLAM community:&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.openslam.org/&#34;&gt;openSLAM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_odometry.php&#34;&gt;Kitti Odometry: benchmark for outdoor visual odometry (codes may be available)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Tracking/Odometry:&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cvlibs.net/software/libviso/&#34;&gt;LIBVISO2: C++ Library for Visual Odometry 2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.robots.ox.ac.uk/~gk/PTAM/&#34;&gt;PTAM: Parallel tracking and mapping&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/GerhardR/kfusion&#34;&gt;KFusion: Implementation of KinectFusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Nerei/kinfu_remake&#34;&gt;kinfu_remake: Lightweight, reworked and optimized version of Kinfu.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://las-vegas.uni-osnabrueck.de/related-projects/lvr-kinfu/&#34;&gt;LVR-KinFu: kinfu_remake based Large Scale KinectFusion with online reconstruction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.robots.ox.ac.uk/~victor/infinitam/&#34;&gt;InfiniTAM: Implementation of multi-platform large-scale depth tracking and fusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nachtmar/VoxelHashing&#34;&gt;VoxelHashing: Large-scale KinectFusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://apt.cs.manchester.ac.uk/projects/PAMELA/tools/SLAMBench/&#34;&gt;SLAMBench: Multiple-implementation of KinectFusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/uzh-rpg/rpg_svo&#34;&gt;SVO: Semi-direct visual odometry&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tum-vision/dvo_slam&#34;&gt;DVO: dense visual odometry&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://code.google.com/p/fovis/&#34;&gt;FOVIS: RGB-D visual odometry&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Graph Optimization:&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://collab.cc.gatech.edu/borg/gtsam?destination=node%2F299&#34;&gt;GTSAM: General smoothing and mapping library for Robotics and SFM&lt;/a&gt; -- Georgia Institute of Technology&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/RainerKuemmerle/g2o&#34;&gt;G2O: General framework for graph optomization&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Loop Closure:&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.robots.ox.ac.uk/~mjc/Software.htm&#34;&gt;FabMap: appearance-based loop closure system&lt;/a&gt; - also available in &lt;a href=&#34;http://docs.opencv.org/2.4/modules/contrib/doc/openfabmap.html&#34;&gt;OpenCV2.4.11&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://webdiis.unizar.es/~dorian/index.php?p=32&#34;&gt;DBoW2: binary bag-of-words loop detection system&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Localization &amp;amp; Mapping:&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://code.google.com/p/ratslam/&#34;&gt;RatSLAM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tum-vision/lsd_slam&#34;&gt;LSD-SLAM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/raulmur/ORB_SLAM&#34;&gt;ORB-SLAM&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Single-view Spatial Understanding&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://web.engr.illinois.edu/~dhoiem/projects/software.html&#34;&gt;Geometric Context&lt;/a&gt; - Derek Hoiem (CMU)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://web.engr.illinois.edu/~dhoiem/software/counter.php?Down=varsha_spatialLayout.zip&#34;&gt;Recovering Spatial Layout&lt;/a&gt; - Varsha Hedau (UIUC)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~./dclee/code/index.html&#34;&gt;Geometric Reasoning&lt;/a&gt; - David C. Lee (CMU)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/arron2003/rgbd2full3d&#34;&gt;RGBD2Full3D&lt;/a&gt; - Ruiqi Guo (UIUC)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Object Detection&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://pascal.inrialpes.fr/soft/olt/&#34;&gt;INRIA Object Detection and Localization Toolkit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.berkeley.edu/~rbg/latent/&#34;&gt;Discriminatively trained deformable part models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rbgirshick/voc-dpm&#34;&gt;VOC-DPM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.ics.uci.edu/~dramanan/software/sparse/&#34;&gt;Histograms of Sparse Codes for Object Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rbgirshick/rcnn&#34;&gt;R-CNN: Regions with Convolutional Neural Network Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ShaoqingRen/SPP_net&#34;&gt;SPP-Net&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mmcheng.net/bing/comment-page-9/&#34;&gt;BING: Objectness Estimation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pdollar/edges&#34;&gt;Edge Boxes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Russell91/ReInspect&#34;&gt;ReInspect&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Nearest Neighbor Search&lt;/h4&gt; &#xA;&lt;h6&gt;General purpose nearest neighbor search&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.umd.edu/~mount/ANN/&#34;&gt;ANN: A Library for Approximate Nearest Neighbor Searching&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.ubc.ca/research/flann/&#34;&gt;FLANN - Fast Library for Approximate Nearest Neighbors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://vincentfpgarcia.github.io/kNN-CUDA/&#34;&gt;Fast k nearest neighbor search using GPU&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Nearest Neighbor Field Estimation&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://gfx.cs.princeton.edu/gfx/pubs/Barnes_2009_PAR/index.php&#34;&gt;PatchMatch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://gfx.cs.princeton.edu/pubs/Barnes_2010_TGP/index.php&#34;&gt;Generalized PatchMatch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.eng.tau.ac.il/~simonk/CSH/&#34;&gt;Coherency Sensitive Hashing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fbesse/pmbp&#34;&gt;PMBP: PatchMatch Belief Propagation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.eng.tau.ac.il/~avidan/papers/TreeCANN_code_20121022.rar&#34;&gt;TreeCANN&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Visual Tracking&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/trackerbenchmark/benchmarks/v10&#34;&gt;Visual Tracker Benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.votchallenge.net/&#34;&gt;Visual Tracking Challenge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.ces.clemson.edu/~stb/klt/&#34;&gt;Kanade-Lucas-Tomasi Feature Tracker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.eng.tau.ac.il/~oron/ELK/ELK.html&#34;&gt;Extended Lucas-Kanade Tracking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.vision.ee.ethz.ch/boostingTrackers/&#34;&gt;Online-boosting Tracking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www4.comp.polyu.edu.hk/~cslzhang/STC/STC.htm&#34;&gt;Spatio-Temporal Context Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.shengfenghe.com/visual-tracking-via-locality-sensitive-histograms.html&#34;&gt;Locality Sensitive Histograms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_workshops_2013/W03/papers/Xiao_An_Enhanced_Adaptive_2013_ICCV_paper.pdf&#34;&gt;Enhanced adaptive coupled-layer LGTracker++&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/tld.html&#34;&gt;TLD: Tracking - Learning - Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.gnebehay.com/cmt/&#34;&gt;CMT: Clustering of Static-Adaptive Correspondences for Deformable Object Tracking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://home.isr.uc.pt/~henriques/circulant/&#34;&gt;Kernelized Correlation Filters&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cvl.isy.liu.se/en/research/objrec/visualtracking/scalvistrack/index.html&#34;&gt;Accurate Scale Estimation for Robust Visual Tracking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cs-people.bu.edu/jmzhang/MEEM/MEEM.html&#34;&gt;Multiple Experts using Entropy Minimization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.dabi.temple.edu/~hbling/code/TGPR.htm&#34;&gt;TGPR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/jbhuang0604/publications/cf2&#34;&gt;CF2: Hierarchical Convolutional Features for Visual Tracking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://webdocs.cs.ualberta.ca/~vis/mtf/index.html&#34;&gt;Modular Tracking Framework&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Saliency Detection&lt;/h4&gt; &#xA;&lt;h4&gt;Attributes&lt;/h4&gt; &#xA;&lt;h4&gt;Action Reconition&lt;/h4&gt; &#xA;&lt;h4&gt;Egocentric cameras&lt;/h4&gt; &#xA;&lt;h4&gt;Human-in-the-loop systems&lt;/h4&gt; &#xA;&lt;h4&gt;Image Captioning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/neuraltalk%EF%BB%BF&#34;&gt;NeuralTalk&lt;/a&gt; -&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Optimization&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ceres-solver.org/&#34;&gt;Ceres Solver&lt;/a&gt; - Nonlinear least-square problem and unconstrained optimization solver&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ab-initio.mit.edu/wiki/index.php/NLopt&#34;&gt;NLopt&lt;/a&gt;- Nonlinear least-square problem and unconstrained optimization solver&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://hci.iwr.uni-heidelberg.de/opengm2/&#34;&gt;OpenGM&lt;/a&gt; - Factor graph based discrete optimization and inference solver&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://collab.cc.gatech.edu/borg/gtsam/&#34;&gt;GTSAM&lt;/a&gt; - Factor graph based lease-square optimization solver&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Deep Learning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kjw0612/awesome-deep-vision&#34;&gt;Awesome Deep Vision&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Machine Learning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/josephmisiti/awesome-machine-learning&#34;&gt;Awesome Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://idiap.github.io/bob/&#34;&gt;Bob: a free signal processing and machine learning toolbox for researchers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&#34;&gt;LIBSVM -- A Library for Support Vector Machines&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;h4&gt;External Dataset Link Collection&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cvpapers.com/datasets.html&#34;&gt;CV Datasets on the web&lt;/a&gt; - CVPapers&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://rodrigob.github.io/are_we_there_yet/build/&#34;&gt;Are we there yet?&lt;/a&gt; - Which paper provides the best results on standard dataset X?&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cvpapers.com/datasets.html&#34;&gt;Computer Vision Dataset on the web&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://riemenschneider.hayko.at/vision/dataset/&#34;&gt;Yet Another Computer Vision Index To Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.computervisiononline.com/datasets&#34;&gt;ComputerVisionOnline Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://homepages.inf.ed.ac.uk/cgi/rbf/CVONLINE/entries.pl?TAG363&#34;&gt;CVOnline Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://clickdamage.com/sourcecode/cv_datasets.php&#34;&gt;CV datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://datasets.visionbib.com/info-index.html&#34;&gt;visionbib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.visualdata.io/&#34;&gt;VisualData&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Low-level Vision&lt;/h4&gt; &#xA;&lt;h6&gt;Stereo Vision&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://vision.middlebury.edu/stereo/&#34;&gt;Middlebury Stereo Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stero&#34;&gt;The KITTI Vision Benchmark Suite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cvlibs.net/software/libelas/&#34;&gt;LIBELAS: Library for Efficient Large-scale Stereo Matching&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.6d-vision.com/ground-truth-stixel-dataset&#34;&gt;Ground Truth Stixel Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Optical Flow&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://vision.middlebury.edu/flow/&#34;&gt;Middlebury Optical Flow Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://sintel.is.tue.mpg.de/&#34;&gt;MPI-Sintel Optical Flow Dataset and Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow&#34;&gt;The KITTI Vision Benchmark Suite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://hci.iwr.uni-heidelberg.de/Benchmarks/document/Challenging_Data_for_Stereo_and_Optical_Flow/&#34;&gt;HCI Challenge&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Video Object Segmentation&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://davischallenge.org/&#34;&gt;DAVIS: Densely Annotated VIdeo Segmentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://web.engr.oregonstate.edu/~lif/SegTrack2/dataset.html&#34;&gt;SegTrack v2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Change Detection&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.gti.ssr.upm.es/data/LASIESTA&#34;&gt;Labeled and Annotated Sequences for Integral Evaluation of SegmenTation Algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.changedetection.net/&#34;&gt;ChangeDetection.net&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Image Super-resolutions&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://eng.ucmerced.edu/people/cyang35/ECCV14/ECCV14.html&#34;&gt;Single-Image Super-Resolution: A Benchmark&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Intrinsic Images&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.mit.edu/~kimo/publications/intrinsic/&#34;&gt;Ground-truth dataset and baseline evaluations for intrinsic image algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://opensurfaces.cs.cornell.edu/intrinsic/&#34;&gt;Intrinsic Images in the Wild&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cic.uab.cat/Datasets/synthetic_intrinsic_image_dataset/&#34;&gt;Intrinsic Image Evaluation on Synthetic Complex Scenes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Material Recognition&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://opensurfaces.cs.cornell.edu/&#34;&gt;OpenSurface&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/celiu/CVPR2010/&#34;&gt;Flickr Material Database&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://opensurfaces.cs.cornell.edu/publications/minc/&#34;&gt;Materials in Context Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Multi-view Reconsturction&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://vision.middlebury.edu/mview/&#34;&gt;Multi-View Stereo Reconstruction&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Saliency Detection&lt;/h4&gt; &#xA;&lt;h4&gt;Visual Tracking&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/trackerbenchmark/benchmarks/v10&#34;&gt;Visual Tracker Benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/benchmarkpami/&#34;&gt;Visual Tracker Benchmark v1.1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.votchallenge.net/&#34;&gt;VOT Challenge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://tracking.cs.princeton.edu/&#34;&gt;Princeton Tracking Benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://webdocs.cs.ualberta.ca/~vis/trackDB/&#34;&gt;Tracking Manipulation Tasks (TMT)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Visual Surveillance&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.viratdata.org/&#34;&gt;VIRAT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cam2.ecn.purdue.edu/&#34;&gt;CAM2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Saliency Detection&lt;/h4&gt; &#xA;&lt;h4&gt;Change detection&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://changedetection.net/&#34;&gt;ChangeDetection.net&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Visual Recognition&lt;/h4&gt; &#xA;&lt;h6&gt;Image Classification&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://pascallin.ecs.soton.ac.uk/challenges/VOC/&#34;&gt;The PASCAL Visual Object Classes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.image-net.org/challenges/LSVRC/2014/&#34;&gt;ImageNet Large Scale Visual Recognition Challenge&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Self-supervised Learning&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yukimasano/PASS&#34;&gt;PASS: An An ImageNet replacement for self-supervised pretraining without humans&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Scene Recognition&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://groups.csail.mit.edu/vision/SUN/&#34;&gt;SUN Database&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://places.csail.mit.edu/&#34;&gt;Place Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Object Detection&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://pascallin.ecs.soton.ac.uk/challenges/VOC/&#34;&gt;The PASCAL Visual Object Classes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.image-net.org/challenges/LSVRC/2014/&#34;&gt;ImageNet Object Detection Challenge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mscoco.org/&#34;&gt;Microsoft COCO&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Semantic labeling&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://dags.stanford.edu/projects/scenedataset.html&#34;&gt;Stanford background dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/&#34;&gt;CamVid&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.unc.edu/~jtighe/Papers/ECCV10/&#34;&gt;Barcelona Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.unc.edu/~jtighe/Papers/ECCV10/siftflow/SiftFlowDataset.zip&#34;&gt;SIFT Flow Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Multi-view Object Detection&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cvgl.stanford.edu/resources.html&#34;&gt;3D Object Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cvlab.epfl.ch/data/pose&#34;&gt;EPFL Car Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_object.php&#34;&gt;KTTI Dection Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://sun3d.cs.princeton.edu/&#34;&gt;SUN 3D Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cvgl.stanford.edu/projects/pascal3d.html&#34;&gt;PASCAL 3D+&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nyc3d.cs.cornell.edu/&#34;&gt;NYU Car Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Fine-grained Visual Recognition&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/fgcomp2013/&#34;&gt;Fine-grained Classification Challenge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.vision.caltech.edu/visipedia/CUB-200.html&#34;&gt;Caltech-UCSD Birds 200&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Pedestrian Detection&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/&#34;&gt;Caltech Pedestrian Detection Benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://data.vision.ee.ethz.ch/cvl/aess/dataset/&#34;&gt;ETHZ Pedestrian Detection&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Action Recognition&lt;/h4&gt; &#xA;&lt;h6&gt;Image-based&lt;/h6&gt; &#xA;&lt;h6&gt;Video-based&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.di.ens.fr/~laptev/actions/hollywood2/&#34;&gt;HOLLYWOOD2 Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://crcv.ucf.edu/data/UCF_Sports_Action.php&#34;&gt;UCF Sports Action Data Set&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Image Deblurring&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cs.brown.edu/~lbsun/deblur2013/deblur2013iccp.html&#34;&gt;Sun dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.wisdom.weizmann.ac.il/~levina/papers/LevinEtalCVPR09Data.rar&#34;&gt;Levin dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Image Captioning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.html&#34;&gt;Flickr 8K&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://shannon.cs.illinois.edu/DenotationGraph/&#34;&gt;Flickr 30K&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mscoco.org/&#34;&gt;Microsoft COCO&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Scene Understanding&lt;/h4&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;http://rgbd.cs.princeton.edu/&#34;&gt;SUN RGB-D&lt;/a&gt; - A RGB-D Scene Understanding Benchmark Suite&lt;/h1&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html&#34;&gt;NYU depth v2&lt;/a&gt; - Indoor Segmentation and Support Inference from RGBD Images&lt;/h1&gt; &#xA;&lt;h4&gt;Aerial images&lt;/h4&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://zenodo.org/record/1154821#.WmN9kHWnHIp&#34;&gt;Aerial Image Segmentation&lt;/a&gt; - Learning Aerial Image Segmentation From Online Maps&lt;/h1&gt; &#xA;&lt;h2&gt;Resources for students&lt;/h2&gt; &#xA;&lt;h4&gt;Resource link collection&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/fredo/student.html&#34;&gt;Resources for students&lt;/a&gt; - Fr√©do Durand (MIT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.dgp.toronto.edu/~hertzman/advice/&#34;&gt;Advice for Graduate Students&lt;/a&gt; - Aaron Hertzmann (Adobe Research)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.dgp.toronto.edu/~hertzman/courses/gradSkills/2010/&#34;&gt;Graduate Skills Seminars&lt;/a&gt; - Yashar Ganjali, Aaron Hertzmann (University of Toronto)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://research.microsoft.com/en-us/um/people/simonpj/papers/giving-a-talk/giving-a-talk.htm&#34;&gt;Research Skills&lt;/a&gt; - Simon Peyton Jones (Microsoft Research)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://web.engr.illinois.edu/~taoxie/advice.htm&#34;&gt;Resource collection&lt;/a&gt; - Tao Xie (UIUC) and Yuan Xie (UCSB)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Writing&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/fredo/FredoGoodWriting.pdf&#34;&gt;Write Good Papers&lt;/a&gt; - Fr√©do Durand (MIT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/fredo/PUBLI/writing.pdf&#34;&gt;Notes on writing&lt;/a&gt; - Fr√©do Durand (MIT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/fredo/FredoBadWriting.pdf&#34;&gt;How to Write a Bad Article&lt;/a&gt; - Fr√©do Durand (MIT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://billf.mit.edu/sites/default/files/documents/cvprPapers.pdf&#34;&gt;How to write a good CVPR submission&lt;/a&gt; - William T. Freeman (MIT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=g3dkRsTqdDA&#34;&gt;How to write a great research paper&lt;/a&gt; - Simon Peyton Jones (Microsoft Research)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/jdily/how-to-write-a-siggraph-paper&#34;&gt;How to write a SIGGRAPH paper&lt;/a&gt; - SIGGRAPH ASIA 2011 Course&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.dgp.toronto.edu/~hertzman/advice/writing-technical-papers.pdf&#34;&gt;Writing Research Papers&lt;/a&gt; - Aaron Hertzmann (Adobe Research)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.computer.org/csdl/mags/cg/1987/12/mcg1987120062.pdf&#34;&gt;How to Write a Paper for SIGGRAPH&lt;/a&gt; - Jim Blinn&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.siggraph.org/sites/default/files/kajiya.pdf&#34;&gt;How to Get Your SIGGRAPH Paper Rejected&lt;/a&gt; - Jim Kajiya (Microsoft Research)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jbhuang0604/awesome-computer-vision/master/www.liyiwei.org/courses/how-siga11/liyiwei.pptx&#34;&gt;How to write a SIGGRAPH paper&lt;/a&gt; - Li-Yi Wei (The University of Hong Kong)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www-hagen.informatik.uni-kl.de/~bertram/talks/getpublished.pdf&#34;&gt;How to Write a Great Paper&lt;/a&gt; - Martin Martin Hering Hering--Bertram (Hochschule Bremen University of Applied Sciences)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www-ui.is.s.u-tokyo.ac.jp/~takeo/writings/siggraph.html&#34;&gt;How to have a paper get into SIGGRAPH?&lt;/a&gt; - Takeo Igarashi (The University of Tokyo)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~pausch/Randy/Randy/raibert.htm&#34;&gt;Good Writing&lt;/a&gt; - Marc H. Raibert (Boston Dynamics, Inc.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://web.engr.illinois.edu/~dhoiem/presentations/How%20to%20Write%20a%20Computer%20Vison%20Paper.ppt&#34;&gt;How to Write a Computer Vision Paper&lt;/a&gt; - Derek Hoiem (UIUC)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.dartmouth.edu/~wjarosz/writing.html&#34;&gt;Common mistakes in technical writing&lt;/a&gt; - Wojciech Jarosz (Dartmouth College)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Presentation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/fredo/TalkAdvice.pdf&#34;&gt;Giving a Research Talk&lt;/a&gt; - Fr√©do Durand (MIT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.dgp.toronto.edu/~hertzman/courses/gradSkills/2010/GivingGoodTalks.pdf&#34;&gt;How to give a good talk&lt;/a&gt; - David Fleet (University of Toronto) and Aaron Hertzmann (Adobe Research)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://colinpurrington.com/tips/poster-design&#34;&gt;Designing conference posters&lt;/a&gt; - Colin Purrington&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Research&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/billf/www/papers/doresearch.pdf&#34;&gt;How to do research&lt;/a&gt; - William T. Freeman (MIT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.virginia.edu/~robins/YouAndYourResearch.html&#34;&gt;You and Your Research&lt;/a&gt; - Richard Hamming&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://yima.csl.illinois.edu/psfile/bogus.pdf&#34;&gt;Warning Signs of Bogus Progress in Research in an Age of Rich Computation and Information&lt;/a&gt; - Yi Ma (UIUC)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.quackwatch.com/01QuackeryRelatedTopics/signs.html&#34;&gt;Seven Warning Signs of Bogus Science&lt;/a&gt; - Robert L. Park&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=v2Qaf8t8I6c&#34;&gt;Five Principles for Choosing Research Problems in Computer Graphics&lt;/a&gt; - Thomas Funkhouser (Cornell University)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.indiana.edu/mit.research.how.to.html&#34;&gt;How To Do Research In the MIT AI Lab&lt;/a&gt; - David Chapman (MIT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/antiw/recent-advances-in-computer-vision&#34;&gt;Recent Advances in Computer Vision&lt;/a&gt; - Ming-Hsuan Yang (UC Merced)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/jbhuang/how-to-come-up-with-new-research-ideas-4005840&#34;&gt;How to Come Up with Research Ideas in Computer Vision?&lt;/a&gt; - Jia-Bin Huang (UIUC)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/jbhuang/how-to-read-academic-papers&#34;&gt;How to Read Academic Papers&lt;/a&gt; - Jia-Bin Huang (UIUC)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Time Management&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oTugjssqOT0&#34;&gt;Time Management&lt;/a&gt; - Randy Pausch (CMU)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Blogs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.learnopencv.com/&#34;&gt;Learn OpenCV&lt;/a&gt; - Satya Mallick&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.computervisionblog.com/&#34;&gt;Tombone&#39;s Computer Vision Blog&lt;/a&gt; - Tomasz Malisiewicz&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.visiondummy.com/&#34;&gt;Computer vision for dummies&lt;/a&gt; - Vincent Spruyt&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://karpathy.github.io/&#34;&gt;Andrej Karpathy blog&lt;/a&gt; - Andrej Karpathy&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://aishack.in/&#34;&gt;AI Shack&lt;/a&gt; - Utkarsh Sinha&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://computer-vision-talks.com/&#34;&gt;Computer Vision Talks&lt;/a&gt; - Eugene Khvedchenya&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jrobchin/Computer-Vision-Basics-with-Python-Keras-and-OpenCV&#34;&gt;Computer Vision Basics with Python Keras and OpenCV&lt;/a&gt; - Jason Chin (University of Western Ontario)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.ubc.ca/~lowe/vision.html&#34;&gt;The Computer Vision Industry&lt;/a&gt; - David Lowe&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://hci.iwr.uni-heidelberg.de/Links/German_Vision/&#34;&gt;German Computer Vision Research Groups &amp;amp; Companies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ChristosChristofidis/awesome-deep-learning&#34;&gt;awesome-deep-learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/josephmisiti/awesome-machine-learning&#34;&gt;awesome-machine-learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.eecs.berkeley.edu/~junyanz/cat/cat_papers.html&#34;&gt;Cat Paper Collection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.rsipvision.com/computer-vision-news/&#34;&gt;Computer Vision News&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Songs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://danielwedge.com/fmatrix/&#34;&gt;The Fundamental Matrix Song&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://danielwedge.com/ransac/&#34;&gt;The RANSAC Song&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=DQWI1kvmwRg&#34;&gt;Machine Learning A Cappella - Overfitting Thriller&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Licenses&lt;/h2&gt; &#xA;&lt;p&gt;License&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://creativecommons.org/publicdomain/zero/1.0/&#34;&gt;&lt;img src=&#34;http://i.creativecommons.org/p/zero/1.0/88x31.png&#34; alt=&#34;CC0&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To the extent possible under law, &lt;a href=&#34;https://raw.githubusercontent.com/jbhuang0604/awesome-computer-vision/master/www.jiabinhuang.com&#34;&gt;Jia-Bin Huang&lt;/a&gt; has waived all copyright and related or neighboring rights to this work.&lt;/p&gt;</summary>
  </entry>
</feed>