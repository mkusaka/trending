<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-01T01:25:11Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>polymorphicshade/Tubular</title>
    <updated>2024-02-01T01:25:11Z</updated>
    <id>tag:github.com,2024-02-01:/polymorphicshade/Tubular</id>
    <link href="https://github.com/polymorphicshade/Tubular" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;&lt;b&gt;Tubular&lt;/b&gt;&lt;/h1&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt;A fork of &lt;a href=&#34;https://newpipe.net/&#34;&gt;NewPipe&lt;/a&gt; that implements &lt;a href=&#34;https://sponsor.ajay.app/&#34;&gt;SponsorBlock&lt;/a&gt; and &lt;a href=&#34;https://www.returnyoutubedislike.com/&#34;&gt;ReturnYouTubeDislike&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;h2&gt;Previews&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/polymorphicshade/Tubular/master/doc/gif/01.gif&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/polymorphicshade/Tubular/master/doc/gif/01.gif&#34; width=&#34;200&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/polymorphicshade/Tubular/master/doc/gif/02.gif&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/polymorphicshade/Tubular/master/doc/gif/02.gif&#34; width=&#34;200&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/polymorphicshade/Tubular/master/doc/gif/03.gif&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/polymorphicshade/Tubular/master/doc/gif/03.gif&#34; width=&#34;200&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;To Do&lt;/h2&gt; &#xA;&lt;p&gt;Things I&#39;ll be working on next (not in any particular order):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; persist custom SponsorBlock segments in the database&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; add SponsorBlock&#39;s &#34;Exclusive Access&#34; / &#34;Sponsored Video feature&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; add SponsorBlock&#39;s chapters feature&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; add a clickbait-remover&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; add keyword/regex filtering&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; add subscription importing with a YouTube login cookie&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; add algorithmic results with a YouTube login cookie&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; add offline YouTube playback&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.en.html&#34;&gt;&lt;img src=&#34;https://www.gnu.org/graphics/gplv3-127x51.png&#34; alt=&#34;GNU GPLv3&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>InkboxSoftware/excelCPU</title>
    <updated>2024-02-01T01:25:11Z</updated>
    <id>tag:github.com,2024-02-01:/InkboxSoftware/excelCPU</id>
    <link href="https://github.com/InkboxSoftware/excelCPU" rel="alternate"></link>
    <summary type="html">&lt;p&gt;16-bit CPU for Excel, and related files&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Excel 16-Bit CPU&lt;/h1&gt; &#xA;&lt;p&gt;The Excel 16-Bit CPU repository contains the following main files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CPU.xlsx - The main spreadsheet which contains the CPU&#xA;ROM.xlsx - The ROM spreadsheet used read by the CPU when the read ROM switch is turned on&#xA;InstructionSet.xlsx - Explains the ISA of the CPU&#xA;compileExcelASM16.py - The Excel-ASM16 compiler&#xA;Excel-ASM16.xml - Markdown for the Excel-ASM16 language compatible with Notepad++&#xA;Sample Programs - Folder of sample programs for the Excel CPU&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The CPU.xlsx file features a 16-bit CPU, 16 general purpose registers, 128KB of RAM, and a 128x128 display.&lt;/p&gt; &#xA;&lt;p&gt;Iterative Calcuation must be turned on. This can be done by going to File -&amp;gt; Options -&amp;gt; Formulas -&amp;gt; then Enable Iterative Calculation and &lt;strong&gt;set Maximum Iterations to 1&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The CPU runs off a clock signal set in B2. This clock signal will update under the normal conditions of recalculation within an Excel spreadsheet. Pressing the F9 key will recalculate the spreadsheet.&lt;/p&gt; &#xA;&lt;p&gt;The Reset Button in the F2 cell, if set to true, will reset the PC register back to 0.&lt;/p&gt; &#xA;&lt;p&gt;The computer in the CPU.xlsx file can be controlled either in automatic or manual mode. This is controlled by the button in J2. If set to true, when the clock signal from B2 is high, then the CPU will carry out the operation specified in the override slot in the Fetch Unit in cell D8. If false, then the CPU will execute the operation retrieved from the memory table as specified by the PC register.&lt;/p&gt; &#xA;&lt;p&gt;The Reset RAM button, if set to true, will reset every memory unit to 0.&lt;/p&gt; &#xA;&lt;p&gt;The Read ROM button, if set to true, will copy the values of the memory table in the ROM.xlsx spreadsheet onto the RAM table of the CPU.xlsx spreadsheet.&lt;/p&gt; &#xA;&lt;p&gt;Normal operation of the CPU consists of setting the Reset Button to high, either flipping the Reset RAM or Read ROM buttons on and off again (causing the RAM to be reset or the ROM to be read into the RAM table), and then turning off the Reset Button. The CPU is then set up to either run a program in Manual mode, or will carry out the program specified in RAM.&lt;/p&gt; &#xA;&lt;p&gt;The CPU is designed to run according to the instruction set architecture specified in the InstructionSet.xlsx spreadsheet.&lt;/p&gt; &#xA;&lt;p&gt;Warning: It is not possible to simply mash the F9 key as fast as possible, it takes time for Excel to update so many cells, it is recommended to wait until the text &#34;Ready&#34; can be seen in the bottom left corner of Excel can be seen before continuing to press the F9 key.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, programs can be written in the Excel-ASM16 language and compiled to the ROM.xlsx spreadsheet.&lt;/p&gt; &#xA;&lt;p&gt;Excel-ASM16 features 24 different case-insensitive instructions. There are three different operands that are used in each instruction&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;REG&#x9;; refers to any of the 16 general purpose registers&#xA;&#x9;E.G. R0, R1, R15 &amp;amp;c.&#xA;&#x9;&#xA;&#x9;MEM&#x9;; refers to any 16-bit addressable memory unit (formatted in hexadecimal)&#xA;&#x9;E.G. @0000, @F000, @FFFF, &amp;amp;c.&#xA;&#xA;&#x9;IMD&#x9;; refers to an immediate number usually 16-bits long, except in the case of ROL and ROR&#xA;&#x9;&#x9;; can be defined either in decimal or hexadecimal&#xA;&#x9;E.G. #0000, $0CCC, #60340, $FF10, &amp;amp;c.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LOAD&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;LOAD REG MEM&#x9;; loads the specified memory unit into REG&#xA;&#x9;LOAD REG IMD&#x9;; load specified 16-bit immediate value into REG&#xA;&#x9;LOAD REG REG&#x9;; loads memory unit at the address stored in REGB into REGA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;STORE&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;STORE REG MEM&#x9;; stores the value of REG to the address specified&#xA;&#x9;STORE REG REG &#x9;; stores the value of REGA into the memory unit at the address in REGB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;JUMP&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;JMP IMD&#x9;&#x9;; sets PC to the immediate 16-bit value&#xA;&#x9;JEQ IMD&#x9;&#x9;; if ZF = 0, sets PC to the immediate 16-bit value&#xA;&#x9;JLT IMD&#x9;&#x9;; if CF = 0, sets PC to the immediate 16-bit value &#xA;&#x9;JGE IMD&#x9;&#x9;; if CF = 1 or ZF = 1, sets PC to the immediate 16-bit value &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;TRAN&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;TRAN REG REG&#x9;; transfers value from REGA to REGB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ALGEBRAIC INSTRUCTIONS&lt;/h3&gt; &#xA;&lt;h3&gt;ADD&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;ADD REG REG&#x9;; REGA + REGB + CF, result stored in REGA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SUB&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;SUB REG REG&#x9;; (REGA - REGB) - CF, result stored in REGA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;MULT&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;MULT REG REG&#x9;; REGA * REGB, low 16-bit result stored in REGA, high 16-bit result stored in REGB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;DIV&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;DIV REG REG&#x9;; REGA / REGB result stored in REGA, REGA MOD REGB stored in REGB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;INC&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;INC REG&#x9;; REGA++, CF not affected&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;DEC&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;DEC REG&#x9;; REGA--, CF not affected&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;BITWISE INSTRUCTIONS&lt;/h3&gt; &#xA;&lt;h3&gt;AND&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;AND REG REG&#x9;; REGA AND REGB, result stored in REGA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;OR&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;OR REG REG&#x9;&#x9;; REGA OR REGB, result stored in REGA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;XOR&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;XOR REG REG&#x9;; REGA XOR REGB, result stored in REGA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;NOT&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;NOT REG &#x9;&#x9;; NOT REGA, result stored in REGA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ROLL INSTRUCTIONS&lt;/h3&gt; &#xA;&lt;h3&gt;ROL&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;ROL REG IMD&#x9;; leftwise roll of bits of REGA carried out IMD times&#xA;&#x9;&#x9;&#x9;&#x9;; IMD is a 4-bit value&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ROR&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;ROR REG IMD&#x9;; rightwise roll of bits of REGA carried out IMD times&#xA;&#x9;&#x9;&#x9;&#x9;; IMD is a 4-bit value&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Flag instructions&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;CLC&#x9;&#x9;&#x9;; sets CF to 0&#xA;&#x9;STC&#x9;&#x9;&#x9;; sets CF to 1 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;NOP&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;NOP&#x9;&#x9;&#x9;; does not effect any registers or memory&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ORG&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;ORG IMD&#x9;&#x9;; sets the location of the next instruction&#xA;&#x9;&#x9;&#x9;&#x9;; must be further than the current length of program&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;INC&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;INC &#34;file.bin&#34;&#x9;; copies the binary file into the program&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Compiling&lt;/h3&gt; &#xA;&lt;p&gt;After having written a program, it is compiled with the commandline instruction&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#x9;py compileExcelASM16.py program.s ROM.xlsx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where &lt;strong&gt;program.s&lt;/strong&gt; is the user&#39;s program file, and ROM.xlsx is the ROM spreadsheet&lt;/p&gt; &#xA;&lt;p&gt;After compiling successfully, the program can be transferred into the CPU.xlsx program by flipping the Read ROM button at the top of the spreadsheet. Note, the ROM.xlsx file must be open for the data to update correctly.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PKU-YuanGroup/MoE-LLaVA</title>
    <updated>2024-02-01T01:25:11Z</updated>
    <id>tag:github.com,2024-02-01:/PKU-YuanGroup/MoE-LLaVA</id>
    <link href="https://github.com/PKU-YuanGroup/MoE-LLaVA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mixture-of-Experts for Large Vision-Language Models&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://s11.ax1x.com/2023/12/28/piqvDMV.png&#34; width=&#34;250&#34; style=&#34;margin-bottom: 0.2;&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2 align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.15947&#34;&gt;MoE-LLaVA: Mixture of Experts for Large Vision-Language Models&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h5 align=&#34;center&#34;&gt; If you like our project, please give us a star ⭐ on GitHub for latest update. &lt;/h5&gt; &#xA;&lt;h5 align=&#34;center&#34;&gt; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/LanguageBind/MoE-LLaVA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20In%20Spaces-blue.svg?sanitize=true&#34; alt=&#34;hf_space&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.15947&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2401.15947-b31b1b.svg?logo=arXiv&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/ICylR6n2LhqQRS0CAHFI1A&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-WeChat@%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83-000000?logo=wechat&amp;amp;logoColor=07C160&#34; alt=&#34;jiqizhixin&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/MoE-LLaVA/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-yellow&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hits.seeyoufarm.com&#34;&gt;&lt;img src=&#34;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FPKU-YuanGroup%2FMoE-LLaVA&amp;amp;count_bg=%2379C83D&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=Visitor&amp;amp;edge_flat=false&#34; alt=&#34;Hits&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/MoE-LLaVA/issues?q=is%3Aopen+is%3Aissue&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/PKU-YuanGroup/MoE-LLaVA?color=critical&amp;amp;label=Issues&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/MoE-LLaVA/issues?q=is%3Aissue+is%3Aclosed&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-closed/PKU-YuanGroup/MoE-LLaVA?color=success&amp;amp;label=Issues&#34; alt=&#34;GitHub closed issues&#34;&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &lt;/h5&gt; &#xA;&lt;details open&gt;&#xA; &lt;summary&gt;💡 I also have other vision-language projects that may interest you ✨. &lt;/summary&gt;&#xA; &lt;p&gt; &#xA;  &lt;!--  may --&gt; &lt;/p&gt;&#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.10122&#34;&gt;&lt;strong&gt;Video-LLaVA: Learning United Visual Representation by Alignment Before Projection&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, Li Yuan &lt;br&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Github-black?logo=github&#34; alt=&#34;github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PKU-YuanGroup/Video-LLaVA.svg?style=social&#34; alt=&#34;github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.10122&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2311.10122-b31b1b.svg?logo=arXiv&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.01852&#34;&gt;&lt;strong&gt;LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Wancai Zhang, Zhifeng Li, Wei Liu, Li Yuan &lt;br&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/LanguageBind&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Github-black?logo=github&#34; alt=&#34;github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/LanguageBind&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PKU-YuanGroup/LanguageBind.svg?style=social&#34; alt=&#34;github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.01852&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2310.01852-b31b1b.svg?logo=arXiv&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;&lt;/p&gt;&#xA;&lt;/details&gt; &#xA;&lt;h2&gt;📰 News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.01.30]&lt;/strong&gt; 🔥 We release a stronger &lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-Phi2-2.7B-4e-384&#34;&gt;MoE-LLaVA-Phi2&lt;/a&gt;. &lt;strong&gt;The average performance surpasses LLaVA-1.5-7B by using 3.6B activated parameters,&lt;/strong&gt; checking our &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/#-model-zoo&#34;&gt;model zoo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.01.27]&lt;/strong&gt; 🤗 &lt;a href=&#34;https://huggingface.co/spaces/LanguageBind/MoE-LLaVA&#34;&gt;Hugging Face demo&lt;/a&gt; and &lt;strong&gt;all codes &amp;amp; datasets&lt;/strong&gt; are available now! Welcome to &lt;strong&gt;watch&lt;/strong&gt; 👀 this repository for the latest updates.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;😮 Highlights&lt;/h2&gt; &#xA;&lt;p&gt;MoE-LLaVA shows excellent performance in multi-modal learning.&lt;/p&gt; &#xA;&lt;h3&gt;🔥 High performance, but with fewer parameters&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;with just &lt;strong&gt;3B sparsely activated parameters&lt;/strong&gt;, MoE-LLaVA demonstrates performance comparable to the LLaVA-1.5-7B on various visual understanding datasets and even surpasses the LLaVA-1.5-13B in object hallucination benchmarks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/assets/intro0.jpg&#34; width=&#34;55%&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;🚀 Simple baseline, learning multi-modal interactions with sparse pathways.&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;With the addition of &lt;strong&gt;a simple MoE tuning stage&lt;/strong&gt;, we can complete the training of MoE-LLaVA on &lt;strong&gt;8 V100 GPUs&lt;/strong&gt; within 2 days.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/assets/intro.jpg&#34; width=&#34;65%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;🤗 Demo&lt;/h2&gt; &#xA;&lt;h3&gt;Gradio Web UI&lt;/h3&gt; &#xA;&lt;p&gt;Highly recommend trying out our web demo by the following command, which incorporates all features currently supported by MoE-LLaVA. We also provide &lt;a href=&#34;https://huggingface.co/spaces/LanguageBind/MoE-LLaVA&#34;&gt;online demo&lt;/a&gt; in Huggingface Spaces.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# use phi2&#xA;deepspeed --include localhost:0 moellava/serve/gradio_web_server.py --model-path &#34;LanguageBind/MoE-LLaVA-Phi2-2.7B-4e&#34; &#xA;# use qwen&#xA;deepspeed --include localhost:0 moellava/serve/gradio_web_server.py --model-path &#34;LanguageBind/MoE-LLaVA-Qwen-1.8B-4e&#34; &#xA;# use stablelm&#xA;deepspeed --include localhost:0 moellava/serve/gradio_web_server.py --model-path &#34;LanguageBind/MoE-LLaVA-StableLM-1.6B-4e&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/MoE-LLaVA/assets/62638829/8541aac6-9ef6-4fde-aa94-80d0375b9bdb&#34;&gt;https://github.com/PKU-YuanGroup/MoE-LLaVA/assets/62638829/8541aac6-9ef6-4fde-aa94-80d0375b9bdb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;CLI Inference&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# use phi2&#xA;deepspeed --include localhost:0 moellava/serve/cli.py --model-path &#34;LanguageBind/MoE-LLaVA-Phi2-2.7B-4e&#34;  --image-file &#34;image.jpg&#34;&#xA;# use qwen&#xA;deepspeed --include localhost:0 moellava/serve/cli.py --model-path &#34;LanguageBind/MoE-LLaVA-Qwen-1.8B-4e&#34;  --image-file &#34;image.jpg&#34;&#xA;# use stablelm&#xA;deepspeed --include localhost:0 moellava/serve/cli.py --model-path &#34;LanguageBind/MoE-LLaVA-StableLM-1.6B-4e&#34;  --image-file &#34;image.jpg&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/assets/imagecli.gif&#34;&gt; &#xA;&lt;h2&gt;🐳 Model Zoo&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Activated Param&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;Avg&lt;/th&gt; &#xA;   &lt;th&gt;VQAv2&lt;/th&gt; &#xA;   &lt;th&gt;GQA&lt;/th&gt; &#xA;   &lt;th&gt;VizWiz&lt;/th&gt; &#xA;   &lt;th&gt;SQA&lt;/th&gt; &#xA;   &lt;th&gt;T-VQA&lt;/th&gt; &#xA;   &lt;th&gt;POPE&lt;/th&gt; &#xA;   &lt;th&gt;MM-Bench&lt;/th&gt; &#xA;   &lt;th&gt;MM-Vet&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MoE-LLaVA-1.6B×4-Top2&lt;/td&gt; &#xA;   &lt;td&gt;2.0B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-StableLM-1.6B-4e&#34;&gt;LanguageBind/MoE-LLaVA-StableLM-1.6B-4e&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;57.3&lt;/td&gt; &#xA;   &lt;td&gt;76.7&lt;/td&gt; &#xA;   &lt;td&gt;60.3&lt;/td&gt; &#xA;   &lt;td&gt;36.2&lt;/td&gt; &#xA;   &lt;td&gt;62.6&lt;/td&gt; &#xA;   &lt;td&gt;50.1&lt;/td&gt; &#xA;   &lt;td&gt;85.7&lt;/td&gt; &#xA;   &lt;td&gt;60.2&lt;/td&gt; &#xA;   &lt;td&gt;26.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MoE-LLaVA-1.8B×4-Top2&lt;/td&gt; &#xA;   &lt;td&gt;2.2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-Qwen-1.8B-4e&#34;&gt;LanguageBind/MoE-LLaVA-Qwen-1.8B-4e&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;56.7&lt;/td&gt; &#xA;   &lt;td&gt;76.2&lt;/td&gt; &#xA;   &lt;td&gt;61.5&lt;/td&gt; &#xA;   &lt;td&gt;32.6&lt;/td&gt; &#xA;   &lt;td&gt;63.1&lt;/td&gt; &#xA;   &lt;td&gt;48.0&lt;/td&gt; &#xA;   &lt;td&gt;87.0&lt;/td&gt; &#xA;   &lt;td&gt;59.6&lt;/td&gt; &#xA;   &lt;td&gt;25.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MoE-LLaVA-2.7B×4-Top2&lt;/td&gt; &#xA;   &lt;td&gt;3.6B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-Phi2-2.7B-4e&#34;&gt;LanguageBind/MoE-LLaVA-Phi2-2.7B-4e&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;60.3&lt;/td&gt; &#xA;   &lt;td&gt;77.1&lt;/td&gt; &#xA;   &lt;td&gt;61.1&lt;/td&gt; &#xA;   &lt;td&gt;43.4&lt;/td&gt; &#xA;   &lt;td&gt;68.7&lt;/td&gt; &#xA;   &lt;td&gt;50.2&lt;/td&gt; &#xA;   &lt;td&gt;85.0&lt;/td&gt; &#xA;   &lt;td&gt;65.5&lt;/td&gt; &#xA;   &lt;td&gt;31.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MoE-LLaVA-2.7B×4-Top2-384&lt;/td&gt; &#xA;   &lt;td&gt;3.6B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-Phi2-2.7B-4e-384&#34;&gt;LanguageBind/MoE-LLaVA-Phi2-2.7B-4e-384&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;62.9&lt;/td&gt; &#xA;   &lt;td&gt;79.9&lt;/td&gt; &#xA;   &lt;td&gt;62.6&lt;/td&gt; &#xA;   &lt;td&gt;43.7&lt;/td&gt; &#xA;   &lt;td&gt;70.3&lt;/td&gt; &#xA;   &lt;td&gt;57.0&lt;/td&gt; &#xA;   &lt;td&gt;85.7&lt;/td&gt; &#xA;   &lt;td&gt;68.0&lt;/td&gt; &#xA;   &lt;td&gt;35.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-1.5&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/liuhaotian/llava-v1.5-7b&#34;&gt;liuhaotian/llava-v1.5-7b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;62.0&lt;/td&gt; &#xA;   &lt;td&gt;78.5&lt;/td&gt; &#xA;   &lt;td&gt;62.0&lt;/td&gt; &#xA;   &lt;td&gt;50.0&lt;/td&gt; &#xA;   &lt;td&gt;66.8&lt;/td&gt; &#xA;   &lt;td&gt;58.2&lt;/td&gt; &#xA;   &lt;td&gt;85.9&lt;/td&gt; &#xA;   &lt;td&gt;64.3&lt;/td&gt; &#xA;   &lt;td&gt;30.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!--&#xA;| LLaVA-1.5 | 13B | [liuhaotian/llava-v1.5-13b](https://huggingface.co/liuhaotian/llava-v1.5-13b) | 64.9 | 80.0 | 63.3 | 53.6 | 71.6 | 61.3 | 85.9 | 67.7 | 35.4 |&#xA;--&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Pretrain Model&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MoE-LLaVA-1.6B×4-Top2&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-StableLM-Pretrain&#34;&gt;LanguageBind/MoE-LLaVA-StableLM-Pretrain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MoE-LLaVA-1.8B×4-Top2&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-Qwen-Pretrain&#34;&gt;LanguageBind/MoE-LLaVA-Qwen-Pretrain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MoE-LLaVA-2.7B×4-Top2&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-Phi2-Pretrain&#34;&gt;LanguageBind/MoE-LLaVA-Phi2-Pretrain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MoE-LLaVA-2.7B×4-Top2-384&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LanguageBind/MoE-LLaVA-Phi2-384-Pretrain&#34;&gt;LanguageBind/MoE-LLaVA-Phi2-384-Pretrain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;⚙️ Requirements and Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.10&lt;/li&gt; &#xA; &lt;li&gt;Pytorch == 2.0.1&lt;/li&gt; &#xA; &lt;li&gt;CUDA Version &amp;gt;= 11.7&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Transformers == 4.36.2&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tokenizers==0.15.1&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install required packages:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/PKU-YuanGroup/MoE-LLaVA&#xA;cd MoE-LLaVA&#xA;conda create -n moellava python=3.10 -y&#xA;conda activate moellava&#xA;pip install --upgrade pip  # enable PEP 660 support&#xA;pip install -e .&#xA;pip install -e &#34;.[train]&#34;&#xA;pip install flash-attn --no-build-isolation&#xA;&#xA;# Below are optional. For Qwen model.&#xA;git clone https://github.com/Dao-AILab/flash-attention&#xA;cd flash-attention &amp;amp;&amp;amp; pip install .&#xA;# Below are optional. Installing them might be slow.&#xA;# pip install csrc/layer_norm&#xA;# If the version of flash-attn is higher than 2.1.1, the following is not needed.&#xA;# pip install csrc/rotary&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🗝️ Training &amp;amp; Validating&lt;/h2&gt; &#xA;&lt;p&gt;The training &amp;amp; validating instruction is in &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/docs/TRAIN.md&#34;&gt;TRAIN.md&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/docs/EVAL.md&#34;&gt;EVAL.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;💡 Customizing your MoE-LLaVA&lt;/h2&gt; &#xA;&lt;p&gt;The instruction is in &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/docs/CUSTOM.md&#34;&gt;CUSTOM.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;😍 Visualization&lt;/h2&gt; &#xA;&lt;p&gt;The instruction is in &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MoE-LLaVA/main/docs/VISUALIZATION.md&#34;&gt;VISUALIZATION.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🤖 API&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;We open source all codes.&lt;/strong&gt; If you want to load the model (e.g. &lt;code&gt;LanguageBind/MoE-LLaVA&lt;/code&gt;) on local, you can use the following code snippets.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using the following command to run the code.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;deepspeed --include localhost:0 predict.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from moellava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN&#xA;from moellava.conversation import conv_templates, SeparatorStyle&#xA;from moellava.model.builder import load_pretrained_model&#xA;from moellava.utils import disable_torch_init&#xA;from moellava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria&#xA;&#xA;def main():&#xA;    disable_torch_init()&#xA;    image = &#39;moellava/serve/examples/extreme_ironing.jpg&#39;&#xA;    inp = &#39;What is unusual about this image?&#39;&#xA;    model_path = &#39;LanguageBind/MoE-LLaVA-Phi2-2.7B-4e&#39;  # LanguageBind/MoE-LLaVA-Qwen-1.8B-4e or LanguageBind/MoE-LLaVA-StableLM-1.6B-4e&#xA;    device = &#39;cuda&#39;&#xA;    load_4bit, load_8bit = False, False  # FIXME: Deepspeed support 4bit or 8bit?&#xA;    model_name = get_model_name_from_path(model_path)&#xA;    tokenizer, model, processor, context_len = load_pretrained_model(model_path, None, model_name, load_8bit, load_4bit, device=device)&#xA;    image_processor = processor[&#39;image&#39;]&#xA;    conv_mode = &#34;phi&#34;  # qwen or stablelm&#xA;    conv = conv_templates[conv_mode].copy()&#xA;    roles = conv.roles&#xA;    image_tensor = image_processor.preprocess(image, return_tensors=&#39;pt&#39;)[&#39;pixel_values&#39;].to(model.device, dtype=torch.float16)&#xA;&#xA;    print(f&#34;{roles[1]}: {inp}&#34;)&#xA;    inp = DEFAULT_IMAGE_TOKEN + &#39;\n&#39; + inp&#xA;    conv.append_message(conv.roles[0], inp)&#xA;    conv.append_message(conv.roles[1], None)&#xA;    prompt = conv.get_prompt()&#xA;    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=&#39;pt&#39;).unsqueeze(0).cuda()&#xA;    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2&#xA;    keywords = [stop_str]&#xA;    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)&#xA;&#xA;    with torch.inference_mode():&#xA;        output_ids = model.generate(&#xA;            input_ids,&#xA;            images=image_tensor,&#xA;            do_sample=True,&#xA;            temperature=0.2,&#xA;            max_new_tokens=1024,&#xA;            use_cache=True,&#xA;            stopping_criteria=[stopping_criteria])&#xA;&#xA;    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:], skip_special_tokens=True).strip()&#xA;    print(outputs)&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    main()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🙌 Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA&#34;&gt;Video-LLaVA&lt;/a&gt; This framework empowers the model to efficiently utilize the united visual tokens.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/LanguageBind&#34;&gt;LanguageBind&lt;/a&gt; An open source five modalities language-based retrieval framework.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;👍 Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt; The codebase we built upon and it is an efficient large language and vision assistant.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🔒 License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The majority of this project is released under the Apache 2.0 license as found in the &lt;a href=&#34;https://github.com/PKU-YuanGroup/MoE-LLaVA/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;The service is a research preview intended for non-commercial use only, subject to the model &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&#34;&gt;License&lt;/a&gt; of LLaMA, &lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;Terms of Use&lt;/a&gt; of the data generated by OpenAI, and &lt;a href=&#34;https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb&#34;&gt;Privacy Practices&lt;/a&gt; of ShareGPT. Please contact us if you find any potential violation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;✏️ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;⭐&lt;/span&gt; and citation &lt;span&gt;📝&lt;/span&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{lin2024moellava,&#xA;      title={MoE-LLaVA: Mixture of Experts for Large Vision-Language Models}, &#xA;      author={Bin Lin and Zhenyu Tang and Yang Ye and Jiaxi Cui and Bin Zhu and Peng Jin and Junwu Zhang and Munan Ning and Li Yuan},&#xA;      year={2024},&#xA;      eprint={2401.15947},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@article{lin2023video,&#xA;  title={Video-LLaVA: Learning United Visual Representation by Alignment Before Projection},&#xA;  author={Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li},&#xA;  journal={arXiv preprint arXiv:2311.10122},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;✨ Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#PKU-YuanGroup/MoE-LLaVA&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=PKU-YuanGroup/MoE-LLaVA&amp;amp;type=Date&#34; alt=&#34;Star History&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🤝 Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/PKU-YuanGroup/MoE-LLaVA/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=PKU-YuanGroup/MoE-LLaVA&#34;&gt; &lt;/a&gt;</summary>
  </entry>
</feed>