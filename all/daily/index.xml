<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-25T01:54:12Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rese1f/StableVideo</title>
    <updated>2023-08-25T01:54:12Z</updated>
    <id>tag:github.com,2023-08-25:/rese1f/StableVideo</id>
    <link href="https://github.com/rese1f/StableVideo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICCV 2023] StableVideo: Text-driven Consistency-aware Diffusion Video Editing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;StableVideo&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.09592&#34;&gt;&lt;img src=&#34;http://img.shields.io/badge/cs.CV-arXiv%3A2308.09592-B31B1B.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;StableVideo: Text-driven Consistency-aware Diffusion Video Editing&lt;br&gt; &lt;a href=&#34;https://rese1f.github.io/&#34;&gt;Wenhao Chai&lt;/a&gt;, Xun Guo, Gaoang Wang, Yan Lu&lt;br&gt; ICCV 2023&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rese1f/StableVideo/assets/58205475/558555f1-711c-46f0-85bc-9c229ff1f511&#34;&gt;https://github.com/rese1f/StableVideo/assets/58205475/558555f1-711c-46f0-85bc-9c229ff1f511&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rese1f/StableVideo/assets/58205475/c152d0fa-16d3-4528-b9c2-ad2ec53944b9&#34;&gt;https://github.com/rese1f/StableVideo/assets/58205475/c152d0fa-16d3-4528-b9c2-ad2ec53944b9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rese1f/StableVideo/assets/58205475/0edbefdd-9b5f-4868-842c-9bf3156a54d3&#34;&gt;https://github.com/rese1f/StableVideo/assets/58205475/0edbefdd-9b5f-4868-842c-9bf3156a54d3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;VRAM requirement&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;VRAM (MiB)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;float32&lt;/td&gt; &#xA;   &lt;td&gt;29145&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;amp&lt;/td&gt; &#xA;   &lt;td&gt;23005&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;amp + cpu&lt;/td&gt; &#xA;   &lt;td&gt;17639&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;amp + cpu + xformers&lt;/td&gt; &#xA;   &lt;td&gt;14185&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cpu: use cpu cache as ControlNet, args: &lt;code&gt;save_memory&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;under default setting (&lt;em&gt;e.g.&lt;/em&gt; resolution, &lt;em&gt;etc.&lt;/em&gt;) in &lt;code&gt;app.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/rese1f/StableVideo.git&#xA;conda create -n stablevideo python=3.11&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;optional but recommanded&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install xformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download Pretrained Model&lt;/h2&gt; &#xA;&lt;p&gt;All models and detectors can be downloaded from ControlNet Hugging Face page at &lt;a href=&#34;https://huggingface.co/lllyasviel/ControlNet&#34;&gt;Download Link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Download example videos&lt;/h2&gt; &#xA;&lt;p&gt;Download the example atlas for car-turn, boat, libby, blackswa, bear, bicycle_tali, giraffe, kite-surf, lucia and motorbike at &lt;a href=&#34;https://www.dropbox.com/s/oiyhbiqdws2p6r1/nla_share.zip?dl=0&#34;&gt;Download Link&lt;/a&gt; shared by &lt;a href=&#34;https://github.com/omerbt/Text2LIVE&#34;&gt;Text2LIVE&lt;/a&gt; authors.&lt;/p&gt; &#xA;&lt;p&gt;You can also train on your own video following &lt;a href=&#34;https://github.com/ykasten/layered-neural-atlases&#34;&gt;NLA&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;And it will create a folder data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;StableVideo&#xA;‚îú‚îÄ‚îÄ ...&#xA;‚îú‚îÄ‚îÄ ckpt&#xA;‚îÇ   ‚îú‚îÄ‚îÄ cldm_v15.yaml&#xA;|   ‚îú‚îÄ‚îÄ dpt_hybrid-midas-501f0c75.pt&#xA;‚îÇ   ‚îú‚îÄ‚îÄ control_sd15_canny.pth&#xA;‚îÇ   ‚îî‚îÄ‚îÄ control_sd15_depth.pth&#xA;‚îú‚îÄ‚îÄ data&#xA;‚îÇ   ‚îî‚îÄ‚îÄ car-turn&#xA;‚îÇ       ‚îú‚îÄ‚îÄ checkpoint # NLA models are stored here&#xA;‚îÇ       ‚îú‚îÄ‚îÄ car-turn # contains video frames&#xA;‚îÇ       ‚îú‚îÄ‚îÄ ...&#xA;‚îÇ   ‚îú‚îÄ‚îÄ blackswan&#xA;‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;‚îî‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run and Play!&lt;/h2&gt; &#xA;&lt;p&gt;Run the following command to start. We provide some &lt;a href=&#34;https://raw.githubusercontent.com/rese1f/StableVideo/master/prompt_template.md&#34;&gt;prompt template&lt;/a&gt; to help you achieve better result.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the result &lt;code&gt;.mp4&lt;/code&gt; video and keyframe will be stored in the directory &lt;code&gt;./log&lt;/code&gt; after clicking &lt;code&gt;render&lt;/code&gt; button.&lt;/p&gt; &#xA;&lt;p&gt;You can also edit the mask region for the foreground atlas as follows.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/rese1f/StableVideo/assets/58205475/13e11c07-39ae-4d2d-8b66-f900d168ceff&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This implementation is built partly on &lt;a href=&#34;https://github.com/omerbt/Text2LIVE&#34;&gt;Text2LIVE&lt;/a&gt; and &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- ## Citation --&gt;</summary>
  </entry>
  <entry>
    <title>spcl/graph-of-thoughts</title>
    <updated>2023-08-25T01:54:12Z</updated>
    <id>tag:github.com,2023-08-25:/spcl/graph-of-thoughts</id>
    <link href="https://github.com/spcl/graph-of-thoughts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Implementation of &#34;Graph of Thoughts: Solving Elaborate Problems with Large Language Models&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Graph of Thoughts (GoT)&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/paper/pics/preview.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This is the official implementation of &lt;a href=&#34;https://arxiv.org/pdf/2308.09687.pdf&#34;&gt;Graph of Thoughts: Solving Elaborate Problems with Large Language Models&lt;/a&gt;.&lt;br&gt; This framework gives you the ability to solve complex problems by modeling them as a Graph of Operations (GoO), which is automatically executed with a Large Language Model (LLM) as the engine.&lt;br&gt; This framework is designed to be flexible and extensible, allowing you to not only solve problems using the new GoT approach, but also to implement GoOs resembling previous approaches like CoT or ToT.&lt;/p&gt; &#xA;&lt;h2&gt;Setup Guide&lt;/h2&gt; &#xA;&lt;p&gt;In order to use this framework, you need to have a working installation of Python 3.8 or newer.&lt;/p&gt; &#xA;&lt;h3&gt;Installing GoT&lt;/h3&gt; &#xA;&lt;p&gt;Before running either of the following two installation methods, make sure to activate your Python environment (if any) beforehand.&lt;br&gt; If you are a user and you just want to use &lt;code&gt;graph_of_thoughts&lt;/code&gt;, you can install it directly from PyPI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install graph_of_thoughts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are a developer and you want to modify the code, you can install it in editable mode from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/spcl/graph-of-thoughts.git&#xA;cd graph-of-thoughts&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configuring the LLM&lt;/h3&gt; &#xA;&lt;p&gt;In order to use the framework, you need to have access to an LLM. Please follow the instructions in the &lt;a href=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/graph_of_thoughts/controller/README.md&#34;&gt;Controller README&lt;/a&gt; to configure the LLM of your choice.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The following code snippet shows how to use the framework to solve the sorting problem for a list of 32 numbers using a CoT-like approach.&lt;br&gt; Make sure you have followed the &lt;a href=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/#setup-guide&#34;&gt;Setup Guide&lt;/a&gt; before running the code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from examples.sorting.sorting_032 import SortingPrompter, SortingParser, utils&#xA;from graph_of_thoughts import controller, operations&#xA;&#xA;# Problem input&#xA;&#xA;to_be_sorted = &#34;[0, 2, 6, 3, 8, 7, 1, 1, 6, 7, 7, 7, 7, 9, 3, 0, 1, 7, 9, 1, 3, 5, 1, 3, 6, 4, 5, 4, 7, 3, 5, 7]&#34;&#xA;&#xA;# Create the Graph of Operations&#xA;gop = operations.GraphOfOperations()&#xA;gop.append_operation(operations.Generate())&#xA;gop.append_operation(operations.Score(scoring_function=utils.num_errors))&#xA;gop.append_operation(operations.GroundTruth(utils.test_sorting))&#xA;&#xA;# Configure the Language Model (Assumes config.json is in the current directory with OpenAI API key)&#xA;lm = controller.ChatGPT(&#34;config.json&#34;, model_name=&#34;chatgpt&#34;)&#xA;&#xA;# Create the Controller&#xA;ctrl = controller.Controller(&#xA;  lm, &#xA;  gop, &#xA;  SortingPrompter(), &#xA;  SortingParser(),&#xA;  # The following dictionary is used to configure the inital thought state&#xA;  {&#xA;    &#34;original&#34;: to_be_sorted,&#xA;    &#34;current&#34;: &#34;&#34;,&#xA;    &#34;method&#34;: &#34;cot&#34;&#xA;  }&#xA;)&#xA;&#xA;# Run the Controller and generate the output graph&#xA;ctrl.run()&#xA;ctrl.output_graph(&#34;output_cot.json&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the more sophisticated GoT approach, you can use the following code snippet.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from examples.sorting.sorting_032 import SortingPrompter, SortingParser, got, utils&#xA;from graph_of_thoughts import controller, operations&#xA;&#xA;# Problem input&#xA;&#xA;to_be_sorted = &#34;[0, 2, 6, 3, 8, 7, 1, 1, 6, 7, 7, 7, 7, 9, 3, 0, 1, 7, 9, 1, 3, 5, 1, 3, 6, 4, 5, 4, 7, 3, 5, 7]&#34;&#xA;&#xA;# Retrieve the Graph of Operations&#xA;gop = got()&#xA;&#xA;# Configure the Language Model (Assumes config.json is in the current directory with OpenAI API key)&#xA;lm = controller.ChatGPT(&#34;config.json&#34;, model_name=&#34;chatgpt&#34;)&#xA;&#xA;# Create the Controller&#xA;ctrl = controller.Controller(&#xA;  lm, &#xA;  gop, &#xA;  SortingPrompter(), &#xA;  SortingParser(),&#xA;  # The following dictionary is used to configure the inital thought state&#xA;  {&#xA;    &#34;original&#34;: to_be_sorted,&#xA;    &#34;current&#34;: &#34;&#34;,&#xA;    &#34;phase&#34;: 0,&#xA;    &#34;method&#34;: &#34;got&#34;&#xA;  }&#xA;)&#xA;&#xA;# Run the Controller and generate the output graph&#xA;ctrl.run()&#xA;ctrl.output_graph(&#34;output_got.json&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can compare the two results by inspecting the output graphs &lt;code&gt;output_cot.json&lt;/code&gt; and &lt;code&gt;output_got.json&lt;/code&gt;.&lt;br&gt; The final thought states&#39; scores indicate the number of errors in the sorted list.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The paper gives a high-level overview of the framework and its components.&lt;br&gt; In order to understand the framework in more detail, you can read the documentation of the individual modules.&lt;br&gt; Especially the &lt;a href=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/graph_of_thoughts/controller/README.md&#34;&gt;Controller&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/graph_of_thoughts/operations/README.md&#34;&gt;Operations&lt;/a&gt; modules are important for understanding how to make the most out of the framework.&lt;br&gt; We took extra care to fully document the code, so that you can easily understand how it works and how to extend it.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/examples&#34;&gt;examples&lt;/a&gt; directory contains several examples of problems that can be solved using the framework, including the ones presented in the paper.&lt;br&gt; It is a great starting point for learning how to use the framework to solve real problems.&lt;br&gt; Each example contains a &lt;code&gt;README.md&lt;/code&gt; file with instructions on how to run it and play with it. The code is fully documented and should be easy to follow.&lt;/p&gt; &#xA;&lt;h2&gt;Paper Results&lt;/h2&gt; &#xA;&lt;p&gt;You can run the experiments from the paper by following the instructions in the &lt;a href=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;br&gt; However, if you just want to inspect and replot the results, you can use the &lt;a href=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/paper&#34;&gt;paper&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository valuable, please give it a star!&lt;br&gt; Got any questions or feedback? Feel free to reach out to &lt;a href=&#34;mailto:nils.blach@inf.ethz.ch&#34;&gt;nils.blach@inf.ethz.ch&lt;/a&gt; or open an issue.&lt;br&gt; Using this in your work? Please reference us using the provided citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{besta2023got,&#xA;  title = {{Graph of Thoughts: Solving Elaborate Problems with Large Language Models}},&#xA;  author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Micha{\l} and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},&#xA;  year = 2023,&#xA;  eprinttype = {arXiv},&#xA;  eprint = {2308.09687}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/seamless_communication</title>
    <updated>2023-08-25T01:54:12Z</updated>
    <id>tag:github.com,2023-08-25:/facebookresearch/seamless_communication</id>
    <link href="https://github.com/facebookresearch/seamless_communication" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Foundational Models for State-of-the-Art Speech and Text Translation&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/seamlessM4T.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;SeamlessM4T&lt;/h1&gt; &#xA;&lt;p&gt;SeamlessM4T is designed to provide high quality translation, allowing people from different linguistic communities to communicate effortlessly through speech and text.&lt;/p&gt; &#xA;&lt;p&gt;SeamlessM4T covers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üì• 101 languages for speech input.&lt;/li&gt; &#xA; &lt;li&gt;‚å®Ô∏è 96 Languages for text input/output.&lt;/li&gt; &#xA; &lt;li&gt;üó£Ô∏è 35 languages for speech output.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This unified model enables multiple tasks without relying on multiple separate models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speech-to-speech translation (S2ST)&lt;/li&gt; &#xA; &lt;li&gt;Speech-to-text translation (S2TT)&lt;/li&gt; &#xA; &lt;li&gt;Text-to-speech translation (T2ST)&lt;/li&gt; &#xA; &lt;li&gt;Text-to-text translation (T2TT)&lt;/li&gt; &#xA; &lt;li&gt;Automatic speech recognition (ASR)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/blog/seamless-m4t&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf&#34;&gt;Paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://seamless.metademolab.com/&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/facebook/seamless_m4t&#34;&gt;ü§ó Hugging Face space&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A temporary extra requirement for fairseq2 is &lt;a href=&#34;https://github.com/libsndfile/libsndfile&#34;&gt;libsndfile&lt;/a&gt;. From &lt;a href=&#34;https://docs.conda.io/en/latest/&#34;&gt;Conda&lt;/a&gt; environment it can be installed via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install -y -c conda-forge libsndfile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;At this point fairseq2 has a confirmed support only for Linux and macOS. Pre-built packages are only available for Linux (macOS is planned).&lt;/p&gt; &#xA;&lt;h2&gt;Running inference&lt;/h2&gt; &#xA;&lt;p&gt;Here‚Äôs an example of using the CLI from the root directory to run inference.&lt;/p&gt; &#xA;&lt;p&gt;S2ST task:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;m4t_predict &amp;lt;path_to_input_audio&amp;gt; s2st &amp;lt;tgt_lang&amp;gt; --output_path &amp;lt;path_to_save_audio&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;T2TT task:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;m4t_predict &amp;lt;input_text&amp;gt; t2tt &amp;lt;tgt_lang&amp;gt; --src_lang &amp;lt;src_lang&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/scripts/m4t/predict&#34;&gt;inference README&lt;/a&gt; for detailed instruction on how to run inference.&lt;/p&gt; &#xA;&lt;h1&gt;Libraries&lt;/h1&gt; &#xA;&lt;p&gt;Seamless Communication depends on 3 libraries developed by Meta.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/facebookresearch/fairseq2&#34;&gt;fairseq2&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;fairseq2 is our next-generation open-source library of sequence modeling components that provides researchers and developers with building blocks for machine translation, language modeling, and other sequence generation tasks. All SeamlessM4T models in this repository are powered by fairseq2.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/facebookresearch/SONAR&#34;&gt;SONAR and BLASER 2.0&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;SONAR, Sentence-level multimOdal and laNguage-Agnostic Representations is a new multilingual and -modal sentence embedding space which outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim and xsim++ multilingual similarity search tasks. SONAR provides text and speech encoders for many languages. SeamlessAlign was mined based on SONAR embeddings.&lt;/p&gt; &#xA;&lt;p&gt;BLASER 2.0 is our latest model-based evaluation metric for multimodal translation. It is an extension of BLASER, supporting both speech and text. It operates directly on the source signal, and as such, does not require any intermediate ASR system like ASR-BLEU. As in the first version, BLASER 2.0 leverages the similarity between input and output sentence embeddings. SONAR is the underlying embedding space for BLASER 2.0. Scripts to run evaluation with BLASER 2.0 can be found in the &lt;a href=&#34;https://github.com/facebookresearch/SONAR&#34;&gt;SONAR repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/facebookresearch/stopes&#34;&gt;stopes&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;As part of the seamless communication project, we&#39;ve extended the stopes library. Version 1 provided a text-to-text mining tool to build training dataset for translation models. Version 2 has been extended thanks to SONAR, to support tasks around training large speech translation models. In particular, we provide tools to read/write the fairseq audiozip datasets and a new mining pipeline that can do speech-to-speech, text-to-speech, speech-to-text and text-to-text mining, all based on the new SONAR embedding space.&lt;/p&gt; &#xA;&lt;h1&gt;Resources and usage&lt;/h1&gt; &#xA;&lt;h2&gt;SeamlessM4T models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th&gt;#params&lt;/th&gt; &#xA;   &lt;th&gt;checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;metrics&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeamlessM4T-Large&lt;/td&gt; &#xA;   &lt;td&gt;2.3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-large&#34;&gt;ü§ó Model card&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-large/resolve/main/multitask_unity_large.pt&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/seamlessM4T/metrics/seamlessM4T_large.zip&#34;&gt;metrics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeamlessM4T-Medium&lt;/td&gt; &#xA;   &lt;td&gt;1.2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-medium&#34;&gt;ü§ó Model card&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-medium/resolve/main/multitask_unity_medium.pt&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/seamlessM4T/metrics/seamlessM4T_medium.zip&#34;&gt;metrics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We provide the extensive evaluation results of seamlessM4T-Large and SeamlessM4T-Medium reported in the paper (as averages) in the &lt;code&gt;metrics&lt;/code&gt; files above.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluating SeamlessM4T models&lt;/h2&gt; &#xA;&lt;p&gt;To reproduce our results, or to evaluate using the same metrics over your own test sets, please check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/m4t/eval_README.md&#34;&gt;README here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Finetuning SeamlessM4T models&lt;/h2&gt; &#xA;&lt;p&gt;Please check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/scripts/m4t/finetune/README.md&#34;&gt;README here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Converting raw audio to units&lt;/h2&gt; &#xA;&lt;p&gt;Please check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/scripts/m4t/audio_to_units/README.md&#34;&gt;README here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;On-device models&lt;/h2&gt; &#xA;&lt;p&gt;Apart from Seamless-M4T large (2.3B) and medium (1.2B) models, we are also releasing a small model (281M) targeted for on-device inference. To learn more about the usage and model details check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/m4t/on_device_README.md&#34;&gt;README here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;SeamlessAlign mined dataset&lt;/h2&gt; &#xA;&lt;p&gt;We open-source the metadata to SeamlessAlign, the largest open dataset for multimodal translation, totaling 270k+ hours of aligned Speech and Text data. The dataset can be rebuilt by the community based on the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/m4t/seamless_align_README.md&#34;&gt;SeamlessAlign readme&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use SeamlessM4T in your work or any models/datasets/artifacts published in SeamlessM4T, please cite :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{seamlessm4t2023,&#xA;  title={SeamlessM4T‚ÄîMassively Multilingual \&amp;amp; Multimodal Machine Translation},&#xA;  author={{Seamless Communication}, Lo\&#34;{i}c Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye,  Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-juss\`{a} \footnotemark[3], Onur \,{C}elebi,Maha Elbayad,Cynthia Gao, Francisco Guzm\&#39;an, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang},&#xA;  journal={ArXiv},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;seamless_communication is CC-BY-NC 4.0 licensed, as found in LICENSE file&lt;/p&gt;</summary>
  </entry>
</feed>