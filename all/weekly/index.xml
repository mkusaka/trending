<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-27T01:49:12Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>modelscope/facechain</title>
    <updated>2023-08-27T01:49:12Z</updated>
    <id>tag:github.com,2023-08-27:/modelscope/facechain</id>
    <link href="https://github.com/modelscope/facechain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FaceChain is a deep-learning toolchain for generating your Digital-Twin.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif&#34; width=&#34;400&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;h1&gt;FaceChain&lt;/h1&gt; &#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;å¦‚æœæ‚¨ç†Ÿæ‚‰ä¸­æ–‡ï¼Œå¯ä»¥é˜…è¯»&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/README_ZH.md&#34;&gt;ä¸­æ–‡ç‰ˆæœ¬çš„README&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;p&gt;FaceChain is a deep-learning toolchain for generating your Digital-Twin. With a minimum of 1 portrait-photo, you can create a Digital-Twin of your own and start generating personal portraits in different settings (multiple styles now supported!). You may train your Digital-Twin model and generate photos via FaceChain&#39;s Python scripts, or via the familiar Gradio interface. FaceChain is powered by &lt;a href=&#34;https://github.com/modelscope/modelscope&#34;&gt;ModelScope&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ModelScope Studio &lt;a href=&#34;https://modelscope.cn/studios/CVstudio/cv_human_portrait/summary&#34;&gt;ğŸ¤–&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&amp;nbsp; ï½œ HuggingFace Space &lt;a href=&#34;https://huggingface.co/spaces/modelscope/FaceChain&#34;&gt;ğŸ¤—&lt;/a&gt;&amp;nbsp; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/git_cover.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;HuggingFace Space is available now! You can experience FaceChain directly with &lt;a href=&#34;https://huggingface.co/spaces/modelscope/FaceChain&#34;&gt;ğŸ¤—&lt;/a&gt; (August 25th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Add awesome prompts! Refer to: &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/awesome-prompts-facechain.txt&#34;&gt;awesome-prompts-facechain&lt;/a&gt; (August 18th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Support a series of new style models in a plug-and-play fashion. Refer to: &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/#Features&#34;&gt;Features&lt;/a&gt; (August 16th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Support customizable prompts. Refer to: &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/#Features&#34;&gt;Features&lt;/a&gt; (August 16th, 2023 UTC)&lt;/li&gt; &#xA; &lt;li&gt;Colab notebook is available now! You can experience FaceChain directly with &lt;a href=&#34;https://colab.research.google.com/drive/1cUhnVXseqD2EJiotZk3k7GsfQK9_yJu_?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;. (August 15th, 2023 UTC)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;To-Do List&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support existing style models (such as those on Civitai) in a plug-an-play fashion. --on-going&lt;/li&gt; &#xA; &lt;li&gt;Support customizable prompts (try on different outfits etc.) --on-going&lt;/li&gt; &#xA; &lt;li&gt;Support customizable poses, with controlnet or composer&lt;/li&gt; &#xA; &lt;li&gt;Support more beauty-retouch effects&lt;/li&gt; &#xA; &lt;li&gt;Support latest foundation models such as SDXL&lt;/li&gt; &#xA; &lt;li&gt;Provide WebUI compatibility&lt;/li&gt; &#xA; &lt;li&gt;Support template images inpainting&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support a series of new style models in a plug-and-play fashion &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Description &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Allow users to select different style models for training distinct types of Digital-Twins.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Installation &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/#installation-guide&#34;&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Usage &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Select &#34;å‡¤å† éœå¸”(Chinese traditional gorgeous suit)&#34; on the &lt;code&gt;inference&lt;/code&gt; tab and change the prompt as you want.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Exampled outcomes &lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/style_lora_xiapei.jpg&#34; alt=&#34;image&#34;&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Reference &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.liblibai.com/modelinfo/f746450340a3a932c99be55c1a82d20c&#34;&gt;xiapei lora model&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;For more LoRA styles, refer to &lt;a href=&#34;https://civitai.com/&#34;&gt;Civitai&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support customizable prompts &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Description &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Allow users to achieve various portrait styles with customized prompts.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Installation &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/#installation-guide&#34;&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Usage &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Edit the prompt on the &lt;code&gt;inference&lt;/code&gt; tab as you want.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Exampled outcomes &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;(prompt: The lord of the rings, ELF, Arwen Undomiel, beautiful, upper_body, best quality, Professional) &lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/prompt_elf_lord_of_rings.jpg&#34; alt=&#34;image&#34;&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Compatibility Verification&lt;/h2&gt; &#xA;&lt;p&gt;We have verified e2e execution on the following environment:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python: py3.8, py3.10&lt;/li&gt; &#xA; &lt;li&gt;pytorch: torch2.0.0, torch2.0.1&lt;/li&gt; &#xA; &lt;li&gt;tensorflow: 2.8.0, tensorflow-cpu&lt;/li&gt; &#xA; &lt;li&gt;CUDA: 11.7&lt;/li&gt; &#xA; &lt;li&gt;CUDNN: 8+&lt;/li&gt; &#xA; &lt;li&gt;OS: Ubuntu 20.04, CentOS 7.9&lt;/li&gt; &#xA; &lt;li&gt;GPU: Nvidia-A10 24G&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resource Requirement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPU: About 19G&lt;/li&gt; &#xA; &lt;li&gt;Disk: About 50GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation Guide&lt;/h2&gt; &#xA;&lt;p&gt;The following installation methods are supported:&lt;/p&gt; &#xA;&lt;h3&gt;1. ModelScope notebookã€recommendedã€‘&lt;/h3&gt; &#xA;&lt;p&gt;The ModelScope Notebook offers a free-tier that allows ModelScope user to run the FaceChain application with minimum setup, refer to &lt;a href=&#34;https://modelscope.cn/my/mynotebook/preset&#34;&gt;ModelScope Notebook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Step1: æˆ‘çš„notebook -&amp;gt; PAI-DSW -&amp;gt; GPUç¯å¢ƒ&#xA;&#xA;# Step2: Entry the Notebook cellï¼Œclone FaceChain from github:&#xA;!GIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/modelscope/facechain.git --depth 1&#xA;&#xA;# Step3: Change the working directory to facechain:&#xA;import os&#xA;os.chdir(&#39;/mnt/workspace/facechain&#39;)    # You may change to your own path&#xA;print(os.getcwd())&#xA;&#xA;!pip3 install gradio&#xA;!python3 app.py&#xA;&#xA;&#xA;# Step4: click &#34;public URL&#34; or &#34;local URL&#34;, upload your images to &#xA;# train your own model and then generate your digital twin.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you may also purchase a &lt;a href=&#34;https://www.aliyun.com/activity/bigdata/pai/dsw&#34;&gt;PAI-DSW&lt;/a&gt; instance (using A10 resource), with the option of ModelScope image to run FaceChain following similar steps.&lt;/p&gt; &#xA;&lt;h3&gt;2. Docker&lt;/h3&gt; &#xA;&lt;p&gt;If you are familiar with using docker, we recommend to use this way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Step1: Prepare the environment with GPU on local or cloud, we recommend to use Alibaba Cloud ECS, refer to: https://www.aliyun.com/product/ecs&#xA;&#xA;# Step2: Download the docker image (for installing docker engine, refer to https://docs.docker.com/engine/install/ï¼‰&#xA;docker pull registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0&#xA;&#xA;# Step3: run the docker container&#xA;docker run -it --name facechain -p 7860:7860 --gpus all registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0 /bin/bash&#xA;(Note: you may need to install the nvidia-container-runtime, refer to https://github.com/NVIDIA/nvidia-container-runtime)&#xA;&#xA;# Step4: Install the gradio in the docker container:&#xA;pip3 install gradio&#xA;&#xA;# Step5 clone facechain from github&#xA;GIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/modelscope/facechain.git --depth 1&#xA;cd facechain&#xA;python3 app.py&#xA;# Note: FaceChain currently assume single-GPU, if your environment has multiple GPU, please use the following instead:&#xA;# CUDA_VISIBLE_DEVICES=0 python3 app.py&#xA;&#xA;# Step6&#xA;Run the app server: click &#34;public URL&#34; --&amp;gt; in the form of: https://xxx.gradio.live&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Conda Virtual Environment&lt;/h3&gt; &#xA;&lt;p&gt;Use the conda virtual environment, and refer to &lt;a href=&#34;https://docs.anaconda.com/anaconda/install/&#34;&gt;Anaconda&lt;/a&gt; to manage your dependencies. After installation, execute the following commands: (Note: mmcv has strict environment requirements and might not be compatible in some cases. It&#39;s recommended to use Docker.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n facechain python=3.8    # Verified environments: 3.8 and 3.10&#xA;conda activate facechain&#xA;&#xA;GIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/modelscope/facechain.git --depth 1&#xA;cd facechain&#xA;&#xA;pip3 install -r requirements.txt&#xA;pip3 install -U openmim &#xA;mim install mmcv-full==1.7.0&#xA;&#xA;# Navigate to the facechain directory and run:&#xA;python3 app.py&#xA;# Note: FaceChain currently assume single-GPU, if your environment has multiple GPU, please use the following instead:&#xA;# CUDA_VISIBLE_DEVICES=0 python3 app.py&#xA;&#xA;# Finally, click on the URL generated in the log to access the web page.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: After the app service is successfully launched, go to the URL in the log, enter the &#34;Image Customization&#34; tab, click &#34;Select Image to Upload&#34;, and choose at least one image with a face. Then, click &#34;Start Training&#34; to begin model training. After the training is completed, there will be corresponding displays in the log. Afterwards, switch to the &#34;Image Experience&#34; tab and click &#34;Start Inference&#34; to generate your own digital image.&lt;/p&gt; &#xA;&lt;h3&gt;4. Colab notebook&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Colab&lt;/th&gt; &#xA;   &lt;th&gt;Info&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1cUhnVXseqD2EJiotZk3k7GsfQK9_yJu_?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FaceChain Installation on Colab&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Script Execution&lt;/h1&gt; &#xA;&lt;p&gt;FaceChain supports direct training and inference in the python environment. Run the following command in the cloned folder to start training:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;PYTHONPATH=. sh train_lora.sh &#34;ly261666/cv_portrait_model&#34; &#34;v2.0&#34; &#34;film/film&#34; &#34;./imgs&#34; &#34;./processed&#34; &#34;./output&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Parameters description:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;ly261666/cv_portrait_model: The stable diffusion base model of the ModelScope model hub, which will be used for training, no need to be changed.&#xA;v2.0: The version number of this base model, no need to be changed&#xA;film/film: This base model may contains multiple subdirectories of different styles, currently we use film/film, no need to be changed&#xA;./imgs: This parameter needs to be replaced with the actual value. It means a local file directory that contains the original photos used for training and generation&#xA;./processed: The folder of the processed images after preprocessing, this parameter needs to be passed the same value in inference, no need to be changed&#xA;./output: The folder where the model weights stored after training, no need to be changed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait for 5-20 minutes to complete the training. Users can also adjust other training hyperparameters. The hyperparameters supported by training can be viewed in the file of &lt;code&gt;train_lora.sh&lt;/code&gt;, or the complete hyperparameter list in &lt;code&gt;facechain/train_text_to_image_lora.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When inferring, please edit the code in run_inference.py:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fill in the folder of the images after preprocessing above, it should be the same as during training&#xA;processed_dir = &#39;./processed&#39;&#xA;# The number of images to generate in inference&#xA;num_generate = 5&#xA;# The stable diffusion base model used in training, no need to be changed&#xA;base_model = &#39;ly261666/cv_portrait_model&#39;&#xA;# The version number of this base model, no need to be changed&#xA;revision = &#39;v2.0&#39;&#xA;# This base model may contains multiple subdirectories of different styles, currently we use film/film, no need to be changed&#xA;base_model_sub_dir = &#39;film/film&#39;&#xA;# The folder where the model weights stored after training, it must be the same as during training&#xA;train_output_dir = &#39;./output&#39;&#xA;# Specify a folder to save the generated images, this parameter can be modified as needed&#xA;output_dir = &#39;./generated&#39;&#xA;# Use Chinese style model, default False&#xA;use_style = False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python run_inference.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find the generated personal digital image photos in the &lt;code&gt;output_dir&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Algorithm Introduction&lt;/h1&gt; &#xA;&lt;h2&gt;Architectural Overview&lt;/h2&gt; &#xA;&lt;p&gt;The ability of the personal portrait generation evolves around the text-to-image capability of Stable Diffusion model. We consider the main factors that affect the generation effect of personal portraits: portrait style information and user character information. For this, we use the style LoRA model trained offline and the face LoRA model trained online to learn the above information. LoRA is a fine-tuning model with fewer trainable parameters. In Stable Diffusion, the information of the input image can be injected into the LoRA model by the way of text generation image training with a small amount of input image. Therefore, the ability of the personal portrait model is divided into training and inference stages. The training stage generates image and text label data for fine-tuning the Stable Diffusion model, and obtains the face LoRA model. The inference stage generates personal portrait images based on the face LoRA model and style LoRA model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/framework_eng.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;Input: User-uploaded images that contain clear face areas&lt;/p&gt; &#xA;&lt;p&gt;Output: Face LoRA model&lt;/p&gt; &#xA;&lt;p&gt;Description: First, we process the user-uploaded images using an image rotation model based on orientation judgment and a face refinement rotation method based on face detection and keypoint models, and obtain images containing forward faces. Next, we use a human body parsing model and a human portrait beautification model to obtain high-quality face training images. Afterwards, we use a face attribute model and a text annotation model, combined with tag post-processing methods, to generate fine-grained labels for training images. Finally, we use the above images and label data to fine-tune the Stable Diffusion model to obtain the face LoRA model.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Input: User-uploaded images in the training phase, preset input prompt words for generating personal portraits&lt;/p&gt; &#xA;&lt;p&gt;Output: Personal portrait image&lt;/p&gt; &#xA;&lt;p&gt;Description: First, we fuse the weights of the face LoRA model and style LoRA model into the Stable Diffusion model. Next, we use the text generation image function of the Stable Diffusion model to preliminarily generate personal portrait images based on the preset input prompt words. Then we further improve the face details of the above portrait image using the face fusion model. The template face used for fusion is selected from the training images through the face quality evaluation model. Finally, we use the face recognition model to calculate the similarity between the generated portrait image and the template face, and use this to sort the portrait images, and output the personal portrait image that ranks first as the final output result.&lt;/p&gt; &#xA;&lt;h2&gt;Model List&lt;/h2&gt; &#xA;&lt;p&gt;The models used in FaceChain:&lt;/p&gt; &#xA;&lt;p&gt;[1] Face detection model DamoFDï¼š&lt;a href=&#34;https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd&#34;&gt;https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2] Image rotating model, offered in the ModelScope studio&lt;/p&gt; &#xA;&lt;p&gt;[3] Human parsing model M2FPï¼š&lt;a href=&#34;https://modelscope.cn/models/damo/cv_resnet101_image-multiple-human-parsing&#34;&gt;https://modelscope.cn/models/damo/cv_resnet101_image-multiple-human-parsing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[4] Skin retouching model ABPNï¼š&lt;a href=&#34;https://modelscope.cn/models/damo/cv_unet_skin-retouching&#34;&gt;https://modelscope.cn/models/damo/cv_unet_skin-retouching&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[5] Face attribute recognition model FairFaceï¼š&lt;a href=&#34;https://modelscope.cn/models/damo/cv_resnet34_face-attribute-recognition_fairface&#34;&gt;https://modelscope.cn/models/damo/cv_resnet34_face-attribute-recognition_fairface&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[6] DeepDanbooru modelï¼š&lt;a href=&#34;https://github.com/KichangKim/DeepDanbooru&#34;&gt;https://github.com/KichangKim/DeepDanbooru&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[7] Face quality assessment FQAï¼š&lt;a href=&#34;https://modelscope.cn/models/damo/cv_manual_face-quality-assessment_fqa&#34;&gt;https://modelscope.cn/models/damo/cv_manual_face-quality-assessment_fqa&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[8] Face fusion modelï¼š&lt;a href=&#34;https://modelscope.cn/models/damo/cv_unet-image-face-fusion_damo&#34;&gt;https://modelscope.cn/models/damo/cv_unet-image-face-fusion_damo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[9] Face recognition model RTSï¼š&lt;a href=&#34;https://modelscope.cn/models/damo/cv_ir_face-recognition-ood_rts&#34;&gt;https://modelscope.cn/models/damo/cv_ir_face-recognition-ood_rts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;More Information&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope/&#34;&gt;ModelScope library&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;â€‹ ModelScope Library provides the foundation for building the model-ecosystem of ModelScope, including the interface and implementation to integrate various models into ModelScope.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88&#34;&gt;Contribute models to ModelScope&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://github.com/modelscope/modelscope/raw/master/LICENSE&#34;&gt;Apache License (Version 2.0)&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pedroslopez/whatsapp-web.js</title>
    <updated>2023-08-27T01:49:12Z</updated>
    <id>tag:github.com,2023-08-27:/pedroslopez/whatsapp-web.js</id>
    <link href="https://github.com/pedroslopez/whatsapp-web.js" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A WhatsApp client library for NodeJS that connects through the WhatsApp Web browser app&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://www.npmjs.com/package/whatsapp-web.js&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/whatsapp-web.js.svg?sanitize=true&#34; alt=&#34;npm&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://depfu.com/github/pedroslopez/whatsapp-web.js?project_id=9765&#34;&gt;&lt;img src=&#34;https://badges.depfu.com/badges/4a65a0de96ece65fdf39e294e0c8dcba/overview.svg?sanitize=true&#34; alt=&#34;Depfu&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/WhatsApp_Web-2.2333.11-brightgreen.svg?sanitize=true&#34; alt=&#34;WhatsApp_Web 2.2333.11&#34;&gt; &lt;a href=&#34;https://discord.gg/H7DqQs4&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/698610475432411196.svg?logo=discord&#34; alt=&#34;Discord Chat&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;whatsapp-web.js&lt;/h1&gt; &#xA;&lt;p&gt;A WhatsApp API client that connects through the WhatsApp Web browser app&lt;/p&gt; &#xA;&lt;p&gt;It uses Puppeteer to run a real instance of Whatsapp Web to avoid getting blocked.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; I can&#39;t guarantee you will not be blocked by using this method, although it has worked for me. WhatsApp does not allow bots or unofficial clients on their platform, so this shouldn&#39;t be considered totally safe.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wwebjs.dev/guide&#34;&gt;Guide / Getting Started&lt;/a&gt; &lt;em&gt;(work in progress)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.wwebjs.dev/&#34;&gt;Reference documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pedroslopez/whatsapp-web.js&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://npmjs.org/package/whatsapp-web.js&#34;&gt;npm&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The module is now available on npm! &lt;code&gt;npm i whatsapp-web.js&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please note that Node v12+ is required.&lt;/p&gt; &#xA;&lt;h2&gt;Example usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;const { Client } = require(&#39;whatsapp-web.js&#39;);&#xA;&#xA;const client = new Client();&#xA;&#xA;client.on(&#39;qr&#39;, (qr) =&amp;gt; {&#xA;    // Generate and scan this code with your phone&#xA;    console.log(&#39;QR RECEIVED&#39;, qr);&#xA;});&#xA;&#xA;client.on(&#39;ready&#39;, () =&amp;gt; {&#xA;    console.log(&#39;Client is ready!&#39;);&#xA;});&#xA;&#xA;client.on(&#39;message&#39;, msg =&amp;gt; {&#xA;    if (msg.body == &#39;!ping&#39;) {&#xA;        msg.reply(&#39;pong&#39;);&#xA;    }&#xA;});&#xA;&#xA;client.initialize();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Take a look at &lt;a href=&#34;https://github.com/pedroslopez/whatsapp-web.js/raw/master/example.js&#34;&gt;example.js&lt;/a&gt; for another example with more use cases.&lt;/p&gt; &#xA;&lt;p&gt;For more information on saving and restoring sessions, check out the available &lt;a href=&#34;https://wwebjs.dev/guide/authentication.html&#34;&gt;Authentication Strategies&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Supported features&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Feature&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi Device&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Send messages&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Receive messages&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Send media (images/audio/documents)&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Send media (video)&lt;/td&gt; &#xA;   &lt;td&gt;âœ… &lt;a href=&#34;https://wwebjs.dev/guide/handling-attachments.html#caveat-for-sending-videos-and-gifs&#34;&gt;(requires google chrome)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Send stickers&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Receive media (images/audio/video/documents)&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Send contact cards&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Send location&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Send buttons&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Send lists&lt;/td&gt; &#xA;   &lt;td&gt;âœ… (business accounts not supported)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Receive location&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Message replies&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Join groups by invite&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Get invite for group&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Modify group info (subject, description)&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Modify group settings (send messages, edit info)&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Add group participants&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kick group participants&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Promote/demote group participants&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mention users&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mute/unmute chats&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Block/unblock contacts&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Get contact info&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Get profile pictures&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Set user status message&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;React to messages&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Something missing? Make an issue and let us know!&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests are welcome! If you see something you&#39;d like to add, please do. For drastic changes, please open an issue first.&lt;/p&gt; &#xA;&lt;h2&gt;Supporting the project&lt;/h2&gt; &#xA;&lt;p&gt;You can support the maintainer of this project through the links below&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sponsors/pedroslopez&#34;&gt;Support via GitHub Sponsors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.paypal.me/psla/&#34;&gt;Support via PayPal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://m.do.co/c/73f906a36ed4&#34;&gt;Sign up for DigitalOcean&lt;/a&gt; and get $200 in credit when you sign up (Referral)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project is not affiliated, associated, authorized, endorsed by, or in any way officially connected with WhatsApp or any of its subsidiaries or its affiliates. The official WhatsApp website can be found at &lt;a href=&#34;https://whatsapp.com&#34;&gt;https://whatsapp.com&lt;/a&gt;. &#34;WhatsApp&#34; as well as related names, marks, emblems and images are registered trademarks of their respective owners.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2019 Pedro S Lopez&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this project except in compliance with the License. You may obtain a copy of the License at &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LorisYounger/VPet</title>
    <updated>2023-08-27T01:49:12Z</updated>
    <id>tag:github.com,2023-08-27:/LorisYounger/VPet</id>
    <link href="https://github.com/LorisYounger/VPet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;è™šæ‹Ÿæ¡Œå® æ¨¡æ‹Ÿå™¨ ä¸€ä¸ªå¼€æºçš„æ¡Œå® è½¯ä»¶, å¯ä»¥å†…ç½®åˆ°ä»»ä½•WPFåº”ç”¨ç¨‹åº&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VPet-Simulator&lt;/h1&gt; &#xA;&lt;p&gt;ç®€ä½“ä¸­æ–‡ | &lt;a href=&#34;https://raw.githubusercontent.com/LorisYounger/VPet/main/README_en.md&#34;&gt;English&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;è™šæ‹Ÿæ¡Œå® æ¨¡æ‹Ÿå™¨ ä¸€ä¸ªå¼€æºçš„æ¡Œå® è½¯ä»¶, å¯ä»¥å†…ç½®åˆ°ä»»ä½•WPFåº”ç”¨ç¨‹åº&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LorisYounger/VPet/main/README.assets/%E4%B8%BB%E5%9B%BE.png&#34; alt=&#34;ä¸»å›¾&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;è·å–è™šæ‹Ÿæ¡Œå® æ¨¡æ‹Ÿå™¨ &lt;a href=&#34;https://store.steampowered.com/app/1920960/VPet&#34;&gt;OnSteam(å…è´¹)&lt;/a&gt; æˆ– é€šè¿‡&lt;a href=&#34;https://www.nuget.org/packages/VPet-Simulator.Core&#34;&gt;Nuget&lt;/a&gt;å†…ç½®åˆ°ä½ çš„WPFåº”ç”¨ç¨‹åº&lt;/p&gt; &#xA;&lt;h2&gt;è™šæ‹Ÿæ¡Œå® æ¨¡æ‹Ÿå™¨ è¯¦ç»†ä»‹ç»&lt;/h2&gt; &#xA;&lt;p&gt;è™šæ‹Ÿæ¡Œå® æ¨¡æ‹Ÿå™¨æ˜¯ä¸€æ¬¾æ¡Œå® è½¯ä»¶,æ”¯æŒå„ç§äº’åŠ¨æŠ•å–‚ç­‰. å¼€æºå…è´¹å¹¶ä¸”æ”¯æŒåˆ›æ„å·¥åŠ.&lt;/p&gt; &#xA;&lt;p&gt;åæ­£å…è´¹ä¸ºå•¥ä¸è¯•è¯•å‘¢(&lt;/p&gt; &#xA;&lt;p&gt;è¯¥æ¸¸æˆä¸º &lt;a href=&#34;https://store.steampowered.com/app/1352140/_/&#34;&gt;è™šæ‹Ÿä¸»æ’­æ¨¡æ‹Ÿå™¨&lt;/a&gt; å†…ç½®æ¡Œå® (æ•™ç¨‹)ç¨‹åºç‹¬ç«‹è€Œæ¥, å¦‚æœå–œæ¬¢çš„è¯æ¬¢è¿æ·»åŠ  &lt;a href=&#34;https://store.steampowered.com/app/1352140/_/&#34;&gt;è™šæ‹Ÿä¸»æ’­æ¨¡æ‹Ÿå™¨&lt;/a&gt; è‡³æ„¿æœ›å•&lt;/p&gt; &#xA;&lt;h3&gt;è¶…å¤šçš„äº’åŠ¨å’ŒåŠ¨ç”»&lt;/h3&gt; &#xA;&lt;p&gt;å¤šè¾¾ 32(ç§) * 4(çŠ¶æ€) * 3(ç±»å‹) ç§åŠ¨ç”», &lt;em&gt;æ³¨:éƒ¨åˆ†ç§ç±»æ²¡æœ‰ç”Ÿç—…çŠ¶æ€æˆ–å¾ªç¯ç­‰å†…å®¹,å®é™…åŠ¨ç”»æ•°é‡ä¼šåå°‘&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;ä¸€äº›åŠ¨ç”»ä¾‹å­:&lt;/h4&gt; &#xA;&lt;h5&gt;æ‘¸å¤´&lt;/h5&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LorisYounger/VPet/main/README.assets/ss0.gif&#34; alt=&#34;ss0&#34;&gt;&lt;/p&gt; &#xA;&lt;h5&gt;æèµ·&lt;/h5&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LorisYounger/VPet/main/README.assets/ss4.gif&#34; alt=&#34;ss4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LorisYounger/VPet/main/README.assets/ss8.gif&#34; alt=&#34;ss4&#34;&gt;&lt;/p&gt; &#xA;&lt;h5&gt;çˆ¬å¢™&lt;/h5&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LorisYounger/VPet/main/README.assets/ss7.gif&#34; alt=&#34;ss7&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;å…è´¹&lt;/h3&gt; &#xA;&lt;p&gt;è¯¥æ¸¸æˆå®Œå…¨å…è´¹! åæ­£ä¸è¦é’±,è¯•è¯•ä¸è¦ç´§(&lt;br&gt; è¯¥æ¸¸æˆä¸»è¦ç›®çš„æ˜¯å®£ä¼ ä¸‹ &lt;a href=&#34;https://store.steampowered.com/app/1352140/_/&#34;&gt;è™šæ‹Ÿä¸»æ’­æ¨¡æ‹Ÿå™¨&lt;/a&gt;, æ¸¸æˆä¸­Qç‰ˆäººç‰©ä¸ºè™šæ‹Ÿä¸»æ’­æ¨¡æ‹Ÿå™¨çš„ä¸»äººå…¬.&lt;/p&gt; &#xA;&lt;h3&gt;å¼€æº&lt;/h3&gt; &#xA;&lt;p&gt;è¯¥æ¸¸æˆåœ¨githubä¸Šå¼€æº, æ¬¢è¿æå‡ºè‡ªå·±çš„æƒ³æ³•,åˆ›æ„æˆ–è€…å‚ä¸å¼€å‘!&lt;br&gt; æ‚¨è¿˜å¯ä»¥ä¿®æ”¹ä»£ç æ¥åˆ¶ä½œè‡ªå·±ä¸“å±çš„æ¡Œå® !(è™½ç„¶è¯´å¤§éƒ¨åˆ†å†…å®¹éƒ½æ”¯æŒåˆ›æ„å·¥åŠ,ä¸éœ€è¦ä¿®æ”¹ä»£ç )&lt;br&gt; é¡¹ç›®åœ°å€: &lt;a href=&#34;https://github.com/LorisYounger/VPet&#34;&gt;https://github.com/LorisYounger/VPet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;æ”¯æŒåˆ›æ„å·¥åŠ&lt;/h3&gt; &#xA;&lt;p&gt;è¯¥æ¸¸æˆæ”¯æŒåˆ›æ„å·¥åŠ,æ‚¨å¯ä»¥åˆ¶ä½œåˆ«çš„äººç‰©æ¡Œå® åŠ¨ç”»æˆ–è€…äº’åŠ¨,å¹¶ä¸Šä¼ è‡³åˆ›æ„å·¥åŠåˆ†äº«ç»™æ›´å¤šäººä½¿ç”¨.&lt;/p&gt; &#xA;&lt;p&gt;åˆ›æ„å·¥åŠæ”¯æŒæ·»åŠ /ä¿®æ”¹ä»¥ä¸‹å†…å®¹&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ¡Œå® åŠ¨ç”»&lt;/li&gt; &#xA; &lt;li&gt;ç‰©å“/é£Ÿç‰©/é¥®æ–™ç­‰&lt;/li&gt; &#xA; &lt;li&gt;è‡ªå®šä¹‰æ¡Œå® å·¥ä½œ&lt;/li&gt; &#xA; &lt;li&gt;è¯´è¯æ–‡æœ¬&lt;/li&gt; &#xA; &lt;li&gt;ä¸»é¢˜&lt;/li&gt; &#xA; &lt;li&gt;ä»£ç æ’ä»¶ - é€šè¿‡ç¼–å†™ä»£ç ç»™æ¡Œå® æ·»åŠ å†…å®¹ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ·»åŠ æ–°çš„åŠ¨ç”»é€»è¾‘/æ˜¾ç¤ºæ–¹æ¡ˆ (eg: l2d/spine ç­‰)&lt;/li&gt; &#xA;   &lt;li&gt;æ·»åŠ æ–°åŠŸèƒ½ (é—¹é’Ÿ/è®°äº‹æ¿ç­‰ç­‰)&lt;/li&gt; &#xA;   &lt;li&gt;å‡ ä¹æ— æ‰€ä¸èƒ½, ç¤ºä¾‹ä¾‹å­å‚è§ &lt;a href=&#34;https://github.com/LorisYounger/VPet.Plugin.Demo&#34;&gt;VPet.Plugin.Demo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;åé¦ˆ&amp;amp;å»ºè®®&amp;amp;è”ç³»æˆ‘ä»¬&lt;/h3&gt; &#xA;&lt;p&gt;å¦‚æœæœ‰å»ºè®®æˆ–è€…æ„è§,å¯ä»¥åœ¨Steamå•†åº—è¯„è®º/ç¤¾åŒº,Github Issue,è™šæ‹Ÿä¸»æ’­æ¨¡æ‹Ÿå™¨è´´å§,è™šæ‹Ÿæ¡Œå® æ¨¡æ‹Ÿå™¨MODDerç¾¤(907101442)æˆ–è€…é‚®ä»¶è”ç³»æˆ‘ &lt;a href=&#34;mailto:service@exlb.net&#34;&gt;mailto:service@exlb.net&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;è½¯ä»¶ç»“æ„&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;VPet-Simulator.Windows: é€‚ç”¨äºæ¡Œé¢ç«¯çš„è™šæ‹Ÿæ¡Œå® æ¨¡æ‹Ÿå™¨&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;em&gt;Function åŠŸèƒ½æ€§ä»£ç å­˜æ”¾ä½ç½®&lt;/em&gt;&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;CoreMOD Modç®¡ç†ç±»&lt;/li&gt; &#xA;     &lt;li&gt;MWController çª—ä½“æ§åˆ¶å™¨&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;*WinDesign çª—å£å’ŒUIè®¾è®¡&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;winBetterBuy æ›´å¥½ä¹°çª—å£&lt;/li&gt; &#xA;     &lt;li&gt;winCGPTSetting ChatGPT è®¾ç½®&lt;/li&gt; &#xA;     &lt;li&gt;winSetting è½¯ä»¶è®¾ç½®/MOD çª—å£&lt;/li&gt; &#xA;     &lt;li&gt;winConsole å¼€å‘æ§åˆ¶å°&lt;/li&gt; &#xA;     &lt;li&gt;winGameSetting æ¸¸æˆè®¾ç½®&lt;/li&gt; &#xA;     &lt;li&gt;winReport åé¦ˆä¸­å¿ƒ&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;MainWindows ä¸»çª—ä½“,å­˜æ”¾å’Œå±•ç¤ºCore&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;PetHelper å¿«é€Ÿåˆ‡æ¢å°æ ‡&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;VPet-Simulator.Tool: æ–¹ä¾¿åˆ¶ä½œMODçš„å·¥å…·(eg:å›¾ç‰‡å¸§ç”Ÿæˆ)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;VPet-Simulator.Core: è½¯ä»¶æ ¸å¿ƒ æ–¹ä¾¿å†…ç½®åˆ°ä»»ä½•WPFåº”ç”¨ç¨‹åº(ä¾‹å¦‚:VUP-Simulator)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Handle æ¥å£ä¸æ§ä»¶ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;IController çª—ä½“æ§åˆ¶å™¨ (è°ƒç”¨ç›¸å…³åŠŸèƒ½å’Œè®¾ç½®,ä¾‹å¦‚ç§»åŠ¨åˆ°ä¾§è¾¹ç­‰)&lt;/li&gt; &#xA;     &lt;li&gt;Function é€šç”¨åŠŸèƒ½&lt;/li&gt; &#xA;     &lt;li&gt;GameCore æ¸¸æˆæ ¸å¿ƒ,åŒ…å«å„ç§æ•°æ®ç­‰å†…å®¹&lt;/li&gt; &#xA;     &lt;li&gt;GameSave æ¸¸æˆå­˜æ¡£&lt;/li&gt; &#xA;     &lt;li&gt;IFood é£Ÿç‰©/ç‰©å“æ¥å£&lt;/li&gt; &#xA;     &lt;li&gt;PetLoader å® ç‰©å›¾å½¢åŠ è½½å™¨&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Graph å›¾å½¢æ¸²æŸ“ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;IGraph åŠ¨ç”»åŸºæœ¬æ¥å£&lt;/li&gt; &#xA;     &lt;li&gt;GraphCore åŠ¨ç”»æ˜¾ç¤ºæ ¸å¿ƒ&lt;/li&gt; &#xA;     &lt;li&gt;GraphHelper åŠ¨ç”»å¸®åŠ©ç±»&lt;/li&gt; &#xA;     &lt;li&gt;GraphInfo åŠ¨ç”»ä¿¡æ¯&lt;/li&gt; &#xA;     &lt;li&gt;FoodAnimation é£Ÿç‰©åŠ¨ç”» æ”¯æŒæ˜¾ç¤ºå‰ä¸­å3å±‚å¤¹å¿ƒåŠ¨ç”» ä¸ä¸€å®šåªç”¨äºé£Ÿç‰©,åªæ˜¯å«è¿™ä¸ªåå­—&lt;/li&gt; &#xA;     &lt;li&gt;PNGAnimation æ¡Œå® åŠ¨æ€åŠ¨ç”»ç»„ä»¶&lt;/li&gt; &#xA;     &lt;li&gt;Picture æ¡Œå® é™æ€åŠ¨ç”»ç»„ä»¶&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Display æ˜¾ç¤º &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;basestyle/Theme åŸºæœ¬é£æ ¼ä¸»é¢˜&lt;/li&gt; &#xA;     &lt;li&gt;Main.xaml æ ¸å¿ƒæ˜¾ç¤ºéƒ¨ä»¶ &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;MainDisplay æ ¸å¿ƒæ˜¾ç¤ºæ–¹æ³•&lt;/li&gt; &#xA;       &lt;li&gt;MainLogic æ ¸å¿ƒæ˜¾ç¤ºé€»è¾‘&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;ToolBar ç‚¹å‡»äººç‰©æ—¶å€™çš„å·¥å…·æ &lt;/li&gt; &#xA;     &lt;li&gt;MessageBar äººç‰©è¯´è¯æ—¶å€™çš„è¯´è¯æ &lt;/li&gt; &#xA;     &lt;li&gt;WorkTimer å·¥ä½œæ—¶é’Ÿ&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å‚ä¸å¼€å‘&lt;/h2&gt; &#xA;&lt;p&gt;æ¬¢è¿å‚ä¸è™šæ‹Ÿæ¡Œå® æ¨¡æ‹Ÿå™¨çš„å¼€å‘! ä¸ºä¿è¯ä»£ç å¯ç»´æŠ¤åº¦å’Œæ¸¸æˆæ€§,å¦‚æœæƒ³è¦å¼€å‘æ–°çš„åŠŸèƒ½,è¯·å…ˆ&lt;a href=&#34;mailto:zoujin.dev@exlb.org&#34;&gt;é‚®ä»¶è”ç³»&lt;/a&gt;æˆ–å‘&lt;a href=&#34;https://github.com/LorisYounger/VPet/issues&#34;&gt;Issues&lt;/a&gt;æˆ‘æƒ³è¦æ·»åŠ çš„åŠŸèƒ½/ç©æ³•, ä»¥ç¡®ä¿è¯¥åŠŸèƒ½/ç©æ³•é€‚ç”¨äºè™šæ‹Ÿæ¡Œå® æ¨¡æ‹Ÿå™¨. ä»¥å…æœªæ¥æäº¤æ—¶å› ä¸åˆé€‚è¢«æ‹’(è€Œé€ æˆä»£ç æµªè´¹)&lt;br&gt; å¦‚æœæ˜¯ä¿®å¤é”™è¯¯æˆ–è€…BUG,æ— éœ€è”ç³»æˆ‘,ä¿®å¥½åç›´æ¥PRå³å¯&lt;/p&gt; &#xA;&lt;p&gt;å½“æƒ³æ³•é€šè¿‡å,æ‚¨å¯ä»¥é€šè¿‡ &lt;a href=&#34;https://github.com/LorisYounger/VPet/fork&#34;&gt;fork&lt;/a&gt; åŠŸèƒ½æ‹·è´ä»£ç è‡³è‡ªå·±çš„githubä»¥æ–¹ä¾¿ç¼–å†™è‡ªå·±çš„ä»£ç , ç¼–å†™å®Œæ¯•åé€šè¿‡&lt;a href=&#34;https://github.com/LorisYounger/VPet/compare&#34;&gt;pull requests&lt;/a&gt; æäº¤&lt;br&gt; å¦‚æœæ‚¨æƒ³æ³•æ²¡æœ‰è¢«é€šè¿‡,ä¹Ÿå¯ä»¥å¦èµ·ç‚‰ç¶,å†™ä¸ªä¸åŒç‰ˆæœ¬åŠŸèƒ½çš„æ¡Œå® è½¯ä»¶. ä½†éœ€éµå®ˆ &lt;a href=&#34;https://github.com/LorisYounger/VPet/raw/main/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt; ä¸ &lt;a href=&#34;https://github.com/LorisYounger/VPet#%E5%8A%A8%E7%94%BB%E7%89%88%E6%9D%83%E5%A3%B0%E6%98%8E%E4%B8%8E%E6%8E%88%E6%9D%83&#34;&gt;åŠ¨ç”»ç‰ˆæƒå£°æ˜ä¸æˆæƒ&lt;/a&gt; æ³¨: ä¸€èˆ¬æ¥è®², æ·»åŠ æ–°åŠŸèƒ½éƒ½å¯ä»¥é€šè¿‡ç¼–å†™ä»£ç æ’ä»¶MODå®ç°, è¯¦æƒ…è¯·å‚è§ &lt;a href=&#34;https://github.com/LorisYounger/VPet.Plugin.Demo&#34;&gt;VPet.Plugin.Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;æˆ‘å¯èƒ½ä¼šå¯¹æ‚¨çš„æäº¤çš„ä»£ç è¿›è¡Œä¿®æ”¹,åˆ å‡ç­‰ä»¥ç¡®ä¿è¯¥åŠŸèƒ½/ç©æ³•é€‚ç”¨äºè™šæ‹Ÿæ¡Œå® æ¨¡æ‹Ÿå™¨.&lt;/p&gt; &#xA;&lt;p&gt;æ„Ÿè°¢ä»¥ä¸‹å‚ä¸çš„å¼€å‘äººå‘˜(æŒ‰è´¡çŒ®ç¨‹åº¦æ’åº)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Wreathlit&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;windofxy&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;bwnotfound&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;aalichao&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Dobby233Liu&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;eltociear&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;yupix&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;å’Œæä¾›ç¤¾åŒºç¿»è¯‘å’Œæ›´å¤šå†…å®¹çš„åˆ›æ„å·¥åŠäººå‘˜&lt;/p&gt; &#xA;&lt;h2&gt;åŠ¨ç”»ç‰ˆæƒå£°æ˜ä¸æˆæƒ&lt;/h2&gt; &#xA;&lt;p&gt;åœ¨githubä¸­ &lt;a href=&#34;https://github.com/LorisYounger/VPet/tree/main/VPet-Simulator.Windows/mod/0000_core/pet/vup&#34;&gt;æ¡Œå® åŠ¨ç”»æ–‡ä»¶&lt;/a&gt; åŠ¨ç”»ç‰ˆæƒå½’ &lt;a href=&#34;https://www.exlb.net/VUP-Simulator&#34;&gt;è™šæ‹Ÿä¸»æ’­æ¨¡æ‹Ÿå™¨åˆ¶ä½œç»„&lt;/a&gt;æ‰€æœ‰, å½“ä½¿ç”¨æœ¬ç±»åº“æ—¶,æ‚¨å¯èƒ½éœ€è¦è‡ªè¡Œå‡†å¤‡åŠ¨ç”»æ–‡ä»¶,æˆ–éµå¾ªä»¥ä¸‹åè®®&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;**æ³¨ ** æœ¬åŠ¨ç”»å£°æ˜ä»…é™äºæ¡Œå® è‡ªå¸¦çš„åŠ¨ç”», è‹¥æœ‰ç”»å¸ˆ/å¼€å‘è€…ç”»è‡ªå·±çš„åŠ¨ç”»é€‚é…ç»™æ¡Œå® ,å¹¶ä¸éµå¾ªç”¨æœ¬å£°æ˜&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;éå•†ç”¨ç”¨é€”æˆæƒ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;éœ€è¦å‘ç”¨æˆ·å‘ŠçŸ¥åŠ¨ç”»æ–‡ä»¶æ¥æºå¹¶æä¾›è®¿é—® &lt;a href=&#34;https://github.com/LorisYounger/VPet&#34;&gt;è¯¥é¡µé¢&lt;/a&gt; çš„é“¾æ¥&lt;/li&gt; &#xA; &lt;li&gt;å½“æ‚¨å®Œæˆä»¥ä¸Šè¦æ±‚å,æ‚¨å¯ä»¥å…è´¹ä½¿ç”¨åŠ¨ç”»æ–‡ä»¶&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;å•†ç”¨ç”¨é€”æˆæƒ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶éœ€å¼¹çª—å¹¶é†’ç›®çš„å‘ç”¨æˆ·å‘ŠçŸ¥åŠ¨ç”»æ–‡ä»¶æ¥æºå¹¶æä¾›è®¿é—® &lt;a href=&#34;https://github.com/LorisYounger/VPet&#34;&gt;è¯¥é¡µé¢&lt;/a&gt; çš„é“¾æ¥&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;åœ¨ç›¸åº”é¡µé¢(ç”¨æˆ·å¯ä»¥å¿«æ·è®¿é—®)å‘ç”¨æˆ·å‘ŠçŸ¥åŠ¨ç”»æ–‡ä»¶æ¥æºå¹¶æä¾›è®¿é—® &lt;a href=&#34;https://github.com/LorisYounger/VPet&#34;&gt;è¯¥é¡µé¢&lt;/a&gt; çš„é“¾æ¥&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ç¦æ­¢é€šè¿‡å‡ºå”®åŠ¨ç”»æ–‡ä»¶è¿›è¡Œç›ˆåˆ©&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;è¯·&lt;a href=&#34;mailto:zoujin.dev@exlb.org&#34;&gt;é‚®ä»¶è”ç³»&lt;/a&gt;æˆ‘&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å½“æ‚¨å®Œæˆä»¥ä¸Šè¦æ±‚å,æ‚¨å¯ä»¥å…è´¹ä½¿ç”¨åŠ¨ç”»æ–‡ä»¶&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;åˆ†å‘åŠ¨ç”»æ–‡ä»¶&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;éœ€è¦å‘ŠçŸ¥ä»¥ä¸Šæ‰€æœ‰æˆæƒä¿¡æ¯&lt;/li&gt; &#xA; &lt;li&gt;éœ€è¦æä¾›è®¿é—® &lt;a href=&#34;https://github.com/LorisYounger/VPet&#34;&gt;è¯¥é¡µé¢&lt;/a&gt; çš„é“¾æ¥&lt;/li&gt; &#xA; &lt;li&gt;åˆ†å‘åŠ¨ç”»æ–‡ä»¶æ—¶ç¦æ­¢ä»»ä½•ä»˜è´¹/æ”¶è´¹è¡Œä¸º&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æ¡Œé¢ç«¯éƒ¨ç½²æ–¹æ³•&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ä¸‹è½½æœ¬é¡¹ç›®, é€šè¿‡VisualStudioæ‰“å¼€ &lt;code&gt;VPet.sln&lt;/code&gt; æ–‡ä»¶&lt;/li&gt; &#xA; &lt;li&gt;åœ¨ç”Ÿæˆæ ä¸­, é€‰æ‹© ä½æ•°ä¸º &lt;code&gt;x64&lt;/code&gt; å’Œç”Ÿæˆé¡¹ç›®ä¸º &lt;code&gt;Vpet-Simulator.Windows&lt;/code&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LorisYounger/VPet/main/README.assets/image-20230208004330895.png&#34; alt=&#34;image-20230208004330895&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;ç‚¹å‡»å¯åŠ¨, å¦‚æœä¸€åˆ‡æ­£å¸¸åˆ™ä¼šæŠ¥é”™ &lt;code&gt;ç¼ºå°‘æ¨¡ç»„Core,æ— æ³•å¯åŠ¨æ¡Œå® &lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ &lt;code&gt;mklink.bat&lt;/code&gt;, è¿™ä¼šè®©modæ–‡ä»¶é“¾æ¥åˆ°ç”Ÿæˆä½ç½®&lt;/li&gt; &#xA; &lt;li&gt;å†æ¬¡ç‚¹å‡»å¯åŠ¨å³å¯æ­£å¸¸è¿è¡Œ&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>