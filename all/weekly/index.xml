<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-18T01:37:27Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Blaizzy/mlx-audio</title>
    <updated>2025-05-18T01:37:27Z</updated>
    <id>tag:github.com,2025-05-18:/Blaizzy/mlx-audio</id>
    <link href="https://github.com/Blaizzy/mlx-audio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A text-to-speech (TTS), speech-to-text (STT) and speech-to-speech (STS) library built on Apple&#39;s MLX framework, providing efficient speech analysis on Apple Silicon.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLX-Audio&lt;/h1&gt; &#xA;&lt;p&gt;A text-to-speech (TTS) and Speech-to-Speech (STS) library built on Apple&#39;s MLX framework, providing efficient speech synthesis on Apple Silicon.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fast inference on Apple Silicon (M series chips)&lt;/li&gt; &#xA; &lt;li&gt;Multiple language support&lt;/li&gt; &#xA; &lt;li&gt;Voice customization options&lt;/li&gt; &#xA; &lt;li&gt;Adjustable speech speed control (0.5x to 2.0x)&lt;/li&gt; &#xA; &lt;li&gt;Interactive web interface with 3D audio visualization&lt;/li&gt; &#xA; &lt;li&gt;REST API for TTS generation&lt;/li&gt; &#xA; &lt;li&gt;Quantization support for optimized performance&lt;/li&gt; &#xA; &lt;li&gt;Direct access to output files via Finder/Explorer integration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install the package&#xA;pip install mlx-audio&#xA;&#xA;# For web interface and API dependencies&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;To generate audio with an LLM use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Basic usage&#xA;mlx_audio.tts.generate --text &#34;Hello, world&#34;&#xA;&#xA;# Specify prefix for output file&#xA;mlx_audio.tts.generate --text &#34;Hello, world&#34; --file_prefix hello&#xA;&#xA;# Adjust speaking speed (0.5-2.0)&#xA;mlx_audio.tts.generate --text &#34;Hello, world&#34; --speed 1.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to call from python&lt;/h3&gt; &#xA;&lt;p&gt;To generate audio with an LLM use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mlx_audio.tts.generate import generate_audio&#xA;&#xA;# Example: Generate an audiobook chapter as mp3 audio&#xA;generate_audio(&#xA;    text=(&#34;In the beginning, the universe was created...\n&#34;&#xA;        &#34;...or the simulation was booted up.&#34;),&#xA;    model_path=&#34;prince-canuma/Kokoro-82M&#34;,&#xA;    voice=&#34;af_heart&#34;,&#xA;    speed=1.2,&#xA;    lang_code=&#34;a&#34;, # Kokoro: (a)f_heart, or comment out for auto&#xA;    file_prefix=&#34;audiobook_chapter1&#34;,&#xA;    audio_format=&#34;wav&#34;,&#xA;    sample_rate=24000,&#xA;    join_audio=True,&#xA;    verbose=True  # Set to False to disable print messages&#xA;)&#xA;&#xA;print(&#34;Audiobook chapter successfully generated!&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Web Interface &amp;amp; API Server&lt;/h3&gt; &#xA;&lt;p&gt;MLX-Audio includes a web interface with a 3D visualization that reacts to audio frequencies. The interface allows you to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate TTS with different voices and speed settings&lt;/li&gt; &#xA; &lt;li&gt;Upload and play your own audio files&lt;/li&gt; &#xA; &lt;li&gt;Visualize audio with an interactive 3D orb&lt;/li&gt; &#xA; &lt;li&gt;Automatically saves generated audio files to the outputs directory in the current working folder&lt;/li&gt; &#xA; &lt;li&gt;Open the output folder directly from the interface (when running locally)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Features&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiple Voice Options&lt;/strong&gt;: Choose from different voice styles (AF Heart, AF Nova, AF Bella, BF Emma)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adjustable Speech Speed&lt;/strong&gt;: Control the speed of speech generation with an interactive slider (0.5x to 2.0x)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Real-time 3D Visualization&lt;/strong&gt;: A responsive 3D orb that reacts to audio frequencies&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Audio Upload&lt;/strong&gt;: Play and visualize your own audio files&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Auto-play Option&lt;/strong&gt;: Automatically play generated audio&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Output Folder Access&lt;/strong&gt;: Convenient button to open the output folder in your system&#39;s file explorer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To start the web interface and API server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Using the command-line interface&#xA;mlx_audio.server&#xA;&#xA;# With custom host and port&#xA;mlx_audio.server --host 0.0.0.0 --port 9000&#xA;&#xA;# With verbose logging&#xA;mlx_audio.server --verbose&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Available command line arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--host&lt;/code&gt;: Host address to bind the server to (default: 127.0.0.1)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--port&lt;/code&gt;: Port to bind the server to (default: 8000)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then open your browser and navigate to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;API Endpoints&lt;/h4&gt; &#xA;&lt;p&gt;The server provides the following REST API endpoints:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;POST /tts&lt;/code&gt;: Generate TTS audio&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Parameters (form data): &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;text&lt;/code&gt;: The text to convert to speech (required)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;voice&lt;/code&gt;: Voice to use (default: &#34;af_heart&#34;)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;speed&lt;/code&gt;: Speech speed from 0.5 to 2.0 (default: 1.0)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Returns: JSON with filename of generated audio&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;GET /audio/{filename}&lt;/code&gt;: Retrieve generated audio file&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;POST /play&lt;/code&gt;: Play audio directly from the server&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Parameters (form data): &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;filename&lt;/code&gt;: The filename of the audio to play (required)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Returns: JSON with status and filename&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;POST /stop&lt;/code&gt;: Stop any currently playing audio&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Returns: JSON with status&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;POST /open_output_folder&lt;/code&gt;: Open the output folder in the system&#39;s file explorer&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Returns: JSON with status and path&lt;/li&gt; &#xA;   &lt;li&gt;Note: This feature only works when running the server locally&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: Generated audio files are stored in &lt;code&gt;~/.mlx_audio/outputs&lt;/code&gt; by default, or in a fallback directory if that location is not writable.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;h3&gt;Kokoro&lt;/h3&gt; &#xA;&lt;p&gt;Kokoro is a multilingual TTS model that supports various languages and voice styles.&lt;/p&gt; &#xA;&lt;h4&gt;Example Usage&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mlx_audio.tts.models.kokoro import KokoroPipeline&#xA;from mlx_audio.tts.utils import load_model&#xA;from IPython.display import Audio&#xA;import soundfile as sf&#xA;&#xA;# Initialize the model&#xA;model_id = &#39;prince-canuma/Kokoro-82M&#39;&#xA;model = load_model(model_id)&#xA;&#xA;# Create a pipeline with American English&#xA;pipeline = KokoroPipeline(lang_code=&#39;a&#39;, model=model, repo_id=model_id)&#xA;&#xA;# Generate audio&#xA;text = &#34;The MLX King lives. Let him cook!&#34;&#xA;for _, _, audio in pipeline(text, voice=&#39;af_heart&#39;, speed=1, split_pattern=r&#39;\n+&#39;):&#xA;    # Display audio in notebook (if applicable)&#xA;    display(Audio(data=audio, rate=24000, autoplay=0))&#xA;&#xA;    # Save audio to file&#xA;    sf.write(&#39;audio.wav&#39;, audio[0], 24000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Language Options&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🇺🇸 &lt;code&gt;&#39;a&#39;&lt;/code&gt; - American English&lt;/li&gt; &#xA; &lt;li&gt;🇬🇧 &lt;code&gt;&#39;b&#39;&lt;/code&gt; - British English&lt;/li&gt; &#xA; &lt;li&gt;🇯🇵 &lt;code&gt;&#39;j&#39;&lt;/code&gt; - Japanese (requires &lt;code&gt;pip install misaki[ja]&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;🇨🇳 &lt;code&gt;&#39;z&#39;&lt;/code&gt; - Mandarin Chinese (requires &lt;code&gt;pip install misaki[zh]&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;CSM (Conversational Speech Model)&lt;/h3&gt; &#xA;&lt;p&gt;CSM is a model from Sesame that allows you text-to-speech and to customize voices using reference audio samples.&lt;/p&gt; &#xA;&lt;h4&gt;Example Usage&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Generate speech using CSM-1B model with reference audio&#xA;python -m mlx_audio.tts.generate --model mlx-community/csm-1b --text &#34;Hello from Sesame.&#34; --play --ref_audio ./conversational_a.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can pass any audio to clone the voice from or download sample audio file from &lt;a href=&#34;https://huggingface.co/mlx-community/csm-1b/tree/main/prompts&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Advanced Features&lt;/h2&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;p&gt;You can quantize models for improved performance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mlx_audio.tts.utils import quantize_model, load_model&#xA;import json&#xA;import mlx.core as mx&#xA;&#xA;model = load_model(repo_id=&#39;prince-canuma/Kokoro-82M&#39;)&#xA;config = model.config&#xA;&#xA;# Quantize to 8-bit&#xA;group_size = 64&#xA;bits = 8&#xA;weights, config = quantize_model(model, config, group_size, bits)&#xA;&#xA;# Save quantized model&#xA;with open(&#39;./8bit/config.json&#39;, &#39;w&#39;) as f:&#xA;    json.dump(config, f)&#xA;&#xA;mx.save_safetensors(&#34;./8bit/kokoro-v1_0.safetensors&#34;, weights, metadata={&#34;format&#34;: &#34;mlx&#34;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MLX&lt;/li&gt; &#xA; &lt;li&gt;Python 3.8+&lt;/li&gt; &#xA; &lt;li&gt;Apple Silicon Mac (for optimal performance)&lt;/li&gt; &#xA; &lt;li&gt;For the web interface and API: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;FastAPI&lt;/li&gt; &#xA;   &lt;li&gt;Uvicorn&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to the Apple MLX team for providing a great framework for building TTS and STS models.&lt;/li&gt; &#xA; &lt;li&gt;This project uses the Kokoro model architecture for text-to-speech synthesis.&lt;/li&gt; &#xA; &lt;li&gt;The 3D visualization uses Three.js for rendering.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>alibaba/spring-ai-alibaba</title>
    <updated>2025-05-18T01:37:27Z</updated>
    <id>tag:github.com,2025-05-18:/alibaba/spring-ai-alibaba</id>
    <link href="https://github.com/alibaba/spring-ai-alibaba" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Agentic AI Framework for Java Developers&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; &#xA; &lt;p&gt;The community driven Spring AI Alibaba OpenManus Java implementtation can be found at &lt;a href=&#34;https://raw.githubusercontent.com/alibaba/spring-ai-alibaba/main/spring-ai-alibaba-jmanus&#34;&gt;spring-ai-alibaba-jmanus&lt;/a&gt; module.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://java2ai.com&#34;&gt;Spring AI Alibaba&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/spring-ai-alibaba/main/README-zh.md&#34;&gt;中文版本&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/alibaba/spring-ai-alibaba/main/README-ja.md&#34;&gt;日本語&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An AI application framework for Java developers built on top of Spring AI that provides seamless integration with Alibaba Cloud QWen LLM services and cloud-native infrastructures.&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://java2ai.com/docs/dev/get-started/&#34;&gt;quick start&lt;/a&gt; for how to quickly add generative AI to your Spring Boot applications.&lt;/p&gt; &#xA;&lt;p&gt;Overall, it takes only two steps to turn your Spring Boot application into an intelligent agent:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Because Spring AI Alibaba is developed based on Spring Boot 3.x, it requires JDK version 17 and above.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;spring-ai-alibaba-starter&lt;/code&gt; dependency to your project.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;&#xA;     &amp;lt;groupId&amp;gt;com.alibaba.cloud.ai&amp;lt;/groupId&amp;gt;&#xA;     &amp;lt;artifactId&amp;gt;spring-ai-alibaba-starter&amp;lt;/artifactId&amp;gt;&#xA;     &amp;lt;version&amp;gt;1.0.0-M8.1-SNAPSHOT&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTICE: Since spring-ai related packages haven&#39;t been published to the central repo yet, it&#39;s needed to add the following maven repository to your project in order to successfully resolve artifacts like spring-ai-core.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;repositories&amp;gt;&#xA;     &amp;lt;repository&amp;gt;&#xA;          &amp;lt;id&amp;gt;spring-milestones&amp;lt;/id&amp;gt;&#xA;          &amp;lt;name&amp;gt;Spring Milestones&amp;lt;/name&amp;gt;&#xA;          &amp;lt;url&amp;gt;https://repo.spring.io/milestone&amp;lt;/url&amp;gt;&#xA;          &amp;lt;snapshots&amp;gt;&#xA;               &amp;lt;enabled&amp;gt;false&amp;lt;/enabled&amp;gt;&#xA;          &amp;lt;/snapshots&amp;gt;&#xA;     &amp;lt;/repository&amp;gt;&#xA;&amp;lt;/repositories&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Addendum: If the mirrorOf tag in your local Maven settings. xml is configured with the wildcard *, please modify it according to the following example.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;mirror&amp;gt;&#xA;      &amp;lt;id&amp;gt;xxxx&amp;lt;/id&amp;gt;&#xA;      &amp;lt;mirrorOf&amp;gt;*,!spring-milestones&amp;lt;/mirrorOf&amp;gt;&#xA;      &amp;lt;name&amp;gt;xxxx&amp;lt;/name&amp;gt;&#xA;      &amp;lt;url&amp;gt;xxxx&amp;lt;/url&amp;gt;&#xA;&amp;lt;/mirror&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Inject &lt;code&gt;ChatClient&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;@RestController&#xA;public class ChatController {&#xA;&#xA;     private final ChatClient chatClient;&#xA;&#xA;     public ChatController(ChatClient.Builder builder) {&#xA;      this.chatClient = builder.build();&#xA;     }&#xA;&#xA;     @GetMapping(&#34;/chat&#34;)&#xA;     public String chat(String input) {&#xA;      return this.chatClient.prompt()&#xA;        .user(input)&#xA;        .call()&#xA;        .content();&#xA;     }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/springaialibaba/spring-ai-alibaba-examples&#34;&gt;Spring AI Alibaba and Spring AI usage examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Core Features&lt;/h2&gt; &#xA;&lt;p&gt;Spring AI Alibaba provides the following features, read the &lt;a href=&#34;https://java2ai.com/&#34;&gt;documentation&lt;/a&gt; on our website for more details of how to use these features.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for Alibaba Cloud QWen Model and Dashscope Model service.&lt;/li&gt; &#xA; &lt;li&gt;Support high-level AI agent abstraction -- ChatClient.&lt;/li&gt; &#xA; &lt;li&gt;Support various Model types like Chat, Text to Image, Audio Transcription, Text to Speech.&lt;/li&gt; &#xA; &lt;li&gt;Both synchronous and stream API options are supported.&lt;/li&gt; &#xA; &lt;li&gt;Mapping of AI Model output to POJOs.&lt;/li&gt; &#xA; &lt;li&gt;Portable API across Vector Store providers.&lt;/li&gt; &#xA; &lt;li&gt;Function calling.&lt;/li&gt; &#xA; &lt;li&gt;Spring Boot Auto Configuration and Starters.&lt;/li&gt; &#xA; &lt;li&gt;RAG (Retrieval-Augmented Generation) support: DocumentReader, Splitter, Embedding, VectorStore, and Retriever.&lt;/li&gt; &#xA; &lt;li&gt;Support conversation with ChatMemory&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Spring AI Alibaba aims to reduce the complexity of building AI native Java applications, from development, evaluation to deployment and observability. In order to achieve that, we provide both open-source framework and ecosystem integrations around it, below are the features that we plan to support in the near future:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Prompt Template Management&lt;/li&gt; &#xA; &lt;li&gt;Event Driven AI Application&lt;/li&gt; &#xA; &lt;li&gt;Support of more Vector Databases&lt;/li&gt; &#xA; &lt;li&gt;Function Deployment&lt;/li&gt; &#xA; &lt;li&gt;Observability&lt;/li&gt; &#xA; &lt;li&gt;AI proxy support: prompt filtering, rate limit, multiple Model, etc.&lt;/li&gt; &#xA; &lt;li&gt;Development Tools&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba/spring-ai-alibaba/main/docs/imgs/spring-ai-alibaba-arch.png&#34; alt=&#34;ai-native-architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribution Guide&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/alibaba/spring-ai-alibaba/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt; to learn how to participate in the development of Spring AI Alibaba.&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.spring.io/spring-ai/reference/index.html&#34;&gt;Spring AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://java2ai.com/docs/dev/overview/&#34;&gt;Spring AI Alibaba&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.aliyun.com/zh/model-studio/getting-started/what-is-model-studio/&#34;&gt;Alibaba Cloud Dashscope Model Service Platform (阿里云百炼模型服务及应用开发平台)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dingtalk Group (钉钉群), search &lt;code&gt;61290041831&lt;/code&gt; and join.&lt;/li&gt; &#xA; &lt;li&gt;Wechat Group (微信公众号), scan the QR code below and follow us.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba/spring-ai-alibaba/main/docs/imgs/wechat-account.png&#34; style=&#34;width:260px;&#34;&gt; &#xA;&lt;h2&gt;Credit&lt;/h2&gt; &#xA;&lt;p&gt;Some of this project&#39;s ideas and codes are inspired by or rewrote from the following projects. Great thanks to those who have created and open-sourced these projects.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/spring-projects/spring-ai&#34;&gt;Spring AI&lt;/a&gt;, a Spring-friendly API and abstractions for developing AI applications licensed under the Apache License 2.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langgraph&#34;&gt;Langgraph&lt;/a&gt;, a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows licensed under the MIT license.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bsorrentino/langgraph4j&#34;&gt;Langgraph4J&lt;/a&gt;, a porting of original &lt;a href=&#34;https://github.com/langchain-ai/langgraph&#34;&gt;LangGraph&lt;/a&gt; from the &lt;a href=&#34;https://github.com/langchain-ai&#34;&gt;LangChain AI project&lt;/a&gt; in Java fashion.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://deepwiki.com/w9lsky/spring-ai-alibaba&#34;&gt;&lt;img src=&#34;https://deepwiki.com/badge.svg?sanitize=true&#34; alt=&#34;Ask DeepWiki&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>xming521/WeClone</title>
    <updated>2025-05-18T01:37:27Z</updated>
    <id>tag:github.com,2025-05-18:/xming521/WeClone</id>
    <link href="https://github.com/xming521/WeClone" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🚀从聊天记录创造数字分身的一站式解决方案💡 使用聊天记录微调大语言模型，让大模型有“那味儿”，并绑定到聊天机器人，实现自己的数字分身。 数字克隆/数字分身/数字永生/LLM/聊天机器人/LoRA&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/5842e84e-004f-4afd-9373-af64e9575b78&#34; alt=&#34;download&#34;&gt;&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;🚀从聊天记录创造数字分身的一站式解决方案💡&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/xming521/WeClone/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/xming521/WeClone?style=for-the-badge&amp;amp;logo=github&amp;amp;label=Stars&amp;amp;logoColor=white&amp;amp;color=ffda65&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xming521/WeClone/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/xming521/WeClone?style=for-the-badge&amp;amp;logo=github&amp;amp;label=Release&amp;amp;logoColor=white&amp;amp;color=06d094&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://qm.qq.com/cgi-bin/qm/qr?k=wNdgbOVT6oFOJ2wlMLsolUXErW9ESLpk&amp;amp;jump_from=webapi&amp;amp;authKey=z/reOp6YLyvR4Tl2k2nYMsLoMC3w9/99ucgKMX0oRGlxDV/WbYnvq2QxODoIkfxn&#34; target=&#34;_blank&#34; style=&#34;text-decoration: none;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/QQ%E7%BE%A4-708067078-12B7F5?style=for-the-badge&amp;amp;logo=qq&amp;amp;logoColor=white&#34; alt=&#34;WeClone①&#34; title=&#34;WeClone①&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://t.me/+JEdak4m0XEQ3NGNl&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&amp;amp;logo=telegram&amp;amp;logoColor=white&#34; alt=&#34;Telegram&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://hellogithub.com/repository/12ab209b56cb4cfd885c8cfd4cfdd53e&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=12ab209b56cb4cfd885c8cfd4cfdd53e&amp;amp;claim_uid=RThlPDoGrFvdMY5&#34; alt=&#34;Featured｜HelloGitHub&#34; style=&#34;width: 150px; height: 28px;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://trendshift.io/repositories/13759&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/13759&#34; alt=&#34;xming521%2FWeClone | Trendshift&#34; style=&#34;width: 220px; height: 50px;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://deepwiki.com/xming521/WeClone&#34;&gt;&lt;img src=&#34;https://deepwiki.com/badge.svg?sanitize=true&#34; alt=&#34;Ask DeepWiki&#34; style=&#34;width: 134px; height: 23px;margin-bottom: 3px;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://blog.051088.xyz/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/&#34; target=&#34;_blank&#34;&gt; Windows部署指南 &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;✨核心功能&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;💫 涵盖打造数字分身的全链路方案，包括聊天数据导出、预处理、模型训练、部署&lt;/li&gt; &#xA; &lt;li&gt;💬 使用微信聊天记录微调LLM，让大模型有&#34;那味儿&#34;&lt;/li&gt; &#xA; &lt;li&gt;🔗 绑定到微信、QQ、Telegram、企微、飞书机器人，实现自己的数字分身&lt;/li&gt; &#xA; &lt;li&gt;🛡️ 隐私信息过滤，本地化微调部署，数据安全可控&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📋特性与说明&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;/p&gt; &#xA; &lt;h3&gt;0.2.1版本支持了命令行工具，使用前需要重新执行 &lt;code&gt;uv pip install -e .&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] 0.2.0版本进行了全面重构，数据集目录和脚本路径全部进行了修改，拉取新代码后，&lt;code&gt;csv&lt;/code&gt;文件夹放在&lt;code&gt;dataset&lt;/code&gt;下，并且需要重新安装依赖。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;WeClone仍在快速迭代期，当前效果不代表最终效果。&lt;/li&gt; &#xA;  &lt;li&gt;微调LLM效果很大程度取决于模型大小、聊天数据的数量和质量，理论上模型越大，数据越多，效果越好。&lt;/li&gt; &#xA;  &lt;li&gt;Windows环境未进行严格测试，可以使用WSL作为运行环境。详细教程可点击&lt;a href=&#34;https://blog.051088.xyz/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/&#34;&gt;Windows部署指南&lt;/a&gt;查看。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;硬件要求&lt;/h3&gt; &#xA;&lt;p&gt;项目默认使用Qwen2.5-7B-Instruct模型，LoRA方法对sft阶段微调，大约需要16GB显存。也可以使用&lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/raw/main/README_zh.md#%E6%A8%A1%E5%9E%8B&#34;&gt;LLaMA Factory&lt;/a&gt;支持的其他模型和方法。&lt;/p&gt; &#xA;&lt;p&gt;需要显存的估算值：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;方法&lt;/th&gt; &#xA;   &lt;th&gt;精度&lt;/th&gt; &#xA;   &lt;th&gt;7B&lt;/th&gt; &#xA;   &lt;th&gt;14B&lt;/th&gt; &#xA;   &lt;th&gt;30B&lt;/th&gt; &#xA;   &lt;th&gt;70B&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;x&lt;/code&gt;B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Full (&lt;code&gt;bf16&lt;/code&gt; or &lt;code&gt;fp16&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;120GB&lt;/td&gt; &#xA;   &lt;td&gt;240GB&lt;/td&gt; &#xA;   &lt;td&gt;600GB&lt;/td&gt; &#xA;   &lt;td&gt;1200GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;18x&lt;/code&gt;GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Full (&lt;code&gt;pure_bf16&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;60GB&lt;/td&gt; &#xA;   &lt;td&gt;120GB&lt;/td&gt; &#xA;   &lt;td&gt;300GB&lt;/td&gt; &#xA;   &lt;td&gt;600GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8x&lt;/code&gt;GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Freeze/LoRA/GaLore/APOLLO/BAdam&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;16GB&lt;/td&gt; &#xA;   &lt;td&gt;32GB&lt;/td&gt; &#xA;   &lt;td&gt;64GB&lt;/td&gt; &#xA;   &lt;td&gt;160GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;2x&lt;/code&gt;GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;10GB&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;40GB&lt;/td&gt; &#xA;   &lt;td&gt;80GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;x&lt;/code&gt;GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;6GB&lt;/td&gt; &#xA;   &lt;td&gt;12GB&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;   &lt;td&gt;48GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;x/2&lt;/code&gt;GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;4GB&lt;/td&gt; &#xA;   &lt;td&gt;8GB&lt;/td&gt; &#xA;   &lt;td&gt;16GB&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;x/4&lt;/code&gt;GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;环境搭建&lt;/h2&gt; &#xA;&lt;p&gt;1.cuda安装(已安装可跳过，&lt;strong&gt;要求版本12.4及以上&lt;/strong&gt;)：&lt;a href=&#34;https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/installation.html#cuda&#34;&gt;LLaMA Factory&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;2.建议使用 &lt;a href=&#34;https://docs.astral.sh/uv/&#34;&gt;uv&lt;/a&gt;安装依赖，这是一个非常快速的 Python 环境管理器。安装uv后，您可以使用以下命令创建一个新的Python环境并安装依赖项，注意这不包含音频克隆功能的依赖：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/xming521/WeClone.git&#xA;cd WeClone&#xA;uv venv .venv --python=3.10&#xA;source .venv/bin/activate # windows下执行 .venv\Scripts\activate&#xA;uv pip install --group main -e . &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] 如果要使用最新的模型进行微调，需要手动安装最新版LLaMA Factory：&lt;code&gt;uv pip install --upgrade git+https://github.com/hiyouga/LLaMA-Factory.git&lt;/code&gt;,同时其他依赖版本也可能需要修改，例如vllm pytorch transforms&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;3.将配置文件模板复制一份并重命名为&lt;code&gt;settings.jsonc&lt;/code&gt;，后续配置修改在此文件进行：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp settings.template.jsonc settings.jsonc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] 训练以及推理相关配置统一在文件&lt;code&gt;settings.jsonc&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;4.使用以下命令测试CUDA环境是否正确配置并可被PyTorch识别，Mac不需要：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -c &#34;import torch; print(&#39;CUDA是否可用:&#39;, torch.cuda.is_available());&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;5.（可选）安装FlashAttention，加速训练和推理：&lt;code&gt;uv pip install flash-attn --no-build-isolation&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;模型下载&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git lfs install&#xA;git clone https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;下载有问题使用其他方式下载：&lt;a href=&#34;https://www.modelscope.cn/docs/models/download&#34;&gt;模型的下载&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;数据准备&lt;/h2&gt; &#xA;&lt;p&gt;请使用&lt;a href=&#34;https://github.com/xaoyaoo/PyWxDump&#34;&gt;PyWxDump&lt;/a&gt;提取微信聊天记录（不支持4.0版本微信）。可以先将手机的聊天记录迁移（备份）到电脑，数据量更多一些。下载软件并解密数据库后，点击聊天备份，导出类型为CSV，可以导出多个联系人（不建议使用群聊记录），然后将导出的位于&lt;code&gt;wxdump_tmp/export&lt;/code&gt; 的 &lt;code&gt;csv&lt;/code&gt; 文件夹放在&lt;code&gt;./dataset&lt;/code&gt;目录即可，也就是不同人聊天记录的文件夹一起放在 &lt;code&gt;./dataset/csv&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;数据预处理&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;项目默认去除了数据中的手机号、身份证号、邮箱、网址。还在&lt;code&gt;settings.jsonc&lt;/code&gt;中提供了一个禁用词词库&lt;code&gt;blocked_words&lt;/code&gt;，可以自行添加需要过滤的词句（会默认去掉包括禁用词的整句）。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] 🚨 请一定注意保护个人隐私，不要泄露个人信息！&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;执行以下命令对数据进行处理，可以根据自己的聊天风格修改settings.jsonc的&lt;code&gt;make_dataset_args&lt;/code&gt;。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;weclone-cli make-dataset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;目前仅支持时间窗口策略，根据&lt;code&gt;single_combine_time_window&lt;/code&gt;将单人连续消息通过逗号连接合并为一句，根据&lt;code&gt;qa_match_time_window&lt;/code&gt;匹配问答对。&lt;/li&gt; &#xA; &lt;li&gt;可以启用&lt;code&gt;clean_dataset&lt;/code&gt;中的&lt;code&gt;enable_clean&lt;/code&gt;选项，对数据进行清洗，以达到更好效果。当前使用llm judge对聊天记录进行打分，使用vllm进行离线推理。在得到&lt;code&gt;llm打分分数分布情况&lt;/code&gt;后，调整&lt;code&gt;accept_score&lt;/code&gt;选择可以接受的分数，再适当降低&lt;code&gt;train_sft_args&lt;/code&gt;的&lt;code&gt;lora_dropout&lt;/code&gt;参数提升拟合效果。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;配置参数并微调模型&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(可选)修改 &lt;code&gt;settings.jsonc&lt;/code&gt; 的 &lt;code&gt;model_name_or_path&lt;/code&gt; 和 &lt;code&gt;template&lt;/code&gt; 选择本地下载好的其他模型。&lt;/li&gt; &#xA; &lt;li&gt;修改&lt;code&gt;per_device_train_batch_size&lt;/code&gt;以及&lt;code&gt;gradient_accumulation_steps&lt;/code&gt;来调整显存占用。&lt;/li&gt; &#xA; &lt;li&gt;可以根据自己数据集的数量和质量修改&lt;code&gt;train_sft_args&lt;/code&gt;的&lt;code&gt;num_train_epochs&lt;/code&gt;、&lt;code&gt;lora_rank&lt;/code&gt;、&lt;code&gt;lora_dropout&lt;/code&gt;等参数。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;单卡训练&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;weclone-cli train-sft&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;多卡环境单卡训练，需要先执行 &lt;code&gt;export CUDA_VISIBLE_DEVICES=0&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;多卡训练&lt;/h3&gt; &#xA;&lt;p&gt;取消&lt;code&gt;settings.jsonc&lt;/code&gt;中&lt;code&gt;deepspeed&lt;/code&gt;行代码注释，使用以下命令多卡训练：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv pip install deepspeed&#xA;deepspeed --num_gpus=使用显卡数量 weclone/train/train_sft.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;使用浏览器demo简单推理&lt;/h3&gt; &#xA;&lt;p&gt;可以在这一步测试出合适的temperature、top_p值，修改settings.jsonc的&lt;code&gt;infer_args&lt;/code&gt;后，供后续推理时使用。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;weclone-cli webchat-demo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;使用接口进行推理&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;weclone-cli server&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;使用常见聊天问题测试&lt;/h3&gt; &#xA;&lt;p&gt;不包含询问个人信息的问题，仅有日常聊天。测试结果在test_result-my.txt。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;weclone-cli server&#xA;weclone-cli test-model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🖼️ 微调效果&lt;/h2&gt; &#xA;&lt;p&gt;使用Qwen2.5-14B-Instruct模型，大概3万条处理后的有效数据，loss降到了3.5左右的效果。&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;截图&lt;/summary&gt; &#xA; &lt;div style=&#34;display: flex; flex-wrap: wrap; gap: 10px;&#34;&gt; &#xA;  &lt;img src=&#34;https://github.com/user-attachments/assets/0775ec52-452b-485f-9785-c6eb7b277132&#34; alt=&#34;alt text&#34; style=&#34;width: 48%; min-width: 150px;&#34;&gt; &#xA;  &lt;img src=&#34;https://github.com/user-attachments/assets/8c7628b5-da70-4c37-9e51-fdfb0eadd2df&#34; alt=&#34;alt text&#34; style=&#34;width: 48%; min-width: 150px;&#34;&gt; &#xA;  &lt;img src=&#34;https://github.com/user-attachments/assets/523aa742-2aa3-40e9-bd67-b98b336e83a8&#34; alt=&#34;alt text&#34; style=&#34;width: 48%; min-width: 150px;&#34;&gt; &#xA;  &lt;img src=&#34;https://github.com/user-attachments/assets/dabf0603-dcc4-4a47-b5c3-2bbc036820d9&#34; alt=&#34;alt text&#34; style=&#34;width: 48%; min-width: 150px;&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;🤖 部署到聊天机器人&lt;/h2&gt; &#xA;&lt;h3&gt;AstrBot&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AstrBotDevs/AstrBot&#34;&gt;AstrBot&lt;/a&gt; 是易上手的多平台 LLM 聊天机器人及开发框架 ✨ 平台支持 QQ、QQ频道、Telegram、微信、企微、飞书。&lt;/p&gt; &#xA;&lt;p&gt;使用步骤：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;部署 AstrBot&lt;/li&gt; &#xA; &lt;li&gt;在 AstrBot 中部署消息平台&lt;/li&gt; &#xA; &lt;li&gt;执行 &lt;code&gt;weclone-cli server&lt;/code&gt; 启动api服务&lt;/li&gt; &#xA; &lt;li&gt;在 AstrBot 中新增服务提供商，类型选择OpenAI，API Base URL 根据AstrBot部署方式填写（例如docker部署可能为&lt;a href=&#34;http://172.17.0.1:8005/v1%EF%BC%89&#34;&gt;http://172.17.0.1:8005/v1）&lt;/a&gt; ，模型填写gpt-3.5-turbo,API Key随意填写一个&lt;/li&gt; &#xA; &lt;li&gt;微调后不支持工具调用，请先关掉默认的工具，消息平台发送指令： &lt;code&gt;/tool off all&lt;/code&gt;，否则会没有微调后的效果。&lt;/li&gt; &#xA; &lt;li&gt;根据微调时使用的default_system，在 AstrBot 中设置系统提示词。 &lt;img src=&#34;https://github.com/user-attachments/assets/19de7072-076a-4cdf-8ae6-46b9b89f536a&#34; alt=&#34;5&#34;&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] 检查api_service的日志，尽量保证大模型服务请求的参数和微调时一致，tool插件能力都关掉。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;调整采样参数，例如temperature、top_p、top_k等 &lt;a href=&#34;https://astrbot.app/config/model-config.html#%E9%85%8D%E7%BD%AE%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0&#34;&gt;配置自定义的模型参数&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;LangBot&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/RockChinQ/LangBot&#34;&gt;LangBot&lt;/a&gt; 是一个开源的接入全球多种即时通信平台的 LLM 机器人平台，适合各种场景使用。&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/RockChinQ/LangBot#-%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8&#34;&gt;部署 LangBot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;在 LangBot 中添加一个机器人&lt;/li&gt; &#xA; &lt;li&gt;在模型页添加新模型，名称&lt;code&gt;gpt-3.5-turbo&lt;/code&gt;，供应商选择 OpenAI，填写 请求 URL 为 WeClone 的地址，详细连接方式可以参考&lt;a href=&#34;https://docs.langbot.app/zh/workshop/network-details.html&#34;&gt;文档&lt;/a&gt;，API Key 任意填写。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img width=&#34;400px&#34; alt=&#34;image&#34; src=&#34;https://github.com/user-attachments/assets/fc167dea-7c93-4d94-9c5f-db709d0320ba&#34;&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;在流水线配置中选择刚才添加的模型，或修改提示词配置&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img width=&#34;400px&#34; alt=&#34;image&#34; src=&#34;https://github.com/user-attachments/assets/dbb0fd0a-f760-42db-acd0-bb99c859b52e&#34;&gt; &#xA;&lt;h2&gt;📌 路线图&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 更丰富的上下文：包括上下文对话、聊天对象信息、时间等 + 思考&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Memory 支持&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 支持多模态&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 数据增强&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 支持GUI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;问题解决&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;微调问题：&lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/issues/4614&#34;&gt;LLaMA-Factory| FAQs | 常见问题&lt;/a&gt; 或者更方便的 &lt;a href=&#34;https://deepwiki.com/hiyouga/LLaMA-Factory&#34;&gt;&lt;img src=&#34;https://deepwiki.com/badge.svg?sanitize=true&#34; alt=&#34;更方便的Ask DeepWiki&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;❤️ 贡献代码&lt;/h2&gt; &#xA;&lt;p&gt;欢迎任何 Issues/Pull Requests！&lt;/p&gt; &#xA;&lt;p&gt;你可以通过查看Issues或帮助审核 PR（拉取请求）来贡献。对于新功能的添加，请先通过 Issue 讨论。&lt;br&gt; 运行&lt;code&gt;uv pip install --group dev -e .&lt;/code&gt;安装开发依赖。&lt;br&gt; 项目使用&lt;code&gt;pytest&lt;/code&gt;测试(测试脚本待完善)，&lt;code&gt;pyright&lt;/code&gt;检查类型，&lt;code&gt;ruff&lt;/code&gt;检查代码格式。&lt;/p&gt; &#xA;&lt;h2&gt;⚠️ 免责声明&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION] 请勿用于非法用途，否则后果自负。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;1. 使用目的&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;本项目仅供学习交流使用，&lt;strong&gt;请勿用于非法用途&lt;/strong&gt;，&lt;strong&gt;请勿用于非法用途&lt;/strong&gt;，&lt;strong&gt;请勿用于非法用途&lt;/strong&gt;，否则后果自负。&lt;/li&gt; &#xA;  &lt;li&gt;用户理解并同意，任何违反法律法规、侵犯他人合法权益的行为，均与本项目及其开发者无关，后果由用户自行承担。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;使用期限&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;您应该在下载保存使用本项目的24小时内，删除本项目的源代码和程序；超出此期限的任何使用行为，一概与本项目及其开发者无关。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;操作规范&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;本项目仅允许在授权情况下使用数据训练，严禁用于非法目的，否则自行承担所有相关责任；用户如因违反此规定而引发的任何法律责任，将由用户自行承担，与本项目及其开发者无关。&lt;/li&gt; &#xA;  &lt;li&gt;严禁用于窃取他人隐私，严禁用于窃取他人隐私，严禁用于窃取他人隐私，否则自行承担所有相关责任。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;免责声明接受&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;下载、保存、进一步浏览源代码或者下载安装、编译使用本程序，表示你同意本警告，并承诺遵守它;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol start=&#34;5&#34;&gt; &#xA;  &lt;li&gt;禁止用于非法测试或渗透&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;禁止利用本项目的相关技术从事非法测试或渗透，禁止利用本项目的相关代码或相关技术从事任何非法工作，如因此产生的一切不良后果与本项目及其开发者无关。&lt;/li&gt; &#xA;  &lt;li&gt;任何因此产生的不良后果，包括但不限于数据泄露、系统瘫痪、侵犯隐私等，均与本项目及其开发者无关，责任由用户自行承担。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol start=&#34;6&#34;&gt; &#xA;  &lt;li&gt;免责声明修改&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;本免责声明可能根据项目运行情况和法律法规的变化进行修改和调整。用户应定期查阅本页面以获取最新版本的免责声明，使用本项目时应遵守最新版本的免责声明。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol start=&#34;7&#34;&gt; &#xA;  &lt;li&gt;其他&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;除本免责声明规定外，用户在使用本项目过程中应遵守相关的法律法规和道德规范。对于因用户违反相关规定而引发的任何纠纷或损失，本项目及其开发者不承担任何责任。&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;请用户慎重阅读并理解本免责声明的所有内容，确保在使用本项目时严格遵守相关规定。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; 请用户慎重阅读并理解本免责声明的所有内容，确保在使用本项目时严格遵守相关规定。 &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;⭐ Star History&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] 如果本项目对您有帮助，或者您关注本项目的未来发展，请给项目 Star，谢谢&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.star-history.com/#xming521/WeClone&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=xming521/WeClone&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  克隆我们，保留灵魂的芬芳 &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>