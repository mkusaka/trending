<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-06T01:41:15Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Koenkk/zigbee2mqtt</title>
    <updated>2024-10-06T01:41:15Z</updated>
    <id>tag:github.com,2024-10-06:/Koenkk/zigbee2mqtt</id>
    <link href="https://github.com/Koenkk/zigbee2mqtt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Zigbee üêù to MQTT bridge üåâ, get rid of your proprietary Zigbee bridges üî®&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/koenkk/zigbee2mqtt&#34;&gt; &lt;img width=&#34;150&#34; height=&#34;150&#34; src=&#34;https://raw.githubusercontent.com/Koenkk/zigbee2mqtt/master/images/logo.png&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;div style=&#34;display: flex;&#34;&gt; &#xA;  &lt;a href=&#34;https://github.com/Koenkk/zigbee2mqtt/actions?query=workflow%3ACI&#34;&gt; &lt;img src=&#34;https://github.com/koenkk/zigbee2mqtt/workflows/CI/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://github.com/Koenkk/zigbee2mqtt/releases&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release/koenkk/zigbee2mqtt.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://github.com/Koenkk/zigbee2mqtt/stargazers&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/koenkk/zigbee2mqtt.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://www.paypal.me/koenkk&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/donate-PayPal-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://discord.gg/dadfWYE&#34;&gt; &lt;img src=&#34;https://img.shields.io/discord/556563650429583360.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;http://zigbee2mqtt.discourse.group/&#34;&gt; &lt;img src=&#34;https://img.shields.io/discourse/https/zigbee2mqtt.discourse.group/status.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;  &lt;a&gt; &lt;img src=&#34;https://img.shields.io/badge/Coverage-100%25-brightgreen.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://www.codacy.com/manual/Koenkk/zigbee2mqtt?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=Koenkk/zigbee2mqtt&amp;amp;utm_campaign=Badge_Grade&#34;&gt; &lt;img src=&#34;https://api.codacy.com/project/badge/Grade/24f1e0fe39f04daa810e8a1416693d3f&#34;&gt; &lt;/a&gt; &#xA;  &lt;a href=&#34;https://www.npmjs.com/package/zigbee2mqtt&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/v/zigbee2mqtt&#34;&gt; &lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;h1&gt;Zigbee2MQTT üåâ üêù&lt;/h1&gt; &#xA; &lt;p&gt; Allows you to use your Zigbee devices &lt;b&gt;without&lt;/b&gt; the vendor&#39;s bridge or gateway. &lt;/p&gt; &#xA; &lt;p&gt; It bridges events and allows you to control your Zigbee devices via MQTT. In this way you can integrate your Zigbee devices with whatever smart home infrastructure you are using. &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://www.zigbee2mqtt.io/guide/getting-started&#34;&gt;Getting started&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://www.zigbee2mqtt.io/&#34;&gt;documentation&lt;/a&gt; provides you all the information needed to get up and running! Make sure you don&#39;t skip sections if this is your first visit, as there might be important details in there for you.&lt;/p&gt; &#xA;&lt;p&gt;If you aren&#39;t familiar with &lt;strong&gt;Zigbee&lt;/strong&gt; terminology make sure you &lt;a href=&#34;https://www.zigbee2mqtt.io/advanced/zigbee/01_zigbee_network.html&#34;&gt;read this&lt;/a&gt; to help you out.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://www.zigbee2mqtt.io/guide/usage/integrations.html&#34;&gt;Integrations&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Zigbee2MQTT integrates well with (almost) every home automation solution because it uses MQTT. However the following integrations are worth mentioning:&lt;/p&gt; &#xA;&lt;img align=&#34;left&#34; height=&#34;100px&#34; width=&#34;100px&#34; src=&#34;https://user-images.githubusercontent.com/7738048/40914297-49e6e560-6800-11e8-8904-36cce896e5a8.png&#34;&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.home-assistant.io/&#34;&gt;Home Assistant&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.home-assistant.io/installation/&#34;&gt;Home Assistant OS&lt;/a&gt;: Using &lt;a href=&#34;https://github.com/zigbee2mqtt/hassio-zigbee2mqtt&#34;&gt;the official addon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Other installation: using instructions &lt;a href=&#34;https://www.zigbee2mqtt.io/guide/usage/integrations/home_assistant.html&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;img align=&#34;left&#34; height=&#34;100px&#34; width=&#34;100px&#34; src=&#34;https://etc.athom.com/logo/white/256.png&#34;&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://homey.app/&#34;&gt;Homey&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integration implemented in the &lt;a href=&#34;https://homey.app/nl-nl/app/com.gruijter.zigbee2mqtt/&#34;&gt;Homey App&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Documentation and support in the &lt;a href=&#34;https://community.homey.app/t/83214&#34;&gt;Homey Forum&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;img align=&#34;left&#34; height=&#34;100px&#34; width=&#34;100px&#34; src=&#34;https://user-images.githubusercontent.com/2734836/47615848-b8dd8700-dabd-11e8-9d77-175002dd8987.png&#34;&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.domoticz.com/&#34;&gt;Domoticz&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integration implemented in Domoticz (&lt;a href=&#34;https://www.domoticz.com/wiki/Zigbee2MQTT&#34;&gt;documentation&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;img align=&#34;left&#34; height=&#34;100px&#34; width=&#34;100px&#34; src=&#34;https://raw.githubusercontent.com/Koenkk/zigbee2mqtt/master/images/gladys-assistant-logo.jpg&#34;&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://gladysassistant.com/&#34;&gt;Gladys Assistant&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integration implemented natively in Gladys Assistant (&lt;a href=&#34;https://gladysassistant.com/docs/integrations/zigbee2mqtt/&#34;&gt;documentation&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;img align=&#34;left&#34; height=&#34;100px&#34; width=&#34;100px&#34; src=&#34;https://forum.iobroker.net/assets/uploads/system/site-logo.png&#34;&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.iobroker.net/&#34;&gt;IoBroker&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integration implemented in IoBroker (&lt;a href=&#34;https://github.com/o0shojo0o/ioBroker.zigbee2mqtt&#34;&gt;documentation&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Koenkk/zigbee2mqtt/master/images/architecture.png&#34; alt=&#34;Architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Internal Architecture&lt;/h3&gt; &#xA;&lt;p&gt;Zigbee2MQTT is made up of three modules, each developed in its own Github project. Starting from the hardware (adapter) and moving up; &lt;a href=&#34;https://github.com/koenkk/zigbee-herdsman&#34;&gt;zigbee-herdsman&lt;/a&gt; connects to your Zigbee adapter and makes an API available to the higher levels of the stack. For e.g. Texas Instruments hardware, zigbee-herdsman uses the &lt;a href=&#34;https://github.com/koenkk/zigbee-herdsman/raw/master/docs/Z-Stack%20Monitor%20and%20Test%20API.pdf&#34;&gt;TI zStack monitoring and test API&lt;/a&gt; to communicate with the adapter. Zigbee-herdsman handles the core Zigbee communication. The module &lt;a href=&#34;https://github.com/koenkk/zigbee-herdsman-converters&#34;&gt;zigbee-herdsman-converters&lt;/a&gt; handles the mapping from individual device models to the Zigbee clusters they support. &lt;a href=&#34;https://github.com/Koenkk/zigbee-herdsman/raw/master/docs/07-5123-08-Zigbee-Cluster-Library.pdf&#34;&gt;Zigbee clusters&lt;/a&gt; are the layers of the Zigbee protocol on top of the base protocol that define things like how lights, sensors and switches talk to each other over the Zigbee network. Finally, the Zigbee2MQTT module drives zigbee-herdsman and maps the zigbee messages to MQTT messages. Zigbee2MQTT also keeps track of the state of the system. It uses a &lt;code&gt;database.db&lt;/code&gt; file to store this state; a text file with a JSON database of connected devices and their capabilities. Zigbee2MQTT provides a &lt;a href=&#34;https://github.com/nurikk/zigbee2mqtt-frontend&#34;&gt;web-based interface&lt;/a&gt; that allows monitoring and configuration.&lt;/p&gt; &#xA;&lt;h3&gt;Developing&lt;/h3&gt; &#xA;&lt;p&gt;Zigbee2MQTT uses TypeScript (partially for now). Therefore after making changes to files in the &lt;code&gt;lib/&lt;/code&gt; directory you need to recompile Zigbee2MQTT. This can be done by executing &lt;code&gt;npm run build&lt;/code&gt;. For faster development instead of running &lt;code&gt;npm run build&lt;/code&gt; you can run &lt;code&gt;npm run build-watch&lt;/code&gt; in another terminal session, this will recompile as you change files.&lt;/p&gt; &#xA;&lt;h2&gt;Supported devices&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://www.zigbee2mqtt.io/supported-devices&#34;&gt;Supported devices&lt;/a&gt; to check whether your device is supported. There is quite an extensive list, including devices from vendors like &lt;a href=&#34;https://www.zigbee2mqtt.io/supported-devices/#v=Xiaomi&#34;&gt;Xiaomi&lt;/a&gt;, &lt;a href=&#34;https://www.zigbee2mqtt.io/supported-devices/#v=IKEA&#34;&gt;Ikea&lt;/a&gt;, &lt;a href=&#34;https://www.zigbee2mqtt.io/supported-devices/#v=Philips&#34;&gt;Philips&lt;/a&gt;, &lt;a href=&#34;https://www.zigbee2mqtt.io/supported-devices/#v=OSRAM&#34;&gt;OSRAM&lt;/a&gt; and more.&lt;/p&gt; &#xA;&lt;p&gt;If it&#39;s not listed in &lt;a href=&#34;https://www.zigbee2mqtt.io/supported-devices&#34;&gt;Supported devices&lt;/a&gt;, support can be added (fairly) easily, see &lt;a href=&#34;https://www.zigbee2mqtt.io/advanced/support-new-devices/01_support_new_devices.html&#34;&gt;How to support new devices&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Support &amp;amp; help&lt;/h2&gt; &#xA;&lt;p&gt;If you need assistance you can check &lt;a href=&#34;https://github.com/Koenkk/zigbee2mqtt/issues&#34;&gt;opened issues&lt;/a&gt;. Feel free to help with Pull Requests when you were able to fix things or add new devices or just share the love on social media.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>meta-llama/llama-stack</title>
    <updated>2024-10-06T01:41:15Z</updated>
    <id>tag:github.com,2024-10-06:/meta-llama/llama-stack</id>
    <link href="https://github.com/meta-llama/llama-stack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Model components of the Llama Stack APIs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Llama Stack&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/llama_stack/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/llama_stack.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/llama-stack/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/llama-stack&#34; alt=&#34;PyPI - Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/llama-stack&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1257833999603335178&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the Llama Stack API specifications as well as API Providers and Llama Stack Distributions.&lt;/p&gt; &#xA;&lt;p&gt;The Llama Stack defines and standardizes the building blocks needed to bring generative AI applications to market. These blocks span the entire development lifecycle: from model training and fine-tuning, through product evaluation, to building and running AI agents in production. Beyond definition, we are building providers for the Llama Stack APIs. These were developing open-source versions and partnering with providers, ensuring developers can assemble AI solutions using consistent, interlocking pieces across platforms. The ultimate goal is to accelerate innovation in the AI space.&lt;/p&gt; &#xA;&lt;p&gt;The Stack APIs are rapidly improving, but still very much work in progress and we invite feedback as well as direct contributions.&lt;/p&gt; &#xA;&lt;h2&gt;APIs&lt;/h2&gt; &#xA;&lt;p&gt;The Llama Stack consists of the following set of APIs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inference&lt;/li&gt; &#xA; &lt;li&gt;Safety&lt;/li&gt; &#xA; &lt;li&gt;Memory&lt;/li&gt; &#xA; &lt;li&gt;Agentic System&lt;/li&gt; &#xA; &lt;li&gt;Evaluation&lt;/li&gt; &#xA; &lt;li&gt;Post Training&lt;/li&gt; &#xA; &lt;li&gt;Synthetic Data Generation&lt;/li&gt; &#xA; &lt;li&gt;Reward Scoring&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each of the APIs themselves is a collection of REST endpoints.&lt;/p&gt; &#xA;&lt;h2&gt;API Providers&lt;/h2&gt; &#xA;&lt;p&gt;A Provider is what makes the API real -- they provide the actual implementation backing the API.&lt;/p&gt; &#xA;&lt;p&gt;As an example, for Inference, we could have the implementation be backed by open source libraries like &lt;code&gt;[ torch | vLLM | TensorRT ]&lt;/code&gt; as possible options.&lt;/p&gt; &#xA;&lt;p&gt;A provider can also be just a pointer to a remote REST service -- for example, cloud providers or dedicated inference providers could serve these APIs.&lt;/p&gt; &#xA;&lt;h2&gt;Llama Stack Distribution&lt;/h2&gt; &#xA;&lt;p&gt;A Distribution is where APIs and Providers are assembled together to provide a consistent whole to the end application developer. You can mix-and-match providers -- some could be backed by local code and some could be remote. As a hobbyist, you can serve a small model locally, but can choose a cloud provider for a large model. Regardless, the higher level APIs your app needs to work with don&#39;t need to change at all. You can even imagine moving across the server / mobile-device boundary as well always using the same uniform set of APIs for developing Generative AI applications.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Llama Stack Implementations&lt;/h2&gt; &#xA;&lt;h3&gt;API Providers&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;API Provider Builder&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Environments&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Safety&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Telemetry&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meta Reference&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Fireworks&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hosted&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AWS Bedrock&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hosted&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Together&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hosted&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Ollama&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TGI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hosted and Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chroma&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PG Vector&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Single Node&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PyTorch ExecuTorch&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;On-device iOS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Distributions&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Distribution Provider&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Safety&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Telemetry&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meta Reference&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://hub.docker.com/repository/docker/llamastack/llamastack-local-gpu/general&#34;&gt;Local GPU&lt;/a&gt;, &lt;a href=&#34;https://hub.docker.com/repository/docker/llamastack/llamastack-local-cpu/general&#34;&gt;Local CPU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dell-TGI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://hub.docker.com/repository/docker/llamastack/llamastack-local-tgi-chroma/general&#34;&gt;Local TGI + Chroma&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can install this repository as a &lt;a href=&#34;https://pypi.org/project/llama-stack/&#34;&gt;package&lt;/a&gt; with &lt;code&gt;pip install llama-stack&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to install from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p ~/local&#xA;cd ~/local&#xA;git clone git@github.com:meta-llama/llama-stack.git&#xA;&#xA;conda create -n stack python=3.10&#xA;conda activate stack&#xA;&#xA;cd llama-stack&#xA;$CONDA_PREFIX/bin/pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;The Llama CLI&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;llama&lt;/code&gt; CLI makes it easy to work with the Llama Stack set of tools, including installing and running Distributions, downloading models, studying model prompt formats, etc. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-stack/main/docs/cli_reference.md&#34;&gt;CLI reference&lt;/a&gt; for details. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-stack/main/docs/getting_started.md&#34;&gt;Getting Started&lt;/a&gt; guide for running a Llama Stack server.&lt;/p&gt; &#xA;&lt;h2&gt;Llama Stack Client SDK&lt;/h2&gt; &#xA;&lt;p&gt;Check out our client SDKs for connecting to Llama Stack server in your preferred language, you can choose from &lt;a href=&#34;https://github.com/meta-llama/llama-stack-client-python&#34;&gt;python&lt;/a&gt;, &lt;a href=&#34;https://github.com/meta-llama/llama-stack-client-node&#34;&gt;node&lt;/a&gt;, &lt;a href=&#34;https://github.com/meta-llama/llama-stack-client-swift&#34;&gt;swift&lt;/a&gt;, and &lt;a href=&#34;https://github.com/meta-llama/llama-stack-client-kotlin&#34;&gt;kotlin&lt;/a&gt; programming languages to quickly build your applications.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>exo-explore/exo</title>
    <updated>2024-10-06T01:41:15Z</updated>
    <id>tag:github.com,2024-10-06:/exo-explore/exo</id>
    <link href="https://github.com/exo-explore/exo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;/docs/exo-logo-black-bg.jpg&#34;&gt; &#xA;  &lt;img alt=&#34;exo logo&#34; src=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-logo-transparent.png&#34; width=&#34;50%&#34; height=&#34;50%&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href=&#34;https://x.com/exolabs&#34;&gt;exo labs&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h3&gt; &lt;p&gt;&lt;a href=&#34;https://discord.gg/EUnjGpsmWw&#34;&gt;Discord&lt;/a&gt; | &lt;a href=&#34;https://t.me/+Kh-KqHTzFYg3MGNk&#34;&gt;Telegram&lt;/a&gt; | &lt;a href=&#34;https://x.com/exolabs&#34;&gt;X&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/exo-explore/exo/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/exo-explore/exo&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main&#34;&gt;&lt;img src=&#34;https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true&#34; alt=&#34;License: GPL v3&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Forget expensive NVIDIA GPUs, unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, Linux, pretty much any device!&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;Update: exo is hiring. See &lt;a href=&#34;https://exolabs.net&#34;&gt;here&lt;/a&gt; for more details.&lt;/h2&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Get Involved&lt;/h2&gt; &#xA;&lt;p&gt;exo is &lt;strong&gt;experimental&lt;/strong&gt; software. Expect bugs early on. Create issues so they can be fixed. The &lt;a href=&#34;https://x.com/exolabs&#34;&gt;exo labs&lt;/a&gt; team will strive to resolve issues quickly.&lt;/p&gt; &#xA;&lt;p&gt;We also welcome contributions from the community. We have a list of bounties in &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing&#34;&gt;this sheet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;h3&gt;Wide Model Support&lt;/h3&gt; &#xA;&lt;p&gt;exo supports different models including LLaMA (&lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/models/llama.py&#34;&gt;MLX&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/models/llama.py&#34;&gt;tinygrad&lt;/a&gt;), Mistral, LlaVA, Qwen and Deepseek.&lt;/p&gt; &#xA;&lt;h3&gt;Dynamic Model Partitioning&lt;/h3&gt; &#xA;&lt;p&gt;exo &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py&#34;&gt;optimally splits up models&lt;/a&gt; based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.&lt;/p&gt; &#xA;&lt;h3&gt;Automatic Device Discovery&lt;/h3&gt; &#xA;&lt;p&gt;exo will &lt;a href=&#34;https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/standard_node.py#L154&#34;&gt;automatically discover&lt;/a&gt; other devices using the best method available. Zero manual configuration.&lt;/p&gt; &#xA;&lt;h3&gt;ChatGPT-compatible API&lt;/h3&gt; &#xA;&lt;p&gt;exo provides a &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/api/chatgpt_api.py&#34;&gt;ChatGPT-compatible API&lt;/a&gt; for running models. It&#39;s a &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/examples/chatgpt_api.sh&#34;&gt;one-line change&lt;/a&gt; in your application to run models on your own hardware using exo.&lt;/p&gt; &#xA;&lt;h3&gt;Device Equality&lt;/h3&gt; &#xA;&lt;p&gt;Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices &lt;a href=&#34;https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/standard_node.py#L161&#34;&gt;connect p2p&lt;/a&gt;. As long as a device is connected somewhere in the network, it can be used to run models.&lt;/p&gt; &#xA;&lt;p&gt;Exo supports different &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/partitioning_strategy.py&#34;&gt;partitioning strategies&lt;/a&gt; to split up a model across devices. The default partitioning strategy is &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py&#34;&gt;ring memory weighted partitioning&lt;/a&gt;. This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.&lt;/p&gt; &#xA;&lt;p&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img alt=&#34;ring topology&#34; src=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/docs/ring-topology.png&#34; width=&#34;30%&#34; height=&#34;30%&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The current recommended way to install exo is from source.&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python&amp;gt;=3.12.0 is required because of &lt;a href=&#34;https://github.com/exo-explore/exo/issues/5&#34;&gt;issues with asyncio&lt;/a&gt; in previous versions.&lt;/li&gt; &#xA; &lt;li&gt;Linux (with NVIDIA card): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;NVIDIA driver (test with &lt;code&gt;nvidia-smi&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;CUDA (&lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation&#34;&gt;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation&lt;/a&gt;) (test with &lt;code&gt;nvcc --version&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;cuDNN (&lt;a href=&#34;https://developer.nvidia.com/cudnn-downloads&#34;&gt;https://developer.nvidia.com/cudnn-downloads&lt;/a&gt;) (test with &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html#verifying-the-install-on-linux:~:text=at%20a%20time.-,Verifying%20the%20Install%20on%20Linux,Test%20passed!,-Upgrading%20From%20Older&#34;&gt;link&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Hardware Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The only requirement to run exo is to have enough memory across all your devices to fit the entire model into memory. For example, if you are running llama 3.1 8B (fp16), you need 16GB of memory across all devices. Any of the following configurations would work since they each have more than 16GB of memory in total: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;2 x 8GB M3 MacBook Airs&lt;/li&gt; &#xA;   &lt;li&gt;1 x 16GB NVIDIA RTX 4070 Ti Laptop&lt;/li&gt; &#xA;   &lt;li&gt;2 x Raspberry Pi 400 with 4GB of RAM each (running on CPU) + 1 x 8GB Mac Mini&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;exo is designed to run on devices with heterogeneous capabilities. For example, you can have some devices with powerful GPUs and others with integrated GPUs or even CPUs. Adding less capable devices will slow down individual inference latency but will increase the overall throughput of the cluster.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;From source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/exo-explore/exo.git&#xA;cd exo&#xA;pip install -e .&#xA;# alternatively, with venv&#xA;source install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If running on Mac, MLX has an &lt;a href=&#34;https://ml-explore.github.io/mlx/build/html/install.html&#34;&gt;install guide&lt;/a&gt; with troubleshooting steps.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Performance&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There are a number of things users have empirically found to improve performance on Apple Silicon Macs:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Upgrade to the latest version of MacOS 15.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;./configure_mlx.sh&lt;/code&gt;. This runs commands to optimize GPU memory allocation on Apple Silicon Macs.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Example Usage on Multiple MacOS Devices&lt;/h3&gt; &#xA;&lt;h4&gt;Device 1:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;exo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Device 2:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;exo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it! No configuration required - exo will automatically discover the other device(s).&lt;/p&gt; &#xA;&lt;p&gt;exo starts a ChatGPT-like WebUI (powered by &lt;a href=&#34;https://github.com/tinygrad/tinygrad/tree/master/examples/tinychat&#34;&gt;tinygrad tinychat&lt;/a&gt;) on &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For developers, exo also starts a ChatGPT-compatible API endpoint on &lt;a href=&#34;http://localhost:8000/v1/chat/completions&#34;&gt;http://localhost:8000/v1/chat/completions&lt;/a&gt;. Examples with curl:&lt;/p&gt; &#xA;&lt;h4&gt;Llama 3.2 3B:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl http://localhost:8000/v1/chat/completions \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;     &#34;model&#34;: &#34;llama-3.2-3b&#34;,&#xA;     &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is the meaning of exo?&#34;}],&#xA;     &#34;temperature&#34;: 0.7&#xA;   }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Llama 3.1 405B:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl http://localhost:8000/v1/chat/completions \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;     &#34;model&#34;: &#34;llama-3.1-405b&#34;,&#xA;     &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is the meaning of exo?&#34;}],&#xA;     &#34;temperature&#34;: 0.7&#xA;   }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Llava 1.5 7B (Vision Language Model):&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl http://localhost:8000/v1/chat/completions \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;     &#34;model&#34;: &#34;llava-1.5-7b-hf&#34;,&#xA;     &#34;messages&#34;: [&#xA;      {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;          {&#xA;            &#34;type&#34;: &#34;text&#34;,&#xA;            &#34;text&#34;: &#34;What are these?&#34;&#xA;          },&#xA;          {&#xA;            &#34;type&#34;: &#34;image_url&#34;,&#xA;            &#34;image_url&#34;: {&#xA;              &#34;url&#34;: &#34;http://images.cocodataset.org/val2017/000000039769.jpg&#34;&#xA;            }&#xA;          }&#xA;        ]&#xA;      }&#xA;    ],&#xA;     &#34;temperature&#34;: 0.0&#xA;   }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example Usage on Multiple Heterogenous Devices (MacOS + Linux)&lt;/h3&gt; &#xA;&lt;h4&gt;Device 1 (MacOS):&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;exo --inference-engine tinygrad&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we explicitly tell exo to use the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine.&lt;/p&gt; &#xA;&lt;h4&gt;Device 2 (Linux):&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;exo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Linux devices will automatically default to using the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine.&lt;/p&gt; &#xA;&lt;p&gt;You can read about tinygrad-specific env vars &lt;a href=&#34;https://docs.tinygrad.org/env_vars/&#34;&gt;here&lt;/a&gt;. For example, you can configure tinygrad to use the cpu by specifying &lt;code&gt;CLANG=1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Example Usage on a single device with &#34;exo run&#34; command&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;exo run llama-3.2-3b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With a custom prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;exo run llama-3.2-3b --prompt &#34;What is the meaning of exo?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Debugging&lt;/h2&gt; &#xA;&lt;p&gt;Enable debug logs with the DEBUG environment variable (0-9).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;DEBUG=9 exo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine specifically, there is a separate DEBUG flag &lt;code&gt;TINYGRAD_DEBUG&lt;/code&gt; that can be used to enable debug logs (1-6).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;TINYGRAD_DEBUG=2 exo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;On some versions of MacOS/Python, certificates are not installed properly which can lead to SSL errors (e.g. SSL error with huggingface.co). To fix this, run the Install Certificates command, usually:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;/Applications/Python 3.x/Install Certificates.command&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üöß As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it&#39;s ready. If you would like access to the iOS implementation now, please email &lt;a href=&#34;mailto:alex@exolabs.net&#34;&gt;alex@exolabs.net&lt;/a&gt; with your GitHub username explaining your use-case and you will be granted access on GitHub.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Inference Engines&lt;/h2&gt; &#xA;&lt;p&gt;exo supports the following inference engines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/sharded_inference_engine.py&#34;&gt;MLX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/inference.py&#34;&gt;tinygrad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöß &lt;a href=&#34;https://github.com/exo-explore/exo/pull/139&#34;&gt;PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöß &lt;a href=&#34;https://github.com/exo-explore/exo/issues/167&#34;&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Networking Modules&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/grpc&#34;&gt;GRPC&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöß &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/TODO&#34;&gt;Radio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöß &lt;a href=&#34;https://raw.githubusercontent.com/exo-explore/exo/main/TODO&#34;&gt;Bluetooth&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>