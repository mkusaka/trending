<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-19T01:46:24Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ollama-webui/ollama-webui</title>
    <updated>2023-11-19T01:46:24Z</updated>
    <id>tag:github.com,2023-11-19:/ollama-webui/ollama-webui</id>
    <link href="https://github.com/ollama-webui/ollama-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ChatGPT-Style Web UI Client for Ollama ü¶ô&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ollama Web UI: A User-Friendly Web Interface for Chat Interactions üëã&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ollama-webui/ollama-webui?style=social&#34; alt=&#34;GitHub stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/ollama-webui/ollama-webui?style=social&#34; alt=&#34;GitHub forks&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/watchers/ollama-webui/ollama-webui?style=social&#34; alt=&#34;GitHub watchers&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/repo-size/ollama-webui/ollama-webui&#34; alt=&#34;GitHub repo size&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/languages/count/ollama-webui/ollama-webui&#34; alt=&#34;GitHub language count&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/languages/top/ollama-webui/ollama-webui&#34; alt=&#34;GitHub top language&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/ollama-webui/ollama-webui?color=red&#34; alt=&#34;GitHub last commit&#34;&gt; &lt;img src=&#34;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Follama-webui%2Follama-wbui&amp;amp;count_bg=%2379C83D&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=hits&amp;amp;edge_flat=false&#34; alt=&#34;Hits&#34;&gt; &lt;a href=&#34;https://discord.gg/5rJgQTnV4s&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-Ollama_Web_UI-blue?logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/tjbck&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Sponsor&amp;amp;message=%E2%9D%A4&amp;amp;logo=GitHub&amp;amp;color=%23fe8e86&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ChatGPT-Style Web Interface for Ollama ü¶ô&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ollama-webui/ollama-webui/main/demo.gif&#34; alt=&#34;Ollama Web UI Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features ‚≠ê&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;üñ•Ô∏è &lt;strong&gt;Intuitive Interface&lt;/strong&gt;: Our chat interface takes inspiration from ChatGPT, ensuring a user-friendly experience.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üì± &lt;strong&gt;Responsive Design&lt;/strong&gt;: Enjoy a seamless experience on both desktop and mobile devices.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚ö° &lt;strong&gt;Swift Responsiveness&lt;/strong&gt;: Enjoy fast and responsive performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üöÄ &lt;strong&gt;Effortless Setup&lt;/strong&gt;: Install seamlessly using Docker for a hassle-free experience.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üíª &lt;strong&gt;Code Syntax Highlighting&lt;/strong&gt;: Enjoy enhanced code readability with our syntax highlighting feature.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úíÔ∏èüî¢ &lt;strong&gt;Full Markdown and LaTeX Support&lt;/strong&gt;: Elevate your LLM experience with comprehensive Markdown and LaTeX capabilities for enriched interaction.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üì•üóëÔ∏è &lt;strong&gt;Download/Delete Models&lt;/strong&gt;: Easily download or remove models directly from the web UI.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ü§ñ &lt;strong&gt;Multiple Model Support&lt;/strong&gt;: Seamlessly switch between different chat models for diverse interactions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚öôÔ∏è &lt;strong&gt;Many Models Conversations&lt;/strong&gt;: Effortlessly engage with various models simultaneously, harnessing their unique strengths for optimal responses. Enhance your experience by leveraging a diverse set of models in parallel.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ü§ù &lt;strong&gt;OpenAI Model Integration&lt;/strong&gt;: Seamlessly utilize OpenAI models alongside Ollama models for a versatile conversational experience.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üîÑ &lt;strong&gt;Regeneration History Access&lt;/strong&gt;: Easily revisit and explore your entire regeneration history.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üìú &lt;strong&gt;Chat History&lt;/strong&gt;: Effortlessly access and manage your conversation history.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üì§üì• &lt;strong&gt;Import/Export Chat History&lt;/strong&gt;: Seamlessly move your chat data in and out of the platform.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üó£Ô∏è &lt;strong&gt;Voice Input Support&lt;/strong&gt;: Engage with your model through voice interactions; enjoy the convenience of talking to your model directly. Additionally, explore the option for sending voice input automatically after 3 seconds of silence for a streamlined experience.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚öôÔ∏è &lt;strong&gt;Fine-Tuned Control with Advanced Parameters&lt;/strong&gt;: Gain a deeper level of control by adjusting parameters such as temperature and defining your system prompts to tailor the conversation to your specific preferences and needs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üîê &lt;strong&gt;Auth Header Support&lt;/strong&gt;: Effortlessly enhance security by adding Authorization headers to Ollama requests directly from the web UI settings, ensuring access to secured Ollama servers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üîó &lt;strong&gt;External Ollama Server Connection&lt;/strong&gt;: Seamlessly link to an external Ollama server hosted on a different address by configuring the environment variable during the Docker build phase. Additionally, you can also set the external server connection URL from the web UI post-build.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üîí &lt;strong&gt;Backend Reverse Proxy Support&lt;/strong&gt;: Strengthen security by enabling direct communication between Ollama Web UI backend and Ollama, eliminating the need to expose Ollama over LAN.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üåü &lt;strong&gt;Continuous Updates&lt;/strong&gt;: We are committed to improving Ollama Web UI with regular updates and new features.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Install üöÄ&lt;/h2&gt; &#xA;&lt;h3&gt;Installing Both Ollama and Ollama Web UI Using Docker Compose&lt;/h3&gt; &#xA;&lt;p&gt;If you don&#39;t have Ollama installed yet, you can use the provided Docker Compose file for a hassle-free installation. Simply run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up -d --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command will install both Ollama and Ollama Web UI on your system. Ensure to modify the &lt;code&gt;compose.yaml&lt;/code&gt; file for GPU support and Exposing Ollama API outside the container stack if needed.&lt;/p&gt; &#xA;&lt;h3&gt;Installing Ollama Web UI Only&lt;/h3&gt; &#xA;&lt;h4&gt;Prerequisites&lt;/h4&gt; &#xA;&lt;p&gt;Make sure you have the latest version of Ollama installed before proceeding with the installation. You can find the latest version of Ollama at &lt;a href=&#34;https://ollama.ai/&#34;&gt;https://ollama.ai/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Checking Ollama&lt;/h5&gt; &#xA;&lt;p&gt;After installing Ollama, verify that Ollama is running by accessing the following link in your web browser: &lt;a href=&#34;http://127.0.0.1:11434/&#34;&gt;http://127.0.0.1:11434/&lt;/a&gt;. Note that the port number may differ based on your system configuration.&lt;/p&gt; &#xA;&lt;h4&gt;Using Docker üê≥&lt;/h4&gt; &#xA;&lt;p&gt;If Ollama is hosted on your local machine and accessible at &lt;a href=&#34;http://127.0.0.1:11434/&#34;&gt;http://127.0.0.1:11434/&lt;/a&gt;, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway --name ollama-webui --restart always ghcr.io/ollama-webui/ollama-webui:main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, if you prefer to build the container yourself, use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t ollama-webui .&#xA;docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway --name ollama-webui --restart always ollama-webui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Your Ollama Web UI should now be hosted at &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; and accessible over LAN (or Network). Enjoy! üòÑ&lt;/p&gt; &#xA;&lt;h4&gt;Accessing External Ollama on a Different Server&lt;/h4&gt; &#xA;&lt;p&gt;Change &lt;code&gt;OLLAMA_API_BASE_URL&lt;/code&gt; environment variable to match the external Ollama Server url:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d -p 3000:8080 -e OLLAMA_API_BASE_URL=https://example.com/api --name ollama-webui --restart always ghcr.io/ollama-webui/ollama-webui:main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, if you prefer to build the container yourself, use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t ollama-webui .&#xA;docker run -d -p 3000:8080 -e OLLAMA_API_BASE_URL=https://example.com/api --name ollama-webui --restart always ollama-webui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to Build for Static Deployment&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone &amp;amp; Enter the project&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/ollama-webui/ollama-webui.git&#xA;pushd ./ollama-webui/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create and edit &lt;code&gt;.env&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cp -RPp example.env .env&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install node dependencies&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm i&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run in dev mode, or build the site for deployment&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Test in Dev mode:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Build for Deploy:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;#`PUBLIC_API_BASE_URL` will overwrite the value in `.env`&#xA;PUBLIC_API_BASE_URL=&#39;https://example.com/api&#39; npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Test the build with &lt;code&gt;caddy&lt;/code&gt; (or the server of your choice)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl https://webi.sh/caddy | sh&#xA;&#xA;PUBLIC_API_BASE_URL=&#39;https://localhost/api&#39; npm run build&#xA;caddy run --envfile .env --config ./Caddyfile.localhost&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/ollama-webui/ollama-webui/main/TROUBLESHOOTING.md&#34;&gt;TROUBLESHOOTING.md&lt;/a&gt; for information on how to troubleshoot and/or join our &lt;a href=&#34;https://discord.gg/5rJgQTnV4s&#34;&gt;Ollama Web UI Discord community&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s Next? üöÄ&lt;/h2&gt; &#xA;&lt;h3&gt;To-Do List üìù&lt;/h3&gt; &#xA;&lt;p&gt;Here are some exciting tasks on our to-do list:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üîê &lt;strong&gt;Access Control&lt;/strong&gt;: Securely manage requests to Ollama by utilizing the backend as a reverse proxy gateway, ensuring only authenticated users can send specific requests.&lt;/li&gt; &#xA; &lt;li&gt;üß™ &lt;strong&gt;Research-Centric Features&lt;/strong&gt;: Empower researchers in the fields of LLM and HCI with a comprehensive web UI for conducting user studies. Stay tuned for ongoing feature enhancements (e.g., surveys, analytics, and participant tracking) to facilitate their research.&lt;/li&gt; &#xA; &lt;li&gt;üìà &lt;strong&gt;User Study Tools&lt;/strong&gt;: Providing specialized tools, like heat maps and behavior tracking modules, to empower researchers in capturing and analyzing user behavior patterns with precision and accuracy.&lt;/li&gt; &#xA; &lt;li&gt;üìö &lt;strong&gt;Enhanced Documentation&lt;/strong&gt;: Elevate your setup and customization experience with improved, comprehensive documentation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Feel free to contribute and help us make Ollama Web UI even better! üôå&lt;/p&gt; &#xA;&lt;h2&gt;Supporters ‚ú®&lt;/h2&gt; &#xA;&lt;p&gt;A big shoutout to our amazing supporters who&#39;s helping to make this project possible! üôè&lt;/p&gt; &#xA;&lt;h3&gt;Platinum Sponsors ü§ç&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lhkim.com/&#34;&gt;Prof. Lawrence Kim @ SFU&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License üìú&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/ollama-webui/ollama-webui/main/LICENSE&#34;&gt;MIT License&lt;/a&gt; - see the &lt;a href=&#34;https://raw.githubusercontent.com/ollama-webui/ollama-webui/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details. üìÑ&lt;/p&gt; &#xA;&lt;h2&gt;Support üí¨&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, suggestions, or need assistance, please open an issue or join our &lt;a href=&#34;https://discord.gg/5rJgQTnV4s&#34;&gt;Ollama Web UI Discord community&lt;/a&gt; or &lt;a href=&#34;https://discord.gg/ollama&#34;&gt;Ollama Discord community&lt;/a&gt; to connect with us! ü§ù&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Created by &lt;a href=&#34;https://github.com/tjbck&#34;&gt;Timothy J. Baek&lt;/a&gt; - Let&#39;s make Ollama Web UI even more amazing together! üí™&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>udlbook/udlbook</title>
    <updated>2023-11-19T01:46:24Z</updated>
    <id>tag:github.com,2023-11-19:/udlbook/udlbook</id>
    <link href="https://github.com/udlbook/udlbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Understanding Deep Learning - Simon J.D. Prince&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>coqui-ai/TTS</title>
    <updated>2023-11-19T01:46:24Z</updated>
    <id>tag:github.com,2023-11-19:/coqui-ai/TTS</id>
    <link href="https://github.com/coqui-ai/TTS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üê∏üí¨ - a deep learning toolkit for Text-to-Speech, battle-tested in research and production&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;üê∏Coqui.ai News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üì£ ‚ìçTTSv2 is here with 16 languages and better performance across the board.&lt;/li&gt; &#xA; &lt;li&gt;üì£ ‚ìçTTS fine-tuning code is out. Check the &lt;a href=&#34;https://github.com/coqui-ai/TTS/tree/dev/recipes/ljspeech&#34;&gt;example recipes&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üì£ ‚ìçTTS can now stream with &amp;lt;200ms latency.&lt;/li&gt; &#xA; &lt;li&gt;üì£ ‚ìçTTS, our production TTS model that can speak 13 languages, is released &lt;a href=&#34;https://coqui.ai/blog/tts/open_xtts&#34;&gt;Blog Post&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/coqui/xtts&#34;&gt;Demo&lt;/a&gt;, &lt;a href=&#34;https://tts.readthedocs.io/en/dev/models/xtts.html&#34;&gt;Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üì£ &lt;a href=&#34;https://github.com/suno-ai/bark&#34;&gt;üê∂Bark&lt;/a&gt; is now available for inference with unconstrained voice cloning. &lt;a href=&#34;https://tts.readthedocs.io/en/dev/models/bark.html&#34;&gt;Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üì£ You can use &lt;a href=&#34;https://github.com/facebookresearch/fairseq/tree/main/examples/mms&#34;&gt;~1100 Fairseq models&lt;/a&gt; with üê∏TTS.&lt;/li&gt; &#xA; &lt;li&gt;üì£ üê∏TTS now supports üê¢Tortoise with faster inference. &lt;a href=&#34;https://tts.readthedocs.io/en/dev/models/tortoise.html&#34;&gt;Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üì£ &lt;strong&gt;Coqui Studio API&lt;/strong&gt; is landed on üê∏TTS. - &lt;a href=&#34;https://github.com/coqui-ai/TTS/raw/dev/README.md#-python-api&#34;&gt;Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üì£ &lt;a href=&#34;https://docs.coqui.ai/docs&#34;&gt;&lt;strong&gt;Coqui Studio API&lt;/strong&gt;&lt;/a&gt; is live.&lt;/li&gt; &#xA; &lt;li&gt;üì£ Voice generation with prompts - &lt;strong&gt;Prompt to Voice&lt;/strong&gt; - is live on &lt;a href=&#34;https://app.coqui.ai/auth/signin&#34;&gt;&lt;strong&gt;Coqui Studio&lt;/strong&gt;&lt;/a&gt;!! - &lt;a href=&#34;https://coqui.ai/blog/tts/prompt-to-voice&#34;&gt;Blog Post&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üì£ Voice generation with fusion - &lt;strong&gt;Voice fusion&lt;/strong&gt; - is live on &lt;a href=&#34;https://app.coqui.ai/auth/signin&#34;&gt;&lt;strong&gt;Coqui Studio&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üì£ Voice cloning is live on &lt;a href=&#34;https://app.coqui.ai/auth/signin&#34;&gt;&lt;strong&gt;Coqui Studio&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2&#34;&gt; &#xA; &lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png&#34; height=&#34;56&#34;&gt;&lt;/h2&gt; &#xA; &lt;p&gt;&lt;strong&gt;üê∏TTS is a library for advanced Text-to-Speech generation.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;üöÄ Pretrained models in +1100 languages.&lt;/p&gt; &#xA; &lt;p&gt;üõ†Ô∏è Tools for training new models and fine-tuning existing models in any language.&lt;/p&gt; &#xA; &lt;p&gt;üìö Utilities for dataset analysis and curation.&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/5eXr5seRrv&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1037326658807533628?color=%239B59B6&amp;amp;label=chat%20on%20discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MPL-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/TTS&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/TTS.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/coqui-ai/TTS/raw/master/CODE_OF_CONDUCT.md&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667&#34; alt=&#34;Covenant&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/tts&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/tts&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://zenodo.org/badge/latestdoi/265612440&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/265612440.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests0.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests1.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests2.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;a href=&#34;https://tts.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/tts/badge/?version=latest&amp;amp;style=plastic&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üí¨ Where to ask questions&lt;/h2&gt; &#xA;&lt;p&gt;Please use our dedicated channels for questions and discussion. Help is much more valuable if it&#39;s shared publicly so that more people can benefit from it.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Platforms&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üö® &lt;strong&gt;Bug Reports&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/tts/issues&#34;&gt;GitHub Issue Tracker&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üéÅ &lt;strong&gt;Feature Requests &amp;amp; Ideas&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/tts/issues&#34;&gt;GitHub Issue Tracker&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üë©‚Äçüíª &lt;strong&gt;Usage Questions&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üóØ &lt;strong&gt;General Discussion&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/discussions&#34;&gt;GitHub Discussions&lt;/a&gt; or &lt;a href=&#34;https://discord.gg/5eXr5seRrv&#34;&gt;Discord&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üîó Links and Resources&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Links&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üíº &lt;strong&gt;Documentation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://tts.readthedocs.io/en/latest/&#34;&gt;ReadTheDocs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üíæ &lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/tree/dev#install-tts&#34;&gt;TTS/README.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üë©‚Äçüíª &lt;strong&gt;Contributing&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üìå &lt;strong&gt;Road Map&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/issues/378&#34;&gt;Main Development Plans&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üöÄ &lt;strong&gt;Released Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/releases&#34;&gt;TTS Releases&lt;/a&gt; and &lt;a href=&#34;https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models&#34;&gt;Experimental Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üì∞ &lt;strong&gt;Papers&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/erogol/TTS-papers&#34;&gt;TTS Papers&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ü•á TTS Performance&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png&#34; width=&#34;800&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Underlined &#34;TTS*&#34; and &#34;Judy*&#34; are &lt;strong&gt;internal&lt;/strong&gt; üê∏TTS models that are not released open-source. They are here to show the potential. Models prefixed with a dot (.Jofish .Abe and .Janice) are real human voices.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;High-performance Deep Learning models for Text2Speech tasks. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).&lt;/li&gt; &#xA;   &lt;li&gt;Speaker Encoder to compute speaker embeddings efficiently.&lt;/li&gt; &#xA;   &lt;li&gt;Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fast and efficient model training.&lt;/li&gt; &#xA; &lt;li&gt;Detailed training logs on the terminal and Tensorboard.&lt;/li&gt; &#xA; &lt;li&gt;Support for Multi-speaker TTS.&lt;/li&gt; &#xA; &lt;li&gt;Efficient, flexible, lightweight but feature complete &lt;code&gt;Trainer API&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Released and ready-to-use models.&lt;/li&gt; &#xA; &lt;li&gt;Tools to curate Text2Speech datasets under&lt;code&gt;dataset_analysis&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Utilities to use and test your models.&lt;/li&gt; &#xA; &lt;li&gt;Modular (but not too much) code base enabling easy implementation of new ideas.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Implementations&lt;/h2&gt; &#xA;&lt;h3&gt;Spectrogram models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tacotron: &lt;a href=&#34;https://arxiv.org/abs/1703.10135&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tacotron2: &lt;a href=&#34;https://arxiv.org/abs/1712.05884&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Glow-TTS: &lt;a href=&#34;https://arxiv.org/abs/2005.11129&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Speedy-Speech: &lt;a href=&#34;https://arxiv.org/abs/2008.03802&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Align-TTS: &lt;a href=&#34;https://arxiv.org/abs/2003.01950&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FastPitch: &lt;a href=&#34;https://arxiv.org/pdf/2006.06873.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FastSpeech: &lt;a href=&#34;https://arxiv.org/abs/1905.09263&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FastSpeech2: &lt;a href=&#34;https://arxiv.org/abs/2006.04558&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SC-GlowTTS: &lt;a href=&#34;https://arxiv.org/abs/2104.05557&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Capacitron: &lt;a href=&#34;https://arxiv.org/abs/1906.03402&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OverFlow: &lt;a href=&#34;https://arxiv.org/abs/2211.06892&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Neural HMM TTS: &lt;a href=&#34;https://arxiv.org/abs/2108.13320&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Delightful TTS: &lt;a href=&#34;https://arxiv.org/abs/2110.12612&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;End-to-End Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ìçTTS: &lt;a href=&#34;https://coqui.ai/blog/tts/open_xtts&#34;&gt;blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VITS: &lt;a href=&#34;https://arxiv.org/pdf/2106.06103&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üê∏ YourTTS: &lt;a href=&#34;https://arxiv.org/abs/2112.02418&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üê¢ Tortoise: &lt;a href=&#34;https://github.com/neonbjb/tortoise-tts&#34;&gt;orig. repo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üê∂ Bark: &lt;a href=&#34;https://github.com/suno-ai/bark&#34;&gt;orig. repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Attention Methods&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Guided Attention: &lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Forward Backward Decoding: &lt;a href=&#34;https://arxiv.org/abs/1907.09006&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Graves Attention: &lt;a href=&#34;https://arxiv.org/abs/1910.10288&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Double Decoder Consistency: &lt;a href=&#34;https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/&#34;&gt;blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Dynamic Convolutional Attention: &lt;a href=&#34;https://arxiv.org/pdf/1910.10288.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Alignment Network: &lt;a href=&#34;https://arxiv.org/abs/2108.10447&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Speaker Encoder&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GE2E: &lt;a href=&#34;https://arxiv.org/abs/1710.10467&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Angular Loss: &lt;a href=&#34;https://arxiv.org/pdf/2003.11982.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Vocoders&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MelGAN: &lt;a href=&#34;https://arxiv.org/abs/1910.06711&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MultiBandMelGAN: &lt;a href=&#34;https://arxiv.org/abs/2005.05106&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ParallelWaveGAN: &lt;a href=&#34;https://arxiv.org/abs/1910.11480&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GAN-TTS discriminators: &lt;a href=&#34;https://arxiv.org/abs/1909.11646&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WaveRNN: &lt;a href=&#34;https://github.com/fatchord/WaveRNN/&#34;&gt;origin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WaveGrad: &lt;a href=&#34;https://arxiv.org/abs/2009.00713&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;HiFiGAN: &lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;UnivNet: &lt;a href=&#34;https://arxiv.org/abs/2106.07889&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Voice Conversion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FreeVC: &lt;a href=&#34;https://arxiv.org/abs/2210.15418&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also help us implement more models.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;üê∏TTS is tested on Ubuntu 18.04 with &lt;strong&gt;python &amp;gt;= 3.9, &amp;lt; 3.12.&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are only interested in &lt;a href=&#34;https://tts.readthedocs.io/en/latest/inference.html&#34;&gt;synthesizing speech&lt;/a&gt; with the released üê∏TTS models, installing from PyPI is the easiest option.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install TTS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you plan to code or train models, clone üê∏TTS and install it locally.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/coqui-ai/TTS&#xA;pip install -e .[all,dev,notebooks]  # Select the relevant extras&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are on Ubuntu (Debian), you can also run following commands for installation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a different OS.&#xA;$ make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are on Windows, üëë@GuyPaddock wrote installation instructions &lt;a href=&#34;https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Docker Image&lt;/h2&gt; &#xA;&lt;p&gt;You can also try TTS without install with the docker image. Simply run the following command and you will be able to run TTS without installing it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu&#xA;python3 TTS/server/server.py --list_models #To get the list of available models&#xA;python3 TTS/server/server.py --model_name tts_models/en/vctk/vits # To start a server&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then enjoy the TTS server &lt;a href=&#34;http://%5B::1%5D:5002/&#34;&gt;here&lt;/a&gt; More details about the docker images (like GPU support) can be found &lt;a href=&#34;https://tts.readthedocs.io/en/latest/docker_images.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Synthesizing speech by üê∏TTS&lt;/h2&gt; &#xA;&lt;h3&gt;üêç Python API&lt;/h3&gt; &#xA;&lt;h4&gt;Running a multi-speaker and multi-lingual model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from TTS.api import TTS&#xA;&#xA;# Get device&#xA;device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;&#xA;# List available üê∏TTS models&#xA;print(TTS().list_models())&#xA;&#xA;# Init TTS&#xA;tts = TTS(&#34;tts_models/multilingual/multi-dataset/xtts_v2&#34;).to(device)&#xA;&#xA;# Run TTS&#xA;# ‚ùó Since this model is multi-lingual voice cloning model, we must set the target speaker_wav and language&#xA;# Text to speech list of amplitude values as output&#xA;wav = tts.tts(text=&#34;Hello world!&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;en&#34;)&#xA;# Text to speech to a file&#xA;tts.tts_to_file(text=&#34;Hello world!&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;en&#34;, file_path=&#34;output.wav&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Running a single speaker model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Init TTS with the target model name&#xA;tts = TTS(model_name=&#34;tts_models/de/thorsten/tacotron2-DDC&#34;, progress_bar=False).to(device)&#xA;&#xA;# Run TTS&#xA;tts.tts_to_file(text=&#34;Ich bin eine Testnachricht.&#34;, file_path=OUTPUT_PATH)&#xA;&#xA;# Example voice cloning with YourTTS in English, French and Portuguese&#xA;tts = TTS(model_name=&#34;tts_models/multilingual/multi-dataset/your_tts&#34;, progress_bar=False).to(device)&#xA;tts.tts_to_file(&#34;This is voice cloning.&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;en&#34;, file_path=&#34;output.wav&#34;)&#xA;tts.tts_to_file(&#34;C&#39;est le clonage de la voix.&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;fr-fr&#34;, file_path=&#34;output.wav&#34;)&#xA;tts.tts_to_file(&#34;Isso √© clonagem de voz.&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;pt-br&#34;, file_path=&#34;output.wav&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example voice conversion&lt;/h4&gt; &#xA;&lt;p&gt;Converting the voice in &lt;code&gt;source_wav&lt;/code&gt; to the voice of &lt;code&gt;target_wav&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tts = TTS(model_name=&#34;voice_conversion_models/multilingual/vctk/freevc24&#34;, progress_bar=False).to(&#34;cuda&#34;)&#xA;tts.voice_conversion_to_file(source_wav=&#34;my/source.wav&#34;, target_wav=&#34;my/target.wav&#34;, file_path=&#34;output.wav&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example voice cloning together with the voice conversion model.&lt;/h4&gt; &#xA;&lt;p&gt;This way, you can clone voices by using any model in üê∏TTS.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;tts = TTS(&#34;tts_models/de/thorsten/tacotron2-DDC&#34;)&#xA;tts.tts_with_vc_to_file(&#xA;    &#34;Wie sage ich auf Italienisch, dass ich dich liebe?&#34;,&#xA;    speaker_wav=&#34;target/speaker.wav&#34;,&#xA;    file_path=&#34;output.wav&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example using &lt;a href=&#34;https://coqui.ai&#34;&gt;üê∏Coqui Studio&lt;/a&gt; voices.&lt;/h4&gt; &#xA;&lt;p&gt;You access all of your cloned voices and built-in speakers in &lt;a href=&#34;https://coqui.ai&#34;&gt;üê∏Coqui Studio&lt;/a&gt;. To do this, you&#39;ll need an API token, which you can obtain from the &lt;a href=&#34;https://coqui.ai/account&#34;&gt;account page&lt;/a&gt;. After obtaining the API token, you&#39;ll need to configure the COQUI_STUDIO_TOKEN environment variable.&lt;/p&gt; &#xA;&lt;p&gt;Once you have a valid API token in place, the studio speakers will be displayed as distinct models within the list. These models will follow the naming convention &lt;code&gt;coqui_studio/en/&amp;lt;studio_speaker_name&amp;gt;/coqui_studio&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# XTTS model&#xA;models = TTS(cs_api_model=&#34;XTTS&#34;).list_models()&#xA;# Init TTS with the target studio speaker&#xA;tts = TTS(model_name=&#34;coqui_studio/en/Torcull Diarmuid/coqui_studio&#34;, progress_bar=False)&#xA;# Run TTS&#xA;tts.tts_to_file(text=&#34;This is a test.&#34;, language=&#34;en&#34;, file_path=OUTPUT_PATH)&#xA;&#xA;# V1 model&#xA;models = TTS(cs_api_model=&#34;V1&#34;).list_models()&#xA;# Run TTS with emotion and speed control&#xA;# Emotion control only works with V1 model&#xA;tts.tts_to_file(text=&#34;This is a test.&#34;, file_path=OUTPUT_PATH, emotion=&#34;Happy&#34;, speed=1.5)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example text to speech using &lt;strong&gt;Fairseq models in ~1100 languages&lt;/strong&gt; ü§Ø.&lt;/h4&gt; &#xA;&lt;p&gt;For Fairseq models, use the following name format: &lt;code&gt;tts_models/&amp;lt;lang-iso_code&amp;gt;/fairseq/vits&lt;/code&gt;. You can find the language ISO codes &lt;a href=&#34;https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html&#34;&gt;here&lt;/a&gt; and learn about the Fairseq models &lt;a href=&#34;https://github.com/facebookresearch/fairseq/tree/main/examples/mms&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# TTS with on the fly voice conversion&#xA;api = TTS(&#34;tts_models/deu/fairseq/vits&#34;)&#xA;api.tts_with_vc_to_file(&#xA;    &#34;Wie sage ich auf Italienisch, dass ich dich liebe?&#34;,&#xA;    speaker_wav=&#34;target/speaker.wav&#34;,&#xA;    file_path=&#34;output.wav&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Command-line &lt;code&gt;tts&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;!-- begin-tts-readme --&gt; &#xA;&lt;p&gt;Synthesize speech on command line.&lt;/p&gt; &#xA;&lt;p&gt;You can either use your trained model or choose a model from the provided list.&lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t specify any models, then it uses LJSpeech based English model.&lt;/p&gt; &#xA;&lt;h4&gt;Single Speaker Models&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;List provided models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --list_models&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Get model info (for both tts_models and vocoder_models):&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Query by type/name: The model_info_by_name uses the name as it from the --list_models.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_name &#34;&amp;lt;model_type&amp;gt;/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_name tts_models/tr/common-voice/glow-tts&#xA;$ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Query by type/idx: The model_query_idx uses the corresponding idx from --list_models.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_idx &#34;&amp;lt;model_type&amp;gt;/&amp;lt;model_query_idx&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_idx tts_models/3&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Query info for model info by full name:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_info_by_name &#34;&amp;lt;model_type&amp;gt;/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run TTS with default models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run TTS and pipe out the generated TTS wav file data:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --pipe_out --out_path output/path/speech.wav | aplay&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run TTS and define speed factor to use for üê∏Coqui Studio models, between 0.0 and 2.0:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;coqui_studio/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34; --speed 1.2 --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run a TTS model with its default vocoder model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;&amp;lt;model_type&amp;gt;/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;tts_models/en/ljspeech/glow-tts&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run with specific TTS and vocoder models from the list:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;&amp;lt;model_type&amp;gt;/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34; --vocoder_name &#34;&amp;lt;model_type&amp;gt;/&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;tts_models/en/ljspeech/glow-tts&#34; --vocoder_name &#34;vocoder_models/en/ljspeech/univnet&#34; --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run your own TTS model (Using Griffin-Lim Vocoder):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run your own TTS and Vocoder models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav&#xA;    --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Multi-speaker Models&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;List the available speakers and choose a &amp;lt;speaker_id&amp;gt; among them:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_name &#34;&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34;  --list_speaker_idxs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the multi-speaker TTS model with the target speaker ID:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS.&#34; --out_path output/path/speech.wav --model_name &#34;&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34;  --speaker_idx &amp;lt;speaker_id&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run your own multi-speaker TTS model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav --model_path path/to/model.pth --config_path path/to/config.json --speakers_file_path path/to/speaker.json --speaker_idx &amp;lt;speaker_id&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Voice Conversion Models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ tts --out_path output/path/speech.wav --model_name &#34;&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34; --source_wav &amp;lt;path/to/speaker/wav&amp;gt; --target_wav &amp;lt;path/to/reference/wav&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- end-tts-readme --&gt; &#xA;&lt;h2&gt;Directory Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)&#xA;|- utils/           (common utilities.)&#xA;|- TTS&#xA;    |- bin/             (folder for all the executables.)&#xA;      |- train*.py                  (train your target model.)&#xA;      |- ...&#xA;    |- tts/             (text to speech models)&#xA;        |- layers/          (model layer definitions)&#xA;        |- models/          (model definitions)&#xA;        |- utils/           (model specific utilities.)&#xA;    |- speaker_encoder/ (Speaker Encoder models.)&#xA;        |- (same)&#xA;    |- vocoder/         (Vocoder models.)&#xA;        |- (same)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>