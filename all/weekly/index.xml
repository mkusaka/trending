<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-21T03:23:34Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>danny-avila/LibreChat</title>
    <updated>2024-04-21T03:23:34Z</updated>
    <id>tag:github.com,2024-04-21:/danny-avila/LibreChat</id>
    <link href="https://github.com/danny-avila/LibreChat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Enhanced ChatGPT Clone: Features OpenAI, Assistants API, Azure, Groq, GPT-4 Vision, Mistral, Bing, Anthropic, OpenRouter, Vertex AI, Gemini, AI model switching, message search, langchain, DALL-E-3, ChatGPT Plugins, OpenAI Functions, Secure Multi-User System, Presets, completely open-source for self-hosting. More features in development&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://librechat.ai&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danny-avila/LibreChat/main/docs/assets/LibreChat.svg?sanitize=true&#34; height=&#34;256&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;a href=&#34;https://librechat.ai&#34;&gt;LibreChat&lt;/a&gt; &lt;/h1&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.librechat.ai&#34;&gt; &lt;img src=&#34;https://img.shields.io/discord/1086345563026489514?label=&amp;amp;logo=discord&amp;amp;style=for-the-badge&amp;amp;logoWidth=20&amp;amp;logoColor=white&amp;amp;labelColor=000000&amp;amp;color=blueviolet&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/@LibreChat&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/YOUTUBE-red.svg?style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white&amp;amp;labelColor=000000&amp;amp;logoWidth=20&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://docs.librechat.ai&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/DOCS-blue.svg?style=for-the-badge&amp;amp;logo=read-the-docs&amp;amp;logoColor=white&amp;amp;labelColor=000000&amp;amp;logoWidth=20&#34;&gt; &lt;/a&gt; &lt;a aria-label=&#34;Sponsors&#34; href=&#34;https://github.com/sponsors/danny-avila&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/SPONSORS-brightgreen.svg?style=for-the-badge&amp;amp;logo=github-sponsors&amp;amp;logoColor=white&amp;amp;labelColor=000000&amp;amp;logoWidth=20&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://railway.app/template/b5k2mn?referralCode=myKrVZ&#34;&gt; &lt;img src=&#34;https://railway.app/button.svg?sanitize=true&#34; alt=&#34;Deploy on Railway&#34; height=&#34;30&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://zeabur.com/templates/0X2ZY8&#34;&gt; &lt;img src=&#34;https://zeabur.com/button.svg?sanitize=true&#34; alt=&#34;Deploy on Zeabur&#34; height=&#34;30&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://template.cloud.sealos.io/deploy?templateName=librechat&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/labring-actions/templates/main/Deploy-on-Sealos.svg?sanitize=true&#34; alt=&#34;Deploy on Sealos&#34; height=&#34;30&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;📃 Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🖥️ UI matching ChatGPT, including Dark mode, Streaming, and latest updates&lt;/li&gt; &#xA; &lt;li&gt;💬 Multimodal Chat: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Upload and analyze images with Claude 3, GPT-4, and Gemini Vision 📸&lt;/li&gt; &#xA;   &lt;li&gt;Chat with Files using Custom Endpoints, OpenAI, Azure, Anthropic, &amp;amp; Google. 🗃️&lt;/li&gt; &#xA;   &lt;li&gt;Advanced Agents with Files, Code Interpreter, Tools, and API Actions 🔦 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Available through the &lt;a href=&#34;https://platform.openai.com/docs/assistants/overview&#34;&gt;OpenAI Assistants API&lt;/a&gt; 🌤️&lt;/li&gt; &#xA;     &lt;li&gt;Non-OpenAI Agents in Active Development 🚧&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;🌎 Multilingual UI: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;English, 中文, Deutsch, Español, Français, Italiano, Polski, Português Brasileiro,&lt;/li&gt; &#xA;   &lt;li&gt;Русский, 日本語, Svenska, 한국어, Tiếng Việt, 繁體中文, العربية, Türkçe, Nederlands, עברית&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;🤖 AI model selection: OpenAI, Azure OpenAI, BingAI, ChatGPT, Google Vertex AI, Anthropic (Claude), Plugins, Assistants API (including Azure Assistants)&lt;/li&gt; &#xA; &lt;li&gt;💾 Create, Save, &amp;amp; Share Custom Presets&lt;/li&gt; &#xA; &lt;li&gt;🔄 Edit, Resubmit, and Continue messages with conversation branching&lt;/li&gt; &#xA; &lt;li&gt;📤 Export conversations as screenshots, markdown, text, json.&lt;/li&gt; &#xA; &lt;li&gt;🔍 Search all messages/conversations&lt;/li&gt; &#xA; &lt;li&gt;🔌 Plugins, including web access, image generation with DALL-E-3 and more&lt;/li&gt; &#xA; &lt;li&gt;👥 Multi-User, Secure Authentication with Moderation and Token spend tools&lt;/li&gt; &#xA; &lt;li&gt;⚙️ Configure Proxy, Reverse Proxy, Docker, &amp;amp; many Deployment options&lt;/li&gt; &#xA; &lt;li&gt;📖 Completely Open-Source &amp;amp; Built in Public&lt;/li&gt; &#xA; &lt;li&gt;🧑‍🤝‍🧑 Community-driven development, support, and feedback&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.librechat.ai/features/plugins/introduction.html&#34;&gt;For a thorough review of our features, see our docs here&lt;/a&gt; 📚&lt;/p&gt; &#xA;&lt;h2&gt;🪶 All-In-One AI Conversations with LibreChat&lt;/h2&gt; &#xA;&lt;p&gt;LibreChat brings together the future of assistant AIs with the revolutionary technology of OpenAI&#39;s ChatGPT. Celebrating the original styling, LibreChat gives you the ability to integrate multiple AI models. It also integrates and enhances original client features such as conversation and message search, prompt templates and plugins.&lt;/p&gt; &#xA;&lt;p&gt;With LibreChat, you no longer need to opt for ChatGPT Plus and can instead use free or pay-per-call APIs. We welcome contributions, cloning, and forking to enhance the capabilities of this advanced chatbot platform.&lt;/p&gt; &#xA;&lt;!-- https://github.com/danny-avila/LibreChat/assets/110412045/c1eb0c0f-41f6-4335-b982-84b278b53d59 --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/pNIOs1ovsXw&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/pNIOs1ovsXw/maxresdefault.jpg&#34; alt=&#34;Watch the video&#34;&gt;&lt;/a&gt; Click on the thumbnail to open the video☝️&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;📚 Documentation&lt;/h2&gt; &#xA;&lt;p&gt;For more information on how to use our advanced features, install and configure our software, and access our guidelines and tutorials, please check out our documentation at &lt;a href=&#34;https://docs.librechat.ai&#34;&gt;docs.librechat.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;📝 Changelog&lt;/h2&gt; &#xA;&lt;p&gt;Keep up with the latest updates by visiting the releases page - &lt;a href=&#34;https://github.com/danny-avila/LibreChat/releases&#34;&gt;Releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;⚠️ &lt;a href=&#34;https://raw.githubusercontent.com/danny-avila/LibreChat/main/docs/general_info/breaking_changes.md&#34;&gt;Breaking Changes&lt;/a&gt;&lt;/strong&gt; Please consult the breaking changes before updating.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;⭐ Star History&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://trendshift.io/repositories/4685&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/4685&#34; alt=&#34;danny-avila%2FLibreChat | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;a href=&#34;https://star-history.com/#danny-avila/LibreChat&amp;amp;Date&#34;&gt; &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=danny-avila/LibreChat&amp;amp;type=Date&amp;amp;theme=dark&#34; onerror=&#34;this.src=&#39;https://api.star-history.com/svg?repos=danny-avila/LibreChat&amp;amp;type=Date&#39;&#34;&gt; &lt;/a&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;✨ Contributions&lt;/h2&gt; &#xA;&lt;p&gt;Contributions, suggestions, bug reports and fixes are welcome!&lt;/p&gt; &#xA;&lt;p&gt;For new features, components, or extensions, please open an issue and discuss before sending a PR.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;💖 This project exists in its current state thanks to all the people who contribute&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/danny-avila/LibreChat/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=danny-avila/LibreChat&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>aixcoder-plugin/aiXcoder-7B</title>
    <updated>2024-04-21T03:23:34Z</updated>
    <id>tag:github.com,2024-04-21:/aixcoder-plugin/aiXcoder-7B</id>
    <link href="https://github.com/aixcoder-plugin/aiXcoder-7B" rel="alternate"></link>
    <summary type="html">&lt;p&gt;official repository of aiXcoder-7B Code Large Language Model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;aiXcoder-7B Code Large Language Model&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 🏠 &lt;a href=&#34;https://www.aixcoder.com/&#34; target=&#34;_blank&#34;&gt;Official website&lt;/a&gt;｜🛠 &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=aixcoder-plugin.aixcoder&#34; target=&#34;_blank&#34;&gt;VS Code Plugin&lt;/a&gt;｜🛠 &lt;a href=&#34;https://plugins.jetbrains.com/plugin/13574-aixcoder-code-completer&#34; target=&#34;_blank&#34;&gt;Jetbrains Plugin&lt;/a&gt;｜🤗 &lt;a href=&#34;https://huggingface.co/aiXcoder/aixcoder-7b-base&#34; target=&#34;_blank&#34;&gt;Model Weights&lt;/a&gt;｜&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/wechat_1.jpg&#34; target=&#34;_blank&#34;&gt;WeChat&lt;/a&gt;｜&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/wechat_2.jpg&#34; target=&#34;_blank&#34;&gt;WeChat Official Account&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Welcome to the official repository of aiXcoder-7B Code Large Language Model. This model is designed to understand and generate code across multiple programming languages, offering state-of-the-art performance in code completion, comprehension, generation, and more tasks about programming languages.&lt;/p&gt; &#xA;&lt;p&gt;Table of Contents&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#model-introduction&#34;&gt;Model Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#environment-requirements&#34;&gt;Environment Requirements&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#model-weights&#34;&gt;Model Weights&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#inference-example&#34;&gt;Inference Example&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#quantized-through-bitsandbytes&#34;&gt;Quantized through bitsandbytes&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#fine-tuning-example&#34;&gt;Fine-tuning example&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#data-for-aixcoder-7b&#34;&gt;Data for aiXcoder 7B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#training&#34;&gt;Training&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#training-hyperparameters&#34;&gt;Training Hyperparameters&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#batch-processing-method&#34;&gt;Batch processing method&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#pre-training-tasks&#34;&gt;Pre-training Tasks&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#details-of-experimental-results&#34;&gt;Details of Experimental Results&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#nl2code-benchmarks&#34;&gt;NL2Code Benchmarks&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#code-completion-fill-in-the-middle&#34;&gt;Code Completion (Fill in the Middle)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#cross-file-code-evaluation&#34;&gt;Cross-file Code Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#acknowledgments&#34;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Model Introduction&lt;/h2&gt; &#xA;&lt;p&gt;As the capabilities of large code models are gradually being unearthed, aiXcoder has consistently pondered on how to make these models more beneficial in real development scenarios. To this end, we have open-sourced aiXcoder 7B Base, which has undergone extensive training on 1.2T Unique Tokens, and the model&#39;s pre-training tasks as well as the contextual information have been uniquely designed for real-world code generation contexts.&lt;/p&gt; &#xA;&lt;p&gt;aiXcoder 7B Base stands out as the most effective model in code completion scenarios among all models of similar parameter sizes, and it also surpasses mainstream models like codellama 34B and StarCoder2 15B in the average performance on the multilingual nl2code benchmark.&lt;/p&gt; &#xA;&lt;p&gt;In our ongoing exploration to apply large code models, the release of aiXcoder 7B Base represents a significant milestone. The current version of aiXcoder 7B Base is a foundational model that focuses on improving the efficiency and accuracy of code completion and code generation tasks, aiming to provide robust support for developers in these scenarios. It is important to note that this version has not undergone specific instruct-tuning, which means it might not yet offer optimal performance for specialized higher-level tasks such as test case generation and code debugging.&lt;/p&gt; &#xA;&lt;p&gt;However, we have plans for further development of the aiXcoder model series already in motion. In the near future, we aim to release new versions of the model that have been meticulously instruct-tuned for a wider range of programming tasks, including but not limited to test case generation and code debugging. Through these instruct-tuned models, we anticipate offering developers more comprehensive and deeper programming support, helping them to maximize efficiency at every stage of software development.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_1.png&#34; alt=&#34;table_1&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;aiXcoder 7B surpasses mainstream models in nl2code benchmark. aiXcoder-7B is an enhancement of aiXcoder-7B-Base, fine-tuned on one hundred thousand data entries similar to Evol-instruct for one epoch.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_3.png&#34; alt=&#34;table_3&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;aiXcoder 7B Base surpasses mainstream models in code completion scenarios.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Environment Requirements&lt;/h3&gt; &#xA;&lt;h4&gt;Option 1: Build Env&lt;/h4&gt; &#xA;&lt;p&gt;To run the model inference code, you&#39;ll need the following environment setup:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8 or higher&lt;/li&gt; &#xA; &lt;li&gt;PyTorch 2.1.0 or higher&lt;/li&gt; &#xA; &lt;li&gt;sentencepiece 0.2.0 or higher&lt;/li&gt; &#xA; &lt;li&gt;transformers 4.34.1 or higher (if run inference by transformers library)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please ensure all dependencies are installed using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n aixcoder-7b python=3.11&#xA;conda activate aixcoder-7b&#xA;git clone git@github.com:aixcoder-plugin/aiXcoder-7b.git&#xA;cd aiXcoder-7b&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;requirements.txt&lt;/code&gt; listed all necessary libraries and their versions.&lt;/p&gt; &#xA;&lt;p&gt;To achieve faster inference speeds, especially for large models, we recommend installing &lt;code&gt;flash attention&lt;/code&gt;. &lt;code&gt;Flash attention&lt;/code&gt; is an optimized attention mechanism that significantly reduces computation time for transformer-based models without sacrificing accuracy.&lt;/p&gt; &#xA;&lt;p&gt;Before proceeding, ensure your environment meets the CUDA requirements as &lt;code&gt;flash attention&lt;/code&gt; leverages GPU acceleration. Follow these steps to install &lt;code&gt;flash attention&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:Dao-AILab/flash-attention.git&#xA;cd flash-attention&#xA;MAX_JOBS=8 python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Option 2: Docker&lt;/h4&gt; &#xA;&lt;p&gt;For a consistent and isolated environment, we recommend running the model inference code using Docker. Here&#39;s how to set up and use Docker for our model:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Docker: If you haven&#39;t already, install Docker on your machine.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pull the Docker Image: Pull the Docker image from Docker Hub.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run the Container: Once the image is pulled, you can run the model inside a Docker container.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all -it -v /dev/shm:/dev/shm --name aix_instance pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel /bin/bash&#xA;pip install sentencepiece&#xA;git clone git@github.com:aixcoder-plugin/aiXcoder-7b.git&#xA;cd aiXcoder-7b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command starts a container named aix_instance from the pytorch image. You can interact with the model inside this container.&lt;/p&gt; &#xA;&lt;p&gt;To achieve faster inference speeds, especially for large models, we recommend installing &lt;code&gt;flash attention&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:Dao-AILab/flash-attention.git&#xA;cd flash-attention&#xA;MAX_JOBS=8 python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Model Inference: Within the Docker container, you can run the model inference code as described in the Inference Example section.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Using Docker provides a clean, controlled environment that minimizes issues related to software versions and dependencies.&lt;/p&gt; &#xA;&lt;h3&gt;Model Weights&lt;/h3&gt; &#xA;&lt;p&gt;You can download the model weights from the following link:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/aiXcoder/aixcoder-7b-base&#34;&gt;aiXcoder Base Download&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;aiXcoder Instruct Download (Comming soon...)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Inference Example&lt;/h3&gt; &#xA;&lt;h4&gt;Command Line Execution&lt;/h4&gt; &#xA;&lt;p&gt;For a quick start, you can run the model inference directly from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node 1 sess_megatron.py --model_dir &#34;path/to/model_weights_dir&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &#34;path/to/model_weights_dir&#34; with the actual path to your downloaded model weights.&lt;/p&gt; &#xA;&lt;p&gt;or run inference with huggingface&#39;s transformers：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python sess_huggingface.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Python Script Execution&lt;/h4&gt; &#xA;&lt;p&gt;Alternatively, you can invoke the model programmatically within your Python scripts. This method provides more flexibility for integrating the model into your applications or workflows. Here&#39;s a simple example on how to do it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from sess_megatron import TestInference&#xA;&#xA;infer = TestInference()&#xA;res = infer.run_infer(&#xA;    # for FIM style input, code_string stands for prefix context&#xA;    code_string=&#34;&#34;&#34;# 快速排序算法&#34;&#34;&#34;, &#xA;    # for FIM style input, later_code stands for suffix context&#xA;    later_code=&#34;\n&#34;,&#xA;    # file_path should be a path from project to file&#xA;    file_path=&#34;test.py&#34;,&#xA;    # max num for generated tokens&#xA;    max_new_tokens=256,&#xA;)&#xA;print(res)&#xA;&#xA;&#34;&#34;&#34;output:&#xA;&#xA;def quick_sort(arr):&#xA;    if len(arr) &amp;lt;= 1:&#xA;        return arr&#xA;    pivot = arr[0]&#xA;    less = [i for i in arr[1:] if i &amp;lt;= pivot]&#xA;    greater = [i for i in arr[1:] if i &amp;gt; pivot]&#xA;    return quick_sort(less) + [pivot] + quick_sort(greater)&#xA;&#xA;&#xA;# 测试&#xA;arr = [3, 2, 1, 4, 5]&#xA;print(quick_sort(arr))  # [1, 2, 3, 4, 5]&#xA;&#34;&#34;&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;&#xA;import torch&#xA;import sys&#xA;from hf_mini.utils import input_wrapper&#xA;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;aiXcoder/aixcoder-7b-base&#34;)&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;aiXcoder/aixcoder-7b-base&#34;, torch_dtype=torch.bfloat16)&#xA;&#xA;&#xA;text = input_wrapper(&#xA;    # for FIM style input, code_string stands for prefix context&#xA;    code_string=&#34;# 快速排序算法&#34;,&#xA;    # for FIM style input, later_code stands for suffix context&#xA;    later_code=&#34;\n# 测试\narr = [3, 2, 1, 4, 5]\nprint(quick_sort(arr))  # [1, 2, 3, 4, 5]&#34;,&#xA;    # file_path should be a path from project to file&#xA;    path=&#34;test.py&#34;&#xA;)&#xA;&#xA;if len(text) == 0:&#xA;    sys.exit()&#xA;&#xA;inputs = tokenizer(text, return_tensors=&#34;pt&#34;, return_token_type_ids=False)&#xA;&#xA;inputs = inputs.to(device)&#xA;model.to(device)&#xA;&#xA;outputs = model.generate(**inputs, max_new_tokens=256)&#xA;print(tokenizer.decode(outputs[0], skip_special_tokens=False))&#xA;&#xA;&#xA;&#xA;&#34;&#34;&#34;output:&#xA;def quick_sort(arr):&#xA;    # 如果数组长度小于等于1，直接返回&#xA;    if len(arr) &amp;lt;= 1:&#xA;        return arr&#xA;    # 选择数组的第一个元素作为基准&#xA;    pivot = arr[0]&#xA;    # 初始化左右指针&#xA;    left, right = 1, len(arr) - 1&#xA;    # 循环直到左指针小于右指针&#xA;    while left &amp;lt; right:&#xA;        # 从右到左找到第一个小于基准的元素，与左指针元素交换&#xA;        if arr[right] &amp;lt; pivot:&#xA;            arr[left], arr[right] = arr[right], arr[left]&#xA;            left += 1&#xA;        # 从左到右找到第一个大于等于基准的元素，与右指针元素交换&#xA;        if arr[left] &amp;gt;= pivot:&#xA;            right -= 1&#xA;    # 将基准元素与左指针元素交换&#xA;    arr[left], arr[0] = arr[0], arr[left]&#xA;    # 对左半部分进行递归排序&#xA;    quick_sort(arr[:left])&#xA;    # 对右半部分进行递归排序&#xA;    quick_sort(arr[left + 1:])&#xA;    return arr&amp;lt;/s&amp;gt;&#xA;&#34;&#34;&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quantized through bitsandbytes&lt;/h3&gt; &#xA;&lt;p&gt;We can also install Bitsandbytes through &lt;code&gt;pip install bitsandbytes acceleration&lt;/code&gt;, and simply add configuration to perform int8 or int4 inference (if you need to further compress the temporary memory applied at runtime, it is recommended to install FlashAttention):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;import sys&#xA;import torch&#xA;from hf_mini.utils import input_wrapper&#xA;from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig    &#xA;&#xA;# to use 4bit use `load_in_4bit=True` instead&#xA;bnb_config = BitsAndBytesConfig(load_in_8bit=True) &#xA;&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;aiXcoder/aixcoder-7b-base&#34;)&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;aiXcoder/aixcoder-7b-base&#34;, quantization_config=bnb_config, device_map=device, attn_implementation=&#39;flash_attention_2&#39;)&#xA;&#xA;text = input_wrapper(&#xA;    code_string=&#34;# 快速排序算法&#34;,&#xA;    later_code=&#34;\n&#34;,&#xA;    path=&#34;test.py&#34;&#xA;)&#xA;&#xA;if len(text) == 0:&#xA;    sys.exit()&#xA;&#xA;inputs = tokenizer(text, return_tensors=&#34;pt&#34;, return_token_type_ids=False)&#xA;&#xA;inputs = inputs.to(device)    &#xA;&#xA;outputs = model.generate(**inputs, max_new_tokens=256)&#xA;print(f&#34;Model memory footprint: {model.get_memory_footprint() / 2**20:.2f} MB&#34;)&#xA;print(f&#34;Torch max memory allocated: {torch.cuda.max_memory_allocated() / 2**20:.2f} MB&#34;)&#xA;&#xA;&#34;&#34;&#34;&#xA;load_in_4bit=True:&#xA;    - Model memory footprint: 5656.52 MB&#xA;    - Torch max memory allocated: 6448.89 MB&#xA;&#xA;load_in_8bit=True:&#xA;    - Model memory footprint: 9008.52 MB&#xA;    - Torch max memory allocated: 10061.51 MB&#xA;&#34;&#34;&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning example&lt;/h3&gt; &#xA;&lt;p&gt;If you want to fine-tune on your own code, you can quickly get started with training using Huggingface&#39;s PEFT tools. Before doing so, you need to install the necessary libraries with &lt;code&gt;pip install -r requirements_peft.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then, execute the training command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch finetune.py \&#xA;        --model_id &#34;aiXcoder/aixcoder-7b-base&#34; \&#xA;        --dataset_name &#34;bigcode/the-stack-smol&#34; \&#xA;        --subset &#34;data/rust&#34; \&#xA;        --dataset_text_field &#34;content&#34; \&#xA;        --split &#34;train&#34; \&#xA;        --max_seq_length 1024 \&#xA;        --max_steps 10000 \&#xA;        --micro_batch_size 1 \&#xA;        --gradient_accumulation_steps 8 \&#xA;        --learning_rate 5e-6 \&#xA;        --warmup_steps 20 \&#xA;        --fim_rate 0.5 \&#xA;        --num_proc &#34;$(nproc)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the fine-tuning script, we have constructed a simple random FIM (Fill-In-the-Middle) training task that can train the model on the completion and generation capabilities on your own data. It should be noted that the aiXcoder-7b-base uses &lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/#pre-training-tasks&#34;&gt;structured FIM&lt;/a&gt; during pre-training, which involves constructing a complete code block as the MIDDLE. However, creating such training data involves syntactic parsing, which may require developers to implement themselves.&lt;/p&gt; &#xA;&lt;h2&gt;Data for aiXcoder 7B&lt;/h2&gt; &#xA;&lt;p&gt;The data for aiXcoder is divided into a core dataset and an extended dataset. The core dataset comprises the programming languages commonly used in development, as well as natural languages closely related to code. The core dataset&#39;s programming languages mainly include nearly a hundred mainstream languages such as C++, Python, Java, and JavaScript, while the natural language component primarily consists of StackOverflow Q&amp;amp;As, technical blogs, code documentation, and computer science papers. The extended data mainly consists of filtered open-source code datasets, high-quality English natural language datasets, and high-quality Chinese natural language datasets.&lt;/p&gt; &#xA;&lt;!-- &lt;br&gt;&#xA;&lt;br&gt;&#xA;&#xA;![table_0](./assets/table_0.png)&#xA;&#xA;&lt;br&gt;&#xA;&lt;br&gt; --&gt; &#xA;&lt;p&gt;The aiXcoder core dataset is mainly used to enhance the performance of the large code model in the aforementioned programming languages, undergoing a rigorous filtering and selection process. Specifically, this process includes the following steps: 1) Selection of raw data; 2) Comprehensive ranking and selection of projects; 3) Code deduplication and the removal of automatically generated code using methods such as MinHashes (Broder, 2000); 4) Identification and handling of personal sensitive information; 5) Cleaning of commented code; 6) Syntactic analysis to filter incorrect or anomalous code files; 7) Static analysis to detect and eliminate 163 types of high-risk bugs and 197 types of defects in mainstream programming languages such as Java, C++, Python, and JavaScript.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Raw Data Selection &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Exclude projects under copyleft licenses.&lt;/li&gt; &#xA;   &lt;li&gt;Deduplicate projects gathered from various code hosting platforms and open-source datasets&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Project-Level Comprehensive Ranking &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Calculate project metrics, including the number of Stars, Git Commit counts, and the quantity of Test files.&lt;/li&gt; &#xA;   &lt;li&gt;Exclude the lowest 10% of data based on a comprehensive score.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Code File-Level Filtering &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Remove automatically generated code.&lt;/li&gt; &#xA;   &lt;li&gt;Employ near-deduplication for redundancy removal.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Sensitive Information Removal &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use named entity recognition models to identify and delete sensitive information such as names, IP addresses, account passwords, and URLs.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Commented Code &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Randomly deleting large sections of commented code&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Syntax Analysis &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Delete code with syntax parsing errors or syntactical errors in the top fifty languages.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Static Analysis &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Utilize static analysis tools to scan for and locate 161 types of Bugs affecting code reliability and maintainability, as well as 197 types of vulnerabilities impacting code security.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# &#34;__init__&#34; method should not return a value&#xA;&#xA;# Noncompliant: a TypeError will be raised&#xA;class MyClass(object):&#xA;    def __init__(self):&#xA;        self.message = &#39;HelloWorld&#39;&#xA;        return self  &#xA;&#xA;# Compliant solution&#xA;class MyClass(object):&#xA;    def __init__(self):&#xA;        self.message = &#39;HelloWorld&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The mentioned code illustrates a bug pattern in Python where the &lt;strong&gt;init&lt;/strong&gt; method should not return a value.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Training Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;Tokenizer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Byte Pair Encoding (BPE) based on bytecode&lt;/li&gt; &#xA; &lt;li&gt;Vocabulary size of 49,152&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Model Structure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;RoPE (Rotary Positional Embedding) for relative position encoding&lt;/li&gt; &#xA; &lt;li&gt;SwiGLU as the intermediate layer&lt;/li&gt; &#xA; &lt;li&gt;Grouped Query Attention&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Training Parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Structured FIM (Fill in the middle) training tasks make up 70% of the training, while autoregressive training tasks account for 30%&lt;/li&gt; &#xA; &lt;li&gt;Pretraining sequence length of 32,768&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Batch processing method&lt;/h3&gt; &#xA;&lt;p&gt;After preprocessing, our code data is organized by project, with the order of files within a project considering both rules and randomness. Specifically, we attempt to cluster similar or dependent code files together using methods like Calling Graph, K-Means clustering, file path similarity, and TF-IDF distance, to help the model better understand the relationships between code files. However, the ordering of code files also incorporates randomness, since in real programming scenarios, projects are not complete, and code files with similarities or dependencies may not be fully developed yet.&lt;/p&gt; &#xA;&lt;p&gt;By ensuring that the project code files overall exhibit randomness while locally having similar or dependent relationships, we stretch the project code files into a vector and organize the sequence of batches using the Transformer-XL style processing. Even though the sequence length of a single batch has already reached 32,768 during the pre-training process, this method still allows for the extension of the visible sequence length to be even longer.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-training Tasks&lt;/h3&gt; &#xA;&lt;p&gt;Unlike other natural language large models or code models, in the context of code programming, aiXcoder considers the structural characteristics of code itself, aiming to have the model predict complete code nodes. In simple terms, the aiXcoder 7b training tasks combine the fill in the middle (FIM, Bavarian et al., 2022) and parser generator tool techniques. When constructing training data, we parse the code into an abstract syntax tree (AST) and randomly select a complete node to construct a FIM task. The rationale behind this approach is twofold: first, we need to ensure that the input data is relatively complete, with both the preceding and subsequent parts being at the same hierarchical level. Secondly, we also want the model&#39;s predictions to be more complete, with the generated code having a full hierarchical structure.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(20):&#xA;    if i % 5 == 0:&#xA;        print(&#34;Hello World&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/graphviz.svg?sanitize=true&#34; alt=&#34;table_0&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Given that simple code can be parsed into an abstract syntax tree (AST), we will construct structured Fill In the Middle (FIM) training tasks based on the nodes of the AST.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Suppose we select the IF node in the above AST, then we will construct training samples from the IF node and its subtree. The following two examples are equivalent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;# fill in the middle, SPM mode&#xA;&#34;&amp;lt;s&amp;gt;▁&amp;lt;AIX-SPAN-PRE&amp;gt;▁&amp;lt;AIX-SPAN-POST&amp;gt;        print(\&#34;Hello World\&#34;)\n▁&amp;lt;AIX-SPAN-MIDDLE&amp;gt;# the file path is: test.py\n# the code file is written by Python\nfor i in range(20):\n    if i % 5 == 0:&amp;lt;\s&amp;gt;&#34;&#xA;&#xA;# fill in the middle, PSM mode&#xA;&#34;&amp;lt;s&amp;gt;▁&amp;lt;AIX-SPAN-PRE&amp;gt;# the file path is: test.py\n# the code file is written by Python\nfor i in range(20):\n    if ▁&amp;lt;AIX-SPAN-POST&amp;gt;        print(\&#34;Hello World\&#34;)\n▁&amp;lt;AIX-SPAN-MIDDLE&amp;gt;i % 5 == 0:&amp;lt;\s&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Details of Experimental Results&lt;/h2&gt; &#xA;&lt;h3&gt;NL2Code Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;Table 1 shows the performance of the aiXcoder-7B Base model on standalone method generation benchmarks. Our model achieves the current best results among the large-scale pre-trained base models within hundreds of billions of parameters.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_1.png&#34; alt=&#34;table_1&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Code Completion (Fill in the Middle)&lt;/h3&gt; &#xA;&lt;p&gt;Different from the standalone nl2code task in Table 1, in real-world programming scenarios, we need to consider the code completion capability in the context of the cursor position. Generally, various open-source large language models for code incorporate the Fill in the Middle (FIM) mode during pre-training to enhance the model&#39;s ability to generate more accurate results when considering the code context. Therefore, we will use FIM as the default code completion method to evaluate the performance of each model in real-world programming scenarios.&lt;/p&gt; &#xA;&lt;p&gt;Currently, the mainstream evaluation dataset for context-aware code completion is the single-line evaluation method proposed by Santacoder (Ben Allal et al., 2023). This evaluation dataset extracts single lines of code from HumanEval or MultiPL-E and evaluates the Exact Match metric of the model&#39;s generated results, given the complete preceding and following context.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_2.png&#34; alt=&#34;table_2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;To further evaluate the code completion capabilities of large language models for code in a more fine-grained manner, aiXcoder has built an evaluation dataset that is larger in size, more diverse in the code being tested, longer in the context length of the code being tested, and closer to real-world development projects. This evaluation dataset will also be open-sourced on GitHub simultaneously. During the evaluation process, we ensure that different large language models for code use the same maximum sequence length of 16K and evaluate the generation performance in different scenarios, such as generating complete method blocks, conditional blocks, loop processing blocks, exception handling blocks, and a total of thirteen cases.&lt;/p&gt; &#xA;&lt;p&gt;Table 3 shows the average generation performance of different models in different languages. The final evaluation results are the average of all completion scenarios and evaluation samples. The aiXcoder 7B Base model achieves the best performance across major programming languages and various evaluation criteria, indicating that aiXcoder 7B Base has the best basic code completion capability among all open-source models of the same scale and is the most suitable base model for providing code completion capabilities in real-world programming scenarios.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_3.png&#34; alt=&#34;table_3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For each evaluation result in Table 3, there are more detailed evaluation dimensions. Tables 4 to 7 show the details of the multi-dimensional evaluation of different models in different languages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Method signature&lt;/strong&gt; indicates the model&#39;s capability to generate method signatures based on context.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Method body&lt;/strong&gt; represents the model&#39;s ability to generate a complete method body based on context, including the function signature.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Single line&lt;/strong&gt; refers to the completion of single lines of code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Method with comment&lt;/strong&gt; denotes generating a corresponding function body based on context, including function signatures and comments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Empty&lt;/strong&gt; indicates the model&#39;s ability to predict emptiness in the case of complete context.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Method body top, mid, bottom&lt;/strong&gt; show the code generation performance respectively in the upper part of the function body, the middle part, and the lower part.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;If, for, while, try, switch statement&lt;/strong&gt; represent the effects of generating conditional code blocks, loop code blocks, exception catch blocks, and conditional branch blocks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_4.png&#34; alt=&#34;table_4&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_5.png&#34; alt=&#34;table_5&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_6.png&#34; alt=&#34;table_6&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_7.png&#34; alt=&#34;table_7&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Cross-file Code Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Another important capability of large language models for code is the ability to understand code context across files, as developers often need to consider information from other files within the current project when writing code. Therefore, we adopted the CrossCodeEval (Ding et al., 2023) evaluation dataset to assess the model&#39;s ability to extract cross-file contextual information.&lt;/p&gt; &#xA;&lt;p&gt;In Table 8, we fix the context length for all models at 16K and format the input using the PSM pattern in FIM. After the model completes inference, all output results are decoded using Greedy Search. First, as a baseline, we evaluate the generation capabilities of various large code models in a single-file scenario.&lt;/p&gt; &#xA;&lt;p&gt;Then, using BM25 as the similarity metric, we search for the three most similar code blocks within the project as prompts to reassess the model&#39;s generation performance. Finally, &#34;w/Ref.&#34; indicates that we assume we know what the correct Reference code looks like, and then search for the three most similar codes within the project as prompts to re-evaluate the model&#39;s generation performance.&lt;/p&gt; &#xA;&lt;p&gt;Ultimately, the aiXcoder-7B model performs very well in all languages, demonstrating our model&#39;s ability to extract contextual information, especially cross-file contextual information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/assets/table_8.png&#34; alt=&#34;table_8&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The source code in this repository is licensed under the &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache-2.0&lt;/a&gt; License - see the LICENSE file for details. The model weights are licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/MODEL_LICENSE&#34;&gt;Model License&lt;/a&gt; for academic research use; for commercial use, please apply by sending an email to &lt;a href=&#34;mailto:support@aiXcoder.com&#34;&gt;support@aiXcoder.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank all contributors to the open-source projects and datasets that made this work possible.&lt;/p&gt; &#xA;&lt;p&gt;Thank you for your interest in our Code Large Language Model. We look forward to your contributions and feedback!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>quilljs/quill</title>
    <updated>2024-04-21T03:23:34Z</updated>
    <id>tag:github.com,2024-04-21:/quilljs/quill</id>
    <link href="https://github.com/quilljs/quill" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Quill is a modern WYSIWYG editor built for compatibility and extensibility.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;a href=&#34;https://quilljs.com/&#34; title=&#34;Quill&#34;&gt;Quill Rich Text Editor&lt;/a&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://quilljs.com/&#34; title=&#34;Quill&#34;&gt;&lt;img alt=&#34;Quill Logo&#34; src=&#34;https://quilljs.com/assets/images/logo.svg?sanitize=true&#34; width=&#34;180&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a title=&#34;Documentation&#34; href=&#34;https://quilljs.com/docs/quickstart&#34;&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; • &lt;a title=&#34;Development&#34; href=&#34;https://github.com/quilljs/quill/raw/main/.github/DEVELOPMENT.md&#34;&gt;&lt;strong&gt;Development&lt;/strong&gt;&lt;/a&gt; • &lt;a title=&#34;Contributing&#34; href=&#34;https://github.com/quilljs/quill/raw/main/.github/CONTRIBUTING.md&#34;&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/a&gt; • &lt;a title=&#34;Interactive Playground&#34; href=&#34;https://quilljs.com/playground/&#34;&gt;&lt;strong&gt;Interactive Playground&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/quilljs/quill/actions&#34; title=&#34;Build Status&#34;&gt; &lt;img src=&#34;https://github.com/quilljs/quill/actions/workflows/main.yml/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://npmjs.com/package/quill&#34; title=&#34;Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/v/quill.svg?sanitize=true&#34; alt=&#34;Version&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://npmjs.com/package/quill&#34; title=&#34;Downloads&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/dm/quill.svg?sanitize=true&#34; alt=&#34;Downloads&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://quilljs.com/&#34;&gt;Quill&lt;/a&gt; is a modern rich text editor built for compatibility and extensibility. It was created by &lt;a href=&#34;https://twitter.com/jhchen&#34;&gt;Jason Chen&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/byronmilligan&#34;&gt;Byron Milligan&lt;/a&gt; and actively maintained by &lt;a href=&#34;https://slab.com&#34;&gt;Slab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To get started, check out &lt;a href=&#34;https://quilljs.com/&#34;&gt;https://quilljs.com/&lt;/a&gt; for documentation, guides, and live demos!&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Instantiate a new Quill object with a css selector for the div that should become the editor.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;!-- Include Quill stylesheet --&amp;gt;&#xA;&amp;lt;link&#xA;  href=&#34;https://cdn.jsdelivr.net/npm/quill@2.0.0/dist/quill.snow.css&#34;&#xA;  rel=&#34;stylesheet&#34;&#xA;/&amp;gt;&#xA;&#xA;&amp;lt;!-- Create the toolbar container --&amp;gt;&#xA;&amp;lt;div id=&#34;toolbar&#34;&amp;gt;&#xA;  &amp;lt;button class=&#34;ql-bold&#34;&amp;gt;Bold&amp;lt;/button&amp;gt;&#xA;  &amp;lt;button class=&#34;ql-italic&#34;&amp;gt;Italic&amp;lt;/button&amp;gt;&#xA;&amp;lt;/div&amp;gt;&#xA;&#xA;&amp;lt;!-- Create the editor container --&amp;gt;&#xA;&amp;lt;div id=&#34;editor&#34;&amp;gt;&#xA;  &amp;lt;p&amp;gt;Hello World!&amp;lt;/p&amp;gt;&#xA;  &amp;lt;p&amp;gt;Some initial &amp;lt;strong&amp;gt;bold&amp;lt;/strong&amp;gt; text&amp;lt;/p&amp;gt;&#xA;  &amp;lt;p&amp;gt;&amp;lt;br /&amp;gt;&amp;lt;/p&amp;gt;&#xA;&amp;lt;/div&amp;gt;&#xA;&#xA;&amp;lt;!-- Include the Quill library --&amp;gt;&#xA;&amp;lt;script src=&#34;https://cdn.jsdelivr.net/npm/quill@2.0.0/dist/quill.js&#34;&amp;gt;&amp;lt;/script&amp;gt;&#xA;&#xA;&amp;lt;!-- Initialize Quill editor --&amp;gt;&#xA;&amp;lt;script&amp;gt;&#xA;  const quill = new Quill(&#34;#editor&#34;, {&#xA;    theme: &#34;snow&#34;,&#xA;  });&#xA;&amp;lt;/script&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Take a look at the &lt;a href=&#34;https://quilljs.com/&#34;&gt;Quill&lt;/a&gt; website for more documentation, guides and &lt;a href=&#34;https://quilljs.com/playground/&#34;&gt;live playground&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm install quill&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CDN&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;!-- Main Quill library --&amp;gt;&#xA;&amp;lt;script src=&#34;https://cdn.jsdelivr.net/npm/quill@2.0.0/dist/quill.js&#34;&amp;gt;&amp;lt;/script&amp;gt;&#xA;&#xA;&amp;lt;!-- Theme included stylesheets --&amp;gt;&#xA;&amp;lt;link&#xA;  href=&#34;https://cdn.jsdelivr.net/npm/quill@2.0.0/dist/quill.snow.css&#34;&#xA;  rel=&#34;stylesheet&#34;&#xA;/&amp;gt;&#xA;&amp;lt;link&#xA;  href=&#34;https://cdn.jsdelivr.net/npm/quill@2.0.0/dist/quill.bubble.css&#34;&#xA;  rel=&#34;stylesheet&#34;&#xA;/&amp;gt;&#xA;&#xA;&amp;lt;!-- Core build with no theme, formatting, non-essential modules --&amp;gt;&#xA;&amp;lt;link&#xA;  href=&#34;https://cdn.jsdelivr.net/npm/quill@2.0.0/dist/quill.core.css&#34;&#xA;  rel=&#34;stylesheet&#34;&#xA;/&amp;gt;&#xA;&amp;lt;script src=&#34;https://cdn.jsdelivr.net/npm/quill@2.0.0/dist/quill.core.js&#34;&amp;gt;&amp;lt;/script&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;Get help or stay up to date.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/quilljs/quill/raw/main/.github/CONTRIBUTING.md&#34;&gt;Contribute&lt;/a&gt; on &lt;a href=&#34;https://github.com/quilljs/quill/issues&#34;&gt;Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ask questions on &lt;a href=&#34;https://github.com/quilljs/quill/discussions&#34;&gt;Discussions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;BSD 3-clause&lt;/p&gt;</summary>
  </entry>
</feed>