<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-14T02:00:50Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Dreamacro/clash</title>
    <updated>2022-08-14T02:00:50Z</updated>
    <id>tag:github.com,2022-08-14:/Dreamacro/clash</id>
    <link href="https://github.com/Dreamacro/clash" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A rule-based tunnel in Go.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Dreamacro/clash/raw/master/docs/logo.png&#34; alt=&#34;Clash&#34; width=&#34;200&#34;&gt; &lt;br&gt;Clash&lt;br&gt; &lt;/h1&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt;A rule-based tunnel in Go.&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Dreamacro/clash/actions&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/workflow/status/Dreamacro/clash/Go?style=flat-square&#34; alt=&#34;Github Actions&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://goreportcard.com/report/github.com/Dreamacro/clash&#34;&gt; &lt;img src=&#34;https://goreportcard.com/badge/github.com/Dreamacro/clash?style=flat-square&#34;&gt; &lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/go-mod/go-version/Dreamacro/clash?style=flat-square&#34;&gt; &lt;a href=&#34;https://github.com/Dreamacro/clash/releases&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release/Dreamacro/clash/all.svg?style=flat-square&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Dreamacro/clash/releases/tag/premium&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/release-Premium-00b4f0?style=flat-square&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Local HTTP/HTTPS/SOCKS server with authentication support&lt;/li&gt; &#xA; &lt;li&gt;VMess, Shadowsocks, Trojan, Snell protocol support for remote connections&lt;/li&gt; &#xA; &lt;li&gt;Built-in DNS server that aims to minimize DNS pollution attack impact, supports DoH/DoT upstream and fake IP.&lt;/li&gt; &#xA; &lt;li&gt;Rules based off domains, GEOIP, IPCIDR or Process to forward packets to different nodes&lt;/li&gt; &#xA; &lt;li&gt;Remote groups allow users to implement powerful rules. Supports automatic fallback, load balancing or auto select node based off latency&lt;/li&gt; &#xA; &lt;li&gt;Remote providers, allowing users to get node lists remotely instead of hardcoding in config&lt;/li&gt; &#xA; &lt;li&gt;Netfilter TCP redirecting. Deploy Clash on your Internet gateway with &lt;code&gt;iptables&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Comprehensive HTTP RESTful API controller&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Premium Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TUN mode on macOS, Linux and Windows. &lt;a href=&#34;https://github.com/Dreamacro/clash/wiki/premium-core-features#tun-device&#34;&gt;Doc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Match your tunnel by &lt;a href=&#34;https://github.com/Dreamacro/clash/wiki/premium-core-features#script&#34;&gt;Script&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Dreamacro/clash/wiki/premium-core-features#rule-providers&#34;&gt;Rule Provider&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Documentations are now moved to &lt;a href=&#34;https://github.com/Dreamacro/clash/wiki&#34;&gt;GitHub Wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Premium Release&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Dreamacro/clash/releases/tag/premium&#34;&gt;Release&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;If you want to build an application that uses clash as a library, check out the the &lt;a href=&#34;https://github.com/Dreamacro/clash/wiki/use-clash-as-a-library&#34;&gt;GitHub Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/riobard/go-shadowsocks2&#34;&gt;riobard/go-shadowsocks2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/v2ray/v2ray-core&#34;&gt;v2ray/v2ray-core&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WireGuard/wireguard-go&#34;&gt;WireGuard/wireguard-go&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This software is released under the GPL-3.0 license.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://app.fossa.io/projects/git%2Bgithub.com%2FDreamacro%2Fclash?ref=badge_large&#34;&gt;&lt;img src=&#34;https://app.fossa.io/api/projects/git%2Bgithub.com%2FDreamacro%2Fclash.svg?type=large&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dragonflydb/dragonfly</title>
    <updated>2022-08-14T02:00:50Z</updated>
    <id>tag:github.com,2022-08-14:/dragonflydb/dragonfly</id>
    <link href="https://github.com/dragonflydb/dragonfly" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A modern replacement for Redis and Memcached&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://dragonflydb.io&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/dragonflydb/dragonfly/main/.github/images/logo-full.svg?sanitize=true&#34; width=&#34;284&#34; border=&#34;0&#34; alt=&#34;Dragonfly&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/dragonflydb/dragonfly/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/dragonflydb/dragonfly/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;ci-tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/romanger&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/romanger?style=social&#34; alt=&#34;Twitter URL&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/dragonflydb/dragonfly/tree/main/docs/quick-start&#34;&gt;Quick Start&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/HsPjXGVH85&#34;&gt;Discord Chat&lt;/a&gt; | &lt;a href=&#34;https://github.com/dragonflydb/dragonfly/discussions&#34;&gt;GitHub Discussions&lt;/a&gt; | &lt;a href=&#34;https://github.com/dragonflydb/dragonfly/issues&#34;&gt;GitHub Issues&lt;/a&gt; | &lt;a href=&#34;https://github.com/dragonflydb/dragonfly/raw/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Probably, the fastest in-memory store in the universe!&lt;/h3&gt; &#xA;&lt;p&gt;Dragonfly is a modern in-memory datastore, fully compatible with Redis and Memcached APIs. Dragonfly implements novel algorithms and data structures on top of a multi-threaded, shared-nothing architecture. As a result, Dragonfly reaches x25 performance compared to Redis and supports millions of QPS on a single instance.&lt;/p&gt; &#xA;&lt;p&gt;Dragonfly&#39;s core properties make it a cost-effective, high-performing, and easy-to-use Redis replacement.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;img src=&#34;http://assets.dragonflydb.io/repo-assets/aws-throughput.svg?sanitize=true&#34; width=&#34;80%&#34; border=&#34;0&#34;&gt; &#xA;&lt;p&gt;Dragonfly is crossing 3.8M QPS on c6gn.16xlarge reaching x25 increase in throughput compared to Redis.&lt;/p&gt; &#xA;&lt;p&gt;99th latency percentile of Dragonfly at its peak throughput:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;op&lt;/th&gt; &#xA;   &lt;th&gt;r6g&lt;/th&gt; &#xA;   &lt;th&gt;c6gn&lt;/th&gt; &#xA;   &lt;th&gt;c7g&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;set&lt;/td&gt; &#xA;   &lt;td&gt;0.8ms&lt;/td&gt; &#xA;   &lt;td&gt;1ms&lt;/td&gt; &#xA;   &lt;td&gt;1ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;get&lt;/td&gt; &#xA;   &lt;td&gt;0.9ms&lt;/td&gt; &#xA;   &lt;td&gt;0.9ms&lt;/td&gt; &#xA;   &lt;td&gt;0.8ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;setex&lt;/td&gt; &#xA;   &lt;td&gt;0.9ms&lt;/td&gt; &#xA;   &lt;td&gt;1.1ms&lt;/td&gt; &#xA;   &lt;td&gt;1.3ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;All benchmarks were performed using &lt;code&gt;memtier_benchmark&lt;/code&gt; (see below) with number of threads tuned per server type and the instance type. &lt;code&gt;memtier&lt;/code&gt; was running on a separate c6gn.16xlarge machine. For setex benchmark we used expiry-range of 500, so it would survive the end of the test.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  memtier_benchmark --ratio ... -t &amp;lt;threads&amp;gt; -c 30 -n 200000 --distinct-client-seed -d 256 \&#xA;     --expiry-range=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When running in pipeline mode &lt;code&gt;--pipeline=30&lt;/code&gt;, Dragonfly reaches &lt;strong&gt;10M qps&lt;/strong&gt; for SET and &lt;strong&gt;15M qps&lt;/strong&gt; for GET operations.&lt;/p&gt; &#xA;&lt;h3&gt;Memcached / Dragonfly&lt;/h3&gt; &#xA;&lt;p&gt;We compared memcached with Dragonfly on &lt;code&gt;c6gn.16xlarge&lt;/code&gt; instance on AWS. As you can see below Dragonfly dominates memcached for both write and read workloads in terms of throughput with a comparable latency. For write workloads, Dragonfly has also better latency, due to contention on the &lt;a href=&#34;https://raw.githubusercontent.com/dragonflydb/dragonfly/main/docs/memcached_benchmark.md&#34;&gt;write path in memcached&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;SET benchmark&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Server&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;QPS(thousands qps)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;latency 99%&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;99.9%&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dragonfly&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üü© 3844&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üü© 0.9ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üü© 2.4ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Memcached&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;806&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.6ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.2ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;GET benchmark&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Server&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;QPS(thousands qps)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;latency 99%&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;99.9%&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dragonfly&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üü© 3717&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.4ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Memcached&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2100&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üü© 0.34ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üü© 0.6ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Memcached exhibited lower latency for the read benchmark, but also lower throughput.&lt;/p&gt; &#xA;&lt;h3&gt;Memory efficiency&lt;/h3&gt; &#xA;&lt;p&gt;In the following test, we filled Dragonfly and Redis with ~5GB of data using &lt;code&gt;debug populate 5000000 key 1024&lt;/code&gt; command. Then we started sending the update traffic with &lt;code&gt;memtier&lt;/code&gt; and kicked off the snapshotting with the &#34;bgsave&#34; command. The following figure demonstrates clearly how both servers behave in terms of memory efficiency.&lt;/p&gt; &#xA;&lt;img src=&#34;http://assets.dragonflydb.io/repo-assets/bgsave-memusage.svg?sanitize=true&#34; width=&#34;70%&#34; border=&#34;0&#34;&gt; &#xA;&lt;p&gt;Dragonfly was 30% more memory efficient than Redis at the idle state. It also did not show any visible memory increase during the snapshot phase. Meanwhile, Redis reached almost x3 memory increase at peak compared to Dragonfly. Dragonfly also finished the snapshot much faster, just a few seconds after it started. For more info about memory efficiency in Dragonfly see &lt;a href=&#34;https://raw.githubusercontent.com/dragonflydb/dragonfly/main/docs/dashtable.md&#34;&gt;dashtable doc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running the server&lt;/h2&gt; &#xA;&lt;p&gt;Dragonfly runs on linux. It uses relatively new linux specific &lt;a href=&#34;https://github.com/axboe/liburing&#34;&gt;io-uring API&lt;/a&gt; for I/O, hence it requires Linux version 5.10 or later. Debian/Bullseye, Ubuntu 20.04.4 or later fit these requirements.&lt;/p&gt; &#xA;&lt;h3&gt;With docker:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --network=host --ulimit memlock=-1 docker.dragonflydb.io/dragonflydb/dragonfly&#xA;&#xA;redis-cli PING  # redis-cli can be installed with &#34;apt install -y redis-tools&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;You need &lt;code&gt;--ulimit memlock=-1&lt;/code&gt; because some Linux distros configure the default memlock limit for containers as 64m and Dragonfly requires more.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Releases&lt;/h3&gt; &#xA;&lt;p&gt;We maintain &lt;a href=&#34;https://github.com/dragonflydb/dragonfly/releases&#34;&gt;binary releases&lt;/a&gt; for x86 and arm64 architectures. You will need to install &lt;code&gt;libunwind8&lt;/code&gt; lib to run the binaries.&lt;/p&gt; &#xA;&lt;h3&gt;Building from source&lt;/h3&gt; &#xA;&lt;p&gt;You need to install dependencies in order to build on Ubuntu 20.04 or later:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/dragonflydb/dragonfly &amp;amp;&amp;amp; cd dragonfly&#xA;&#xA;# to install dependencies&#xA;sudo apt install ninja-build libunwind-dev libboost-fiber-dev libssl-dev \&#xA;     autoconf-archive libtool cmake g++&#xA;&#xA;# Configure the build&#xA;./helio/blaze.sh -release&#xA;&#xA;# Build&#xA;cd build-opt &amp;amp;&amp;amp; ninja dragonfly&#xA;&#xA;# Run&#xA;./dragonfly --alsologtostderr&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Dragonfly supports common redis arguments where applicable. For example, you can run: &lt;code&gt;dragonfly --requirepass=foo --bind localhost&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Dragonfly currently supports the following Redis-specific arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;port&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;bind&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;requirepass&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;maxmemory&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dir&lt;/code&gt; - by default, dragonfly docker uses &lt;code&gt;/data&lt;/code&gt; folder for snapshotting. You can use &lt;code&gt;-v&lt;/code&gt; docker option to map it to your host folder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dbfilename&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In addition, it has Dragonfly specific arguments options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;memcache_port&lt;/code&gt; - to enable memcached compatible API on this port. Disabled by default.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;keys_output_limit&lt;/code&gt; - maximum number of returned keys in &lt;code&gt;keys&lt;/code&gt; command. Default is 8192. &lt;code&gt;keys&lt;/code&gt; is a dangerous command. We truncate its result to avoid blowup in memory when fetching too many keys.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dbnum&lt;/code&gt; - maximum number of supported databases for &lt;code&gt;select&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cache_mode&lt;/code&gt; - see &lt;a href=&#34;https://raw.githubusercontent.com/dragonflydb/dragonfly/main/#novel-cache-design&#34;&gt;Cache&lt;/a&gt; section below.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;hz&lt;/code&gt; - key expiry evaluation frequency. Default is 1000. Lower frequency uses less cpu when idle at the expense of precision in key eviction.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;for more options like logs management or tls support, run &lt;code&gt;dragonfly --help&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap and status&lt;/h2&gt; &#xA;&lt;p&gt;Currently Dragonfly supports ~130 Redis commands and all memcache commands besides &lt;code&gt;cas&lt;/code&gt;. We are almost on par with Redis 2.8 API. Our first milestone will be to stabilize basic functionality and reach API parity with Redis 2.8 and Memcached APIs. If you see that a command you need, is not implemented yet, please open an issue.&lt;/p&gt; &#xA;&lt;p&gt;The next milestone will be implementing H/A with &lt;code&gt;redis -&amp;gt; dragonfly&lt;/code&gt; and &lt;code&gt;dragonfly&amp;lt;-&amp;gt;dragonfly&lt;/code&gt; replication.&lt;/p&gt; &#xA;&lt;p&gt;For dragonfly-native replication, we are planning to design a distributed log format that will support order of magnitude higher speeds when replicating.&lt;/p&gt; &#xA;&lt;p&gt;After replication and failover feature we will continue with other Redis commands from APIs 3,4 and 5.&lt;/p&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/dragonflydb/dragonfly/main/docs/api_status.md&#34;&gt;API readiness doc&lt;/a&gt; for the current status of Dragonfly.&lt;/p&gt; &#xA;&lt;h3&gt;Milestone - H/A&lt;/h3&gt; &#xA;&lt;p&gt;Implement leader/follower replication (PSYNC/REPLICAOF/...).&lt;/p&gt; &#xA;&lt;h3&gt;Milestone - &#34;Maturity&#34;&lt;/h3&gt; &#xA;&lt;p&gt;APIs 3,4,5 without cluster support, without modules and without memory introspection commands. Also without geo commands and without support for keyspace notifications, without streams. Probably design config support. Overall - few dozens commands... Probably implement cluster-API decorators to allow cluster-configured clients to connect to a single instance.&lt;/p&gt; &#xA;&lt;h3&gt;Next milestones will be determined along the way.&lt;/h3&gt; &#xA;&lt;h2&gt;Design decisions&lt;/h2&gt; &#xA;&lt;h3&gt;Novel cache design&lt;/h3&gt; &#xA;&lt;p&gt;Dragonfly has a single unified adaptive caching algorithm that is very simple and memory efficient. You can enable caching mode by passing &lt;code&gt;--cache_mode=true&lt;/code&gt; flag. Once this mode is on, Dragonfly will evict items least likely to be stumbled upon in the future but only when it is near maxmemory limit.&lt;/p&gt; &#xA;&lt;h3&gt;Expiration deadlines with relative accuracy&lt;/h3&gt; &#xA;&lt;p&gt;Expiration ranges are limited to ~4 years. Moreover, expiration deadlines with millisecond precision (PEXPIRE/PSETEX etc) will be rounded to closest second &lt;strong&gt;for deadlines greater than 134217727ms (approximately 37 hours)&lt;/strong&gt;. Such rounding has less than 0.001% error which I hope is acceptable for large ranges. If it breaks your use-cases - talk to me or open an issue and explain your case.&lt;/p&gt; &#xA;&lt;p&gt;For more detailed differences between this and Redis implementations &lt;a href=&#34;https://raw.githubusercontent.com/dragonflydb/dragonfly/main/docs/differences.md&#34;&gt;see here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Native Http console and Prometheus compatible metrics&lt;/h3&gt; &#xA;&lt;p&gt;By default Dragonfly allows http access via its main TCP port (6379). That&#39;s right, you can connect to Dragonfly via Redis protocol and via HTTP protocol - the server recognizes the protocol automatically during the connection initiation. Go ahead and try it with your browser. Right now it does not have much info but in the future we are planning to add there useful debugging and management info. If you go to &lt;code&gt;:6379/metrics&lt;/code&gt; url you will see some prometheus compatible metrics.&lt;/p&gt; &#xA;&lt;p&gt;The Prometheus exported metrics are compatible with the Grafana dashboard &lt;a href=&#34;https://raw.githubusercontent.com/dragonflydb/dragonfly/main/examples/grafana/dashboard.json&#34;&gt;see here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Important! Http console is meant to be accessed within a safe network. If you expose Dragonfly&#39;s TCP port externally, it is advised to disable the console with &lt;code&gt;--http_admin_console=false&lt;/code&gt; or &lt;code&gt;--nohttp_admin_console&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Background&lt;/h2&gt; &#xA;&lt;p&gt;Dragonfly started as an experiment to see how an in-memory datastore could look like if it was designed in 2022. Based on lessons learned from our experience as users of memory stores and as engineers who worked for cloud companies, we knew that we need to preserve two key properties for Dragonfly: a) to provide atomicity guarantees for all its operations, and b) to guarantee low, sub-millisecond latency over very high throughput.&lt;/p&gt; &#xA;&lt;p&gt;Our first challenge was how to fully utilize CPU, memory, and i/o resources using servers that are available today in public clouds. To solve this, we used &lt;a href=&#34;https://en.wikipedia.org/wiki/Shared-nothing_architecture&#34;&gt;shared-nothing architecture&lt;/a&gt;, which allows us to partition the keyspace of the memory store between threads, so that each thread would manage its own slice of dictionary data. We call these slices - shards. The library that powers thread and I/O management for shared-nothing architecture is open-sourced &lt;a href=&#34;https://github.com/romange/helio&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To provide atomicity guarantees for multi-key operations, we used the advancements from recent academic research. We chose the paper &lt;a href=&#34;https://www.cs.umd.edu/~abadi/papers/vldbj-vll.pdf&#34;&gt;&#34;VLL: a lock manager redesign for main memory database systems‚Äù&lt;/a&gt; to develop the transactional framework for Dragonfly. The choice of shared-nothing architecture and VLL allowed us to compose atomic multi-key operations without using mutexes or spinlocks. This was a major milestone for our PoC and its performance stood out from other commercial and open-source solutions.&lt;/p&gt; &#xA;&lt;p&gt;Our second challenge was to engineer more efficient data structures for the new store. To achieve this goal, we based our core hashtable structure on paper &lt;a href=&#34;https://arxiv.org/pdf/2003.07302.pdf&#34;&gt;&#34;Dash: Scalable Hashing on Persistent Memory&#34;&lt;/a&gt;. The paper itself is centered around persistent memory domain and is not directly related to main-memory stores. Nevertheless, its very much applicable for our problem. It suggested a hashtable design that allowed us to maintain two special properties that are present in the Redis dictionary: a) its incremental hashing ability during datastore growth b) its ability to traverse the dictionary under changes using a stateless scan operation. Besides these 2 properties, Dash is much more efficient in CPU and memory. By leveraging Dash&#39;s design, we were able to innovate further with the following features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Efficient record expiry for TTL records.&lt;/li&gt; &#xA; &lt;li&gt;A novel cache eviction algorithm that achieves higher hit rates than other caching strategies like LRU and LFU with &lt;strong&gt;zero memory overhead&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A novel &lt;strong&gt;fork-less&lt;/strong&gt; snapshotting algorithm.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After we built the foundation for Dragonfly and &lt;a href=&#34;https://raw.githubusercontent.com/dragonflydb/dragonfly/main/#benchmarks&#34;&gt;we were happy with its performance&lt;/a&gt;, we went on to implement the Redis and Memcached functionality. By now, we have implemented ~130 Redis commands (equivalent to v2.8) and 13 Memcached commands.&lt;/p&gt; &#xA;&lt;p&gt;And finally, &lt;br&gt; &lt;em&gt;Our mission is to build a well-designed, ultra-fast, cost-efficient in-memory datastore for cloud workloads that takes advantage of the latest hardware advancements. We intend to address the pain points of current solutions while preserving their product APIs and propositions. &lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pesser/stable-diffusion</title>
    <updated>2022-08-14T02:00:50Z</updated>
    <id>tag:github.com,2022-08-14:/pesser/stable-diffusion</id>
    <link href="https://github.com/pesser/stable-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Development repository. Please see &lt;a href=&#34;https://github.com/CompVis/stable-diffusion/&#34;&gt;CompVis/stable-diffusion&lt;/a&gt; for the Stable Diffusion release.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Latent Diffusion Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/#bibtex&#34;&gt;BibTeX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/assets/results.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/rromb&#34;&gt;Robin Rombach&lt;/a&gt;*, &lt;a href=&#34;https://github.com/ablattmann&#34;&gt;Andreas Blattmann&lt;/a&gt;*, &lt;a href=&#34;https://github.com/qp-qp&#34;&gt;Dominik Lorenz&lt;/a&gt;, &lt;a href=&#34;https://github.com/pesser&#34;&gt;Patrick Esser&lt;/a&gt;, &lt;a href=&#34;https://hci.iwr.uni-heidelberg.de/Staff/bommer&#34;&gt;Bj√∂rn Ommer&lt;/a&gt;&lt;br&gt; * equal contribution&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/assets/modelfigure.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;h3&gt;April 2022&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;Katherine Crowson&lt;/a&gt;, classifier-free guidance received a ~2x speedup and the &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;PLMS sampler&lt;/a&gt; is available. See also &lt;a href=&#34;https://github.com/CompVis/latent-diffusion/pull/51&#34;&gt;this PR&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Our 1.45B &lt;a href=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/#text-to-image&#34;&gt;latent diffusion LAION model&lt;/a&gt; was integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces ü§ó&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo: &lt;a href=&#34;https://huggingface.co/spaces/multimodalart/latentdiffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;More pre-trained LDMs are available:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A 1.45B &lt;a href=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/#text-to-image&#34;&gt;model&lt;/a&gt; trained on the &lt;a href=&#34;https://arxiv.org/abs/2111.02114&#34;&gt;LAION-400M&lt;/a&gt; database.&lt;/li&gt; &#xA;   &lt;li&gt;A class-conditional model on ImageNet, achieving a FID of 3.6 when using &lt;a href=&#34;https://openreview.net/pdf?id=qw8AKxfYbI&#34;&gt;classifier-free guidance&lt;/a&gt; Available via a &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;colab notebook&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;A suitable &lt;a href=&#34;https://conda.io/&#34;&gt;conda&lt;/a&gt; environment named &lt;code&gt;ldm&lt;/code&gt; can be created and activated with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Pretrained Models&lt;/h1&gt; &#xA;&lt;p&gt;A general list of all available checkpoints is available in via our &lt;a href=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/#model-zoo&#34;&gt;model zoo&lt;/a&gt;. If you use any of these models in your work, we are always happy to receive a &lt;a href=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/#bibtex&#34;&gt;citation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Text-to-Image&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/assets/txt2img-preview.png&#34; alt=&#34;text2img-figure&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the pre-trained weights (5.7GB)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p models/ldm/text2img-large/&#xA;wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and sample with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a virus monster is playing guitar, oil on canvas&#34; --ddim_eta 0.0 --n_samples 4 --n_iter 4 --scale 5.0  --ddim_steps 50&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will save each sample individually as well as a grid of size &lt;code&gt;n_iter&lt;/code&gt; x &lt;code&gt;n_samples&lt;/code&gt; at the specified output location (default: &lt;code&gt;outputs/txt2img-samples&lt;/code&gt;). Quality, sampling speed and diversity are best controlled via the &lt;code&gt;scale&lt;/code&gt;, &lt;code&gt;ddim_steps&lt;/code&gt; and &lt;code&gt;ddim_eta&lt;/code&gt; arguments. As a rule of thumb, higher values of &lt;code&gt;scale&lt;/code&gt; produce better samples at the cost of a reduced output diversity.&lt;br&gt; Furthermore, increasing &lt;code&gt;ddim_steps&lt;/code&gt; generally also gives higher quality samples, but returns are diminishing for values &amp;gt; 250. Fast sampling (i.e. low values of &lt;code&gt;ddim_steps&lt;/code&gt;) while retaining good quality can be achieved by using &lt;code&gt;--ddim_eta 0.0&lt;/code&gt;.&lt;br&gt; Faster sampling (i.e. even lower values of &lt;code&gt;ddim_steps&lt;/code&gt;) while retaining good quality can be achieved by using &lt;code&gt;--ddim_eta 0.0&lt;/code&gt; and &lt;code&gt;--plms&lt;/code&gt; (see &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;Pseudo Numerical Methods for Diffusion Models on Manifolds&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;Beyond 256¬≤&lt;/h4&gt; &#xA;&lt;p&gt;For certain inputs, simply running the model in a convolutional fashion on larger features than it was trained on can sometimes result in interesting results. To try it out, tune the &lt;code&gt;H&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt; arguments (which will be integer-divided by 8 in order to calculate the corresponding latent size), e.g. run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a sunset behind a mountain range, vector image&#34; --ddim_eta 1.0 --n_samples 1 --n_iter 1 --H 384 --W 1024 --scale 5.0  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to create a sample of size 384x1024. Note, however, that controllability is reduced compared to the 256x256 setting.&lt;/p&gt; &#xA;&lt;p&gt;The example below was generated using the above command. &lt;img src=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/assets/txt2img-convsample.png&#34; alt=&#34;text2img-figure-conv&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inpainting&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/assets/inpainting.png&#34; alt=&#34;inpainting&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the pre-trained weights&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget -O models/ldm/inpainting_big/last.ckpt https://heibox.uni-heidelberg.de/f/4d9ac7ea40c64582b7c9/?dl=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and sample with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/inpaint.py --indir data/inpainting_examples/ --outdir outputs/inpainting_results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;indir&lt;/code&gt; should contain images &lt;code&gt;*.png&lt;/code&gt; and masks &lt;code&gt;&amp;lt;image_fname&amp;gt;_mask.png&lt;/code&gt; like the examples provided in &lt;code&gt;data/inpainting_examples&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Class-Conditional ImageNet&lt;/h2&gt; &#xA;&lt;p&gt;Available via a &lt;a href=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;notebook&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;. &lt;img src=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/assets/birdhouse.png&#34; alt=&#34;class-conditional&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Unconditional Models&lt;/h2&gt; &#xA;&lt;p&gt;We also provide a script for sampling from unconditional LDMs (e.g. LSUN, FFHQ, ...). Start it via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python scripts/sample_diffusion.py -r models/ldm/&amp;lt;model_spec&amp;gt;/model.ckpt -l &amp;lt;logdir&amp;gt; -n &amp;lt;\#samples&amp;gt; --batch_size &amp;lt;batch_size&amp;gt; -c &amp;lt;\#ddim steps&amp;gt; -e &amp;lt;\#eta&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Train your own LDMs&lt;/h1&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;h3&gt;Faces&lt;/h3&gt; &#xA;&lt;p&gt;For downloading the CelebA-HQ and FFHQ datasets, proceed as described in the &lt;a href=&#34;https://github.com/CompVis/taming-transformers#celeba-hq&#34;&gt;taming-transformers&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h3&gt;LSUN&lt;/h3&gt; &#xA;&lt;p&gt;The LSUN datasets can be conveniently downloaded via the script available &lt;a href=&#34;https://github.com/fyu/lsun&#34;&gt;here&lt;/a&gt;. We performed a custom split into training and validation images, and provide the corresponding filenames at &lt;a href=&#34;https://ommer-lab.com/files/lsun.zip&#34;&gt;https://ommer-lab.com/files/lsun.zip&lt;/a&gt;. After downloading, extract them to &lt;code&gt;./data/lsun&lt;/code&gt;. The beds/cats/churches subsets should also be placed/symlinked at &lt;code&gt;./data/lsun/bedrooms&lt;/code&gt;/&lt;code&gt;./data/lsun/cats&lt;/code&gt;/&lt;code&gt;./data/lsun/churches&lt;/code&gt;, respectively.&lt;/p&gt; &#xA;&lt;h3&gt;ImageNet&lt;/h3&gt; &#xA;&lt;p&gt;The code will try to download (through &lt;a href=&#34;http://academictorrents.com/&#34;&gt;Academic Torrents&lt;/a&gt;) and prepare ImageNet the first time it is used. However, since ImageNet is quite large, this requires a lot of disk space and time. If you already have ImageNet on your disk, you can speed things up by putting the data into &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt; (which defaults to &lt;code&gt;~/.cache/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt;), where &lt;code&gt;{split}&lt;/code&gt; is one of &lt;code&gt;train&lt;/code&gt;/&lt;code&gt;validation&lt;/code&gt;. It should have the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&#xA;‚îú‚îÄ‚îÄ n01440764&#xA;‚îÇ   ‚îú‚îÄ‚îÄ n01440764_10026.JPEG&#xA;‚îÇ   ‚îú‚îÄ‚îÄ n01440764_10027.JPEG&#xA;‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;‚îú‚îÄ‚îÄ n01443537&#xA;‚îÇ   ‚îú‚îÄ‚îÄ n01443537_10007.JPEG&#xA;‚îÇ   ‚îú‚îÄ‚îÄ n01443537_10014.JPEG&#xA;‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;‚îú‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you haven&#39;t extracted the data, you can also place &lt;code&gt;ILSVRC2012_img_train.tar&lt;/code&gt;/&lt;code&gt;ILSVRC2012_img_val.tar&lt;/code&gt; (or symlinks to them) into &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_train/&lt;/code&gt; / &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_validation/&lt;/code&gt;, which will then be extracted into above structure without downloading it again. Note that this will only happen if neither a folder &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt; nor a file &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/.ready&lt;/code&gt; exist. Remove them if you want to force running the dataset preparation again.&lt;/p&gt; &#xA;&lt;h2&gt;Model Training&lt;/h2&gt; &#xA;&lt;p&gt;Logs and checkpoints for trained models are saved to &lt;code&gt;logs/&amp;lt;START_DATE_AND_TIME&amp;gt;_&amp;lt;config_spec&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Training autoencoder models&lt;/h3&gt; &#xA;&lt;p&gt;Configs for training a KL-regularized autoencoder on ImageNet are provided at &lt;code&gt;configs/autoencoder&lt;/code&gt;. Training can be started by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python main.py --base configs/autoencoder/&amp;lt;config_spec&amp;gt;.yaml -t --gpus 0,    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;config_spec&lt;/code&gt; is one of {&lt;code&gt;autoencoder_kl_8x8x64&lt;/code&gt;(f=32, d=64), &lt;code&gt;autoencoder_kl_16x16x16&lt;/code&gt;(f=16, d=16), &lt;code&gt;autoencoder_kl_32x32x4&lt;/code&gt;(f=8, d=4), &lt;code&gt;autoencoder_kl_64x64x3&lt;/code&gt;(f=4, d=3)}.&lt;/p&gt; &#xA;&lt;p&gt;For training VQ-regularized models, see the &lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;taming-transformers&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h3&gt;Training LDMs&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;configs/latent-diffusion/&lt;/code&gt; we provide configs for training LDMs on the LSUN-, CelebA-HQ, FFHQ and ImageNet datasets. Training can be started by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python main.py --base configs/latent-diffusion/&amp;lt;config_spec&amp;gt;.yaml -t --gpus 0,&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;&amp;lt;config_spec&amp;gt;&lt;/code&gt; is one of {&lt;code&gt;celebahq-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3),&lt;code&gt;ffhq-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3), &lt;code&gt;lsun_bedrooms-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3), &lt;code&gt;lsun_churches-ldm-vq-4&lt;/code&gt;(f=8, KL-reg. autoencoder, spatial size 32x32x4),&lt;code&gt;cin-ldm-vq-8&lt;/code&gt;(f=8, VQ-reg. autoencoder, spatial size 32x32x4)}.&lt;/p&gt; &#xA;&lt;h1&gt;Model Zoo&lt;/h1&gt; &#xA;&lt;h2&gt;Pretrained Autoencoding Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/assets/reconstruction2.png&#34; alt=&#34;rec2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;All models were trained until convergence (no further substantial improvement in rFID).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;rFID vs val&lt;/th&gt; &#xA;   &lt;th&gt;train steps&lt;/th&gt; &#xA;   &lt;th&gt;PSNR&lt;/th&gt; &#xA;   &lt;th&gt;PSIM&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Comments&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, VQ (Z=8192, d=3)&lt;/td&gt; &#xA;   &lt;td&gt;0.58&lt;/td&gt; &#xA;   &lt;td&gt;533066&lt;/td&gt; &#xA;   &lt;td&gt;27.43 +/- 4.26&lt;/td&gt; &#xA;   &lt;td&gt;0.53 +/- 0.21&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f4.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f4.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, VQ (Z=8192, d=3)&lt;/td&gt; &#xA;   &lt;td&gt;1.06&lt;/td&gt; &#xA;   &lt;td&gt;658131&lt;/td&gt; &#xA;   &lt;td&gt;25.21 +/- 4.17&lt;/td&gt; &#xA;   &lt;td&gt;0.72 +/- 0.26&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://heibox.uni-heidelberg.de/f/9c6681f64bb94338a069/?dl=1&#34;&gt;https://heibox.uni-heidelberg.de/f/9c6681f64bb94338a069/?dl=1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no attention&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, VQ (Z=16384, d=4)&lt;/td&gt; &#xA;   &lt;td&gt;1.14&lt;/td&gt; &#xA;   &lt;td&gt;971043&lt;/td&gt; &#xA;   &lt;td&gt;23.07 +/- 3.99&lt;/td&gt; &#xA;   &lt;td&gt;1.17 +/- 0.36&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f8.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f8.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, VQ (Z=256, d=4)&lt;/td&gt; &#xA;   &lt;td&gt;1.49&lt;/td&gt; &#xA;   &lt;td&gt;1608649&lt;/td&gt; &#xA;   &lt;td&gt;22.35 +/- 3.81&lt;/td&gt; &#xA;   &lt;td&gt;1.26 +/- 0.37&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f8-n256.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f8-n256.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=16, VQ (Z=16384, d=8)&lt;/td&gt; &#xA;   &lt;td&gt;5.15&lt;/td&gt; &#xA;   &lt;td&gt;1101166&lt;/td&gt; &#xA;   &lt;td&gt;20.83 +/- 3.61&lt;/td&gt; &#xA;   &lt;td&gt;1.73 +/- 0.43&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://heibox.uni-heidelberg.de/f/0e42b04e2e904890a9b6/?dl=1&#34;&gt;https://heibox.uni-heidelberg.de/f/0e42b04e2e904890a9b6/?dl=1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, KL&lt;/td&gt; &#xA;   &lt;td&gt;0.27&lt;/td&gt; &#xA;   &lt;td&gt;176991&lt;/td&gt; &#xA;   &lt;td&gt;27.53 +/- 4.54&lt;/td&gt; &#xA;   &lt;td&gt;0.55 +/- 0.24&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f4.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f4.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, KL&lt;/td&gt; &#xA;   &lt;td&gt;0.90&lt;/td&gt; &#xA;   &lt;td&gt;246803&lt;/td&gt; &#xA;   &lt;td&gt;24.19 +/- 4.19&lt;/td&gt; &#xA;   &lt;td&gt;1.02 +/- 0.35&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f8.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f8.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=16, KL (d=16)&lt;/td&gt; &#xA;   &lt;td&gt;0.87&lt;/td&gt; &#xA;   &lt;td&gt;442998&lt;/td&gt; &#xA;   &lt;td&gt;24.08 +/- 4.22&lt;/td&gt; &#xA;   &lt;td&gt;1.07 +/- 0.36&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f16.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f16.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=32, KL (d=64)&lt;/td&gt; &#xA;   &lt;td&gt;2.04&lt;/td&gt; &#xA;   &lt;td&gt;406763&lt;/td&gt; &#xA;   &lt;td&gt;22.27 +/- 3.93&lt;/td&gt; &#xA;   &lt;td&gt;1.41 +/- 0.40&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f32.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f32.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Get the models&lt;/h3&gt; &#xA;&lt;p&gt;Running the following script downloads und extracts all available pretrained autoencoding models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/download_first_stages.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first stage models can then be found in &lt;code&gt;models/first_stage_models/&amp;lt;model_spec&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained LDMs&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Datset&lt;/th&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;FID&lt;/th&gt; &#xA;   &lt;th&gt;IS&lt;/th&gt; &#xA;   &lt;th&gt;Prec&lt;/th&gt; &#xA;   &lt;th&gt;Recall&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Comments&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CelebA-HQ&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;5.11 (5.11)&lt;/td&gt; &#xA;   &lt;td&gt;3.29&lt;/td&gt; &#xA;   &lt;td&gt;0.72&lt;/td&gt; &#xA;   &lt;td&gt;0.49&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/celeba.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/celeba.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FFHQ&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;4.98 (4.98)&lt;/td&gt; &#xA;   &lt;td&gt;4.50 (4.50)&lt;/td&gt; &#xA;   &lt;td&gt;0.73&lt;/td&gt; &#xA;   &lt;td&gt;0.50&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/ffhq.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/ffhq.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LSUN-Churches&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-KL-8 (400 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;4.02 (4.02)&lt;/td&gt; &#xA;   &lt;td&gt;2.72&lt;/td&gt; &#xA;   &lt;td&gt;0.64&lt;/td&gt; &#xA;   &lt;td&gt;0.52&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/lsun_churches.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/lsun_churches.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LSUN-Bedrooms&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;2.95 (3.0)&lt;/td&gt; &#xA;   &lt;td&gt;2.22 (2.23)&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;   &lt;td&gt;0.48&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/lsun_bedrooms.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/lsun_bedrooms.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;Class-conditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-8 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;7.77(7.76)* /15.82**&lt;/td&gt; &#xA;   &lt;td&gt;201.56(209.52)* /78.82**&lt;/td&gt; &#xA;   &lt;td&gt;0.84* / 0.65**&lt;/td&gt; &#xA;   &lt;td&gt;0.35* / 0.63**&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/cin.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/cin.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;*: w/ guiding, classifier_scale 10 **: w/o guiding, scores in bracket calculated with script provided by &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;ADM&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Conceptual Captions&lt;/td&gt; &#xA;   &lt;td&gt;Text-conditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-f4 (100 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;16.79&lt;/td&gt; &#xA;   &lt;td&gt;13.89&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/text2img.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/text2img.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;finetuned from LAION&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenImages&lt;/td&gt; &#xA;   &lt;td&gt;Super-resolution&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/sr_bsr.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/sr_bsr.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BSR image degradation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenImages&lt;/td&gt; &#xA;   &lt;td&gt;Layout-to-Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;32.02&lt;/td&gt; &#xA;   &lt;td&gt;15.92&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/layout2img_model.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/layout2img_model.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Landscapes&lt;/td&gt; &#xA;   &lt;td&gt;Semantic Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis256.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis256.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Landscapes&lt;/td&gt; &#xA;   &lt;td&gt;Semantic Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;finetuned on resolution 512x512&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Get the models&lt;/h3&gt; &#xA;&lt;p&gt;The LDMs listed above can jointly be downloaded and extracted via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/download_models.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The models can then be found in &lt;code&gt;models/ldm/&amp;lt;model_spec&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Coming Soon...&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More inference scripts for conditional LDMs.&lt;/li&gt; &#xA; &lt;li&gt;In the meantime, you can play with our colab notebook &lt;a href=&#34;https://colab.research.google.com/drive/1xqzUi2iXQXDqXBHQGP9Mqt2YrYW6cx-J?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1xqzUi2iXQXDqXBHQGP9Mqt2YrYW6cx-J?usp=sharing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Comments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Our codebase for the diffusion models builds heavily on &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;OpenAI&#39;s ADM codebase&lt;/a&gt; and &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;https://github.com/lucidrains/denoising-diffusion-pytorch&lt;/a&gt;. Thanks for open-sourcing!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The implementation of the transformer encoder is from &lt;a href=&#34;https://github.com/lucidrains/x-transformers&#34;&gt;x-transformers&lt;/a&gt; by &lt;a href=&#34;https://github.com/lucidrains?tab=repositories&#34;&gt;lucidrains&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;      title={High-Resolution Image Synthesis with Latent Diffusion Models}, &#xA;      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Bj√∂rn Ommer},&#xA;      year={2021},&#xA;      eprint={2112.10752},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>