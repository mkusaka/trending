<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-29T01:46:17Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cpacker/MemGPT</title>
    <updated>2023-10-29T01:46:17Z</updated>
    <id>tag:github.com,2023-10-29:/cpacker/MemGPT</id>
    <link href="https://github.com/cpacker/MemGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Teaching LLMs memory management for unbounded context üìöü¶ô&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cpacker/MemGPT/main/#user-content-memgpt&#34;&gt;&lt;img src=&#34;https://memgpt.ai/assets/img/memgpt_logo_circle.png&#34; alt=&#34;MemGPT logo&#34; width=&#34;75&#34; align=&#34;right&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://memgpt.ai&#34;&gt;MemGPT&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;Try out our MemGPT chatbot on &lt;a href=&#34;https://discord.gg/9GEQrxmVyE&#34;&gt;Discord&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;‚≠ê NEW: You can now run MemGPT with &lt;a href=&#34;https://github.com/cpacker/MemGPT/discussions/67&#34;&gt;local LLMs&lt;/a&gt; and &lt;a href=&#34;https://github.com/cpacker/MemGPT/discussions/65&#34;&gt;AutoGen&lt;/a&gt;! ‚≠ê &lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/9GEQrxmVyE&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1161736243340640419?label=Discord&amp;amp;logo=discord&amp;amp;logoColor=5865F2&amp;amp;style=flat-square&amp;amp;color=5865F2&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.08560&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2310.08560-B31B1B?logo=arxiv&amp;amp;style=flat-square&#34; alt=&#34;arXiv 2310.08560&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;h2&gt;ü§ñ Create perpetual chatbots with self-editing memory!&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;br&gt; &#xA;  &lt;img src=&#34;https://memgpt.ai/assets/img/demo.gif&#34; alt=&#34;MemGPT demo video&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2&gt;üóÉÔ∏è Chat with your data - talk to your SQL database or your local files!&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;strong&gt;SQL Database&lt;/strong&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://memgpt.ai/assets/img/sql_demo.gif&#34; alt=&#34;MemGPT demo video for sql search&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;strong&gt;Local files&lt;/strong&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://memgpt.ai/assets/img/preload_archival_demo.gif&#34; alt=&#34;MemGPT demo video for sql search&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h2&gt;üìÑ You can also talk to docs - for example ask about &lt;a href=&#34;https://raw.githubusercontent.com/cpacker/MemGPT/main/memgpt/personas/examples/docqa&#34;&gt;LlamaIndex&lt;/a&gt;!&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://memgpt.ai/assets/img/docqa_demo.gif&#34; alt=&#34;MemGPT demo video for llamaindex api docs search&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;&lt;b&gt;ChatGPT (GPT-4) when asked the same question:&lt;/b&gt;&lt;/summary&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;img src=&#34;https://memgpt.ai/assets/img/llama_index_gpt4.png&#34; alt=&#34;GPT-4 when asked about llamaindex api docs&#34; width=&#34;800&#34;&gt; &#xA;  &lt;/div&gt; (Question from https://github.com/run-llama/llama_index/issues/7756) &#xA; &lt;/details&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Quick setup&lt;/h2&gt; &#xA;&lt;p&gt;Join &lt;a href=&#34;https://discord.gg/9GEQrxmVyE&#34;&gt;Discord&lt;/a&gt; and message the MemGPT bot (in the &lt;code&gt;#memgpt&lt;/code&gt; channel). Then run the following commands (messaged to &#34;MemGPT Bot&#34;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;/profile&lt;/code&gt; (to create your profile)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/key&lt;/code&gt; (to enter your OpenAI key)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/create&lt;/code&gt; (to create a MemGPT chatbot)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Make sure your privacy settings on this server are open so that MemGPT Bot can DM you: &lt;br&gt; MemGPT ‚Üí Privacy Settings ‚Üí Direct Messages set to ON&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://memgpt.ai/assets/img/discord/dm_settings.png&#34; alt=&#34;set DMs settings on MemGPT server to be open in MemGPT so that MemGPT Bot can message you&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;You can see the full list of available commands when you enter &lt;code&gt;/&lt;/code&gt; into the message box.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://memgpt.ai/assets/img/discord/slash_commands.png&#34; alt=&#34;MemGPT Bot slash commands&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;What is MemGPT?&lt;/h2&gt; &#xA;&lt;p&gt;Memory-GPT (or MemGPT in short) is a system that intelligently manages different memory tiers in LLMs in order to effectively provide extended context within the LLM&#39;s limited context window. For example, MemGPT knows when to push critical information to a vector database and when to retrieve it later in the chat, enabling perpetual conversations. Learn more about MemGPT in our &lt;a href=&#34;https://arxiv.org/abs/2310.08560&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Running MemGPT locally&lt;/h2&gt; &#xA;&lt;p&gt;Install MemGPT:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install pymemgpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To update the package, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install pymemgpt -U&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add your OpenAI API key to your environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# on Linux/Mac&#xA;export OPENAI_API_KEY=YOUR_API_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# on Windows&#xA;set OPENAI_API_KEY=YOUR_API_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# on Windows (PowerShell)&#xA;$Env:OPENAI_API_KEY = &#34;YOUR_API_KEY&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run MemGPT for as a conversation agent in CLI mode, simply run &lt;code&gt;memgpt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;memgpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;Debugging command not found&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;p&gt;If you get &lt;code&gt;command not found&lt;/code&gt; (Linux/MacOS), or a &lt;code&gt;CommandNotFoundException&lt;/code&gt; (Windows), the directory where pip installs scripts is not in your PATH. You can either add that directory to your path (&lt;code&gt;pip show pip | grep Scripts&lt;/code&gt;) or instead just run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python -m memgpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;Building from source&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Clone this repo: &lt;code&gt;git clone https://github.com/cpacker/MemGPT.git&lt;/code&gt;&lt;/p&gt; &#xA; &lt;p&gt;Using poetry:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install poetry: &lt;code&gt;pip install poetry&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Run &lt;code&gt;poetry install&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Run &lt;code&gt;poetry run memgpt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;Using pip:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Run &lt;code&gt;pip install -e .&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Run &lt;code&gt;python3 main.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;If you&#39;re using Azure OpenAI, set these variables instead:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# see https://github.com/openai/openai-python#microsoft-azure-endpoints&#xA;export AZURE_OPENAI_KEY = ...&#xA;export AZURE_OPENAI_ENDPOINT = ...&#xA;export AZURE_OPENAI_VERSION = ...&#xA;&#xA;# set the below if you are using deployment ids&#xA;export AZURE_OPENAI_DEPLOYMENT = ...&#xA;export AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT = ...&#xA;&#xA;# then use the --use_azure_openai flag&#xA;memgpt --use_azure_openai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To create a new starter user or starter persona (that MemGPT gets initialized with), create a new &lt;code&gt;.txt&lt;/code&gt; file in &lt;code&gt;~/.memgpt/humans&lt;/code&gt; or &lt;code&gt;~/.memgpt/personas&lt;/code&gt;, then use the &lt;code&gt;--persona&lt;/code&gt; or &lt;code&gt;--human&lt;/code&gt; flag when running &lt;code&gt;main.py&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# assuming you created a new file ~/.memgpt/humans/me.txt&#xA;memgpt&#xA;# Select me.txt during configuration process&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;-- OR --&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# assuming you created a new file ~/.memgpt/humans/me.txt&#xA;memgpt --human me.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also specify any of the starter users in &lt;a href=&#34;https://raw.githubusercontent.com/cpacker/MemGPT/main/memgpt/humans/examples&#34;&gt;/memgpt/humans/examples&lt;/a&gt; or any of the starter personas in &lt;a href=&#34;https://raw.githubusercontent.com/cpacker/MemGPT/main/memgpt/personas/examples&#34;&gt;/memgpt/personas/examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;GPT-3.5 support&lt;/h3&gt; &#xA;&lt;p&gt;You can run MemGPT with GPT-3.5 as the LLM instead of GPT-4:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;memgpt&#xA;# Select gpt-3.5 during configuration process&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;-- OR --&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;memgpt --model gpt-3.5-turbo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note that this is experimental gpt-3.5-turbo support. It&#39;s quite buggy compared to gpt-4, but it should be runnable.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please report any bugs you encounter regarding MemGPT running on GPT-3.5 to &lt;a href=&#34;https://github.com/cpacker/MemGPT/issues/59&#34;&gt;https://github.com/cpacker/MemGPT/issues/59&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Local LLM support&lt;/h3&gt; &#xA;&lt;p&gt;You can run MemGPT with local LLMs too. See &lt;a href=&#34;https://raw.githubusercontent.com/cpacker/MemGPT/main/memgpt/local_llm&#34;&gt;instructions here&lt;/a&gt; and report any bugs/improvements here &lt;a href=&#34;https://github.com/cpacker/MemGPT/discussions/67&#34;&gt;https://github.com/cpacker/MemGPT/discussions/67&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;main.py&lt;/code&gt; flags&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;--first&#xA;  allows you to send the first message in the chat (by default, MemGPT will send the first message)&#xA;--debug&#xA;  enables debugging output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Configure via legacy flags&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;--model&#xA;  select which model to use (&#39;gpt-4&#39;, &#39;gpt-3.5-turbo-0613&#39;, &#39;gpt-3.5-turbo&#39;)&#xA;--persona&#xA;  load a specific persona file&#xA;--human&#xA;  load a specific human file&#xA;--archival_storage_faiss_path=&amp;lt;ARCHIVAL_STORAGE_FAISS_PATH&amp;gt;&#xA;  load in document database (backed by FAISS index)&#xA;--archival_storage_files=&#34;&amp;lt;ARCHIVAL_STORAGE_FILES_GLOB_PATTERN&amp;gt;&#34;&#xA;  pre-load files into archival memory&#xA;--archival_storage_files_compute_embeddings=&#34;&amp;lt;ARCHIVAL_STORAGE_FILES_GLOB_PATTERN&amp;gt;&#34;&#xA;  pre-load files into archival memory and also compute embeddings for embedding search&#xA;--archival_storage_sqldb=&amp;lt;SQLDB_PATH&amp;gt;&#xA;  load in SQL database&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Interactive CLI commands&lt;/h3&gt; &#xA;&lt;p&gt;These are the commands for the CLI, &lt;strong&gt;not the Discord bot&lt;/strong&gt;! The Discord bot has separate commands you can see in Discord by typing &lt;code&gt;/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;While using MemGPT via the CLI (not Discord!) you can run various commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;//&#xA;  toggle multiline input mode&#xA;/exit&#xA;  exit the CLI&#xA;/save&#xA;  save a checkpoint of the current agent/conversation state&#xA;/load&#xA;  load a saved checkpoint&#xA;/dump&#xA;  view the current message log (see the contents of main context)&#xA;/memory&#xA;  print the current contents of agent memory&#xA;/pop&#xA;  undo the last message in the conversation&#xA;/heartbeat&#xA;  send a heartbeat system message to the agent&#xA;/memorywarning&#xA;  send a memory warning system message to the agent&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example applications&lt;/h2&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Use MemGPT to talk to your Database!&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;MemGPT&#39;s archival memory let&#39;s you load your database and talk to it! To motivate this use-case, we have included a toy example.&lt;/p&gt; &#xA; &lt;p&gt;Consider the &lt;code&gt;test.db&lt;/code&gt; already included in the repository.&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;id&lt;/th&gt; &#xA;    &lt;th&gt;name&lt;/th&gt; &#xA;    &lt;th&gt;age&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;1&lt;/td&gt; &#xA;    &lt;td&gt;Alice&lt;/td&gt; &#xA;    &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;2&lt;/td&gt; &#xA;    &lt;td&gt;Bob&lt;/td&gt; &#xA;    &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;3&lt;/td&gt; &#xA;    &lt;td&gt;Charlie&lt;/td&gt; &#xA;    &lt;td&gt;35&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;To talk to this database, run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;memgpt --archival_storage_sqldb=memgpt/personas/examples/sqldb/test.db&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;And then you can input the path to your database, and your query.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Please enter the path to the database. test.db&#xA;...&#xA;Enter your message: How old is Bob?&#xA;...&#xA;ü§ñ Bob is 25 years old.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Loading local files into archival memory&lt;/h3&gt;&lt;/summary&gt; MemGPT enables you to chat with your data locally -- this example gives the workflow for loading documents into MemGPT&#39;s archival memory. &#xA; &lt;p&gt;To run our example where you can search over the SEC 10-K filings of Uber, Lyft, and Airbnb,&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Download the .txt files from &lt;a href=&#34;https://huggingface.co/datasets/MemGPT/example-sec-filings/tree/main&#34;&gt;Hugging Face&lt;/a&gt; and place them in &lt;code&gt;memgpt/personas/examples/preload_archival&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;In the root &lt;code&gt;MemGPT&lt;/code&gt; directory, run&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;memgpt --archival_storage_files=&#34;memgpt/personas/examples/preload_archival/*.txt&#34; --persona=memgpt_doc --human=basic&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;If you would like to load your own local files into MemGPT&#39;s archival memory, run the command above but replace &lt;code&gt;--archival_storage_files=&#34;memgpt/personas/examples/preload_archival/*.txt&#34;&lt;/code&gt; with your own file glob expression (enclosed in quotes).&lt;/p&gt; &#xA; &lt;h4&gt;Enhance with embeddings search&lt;/h4&gt; &#xA; &lt;p&gt;In the root &lt;code&gt;MemGPT&lt;/code&gt; directory, run&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;memgpt main.py --archival_storage_files_compute_embeddings=&#34;&amp;lt;GLOB_PATTERN&amp;gt;&#34; --persona=memgpt_doc --human=basic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;This will generate embeddings, stick them into a FAISS index, and write the index to a directory, and then output:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;  To avoid computing embeddings next time, replace --archival_storage_files_compute_embeddings=&amp;lt;GLOB_PATTERN&amp;gt; with&#xA;    --archival_storage_faiss_path=&amp;lt;DIRECTORY_WITH_EMBEDDINGS&amp;gt; (if your files haven&#39;t changed).&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If you want to reuse these embeddings, run&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;memgpt --archival_storage_faiss_path=&#34;&amp;lt;DIRECTORY_WITH_EMBEDDINGS&amp;gt;&#34; --persona=memgpt_doc --human=basic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Talking to LlamaIndex API Docs&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;MemGPT also enables you to chat with docs -- try running this example to talk to the LlamaIndex API docs!&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;a. Download LlamaIndex API docs and FAISS index from &lt;a href=&#34;https://huggingface.co/datasets/MemGPT/llamaindex-api-docs&#34;&gt;Hugging Face&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Make sure you have git-lfs installed (https://git-lfs.com)&#xA;git lfs install&#xA;git clone https://huggingface.co/datasets/MemGPT/llamaindex-api-docs&#xA;mv llamaindex-api-docs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;-- OR --&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;b. Build the index:&lt;/p&gt; &#xA;   &lt;ol&gt; &#xA;    &lt;li&gt;Build &lt;code&gt;llama_index&lt;/code&gt; API docs with &lt;code&gt;make text&lt;/code&gt;. Instructions &lt;a href=&#34;https://github.com/run-llama/llama_index/raw/main/docs/DOCS_README.md&#34;&gt;here&lt;/a&gt;. Copy over the generated &lt;code&gt;_build/text&lt;/code&gt; folder to &lt;code&gt;memgpt/personas/docqa&lt;/code&gt;.&lt;/li&gt; &#xA;    &lt;li&gt;Generate embeddings and FAISS index. &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd memgpt/personas/docqa&#xA;python3 scrape_docs.py&#xA;python3 generate_embeddings_for_docs.py all_docs.jsonl&#xA;python3 build_index.py --embedding_files all_docs.embeddings.jsonl --output_index_file all_docs.index&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;/ol&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;In the root &lt;code&gt;MemGPT&lt;/code&gt; directory, run&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;memgpt --archival_storage_faiss_path=&amp;lt;ARCHIVAL_STORAGE_FAISS_PATH&amp;gt; --persona=memgpt_doc --human=basic&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;where &lt;code&gt;ARCHIVAL_STORAGE_FAISS_PATH&lt;/code&gt; is the directory where &lt;code&gt;all_docs.jsonl&lt;/code&gt; and &lt;code&gt;all_docs.index&lt;/code&gt; are located. If you downloaded from Hugging Face, it will be &lt;code&gt;memgpt/personas/docqa/llamaindex-api-docs&lt;/code&gt;. If you built the index yourself, it will be &lt;code&gt;memgpt/personas/docqa&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;If you have any further questions, or have anything to share, we are excited to hear your feedback!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;By default MemGPT will use &lt;code&gt;gpt-4&lt;/code&gt;, so your API key will require &lt;code&gt;gpt-4&lt;/code&gt; API access&lt;/li&gt; &#xA; &lt;li&gt;For issues and feature requests, please &lt;a href=&#34;https://github.com/cpacker/MemGPT/issues&#34;&gt;open a GitHub issue&lt;/a&gt; or message us on our &lt;code&gt;#support&lt;/code&gt; channel on &lt;a href=&#34;https://discord.gg/9GEQrxmVyE&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Datasets used in our &lt;a href=&#34;https://arxiv.org/abs/2310.08560&#34;&gt;paper&lt;/a&gt; can be downloaded at &lt;a href=&#34;https://huggingface.co/MemGPT&#34;&gt;Hugging Face&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Project Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release MemGPT Discord bot demo (perpetual chatbot)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add additional workflows (load SQL/text into MemGPT external context)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Integration tests&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Integrate with AutoGen (&lt;a href=&#34;https://github.com/cpacker/MemGPT/discussions/65&#34;&gt;discussion&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add official gpt-3.5-turbo support (&lt;a href=&#34;https://github.com/cpacker/MemGPT/discussions/66&#34;&gt;discussion&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; CLI UI improvements (&lt;a href=&#34;https://github.com/cpacker/MemGPT/issues/11&#34;&gt;issue&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add support for other LLM backends (&lt;a href=&#34;https://github.com/cpacker/MemGPT/issues/18&#34;&gt;issue&lt;/a&gt;, &lt;a href=&#34;https://github.com/cpacker/MemGPT/discussions/67&#34;&gt;discussion&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release MemGPT family of open models (eg finetuned Mistral) (&lt;a href=&#34;https://github.com/cpacker/MemGPT/discussions/67&#34;&gt;discussion&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Reminder: if you do not plan on modifying the source code, simply install MemGPT with &lt;code&gt;pip install pymemgpt&lt;/code&gt;!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;First, install Poetry using &lt;a href=&#34;https://python-poetry.org/docs/#installing-with-the-official-installer&#34;&gt;the official instructions here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then, you can install MemGPT from source with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:cpacker/MemGPT.git&#xA;poetry shell&#xA;poetry install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend installing pre-commit to ensure proper formatting during development:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install pre-commit&#xA;pre-commit install&#xA;pre-commit run --all-files&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;p&gt;We welcome pull requests! Please run the formatter before submitting a pull request:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;poetry run black . -l 140&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>apache/incubator-answer</title>
    <updated>2023-10-29T01:46:17Z</updated>
    <id>tag:github.com,2023-10-29:/apache/incubator-answer</id>
    <link href="https://github.com/apache/incubator-answer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Q&amp;A platform software for teams at any scales. Whether it&#39;s a community forum, help center, or knowledge management platform, you can always count on Answer.&lt;/p&gt;&lt;hr&gt;&lt;a href=&#34;https://answer.dev&#34;&gt; &lt;img alt=&#34;logo&#34; src=&#34;https://raw.githubusercontent.com/apache/incubator-answer/main/docs/img/logo.svg?sanitize=true&#34; height=&#34;99px&#34;&gt; &lt;/a&gt; &#xA;&lt;h1&gt;Answer - Build Q&amp;amp;A platform&lt;/h1&gt; &#xA;&lt;p&gt;A Q&amp;amp;A platform software for teams at any scales. Whether it‚Äôs a community forum, help center, or knowledge management platform, you can always count on Answer.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about the project, visit &lt;a href=&#34;https://answer.dev&#34;&gt;answer.dev&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/answerdev/answer/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/answerdev/answer&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://golang.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/language-go-blue.svg?sanitize=true&#34; alt=&#34;Language&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://reactjs.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/language-react-blue.svg?sanitize=true&#34; alt=&#34;Language&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://goreportcard.com/report/github.com/answerdev/answer&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/github.com/answerdev/answer&#34; alt=&#34;Go Report Card&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/Jm7Y4cbUej&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/discord-chat-5865f2?logo=discord&amp;amp;logoColor=f5f5f5&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/incubator-answer/main/docs/img/screenshot.png&#34; alt=&#34;screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;h3&gt;Running with docker&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d -p 9080:80 -v answer-data:/data --name answer answerdev/answer:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information, see &lt;a href=&#34;https://answer.dev/docs/installation&#34;&gt;Installation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Plugins&lt;/h3&gt; &#xA;&lt;p&gt;Answer provides a plugin system for developers to create custom plugins and expand Answer‚Äôs features. You can find the &lt;a href=&#34;https://answer.dev/docs/development/extending/&#34;&gt;plugin documentation here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We value your feedback and suggestions to improve our documentation. If you have any comments or questions, please feel free to contact us. We‚Äôre excited to see what you can create using our plugin system!&lt;/p&gt; &#xA;&lt;p&gt;You can also check out the &lt;a href=&#34;https://github.com/answerdev/plugins&#34;&gt;plugins here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are always welcome!&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://answer.dev/docs/development/contributing/&#34;&gt;CONTRIBUTING&lt;/a&gt; for ways to get started.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/answerdev/answer/raw/main/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenBMB/XAgent</title>
    <updated>2023-10-29T01:46:17Z</updated>
    <id>tag:github.com,2023-10-29:/OpenBMB/XAgent</id>
    <link href="https://github.com/OpenBMB/XAgent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An Autonomous LLM Agent for Complex Task Solving&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/assets/readme/xagent_logo.png&#34; height=&#34;40&#34; align=&#34;texttop&#34;&gt;XAgent&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://twitter.com/XAgentTeam&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/XAgent?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/zncs5aQkWZ&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/XAgent-Discord-purple?style=flat&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/license/apache-2-0/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;License: Apache 2.0&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat&#34; alt=&#34;Welcome&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a&gt;English&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/README_ZH.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/#Quickstart&#34;&gt;Tutorial&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://www.youtube.com/watch?v=QGkpd-tsFPA&#34;&gt;Demo&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://blog.x-agent.net/blog/xagent/&#34;&gt;Blog&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/#Citation&#34;&gt;Citation&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;üìñ Introduction&lt;/h1&gt; &#xA;&lt;p&gt;XAgent is an open-source experimental Large Language Model (LLM) driven autonomous agent that can automatically solve various tasks. It is designed to be a general-purpose agent that can be applied to a wide range of tasks. XAgent is still in its early stages, and we are working hard to improve it.&lt;/p&gt; &#xA;&lt;p&gt;üèÜ Our goal is to create a super-intelligent agent that can solve any given task!&lt;/p&gt; &#xA;&lt;p&gt;We welcome diverse forms of collaborations, including full-time and part-time roles and more. If you are interested in the frontiers of agents and want to join us in realizing true autonomous agents, please contact us at &lt;a href=&#34;mailto:xagentteam@gmail.com&#34;&gt;xagentteam@gmail.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/assets/readme/overview.png&#34; alt=&#34;Overview of Xagent&#34; width=&#34;700&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;figcaption&gt;&#xA;  Overview of XAgent.&#xA; &lt;/figcaption&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/assets/readme/xagent_logo.png&#34; height=&#34;30&#34; align=&#34;texttop&#34;&gt; XAgent&lt;/h2&gt; &#xA;&lt;p&gt;XAgent is designed with the following features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Autonomy&lt;/strong&gt;: XAgent can automatically solve various tasks without human participation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Safety&lt;/strong&gt;: XAgent is designed to run safely. All actions are constrained inside a docker container. Run it anyway!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extensibility&lt;/strong&gt;: XAgent is designed to be extensible. You can easily add new tools to enhance agent&#39;s abilities and even new agentsÔºÅ&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GUI&lt;/strong&gt;: XAgent provides a friendly GUI for users to interact with the agent. You can also use the command line interface to interact with the agent.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cooperation with Human&lt;/strong&gt;: XAgent can collaborate with you to tackle tasks. It not only has the capability to follow your guidance in solving complex tasks on the go but it can also seek your assistance when it encounters challenges.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;XAgent is composed of three parts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;ü§ñ Dispatcher&lt;/strong&gt; is responsible for dynamically instantiating and dispatching tasks to different agents. It allows us to add new agents and improve the agents&#39; abilities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üßê Planner&lt;/strong&gt; is responsible for generating and rectifying plans for tasks. It divides tasks into subtasks and generates milestones for them, allowing agents to solve tasks step by step.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ü¶æ Actor&lt;/strong&gt; is responsible for conducting actions to achieve goals and finish subtasks. The actor utilizes various tools to solve subtasks, and it can also collaborate with humans to solve tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/assets/readme/loop.png&#34; alt=&#34;Planner loop&#34; width=&#34;700&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;figcaption&gt;&#xA;  The inner loop and outer loop mechanism of XAgent.&#xA; &lt;/figcaption&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üß∞ ToolServer&lt;/h2&gt; &#xA;&lt;p&gt;ToolServer is the server that provides XAgent with powerful and safe tools to solve tasks. It is a docker container that provides a safe environment for XAgent to run. Currently, ToolServer provides the following tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;üìù File Editor&lt;/strong&gt; provides a text editing tool to write, read, and modify files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üìò Python Notebook&lt;/strong&gt; provides an interactive Python notebook that can run Python code to validate ideas, draw figures, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üåè Web Browser&lt;/strong&gt; provides a web browser to search and visit webpages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üñ•Ô∏è Shell&lt;/strong&gt; provides a bash shell tool that can execute any shell commands, even install programs and host services.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üß© Rapid API&lt;/strong&gt; provides a tool to retrieve APIs from Rapid API and call them, which offers a wide range of APIs for XAgent to use. See &lt;a href=&#34;https://github.com/OpenBMB/ToolBench&#34;&gt;ToolBench&lt;/a&gt; to get more information about the Rapid API collections. You can also easily add new tools to ToolServer to enhance XAgent&#39;s abilities.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div&gt;&#xA; &lt;a id=&#34;Quickstart&#34;&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;h1&gt;‚ú® Quickstart&lt;/h1&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Build and Setup ToolServer&lt;/h2&gt; &#xA;&lt;p&gt;ToolServer is where XAgent&#39;s action takes place. It is a docker container that provides a safe environment for XAgent to run. So you should install &lt;code&gt;docker&lt;/code&gt; and &lt;code&gt;docker-compose&lt;/code&gt; first. After that, you should build the docker image for ToolServer and start the docker container.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/ToolServer/README.md&#34;&gt;here&lt;/a&gt; for detailed information about our ToolServer.&lt;/p&gt; &#xA;&lt;p&gt;If the ToolServer is updated, you have to rebuild the images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üéÆ Setup and Run XAgent&lt;/h2&gt; &#xA;&lt;p&gt;After setting up ToolServer, you can start to run XAgent.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install requirements (Require Python &amp;gt;= 3.10)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Configure XAgent&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You should configure XAgent in &lt;code&gt;assets/config.yml&lt;/code&gt; before running it.&lt;/li&gt; &#xA; &lt;li&gt;At least one OpenAI key is provided in &lt;code&gt;assets/config.yml&lt;/code&gt;, which is used to access OpenAI API. We highly recommend using &lt;code&gt;gpt-4-32k&lt;/code&gt; to run XAgent; &lt;code&gt;gpt-4&lt;/code&gt; is also OK for most simple tasks. In any case, at least one &lt;code&gt;gpt-3.5-turbo-16k&lt;/code&gt; API key should be provided as a backup model. We do not test or recommend using &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; to run XAgent due to minimal context length; you should not try to run XAgent on that.&lt;/li&gt; &#xA; &lt;li&gt;If you want to change the config_file path for &lt;code&gt;XAgentServer&lt;/code&gt;, you should modify the &lt;code&gt;CONFIG_FILE&lt;/code&gt; value in &lt;code&gt;.env&lt;/code&gt; file and restart the docker container.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run XAgent&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run.py --task &#34;put your task here&#34; --model &#34;gpt-4&#34; --config_file &#34;assets/config.yml&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;You can use the argument &lt;code&gt;--upload_files&lt;/code&gt; to select the initial files you want to submit to XAgent.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The local workspace for your XAgent is in &lt;code&gt;local_workspace&lt;/code&gt;, where you can find all the files generated by XAgent throughout the running process.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;After execution, the entire &lt;code&gt;workspace&lt;/code&gt; in &lt;code&gt;ToolServerNode&lt;/code&gt; will be copied to &lt;code&gt;running_records&lt;/code&gt; for your convenience.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Besides, in &lt;code&gt;running_records&lt;/code&gt;, you can find all the intermediate steps information, e.g., task statuses, LLM&#39;s input-output pairs, used tools, etc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can load from a record to reproduce a former run, just by setting &lt;code&gt;record_dir&lt;/code&gt; in config(default to &lt;code&gt;Null&lt;/code&gt;). The record is a system-level recording tied to the code version of XAgent. All running-config„ÄÅquery„ÄÅcode execution statuses (including errors)„ÄÅserver behavior will be documented.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We have removed all sensitive information (including API keys) from the record so you can safely share it with others. In the near future, we will introduce more granular sharing options highlighting the contributions of humans during execution.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run XAgent with GUI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# We ran the web ui docker when building the ToolServer network&#xA;# run nginx in docker&#xA;docker exec XAgent-Server systemctl start nginx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build the docker image for XAgent-Server and start the docker container. You will see the XAgent Server listening on port &lt;code&gt;8090&lt;/code&gt;. You could visit &lt;code&gt;http://localhost:5173&lt;/code&gt; to interact with XAgent by using web UI. Refer &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/XAgentServer/README.md&#34;&gt;here&lt;/a&gt; for the detailed information about our GUI Demo.&lt;/p&gt; &#xA;&lt;div&gt;&#xA; &lt;a id=&#34;Demo&#34;&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;h1&gt;üé¨ Demo&lt;/h1&gt; &#xA;&lt;p&gt;Here, we also show some cases of solving tasks by XAgent: You can check our live demo on &lt;a href=&#34;https://www.x-agent.net/&#34;&gt;XAgent Official Website&lt;/a&gt;. We also provide a video demo and showcases of using XAgent here: &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/assets/readme/demo.gif&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Case 1. Data Analysis: Demonstrating the Effectiveness of Dual-Loop Mechanism&lt;/h2&gt; &#xA;&lt;p&gt;We start with a case of aiding users in intricate data analysis. Here, our user submitted an &lt;code&gt;iris.zip&lt;/code&gt; file to XAgent, seeking assistance in data analysis. XAgent swiftly broke down the task into four sub-tasks: (1) data inspection and comprehension, (2) verification of the system&#39;s Python environment for relevant data analysis libraries, (3) crafting data analysis code for data processing and analysis, and (4) compiling an analytical report based on the Python code&#39;s execution results. Here is a figure drawn by XAgent. &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/assets/readme/statistics.png&#34; alt=&#34;Data Statics by XAgent&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Case 2. Recommendation: A New Paradigm of Human-Agent Interaction&lt;/h2&gt; &#xA;&lt;p&gt;Empowered with the unique capability to actively seek human assistance and collaborate in problem-solving, XAgent continues to redefine the boundaries of human-agent cooperation. As depicted in the screenshot below, a user sought XAgent&#39;s aid in recommending some great restaurants for a friendly gathering yet failed to provide specific details. Recognizing the insufficiency of the provided information, XAgent employed the AskForHumanHelp tool, prompting human intervention to elicit the user&#39;s preferred location, budget constraints, culinary preferences, and dietary restrictions. Armed with this valuable feedback, XAgent seamlessly generated tailored restaurant recommendations, ensuring a personalized and satisfying experience for the user and their friends.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/assets/readme/ask_for_human_help.png&#34; alt=&#34;Illustration of Ask for Human Help of XAgent&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Case 3. Training Model: A Sophisticated Tool User&lt;/h2&gt; &#xA;&lt;p&gt;XAgent not only tackles mundane tasks but also serves as an invaluable aid in complex tasks such as model training. Here, we show a scenario where a user desires to analyze movie reviews and evaluate the public sentiment surrounding particular films. In response, XAgent promptly initiates the process by downloading the IMDB dataset to train a cutting-edge BERT model (see screenshot below), harnessing the power of deep learning. Armed with this trained BERT model, XAgent seamlessly navigates the intricate nuances of movie reviews, offering insightful predictions regarding the public&#39;s perception of various films.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/assets/readme/bert_1.png&#34; alt=&#34;bert_1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/assets/readme/bert_2.png&#34; alt=&#34;bert_2&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/assets/readme/bert_3.png&#34; alt=&#34;bert_3&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìä Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We conduct human preference evaluation to evaluate XAgent&#39;s performance. We prepare over 50 real-world complex tasks for assessment, which can be categorized into 5 classes: Search and Report, Coding and Developing, Data Analysis, Math, and Life Assistant. We compare the results of XAgent with &lt;a href=&#34;https://github.com/Significant-Gravitas/AutoGPT&#34;&gt;AutoGPT&lt;/a&gt;, which shows a total win of XAgent over AutoGPT. All running records will be released soon.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/assets/readme/agent_comparison.png&#34; alt=&#34;HumanPrefer&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We report a significant improvement of XAgent over AutoGPT in terms of human preference.&lt;/p&gt; &#xA;&lt;p&gt;We also evaluate XAgent on the following benchmarks: &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/XAgent/main/assets/readme/eval_on_dataset.png&#34; alt=&#34;Benchmarks&#34;&gt;&lt;/p&gt; &#xA;&lt;div&gt;&#xA; &lt;a id=&#34;Blog&#34;&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;h1&gt;üñåÔ∏è Blog&lt;/h1&gt; &#xA;&lt;p&gt;Our blog is available at &lt;a href=&#34;https://blog.x-agent.net/&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;div&gt;&#xA; &lt;a id=&#34;Citation&#34;&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;h1&gt;üåü Star History&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#openbmb/xagent&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=openbmb/xagent&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you find our repo useful, please kindly consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-angular2&#34;&gt;@misc{xagent2023,&#xA;      title={XAgent: An Autonomous Agent for Complex Task Solving}, &#xA;      author={XAgent Team},&#xA;      year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>