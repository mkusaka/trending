<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-17T01:36:37Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>haydenbleasel/next-forge</title>
    <updated>2024-11-17T01:36:37Z</updated>
    <id>tag:github.com,2024-11-17:/haydenbleasel/next-forge</id>
    <link href="https://github.com/haydenbleasel/next-forge" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Production-grade Turborepo template for Next.js apps.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;next-forge&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Production-grade Turborepo template for Next.js apps.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/haydenbleasel/next-forge&#34;&gt;next-forge&lt;/a&gt; is a &lt;a href=&#34;https://nextjs.org/&#34;&gt;Next.js&lt;/a&gt; project boilerplate for modern web application. It is designed to be a comprehensive starting point for new apps, providing a solid, opinionated foundation with a minimal amount of configuration.&lt;/p&gt; &#xA;&lt;p&gt;Clone the repo using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npx next-forge@latest init [my-project]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then read the &lt;a href=&#34;https://docs.next-forge.com&#34;&gt;docs&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/haydenbleasel/next-forge/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=haydenbleasel/next-forge&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Made with &lt;a href=&#34;https://contrib.rocks&#34;&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LibraHp/GetQzonehistory</title>
    <updated>2024-11-17T01:36:37Z</updated>
    <id>tag:github.com,2024-11-17:/LibraHp/GetQzonehistory</id>
    <link href="https://github.com/LibraHp/GetQzonehistory" rel="alternate"></link>
    <summary type="html">&lt;p&gt;获取QQ空间发布的历史说说&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GetQzonehistory(获取qq发布的历史说说)&lt;/h1&gt; &#xA;&lt;p&gt;已经打包好的单文件可以直接执行，在release页面。&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;font color=&#34;#FF0000&#34; size=&#34;5&#34;&gt;免责声明【必读】&lt;/font&gt;&lt;/summary&gt; &#xA; &lt;p&gt;本工具仅供学习和技术研究使用，不得用于任何商业或非法行为，否则后果自负。&lt;/p&gt; &#xA; &lt;p&gt;本工具的作者不对本工具的安全性、完整性、可靠性、有效性、正确性或适用性做任何明示或暗示的保证，也不对本工具的使用或滥用造成的任何直接或间接的损失、责任、索赔、要求或诉讼承担任何责任。&lt;/p&gt; &#xA; &lt;p&gt;本工具的作者保留随时修改、更新、删除或终止本工具的权利，无需事先通知或承担任何义务。&lt;/p&gt; &#xA; &lt;p&gt;本工具的使用者应遵守相关法律法规，尊重QQ的版权和隐私，不得侵犯QQ或其他第三方的合法权益，不得从事任何违法或不道德的行为。&lt;/p&gt; &#xA; &lt;p&gt;本工具的使用者在下载、安装、运行或使用本工具时，即表示已阅读并同意本免责声明。如有异议，请立即停止使用本工具，并删除所有相关文件。&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;该项目通过获取QQ空间的历史消息列表来获取该账号下发布的所有说说（当然消息列表中没有的就获取不到，例如一些仅自己可见的说说）&lt;a href=&#34;https://space.bilibili.com/1117414477&#34;&gt;B站有详细食用教程&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;主要实现还是通过模拟登录QQ空间来获取历史消息列表，然后进行数据分析，最后将爬取的说说存放到/resource/result目录下&lt;/p&gt; &#xA;&lt;p&gt;由于对python编程还不是很熟悉，所以代码有很多疏漏，可以通过自己的想法来完善代码&lt;/p&gt; &#xA;&lt;h2&gt;目录结构&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;project/&#xA;├── resource/                # 资源目录&#xA;│   ├── config/              # 配置目录，文件保存位置配置&#xA;│   │   └── config.ini&#xA;│   ├── result/              # 导出结果的目录，格式为“你的qq.xlsx”&#xA;│   │   ├── ...&#xA;│   │   └── ...&#xA;│   ├── temp/                # 缓存目录&#xA;│   │   ├── ...&#xA;│   │   └── ...&#xA;│   ├── user/                # 用户信息&#xA;│   │   ├── ...&#xA;│   │   └── ...&#xA;├── util/                    # 单元工具目录&#xA;│   ├── ConfigUtil.py        # 读取配置&#xA;│   ├── GetAllMomentsUtil.py # 获取未删除的所有说说&#xA;│   ├── LoginUtil.py         # 登录相关&#xA;│   ├── RequestUtil.py       # 请求数据相关&#xA;│   └── ToolsUtil.py         # 工具&#xA;├── main.py                  # 主程序入口&#xA;├── fetch_all_message.py     # 主程序入口&#xA;├── README.md                # 项目说明文件&#xA;├── requirements.txt         # 依赖项列表&#xA;└── LICENSE                  # 许可证文件&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;安装&lt;/h2&gt; &#xA;&lt;h4&gt;使用虚拟环境（推荐）&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 克隆储存库&#xA;git clone https://github.com/LibraHp/GetQzonehistory.git&#xA;# 打开目录&#xA;cd GetQzonehistory&#xA;# 创建名为 myenv 的虚拟环境&#xA;python -m venv myenv&#xA;# 激活虚拟环境。在终端或命令提示符中运行以下命令：&#xA;# 对于 Windows：&#xA;myenv\Scripts\activate&#xA;# 对于 macOS/Linux：&#xA;source myenv/bin/activate&#xA;# 安装依赖&#xA;pip install -i https://mirrors.aliyun.com/pypi/simple/ -r requirements.txt&#xA;# 运行脚本&#xA;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;使用本机环境（不推荐）&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 克隆储存库&#xA;git clone https://github.com/LibraHp/GetQzonehistory.git&#xA;# 打开目录&#xA;cd GetQzonehistory&#xA;# 安装依赖&#xA;pip install -i https://mirrors.aliyun.com/pypi/simple/ -r requirements.txt&#xA;# 运行脚本&#xA;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;参考&lt;/h2&gt; &#xA;&lt;p&gt;登录方法参考自 &lt;a href=&#34;https://blog.csdn.net/m0_50153253/article/details/113780595&#34;&gt;python-QQ空间扫码登录&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kijai/ComfyUI-CogVideoXWrapper</title>
    <updated>2024-11-17T01:36:37Z</updated>
    <id>tag:github.com,2024-11-17:/kijai/ComfyUI-CogVideoXWrapper</id>
    <link href="https://github.com/kijai/ComfyUI-CogVideoXWrapper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;WORK IN PROGRESS&lt;/h1&gt; &#xA;&lt;h2&gt;Update7&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Refactored the Fun version&#39;s sampler to accept any resolution, this should make it lot simpler to use with Tora. &lt;strong&gt;BREAKS OLD WORKFLOWS&lt;/strong&gt;, old FunSampler nodes need to be remade.&lt;/li&gt; &#xA; &lt;li&gt;The old bucket resizing is now on it&#39;s own node (CogVideoXFunResizeToClosestBucket) to keep the functionality, I honestly don&#39;t know if it matters at all, but just in case.&lt;/li&gt; &#xA; &lt;li&gt;Fun version&#39;s vid2vid is now also in the same node, the old vid2vid node is deprecated.&lt;/li&gt; &#xA; &lt;li&gt;Added support for FasterCache, this trades more VRAM use for speed with slight quality hit, similar to PAB: &lt;a href=&#34;https://github.com/Vchitect/FasterCache&#34;&gt;https://github.com/Vchitect/FasterCache&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Improved torch.compile support, it actually works now&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Update6&lt;/h2&gt; &#xA;&lt;p&gt;Initial support for Tora (&lt;a href=&#34;https://github.com/alibaba/Tora&#34;&gt;https://github.com/alibaba/Tora&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Converted model (included in the autodownload node):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/Kijai/CogVideoX-5b-Tora/tree/main&#34;&gt;https://huggingface.co/Kijai/CogVideoX-5b-Tora/tree/main&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/d5334237-03dc-48f5-8bec-3ae5998660c6&#34;&gt;https://github.com/user-attachments/assets/d5334237-03dc-48f5-8bec-3ae5998660c6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Update5&lt;/h2&gt; &#xA;&lt;p&gt;This week there&#39;s been some bigger updates that will most likely affect some old workflows, sampler node especially probably need to be refreshed (re-created) if it errors out!&lt;/p&gt; &#xA;&lt;p&gt;New features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial context windowing with FreeNoise noise shuffling mainly for vid2vid and pose2vid pipelines for longer generations, haven&#39;t figured it out for img2vid yet&lt;/li&gt; &#xA; &lt;li&gt;GGUF models and tiled encoding for I2V and pose pipelines (thanks to MinusZoneAI)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thu-ml/SageAttention&#34;&gt;sageattention&lt;/a&gt; support (Linux only) for a speed boost, I experienced ~20-30% increase with it, stacks with fp8 fast mode, doesn&#39;t need compiling&lt;/li&gt; &#xA; &lt;li&gt;Support CogVideoX-Fun 1.1 and it&#39;s pose models with additional control strength and application step settings, this model&#39;s input does NOT have to be just dwpose skeletons, just about anything can work&lt;/li&gt; &#xA; &lt;li&gt;Support LoRAs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/ddeb8f38-a647-42b3-a4b1-c6936f961deb&#34;&gt;https://github.com/user-attachments/assets/ddeb8f38-a647-42b3-a4b1-c6936f961deb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/c78b2832-9571-4941-8c97-fbcc1a4cc23d&#34;&gt;https://github.com/user-attachments/assets/c78b2832-9571-4941-8c97-fbcc1a4cc23d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/d9ed98b1-f917-432b-a16e-e01e87efb1f9&#34;&gt;https://github.com/user-attachments/assets/d9ed98b1-f917-432b-a16e-e01e87efb1f9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Update4&lt;/h2&gt; &#xA;&lt;p&gt;Initial support for the official I2V version of CogVideoX: &lt;a href=&#34;https://huggingface.co/THUDM/CogVideoX-5b-I2V&#34;&gt;https://huggingface.co/THUDM/CogVideoX-5b-I2V&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Also needs diffusers 0.30.3&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/c672d0af-a676-495d-a42c-7e3dd802b4b0&#34;&gt;https://github.com/user-attachments/assets/c672d0af-a676-495d-a42c-7e3dd802b4b0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Update3&lt;/h2&gt; &#xA;&lt;p&gt;Added initial support for CogVideoX-Fun: &lt;a href=&#34;https://github.com/aigc-apps/CogVideoX-Fun&#34;&gt;https://github.com/aigc-apps/CogVideoX-Fun&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that while this one can do image2vid, this is NOT the official I2V model yet, though it should also be released very soon.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/68f9ed16-ee53-4955-b931-1799461ac561&#34;&gt;https://github.com/user-attachments/assets/68f9ed16-ee53-4955-b931-1799461ac561&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updade2&lt;/h2&gt; &#xA;&lt;p&gt;Added &lt;strong&gt;experimental&lt;/strong&gt; support for onediff, this reduced sampling time by ~40% for me, reaching 4.23 s/it on 4090 with 49 frames. This requires using Linux, torch 2.4.0, onediff and nexfort installation:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install --pre onediff onediffx&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install nexfort&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;First run will take around 5 mins for the compilation.&lt;/p&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;p&gt;5b model is now also supported for basic text2vid: &lt;a href=&#34;https://huggingface.co/THUDM/CogVideoX-5b&#34;&gt;https://huggingface.co/THUDM/CogVideoX-5b&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;It is also autodownloaded to &lt;code&gt;ComfyUI/models/CogVideo/CogVideoX-5b&lt;/code&gt;, text encoder is not needed as we use the ComfyUI T5.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/991205cc-826e-4f93-831a-c10441f0f2ce&#34;&gt;https://github.com/user-attachments/assets/991205cc-826e-4f93-831a-c10441f0f2ce&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Requires diffusers 0.30.1 (this is specified in requirements.txt)&lt;/p&gt; &#xA;&lt;p&gt;Uses same T5 model than SD3 and Flux, fp8 works fine too. Memory requirements depend mostly on the video length. VAE decoding seems to be the only big that takes a lot of VRAM when everything is offloaded, peaks at around 13-14GB momentarily at that stage. Sampling itself takes only maybe 5-6GB.&lt;/p&gt; &#xA;&lt;p&gt;Hacked in img2img to attempt vid2vid workflow, works interestingly with some inputs, highly experimental.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/e6951ef4-ea7a-4752-94f6-cf24f2503d83&#34;&gt;https://github.com/user-attachments/assets/e6951ef4-ea7a-4752-94f6-cf24f2503d83&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/9e41f37b-2bb3-411c-81fa-e91b80da2559&#34;&gt;https://github.com/user-attachments/assets/9e41f37b-2bb3-411c-81fa-e91b80da2559&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Also added temporal tiling as means of generating endless videos:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/kijai/ComfyUI-CogVideoXWrapper&#34;&gt;https://github.com/kijai/ComfyUI-CogVideoXWrapper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/ecdac8b8-d434-48b6-abd6-90755b6b552d&#34;&gt;https://github.com/user-attachments/assets/ecdac8b8-d434-48b6-abd6-90755b6b552d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Original repo: &lt;a href=&#34;https://github.com/THUDM/CogVideo&#34;&gt;https://github.com/THUDM/CogVideo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;CogVideoX-Fun: &lt;a href=&#34;https://github.com/aigc-apps/CogVideoX-Fun&#34;&gt;https://github.com/aigc-apps/CogVideoX-Fun&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Controlnet: &lt;a href=&#34;https://github.com/TheDenk/cogvideox-controlnet&#34;&gt;https://github.com/TheDenk/cogvideox-controlnet&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>