<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-19T01:50:37Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>oobabooga/text-generation-webui</title>
    <updated>2023-03-19T01:50:37Z</updated>
    <id>tag:github.com,2023-03-19:/oobabooga/text-generation-webui</id>
    <link href="https://github.com/oobabooga/text-generation-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Text generation web UI&lt;/h1&gt; &#xA;&lt;p&gt;A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.&lt;/p&gt; &#xA;&lt;p&gt;Its goal is to become the &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt; of text generation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/oobabooga/AI-Notebooks/blob/main/Colab-TextGen-GPU.ipynb&#34;&gt;[Try it on Google Colab]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/qa.png&#34; alt=&#34;Image1&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/cai3.png&#34; alt=&#34;Image2&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/gpt4chan.png&#34; alt=&#34;Image3&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/galactica.png&#34; alt=&#34;Image4&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Switch between different models using a dropdown menu.&lt;/li&gt; &#xA; &lt;li&gt;Notebook mode that resembles OpenAI&#39;s playground.&lt;/li&gt; &#xA; &lt;li&gt;Chat mode for conversation and role playing.&lt;/li&gt; &#xA; &lt;li&gt;Generate nice HTML output for GPT-4chan.&lt;/li&gt; &#xA; &lt;li&gt;Generate Markdown output for &lt;a href=&#34;https://github.com/paperswithcode/galai&#34;&gt;GALACTICA&lt;/a&gt;, including LaTeX support.&lt;/li&gt; &#xA; &lt;li&gt;Support for &lt;a href=&#34;https://huggingface.co/models?search=pygmalionai/pygmalion&#34;&gt;Pygmalion&lt;/a&gt; and custom characters in JSON or TavernAI Character Card formats (&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Pygmalion-chat-model-FAQ&#34;&gt;FAQ&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Advanced chat features (send images, get audio responses with TTS).&lt;/li&gt; &#xA; &lt;li&gt;Stream the text output in real time very efficiently.&lt;/li&gt; &#xA; &lt;li&gt;Load parameter presets from text files.&lt;/li&gt; &#xA; &lt;li&gt;Load large models in 8-bit mode.&lt;/li&gt; &#xA; &lt;li&gt;Split large models across your GPU(s), CPU, and disk.&lt;/li&gt; &#xA; &lt;li&gt;CPU mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/FlexGen&#34;&gt;FlexGen offload&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/DeepSpeed&#34;&gt;DeepSpeed ZeRO-3 offload&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Get responses via API, &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/raw/main/api-example-streaming.py&#34;&gt;with&lt;/a&gt; or &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/raw/main/api-example.py&#34;&gt;without&lt;/a&gt; streaming.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model&#34;&gt;LLaMA model, including 4-bit mode&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/RWKV-model&#34;&gt;RWKV model&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Using-LoRAs&#34;&gt;Supports LoRAs&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Supports softprompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Extensions&#34;&gt;Supports extensions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Running-on-Colab&#34;&gt;Works on Google Colab&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The recommended installation methods are the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux and MacOS: using conda natively.&lt;/li&gt; &#xA; &lt;li&gt;Windows: using conda on WSL (&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Windows-Subsystem-for-Linux-(Ubuntu)-Installation-Guide&#34;&gt;WSL installation guide&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Conda can be downloaded here: &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;https://docs.conda.io/en/latest/miniconda.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;On Linux or WSL, it can be automatically installed with these two commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -sL &#34;https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh&#34; &amp;gt; &#34;Miniconda3.sh&#34;&#xA;bash Miniconda3.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Source: &lt;a href=&#34;https://educe-ubc.github.io/conda.html&#34;&gt;https://educe-ubc.github.io/conda.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1. Create a new conda environment&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n textgen python=3.10.9&#xA;conda activate textgen&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Install Pytorch&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;System&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;Command&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux/WSL&lt;/td&gt; &#xA;   &lt;td&gt;NVIDIA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;pip3 install torch torchvision torchaudio&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux&lt;/td&gt; &#xA;   &lt;td&gt;AMD&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.4.2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MacOS + MPS (untested)&lt;/td&gt; &#xA;   &lt;td&gt;Any&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;pip3 install torch torchvision torchaudio&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The up to date commands can be found here: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;MacOS users, refer to the comments here: &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/pull/393&#34;&gt;https://github.com/oobabooga/text-generation-webui/pull/393&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3. Install the web UI&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/oobabooga/text-generation-webui&#xA;cd text-generation-webui&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;For bitsandbytes and &lt;code&gt;--load-in-8bit&lt;/code&gt; to work on Linux/WSL, this dirty fix is currently necessary: &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/400#issuecomment-1474876859&#34;&gt;https://github.com/oobabooga/text-generation-webui/issues/400#issuecomment-1474876859&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Alternative: native Windows installation&lt;/h3&gt; &#xA;&lt;p&gt;As an alternative to the recommended WSL method, you can install the web UI natively on Windows using this guide. It will be a lot harder and the performance may be slower: &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Installation-instructions-for-human-beings&#34;&gt;Installation instructions for human beings&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Alternative: one-click installers&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/oobabooga/one-click-installers/archive/refs/heads/oobabooga-windows.zip&#34;&gt;oobabooga-windows.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/oobabooga/one-click-installers/archive/refs/heads/oobabooga-linux.zip&#34;&gt;oobabooga-linux.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Just download the zip above, extract it, and double click on &#34;install&#34;. The web UI and all its dependencies will be installed in the same folder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To download a model, double click on &#34;download-model&#34;&lt;/li&gt; &#xA; &lt;li&gt;To start the web UI, double click on &#34;start-webui&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Source codes: &lt;a href=&#34;https://github.com/oobabooga/one-click-installers&#34;&gt;https://github.com/oobabooga/one-click-installers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This method lags behind the newest developments and does not support 8-bit mode on Windows without additional set up: &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/147#issuecomment-1456040134&#34;&gt;https://github.com/oobabooga/text-generation-webui/issues/147#issuecomment-1456040134&lt;/a&gt;, &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/20#issuecomment-1411650652&#34;&gt;https://github.com/oobabooga/text-generation-webui/issues/20#issuecomment-1411650652&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Alternative: Docker&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/174&#34;&gt;https://github.com/oobabooga/text-generation-webui/issues/174&lt;/a&gt;, &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/87&#34;&gt;https://github.com/oobabooga/text-generation-webui/issues/87&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Downloading models&lt;/h2&gt; &#xA;&lt;p&gt;Models should be placed inside the &lt;code&gt;models&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&#34;&gt;Hugging Face&lt;/a&gt; is the main place to download models. These are some noteworthy examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=eleutherai/pythia&#34;&gt;Pythia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=facebook/opt&#34;&gt;OPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=facebook/galactica&#34;&gt;GALACTICA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/tree/main&#34;&gt;GPT-J 6B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&amp;amp;search=eleutherai+%2F+gpt-neo&#34;&gt;GPT-Neo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=erebus&#34;&gt;*-Erebus&lt;/a&gt; (NSFW)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=pygmalion&#34;&gt;Pygmalion&lt;/a&gt; (NSFW)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can automatically download a model from HF using the script &lt;code&gt;download-model.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py organization/model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py facebook/opt-1.3b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to download a model manually, note that all you need are the json, txt, and pytorch*.bin (or model*.safetensors) files. The remaining files are not necessary.&lt;/p&gt; &#xA;&lt;h3&gt;GPT-4chan&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/ykilcher/gpt-4chan&#34;&gt;GPT-4chan&lt;/a&gt; has been shut down from Hugging Face, so you need to download it elsewhere. You have two options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Torrent: &lt;a href=&#34;https://archive.org/details/gpt4chan_model_float16&#34;&gt;16-bit&lt;/a&gt; / &lt;a href=&#34;https://archive.org/details/gpt4chan_model&#34;&gt;32-bit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Direct download: &lt;a href=&#34;https://theswissbay.ch/pdf/_notpdf_/gpt4chan_model_float16/&#34;&gt;16-bit&lt;/a&gt; / &lt;a href=&#34;https://theswissbay.ch/pdf/_notpdf_/gpt4chan_model/&#34;&gt;32-bit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The 32-bit version is only relevant if you intend to run the model in CPU mode. Otherwise, you should use the 16-bit version.&lt;/p&gt; &#xA;&lt;p&gt;After downloading the model, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Place the files under &lt;code&gt;models/gpt4chan_model_float16&lt;/code&gt; or &lt;code&gt;models/gpt4chan_model&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Place GPT-J 6B&#39;s config.json file in that same folder: &lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/raw/main/config.json&#34;&gt;config.json&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download GPT-J 6B&#39;s tokenizer files (they will be automatically detected when you attempt to load GPT-4chan):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py EleutherAI/gpt-j-6B --text-only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Starting the web UI&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate textgen&#xA;cd text-generation-webui&#xA;python server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then browse to&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;http://localhost:7860/?__theme=dark&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Optionally, you can use the following command-line flags:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Flag&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;show this help message and exit&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--model MODEL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Name of the model to load by default.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--lora LORA&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Name of the LoRA to apply to the model by default.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--notebook&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in notebook mode, where the output is written to the same text box as the input.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--chat&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in chat mode.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cai-chat&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in chat mode with a style similar to Character.AI&#39;s. If the file &lt;code&gt;img_bot.png&lt;/code&gt; or &lt;code&gt;img_bot.jpg&lt;/code&gt; exists in the same folder as server.py, this image will be used as the bot&#39;s profile picture. Similarly, &lt;code&gt;img_me.png&lt;/code&gt; or &lt;code&gt;img_me.jpg&lt;/code&gt; will be used as your profile picture.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cpu&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Use the CPU to generate text.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--load-in-8bit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the model with 8-bit precision.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--load-in-4bit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DEPRECATED: use &lt;code&gt;--gptq-bits 4&lt;/code&gt; instead.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--gptq-bits GPTQ_BITS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load a pre-quantized model with specified precision. 2, 3, 4 and 8 (bit) are supported. Currently only works with LLaMA and OPT.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--gptq-model-type MODEL_TYPE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Model type of pre-quantized model. Currently only LLaMa and OPT are supported.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--bf16&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the model with bfloat16 precision. Requires NVIDIA Ampere GPU.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--auto-devices&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Automatically split the model across the available GPU(s) and CPU.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--disk&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;If the model is too large for your GPU(s) and CPU combined, send the remaining layers to the disk.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--disk-cache-dir DISK_CACHE_DIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Directory to save the disk cache to. Defaults to &lt;code&gt;cache/&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--gpu-memory GPU_MEMORY [GPU_MEMORY ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maxmimum GPU memory in GiB to be allocated per GPU. Example: &lt;code&gt;--gpu-memory 10&lt;/code&gt; for a single GPU, &lt;code&gt;--gpu-memory 10 5&lt;/code&gt; for two GPUs.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cpu-memory CPU_MEMORY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maximum CPU memory in GiB to allocate for offloaded weights. Must be an integer number. Defaults to 99.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--flexgen&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Enable the use of FlexGen offloading.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--percent PERCENT [PERCENT ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: allocation percentages. Must be 6 numbers separated by spaces (default: 0, 100, 100, 0, 100, 0).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--compress-weight&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: Whether to compress weight (default: False).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--pin-weight [PIN_WEIGHT]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: whether to pin weights (setting this to False reduces CPU memory by 20%).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--deepspeed&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Enable the use of DeepSpeed ZeRO-3 for inference via the Transformers integration.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--nvme-offload-dir NVME_OFFLOAD_DIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeed: Directory to use for ZeRO-3 NVME offloading.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--local_rank LOCAL_RANK&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeed: Optional argument for distributed setups.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--rwkv-strategy RWKV_STRATEGY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RWKV: The strategy to use while loading the model. Examples: &#34;cpu fp32&#34;, &#34;cuda fp16&#34;, &#34;cuda fp16i8&#34;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--rwkv-cuda-on&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RWKV: Compile the CUDA kernel for better performance.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--no-stream&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Don&#39;t stream the text output in real time.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--settings SETTINGS_FILE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the default interface settings from this json file. See &lt;code&gt;settings-template.json&lt;/code&gt; for an example. If you create a file called &lt;code&gt;settings.json&lt;/code&gt;, this file will be loaded by default without the need to use the &lt;code&gt;--settings&lt;/code&gt; flag.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--extensions EXTENSIONS [EXTENSIONS ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The list of extensions to load. If you want to load more than one extension, write the names separated by spaces.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--listen&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Make the web UI reachable from your local network.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--listen-port LISTEN_PORT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The listening port that the server will use.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--share&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Create a public URL. This is useful for running the web UI on Google Colab or similar.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--auto-launch&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Open the web UI in the default browser upon launch.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--verbose&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Print the prompts to the terminal.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Out of memory errors? &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide&#34;&gt;Check this guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Presets&lt;/h2&gt; &#xA;&lt;p&gt;Inference settings presets can be created under &lt;code&gt;presets/&lt;/code&gt; as text files. These files are detected automatically at startup.&lt;/p&gt; &#xA;&lt;p&gt;By default, 10 presets by NovelAI and KoboldAI are included. These were selected out of a sample of 43 presets after applying a K-Means clustering algorithm and selecting the elements closest to the average of each cluster.&lt;/p&gt; &#xA;&lt;h2&gt;System requirements&lt;/h2&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/System-requirements&#34;&gt;wiki&lt;/a&gt; for some examples of VRAM and RAM usage in both GPU and CPU mode.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests, suggestions, and issue reports are welcome.&lt;/p&gt; &#xA;&lt;p&gt;Before reporting a bug, make sure that you have:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Created a conda environment and installed the dependencies exactly as in the &lt;em&gt;Installation&lt;/em&gt; section above.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues&#34;&gt;Searched&lt;/a&gt; to see if an issue already exists for the issue you encountered.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gradio dropdown menu refresh button, code for reloading the interface: &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Verbose preset: Anonymous 4chan user.&lt;/li&gt; &#xA; &lt;li&gt;NovelAI and KoboldAI presets: &lt;a href=&#34;https://github.com/KoboldAI/KoboldAI-Client/wiki/Settings-Presets&#34;&gt;https://github.com/KoboldAI/KoboldAI-Client/wiki/Settings-Presets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pygmalion preset, code for early stopping in chat mode, code for some of the sliders, --chat mode colors: &lt;a href=&#34;https://github.com/PygmalionAI/gradio-ui/&#34;&gt;https://github.com/PygmalionAI/gradio-ui/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>comfyanonymous/ComfyUI</title>
    <updated>2023-03-19T01:50:37Z</updated>
    <id>tag:github.com,2023-03-19:/comfyanonymous/ComfyUI</id>
    <link href="https://github.com/comfyanonymous/ComfyUI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A powerful and modular stable diffusion GUI with a graph/nodes interface.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI&lt;/h1&gt; &#xA;&lt;h2&gt;A powerful and modular stable diffusion GUI.&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/comfyui_screenshot.png&#34; alt=&#34;ComfyUI Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This ui will let you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. For some workflow examples and see what ComfyUI can do you can check out:&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/&#34;&gt;ComfyUI Examples&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.&lt;/li&gt; &#xA; &lt;li&gt;Fully supports SD1.x and SD2.x&lt;/li&gt; &#xA; &lt;li&gt;Asynchronous Queue system&lt;/li&gt; &#xA; &lt;li&gt;Many optimizations: Only re-executes the parts of the workflow that changes between executions.&lt;/li&gt; &#xA; &lt;li&gt;Command line option: &lt;code&gt;--lowvram&lt;/code&gt; to make it work on GPUs with less than 3GB vram (enabled automatically on GPUs with low vram)&lt;/li&gt; &#xA; &lt;li&gt;Works even if you don&#39;t have a GPU with: &lt;code&gt;--cpu&lt;/code&gt; (slow)&lt;/li&gt; &#xA; &lt;li&gt;Can load both ckpt and safetensors models/checkpoints. Standalone VAEs and CLIP models.&lt;/li&gt; &#xA; &lt;li&gt;Embeddings/Textual inversion&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/lora/&#34;&gt;Loras (regular and locon)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Loading full workflows (with seeds) from generated PNG files.&lt;/li&gt; &#xA; &lt;li&gt;Saving/Loading workflows as Json files.&lt;/li&gt; &#xA; &lt;li&gt;Nodes interface can be used to create complex workflows like one for &lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/&#34;&gt;Hires fix&lt;/a&gt; or much more advanced ones.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/area_composition/&#34;&gt;Area Composition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/inpaint/&#34;&gt;Inpainting&lt;/a&gt; with both regular and inpainting models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/controlnet/&#34;&gt;ControlNet and T2I-Adapter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/&#34;&gt;Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Starts up very fast.&lt;/li&gt; &#xA; &lt;li&gt;Works fully offline: will never download anything.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Workflow examples can be found on the &lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/&#34;&gt;Examples page&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installing&lt;/h1&gt; &#xA;&lt;h2&gt;Windows&lt;/h2&gt; &#xA;&lt;p&gt;There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI/releases&#34;&gt;releases page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_windows_portable_nvidia_cu118_or_cpu.7z&#34;&gt;Direct link to download&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Just download, extract and run. Make sure you put your Stable Diffusion checkpoints/models (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints&lt;/p&gt; &#xA;&lt;h2&gt;Colab Notebook&lt;/h2&gt; &#xA;&lt;p&gt;To run it on colab or paperspace you can use my &lt;a href=&#34;https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/notebooks/comfyui_colab.ipynb&#34;&gt;Colab Notebook&lt;/a&gt; here: &lt;a href=&#34;https://colab.research.google.com/github/comfyanonymous/ComfyUI/blob/master/notebooks/comfyui_colab.ipynb&#34;&gt;Link to open with google colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Manual Install (Windows, Linux)&lt;/h2&gt; &#xA;&lt;p&gt;Git clone this repo.&lt;/p&gt; &#xA;&lt;p&gt;Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints&lt;/p&gt; &#xA;&lt;p&gt;Put your VAE in: models/vae&lt;/p&gt; &#xA;&lt;p&gt;At the time of writing this pytorch has issues with python versions higher than 3.10 so make sure your python/pip versions are 3.10.&lt;/p&gt; &#xA;&lt;h3&gt;AMD (Linux only)&lt;/h3&gt; &#xA;&lt;p&gt;AMD users can install rocm and pytorch with pip if you don&#39;t have it already installed, this is the command to install the stable version:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/rocm5.4.2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;NVIDIA&lt;/h3&gt; &#xA;&lt;p&gt;Nvidia users should install torch and xformers using this command:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install torch==1.13.1 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117 xformers&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Troubleshooting&lt;/h4&gt; &#xA;&lt;p&gt;If you get the &#34;Torch not compiled with CUDA enabled&#34; error, uninstall torch with:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip uninstall torch&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;And install it again with the command above.&lt;/p&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Install the dependencies by opening your terminal inside the ComfyUI folder and:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;After this you should have everything installed and can proceed to running ComfyUI.&lt;/p&gt; &#xA;&lt;h3&gt;I already have another UI for Stable Diffusion installed do I really have to install all of these dependencies?&lt;/h3&gt; &#xA;&lt;p&gt;You don&#39;t. If you have another UI installed and working with it&#39;s own python venv you can use that venv to run ComfyUI. You can open up your favorite terminal and activate it:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;source path_to_other_sd_gui/venv/bin/activate&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;or on Windows:&lt;/p&gt; &#xA;&lt;p&gt;With Powershell: &lt;code&gt;&#34;path_to_other_sd_gui\venv\Scripts\Activate.ps1&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;With cmd.exe: &lt;code&gt;&#34;path_to_other_sd_gui\venv\Scripts\activate.bat&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;And then you can use that terminal to run Comfyui without installing any dependencies. Note that the venv folder might be called something else depending on the SD UI.&lt;/p&gt; &#xA;&lt;h1&gt;Running&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;For AMD 6700, 6600 and maybe others&lt;/h3&gt; &#xA;&lt;p&gt;Try running it with this command if you have issues:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Notes&lt;/h1&gt; &#xA;&lt;p&gt;Only parts of the graph that have an output with all the correct inputs will be executed.&lt;/p&gt; &#xA;&lt;p&gt;Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.&lt;/p&gt; &#xA;&lt;p&gt;Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.&lt;/p&gt; &#xA;&lt;p&gt;You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \( or \).&lt;/p&gt; &#xA;&lt;p&gt;You can use {day|night}, for wildcard/dynamic prompts. With this syntax &#34;{wild|card|test}&#34; will be randomly replaced by either &#34;wild&#34;, &#34;card&#34; or &#34;test&#34; by the frontend every time you queue the prompt. To use {} characters in your actual prompt escape them like: \{ or \}.&lt;/p&gt; &#xA;&lt;p&gt;To use a textual inversion concepts/embeddings in a text prompt put them in the models/embeddings directory and use them in the CLIPTextEncode node like this (you can omit the .pt extension):&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;embedding:embedding_filename.pt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Fedora&lt;/h3&gt; &#xA;&lt;p&gt;To get python 3.10 on fedora: &lt;code&gt;dnf install python3.10&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you can:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python3.10 -m ensurepip&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will let you use: pip3.10 to install all the dependencies.&lt;/p&gt; &#xA;&lt;h2&gt;How to increase generation speed?&lt;/h2&gt; &#xA;&lt;p&gt;The fp16 model configs in the CheckpointLoader can be used to load them in fp16 mode, depending on your GPU this will increase your gen speed by a significant amount.&lt;/p&gt; &#xA;&lt;p&gt;You can also set this command line setting to disable the upcasting to fp32 in some cross attention operations which will increase your speed. Note that this will very likely give you black images on SD2.x models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--dont-upcast-attention&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Support and dev channel&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://app.element.io/#/room/%23comfyui_space%3Amatrix.org&#34;&gt;Matrix space: #comfyui_space:matrix.org&lt;/a&gt; (it&#39;s like discord but open source).&lt;/p&gt; &#xA;&lt;h1&gt;QA&lt;/h1&gt; &#xA;&lt;h3&gt;Why did you make this?&lt;/h3&gt; &#xA;&lt;p&gt;I wanted to learn how Stable Diffusion worked in detail. I also wanted something clean and powerful that would let me experiment with SD without restrictions.&lt;/p&gt; &#xA;&lt;h3&gt;Who is this for?&lt;/h3&gt; &#xA;&lt;p&gt;This is for anyone that wants to make complex workflows with SD or that wants to learn more how SD works. The interface follows closely how SD works and the code should be much more simple to understand than other SD UIs.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yetone/openai-translator</title>
    <updated>2023-03-19T01:50:37Z</updated>
    <id>tag:github.com,2023-03-19:/yetone/openai-translator</id>
    <link href="https://github.com/yetone/openai-translator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Âü∫‰∫é ChatGPT API ÁöÑÂàíËØçÁøªËØëÊµèËßàÂô®Êèí‰ª∂ÂíåË∑®Âπ≥Âè∞Ê°åÈù¢Á´ØÂ∫îÁî® - Browser extension and cross-platform desktop application for translation based on ChatGPT API.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://user-images.githubusercontent.com/1651790/224081217-86521beb-1b69-4071-b195-f2ce0bb33db7.png&#34;&gt; &#xA; &lt;img alt=&#34;NebulaGraph Data Intelligence Suite(ngdi)&#34; src=&#34;https://user-images.githubusercontent.com/1651790/224081979-d3aa7867-94a6-4a85-a5d7-603e02360cee.png&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; English | &lt;a href=&#34;https://raw.githubusercontent.com/yetone/openai-translator/main/README-CN.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;em&gt;The translator that does more than just translation - powered by OpenAI.&lt;/em&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yetone/openai-translator/main/LICENSE&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;MIT License&#34; src=&#34;https://img.shields.io/github/license/yetone/openai-translator.svg?style=flat-square&#34;&gt; &lt;/a&gt; &#xA; &lt;!-- TypeScript Badge --&gt; &lt;img alt=&#34;TypeScript&#34; src=&#34;https://img.shields.io/badge/-TypeScript-blue?style=flat-square&amp;amp;logo=typescript&amp;amp;logoColor=white&#34;&gt; &#xA; &lt;!-- Rust Badge --&gt; &lt;img alt=&#34;Rust&#34; src=&#34;https://img.shields.io/badge/-Rust-orange?style=flat-square&amp;amp;logo=rust&amp;amp;logoColor=white&#34;&gt; &lt;a href=&#34;https://chrome.google.com/webstore/detail/openai-translator/ogjibjphoadhljaoicdnjnmgokohngcc&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Chrome&#34; src=&#34;https://img.shields.io/chrome-web-store/stars/ogjibjphoadhljaoicdnjnmgokohngcc?color=blue&amp;amp;label=Chrome&amp;amp;style=flat-square&amp;amp;logo=google-chrome&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://addons.mozilla.org/en-US/firefox/addon/openai-translator/&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Firefox&#34; src=&#34;https://img.shields.io/amo/stars/openai-translator?color=orange&amp;amp;label=Firefox&amp;amp;style=flat-square&amp;amp;logo=firefox&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/yetone/openai-translator/releases&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;macOS&#34; src=&#34;https://img.shields.io/badge/-macOS-black?style=flat-square&amp;amp;logo=apple&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/yetone/openai-translator/releases&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Windows&#34; src=&#34;https://img.shields.io/badge/-Windows-blue?style=flat-square&amp;amp;logo=windows&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/yetone/openai-translator/releases&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Linux&#34; src=&#34;https://img.shields.io/badge/-Linux-yellow?style=flat-square&amp;amp;logo=linux&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Why Yet another Translator&lt;/h1&gt; &#xA;&lt;p&gt;I have developed a &lt;a href=&#34;https://bobtranslate.com/&#34;&gt;Bob&lt;/a&gt; &lt;a href=&#34;https://github.com/yetone/bob-plugin-openai-translator&#34;&gt;plugin&lt;/a&gt; that utilizes ChatGPT API to provide global word translation on macOS. However, since not all users have access to macOS to benefit from the plugin, I have created this project!&lt;/p&gt; &#xA;&lt;h1&gt;More than just a browser extension&lt;/h1&gt; &#xA;&lt;p&gt;What began as a Chrome extension has now evolved into a multi-platform desktop app that I am currently developing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Attention! The desktop application does not support the pop-up icon after word selection. You must press the shortcut key to trigger the translation after selecting a word.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;560&#34; src=&#34;https://user-images.githubusercontent.com/1206493/223899374-ff386436-63b8-4618-afdd-fed2e6b48d56.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;More than just translation&lt;/h1&gt; &#xA;&lt;p&gt;What began as a translation tool has now evolved to include surprisingly effective word polishing and summarization capabilities, &lt;del&gt;accidentally&lt;/del&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;How to use&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;800&#34; src=&#34;https://user-images.githubusercontent.com/1206493/223200182-6a1d2a02-3fe0-4723-bdae-99d8b7212a33.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;It offers three modes: translation, polishing and summarization.&lt;/li&gt; &#xA; &lt;li&gt;Our tool allows for mutual translation, polishing and summarization across 55 different languages.&lt;/li&gt; &#xA; &lt;li&gt;Streaming mode is supported!&lt;/li&gt; &#xA; &lt;li&gt;It allows users to customize their translation text.&lt;/li&gt; &#xA; &lt;li&gt;One-click copying&lt;/li&gt; &#xA; &lt;li&gt;Text-to-Speech (TTS)&lt;/li&gt; &#xA; &lt;li&gt;Available on all platforms (Windows, macOS, and Linux) for both browsers and Desktop&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Preparation&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(required) Apply for an OpenAI API key &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(optional) If you cannot access OpenAI, you can use the OpenAI API Proxy.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Windows&lt;/h2&gt; &#xA;&lt;h3&gt;Install via &lt;a href=&#34;https://github.com/microsoft/winget-cli&#34;&gt;winget&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;winget install yetone.OpenAITranslator&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install via &lt;a href=&#34;https://scoop.sh&#34;&gt;Scoop&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;scoop bucket add extras&#xA;scoop install openai-translator&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install Manually&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the installation package ending in &lt;code&gt;.msi&lt;/code&gt; from the &lt;a href=&#34;https://github.com/yetone/openai-translator/releases/latest&#34;&gt;Latest Release&lt;/a&gt; page.&lt;/li&gt; &#xA; &lt;li&gt;Double click the downloaded file to install it.&lt;/li&gt; &#xA; &lt;li&gt;If prompted as unsafe, you can click on &lt;code&gt;More Info&lt;/code&gt; -&amp;gt; &lt;code&gt;Run Anyway&lt;/code&gt; to proceed with the installation.&lt;/li&gt; &#xA; &lt;li&gt;Ready to use!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;MacOS&lt;/h2&gt; &#xA;&lt;h3&gt;Install Manually&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go to the &lt;a href=&#34;https://github.com/yetone/openai-translator/releases/latest&#34;&gt;Latest Release&lt;/a&gt; page and download the corresponding chip&#39;s &lt;code&gt;.dmg&lt;/code&gt; installation package.&lt;/li&gt; &#xA; &lt;li&gt;Double click the downloaded file to install it.&lt;/li&gt; &#xA; &lt;li&gt;Ready to use!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&#34;OpenAI Translator&#34; can‚Äôt be opened because the developer cannot be verified.&lt;/p&gt; &lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;300&#34; src=&#34;https://user-images.githubusercontent.com/1206493/223916804-45ce3f34-6a4a-4baf-a0c1-4ab5c54c521f.png&#34;&gt; &lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Click the &lt;code&gt;Cancel&lt;/code&gt; button, then go to the &lt;code&gt;Settings&lt;/code&gt; -&amp;gt; &lt;code&gt;Privacy and Security&lt;/code&gt; page, click the &lt;code&gt;Still Open&lt;/code&gt; button, and then click the &lt;code&gt;Open&lt;/code&gt; button in the pop-up window. After that, there will be no more pop-up warnings when opening &lt;code&gt;OpenAI Translator&lt;/code&gt;. üéâ&lt;/p&gt; &lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;500&#34; src=&#34;https://user-images.githubusercontent.com/1206493/223916970-9c99f15e-cf61-4770-b92d-4a78f980bb26.png&#34;&gt; &lt;img width=&#34;200&#34; src=&#34;https://user-images.githubusercontent.com/1206493/223917449-ed1ac19f-c43d-4b13-9888-79ba46ceb862.png&#34;&gt; &lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;If you cannot find the above options in &lt;code&gt;Privacy &amp;amp; Security&lt;/code&gt;. Open &lt;code&gt;Terminal.app&lt;/code&gt; and enter the following command (you may need to enter a password halfway through), then restart &lt;code&gt;OpenAI Translator&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo xattr -d com.apple.quarantine /Applications/OpenAI\ Translator.app&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you encounter a permission prompt every time you open it, or if you cannot perform a shortcut translation, please go to &lt;code&gt;Settings&lt;/code&gt; -&amp;gt; &lt;code&gt;Privacy &amp;amp; Security&lt;/code&gt; -&amp;gt; &lt;code&gt;Supporting Features&lt;/code&gt; to remove OpenAI Translator, and then re-add OpenAI Translator.&lt;/p&gt; &lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;500&#34; src=&#34;https://user-images.githubusercontent.com/1206493/224536148-eec559bf-4d99-48c1-bbd3-2cc105aff084.png&#34;&gt; &lt;img width=&#34;600&#34; src=&#34;https://user-images.githubusercontent.com/1206493/224536277-4200f58e-8dc0-4c01-a27a-a30d7d8dc69e.gif&#34;&gt; &lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Browser Extension&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Visit your Browser Extension Store to install this plugin:&lt;/p&gt; &lt;p align=&#34;center&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://chrome.google.com/webstore/detail/openai-translator/ogjibjphoadhljaoicdnjnmgokohngcc&#34;&gt; &lt;img src=&#34;https://img.shields.io/chrome-web-store/v/ogjibjphoadhljaoicdnjnmgokohngcc?label=Chrome%20Web%20Store&amp;amp;style=for-the-badge&amp;amp;color=blue&amp;amp;logo=google-chrome&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://addons.mozilla.org/en-US/firefox/addon/openai-translator/&#34;&gt; &lt;img src=&#34;https://img.shields.io/amo/v/openai-translator?label=Firefox%20Add-on&amp;amp;style=for-the-badge&amp;amp;color=orange&amp;amp;logo=firefox&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click on the OpenAI Translator icon in the browser plugin list, and enter the obtained API KEY into the configuration interface that pops up from this plugin.&lt;/p&gt; &lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;600&#34; src=&#34;https://user-images.githubusercontent.com/1206493/222958165-159719b4-28a5-44a4-b700-567786df7f03.png&#34;&gt; &lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Refresh the page in the browser to enjoy the smooth translation experience üéâ!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yetone/openai-translator/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Buy me a coffee&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img height=&#34;360&#34; src=&#34;https://user-images.githubusercontent.com/1206493/220753437-90e4039c-d95f-4b6a-9a08-b3d6de13211f.png&#34;&gt; &lt;img height=&#34;360&#34; src=&#34;https://user-images.githubusercontent.com/1206493/220756036-d9ac4512-0375-4a32-8c2e-8697021058a2.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Star History&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://star-history.com/#yetone/openai-translator&amp;amp;Date&#34;&gt; &lt;img src=&#34;https://api.star-history.com/svg?repos=yetone/openai-translator&amp;amp;type=Date&#34;&gt; &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
</feed>