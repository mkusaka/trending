<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-28T01:36:12Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PawanOsman/ChatGPT</title>
    <updated>2024-04-28T01:36:12Z</updated>
    <id>tag:github.com,2024-04-28:/PawanOsman/ChatGPT</id>
    <link href="https://github.com/PawanOsman/ChatGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenAI API Free Reverse Proxy&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;ChatGPT &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; API for Free (as a Reverse Proxy)&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to the ChatGPT API Free Reverse Proxy, offering free self-hosted API access to ChatGPT (&lt;code&gt;gpt-3.5-turbo&lt;/code&gt;) with OpenAI&#39;s familiar structure, so no code changes are needed.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.pawan.krd&#34;&gt;Join our Discord Community&lt;/a&gt; for support and questions. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;⚡Note: Your Discord account must be at least 7 days old to be able join our Discord community.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Option 1: &lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#installingself-hosting-guide&#34;&gt;Installing/Self-Hosting Guide&lt;/a&gt; (Without using any API key) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Method 1: &lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#using-docker&#34;&gt;Using Docker&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/docker-compose&#34;&gt;Run it with a Chat Web UI using docker-compose&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Method 2: &lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#your-pcserver&#34;&gt;Your PC/Server&lt;/a&gt; (manually)&lt;/li&gt; &#xA;   &lt;li&gt;Method 3: &lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#termux-on-android-phones&#34;&gt;Termux on Android Phones&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Option 2: &lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#accessing-our-hosted-api&#34;&gt;Accessing Our Hosted API&lt;/a&gt; (Free)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#usage-examples&#34;&gt;Usage Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Streaming Response&lt;/strong&gt;: The API supports streaming response, so you can get the response as soon as it&#39;s available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;API Endpoint Compatibility&lt;/strong&gt;: Full alignment with official OpenAI API endpoints, ensuring hassle-free integration with existing OpenAI libraries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Complimentary Access&lt;/strong&gt;: No charges for API usage, making advanced AI accessible to everyone even &lt;strong&gt;without an API key&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installing/Self-Hosting Guide&lt;/h2&gt; &#xA;&lt;h3&gt;Using Docker&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ensure Docker is installed by referring to the &lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Docker Installation Docs&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run the following command: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -dp 3040:3040 pawanosman/chatgpt:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Done! You can now connect to your local server&#39;s API at: &lt;pre&gt;&lt;code&gt;http://localhost:3040/v1/chat/completions&#xA;&lt;/code&gt;&lt;/pre&gt; Note that the base URL is &lt;code&gt;http://localhost:3040/v1&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Install with chat web interfaces&lt;/h3&gt; &#xA;&lt;p&gt;✅ You can run third-party chat web interfaces, such as BetterChatGPT and LobeChat, with this API using Docker Compose. &lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/docker-compose&#34;&gt;Click here for the installation guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Your PC/Server&lt;/h3&gt; &#xA;&lt;p&gt;To install and run the ChatGPT API Reverse Proxy on your PC/Server by following these steps:&lt;/p&gt; &#xA;&lt;p&gt;Note: This option is not available to all countries yet. if you are from a country that is not supported, you can use a &lt;strong&gt;U.S. VPN&lt;/strong&gt; or use &lt;strong&gt;our hosted API&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ensure NodeJs (v19+) is installed: &lt;a href=&#34;https://nodejs.org/en/download&#34;&gt;Download NodeJs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Clone this repository: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/PawanOsman/ChatGPT.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Open &lt;code&gt;start.bat&lt;/code&gt; (Windows) or &lt;code&gt;start.sh&lt;/code&gt; (Linux with &lt;code&gt;bash start.sh&lt;/code&gt; command) to install dependencies and launch the server.&lt;/li&gt; &#xA; &lt;li&gt;Done, you can connect to your local server&#39;s API at: &lt;pre&gt;&lt;code&gt;http://localhost:3040/v1/chat/completions&#xA;&lt;/code&gt;&lt;/pre&gt; Note that the base url will be &lt;code&gt;http://localhost:3040/v1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To include installation instructions for Termux on Android devices, you can add the following section right after the instructions for Linux in the &lt;strong&gt;Installing/Self-Hosting Guide&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;h3&gt;Termux on Android Phones&lt;/h3&gt; &#xA;&lt;p&gt;To install and run the ChatGPT API Reverse Proxy on Android using Termux, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.termux&#34;&gt;Termux&lt;/a&gt; from the Play Store.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Update Termux packages:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt update&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Upgrade Termux packages:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install git, Node.js, and npm:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt install -y git nodejs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/PawanOsman/ChatGPT.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the cloned directory:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ChatGPT&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start the server with:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash start.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Your local server will now be running and accessible at:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;http://localhost:3040/v1/chat/completions&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the base url will be &lt;code&gt;http://localhost:3040/v1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You can now use this address to connect to your self-hosted ChatGPT API Reverse Proxy from Android applications/websites that support reverse proxy configurations, on the same device.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Accessing Our Hosted API&lt;/h2&gt; &#xA;&lt;p&gt;Utilize our pre-hosted ChatGPT-like API for free by:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Joining our &lt;a href=&#34;https://discord.pawan.krd&#34;&gt;Discord server&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Obtaining an API key from the &lt;code&gt;#Bot&lt;/code&gt; channel with the &lt;code&gt;/key&lt;/code&gt; command.&lt;/li&gt; &#xA; &lt;li&gt;Incorporating the API key into your requests to: &lt;pre&gt;&lt;code&gt;https://api.pawan.krd/v1/chat/completions&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage Examples&lt;/h2&gt; &#xA;&lt;p&gt;Leverage the same integration code as OpenAI&#39;s official libraries by simply adjusting the API key and base URL in your requests. For self-hosted setups, ensure to switch the base URL to your local server&#39;s address as mentioned above.&lt;/p&gt; &#xA;&lt;h3&gt;Example Usage with OpenAI Libraries&lt;/h3&gt; &#xA;&lt;h4&gt;Python Example&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;&#xA;openai.api_key = &#39;anything&#39;&#xA;openai.base_url = &#34;http://localhost:3040/v1/&#34;&#xA;&#xA;completion = openai.chat.completions.create(&#xA;    model=&#34;gpt-3.5-turbo&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;How do I list all files in a directory using Python?&#34;},&#xA;    ],&#xA;)&#xA;&#xA;print(completion.choices[0].message.content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Node.js Example&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const openai = new OpenAI({&#xA;&#x9;apiKey: &#34;anything&#34;,&#xA;&#x9;baseURL: &#34;http://localhost:3040/v1&#34;,&#xA;});&#xA;&#xA;const chatCompletion = await openai.chat.completions.create({&#xA;  messages: [{ role: &#39;user&#39;, content: &#39;Say this is a test&#39; }],&#xA;  model: &#39;gpt-3.5-turbo&#39;,&#xA;});&#xA;&#xA;console.log(chatCompletion.choices[0].message.content);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is under the AGPL-3.0 License. Refer to the &lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for detailed information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>unslothai/unsloth</title>
    <updated>2024-04-28T01:36:12Z</updated>
    <id>tag:github.com,2024-04-28:/unslothai/unsloth</id>
    <link href="https://github.com/unslothai/unsloth" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Finetune Llama 3, Mistral &amp; Gemma LLMs 2-5x faster with 80% less memory&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://unsloth.ai&#34;&gt;&#xA;   &lt;picture&gt; &#xA;    &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png&#34;&gt; &#xA;    &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&#34;&gt; &#xA;    &lt;img alt=&#34;unsloth logo&#34; src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&#34; height=&#34;110&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA;   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png&#34; height=&#34;48&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/u54VK8m8tk&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png&#34; height=&#34;48&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ko-fi.com/unsloth&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png&#34; height=&#34;48&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Finetune Mistral, Gemma, Llama 2-5x faster with 80% less memory!&lt;/h3&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://i.ibb.co/sJ7RhGG/image-41.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;✨ Finetune for Free&lt;/h2&gt; &#xA;&lt;p&gt;All notebooks are &lt;strong&gt;beginner friendly&lt;/strong&gt;! Add your dataset, click &#34;Run All&#34;, and you&#39;ll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Unsloth supports&lt;/th&gt; &#xA;   &lt;th&gt;Free Notebooks&lt;/th&gt; &#xA;   &lt;th&gt;Performance&lt;/th&gt; &#xA;   &lt;th&gt;Memory use&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama-3 8b&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing&#34;&gt;▶️ Start on Colab&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2x faster&lt;/td&gt; &#xA;   &lt;td&gt;60% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Gemma 7b&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing&#34;&gt;▶️ Start on Colab&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.4x faster&lt;/td&gt; &#xA;   &lt;td&gt;71% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Mistral 7b&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing&#34;&gt;▶️ Start on Colab&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.2x faster&lt;/td&gt; &#xA;   &lt;td&gt;73% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;TinyLlama&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing&#34;&gt;▶️ Start on Colab&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.9x faster&lt;/td&gt; &#xA;   &lt;td&gt;82% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CodeLlama 34b&lt;/strong&gt; A100&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing&#34;&gt;▶️ Start on Colab&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.9x faster&lt;/td&gt; &#xA;   &lt;td&gt;49% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Mistral 7b&lt;/strong&gt; 1xT4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook&#34;&gt;▶️ Start on Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;5x faster*&lt;/td&gt; &#xA;   &lt;td&gt;73% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;DPO - Zephyr&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing&#34;&gt;▶️ Start on Colab&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.9x faster&lt;/td&gt; &#xA;   &lt;td&gt;43% less&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Benchmarking compared to FA2 + Hugging Face combined.&lt;/li&gt; &#xA; &lt;li&gt;This &lt;a href=&#34;https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing&#34;&gt;conversational notebook&lt;/a&gt; is useful for ShareGPT ChatML / Vicuna templates.&lt;/li&gt; &#xA; &lt;li&gt;This &lt;a href=&#34;https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing&#34;&gt;text completion notebook&lt;/a&gt; is for raw text. This &lt;a href=&#34;https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing&#34;&gt;DPO notebook&lt;/a&gt; replicates Zephyr.&lt;/li&gt; &#xA; &lt;li&gt;* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🦥 Unsloth.ai News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📣 NEW! &lt;a href=&#34;https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing&#34;&gt;Llama-3 8b&lt;/a&gt; now works! Llama-3 70b also works (just change the model name in the notebook).&lt;/li&gt; &#xA; &lt;li&gt;📣 NEW! We cut memory usage by a &lt;a href=&#34;https://unsloth.ai/blog/long-context&#34;&gt;further 30%&lt;/a&gt; and now support fine-tuning of LLMs with &lt;a href=&#34;https://unsloth.ai/blog/long-context&#34;&gt;4x longer context windows&lt;/a&gt;! No change required if you&#39;re using our notebooks. To enable, simply change 1 line:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = FastLanguageModel.get_peft_model(&#xA;    model,&#xA;    use_gradient_checkpointing = &#34;unsloth&#34;, # &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📣 &lt;a href=&#34;https://colab.research.google.com/drive/19lwcRk_ZQ_ZtX-qzFP3qZBBHZNcMD1hh?usp=sharing&#34;&gt;CodeGemma&lt;/a&gt; now works along with &lt;a href=&#34;https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing&#34;&gt;Gemma 7b&lt;/a&gt; and &lt;a href=&#34;https://colab.research.google.com/drive/15gGm7x_jTm017_Ic8e317tdIpDG53Mtu?usp=sharing&#34;&gt;Gemma 2b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;📣 &lt;a href=&#34;https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing&#34;&gt;2x faster inference&lt;/a&gt; added for all our models&lt;/li&gt; &#xA; &lt;li&gt;📣 &lt;a href=&#34;https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing&#34;&gt;DPO support&lt;/a&gt; is now included. &lt;a href=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/#DPO&#34;&gt;More info&lt;/a&gt; on DPO&lt;/li&gt; &#xA; &lt;li&gt;📣 We did a &lt;a href=&#34;https://huggingface.co/blog/unsloth-trl&#34;&gt;blog&lt;/a&gt; with 🤗Hugging Face and are in their official docs! Check out the &lt;a href=&#34;https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth&#34;&gt;SFT docs&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth&#34;&gt;DPO docs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🔗 Links and Resources&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Links&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;📚 &lt;strong&gt;Wiki &amp;amp; FAQ&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/unslothai/unsloth/wiki&#34;&gt;Read Our Wiki&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img height=&#34;14&#34; src=&#34;https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg?sanitize=true&#34;&gt;&amp;nbsp; &lt;strong&gt;Twitter (aka X)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/unslothai&#34;&gt;Follow us on X&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;📜 &lt;strong&gt;Documentation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/unslothai/unsloth/tree/main#-documentation&#34;&gt;Read The Doc&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;💾 &lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/unslothai/unsloth/tree/main#installation-instructions&#34;&gt;unsloth/README.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🥇 &lt;strong&gt;Benchmarking&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking&#34;&gt;Performance Tables&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🌐 &lt;strong&gt;Released Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/unsloth&#34;&gt;Unsloth Releases&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;✍️ &lt;strong&gt;Blog&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://unsloth.ai/blog&#34;&gt;Read our Blogs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;⭐ Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All kernels written in &lt;a href=&#34;https://openai.com/research/triton&#34;&gt;OpenAI&#39;s Triton&lt;/a&gt; language. &lt;strong&gt;Manual backprop engine&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;0% loss in accuracy&lt;/strong&gt; - no approximation methods - all exact.&lt;/li&gt; &#xA; &lt;li&gt;No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) &lt;a href=&#34;https://developer.nvidia.com/cuda-gpus&#34;&gt;Check your GPU!&lt;/a&gt; GTX 1070, 1080 works, but is slow.&lt;/li&gt; &#xA; &lt;li&gt;Works on &lt;strong&gt;Linux&lt;/strong&gt; and &lt;strong&gt;Windows&lt;/strong&gt; via WSL.&lt;/li&gt; &#xA; &lt;li&gt;Supports 4bit and 16bit QLoRA / LoRA finetuning via &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Open source trains 5x faster - see &lt;a href=&#34;https://unsloth.ai/&#34;&gt;Unsloth Pro&lt;/a&gt; for up to &lt;strong&gt;30x faster training&lt;/strong&gt;!&lt;/li&gt; &#xA; &lt;li&gt;If you trained a model with 🦥Unsloth, you can use this cool sticker! &amp;nbsp; &lt;img src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png&#34; height=&#34;50&#34; align=&#34;center&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🥇 Performance Benchmarking&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For the full list of &lt;strong&gt;reproducable&lt;/strong&gt; benchmarking tables, &lt;a href=&#34;https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables&#34;&gt;go to our website&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;1 A100 40GB&lt;/th&gt; &#xA;   &lt;th&gt;🤗Hugging Face&lt;/th&gt; &#xA;   &lt;th&gt;Flash Attention&lt;/th&gt; &#xA;   &lt;th&gt;🦥Unsloth Open Source&lt;/th&gt; &#xA;   &lt;th&gt;🦥&lt;a href=&#34;https://unsloth.ai/pricing&#34;&gt;Unsloth Pro&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;1x&lt;/td&gt; &#xA;   &lt;td&gt;1.04x&lt;/td&gt; &#xA;   &lt;td&gt;1.98x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;15.64x&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LAION Chip2&lt;/td&gt; &#xA;   &lt;td&gt;1x&lt;/td&gt; &#xA;   &lt;td&gt;0.92x&lt;/td&gt; &#xA;   &lt;td&gt;1.61x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;20.73x&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OASST&lt;/td&gt; &#xA;   &lt;td&gt;1x&lt;/td&gt; &#xA;   &lt;td&gt;1.19x&lt;/td&gt; &#xA;   &lt;td&gt;2.17x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;14.83x&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Slim Orca&lt;/td&gt; &#xA;   &lt;td&gt;1x&lt;/td&gt; &#xA;   &lt;td&gt;1.18x&lt;/td&gt; &#xA;   &lt;td&gt;2.22x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;14.82x&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Benchmarking table below was conducted by &lt;a href=&#34;https://huggingface.co/blog/unsloth-trl&#34;&gt;🤗Hugging Face&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Free Colab T4&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;🤗Hugging Face&lt;/th&gt; &#xA;   &lt;th&gt;Pytorch 2.1.1&lt;/th&gt; &#xA;   &lt;th&gt;🦥Unsloth&lt;/th&gt; &#xA;   &lt;th&gt;🦥 VRAM reduction&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2 7b&lt;/td&gt; &#xA;   &lt;td&gt;OASST&lt;/td&gt; &#xA;   &lt;td&gt;1x&lt;/td&gt; &#xA;   &lt;td&gt;1.19x&lt;/td&gt; &#xA;   &lt;td&gt;1.95x&lt;/td&gt; &#xA;   &lt;td&gt;-43.3%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral 7b&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;1x&lt;/td&gt; &#xA;   &lt;td&gt;1.07x&lt;/td&gt; &#xA;   &lt;td&gt;1.56x&lt;/td&gt; &#xA;   &lt;td&gt;-13.7%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tiny Llama 1.1b&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;1x&lt;/td&gt; &#xA;   &lt;td&gt;2.06x&lt;/td&gt; &#xA;   &lt;td&gt;3.87x&lt;/td&gt; &#xA;   &lt;td&gt;-73.8%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DPO with Zephyr&lt;/td&gt; &#xA;   &lt;td&gt;Ultra Chat&lt;/td&gt; &#xA;   &lt;td&gt;1x&lt;/td&gt; &#xA;   &lt;td&gt;1.09x&lt;/td&gt; &#xA;   &lt;td&gt;1.55x&lt;/td&gt; &#xA;   &lt;td&gt;-18.6%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/sJ7RhGG/image-41.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;💾 Installation Instructions&lt;/h2&gt; &#xA;&lt;h3&gt;Conda Installation&lt;/h3&gt; &#xA;&lt;p&gt;Select either &lt;code&gt;pytorch-cuda=11.8&lt;/code&gt; for CUDA 11.8 or &lt;code&gt;pytorch-cuda=12.1&lt;/code&gt; for CUDA 12.1. If you have &lt;code&gt;mamba&lt;/code&gt;, use &lt;code&gt;mamba&lt;/code&gt; instead of &lt;code&gt;conda&lt;/code&gt; for faster solving. See this &lt;a href=&#34;https://github.com/unslothai/unsloth/issues/73&#34;&gt;Github issue&lt;/a&gt; for help on debugging Conda installs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name unsloth_env python=3.10&#xA;conda activate unsloth_env&#xA;&#xA;conda install pytorch-cuda=&amp;lt;12.1/11.8&amp;gt; pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers&#xA;&#xA;pip install &#34;unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;&#xA;pip install --no-deps trl peft accelerate bitsandbytes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pip Installation&lt;/h3&gt; &#xA;&lt;p&gt;Do &lt;strong&gt;NOT&lt;/strong&gt; use this if you have Anaconda. You must use the Conda install method, or else stuff will BREAK.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Find your CUDA version via&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch; torch.version.cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;For Pytorch 2.1.0: You can update Pytorch via Pip (interchange &lt;code&gt;cu121&lt;/code&gt; / &lt;code&gt;cu118&lt;/code&gt;). Go to &lt;a href=&#34;https://pytorch.org/&#34;&gt;https://pytorch.org/&lt;/a&gt; to learn more. Select either &lt;code&gt;cu118&lt;/code&gt; for CUDA 11.8 or &lt;code&gt;cu121&lt;/code&gt; for CUDA 12.1. If you have a RTX 3060 or higher (A100, H100 etc), use the &lt;code&gt;&#34;ampere&#34;&lt;/code&gt; path. For Pytorch 2.1.1: go to step 3. For Pytorch 2.2.0: go to step 4.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton \&#xA;  --index-url https://download.pytorch.org/whl/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu118-ampere] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu121-ampere] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;For Pytorch 2.1.1: Use the &lt;code&gt;&#34;ampere&#34;&lt;/code&gt; path for newer RTX 30xx GPUs or higher.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.1 triton \&#xA;  --index-url https://download.pytorch.org/whl/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;unsloth[cu118-torch211] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu121-torch211] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu118-ampere-torch211] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu121-ampere-torch211] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;For Pytorch 2.2.0: Use the &lt;code&gt;&#34;ampere&#34;&lt;/code&gt; path for newer RTX 30xx GPUs or higher.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \&#xA;  --index-url https://download.pytorch.org/whl/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;unsloth[cu118-torch220] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu121-torch220] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu118-ampere-torch220] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;If you get errors, try the below first, then go back to step 1:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade pip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;For Pytorch 2.2.1:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# RTX 3090, 4090 Ampere GPUs:&#xA;pip install &#34;unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes&#xA;&#xA;# Pre Ampere RTX 2080, T4, GTX 1080 GPUs:&#xA;pip install &#34;unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install --no-deps xformers trl peft accelerate bitsandbytes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;To troubleshoot installs try the below (all must succeed). Xformers should mostly all be available.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nvcc&#xA;python -m xformers.info&#xA;python -m bitsandbytes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;📜 Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go to our &lt;a href=&#34;https://github.com/unslothai/unsloth/wiki&#34;&gt;Wiki page&lt;/a&gt; for saving to GGUF, checkpointing, evaluation and more!&lt;/li&gt; &#xA; &lt;li&gt;We support Huggingface&#39;s TRL, Trainer, Seq2SeqTrainer or even Pytorch code!&lt;/li&gt; &#xA; &lt;li&gt;We&#39;re in 🤗Hugging Face&#39;s official docs! Check out the &lt;a href=&#34;https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth&#34;&gt;SFT docs&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth&#34;&gt;DPO docs&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from unsloth import FastLanguageModel&#xA;import torch&#xA;from trl import SFTTrainer&#xA;from transformers import TrainingArguments&#xA;from datasets import load_dataset&#xA;max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!&#xA;# Get LAION dataset&#xA;url = &#34;https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl&#34;&#xA;dataset = load_dataset(&#34;json&#34;, data_files = {&#34;train&#34; : url}, split = &#34;train&#34;)&#xA;&#xA;# 4bit pre quantized models we support - 4x faster downloading!&#xA;fourbit_models = [&#xA;    &#34;unsloth/mistral-7b-bnb-4bit&#34;,&#xA;    &#34;unsloth/llama-2-7b-bnb-4bit&#34;,&#xA;    &#34;unsloth/llama-2-13b-bnb-4bit&#34;,&#xA;    &#34;unsloth/codellama-34b-bnb-4bit&#34;,&#xA;    &#34;unsloth/tinyllama-bnb-4bit&#34;,&#xA;] # Go to https://huggingface.co/unsloth for more 4-bit models!&#xA;&#xA;# Load Llama model&#xA;model, tokenizer = FastLanguageModel.from_pretrained(&#xA;    model_name = &#34;unsloth/mistral-7b-bnb-4bit&#34;, # Supports Llama, Mistral - replace this!&#xA;    max_seq_length = max_seq_length,&#xA;    dtype = None,&#xA;    load_in_4bit = True,&#xA;)&#xA;&#xA;# Do model patching and add fast LoRA weights&#xA;model = FastLanguageModel.get_peft_model(&#xA;    model,&#xA;    r = 16,&#xA;    target_modules = [&#34;q_proj&#34;, &#34;k_proj&#34;, &#34;v_proj&#34;, &#34;o_proj&#34;,&#xA;                      &#34;gate_proj&#34;, &#34;up_proj&#34;, &#34;down_proj&#34;,],&#xA;    lora_alpha = 16,&#xA;    lora_dropout = 0, # Supports any, but = 0 is optimized&#xA;    bias = &#34;none&#34;,    # Supports any, but = &#34;none&#34; is optimized&#xA;    use_gradient_checkpointing = True,&#xA;    random_state = 3407,&#xA;    max_seq_length = max_seq_length,&#xA;    use_rslora = False,  # We support rank stabilized LoRA&#xA;    loftq_config = None, # And LoftQ&#xA;)&#xA;&#xA;trainer = SFTTrainer(&#xA;    model = model,&#xA;    train_dataset = dataset,&#xA;    dataset_text_field = &#34;text&#34;,&#xA;    max_seq_length = max_seq_length,&#xA;    tokenizer = tokenizer,&#xA;    args = TrainingArguments(&#xA;        per_device_train_batch_size = 2,&#xA;        gradient_accumulation_steps = 4,&#xA;        warmup_steps = 10,&#xA;        max_steps = 60,&#xA;        fp16 = not torch.cuda.is_bf16_supported(),&#xA;        bf16 = torch.cuda.is_bf16_supported(),&#xA;        logging_steps = 1,&#xA;        output_dir = &#34;outputs&#34;,&#xA;        optim = &#34;adamw_8bit&#34;,&#xA;        seed = 3407,&#xA;    ),&#xA;)&#xA;trainer.train()&#xA;&#xA;# Go to https://github.com/unslothai/unsloth/wiki for advanced tips like&#xA;# (1) Saving to GGUF / merging to 16bit for vLLM&#xA;# (2) Continued training from a saved LoRA adapter&#xA;# (3) Adding an evaluation loop / OOMs&#xA;# (4) Cutomized chat templates&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a name=&#34;DPO&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;DPO Support&lt;/h2&gt; &#xA;&lt;p&gt;DPO (Direct Preference Optimization), PPO, Reward Modelling all seem to work as per 3rd party independent testing from &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory&#34;&gt;Llama-Factory&lt;/a&gt;. We have a preliminary Google Colab notebook for reproducing Zephyr on Tesla T4 here: &lt;a href=&#34;https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing&#34;&gt;notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re in 🤗Hugging Face&#39;s official docs! We&#39;re on the &lt;a href=&#34;https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth&#34;&gt;SFT docs&lt;/a&gt; and the &lt;a href=&#34;https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth&#34;&gt;DPO docs&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from unsloth import FastLanguageModel, PatchDPOTrainer&#xA;PatchDPOTrainer()&#xA;import torch&#xA;from transformers import TrainingArguments&#xA;from trl import DPOTrainer&#xA;&#xA;model, tokenizer = FastLanguageModel.from_pretrained(&#xA;    model_name = &#34;unsloth/zephyr-sft-bnb-4bit&#34;,&#xA;    max_seq_length = max_seq_length,&#xA;    dtype = None,&#xA;    load_in_4bit = True,&#xA;)&#xA;&#xA;# Do model patching and add fast LoRA weights&#xA;model = FastLanguageModel.get_peft_model(&#xA;    model,&#xA;    r = 64,&#xA;    target_modules = [&#34;q_proj&#34;, &#34;k_proj&#34;, &#34;v_proj&#34;, &#34;o_proj&#34;,&#xA;                      &#34;gate_proj&#34;, &#34;up_proj&#34;, &#34;down_proj&#34;,],&#xA;    lora_alpha = 64,&#xA;    lora_dropout = 0, # Supports any, but = 0 is optimized&#xA;    bias = &#34;none&#34;,    # Supports any, but = &#34;none&#34; is optimized&#xA;    use_gradient_checkpointing = True,&#xA;    random_state = 3407,&#xA;    max_seq_length = max_seq_length,&#xA;)&#xA;&#xA;dpo_trainer = DPOTrainer(&#xA;    model = model,&#xA;    ref_model = None,&#xA;    args = TrainingArguments(&#xA;        per_device_train_batch_size = 4,&#xA;        gradient_accumulation_steps = 8,&#xA;        warmup_ratio = 0.1,&#xA;        num_train_epochs = 3,&#xA;        fp16 = not torch.cuda.is_bf16_supported(),&#xA;        bf16 = torch.cuda.is_bf16_supported(),&#xA;        logging_steps = 1,&#xA;        optim = &#34;adamw_8bit&#34;,&#xA;        seed = 42,&#xA;        output_dir = &#34;outputs&#34;,&#xA;    ),&#xA;    beta = 0.1,&#xA;    train_dataset = YOUR_DATASET_HERE,&#xA;    # eval_dataset = YOUR_DATASET_HERE,&#xA;    tokenizer = tokenizer,&#xA;    max_length = 1024,&#xA;    max_prompt_length = 512,&#xA;)&#xA;dpo_trainer.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🥇 Detailed Benchmarking Tables&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Click &#34;Code&#34; for fully reproducible examples&lt;/li&gt; &#xA; &lt;li&gt;&#34;Unsloth Equal&#34; is a preview of our PRO version, with code stripped out. All settings and the loss curve remains identical.&lt;/li&gt; &#xA; &lt;li&gt;For the full list of benchmarking tables, &lt;a href=&#34;https://unsloth.ai/blog/mistral-benchmark#Benchmark%20tables&#34;&gt;go to our website&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;1 A100 40GB&lt;/th&gt; &#xA;   &lt;th&gt;🤗Hugging Face&lt;/th&gt; &#xA;   &lt;th&gt;Flash Attention 2&lt;/th&gt; &#xA;   &lt;th&gt;🦥Unsloth Open&lt;/th&gt; &#xA;   &lt;th&gt;Unsloth Equal&lt;/th&gt; &#xA;   &lt;th&gt;Unsloth Pro&lt;/th&gt; &#xA;   &lt;th&gt;Unsloth Max&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;1x&lt;/td&gt; &#xA;   &lt;td&gt;1.04x&lt;/td&gt; &#xA;   &lt;td&gt;1.98x&lt;/td&gt; &#xA;   &lt;td&gt;2.48x&lt;/td&gt; &#xA;   &lt;td&gt;5.32x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;15.64x&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;code&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1u4dBeM-0vGNVmmO6X7cScAut-Hyt4KDF?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1fgTOxpMbVjloQBvZyz4lF4BacKSZOB2A?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1YIPY_18xm-K0iJDgvNkRoJsgkPMPAO3G?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1ANW8EFL3LVyTD7Gq4TkheC1Z7Rxw-rHp?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;seconds&lt;/td&gt; &#xA;   &lt;td&gt;1040&lt;/td&gt; &#xA;   &lt;td&gt;1001&lt;/td&gt; &#xA;   &lt;td&gt;525&lt;/td&gt; &#xA;   &lt;td&gt;419&lt;/td&gt; &#xA;   &lt;td&gt;196&lt;/td&gt; &#xA;   &lt;td&gt;67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;memory MB&lt;/td&gt; &#xA;   &lt;td&gt;18235&lt;/td&gt; &#xA;   &lt;td&gt;15365&lt;/td&gt; &#xA;   &lt;td&gt;9631&lt;/td&gt; &#xA;   &lt;td&gt;8525&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;% saved&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;15.74&lt;/td&gt; &#xA;   &lt;td&gt;47.18&lt;/td&gt; &#xA;   &lt;td&gt;53.25&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Llama-Factory 3rd party benchmarking&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-Comparison&#34;&gt;Link to performance table.&lt;/a&gt; TGS: tokens per GPU per second. Model: LLaMA2-7B. GPU: NVIDIA A100 * 1. Batch size: 4. Gradient accumulation: 2. LoRA rank: 8. Max length: 1024.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Bits&lt;/th&gt; &#xA;   &lt;th&gt;TGS&lt;/th&gt; &#xA;   &lt;th&gt;GRAM&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HF&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;2392&lt;/td&gt; &#xA;   &lt;td&gt;18GB&lt;/td&gt; &#xA;   &lt;td&gt;100%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HF+FA2&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;2954&lt;/td&gt; &#xA;   &lt;td&gt;17GB&lt;/td&gt; &#xA;   &lt;td&gt;123%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unsloth+FA2&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;4007&lt;/td&gt; &#xA;   &lt;td&gt;16GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;168%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HF&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;2415&lt;/td&gt; &#xA;   &lt;td&gt;9GB&lt;/td&gt; &#xA;   &lt;td&gt;101%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unsloth+FA2&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;3726&lt;/td&gt; &#xA;   &lt;td&gt;7GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;160%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Performance comparisons between popular models&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click for specific model benchmarking tables (Mistral 7b, CodeLlama 34b etc.)&lt;/summary&gt; &#xA; &lt;h3&gt;Mistral 7b&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;1 A100 40GB&lt;/th&gt; &#xA;    &lt;th&gt;Hugging Face&lt;/th&gt; &#xA;    &lt;th&gt;Flash Attention 2&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Open&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Equal&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Pro&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Max&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Mistral 7B Slim Orca&lt;/td&gt; &#xA;    &lt;td&gt;1x&lt;/td&gt; &#xA;    &lt;td&gt;1.15x&lt;/td&gt; &#xA;    &lt;td&gt;2.15x&lt;/td&gt; &#xA;    &lt;td&gt;2.53x&lt;/td&gt; &#xA;    &lt;td&gt;4.61x&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;13.69x&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;code&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1mePk3KzwTD81hr5mcNcs_AX3Kbg_Ha0x?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1dgHxjvTmX6hb0bPcLp26RXSE6_n9DKj7?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1SKrKGV-BZoU4kv5q3g0jtE_OhRgPtrrQ?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/18yOiyX0T81mTwZqOALFSCX_tSAqju6aD?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;seconds&lt;/td&gt; &#xA;    &lt;td&gt;1813&lt;/td&gt; &#xA;    &lt;td&gt;1571&lt;/td&gt; &#xA;    &lt;td&gt;842&lt;/td&gt; &#xA;    &lt;td&gt;718&lt;/td&gt; &#xA;    &lt;td&gt;393&lt;/td&gt; &#xA;    &lt;td&gt;132&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;memory MB&lt;/td&gt; &#xA;    &lt;td&gt;32853&lt;/td&gt; &#xA;    &lt;td&gt;19385&lt;/td&gt; &#xA;    &lt;td&gt;12465&lt;/td&gt; &#xA;    &lt;td&gt;10271&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;% saved&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;40.99&lt;/td&gt; &#xA;    &lt;td&gt;62.06&lt;/td&gt; &#xA;    &lt;td&gt;68.74&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;CodeLlama 34b&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;1 A100 40GB&lt;/th&gt; &#xA;    &lt;th&gt;Hugging Face&lt;/th&gt; &#xA;    &lt;th&gt;Flash Attention 2&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Open&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Equal&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Pro&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Max&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Code Llama 34B&lt;/td&gt; &#xA;    &lt;td&gt;OOM ❌&lt;/td&gt; &#xA;    &lt;td&gt;0.99x&lt;/td&gt; &#xA;    &lt;td&gt;1.87x&lt;/td&gt; &#xA;    &lt;td&gt;2.61x&lt;/td&gt; &#xA;    &lt;td&gt;4.27x&lt;/td&gt; &#xA;    &lt;td&gt;12.82x&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;code&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1ykfz3BqrtC_AUFegCzUQjjfUNlxp6Otc?usp=sharing&#34;&gt;▶️ Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/12ZypxQh7OC6kBXvWZI-5d05I4m-B_hoR?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1gdHyAx8XJsz2yNV-DHvbHjR1iCef5Qmh?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1fm7wqx9MJ0kRrwKOfmLkK1Rmw-pySahB?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;seconds&lt;/td&gt; &#xA;    &lt;td&gt;1953&lt;/td&gt; &#xA;    &lt;td&gt;1982&lt;/td&gt; &#xA;    &lt;td&gt;1043&lt;/td&gt; &#xA;    &lt;td&gt;748&lt;/td&gt; &#xA;    &lt;td&gt;458&lt;/td&gt; &#xA;    &lt;td&gt;152&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;memory MB&lt;/td&gt; &#xA;    &lt;td&gt;40000&lt;/td&gt; &#xA;    &lt;td&gt;33217&lt;/td&gt; &#xA;    &lt;td&gt;27413&lt;/td&gt; &#xA;    &lt;td&gt;22161&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;% saved&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;16.96&lt;/td&gt; &#xA;    &lt;td&gt;31.47&lt;/td&gt; &#xA;    &lt;td&gt;44.60&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;1 Tesla T4&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;1 T4 16GB&lt;/th&gt; &#xA;    &lt;th&gt;Hugging Face&lt;/th&gt; &#xA;    &lt;th&gt;Flash Attention&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Open&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Pro Equal&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Pro&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Max&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Alpaca&lt;/td&gt; &#xA;    &lt;td&gt;1x&lt;/td&gt; &#xA;    &lt;td&gt;1.09x&lt;/td&gt; &#xA;    &lt;td&gt;1.69x&lt;/td&gt; &#xA;    &lt;td&gt;1.79x&lt;/td&gt; &#xA;    &lt;td&gt;2.93x&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;8.3x&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;code&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1XpLIV4s8Bj5uryB-X2gqM88oRGHEGdaB?usp=sharing&#34;&gt;▶️ Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1LyXu6CjuymQg6ddHX8g1dpUvrMa1nn4L?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1gsv4LpY7C32otl1rgRo5wXTk4HIitXoM?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1VtULwRQwhEnVdNryjm27zXfdSM1tNfFK?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;seconds&lt;/td&gt; &#xA;    &lt;td&gt;1599&lt;/td&gt; &#xA;    &lt;td&gt;1468&lt;/td&gt; &#xA;    &lt;td&gt;942&lt;/td&gt; &#xA;    &lt;td&gt;894&lt;/td&gt; &#xA;    &lt;td&gt;545&lt;/td&gt; &#xA;    &lt;td&gt;193&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;memory MB&lt;/td&gt; &#xA;    &lt;td&gt;7199&lt;/td&gt; &#xA;    &lt;td&gt;7059&lt;/td&gt; &#xA;    &lt;td&gt;6459&lt;/td&gt; &#xA;    &lt;td&gt;5443&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;% saved&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;1.94&lt;/td&gt; &#xA;    &lt;td&gt;10.28&lt;/td&gt; &#xA;    &lt;td&gt;24.39&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;2 Tesla T4s via DDP&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;2 T4 DDP&lt;/th&gt; &#xA;    &lt;th&gt;Hugging Face&lt;/th&gt; &#xA;    &lt;th&gt;Flash Attention&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Open&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Equal&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Pro&lt;/th&gt; &#xA;    &lt;th&gt;Unsloth Max&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Alpaca&lt;/td&gt; &#xA;    &lt;td&gt;1x&lt;/td&gt; &#xA;    &lt;td&gt;0.99x&lt;/td&gt; &#xA;    &lt;td&gt;4.95x&lt;/td&gt; &#xA;    &lt;td&gt;4.44x&lt;/td&gt; &#xA;    &lt;td&gt;7.28x&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;20.61x&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;code&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/danielhanchen/hf-original-alpaca-t4-ddp&#34;&gt;▶️ Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/danielhanchen/hf-sdpa-alpaca-t4-ddp&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;seconds&lt;/td&gt; &#xA;    &lt;td&gt;9882&lt;/td&gt; &#xA;    &lt;td&gt;9946&lt;/td&gt; &#xA;    &lt;td&gt;1996&lt;/td&gt; &#xA;    &lt;td&gt;2227&lt;/td&gt; &#xA;    &lt;td&gt;1357&lt;/td&gt; &#xA;    &lt;td&gt;480&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;memory MB&lt;/td&gt; &#xA;    &lt;td&gt;9176&lt;/td&gt; &#xA;    &lt;td&gt;9128&lt;/td&gt; &#xA;    &lt;td&gt;6904&lt;/td&gt; &#xA;    &lt;td&gt;6782&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;% saved&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;0.52&lt;/td&gt; &#xA;    &lt;td&gt;24.76&lt;/td&gt; &#xA;    &lt;td&gt;26.09&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Performance comparisons on 1 Tesla T4 GPU:&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click for Time taken for 1 epoch&lt;/summary&gt; &#xA; &lt;p&gt;One Tesla T4 on Google Colab &lt;code&gt;bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = &#34;adamw_8bit&#34;, schedule = &#34;linear&#34;, schedule_steps = 10&lt;/code&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;System&lt;/th&gt; &#xA;    &lt;th&gt;GPU&lt;/th&gt; &#xA;    &lt;th&gt;Alpaca (52K)&lt;/th&gt; &#xA;    &lt;th&gt;LAION OIG (210K)&lt;/th&gt; &#xA;    &lt;th&gt;Open Assistant (10K)&lt;/th&gt; &#xA;    &lt;th&gt;SlimOrca (518K)&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Huggingface&lt;/td&gt; &#xA;    &lt;td&gt;1 T4&lt;/td&gt; &#xA;    &lt;td&gt;23h 15m&lt;/td&gt; &#xA;    &lt;td&gt;56h 28m&lt;/td&gt; &#xA;    &lt;td&gt;8h 38m&lt;/td&gt; &#xA;    &lt;td&gt;391h 41m&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Unsloth Open&lt;/td&gt; &#xA;    &lt;td&gt;1 T4&lt;/td&gt; &#xA;    &lt;td&gt;13h 7m (1.8x)&lt;/td&gt; &#xA;    &lt;td&gt;31h 47m (1.8x)&lt;/td&gt; &#xA;    &lt;td&gt;4h 27m (1.9x)&lt;/td&gt; &#xA;    &lt;td&gt;240h 4m (1.6x)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Unsloth Pro&lt;/td&gt; &#xA;    &lt;td&gt;1 T4&lt;/td&gt; &#xA;    &lt;td&gt;3h 6m (7.5x)&lt;/td&gt; &#xA;    &lt;td&gt;5h 17m (10.7x)&lt;/td&gt; &#xA;    &lt;td&gt;1h 7m (7.7x)&lt;/td&gt; &#xA;    &lt;td&gt;59h 53m (6.5x)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Unsloth Max&lt;/td&gt; &#xA;    &lt;td&gt;1 T4&lt;/td&gt; &#xA;    &lt;td&gt;2h 39m (8.8x)&lt;/td&gt; &#xA;    &lt;td&gt;4h 31m (12.5x)&lt;/td&gt; &#xA;    &lt;td&gt;0h 58m (8.9x)&lt;/td&gt; &#xA;    &lt;td&gt;51h 30m (7.6x)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;&lt;strong&gt;Peak Memory Usage&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;System&lt;/th&gt; &#xA;    &lt;th&gt;GPU&lt;/th&gt; &#xA;    &lt;th&gt;Alpaca (52K)&lt;/th&gt; &#xA;    &lt;th&gt;LAION OIG (210K)&lt;/th&gt; &#xA;    &lt;th&gt;Open Assistant (10K)&lt;/th&gt; &#xA;    &lt;th&gt;SlimOrca (518K)&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Huggingface&lt;/td&gt; &#xA;    &lt;td&gt;1 T4&lt;/td&gt; &#xA;    &lt;td&gt;7.3GB&lt;/td&gt; &#xA;    &lt;td&gt;5.9GB&lt;/td&gt; &#xA;    &lt;td&gt;14.0GB&lt;/td&gt; &#xA;    &lt;td&gt;13.3GB&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Unsloth Open&lt;/td&gt; &#xA;    &lt;td&gt;1 T4&lt;/td&gt; &#xA;    &lt;td&gt;6.8GB&lt;/td&gt; &#xA;    &lt;td&gt;5.7GB&lt;/td&gt; &#xA;    &lt;td&gt;7.8GB&lt;/td&gt; &#xA;    &lt;td&gt;7.7GB&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Unsloth Pro&lt;/td&gt; &#xA;    &lt;td&gt;1 T4&lt;/td&gt; &#xA;    &lt;td&gt;6.4GB&lt;/td&gt; &#xA;    &lt;td&gt;6.4GB&lt;/td&gt; &#xA;    &lt;td&gt;6.4GB&lt;/td&gt; &#xA;    &lt;td&gt;6.4GB&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Unsloth Max&lt;/td&gt; &#xA;    &lt;td&gt;1 T4&lt;/td&gt; &#xA;    &lt;td&gt;11.4GB&lt;/td&gt; &#xA;    &lt;td&gt;12.4GB&lt;/td&gt; &#xA;    &lt;td&gt;11.9GB&lt;/td&gt; &#xA;    &lt;td&gt;14.4GB&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click for Performance Comparisons on 2 Tesla T4 GPUs via DDP:&lt;/summary&gt; **Time taken for 1 epoch** &#xA; &lt;p&gt;Two Tesla T4s on Kaggle &lt;code&gt;bsz = 2, ga = 4, max_grad_norm = 0.3, num_train_epochs = 1, seed = 3047, lr = 2e-4, wd = 0.01, optim = &#34;adamw_8bit&#34;, schedule = &#34;linear&#34;, schedule_steps = 10&lt;/code&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;System&lt;/th&gt; &#xA;    &lt;th&gt;GPU&lt;/th&gt; &#xA;    &lt;th&gt;Alpaca (52K)&lt;/th&gt; &#xA;    &lt;th&gt;LAION OIG (210K)&lt;/th&gt; &#xA;    &lt;th&gt;Open Assistant (10K)&lt;/th&gt; &#xA;    &lt;th&gt;SlimOrca (518K) *&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Huggingface&lt;/td&gt; &#xA;    &lt;td&gt;2 T4&lt;/td&gt; &#xA;    &lt;td&gt;84h 47m&lt;/td&gt; &#xA;    &lt;td&gt;163h 48m&lt;/td&gt; &#xA;    &lt;td&gt;30h 51m&lt;/td&gt; &#xA;    &lt;td&gt;1301h 24m *&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Unsloth Pro&lt;/td&gt; &#xA;    &lt;td&gt;2 T4&lt;/td&gt; &#xA;    &lt;td&gt;3h 20m (25.4x)&lt;/td&gt; &#xA;    &lt;td&gt;5h 43m (28.7x)&lt;/td&gt; &#xA;    &lt;td&gt;1h 12m (25.7x)&lt;/td&gt; &#xA;    &lt;td&gt;71h 40m (18.1x) *&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Unsloth Max&lt;/td&gt; &#xA;    &lt;td&gt;2 T4&lt;/td&gt; &#xA;    &lt;td&gt;3h 4m (27.6x)&lt;/td&gt; &#xA;    &lt;td&gt;5h 14m (31.3x)&lt;/td&gt; &#xA;    &lt;td&gt;1h 6m (28.1x)&lt;/td&gt; &#xA;    &lt;td&gt;54h 20m (23.9x) *&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;&lt;strong&gt;Peak Memory Usage on a Multi GPU System (2 GPUs)&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;System&lt;/th&gt; &#xA;    &lt;th&gt;GPU&lt;/th&gt; &#xA;    &lt;th&gt;Alpaca (52K)&lt;/th&gt; &#xA;    &lt;th&gt;LAION OIG (210K)&lt;/th&gt; &#xA;    &lt;th&gt;Open Assistant (10K)&lt;/th&gt; &#xA;    &lt;th&gt;SlimOrca (518K) *&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Huggingface&lt;/td&gt; &#xA;    &lt;td&gt;2 T4&lt;/td&gt; &#xA;    &lt;td&gt;8.4GB | 6GB&lt;/td&gt; &#xA;    &lt;td&gt;7.2GB | 5.3GB&lt;/td&gt; &#xA;    &lt;td&gt;14.3GB | 6.6GB&lt;/td&gt; &#xA;    &lt;td&gt;10.9GB | 5.9GB *&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Unsloth Pro&lt;/td&gt; &#xA;    &lt;td&gt;2 T4&lt;/td&gt; &#xA;    &lt;td&gt;7.7GB | 4.9GB&lt;/td&gt; &#xA;    &lt;td&gt;7.5GB | 4.9GB&lt;/td&gt; &#xA;    &lt;td&gt;8.5GB | 4.9GB&lt;/td&gt; &#xA;    &lt;td&gt;6.2GB | 4.7GB *&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Unsloth Max&lt;/td&gt; &#xA;    &lt;td&gt;2 T4&lt;/td&gt; &#xA;    &lt;td&gt;10.5GB | 5GB&lt;/td&gt; &#xA;    &lt;td&gt;10.6GB | 5GB&lt;/td&gt; &#xA;    &lt;td&gt;10.6GB | 5GB&lt;/td&gt; &#xA;    &lt;td&gt;10.5GB | 5GB *&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Slim Orca &lt;code&gt;bsz=1&lt;/code&gt; for all benchmarks since &lt;code&gt;bsz=2&lt;/code&gt; OOMs. We can handle &lt;code&gt;bsz=2&lt;/code&gt;, but we benchmark it with &lt;code&gt;bsz=1&lt;/code&gt; for consistency.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/sJ7RhGG/image-41.png&#34; alt=&#34;&#34;&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Thank You to&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HuyNguyen-hust&#34;&gt;HuyNguyen-hust&lt;/a&gt; for making &lt;a href=&#34;https://github.com/unslothai/unsloth/pull/238&#34;&gt;RoPE Embeddings 28% faster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/RandomInternetPreson&#34;&gt;RandomInternetPreson&lt;/a&gt; for confirming WSL support&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/152334H&#34;&gt;152334H&lt;/a&gt; for experimental DPO support&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/atgctg&#34;&gt;atgctg&lt;/a&gt; for syntax highlighting&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>1Panel-dev/MaxKB</title>
    <updated>2024-04-28T01:36:12Z</updated>
    <id>tag:github.com,2024-04-28:/1Panel-dev/MaxKB</id>
    <link href="https://github.com/1Panel-dev/MaxKB" rel="alternate"></link>
    <summary type="html">&lt;p&gt;💬 基于 LLM 大语言模型的知识库问答系统。开箱即用，支持快速嵌入到第三方业务系统，1Panel 官方出品。&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/1Panel-dev/maxkb/assets/52996290/c0694996-0eed-40d8-b369-322bf2a380bf&#34; alt=&#34;MaxKB&#34; width=&#34;300&#34;&gt;&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;基于 LLM 大语言模型的知识库问答系统&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.gnu.org/licenses/old-licenses/gpl-3.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/1Panel-dev/maxkb?color=%231890FF&#34; alt=&#34;License: GPL v3&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.codacy.com/gh/1Panel-dev/maxkb?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=1Panel-dev/maxkb&amp;amp;utm_campaign=Badge_Grade_Dashboard&#34;&gt;&lt;img src=&#34;https://app.codacy.com/project/badge/Grade/da67574fd82b473992781d1386b937ef&#34; alt=&#34;Codacy&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/1Panel-dev/maxkb/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/1Panel-dev/maxkb&#34; alt=&#34;Latest release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/1Panel-dev/maxkb&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/1Panel-dev/maxkb?color=%231890FF&amp;amp;style=flat-square&#34; alt=&#34;Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/1panel/maxkb&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/1panel/maxkb?label=downloads&#34; alt=&#34;Download&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;MaxKB 是一款基于 LLM 大语言模型的知识库问答系统。MaxKB = Max Knowledge Base，旨在成为企业的最强大脑。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;开箱即用&lt;/strong&gt;：支持直接上传文档、自动爬取在线文档，支持文本自动拆分、向量化，智能问答交互体验好；&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;无缝嵌入&lt;/strong&gt;：支持零编码快速嵌入到第三方业务系统；&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;多模型支持&lt;/strong&gt;：支持对接主流的大模型，包括 Ollama 本地私有大模型（如 Llama 2、Llama 3、qwen）、通义千问、OpenAI、Azure OpenAI、Kimi、智谱 AI、讯飞星火和百度千帆大模型等。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;快速开始&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -d --name=maxkb -p 8080:8080 -v ~/.maxkb:/var/lib/postgresql/data 1panel/maxkb&#xA;&#xA;# 用户名: admin&#xA;# 密码: MaxKB@123..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;你也可以通过 &lt;a href=&#34;https://apps.fit2cloud.com/1panel&#34;&gt;1Panel 应用商店&lt;/a&gt; 快速部署 MaxKB + Ollama + Llama 2，30 分钟内即可上线基于本地大模型的知识库问答系统，并嵌入到第三方业务系统中。&lt;/p&gt; &#xA;&lt;p&gt;你也可以在线体验：&lt;a href=&#34;https://dataease.io/docs/v2/&#34;&gt;DataEase 小助手&lt;/a&gt;，它是基于 MaxKB 搭建的智能问答系统，已经嵌入到 DataEase 产品及在线文档中。&lt;/p&gt; &#xA;&lt;p&gt;如你有更多问题，可以查看使用手册，或者通过论坛与我们交流。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/1Panel-dev/MaxKB/wiki/1-%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2&#34;&gt;使用手册&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1BE421M7YM/&#34;&gt;演示视频&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bbs.fit2cloud.com/c/mk/11&#34;&gt;论坛求助&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;技术交流群&lt;br&gt; &lt;img height=&#34;150px&#34; width=&#34;150px&#34; src=&#34;https://github.com/1Panel-dev/MaxKB/assets/52996290/a4f6303d-9667-4be0-bc2d-0110af782f67&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;UI 展示&lt;/h2&gt; &#xA;&lt;table style=&#34;border-collapse: collapse; border: 1px solid black;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/1Panel-dev/MaxKB/assets/80892890/2b893a25-ae46-48da-b6a1-61d23015565e&#34; alt=&#34;MaxKB Demo1&#34;&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/1Panel-dev/MaxKB/assets/80892890/3e50e7ff-cdc4-4a37-b430-d84975f11d4e&#34; alt=&#34;MaxKB Demo2&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/1Panel-dev/MaxKB/assets/80892890/dfdcc03f-ef36-4f75-bb82-797c0f9da1ad&#34; alt=&#34;MaxKB Demo3&#34;&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;padding: 5px;background-color:#fff;&#34;&gt;&lt;img src=&#34;https://github.com/1Panel-dev/MaxKB/assets/52996290/f8e36cad-b6d5-44bb-a9ab-8fa8e289377a&#34; alt=&#34;MaxKB Demo4&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;技术栈&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;前端：&lt;a href=&#34;https://cn.vuejs.org/&#34;&gt;Vue.js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;后端：&lt;a href=&#34;https://www.djangoproject.com/&#34;&gt;Python / Django&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LangChain：&lt;a href=&#34;https://www.langchain.com/&#34;&gt;LangChain&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;向量数据库：&lt;a href=&#34;https://www.postgresql.org/&#34;&gt;PostgreSQL / pgvector&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;大模型：Azure OpenAI、OpenAI、百度千帆大模型、&lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;Ollama&lt;/a&gt;、通义千问、Kimi、智谱 AI、讯飞星火&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#1Panel-dev/MaxKB&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=1Panel-dev/MaxKB&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;我们的其他开源产品&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jumpserver/jumpserver/&#34;&gt;JumpServer&lt;/a&gt; - 广受欢迎的开源堡垒机&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dataease/dataease/&#34;&gt;DataEase&lt;/a&gt; - 人人可用的开源数据可视化分析工具&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/metersphere/metersphere/&#34;&gt;MeterSphere&lt;/a&gt; - 一站式开源自动化测试平台&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/1panel-dev/1panel/&#34;&gt;1Panel&lt;/a&gt; - 现代化、开源的 Linux 服务器运维管理面板&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/halo-dev/halo/&#34;&gt;Halo&lt;/a&gt; - 强大易用的开源建站工具&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright (c) 2014-2024 飞致云 FIT2CLOUD, All rights reserved.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under The GNU General Public License version 3 (GPLv3) (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.html&#34;&gt;https://www.gnu.org/licenses/gpl-3.0.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
</feed>