<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-07T01:47:55Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Zejun-Yang/AniPortrait</title>
    <updated>2024-04-07T01:47:55Z</updated>
    <id>tag:github.com,2024-04-07:/Zejun-Yang/AniPortrait</id>
    <link href="https://github.com/Zejun-Yang/AniPortrait" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AniPortrait&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animations&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Author: Huawei Wei, Zejun Yang, Zhisheng Wang&lt;/p&gt; &#xA;&lt;p&gt;Organization: Tencent Games Zhiji, Tencent&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Zejun-Yang/AniPortrait/main/asset/zhiji_logo.png&#34; alt=&#34;zhiji_logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. You can also provide a video to achieve face reenacment.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.17694&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ZJYang/AniPortrait/tree/main&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-orange&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/ZJYang/AniPortrait_official&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-green&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pipeline&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Zejun-Yang/AniPortrait/main/asset/pipeline.png&#34; alt=&#34;pipeline&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updates / TODO List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ [2024/03/27] Now our paper is available on arXiv.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ [2024/03/27] Update the code to generate pose_temp.npy for head pose control.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ [2024/04/02] Update a new pose retarget strategy for vid2vid. Now we support substantial pose difference between ref_image and source video.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ [2024/04/03] We release our Gradio &lt;a href=&#34;https://huggingface.co/spaces/ZJYang/AniPortrait_official&#34;&gt;demo&lt;/a&gt; on HuggingFace Spaces (thanks to the HF team for their free GPU support)!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üî≤ We will release audio2pose pre-trained weight for audio2video after futher optimization. You can choose head pose template in &lt;code&gt;./configs/inference/head_pose_temp&lt;/code&gt; as substitution.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Various Generated Videos&lt;/h2&gt; &#xA;&lt;h3&gt;Self driven&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/82c0f0b0-9c7c-4aad-bf0e-27e6098ffbe1&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/51a502d9-1ce2-48d2-afbe-767a0b9b9166&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Face reenacment&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/d4e0add6-20a2-4f4b-808c-530a6f4d3331&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/849fce22-0db1-4257-a75f-a5dc655e6b9e&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Video Source: &lt;a href=&#34;https://www.bilibili.com/video/BV1H4421F7dE/?spm_id_from=333.337.search-card.all.click&#34;&gt;ÈπøÁÅ´CAVY from bilibili&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Audio driven&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/63171e5a-e4c1-4383-8f20-9764524928d0&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/6fd74024-ba19-4f6b-b37a-10df5cf2c934&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/9e516cc5-bf09-4d45-b5e3-820030764982&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/7c68148b-8022-453f-be9a-c69590038197&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Build environment&lt;/h3&gt; &#xA;&lt;p&gt;We recommend a python version &amp;gt;=3.10 and cuda version =11.7. Then build environment as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download weights&lt;/h3&gt; &#xA;&lt;p&gt;All the weights should be placed under the &lt;code&gt;./pretrained_weights&lt;/code&gt; direcotry. You can download weights manually as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download our trained &lt;a href=&#34;https://huggingface.co/ZJYang/AniPortrait/tree/main&#34;&gt;weights&lt;/a&gt;, which include four parts: &lt;code&gt;denoising_unet.pth&lt;/code&gt;, &lt;code&gt;reference_unet.pth&lt;/code&gt;, &lt;code&gt;pose_guider.pth&lt;/code&gt;, &lt;code&gt;motion_module.pth&lt;/code&gt; and &lt;code&gt;audio2mesh.pt&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download pretrained weight of based models and other components:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;StableDiffusion V1.5&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;sd-vae-ft-mse&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lambdalabs/sd-image-variations-diffusers/tree/main/image_encoder&#34;&gt;image_encoder&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/wav2vec2-base-960h&#34;&gt;wav2vec2-base-960h&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Finally, these weights should be orgnized as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;./pretrained_weights/&#xA;|-- image_encoder&#xA;|   |-- config.json&#xA;|   `-- pytorch_model.bin&#xA;|-- sd-vae-ft-mse&#xA;|   |-- config.json&#xA;|   |-- diffusion_pytorch_model.bin&#xA;|   `-- diffusion_pytorch_model.safetensors&#xA;|-- stable-diffusion-v1-5&#xA;|   |-- feature_extractor&#xA;|   |   `-- preprocessor_config.json&#xA;|   |-- model_index.json&#xA;|   |-- unet&#xA;|   |   |-- config.json&#xA;|   |   `-- diffusion_pytorch_model.bin&#xA;|   `-- v1-inference.yaml&#xA;|-- wav2vec2-base-960h&#xA;|   |-- config.json&#xA;|   |-- feature_extractor_config.json&#xA;|   |-- preprocessor_config.json&#xA;|   |-- pytorch_model.bin&#xA;|   |-- README.md&#xA;|   |-- special_tokens_map.json&#xA;|   |-- tokenizer_config.json&#xA;|   `-- vocab.json&#xA;|-- audio2mesh.pt&#xA;|-- denoising_unet.pth&#xA;|-- motion_module.pth&#xA;|-- pose_guider.pth&#xA;`-- reference_unet.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: If you have installed some of the pretrained models, such as &lt;code&gt;StableDiffusion V1.5&lt;/code&gt;, you can specify their paths in the config file (e.g. &lt;code&gt;./config/prompts/animation.yaml&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Here are the cli commands for running inference scripts:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Kindly note that you can set -L to the desired number of generating frames in the command, for example, -L 300.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Self driven&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.pose2vid --config ./configs/prompts/animation.yaml -W 512 -H 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can refer the format of animation.yaml to add your own reference images or pose videos. To convert the raw video into a pose video (keypoint sequence), you can run with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.vid2pose --video_path pose_video_path.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Face reenacment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.vid2vid --config ./configs/prompts/animation_facereenac.yaml -W 512 -H 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add source face videos and reference images in the animation_facereenac.yaml.&lt;/p&gt; &#xA;&lt;h3&gt;Audio driven&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.audio2vid --config ./configs/prompts/animation_audio.yaml -W 512 -H 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add audios and reference images in the animation_audio.yaml.&lt;/p&gt; &#xA;&lt;p&gt;You can use this command to generate a pose_temp.npy for head pose control:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.generate_ref_pose --ref_video ./configs/inference/head_pose_temp/pose_ref_video.mp4 --save_path ./configs/inference/head_pose_temp/pose.npy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Data preparation&lt;/h3&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://liangbinxie.github.io/projects/vfhq/&#34;&gt;VFHQ&lt;/a&gt; and &lt;a href=&#34;https://github.com/CelebV-HQ/CelebV-HQ&#34;&gt;CelebV-HQ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Extract keypoints from raw videos and write training json file (here is an example of processing VFHQ):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.preprocess_dataset --input_dir VFHQ_PATH --output_dir SAVE_PATH --training_json JSON_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Update lines in the training config file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;data:&#xA;  json_path: JSON_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Stage1&lt;/h3&gt; &#xA;&lt;p&gt;Run command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;accelerate launch train_stage_1.py --config ./configs/train/stage1.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Stage2&lt;/h3&gt; &#xA;&lt;p&gt;Put the pretrained motion module weights &lt;code&gt;mm_sd_v15_v2.ckpt&lt;/code&gt; (&lt;a href=&#34;https://huggingface.co/guoyww/animatediff/blob/main/mm_sd_v15_v2.ckpt&#34;&gt;download link&lt;/a&gt;) under &lt;code&gt;./pretrained_weights&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Specify the stage1 training weights in the config file &lt;code&gt;stage2.yaml&lt;/code&gt;, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;stage1_ckpt_dir: &#39;./exp_output/stage1&#39;&#xA;stage1_ckpt_step: 30000 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;accelerate launch train_stage_2.py --config ./configs/train/stage2.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We first thank the authors of &lt;a href=&#34;https://github.com/HumanAIGC/EMO&#34;&gt;EMO&lt;/a&gt;, and part of the images and audios in our demos are from EMO. Additionally, we would like to thank the contributors to the &lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone&#34;&gt;Moore-AnimateAnyone&lt;/a&gt;, &lt;a href=&#34;https://github.com/magic-research/magic-animate&#34;&gt;majic-animate&lt;/a&gt;, &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;animatediff&lt;/a&gt; and &lt;a href=&#34;https://github.com/guoqincode/Open-AnimateAnyone&#34;&gt;Open-AnimateAnyone&lt;/a&gt; repositories, for their open research and exploration.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{wei2024aniportrait,&#xA;      title={AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animations}, &#xA;      author={Huawei Wei and Zejun Yang and Zhisheng Wang},&#xA;      year={2024},&#xA;      eprint={2403.17694},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>jasonppy/VoiceCraft</title>
    <updated>2024-04-07T01:47:55Z</updated>
    <id>tag:github.com,2024-04-07:/jasonppy/VoiceCraft</id>
    <link href="https://github.com/jasonppy/VoiceCraft" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Zero-Shot Speech Editing and Text-to-Speech in the Wild&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://jasonppy.github.io/VoiceCraft_web&#34;&gt;Demo&lt;/a&gt; &lt;a href=&#34;https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;TL;DR&lt;/h3&gt; &#xA;&lt;p&gt;VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both &lt;strong&gt;speech editing&lt;/strong&gt; and &lt;strong&gt;zero-shot text-to-speech (TTS)&lt;/strong&gt; on in-the-wild data including audiobooks, internet videos, and podcasts.&lt;/p&gt; &#xA;&lt;p&gt;To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.&lt;/p&gt; &#xA;&lt;h2&gt;How to run inference&lt;/h2&gt; &#xA;&lt;p&gt;There are three ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;with Google Colab. see &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/#quickstart-colab&#34;&gt;quickstart colab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;with docker. see &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/#quickstart-docker&#34;&gt;quickstart docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;without docker. see &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/#environment-setup&#34;&gt;environment setup&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;When you are inside the docker image or you have installed all dependencies, Checkout &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/inference_tts.ipynb&#34;&gt;&lt;code&gt;inference_tts.ipynb&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to do model development such as training/finetuning, I recommend following &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/#environment-setup&#34;&gt;envrionment setup&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/#training&#34;&gt;training&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; 03/28/2024: Model weights for giga330M and giga830M are up on HuggingFaceü§ó &lt;a href=&#34;https://huggingface.co/pyp1/VoiceCraft/tree/main&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; 04/05/2024: I finetuned giga330M with the TTS objective on gigaspeech and 1/5 of librilight, the model outperforms giga830M on TTS. Weights are &lt;a href=&#34;https://huggingface.co/pyp1/VoiceCraft/tree/main&#34;&gt;here&lt;/a&gt;. Make sure maximal prompt + generation length &amp;lt;= 16 seconds (due to our limited compute, we had to drop utterances longer than 16s in training data)&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Codebase upload&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Environment setup&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference demo for speech editing and TTS&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Training guidance&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; RealEdit dataset and training manifest&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Model weights (giga330M.pth, giga830M.pth, and gigaHalfLibri330M_TTSEnhanced_max16s.pth)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Write colab notebooks for better hands-on experience&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; HuggingFace Spaces demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Better guidance on training/finetuning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;QuickStart Colab&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; To try out speech editing or TTS Inference with VoiceCraft, the simplest way is using Google Colab. Instructions to run are on the Colab itself.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;To try &lt;a href=&#34;https://colab.research.google.com/drive/1FV7EC36dl8UioePY1xXijXTMl7X47kR_?usp=sharing&#34;&gt;Speech Editing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;To try &lt;a href=&#34;https://colab.research.google.com/drive/1lch_6it5-JpXgAQlUTRRI2z2_rk5K67Z?usp=sharing&#34;&gt;TTS Inference&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;QuickStart Docker&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; To try out TTS inference with VoiceCraft, you can also use docker. Thank &lt;a href=&#34;https://github.com/ubergarm&#34;&gt;@ubergarm&lt;/a&gt; and &lt;a href=&#34;https://github.com/jay-c88&#34;&gt;@jayc88&lt;/a&gt; for making this happen.&lt;/p&gt; &#xA;&lt;p&gt;Tested on Linux and Windows and should work with any host with docker installed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1. clone the repo on in a directory on a drive with plenty of free space&#xA;git clone git@github.com:jasonppy/VoiceCraft.git&#xA;cd VoiceCraft&#xA;&#xA;# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)&#xA;# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html&#xA;# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...&#xA;&#xA;# 3. First build the docker image&#xA;docker build --tag &#34;voicecraft&#34; .&#xA;&#xA;# 4. Try to start an existing container otherwise create a new one passing in all GPUs&#xA;./start-jupyter.sh  # linux&#xA;start-jupyter.bat   # windows&#xA;&#xA;# 5. now open a webpage on the host box to the URL shown at the bottom of:&#xA;docker logs jupyter&#xA;&#xA;# 6. optionally look inside from another terminal&#xA;docker exec -it jupyter /bin/bash&#xA;export USER=(your_linux_username_used_above)&#xA;export HOME=/home/$USER&#xA;sudo apt-get update&#xA;&#xA;# 7. confirm video card(s) are visible inside container&#xA;nvidia-smi&#xA;&#xA;# 8. Now in browser, open inference_tts.ipynb and work through one cell at a time&#xA;echo GOOD LUCK&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Environment setup&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n voicecraft python=3.9.16&#xA;conda activate voicecraft&#xA;&#xA;pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft&#xA;pip install xformers==0.0.22&#xA;pip install torchaudio==2.0.2 torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201&#xA;apt-get install ffmpeg # if you don&#39;t already have ffmpeg installed&#xA;apt-get install espeak-ng # backend for the phonemizer installed below&#xA;pip install tensorboard==2.16.2&#xA;pip install phonemizer==3.2.1&#xA;pip install datasets==2.16.0&#xA;pip install torchmetrics==0.11.1&#xA;# install MFA for getting forced-alignment, this could take a few minutes&#xA;conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068&#xA;# conda install pocl # above gives an warning for installing pocl, not sure if really need this&#xA;&#xA;# to run ipynb&#xA;conda install -n voicecraft ipykernel --no-deps --force-reinstall&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have encountered version issues when running things, checkout &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/environment.yml&#34;&gt;environment.yml&lt;/a&gt; for exact matching.&lt;/p&gt; &#xA;&lt;h2&gt;Inference Examples&lt;/h2&gt; &#xA;&lt;p&gt;Checkout &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/inference_speech_editing.ipynb&#34;&gt;&lt;code&gt;inference_speech_editing.ipynb&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/inference_tts.ipynb&#34;&gt;&lt;code&gt;inference_tts.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To train an VoiceCraft model, you need to prepare the following parts:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;utterances and their transcripts&lt;/li&gt; &#xA; &lt;li&gt;encode the utterances into codes using e.g. Encodec&lt;/li&gt; &#xA; &lt;li&gt;convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)&lt;/li&gt; &#xA; &lt;li&gt;manifest (i.e. metadata)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Step 1,2,3 are handled in &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/data/phonemize_encodec_encode_hf.py&#34;&gt;./data/phonemize_encodec_encode_hf.py&lt;/a&gt;, where&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)&lt;/li&gt; &#xA; &lt;li&gt;phoneme sequence and encodec codes are also extracted using the script.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;An example run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate voicecraft&#xA;export CUDA_VISIBLE_DEVICES=0&#xA;cd ./data&#xA;python phonemize_encodec_encode_hf.py \&#xA;--dataset_size xs \&#xA;--download_to path/to/store_huggingface_downloads \&#xA;--save_dir path/to/store_extracted_codes_and_phonemes \&#xA;--encodec_model_path path/to/encodec_model \&#xA;--mega_batch_size 120 \&#xA;--batch_size 32 \&#xA;--max_len 30000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where encodec_model_path is avaliable &lt;a href=&#34;https://huggingface.co/pyp1/VoiceCraft&#34;&gt;here&lt;/a&gt;. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our &lt;a href=&#34;https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf&#34;&gt;paper&lt;/a&gt;. If you encounter OOM during extraction, try decrease the batch_size and/or max_len. The extracted codes, phonemes, and vocab.txt will be stored at &lt;code&gt;path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;As for manifest, please download train.txt and validation.txt from &lt;a href=&#34;https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main&#34;&gt;here&lt;/a&gt;, and put them under &lt;code&gt;path/to/store_extracted_codes_and_phonemes/manifest/&lt;/code&gt;. Please also download vocab.txt from &lt;a href=&#34;https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main&#34;&gt;here&lt;/a&gt; if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same).&lt;/p&gt; &#xA;&lt;p&gt;Now, you are good to start training!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate voicecraft&#xA;cd ./z_scripts&#xA;bash e830M.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The codebase is under CC BY-NC-SA 4.0 (&lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/LICENSE-CODE&#34;&gt;LICENSE-CODE&lt;/a&gt;), and the model weights are under Coqui Public Model License 1.0.0 (&lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/LICENSE-MODEL&#34;&gt;LICENSE-MODEL&lt;/a&gt;). Note that we use some of the code from other repository that are under different licenses: &lt;code&gt;./models/codebooks_patterns.py&lt;/code&gt; is under MIT license; &lt;code&gt;./models/modules&lt;/code&gt;, &lt;code&gt;./steps/optim.py&lt;/code&gt;, &lt;code&gt;data/tokenizer.py&lt;/code&gt; are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License.&lt;/p&gt; &#xA;&lt;!-- How to use g2p to convert english text into IPA phoneme sequence&#xA;first install it with `pip install g2p`&#xA;```python&#xA;from g2p import make_g2p&#xA;transducer = make_g2p(&#39;eng&#39;, &#39;eng-ipa&#39;)&#xA;transducer(&#34;hello&#34;).output_string&#xA;# it will output: &#39;h ålo ä&#39;&#xA;``` --&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We thank Feiteng for his &lt;a href=&#34;https://github.com/lifeiteng/vall-e&#34;&gt;VALL-E reproduction&lt;/a&gt;, and we thank audiocraft team for open-sourcing &lt;a href=&#34;https://github.com/facebookresearch/audiocraft&#34;&gt;encodec&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{peng2024voicecraft,&#xA;  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},&#xA;  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},&#xA;  journal   = {arXiv},&#xA;  year      = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone&#39;s speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenDevin/OpenDevin</title>
    <updated>2024-04-07T01:47:55Z</updated>
    <id>tag:github.com,2024-04-07:/OpenDevin/OpenDevin</id>
    <link href="https://github.com/OpenDevin/OpenDevin" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üêö OpenDevin: Code Less, Make More&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name=&#34;readme-top&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!--&#xA;*** Thanks for checking out the Best-README-Template. If you have a suggestion&#xA;*** that would make this better, please fork the repo and create a pull request&#xA;*** or simply open an issue with the tag &#34;enhancement&#34;.&#xA;*** Don&#39;t forget to give the project a star!&#xA;*** Thanks again! Now go create something AMAZING! :D&#xA;--&gt; &#xA;&lt;!-- PROJECT SHIELDS --&gt; &#xA;&lt;!--&#xA;*** I&#39;m using markdown &#34;reference style&#34; links for readability.&#xA;*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).&#xA;*** See the bottom of this document for the declaration of the reference variables&#xA;*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.&#xA;*** https://www.markdownguide.org/basic-syntax/#reference-style-links&#xA;--&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/OpenDevin/OpenDevin/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/opendevin/opendevin?style=for-the-badge&#34; alt=&#34;Contributors&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/OpenDevin/OpenDevin/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/opendevin/opendevin?style=for-the-badge&#34; alt=&#34;Forks&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/OpenDevin/OpenDevin/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/opendevin/opendevin?style=for-the-badge&#34; alt=&#34;Stargazers&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/OpenDevin/OpenDevin/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/opendevin/opendevin?style=for-the-badge&#34; alt=&#34;Issues&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/OpenDevin/OpenDevin/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/opendevin/opendevin?style=for-the-badge&#34; alt=&#34;MIT License&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://join.slack.com/t/opendevin/shared_invite/zt-2etftj1dd-X1fDL2PYIVpsmJZkqEYANw&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;amp;logoColor=white&amp;amp;style=for-the-badge&#34; alt=&#34;Join our Slack community&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://discord.gg/mBuDGRzzES&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;amp;logoColor=white&amp;amp;style=for-the-badge&#34; alt=&#34;Join our Discord community&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- PROJECT LOGO --&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/logo.png&#34; alt=&#34;Logo&#34; width=&#34;200&#34; height=&#34;200&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;OpenDevin: Code Less, Make More&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- TABLE OF CONTENTS --&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;üóÇÔ∏è Table of Contents&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#-mission&#34;&gt;üéØ Mission&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#-what-is-devin&#34;&gt;ü§î What is Devin?&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#-why-opendevin&#34;&gt;üêö Why OpenDevin?&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#-project-status&#34;&gt;üöß Project Status&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#-get-started&#34;&gt;üöÄ Get Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#1-requirements&#34;&gt;1. Requirements&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#2-build-and-setup&#34;&gt;2. Build and Setup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#3-run-the-application&#34;&gt;3. Run the Application&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#4-individual-server-startup&#34;&gt;4. Individual Server Startup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#5-help&#34;&gt;5. Help&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt;  &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#%EF%B8%8F-research-strategy&#34;&gt;‚≠êÔ∏è Research Strategy&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#-how-to-contribute&#34;&gt;ü§ù How to Contribute&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#-join-our-community&#34;&gt;ü§ñ Join Our Community&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#%EF%B8%8F-built-with&#34;&gt;üõ†Ô∏è Built With&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#-license&#34;&gt;üìú License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üéØ Mission&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenDevin/OpenDevin/assets/38853559/71a472cc-df34-430c-8b1d-4d7286c807c9&#34;&gt;Project Demo Video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome to OpenDevin, an open-source project aiming to replicate Devin, an autonomous AI software engineer who is capable of executing complex engineering tasks and collaborating actively with users on software development projects. This project aspires to replicate, enhance, and innovate upon Devin through the power of the open-source community.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34; style=&#34;font-size: 14px; color: #555; margin-top: 20px;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#readme-top&#34; style=&#34;text-decoration: none; color: #007bff; font-weight: bold;&#34;&gt; ‚Üë Back to Top ‚Üë &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;ü§î What is Devin?&lt;/h2&gt; &#xA;&lt;p&gt;Devin represents a cutting-edge autonomous agent designed to navigate the complexities of software engineering. It leverages a combination of tools such as a shell, code editor, and web browser, showcasing the untapped potential of LLMs in software development. Our goal is to explore and expand upon Devin&#39;s capabilities, identifying both its strengths and areas for improvement, to guide the progress of open code models.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34; style=&#34;font-size: 14px; color: #555; margin-top: 20px;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#readme-top&#34; style=&#34;text-decoration: none; color: #007bff; font-weight: bold;&#34;&gt; ‚Üë Back to Top ‚Üë &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üêö Why OpenDevin?&lt;/h2&gt; &#xA;&lt;p&gt;The OpenDevin project is born out of a desire to replicate, enhance, and innovate beyond the original Devin model. By engaging the open-source community, we aim to tackle the challenges faced by Code LLMs in practical scenarios, producing works that significantly contribute to the community and pave the way for future advancements.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34; style=&#34;font-size: 14px; color: #555; margin-top: 20px;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#readme-top&#34; style=&#34;text-decoration: none; color: #007bff; font-weight: bold;&#34;&gt; ‚Üë Back to Top ‚Üë &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üöß Project Status&lt;/h2&gt; &#xA;&lt;p&gt;OpenDevin is currently a work in progress, but you can already run the alpha version to see the end-to-end system in action. The project team is actively working on the following key milestones:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;UI&lt;/strong&gt;: Developing a user-friendly interface, including a chat interface, a shell demonstrating commands, and a web browser.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: Building a stable agent framework with a robust backend that can read, write, and run simple commands.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agent Capabilities&lt;/strong&gt;: Enhancing the agent&#39;s abilities to generate bash scripts, run tests, and perform other software engineering tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: Establishing a minimal evaluation pipeline that is consistent with Devin&#39;s evaluation criteria.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After completing the MVP, the team will focus on research in various areas, including foundation models, specialist capabilities, evaluation, and agent studies.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34; style=&#34;font-size: 14px; color: #555; margin-top: 20px;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#readme-top&#34; style=&#34;text-decoration: none; color: #007bff; font-weight: bold;&#34;&gt; ‚Üë Back to Top ‚Üë &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Getting started with the OpenDevin project is incredibly easy. Follow these simple steps to set up and run OpenDevin on your system:&lt;/p&gt; &#xA;&lt;h3&gt;1. Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux, Mac OS, or &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/wsl/install&#34;&gt;WSL on Windows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Docker&lt;/a&gt;(For those on MacOS, make sure to allow the default Docker socket to be used from advanced settings!)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt; &amp;gt;= 3.11&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nodejs.org/en/download/package-manager&#34;&gt;NodeJS&lt;/a&gt; &amp;gt;= 18.17.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python-poetry.org/docs/#installing-with-the-official-installer&#34;&gt;Poetry&lt;/a&gt; &amp;gt;= 1.8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2. Build and Setup The Environment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Build the Project:&lt;/strong&gt; Begin by building the project, which includes setting up the environment and installing dependencies. This step ensures that OpenDevin is ready to run smoothly on your system. &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. Configuring the Language Model&lt;/h3&gt; &#xA;&lt;p&gt;OpenDevin supports a diverse array of Language Models (LMs) through the powerful &lt;a href=&#34;https://docs.litellm.ai&#34;&gt;litellm&lt;/a&gt; library. By default, we&#39;ve chosen the mighty GPT-4 from OpenAI as our go-to model, but the world is your oyster! You can unleash the potential of Anthropic&#39;s suave Claude, the enigmatic Llama, or any other LM that piques your interest.&lt;/p&gt; &#xA;&lt;p&gt;To configure the LM of your choice, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Using the Makefile: The Effortless Approach&lt;/strong&gt; With a single command, you can have a smooth LM setup for your OpenDevin experience. Simply run:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make setup-config&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command will prompt you to enter the LLM API key and model name, ensuring that OpenDevin is tailored to your specific needs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Manual Config: The Artisanal Touch&lt;/strong&gt; If you&#39;re feeling particularly adventurous, you can manually update the &lt;code&gt;config.toml&lt;/code&gt; file located in the project&#39;s root directory. Here, you&#39;ll find the &lt;code&gt;llm_api_key&lt;/code&gt; and &lt;code&gt;llm_model_name&lt;/code&gt; fields, where you can set the LM of your choosing.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note on Alternative Models:&lt;/strong&gt; Some alternative models may prove more challenging to tame than others. Fear not, brave adventurer! We shall soon unveil LLM-specific documentation to guide you on your quest. And if you&#39;ve already mastered the art of wielding a model other than OpenAI&#39;s GPT, we encourage you to &lt;a href=&#34;https://github.com/OpenDevin/OpenDevin/issues/417&#34;&gt;share your setup instructions with us&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a full list of the LM providers and models available, please consult the &lt;a href=&#34;https://docs.litellm.ai/docs/providers&#34;&gt;litellm documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;4. Run the Application&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Run the Application:&lt;/strong&gt; Once the setup is complete, launching OpenDevin is as simple as running a single command. This command starts both the backend and frontend servers seamlessly, allowing you to interact with OpenDevin without any hassle. &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make run&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;5. Individual Server Startup&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the Backend Server:&lt;/strong&gt; If you prefer, you can start the backend server independently to focus on backend-related tasks or configurations.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make start-backend&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the Frontend Server:&lt;/strong&gt; Similarly, you can start the frontend server on its own to work on frontend-related components or interface enhancements.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make start-frontend&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;6. Help&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Get Some Help:&lt;/strong&gt; Need assistance or information on available targets and commands? The help command provides all the necessary guidance to ensure a smooth experience with OpenDevin. &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make help&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34; style=&#34;font-size: 14px; color: #555; margin-top: 20px;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#readme-top&#34; style=&#34;text-decoration: none; color: #007bff; font-weight: bold;&#34;&gt; ‚Üë Back to Top ‚Üë &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;‚≠êÔ∏è Research Strategy&lt;/h2&gt; &#xA;&lt;p&gt;Achieving full replication of production-grade applications with LLMs is a complex endeavor. Our strategy involves:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Core Technical Research:&lt;/strong&gt; Focusing on foundational research to understand and improve the technical aspects of code generation and handling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Specialist Abilities:&lt;/strong&gt; Enhancing the effectiveness of core components through data curation, training methods, and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Task Planning:&lt;/strong&gt; Developing capabilities for bug detection, codebase management, and optimization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluation:&lt;/strong&gt; Establishing comprehensive evaluation metrics to better understand and improve our models.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;right&#34; style=&#34;font-size: 14px; color: #555; margin-top: 20px;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#readme-top&#34; style=&#34;text-decoration: none; color: #007bff; font-weight: bold;&#34;&gt; ‚Üë Back to Top ‚Üë &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;ü§ù How to Contribute&lt;/h2&gt; &#xA;&lt;p&gt;OpenDevin is a community-driven project, and we welcome contributions from everyone. Whether you&#39;re a developer, a researcher, or simply enthusiastic about advancing the field of software engineering with AI, there are many ways to get involved:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Code Contributions:&lt;/strong&gt; Help us develop the core functionalities, frontend interface, or sandboxing solutions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Research and Evaluation:&lt;/strong&gt; Contribute to our understanding of LLMs in software engineering, participate in evaluating the models, or suggest improvements.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feedback and Testing:&lt;/strong&gt; Use the OpenDevin toolset, report bugs, suggest features, or provide feedback on usability.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For details, please check &lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/CONTRIBUTING.md&#34;&gt;this document&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34; style=&#34;font-size: 14px; color: #555; margin-top: 20px;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#readme-top&#34; style=&#34;text-decoration: none; color: #007bff; font-weight: bold;&#34;&gt; ‚Üë Back to Top ‚Üë &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;ü§ñ Join Our Community&lt;/h2&gt; &#xA;&lt;p&gt;Now we have both Slack workspace for the collaboration on building OpenDevin and Discord server for discussion about anything related, e.g., this project, LLM, agent, etc.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/opendevin/shared_invite/zt-2etftj1dd-X1fDL2PYIVpsmJZkqEYANw&#34;&gt;Slack workspace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/mBuDGRzzES&#34;&gt;Discord server&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you would love to contribute, feel free to join our community (note that now there is no need to fill in the &lt;a href=&#34;https://forms.gle/758d5p6Ve8r2nxxq6&#34;&gt;form&lt;/a&gt;). Let&#39;s simplify software engineering together!&lt;/p&gt; &#xA;&lt;p&gt;üêö &lt;strong&gt;Code less, make more with OpenDevin.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#OpenDevin/OpenDevin&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=OpenDevin/OpenDevin&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Built With&lt;/h2&gt; &#xA;&lt;p&gt;OpenDevin is built using a combination of powerful frameworks and libraries, providing a robust foundation for its development. Here are the key technologies used in the project:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/FastAPI-black?style=for-the-badge&#34; alt=&#34;FastAPI&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/uvicorn-black?style=for-the-badge&#34; alt=&#34;uvicorn&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/LiteLLM-black?style=for-the-badge&#34; alt=&#34;LiteLLM&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Docker-black?style=for-the-badge&#34; alt=&#34;Docker&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Ruff-black?style=for-the-badge&#34; alt=&#34;Ruff&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/MyPy-black?style=for-the-badge&#34; alt=&#34;MyPy&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/LlamaIndex-black?style=for-the-badge&#34; alt=&#34;LlamaIndex&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/React-black?style=for-the-badge&#34; alt=&#34;React&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please note that the selection of these technologies is in progress, and additional technologies may be added or existing ones may be removed as the project evolves. We strive to adopt the most suitable and efficient tools to enhance the capabilities of OpenDevin.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34; style=&#34;font-size: 14px; color: #555; margin-top: 20px;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#readme-top&#34; style=&#34;text-decoration: none; color: #007bff; font-weight: bold;&#34;&gt; ‚Üë Back to Top ‚Üë &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üìú License&lt;/h2&gt; &#xA;&lt;p&gt;Distributed under the MIT License. See &lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/LICENSE&#34;&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34; style=&#34;font-size: 14px; color: #555; margin-top: 20px;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/#readme-top&#34; style=&#34;text-decoration: none; color: #007bff; font-weight: bold;&#34;&gt; ‚Üë Back to Top ‚Üë &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
</feed>