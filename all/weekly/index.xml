<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-04T01:52:21Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gptlink/gptlink</title>
    <updated>2023-06-04T01:52:21Z</updated>
    <id>tag:github.com,2023-06-04:/gptlink/gptlink</id>
    <link href="https://github.com/gptlink/gptlink" rel="alternate"></link>
    <summary type="html">&lt;p&gt;10分钟搭建自己可免费商用的ChatGPT环境，搭建简单，包含用户，订单，任务，付费等功能&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;GPTLink&lt;/h1&gt; &#xA; &lt;p&gt; 只需简单几步，即可快速搭建可商用的 ChatGPT 站点。&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://gpt-link.com/?shareOpenId=mjOfmdjyCBEku7fY&#34;&gt;体验地址&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/gptlink/gptlink/master/docs/show/README.md&#34;&gt;演示图片&lt;/a&gt; · &lt;a href=&#34;https://github.com/gptlink/gptlink/issues&#34;&gt;反馈&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/gptlink/gptlink/master/docs/images/qrcode.png&#34;&gt;微信加群&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gptlink/gptlink/master/docs/images/qrcode.png&#34;&gt;商务合作&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/gptlink/gptlink/master/docs/images/official.jpg&#34;&gt;关注公众号&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/gptlink/gptlink/master/docs/images/payment.jpeg&#34;&gt;打赏开发者&lt;/a&gt;&lt;/p&gt; &#xA; &lt;img src=&#34;https://github.com/gptlink/gptlink/assets/1472352/98a5012b-3111-4c50-bd36-c8eabf17f6e7&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;功能概览&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;支持 Docker 部署&lt;/li&gt; &#xA; &lt;li&gt;开箱即用的控制台&lt;/li&gt; &#xA; &lt;li&gt;完美适配移动端&lt;/li&gt; &#xA; &lt;li&gt;自定义付费套餐&lt;/li&gt; &#xA; &lt;li&gt;一键导出对话&lt;/li&gt; &#xA; &lt;li&gt;任务拉新获客&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;开始使用&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;项目基于 PHP (Hyperf) + Vue 开发，推荐使用 Docker 进行部署；&lt;/li&gt; &#xA; &lt;li&gt;准备好一个 API Key，推荐使用 &lt;a href=&#34;https://gpt-link.com&#34;&gt;GPTLINK&lt;/a&gt; Key； &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://gpt-link.com&#34;&gt;GPTLINK&lt;/a&gt; Key ，注册完成之后进入个人中心申请开发者后获取 API Key，过程非常简单，无需审核，接口无需代理；&lt;/li&gt; &#xA;   &lt;li&gt;OpenAi 官方 Key；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;微信相关应用（非必须） &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developers.weixin.qq.com/doc/oplatform/Website_App/WeChat_Login/Wechat_Login.html&#34;&gt;微信网站应用&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/&#34;&gt;微信公众号&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pay.weixin.qq.com/&#34;&gt;微信支付&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;项目配置&lt;/h2&gt; &#xA;&lt;p&gt;项目提供有限的权限控制功能，项目配置文件位于 &lt;code&gt;gptserver/.env&lt;/code&gt;，如诺不存在此文件，将 &lt;code&gt;gptserver/.env.example&lt;/code&gt; 更名为 &lt;code&gt;.env&lt;/code&gt; 作为配置项进行使用，详细的配置说明 &lt;a href=&#34;https://raw.githubusercontent.com/gptlink/gptlink/master/docs/ENV.md&#34;&gt;点此查看&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;部署&lt;/h2&gt; &#xA;&lt;p&gt;项目支持多种部署方式，部署文档参考：&lt;a href=&#34;https://github.com/gptlink/gptlink-deploy&#34;&gt;点此查看&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PHP 环境部署&lt;/li&gt; &#xA; &lt;li&gt;Docker 部署&lt;/li&gt; &#xA; &lt;li&gt;Docker Compose 部署&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;访问&lt;/h3&gt; &#xA;&lt;p&gt;部署完成后访问 &lt;code&gt;http://域名或IP&lt;/code&gt; 进入对话页面，&lt;code&gt;/admin/&lt;/code&gt; 路径访问管理页，管理员账号密码为配置项设置的 &lt;code&gt;ADMIN_USERNAME&lt;/code&gt; 与 &lt;code&gt;ADMIN_USERNAME&lt;/code&gt; ，如不传入，默认账号密码为 &lt;code&gt;admin&lt;/code&gt; &lt;code&gt;admin888&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;版本计划&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 前端开源&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 管理端开源&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 账号密码登录&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 线下收款配置&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 兑换码&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 对话记录&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; AI 生图&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 分销&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 统计视图&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;参与贡献&lt;/h2&gt; &#xA;&lt;p&gt;我们深知这不是一个完美的产品，但是它只是一个开始，欢迎加入我们一起完善！&lt;span&gt;❤️&lt;/span&gt; 请参阅 &lt;a href=&#34;https://raw.githubusercontent.com/gptlink/gptlink/master/CONTRIBUTING.md&#34;&gt;贡献指南&lt;/a&gt;&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/gptlink/gptlink/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=gptlink/gptlink&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;特别鸣谢&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/overtrue&#34;&gt;@overtrue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lainy0307&#34;&gt;@Lainy0307&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;疑难解答&lt;/h2&gt; &#xA;&lt;p&gt;常见问题汇总：&lt;a href=&#34;https://raw.githubusercontent.com/gptlink/gptlink/master/docs/FAQ.md&#34;&gt;点击查看&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;微信交流群&lt;/h2&gt; &#xA;&lt;img src=&#34;https://github.com/gptlink/gptlink/assets/1472352/904f0b13-b4d7-4fba-867b-c9ad170336cc&#34; width=&#34;300&#34;&gt; &#xA;&lt;h2&gt;开源协议&lt;/h2&gt; &#xA;&lt;p&gt;Apache License Version 2.0 see &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0.html&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0.html&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SevaSk/ecoute</title>
    <updated>2023-06-04T01:52:21Z</updated>
    <id>tag:github.com,2023-06-04:/SevaSk/ecoute</id>
    <link href="https://github.com/SevaSk/ecoute" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ecoute is a live transcription tool that provides real-time transcripts for both the user&#39;s microphone input (You) and the user&#39;s speakers output (Speaker) in a textbox. It also generates a suggested response using OpenAI&#39;s GPT-3.5 for the user to say based on the live transcription of the conversation.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🎧 Ecoute&lt;/h1&gt; &#xA;&lt;p&gt;Ecoute is a live transcription tool that provides real-time transcripts for both the user&#39;s microphone input (You) and the user&#39;s speakers output (Speaker) in a textbox. It also generates a suggested response using OpenAI&#39;s GPT-3.5 for the user to say based on the live transcription of the conversation.&lt;/p&gt; &#xA;&lt;h2&gt;📖 Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SevaSk/ecoute/assets/50382291/8ac48927-8a26-49fd-80e9-48f980986208&#34;&gt;https://github.com/SevaSk/ecoute/assets/50382291/8ac48927-8a26-49fd-80e9-48f980986208&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ecoute is designed to help users in their conversations by providing live transcriptions and generating contextually relevant responses. By leveraging the power of OpenAI&#39;s GPT-3.5, Ecoute aims to make communication more efficient and enjoyable.&lt;/p&gt; &#xA;&lt;h2&gt;🚀 Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Follow these steps to set up and run Ecoute on your local machine.&lt;/p&gt; &#xA;&lt;h3&gt;📋 Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;=3.8.0&lt;/li&gt; &#xA; &lt;li&gt;An OpenAI API key&lt;/li&gt; &#xA; &lt;li&gt;Windows OS (Not tested on others)&lt;/li&gt; &#xA; &lt;li&gt;FFmpeg&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If FFmpeg is not installed in your system, you can follow the steps below to install it.&lt;/p&gt; &#xA;&lt;p&gt;First, you need to install Chocolatey, a package manager for Windows. Open your PowerShell as Administrator and run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(&#39;https://community.chocolatey.org/install.ps1&#39;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once Chocolatey is installed, you can install FFmpeg by running the following command in your PowerShell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;choco install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please ensure that you run these commands in a PowerShell window with administrator privileges. If you face any issues during the installation, you can visit the official Chocolatey and FFmpeg websites for troubleshooting.&lt;/p&gt; &#xA;&lt;h3&gt;🔧 Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/SevaSk/ecoute&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the &lt;code&gt;ecoute&lt;/code&gt; folder:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd ecoute&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the required packages:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;keys.py&lt;/code&gt; file in the ecoute directory and add your OpenAI API key:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Option 1: You can utilize a command on your command prompt. Run the following command, ensuring to replace &#34;API KEY&#34; with your actual OpenAI API key:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python -c &#34;with open(&#39;keys.py&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f: f.write(&#39;OPENAI_API_KEY=\&#34;API KEY\&#34;&#39;)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Option 2: You can create the keys.py file manually. Open up your text editor of choice and enter the following content:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&#34;API KEY&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Replace &#34;API KEY&#34; with your actual OpenAI API key. Save this file as keys.py within the ecoute directory.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;🎬 Running Ecoute&lt;/h3&gt; &#xA;&lt;p&gt;Run the main script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a more better and faster version that also works with most languages, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Upon initiation, Ecoute will begin transcribing your microphone input and speaker output in real-time, generating a suggested response based on the conversation. Please note that it might take a few seconds for the system to warm up before the transcription becomes real-time.&lt;/p&gt; &#xA;&lt;p&gt;The --api flag will use the whisper api for transcriptions. This significantly enhances transcription speed and accuracy, and it works in most languages (rather than just English without the flag). It&#39;s expected to become the default option in future releases. However, keep in mind that using the Whisper API will consume more OpenAI credits than using the local model. This increased cost is attributed to the advanced features and capabilities that the Whisper API provides. Despite the additional expense, the substantial improvements in speed and transcription accuracy may make it a worthwhile investment for your use case.&lt;/p&gt; &#xA;&lt;h3&gt;⚠️ Limitations&lt;/h3&gt; &#xA;&lt;p&gt;While Ecoute provides real-time transcription and response suggestions, there are several known limitations to its functionality that you should be aware of:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Default Mic and Speaker:&lt;/strong&gt; Ecoute is currently configured to listen only to the default microphone and speaker set in your system. It will not detect sound from other devices or systems. If you wish to use a different mic or speaker, you will need to set it as your default device in your system settings.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Whisper Model&lt;/strong&gt;: If the --api flag is not used, we utilize the &#39;tiny&#39; version of the Whisper ASR model, due to its low resource consumption and fast response times. However, this model may not be as accurate as the larger models in transcribing certain types of speech, including accents or uncommon words.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Language&lt;/strong&gt;: If you are not using the --api flag the Whisper model used in Ecoute is set to English. As a result, it may not accurately transcribe non-English languages or dialects. We are actively working to add multi-language support to future versions of the program.&lt;/p&gt; &#xA;&lt;h2&gt;📖 License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/SevaSk/ecoute/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;🤝 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! Feel free to open issues or submit pull requests to improve Ecoute.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kyegomez/tree-of-thoughts</title>
    <updated>2023-06-04T01:52:21Z</updated>
    <id>tag:github.com,2023-06-04:/kyegomez/tree-of-thoughts</id>
    <link href="https://github.com/kyegomez/tree-of-thoughts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Plug in and Play Implementation of Tree of Thoughts: Deliberate Problem Solving with Large Language Models that Elevates Model Reasoning by atleast 70%&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tree of Thoughts 🌳🌲🌴🌿🍃&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kyegomez/tree-of-thoughts/main/tree-of-thoughts.png&#34; alt=&#34;tree of thoughts banner&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.10601.pdf&#34;&gt;Paper link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tree of Thoughts (ToT) is an all-new powerful and flexible algorithm that advances model reasoning by a whopping 70%. This is an plug in and play verision, connect your own models and enjoy superintelligence!&lt;/p&gt; &#xA;&lt;p&gt;Share this repository by clicking on the following buttons 😊&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/intent/tweet?text=Check%20out%20this%20amazing%20project%20on%20improving%20AI%20reasoning%20-%20Tree%20of%20Thoughts!%20https://github.com/kyegomez/tree-of-thoughts&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url?style=social&amp;amp;url=https%3A%2F%2Fgithub.com%2Fkyegomez%2Ftree-of-thoughts&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fgithub.com%2Fkyegomez%2Ftree-of-thoughts&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Share-LinkedIn-blue?style=social&amp;amp;logo=linkedin&#34; alt=&#34;LinkedIn&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Join Agora, Creators United&lt;/h1&gt; &#xA;&lt;p&gt;This implementation of Tree of Thoughts is brought to you by Agora, Agora advances Humanity with open source SOTA Multi-Modality AI research!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/qUtxnK2NMf&#34;&gt;Join our Discord and contribute to this project&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Basic Prompts:&lt;/h1&gt; &#xA;&lt;p&gt;No complex implementations, just pass in one of these prompts to your model: head over to &lt;code&gt;prompts.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&#39;Three experts with exceptional logical thinking skills are collaboratively answering a question using a tree of thoughts method. Each expert will share their thought process in detail, taking into account the previous thoughts of others and admitting any errors. They will iteratively refine and expand upon each other&#39;s ideas, giving credit where it&#39;s due. The process continues until a conclusive answer is found. Organize the entire response in a markdown table format. The question is...&#39;&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Set Openai key in an environment file,&lt;/p&gt; &#xA;&lt;p&gt;first create an file called: &lt;code&gt;.env&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then get your openai key and input it inside the &#39;&#39; as &lt;code&gt;OPENAI_API_KEY=&#39;SK-YOUR KEY&#39;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Method1&lt;/h2&gt; &#xA;&lt;p&gt;Clone this repository with&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;git clone https://github.com/kyegomez/tree-of-thoughts&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd tree-of-thoughts&#xA;python3 -m pip install -r requirements.txt&#xA;cd tree_of_thoughts&#xA;python3 treeofthoughts.py --problem &#34;design an new transportation system for an all-new city&#34; --search_algorithm=&#34;BFS&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add &lt;code&gt; OPENAI_API_KEY=&#39;API KEY&#39;&lt;/code&gt; in the .env!&lt;/p&gt; &#xA;&lt;p&gt;!!!! For much improved performance provide custom few prompt shots in the generate thoughts and generate states! !!!!!&lt;/p&gt; &#xA;&lt;p&gt;And in the &lt;code&gt;examples&lt;/code&gt; folder we have other examples for huggingface transformers + hugginggface pipelines&lt;/p&gt; &#xA;&lt;h2&gt;Method2&lt;/h2&gt; &#xA;&lt;p&gt;or:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install tree-of-thoughts &lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Create a Python script (e.g., example.py) and import the necessary classes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;from tree_of_thoughts import OptimizedOpenAILanguageModel&#xA;from tree_of_thoughts import TreeofThoughts&#xA;&#xA;&#xA;api_key = os.getenv(&#34;OPENAI_API_KEY&#34;)&#xA;model = OptimizedOpenAILanguageModel(api_key=api_key) # api_model=&#34;gpt4&#34; # for higher performance base model is not smart&#xA;&#xA;&#xA;#choose search algorithm(&#39;BFS&#39; or &#39;DFS&#39;)&#xA;search_algorithm = &#34;BFS&#34;&#xA;&#xA;#cot or propose&#xA;strategy=&#34;cot&#34;&#xA;&#xA;# value or vote&#xA;evaluation_strategy = &#34;value&#34;&#xA;&#xA;#initialize the class&#xA;tree_of_thoughts = TreeofThoughts(model, search_algorithm)&#xA;&#xA;#enter an problem if you want!&#xA;input_problem = &#34;use 4 numbers and basic arithmetic operations (+-*/) to obtain 24&#34; #note for superior intelligent responses you&#39;ll have to be more explicit in your prompt and select a better model&#xA;    &#xA;&#xA;num_thoughts = 2&#xA;max_steps= 3&#xA;max_states = 5&#xA;value_threshold= 0.5&#xA;&#xA;#call the solve emthod with the input problem and other params&#xA;&#xA;solution = tree_of_thoughts.solve(input_problem, &#xA;    num_thoughts=num_thoughts,&#xA;    max_steps=max_states,&#xA;    max_states=5,&#xA;    value_threshold=value_threshold,&#xA;    )&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or Integrate your own custom language model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;class CustomLanguageModel(AbstractLanguageModel):&#xA;    def __init__(self, model):&#xA;        self.model = model&#xA;&#xA;    def generate_thoughts(self, state, k):&#xA;        #implement the thought generation logic using self.model&#xA;        pass&#xA;&#xA;    def evaluate_states(self, states):&#xA;        #implement state evaluation logic using self.model&#xA;        pass&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the example script&lt;/p&gt; &#xA;&lt;h2&gt;🌟 Features:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;General problem-solving framework for language models&lt;/li&gt; &#xA; &lt;li&gt;Supports both breadth-first search (BFS) and depth-first search (DFS) algorithms&lt;/li&gt; &#xA; &lt;li&gt;Easy integration with popular language models like OpenAI and Hugging Face&lt;/li&gt; &#xA; &lt;li&gt;Extensible and adaptable to different problem properties and resource constraints&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Algorithmic Pseudocode&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Define the thought decomposition based on the problem properties.&lt;/li&gt; &#xA; &lt;li&gt;Create a thought generator function G(pθ, s, k) with two strategies: a. Sample i.i.d. thoughts from a CoT prompt. b. Propose thoughts sequentially using a &#34;propose prompt&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Create a state evaluator function V(pθ, S) with two strategies: a. Value each state independently. b. Vote across states.&lt;/li&gt; &#xA; &lt;li&gt;Choose a search algorithm (BFS or DFS) based on the tree structure.&lt;/li&gt; &#xA; &lt;li&gt;Implement the chosen search algorithm.&lt;/li&gt; &#xA; &lt;li&gt;Execute the chosen search algorithm with the input problem, thought generator, state evaluator, and other required parameters.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Tree of Thoughts Class&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class TreeofThoughts:&#xA;    &#xA;    def __init__(self, model, search_algorithm):&#xA;        self.model = model&#xA;        self.search_algorithm = search_algorithm&#xA;&#xA;    def solve(self, x, k, T, b, vth):&#xA;        if self.search_algorithm == &#39;BFS&#39;:&#xA;            return self.tot_bfs(x, k, T, b)&#xA;        elif self.search_algorithm == &#39;DFS&#39;:&#xA;            return self.tot_dfs(x, k, T, vth)&#xA;        else:&#xA;            raise ValueError(&#34;Invalid search algorithm. Choose &#39;BFS&#39; or &#39;DFS&#39;.&#34;)&#xA;&#xA;    def tot_bfs(self, x, k, T, b):&#xA;        S0 = {x}&#xA;        for t in range(1, T + 1):&#xA;            S0_t = {(*s, z) for s in S0 for z in self.model.generate_thoughts(s, k)}&#xA;            Vt = self.model.evaluate_states(S0_t)&#xA;            St = sorted(S0_t, key=lambda s: Vt[s], reverse=True)[:b]&#xA;            S0 = set(St)&#xA;        return self.model.generate_thoughts(max(St, key=lambda s: Vt[s]), 1)&#xA;&#xA;    def tot_dfs(self, x, k, T, vth):&#xA;        output = []&#xA;&#xA;        def dfs(s, t):&#xA;            if t &amp;gt; T:&#xA;                output.append(self.model.generate_thoughts(s, 1))&#xA;                return&#xA;            for s_prime in sorted(self.model.generate_thoughts(s, k)):&#xA;                if self.model.evaluate_states({s_prime})[s_prime] &amp;gt; vth:&#xA;                    dfs((*s, s_prime), t + 1)&#xA;&#xA;        dfs(x, 1)&#xA;        return output&#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage Examples&lt;/h2&gt; &#xA;&lt;h3&gt;OpenAI API&lt;/h3&gt; &#xA;&lt;p&gt;To use Tree of Thoughts with OpenAI&#39;s API, create a custom model class that inherits from &lt;code&gt;AbstractLanguageModel&lt;/code&gt; and implements the required methods using OpenAI&#39;s API. Then, create an instance of the &lt;code&gt;TreeOfThoughts&lt;/code&gt; class with the custom model and the desired search algorithm (&#39;BFS&#39; or &#39;DFS&#39;).&lt;/p&gt; &#xA;&lt;h3&gt;Hugging Face Transformers&lt;/h3&gt; &#xA;&lt;p&gt;To run huggingface transformers with Tree of Thoughts&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/kyegomez/tree-of-thoughts&#xA;cd tree-of-thoughts&#xA;python3 huggingfaceExample.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tree_of_thoughts import HuggingLanguageModel&#xA;&#xA;model_name=&#34;gpt2&#34;&#xA;model_tokenizer=&#34;your tokenizer&#34;&#xA;&#xA;huggingface_model = HuggingLanguageModel(model_name, model_tokenizer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class HuggingLanguageModel(AbstractLanguageModel):&#xA;    def __init__(self, model_name):&#xA;        self.model = AutoModelForCausalLM.from_pretrained(model_name)&#xA;        self.tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;&#xA;    def generate_thoughts(self, state, k):&#xA;        state_text = &#39; &#39;.join(state)&#xA;        prompt = f&#34;Given the current state of reasoning: &#39;{state_text}&#39;, generate {k} coherent thoughts to achieve the reasoning process:&#34;&#xA;&#xA;        inputs = self.tokenizer(prompt, return_tensors=&#34;pt&#34;)&#xA;        outputs = self.model.generate(**inputs, num_return_sequences=k)&#xA;        thoughts = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]&#xA;&#xA;        return thoughts&#xA;&#xA;    def evaluate_states(self, states, initial_prompt):&#xA;        state_values = {}&#xA;        for state in states:&#xA;            state_text = &#39; &#39;.join(state)&#xA;            prompt = f&#34;Given the current state of reasoning: &#39;{state_text}&#39;, pessimitically evaluate its value as a float between 0 and 1 based on it&#39;s potential to achieve {initial_prompt}&#34;&#xA;&#xA;            inputs = self.tokenizer(prompt, return_tensors=&#34;pt&#34;)&#xA;            outputs = self.model.generate(**inputs, num_return_sequences=1)&#xA;            value_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)&#xA;&#xA;            try:&#xA;                value = float(value_text)&#xA;            except ValueError:&#xA;                value = 0  # Assign a default value if the conversion fails&#xA;&#xA;            state_values[state] = value&#xA;&#xA;        return state_values&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;This algorithm is still infant yet it&#39;s potential remains unimaginable, let&#39;s advance the reasoning of AI&#39;s together under this banner.&lt;/p&gt; &#xA;&lt;h1&gt;Share With Your Network&lt;/h1&gt; &#xA;&lt;p&gt;You can easily share this repository by clicking on the following buttons:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/intent/tweet?text=Check%20out%20this%20amazing%20project%20on%20improving%20AI%20reasoning%20-%20Tree%20of%20Thoughts!%20https://github.com/kyegomez/tree-of-thoughts&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url?style=social&amp;amp;url=https%3A%2F%2Fgithub.com%2Fkyegomez%2Ftree-of-thoughts&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fgithub.com%2Fkyegomez%2Ftree-of-thoughts&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Share-LinkedIn-blue?style=social&amp;amp;logo=linkedin&#34; alt=&#34;LinkedIn&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For Instagram, while it doesn&#39;t directly support sharing of web links, you can share the screenshot of our project and the link in your caption or bio. You can download the project screenshot by clicking the image below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/kyegomez/tree-of-thoughts/raw/main/tree-of-thoughts.jpeg&#34;&gt;&lt;img src=&#34;https://github.com/kyegomez/tree-of-thoughts/raw/main/tree-of-thoughts.jpeg&#34; alt=&#34;Tree of Thoughts&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We greatly appreciate any help in spreading the word about our project. Thank you for your support!&lt;/p&gt; &#xA;&lt;h1&gt;Roadmap:&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Resilient Prompting: Teach model how to think rather than what to think.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add pruning treshold management for precise bad state cutoff&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Evaluating each thought as soon as thought generated then evaluating an chain of thoughts or the state of thoughts by averaging out the values of each thought evaluation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add Traversal method, which &#34;will incapsulate the run of either dfs or bfs under the hood so that the issue of different args is solved from @ivanzhovannik&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add Delay between generate solutions and generate values&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Dynamic and adaptive parameters, like max steps, num_thoughts, max_states and value threshold that shift depending on the complexity of the user objective.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add Rejected reasoning metadata (thought, state, reasoning_on_state) into generate solutions&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;any other ideas? Please pr request this algorithm is very infant and it&#39;s potential is limitless&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Chain of Thought Hub Evaluation tests!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Documentation:&lt;/h1&gt; &#xA;&lt;h2&gt;input_problem (str):&lt;/h2&gt; &#xA;&lt;p&gt;The initial problem statement or prompt for which the Tree of Thoughts algorithm will generate a solution.&lt;/p&gt; &#xA;&lt;h2&gt;num_thoughts (int, default=5):&lt;/h2&gt; &#xA;&lt;p&gt;The number of thoughts to generate at each state. A higher value of k will result in more thoughts being generated, potentially leading to a more diverse set of solutions. However, increasing k may also increase the computational complexity and time required to find a solution.&lt;/p&gt; &#xA;&lt;h2&gt;max_steps (int, default=3):&lt;/h2&gt; &#xA;&lt;p&gt;The maximum depth of the search tree. A higher value of T allows the algorithm to explore deeper states, potentially leading to better solutions. However, increasing T may also increase the computational complexity and time required to find a solution.&lt;/p&gt; &#xA;&lt;h2&gt;max_states (int, default=5):&lt;/h2&gt; &#xA;&lt;p&gt;The branching factor of the search tree, which determines the maximum number of child nodes for each parent node. A higher value of b allows the algorithm to explore more states, potentially leading to better solutions. However, increasing b may also increase the computational complexity and time required to find a solution.&lt;/p&gt; &#xA;&lt;h2&gt;value_threshold (float, default=0.5):&lt;/h2&gt; &#xA;&lt;p&gt;The value threshold for pruning states. States with a value below this threshold will be discarded, reducing the search space. A higher value of vth will result in a more aggressive pruning strategy, potentially speeding up the search process. However, setting vth too high may cause the algorithm to discard promising states, leading to suboptimal solutions.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Thanks to: Shunyu Yao Princeton University, Dian Yu Google DeepMind, Jeffrey Zhao, Google DeepMind, Izhak Shafran Google DeepMind, Thomas L. Griffiths, Princeton University, Yuan Cao Google DeepMind, Karthik Narasimha, Princeton University for sharing this amazing work with the world!&lt;/p&gt; &#xA;&lt;p&gt;And, thanks to Phil Wang or Lucidrains for inspiring me to devote myself to open source AI Research&lt;/p&gt;</summary>
  </entry>
</feed>