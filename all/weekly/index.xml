<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-16T01:40:20Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xcad2k/cheat-sheets</title>
    <updated>2022-10-16T01:40:20Z</updated>
    <id>tag:github.com,2022-10-16:/xcad2k/cheat-sheets</id>
    <link href="https://github.com/xcad2k/cheat-sheets" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is my personal knowledge-base. Here you&#39;ll find code-snippets, technical documentation, and command reference for various tools, and technologies.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Christian&#39;s &#34;Cheat-Sheets&#34;&lt;/h1&gt; &#xA;&lt;p&gt;Hi, there! üëã&lt;/p&gt; &#xA;&lt;p&gt;I‚Äôm Christian, a 35 year-old tech enthusiast from Germany, and I love to inspire and educate people in IT.&lt;/p&gt; &#xA;&lt;p&gt;This Repository &lt;strong&gt;Cheat-Sheets&lt;/strong&gt; is my personal knowledge-base. Here you&#39;ll find code-snippets, technical documentation, and command reference for various tools, and technologies.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; Be aware, products can change over time. I do my best to keep up with the latest changes and releases, but please understand that this won‚Äôt always be the case.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;I created them as free resources to be used in your specific use cases. If you&#39;re searching for detailed, in-depth tutorials on some tools or technologies, check out my YouTube Channel: &lt;a href=&#34;https://www.youtube.com/channel/UCZNhwA1B5YqiY1nLzmM0ZRg&#34;&gt;The Digital Life&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;If you‚Äôd like to contribute to this project, reach out to me on social media or &lt;a href=&#34;https://discord.gg/bz2SN7d&#34;&gt;Discord&lt;/a&gt;, or create a pull request for the necessary changes.&lt;/p&gt; &#xA;&lt;h2&gt;Other Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xcad2k/videos&#34;&gt;Videos&lt;/a&gt; - Documentation and project files for all my video tutorials on YouTube&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xcad2k/dotfiles&#34;&gt;Dotfiles&lt;/a&gt; - My personal configuration files on Linux and Windows&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xcad2k/boilerplates&#34;&gt;Boilerplates&lt;/a&gt; - Templates for various projects like Docker, K8S, Ansible, etc&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xcad2k/cheat-sheets&#34;&gt;Cheat-Sheets&lt;/a&gt; - Command Reference for various tools and technologies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Support me&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.patreon.com/christianlempa&#34;&gt;Becoming a Patreon&lt;/a&gt; will support my mission to create free, high-quality content for tech enthusiasts and IT professionals.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Help me to create something that matters to people!&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>wagtail/wagtail</title>
    <updated>2022-10-16T01:40:20Z</updated>
    <id>tag:github.com,2022-10-16:/wagtail/wagtail</id>
    <link href="https://github.com/wagtail/wagtail" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Django content management system focused on flexibility and user experience&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;img width=&#34;343&#34; src=&#34;https://raw.githubusercontent.com/wagtail/wagtail/main/.github/wagtail.svg#gh-light-mode-only&#34; alt=&#34;Wagtail&#34;&gt; &lt;img width=&#34;343&#34; src=&#34;https://raw.githubusercontent.com/wagtail/wagtail/main/.github/wagtail-inverse.svg#gh-dark-mode-only&#34; alt=&#34;Wagtail&#34;&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/wagtail/wagtail/actions&#34;&gt; &lt;img src=&#34;https://github.com/wagtail/wagtail/workflows/Wagtail%20CI/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/BSD-3-Clause&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/license-BSD-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/wagtail/&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/wagtail.svg?sanitize=true&#34; alt=&#34;Version&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://lgtm.com/projects/g/wagtail/wagtail/alerts/&#34;&gt; &lt;img src=&#34;https://img.shields.io/lgtm/alerts/g/wagtail/wagtail.svg?logo=lgtm&amp;amp;logoWidth=18&#34; alt=&#34;Total alerts&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://lgtm.com/projects/g/wagtail/wagtail/context:python&#34;&gt; &lt;img src=&#34;https://img.shields.io/lgtm/grade/python/g/wagtail/wagtail.svg?logo=lgtm&amp;amp;logoWidth=18&#34; alt=&#34;Language grade: Python&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://lgtm.com/projects/g/wagtail/wagtail/context:javascript&#34;&gt; &lt;img src=&#34;https://img.shields.io/lgtm/grade/javascript/g/wagtail/wagtail.svg?logo=lgtm&amp;amp;logoWidth=18&#34; alt=&#34;Language grade: JavaScript&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/wagtail/&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/dm/wagtail?logo=Downloads&#34; alt=&#34;Monthly downloads&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://twitter.com/WagtailCMS&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/WagtailCMS?style=social&amp;amp;logo=twitter&#34; alt=&#34;follow on Twitter&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Wagtail is an open source content management system built on Django, with a strong community and commercial support. It&#39;s focused on user experience, and offers precise control for designers and developers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/wagtail-screenshot-with-browser.png&#34; alt=&#34;Wagtail screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;üî• Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A fast, attractive interface for authors&lt;/li&gt; &#xA; &lt;li&gt;Complete control over front-end design and structure&lt;/li&gt; &#xA; &lt;li&gt;Scales to millions of pages and thousands of editors&lt;/li&gt; &#xA; &lt;li&gt;Fast out of the box, cache-friendly when you need it&lt;/li&gt; &#xA; &lt;li&gt;Content API for &#39;headless&#39; sites with de-coupled front-end&lt;/li&gt; &#xA; &lt;li&gt;Runs on a Raspberry Pi or a multi-datacenter cloud platform&lt;/li&gt; &#xA; &lt;li&gt;StreamField encourages flexible content without compromising structure&lt;/li&gt; &#xA; &lt;li&gt;Powerful, integrated search, using Elasticsearch or PostgreSQL&lt;/li&gt; &#xA; &lt;li&gt;Excellent support for images and embedded content&lt;/li&gt; &#xA; &lt;li&gt;Multi-site and multi-language ready&lt;/li&gt; &#xA; &lt;li&gt;Embraces and extends Django&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Find out more at &lt;a href=&#34;https://wagtail.org/&#34;&gt;wagtail.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;üëâ Getting started&lt;/h3&gt; &#xA;&lt;p&gt;Wagtail works with &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python 3&lt;/a&gt;, on any platform.&lt;/p&gt; &#xA;&lt;p&gt;To get started with using Wagtail, run the following in a virtual environment:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wagtail/wagtail/main/.github/install-animation.gif&#34; alt=&#34;Installing Wagtail&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install wagtail&#xA;wagtail start mysite&#xA;cd mysite&#xA;pip install -r requirements.txt&#xA;python manage.py migrate&#xA;python manage.py createsuperuser&#xA;python manage.py runserver&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For detailed installation and setup docs, see &lt;a href=&#34;https://docs.wagtail.org/en/stable/getting_started/tutorial.html&#34;&gt;the getting started tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Who‚Äôs using it?&lt;/h3&gt; &#xA;&lt;p&gt;Wagtail is used by &lt;a href=&#34;https://www.nasa.gov/&#34;&gt;NASA&lt;/a&gt;, &lt;a href=&#34;https://www.google.com/&#34;&gt;Google&lt;/a&gt;, &lt;a href=&#34;https://www.oxfam.org/en&#34;&gt;Oxfam&lt;/a&gt;, the &lt;a href=&#34;https://www.nhs.uk/&#34;&gt;NHS&lt;/a&gt;, &lt;a href=&#34;https://www.mozilla.org/en-US/&#34;&gt;Mozilla&lt;/a&gt;, &lt;a href=&#34;https://www.mit.edu/&#34;&gt;MIT&lt;/a&gt;, the &lt;a href=&#34;https://www.icrc.org/en&#34;&gt;Red Cross&lt;/a&gt;, &lt;a href=&#34;https://www.salesforce.com/&#34;&gt;Salesforce&lt;/a&gt;, &lt;a href=&#34;https://www.nbc.com/&#34;&gt;NBC&lt;/a&gt;, &lt;a href=&#34;https://www.bmw.com/en/index.html&#34;&gt;BMW&lt;/a&gt;, and the US and UK governments. Add your own Wagtail site to &lt;a href=&#34;https://madewithwagtail.org&#34;&gt;madewithwagtail.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;üìñ Documentation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.wagtail.org/&#34;&gt;docs.wagtail.org&lt;/a&gt; is the full reference for Wagtail, and includes guides for developers, designers and editors, alongside release notes and our roadmap.&lt;/p&gt; &#xA;&lt;p&gt;For those who are &lt;strong&gt;new to Wagtail&lt;/strong&gt;, the &lt;a href=&#34;https://docs.wagtail.org/en/stable/getting_started/the_zen_of_wagtail.html&#34;&gt;Zen of Wagtail&lt;/a&gt; will help you understand what Wagtail is, and what Wagtail is &lt;em&gt;not&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;For developers&lt;/strong&gt; who are ready to jump in to their first Wagtail website the &lt;a href=&#34;https://docs.wagtail.org/en/stable/getting_started/tutorial.html&#34;&gt;Getting Started Tutorial&lt;/a&gt; will guide you through creating and editing your first page.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Do you have an existing Django project?&lt;/strong&gt; The &lt;a href=&#34;https://docs.wagtail.org/en/stable/getting_started/integrating_into_django.html&#34;&gt;Wagtail Integration documentation&lt;/a&gt; is the best place to start.&lt;/p&gt; &#xA;&lt;h3&gt;üìå Compatibility&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;(If you are reading this on GitHub, the details here may not be indicative of the current released version - please see &lt;a href=&#34;https://docs.wagtail.org/en/stable/releases/upgrading.html#compatible-django-python-versions&#34;&gt;Compatible Django / Python versions&lt;/a&gt; in the Wagtail documentation.)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Wagtail supports:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Django 3.2.x, 4.0.x and 4.1.x&lt;/li&gt; &#xA; &lt;li&gt;Python 3.7, 3.8, 3.9 and 3.10&lt;/li&gt; &#xA; &lt;li&gt;PostgreSQL, MySQL and SQLite (with JSON1) as database backends&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.wagtail.org/en/stable/releases/upgrading.html#compatible-django-python-versions&#34;&gt;Previous versions of Wagtail&lt;/a&gt; additionally supported Python 2.7 and earlier Django versions.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üì¢ Community Support&lt;/h3&gt; &#xA;&lt;p&gt;There is an active community of Wagtail users and developers responding to questions on &lt;a href=&#34;https://stackoverflow.com/questions/tagged/wagtail&#34;&gt;Stack Overflow&lt;/a&gt;. When posting questions, please read Stack Overflow&#39;s advice on &lt;a href=&#34;https://stackoverflow.com/help/how-to-ask&#34;&gt;how to ask questions&lt;/a&gt; and remember to tag your question &#34;wagtail&#34;.&lt;/p&gt; &#xA;&lt;p&gt;For topics and discussions that do not fit Stack Overflow&#39;s question and answer format we have a &lt;a href=&#34;https://github.com/wagtail/wagtail/wiki/Slack&#34;&gt;Slack workspace&lt;/a&gt;. Please respect the time and effort of volunteers by not asking the same question in multiple places.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wagtail/wagtail/wiki/Slack&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wagtail/wagtail/main/.github/join-slack-community.png&#34; alt=&#34;Join slack community&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our &lt;a href=&#34;https://github.com/wagtail/wagtail/discussions&#34;&gt;Github discussion boards&lt;/a&gt; are open for sharing ideas and plans for the Wagtail project.&lt;/p&gt; &#xA;&lt;p&gt;We maintain a curated list of third party packages, articles and other resources at &lt;a href=&#34;https://github.com/springload/awesome-wagtail&#34;&gt;Awesome Wagtail&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;üßë‚Äçüíº Commercial Support&lt;/h3&gt; &#xA;&lt;p&gt;Wagtail is sponsored by &lt;a href=&#34;https://torchbox.com/&#34;&gt;Torchbox&lt;/a&gt;. If you need help implementing or hosting Wagtail, please contact us: &lt;a href=&#34;mailto:hello@torchbox.com&#34;&gt;hello@torchbox.com&lt;/a&gt;. See also &lt;a href=&#34;https://madewithwagtail.org/developers/&#34;&gt;madewithwagtail.org/developers/&lt;/a&gt; for expert Wagtail developers around the world.&lt;/p&gt; &#xA;&lt;h3&gt;üîê Security&lt;/h3&gt; &#xA;&lt;p&gt;We take the security of Wagtail, and related packages we maintain, seriously. If you have found a security issue with any of our projects please email us at &lt;a href=&#34;mailto:security@wagtail.org&#34;&gt;security@wagtail.org&lt;/a&gt; so we can work together to find and patch the issue. We appreciate responsible disclosure with any security related issues, so please contact us first before creating a Github issue.&lt;/p&gt; &#xA;&lt;p&gt;If you want to send an encrypted email (optional), the public key ID for &lt;a href=&#34;mailto:security@wagtail.org&#34;&gt;security@wagtail.org&lt;/a&gt; is 0xbed227b4daf93ff9, and this public key is available from most commonly-used keyservers.&lt;/p&gt; &#xA;&lt;h3&gt;üïí Release schedule&lt;/h3&gt; &#xA;&lt;p&gt;Feature releases of Wagtail are released every three months. Selected releases are designated as Long Term Support (LTS) releases, and will receive maintenance updates for an extended period to address any security and data-loss related issues. For dates of past and upcoming releases and support periods, see &lt;a href=&#34;https://github.com/wagtail/wagtail/wiki/Release-schedule&#34;&gt;Release Schedule&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;üïõ Nightly releases&lt;/h4&gt; &#xA;&lt;p&gt;To try out the latest features before a release, we also create builds from &lt;code&gt;main&lt;/code&gt; every night. You can find instructions on how to install the latest nightly release at &lt;a href=&#34;https://releases.wagtail.org/nightly/index.html&#34;&gt;https://releases.wagtail.org/nightly/index.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;üôãüèΩ Contributing&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re a Python or Django developer, fork the repo and get stuck in! We have several developer focused channels on the &lt;a href=&#34;https://github.com/wagtail/wagtail/wiki/Slack&#34;&gt;Slack workspace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You might like to start by reviewing the &lt;a href=&#34;https://docs.wagtail.org/en/latest/contributing/index.html&#34;&gt;contributing guidelines&lt;/a&gt; and checking issues with the &lt;a href=&#34;https://github.com/wagtail/wagtail/labels/good%20first%20issue&#34;&gt;good first issue&lt;/a&gt; label.&lt;/p&gt; &#xA;&lt;p&gt;We also welcome translations for Wagtail&#39;s interface. Translation work should be submitted through &lt;a href=&#34;https://explore.transifex.com/torchbox/wagtail/&#34;&gt;Transifex&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;üîì License&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wagtail/wagtail/raw/main/LICENSE&#34;&gt;BSD&lt;/a&gt; - Free to use and modify for any purpose, including both open and closed-source code.&lt;/p&gt; &#xA;&lt;h3&gt;üëè Thanks&lt;/h3&gt; &#xA;&lt;p&gt;We thank the following organisations for their services used in Wagtail&#39;s development:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.browserstack.com/&#34;&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/browserstack-logo.svg?sanitize=true&#34; alt=&#34;Browserstack&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.browserstack.com/&#34;&gt;BrowserStack&lt;/a&gt; provides the project with free access to their live web-based browser testing tool, and automated Selenium cloud testing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.squash.io/&#34;&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/squash-logo.svg?sanitize=true&#34; alt=&#34;squash.io&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.squash.io/&#34;&gt;Squash&lt;/a&gt; provides the project with free test environments for reviewing pull requests.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://assistivlabs.com/&#34;&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/assistivlabs-logo.png&#34; alt=&#34;Assistiv Labs&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://assistivlabs.com/&#34;&gt;Assistiv Labs&lt;/a&gt; provides the project with unlimited access to their remote testing with assistive technologies.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>williamyang1991/VToonify</title>
    <updated>2022-10-16T01:40:20Z</updated>
    <id>tag:github.com,2022-10-16:/williamyang1991/VToonify</id>
    <link href="https://github.com/williamyang1991/VToonify" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[SIGGRAPH Asia 2022] VToonify: Controllable High-Resolution Portrait Video Style Transfer&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VToonify - Official PyTorch Implementation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/18130694/189483939-0fc4a358-fb34-43cc-811a-b22adb820d57.mp4&#34;&gt;https://user-images.githubusercontent.com/18130694/189483939-0fc4a358-fb34-43cc-811a-b22adb820d57.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository provides the official PyTorch implementation for the following paper:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VToonify: Controllable High-Resolution Portrait Video Style Transfer&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://williamyang1991.github.io/&#34;&gt;Shuai Yang&lt;/a&gt;, &lt;a href=&#34;https://liming-jiang.com/&#34;&gt;Liming Jiang&lt;/a&gt;, &lt;a href=&#34;https://liuziwei7.github.io/&#34;&gt;Ziwei Liu&lt;/a&gt; and &lt;a href=&#34;https://www.mmlab-ntu.com/person/ccloy/&#34;&gt;Chen Change Loy&lt;/a&gt;&lt;br&gt; In ACM TOG (Proceedings of SIGGRAPH Asia), 2022.&lt;br&gt; &lt;a href=&#34;https://www.mmlab-ntu.com/project/vtoonify/&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2209.11224&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/0_OmVhDgYuY&#34;&gt;&lt;strong&gt;Supplementary Video&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/file/d/1A2gC2PW1ZmU6VWQRvMN98njqRxfLjqbk/view?usp=sharing&#34;&gt;&lt;strong&gt;Input Data and Video Results&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://colab.research.google.com/github/williamyang1991/VToonify/blob/master/notebooks/inference_playground.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;google colab logo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/PKUWilliamYang/VToonify&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://visitor-badge.laobi.icu/badge?page_id=williamyang1991/VToonify&#34; alt=&#34;visitors&#34;&gt;&lt;/p&gt; &#xA;&lt;!--![visitors](https://visitor-badge.glitch.me/badge?page_id=williamyang1991/VToonify)--&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; &lt;em&gt;Generating high-quality artistic portrait videos is an important and desirable task in computer graphics and vision. Although a series of successful portrait image toonification models built upon the powerful StyleGAN have been proposed, these image-oriented methods have obvious limitations when applied to videos, such as the fixed frame size, the requirement of face alignment, missing non-facial details and temporal inconsistency. In this work, we investigate the challenging controllable high-resolution portrait video style transfer by introducing a novel &lt;strong&gt;VToonify&lt;/strong&gt; framework. Specifically, VToonify leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details. The resulting fully convolutional architecture accepts non-aligned faces in videos of variable size as input, contributing to complete face regions with natural motions in the output. Our framework is compatible with existing StyleGAN-based image toonification models to extend them to video toonification, and inherits appealing features of these models for flexible style control on color and intensity. This work presents two instantiations of VToonify built upon Toonify and DualStyleGAN for collection-based and exemplar-based portrait video style transfer, respectively. Extensive experimental results demonstrate the effectiveness of our proposed VToonify framework over existing methods in generating high-quality and temporally-coherent artistic portrait videos with flexible style controls.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;br&gt; &lt;strong&gt;High-Resolution Video&lt;/strong&gt; (&amp;gt;1024, support unaligned faces) | &lt;strong&gt;Data-Friendly&lt;/strong&gt; (no real training data) | &lt;strong&gt;Style Control&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18130694/189509940-91c5e1e2-83a8-491e-962e-64775e56d7f6.jpg&#34; alt=&#34;overview&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[10/2022] Integrate &lt;a href=&#34;https://gradio.app/&#34;&gt;Gradio&lt;/a&gt; interface into &lt;a href=&#34;http://colab.research.google.com/github/williamyang1991/VToonify/blob/master/notebooks/inference_playground.ipynb&#34;&gt;Colab notebook&lt;/a&gt;. Enjoy the web demo!&lt;/li&gt; &#xA; &lt;li&gt;[10/2022] Integrated to ü§ó &lt;a href=&#34;https://huggingface.co/spaces/PKUWilliamYang/VToonify&#34;&gt;Hugging Face&lt;/a&gt;. Enjoy the web demo!&lt;/li&gt; &#xA; &lt;li&gt;[09/2022] Input videos and video results are released.&lt;/li&gt; &#xA; &lt;li&gt;[09/2022] Paper is released.&lt;/li&gt; &#xA; &lt;li&gt;[09/2022] Code is released.&lt;/li&gt; &#xA; &lt;li&gt;[09/2022] This website is created.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Web Demo&lt;/h2&gt; &#xA;&lt;p&gt;Integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces ü§ó&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo &lt;a href=&#34;https://huggingface.co/spaces/PKUWilliamYang/VToonify&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Clone this repo:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/williamyang1991/VToonify.git&#xA;cd VToonify&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dependencies:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We have tested on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA 10.1&lt;/li&gt; &#xA; &lt;li&gt;PyTorch 1.7.0&lt;/li&gt; &#xA; &lt;li&gt;Pillow 8.3.1; Matplotlib 3.3.4; opencv-python 4.5.3; Faiss 1.7.1; tqdm 4.61.2; Ninja 1.10.2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All dependencies for defining the environment are provided in &lt;code&gt;environment/vtoonify_env.yaml&lt;/code&gt;. We recommend running this repository using &lt;a href=&#34;https://docs.anaconda.com/anaconda/install/&#34;&gt;Anaconda&lt;/a&gt; (you may need to modify &lt;code&gt;vtoonify_env.yaml&lt;/code&gt; to install PyTorch that matches your own CUDA version following &lt;a href=&#34;https://pytorch.org/&#34;&gt;https://pytorch.org/&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f ./environment/vtoonify_env.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have a problem regarding the cpp extention (fused and upfirdn2d), or no GPU is available, you may refer to &lt;a href=&#34;https://raw.githubusercontent.com/williamyang1991/VToonify/main/model/stylegan/op_cpu#readme&#34;&gt;CPU compatible version&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;(1) Inference for Image/Video Toonification&lt;/h2&gt; &#xA;&lt;h3&gt;Inference Notebook&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://colab.research.google.com/github/williamyang1991/VToonify/blob/master/notebooks/inference_playground.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; height=&#34;22.5&#34;&gt;&lt;/a&gt;&lt;br&gt; To help users get started, we provide a Jupyter notebook found in &lt;code&gt;./notebooks/inference_playground.ipynb&lt;/code&gt; that allows one to visualize the performance of VToonify. The notebook will download the necessary pretrained models and run inference on the images found in &lt;code&gt;./data/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-trained Models&lt;/h3&gt; &#xA;&lt;p&gt;Pre-trained models can be downloaded from &lt;a href=&#34;https://drive.google.com/drive/folders/1Nmbz9zBM78I1nRVokhHLuBKHleDmjDxv?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/1Io3PKNV1wD7ttxaVz-MPww?pwd=sigg&#34;&gt;Baidu Cloud&lt;/a&gt; (access code: sigg) or &lt;a href=&#34;https://huggingface.co/PKUWilliamYang/VToonify/tree/main/models&#34;&gt;Hugging Face&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Backbone&lt;/th&gt;&#xA;   &lt;th&gt;Model&lt;/th&gt;&#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;6&#34;&gt;DualStyleGAN&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1DuZfXt6b_xhTAQSN0D8m7N1np0Web0Ky&#34;&gt;cartoon&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained VToonify-D models and 317 cartoon style codes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/12TzTQqwBedsYX3kE_420mdTbWl9lwv4Y&#34;&gt;caricature&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained VToonify-D models and 199 caricature style codes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1MpEqS26Q1ngTPeex_4MN9qOJxfXKH-k-&#34;&gt;arcane&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained VToonify-D models and 100 arcane style codes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/15mxb7DxTzEBrKtx5aJ_I5WGDjSWBmcUi&#34;&gt;comic&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained VToonify-D models and 101 comic style codes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1Hld7OeZqYBrg6r35IA_x4sNtt1abHUMU&#34;&gt;pixar&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained VToonify-D models and 122 pixar style codes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1LQGNMDEHM70nOhm3-xY228YpJNlPnf_s&#34;&gt;illustration&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained VToonify-D models and 156 illustration style codes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;5&#34;&gt;Toonify&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1FFtTVgiDKZ_InnwUJLDuA1wfghZp41nX&#34;&gt;cartoon&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained VToonify-T model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1ReRxttV-macgV3epC61qg4TQ3FGAhGqG&#34;&gt;caricature&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained VToonify-T model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1OXU95BOCCT0f6pGbwQ4yQ1EHb2LPd2yb&#34;&gt;arcane&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained VToonify-T model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1KvawsOXzKgwDM3Z27sagO_KGE_Kc5GZS&#34;&gt;comic&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained VToonify-T model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/19N4ddcTXhXbTEayTbrFc533EktbhOXMz&#34;&gt;pixar&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained VToonify-T model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Supporting model&lt;/th&gt;&#xA;   &lt;th&gt; &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1NgI4mPkboYvYw3MWcdUaQhkr0OWgs9ej/view?usp=sharing&#34;&gt;encoder.pt&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;Pixel2style2pixel encoder to map real faces into Z+ space of StyleGAN&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1jY0mTjVB8njDh6e0LP_2UxuRK3MnjoIR/view&#34;&gt;faceparsing.pth&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;BiSeNet for face parsing from &lt;a href=&#34;https://github.com/zllrunning/face-parsing.PyTorch&#34;&gt;face-parsing.PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;The downloaded models are suggested to be arranged in &lt;a href=&#34;https://raw.githubusercontent.com/williamyang1991/VToonify/main/checkpoint/&#34;&gt;this folder structure&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The VToonify-D models are named with suffixes to indicate the settings, where&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;_sXXX&lt;/code&gt;: supports only one fixed style with &lt;code&gt;XXX&lt;/code&gt; the index of this style. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;_s&lt;/code&gt; without &lt;code&gt;XXX&lt;/code&gt; means the model supports examplar-based style transfer&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;_dXXX&lt;/code&gt;: supports only a fixed style degree of &lt;code&gt;XXX&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;_d&lt;/code&gt; without &lt;code&gt;XXX&lt;/code&gt; means the model supports style degrees ranging from 0 to 1&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;_c&lt;/code&gt;: supports color transfer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Style Transfer with VToonify-D&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;‚úî A quick start &lt;a href=&#34;https://raw.githubusercontent.com/williamyang1991/VToonify/main/output#readme&#34;&gt;HERE&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Transfer a default cartoon style onto a default face image &lt;code&gt;./data/077436.jpg&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python style_transfer.py --scale_image&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results are saved in the folder &lt;code&gt;./output/&lt;/code&gt;, where &lt;code&gt;077436_input.jpg&lt;/code&gt; is the rescaled input image to fit VToonify (this image can serve as the input without &lt;code&gt;--scale_image&lt;/code&gt;) and &lt;code&gt;077436_vtoonify_d.jpg&lt;/code&gt; is the result.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18130694/189530937-eb468f96-ac02-4f33-8621-03cb93d17e73.jpg&#34; alt=&#34;077436_overview&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Specify the content image and the model, control the style with the following options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--content&lt;/code&gt;: path to the target face image or video&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--style_id&lt;/code&gt;: the index of the style image (find the mapping between index and the style image &lt;a href=&#34;https://github.com/williamyang1991/DualStyleGAN/tree/main/doc_images&#34;&gt;here&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--style_degree&lt;/code&gt; (default: 0.5): adjust the degree of style.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--color_transfer&lt;/code&gt;(default: False): perform color transfer if loading a VToonify-Dsdc model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--ckpt&lt;/code&gt;: path of the VToonify-D model. By default, a VToonify-Dsd trained on cartoon style is loaded.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--exstyle_path&lt;/code&gt;: path of the extrinsic style code. By default, codes in the same directory as &lt;code&gt;--ckpt&lt;/code&gt; are loaded.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--scale_image&lt;/code&gt;: rescale the input image/video to fit VToonify (highly recommend).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--padding&lt;/code&gt; (default: 200, 200, 200, 200): left, right, top, bottom paddings to the eye center.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an example of arcane style transfer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python style_transfer.py --content ./data/038648.jpg \&#xA;       --scale_image --style_id 77 --style_degree 0.5 \&#xA;       --ckpt ./checkpoint/vtoonify_d_arcane/vtoonify_s_d.pt \&#xA;       --padding 600 600 600 600     # use large padding to avoid cropping the image&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18130694/189533139-94c3d086-7fe9-49f9-b31f-dbd2a4798e9f.jpg&#34; alt=&#34;arcane&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Specify &lt;code&gt;--video&lt;/code&gt; to perform video toonification:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python style_transfer.py --scale_image --content ./data/YOUR_VIDEO.mp4 --video&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above style control options (&lt;code&gt;--style_id&lt;/code&gt;, &lt;code&gt;--style_degree&lt;/code&gt;, &lt;code&gt;--color_transfer&lt;/code&gt;) also work for videos.&lt;/p&gt; &#xA;&lt;h3&gt;Style Transfer with VToonify-T&lt;/h3&gt; &#xA;&lt;p&gt;Specify &lt;code&gt;--backbone&lt;/code&gt; as &#39;&#39;toonify&#39;&#39; to load and use a VToonify-T model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python style_transfer.py --content ./data/038648.jpg \&#xA;       --scale_image --backbone toonify \&#xA;       --ckpt ./checkpoint/vtoonify_t_arcane/vtoonify.pt \&#xA;       --padding 600 600 600 600     # use large padding to avoid cropping the image&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18130694/189540365-d04ffb2a-d72f-4ada-a2a8-89b8ac9ea441.jpg&#34; alt=&#34;arcane2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In VToonify-T, &lt;code&gt;--style_id&lt;/code&gt;, &lt;code&gt;--style_degree&lt;/code&gt;, &lt;code&gt;--color_transfer&lt;/code&gt;, &lt;code&gt;--exstyle_path&lt;/code&gt; are not used.&lt;/p&gt; &#xA;&lt;p&gt;As with VToonify-D, specify &lt;code&gt;--video&lt;/code&gt; to perform video toonification.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;(2) Training VToonify&lt;/h2&gt; &#xA;&lt;p&gt;Download the supporting models to the &lt;code&gt;./checkpoint/&lt;/code&gt; folder and arrange them in &lt;a href=&#34;https://raw.githubusercontent.com/williamyang1991/VToonify/main/checkpoint/&#34;&gt;this folder structure&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT/view&#34;&gt;stylegan2-ffhq-config-f.pt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;StyleGAN model trained on FFHQ taken from &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;rosinality&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1NgI4mPkboYvYw3MWcdUaQhkr0OWgs9ej/view&#34;&gt;encoder.pt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pixel2style2pixel encoder that embeds FFHQ images into StyleGAN2 Z+ latent code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1jY0mTjVB8njDh6e0LP_2UxuRK3MnjoIR/view&#34;&gt;faceparsing.pth&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;BiSeNet for face parsing from &lt;a href=&#34;https://github.com/zllrunning/face-parsing.PyTorch&#34;&gt;face-parsing.PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1HbjmOIOfxqTAVScZOI2m7_tPgMPnc0uM/view&#34;&gt;directions.npy&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Editing vectors taken from &lt;a href=&#34;https://github.com/zhujiapeng/LowRankGAN&#34;&gt;LowRankGAN&lt;/a&gt; for editing face attributes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1GZQ6Gs5AzJq9lUL-ldIQexi0JYPKNy8b&#34;&gt;Toonify&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/drive/folders/1GZQ6Gs5AzJq9lUL-ldIQexi0JYPKNy8b&#34;&gt;DualStyleGAN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;pre-trained stylegan-based toonification models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To customize your own style, you may need to train a new Toonify/DualStyleGAN model following &lt;a href=&#34;https://github.com/williamyang1991/DualStyleGAN#3-training-dualstylegan&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Train VToonify-D&lt;/h3&gt; &#xA;&lt;p&gt;Given the supporting models arranged in the &lt;a href=&#34;https://raw.githubusercontent.com/williamyang1991/VToonify/main/checkpoint/&#34;&gt;default folder structure&lt;/a&gt;, we can simply pre-train the encoder and train the whole VToonify-D by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# for pre-training the encoder&#xA;python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_d.py \&#xA;       --iter ITERATIONS --stylegan_path DUALSTYLEGAN_PATH --exstyle_path EXSTYLE_CODE_PATH \&#xA;       --batch BATCH_SIZE --name SAVE_NAME --pretrain&#xA;# for training VToonify-D given the pre-trained encoder&#xA;python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_d.py \&#xA;       --iter ITERATIONS --stylegan_path DUALSTYLEGAN_PATH --exstyle_path EXSTYLE_CODE_PATH \&#xA;       --batch BATCH_SIZE --name SAVE_NAME                  # + ADDITIONAL STYLE CONTROL OPTIONS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The models and the intermediate results are saved in &lt;code&gt;./checkpoint/SAVE_NAME/&lt;/code&gt; and &lt;code&gt;./log/SAVE_NAME/&lt;/code&gt;, respectively.&lt;/p&gt; &#xA;&lt;p&gt;VToonify-D provides the following STYLE CONTROL OPTIONS:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--fix_degree&lt;/code&gt;: if specified, model is trained with a fixed style degree (no degree adjustment)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--fix_style&lt;/code&gt;: if specified, model is trained with a fixed style image (no examplar-based style transfer)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--fix_color&lt;/code&gt;: if specified, model is trained with color preservation (no color transfer)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--style_id&lt;/code&gt;: the index of the style image (find the mapping between index and the style image &lt;a href=&#34;https://github.com/williamyang1991/DualStyleGAN/tree/main/doc_images&#34;&gt;here&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--style_degree&lt;/code&gt; (default: 0.5): the degree of style.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an example to reproduce the VToonify-Dsd on Cartoon style and the VToonify-D specialized for a mild toonification on the 26th cartoon style:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_d.py \&#xA;       --iter 30000 --stylegan_path ./checkpoint/cartoon/generator.pt --exstyle_path ./checkpoint/cartoon/refined_exstyle_code.npy \&#xA;       --batch 1 --name vtoonify_d_cartoon --pretrain      &#xA;python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_d.py \&#xA;       --iter 2000 --stylegan_path ./checkpoint/cartoon/generator.pt --exstyle_path ./checkpoint/cartoon/refined_exstyle_code.npy \&#xA;       --batch 4 --name vtoonify_d_cartoon --fix_color &#xA;python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_d.py \&#xA;       --iter 2000 --stylegan_path ./checkpoint/cartoon/generator.pt --exstyle_path ./checkpoint/cartoon/refined_exstyle_code.npy \&#xA;       --batch 4 --name vtoonify_d_cartoon --fix_color --fix_degree --style_degree 0.5 --fix_style --style_id 26&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the pre-trained encoder is shared by different STYLE CONTROL OPTIONS. VToonify-D only needs to pre-train the encoder once for each DualStyleGAN model. Eight GPUs are not necessary, one can train the model with a single GPU with larger &lt;code&gt;--iter&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;: [how to find an ideal model] we can first train a versatile model VToonify-Dsd, and navigate around different styles and degrees. After finding the ideal setting, we can then train the model specialized in that setting for high-quality stylization.&lt;/p&gt; &#xA;&lt;h3&gt;Train VToonify-T&lt;/h3&gt; &#xA;&lt;p&gt;The training of VToonify-T is similar to VToonify-D,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# for pre-training the encoder&#xA;python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_t.py \&#xA;       --iter ITERATIONS --finetunegan_path FINETUNED_MODEL_PATH \&#xA;       --batch BATCH_SIZE --name SAVE_NAME --pretrain       # + ADDITIONAL STYLE CONTROL OPTION&#xA;# for training VToonify-T given the pre-trained encoder&#xA;python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT train_vtoonify_t.py \&#xA;       --iter ITERATIONS --finetunegan_path FINETUNED_MODEL_PATH \&#xA;       --batch BATCH_SIZE --name SAVE_NAME                  # + ADDITIONAL STYLE CONTROL OPTION&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;VToonify-T only has one STYLE CONTROL OPTION:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--weight&lt;/code&gt; (default: 1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0): 18 numbers indicate how the 18 layers of the ffhq stylegan model and the finetuned model are blended to obtain the final Toonify model. &lt;a href=&#34;https://github.com/williamyang1991/VToonify/raw/edfd68e96eb0c0ab4c31628feef1b667e890a2cd/train_vtoonify_t.py#L30&#34;&gt;Here&lt;/a&gt; is the &lt;code&gt;--weight&lt;/code&gt; we use in the paper for different styles. Please refer to &lt;a href=&#34;https://github.com/justinpinkney/toonify&#34;&gt;toonify&lt;/a&gt; for the details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an example to reproduce the VToonify-T model on Arcane style:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_t.py \&#xA;       --iter 30000 --finetunegan_path ./checkpoint/arcane/finetune-000600.pt \&#xA;       --batch 1 --name vtoonify_t_arcane --pretrain --weight 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1 1 1 1 1 1 1 1 1 1 1&#xA;python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train_vtoonify_t.py \&#xA;       --iter 2000 --finetunegan_path ./checkpoint/arcane/finetune-000600.pt \&#xA;       --batch 4 --name vtoonify_t_arcane --weight 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1 1 1 1 1 1 1 1 1 1 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;(3) Results&lt;/h2&gt; &#xA;&lt;p&gt;Our framework is compatible with existing StyleGAN-based image toonification models to extend them to video toonification, and inherits their appealing features for flexible style control. With DualStyleGAN as the backbone, our VToonify is able to transfer the style of various reference images and adjust the style degree in one model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/18130694/189510094-4378caca-e8d9-48e1-9e5d-c8ec038e4bc5.mp4&#34;&gt;https://user-images.githubusercontent.com/18130694/189510094-4378caca-e8d9-48e1-9e5d-c8ec038e4bc5.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here are the color interpolated results of VToonify-D and VToonify-Dc on Arcane, Pixar and Comic styles.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/18130694/189510233-b4e3b4f7-5a37-4e0c-9821-a8049ce5f781.mp4&#34;&gt;https://user-images.githubusercontent.com/18130694/189510233-b4e3b4f7-5a37-4e0c-9821-a8049ce5f781.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this work useful for your research, please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{yang2022Vtoonify,&#xA;  title={VToonify: Controllable High-Resolution Portrait Video Style Transfer},&#xA;  author={Yang, Shuai and Jiang, Liming and Liu, Ziwei and Loy, Chen Change},&#xA;  journal={ACM Transactions on Graphics (TOG)},&#xA;  volume={41},&#xA;  number={6},&#xA;  articleno={203},&#xA;  pages={1--15},&#xA;  year={2022},&#xA;  publisher={ACM New York, NY, USA},&#xA;  doi={10.1145/3550454.3555437},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;The code is mainly developed based on &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;stylegan2-pytorch&lt;/a&gt;, &lt;a href=&#34;https://github.com/eladrich/pixel2style2pixel&#34;&gt;pixel2style2pixel&lt;/a&gt; and &lt;a href=&#34;https://github.com/williamyang1991/DualStyleGAN&#34;&gt;DualStyleGAN&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>