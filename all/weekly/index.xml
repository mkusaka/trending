<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-05T01:46:52Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hwchase17/langchain</title>
    <updated>2023-03-05T01:46:52Z</updated>
    <id>tag:github.com,2023-03-05:/hwchase17/langchain</id>
    <link href="https://github.com/hwchase17/langchain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;‚ö° Building applications with LLMs through composability ‚ö°&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ü¶úÔ∏èüîó LangChain&lt;/h1&gt; &#xA;&lt;p&gt;‚ö° Building applications with LLMs through composability ‚ö°&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hwchase17/langchain/actions/workflows/lint.yml&#34;&gt;&lt;img src=&#34;https://github.com/hwchase17/langchain/actions/workflows/lint.yml/badge.svg?sanitize=true&#34; alt=&#34;lint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hwchase17/langchain/actions/workflows/test.yml&#34;&gt;&lt;img src=&#34;https://github.com/hwchase17/langchain/actions/workflows/test.yml/badge.svg?sanitize=true&#34; alt=&#34;test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hwchase17/langchain/actions/workflows/linkcheck.yml&#34;&gt;&lt;img src=&#34;https://github.com/hwchase17/langchain/actions/workflows/linkcheck.yml/badge.svg?sanitize=true&#34; alt=&#34;linkcheck&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/langchainai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&amp;amp;label=Follow%20%40LangChainAI&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/6adMQxSpJS&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/6adMQxSpJS?compact=true&amp;amp;style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Production Support:&lt;/strong&gt; As you move your LangChains into production, we&#39;d love to offer more comprehensive support. Please fill out &lt;a href=&#34;https://forms.gle/57d8AmXBYp8PP8tZA&#34;&gt;this form&lt;/a&gt; and we&#39;ll set up a dedicated support Slack channel.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Install&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install langchain&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ü§î What is this?&lt;/h2&gt; &#xA;&lt;p&gt;Large language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. But using these LLMs in isolation is often not enough to create a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.&lt;/p&gt; &#xA;&lt;p&gt;This library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;‚ùì Question Answering over specific documents&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://langchain.readthedocs.io/en/latest/use_cases/question_answering.html&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;End-to-end Example: &lt;a href=&#34;https://github.com/hwchase17/notion-qa&#34;&gt;Question Answering over Notion Database&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;üí¨ Chatbots&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://langchain.readthedocs.io/en/latest/use_cases/chatbots.html&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;End-to-end Example: &lt;a href=&#34;https://github.com/hwchase17/chat-langchain&#34;&gt;Chat-LangChain&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;ü§ñ Agents&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://langchain.readthedocs.io/en/latest/use_cases/agents.html&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;End-to-end Example: &lt;a href=&#34;https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain&#34;&gt;GPT+WolframAlpha&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìñ Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://langchain.readthedocs.io/en/latest/?&#34;&gt;here&lt;/a&gt; for full documentation on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Getting started (installation, setting up the environment, simple examples)&lt;/li&gt; &#xA; &lt;li&gt;How-To examples (demos, integrations, helper functions)&lt;/li&gt; &#xA; &lt;li&gt;Reference (full API docs)&lt;/li&gt; &#xA; &lt;li&gt;Resources (high-level explanation of core concepts)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ What can this help with?&lt;/h2&gt; &#xA;&lt;p&gt;There are six main areas that LangChain is designed to help with. These are, in increasing order of complexity:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üìÉ LLMs and Prompts:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This includes prompt management, prompt optimization, generic interface for all LLMs, and common utilities for working with LLMs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üîó Chains:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üìö Data Augmented Generation:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Data Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ü§ñ Agents:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üß† Memory:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üßê Evaluation:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;[BETA] Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.&lt;/p&gt; &#xA;&lt;p&gt;For more information on these concepts, please see our &lt;a href=&#34;https://langchain.readthedocs.io/en/latest/?&#34;&gt;full documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üíÅ Contributing&lt;/h2&gt; &#xA;&lt;p&gt;As an open source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infra, or better documentation.&lt;/p&gt; &#xA;&lt;p&gt;For detailed information on how to contribute, see &lt;a href=&#34;https://raw.githubusercontent.com/hwchase17/langchain/master/CONTRIBUTING.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AmbientRun/Ambient</title>
    <updated>2023-03-05T01:46:52Z</updated>
    <id>tag:github.com,2023-03-05:/AmbientRun/Ambient</id>
    <link href="https://github.com/AmbientRun/Ambient" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The multiplayer game engine&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ambient&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://crates.io/crates/ambient_api&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/ambient_api&#34; alt=&#34;Crates.io&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.rs/ambient_api&#34;&gt;&lt;img src=&#34;https://img.shields.io/docsrs/ambient_api&#34; alt=&#34;docs.rs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AmbientRun/Ambient#license&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT%2FApache-blue.svg?sanitize=true&#34; alt=&#34;MIT/Apache 2.0&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/PhmPn6m8Tw&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/894505972289134632&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ambient is a runtime for building high-performance multiplayer games and 3D applications, powered by WebAssembly, Rust and WebGPU.&lt;/p&gt; &#xA;&lt;p&gt;See our &lt;a href=&#34;https://www.ambient.run/post/introducing-ambient&#34;&gt;announcement blog post&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Design principles&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Seamless networking&lt;/strong&gt;: Ambient is both your server and client. All you need to do is to build your server and/or client-side logic: the runtime handles synchronization of data for you.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Isolation&lt;/strong&gt;: Projects you build for Ambient are executed in isolation through the power of &lt;a href=&#34;https://webassembly.org/&#34;&gt;WebAssembly&lt;/a&gt; - so that if something crashes, it won‚Äôt take down your entire program. It also means that you can run untrusted code safely.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data-oriented design&lt;/strong&gt;: The core data model of Ambient is an &lt;a href=&#34;https://en.wikipedia.org/wiki/Entity_component_system&#34;&gt;entity component system&lt;/a&gt; which each WASM module can manipulate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Language-agnostic&lt;/strong&gt;: You will be able to build Ambient modules in any language that can compile to WebAssembly. At present, Rust is the only supported language, but we are working on expanding to other languages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Single executable&lt;/strong&gt;: Ambient is a single executable which can run on Windows, Mac and Linux. It can act as a server or as a client.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Interoperability&lt;/strong&gt;: Ambient allows you to define custom components and &#34;concepts&#34; (collections of components). As long as your Ambient projects use the same components and concepts, they will be able to share data and interoperate, even if they have no awareness of each other.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Asset pipeline and streaming&lt;/strong&gt;: Ambient has an &lt;a href=&#34;https://ambientrun.github.io/Ambient/reference/asset_pipeline.html&#34;&gt;asset pipeline&lt;/a&gt; that is capable of compiling multiple asset formats, including &lt;code&gt;.glb&lt;/code&gt; and &lt;code&gt;.fbx&lt;/code&gt;. The assets are always streamed over the network, so your clients will receive everything they need when they join.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Powerful renderer&lt;/strong&gt;: The Ambient renderer is GPU-driven, with both culling and level-of-detail switching being handled entirely by the GPU. By default, it uses &lt;a href=&#34;https://en.wikipedia.org/wiki/Physically_based_rendering&#34;&gt;PBR&lt;/a&gt;. It also supports cascading shadow maps and instances everything that can be instanced.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://ambientrun.github.io/Ambient/&#34;&gt;documentation&lt;/a&gt; for a guide on how to get started, or browse the &lt;a href=&#34;https://github.com/AmbientRun/Ambient/tree/main/guest/rust/examples&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installing&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to get Ambient is by downloading the latest release &lt;a href=&#34;https://github.com/AmbientRun/Ambient/releases&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For alternative installation options, go to the &lt;a href=&#34;https://ambientrun.github.io/Ambient/user/installing.html&#34;&gt;documentation on installing&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note: Ambient is in an alpha stage and the API will be iterated on heavily. We are working towards a stable release.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Feature&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ECS&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Single-threaded.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WASM API&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Rust is the only supported guest language right now, and WASM can only be used on the server. We are working on clientside WASM; see &lt;a href=&#34;https://github.com/AmbientRun/Ambient/issues/6&#34;&gt;this issue&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multiplayer/networking&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Multiplayer is server-authoritative without any prediction or compensation. See &lt;a href=&#34;https://github.com/AmbientRun/Ambient/issues/150&#34;&gt;this issue&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPU-driven renderer&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FBX &amp;amp; glTF loading&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Physics (through PhysX)&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Using PhysX 4.1. PhysX 5 support is tracked in &lt;a href=&#34;https://github.com/AmbientRun/Ambient/issues/155&#34;&gt;this issue&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Animations&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Skinmeshing&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Shadow maps&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Decals&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPU culling and LoD&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi-platform&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Windows, Mac, and Linux so far. x86-64 and ARM64 are actively supported; other platforms may also work, but require testing.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Run on Web&lt;/td&gt; &#xA;   &lt;td&gt;üöß&lt;/td&gt; &#xA;   &lt;td&gt;See &lt;a href=&#34;https://github.com/AmbientRun/Ambient/issues/151&#34;&gt;this issue&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multithreading API&lt;/td&gt; &#xA;   &lt;td&gt;üöß&lt;/td&gt; &#xA;   &lt;td&gt;Multithreading is already used internally, but we want to expose multithreading functionality within the WASM API. This may be explicit (i.e. task- or thread-spawning) or implicit (WASM modules being scheduled across threads).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UI API&lt;/td&gt; &#xA;   &lt;td&gt;üöß&lt;/td&gt; &#xA;   &lt;td&gt;A React-like UI library already exists in the repo, and we&#39;re working on exposing it through the WASM API. See &lt;a href=&#34;https://github.com/AmbientRun/Ambient/issues/40&#34;&gt;this issue&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Custom shaders&lt;/td&gt; &#xA;   &lt;td&gt;üöß&lt;/td&gt; &#xA;   &lt;td&gt;Custom shaders are supported by the renderer, but are not yet exposed in the API. See &lt;a href=&#34;https://github.com/AmbientRun/Ambient/issues/98&#34;&gt;this issue&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hot-reloading assets&lt;/td&gt; &#xA;   &lt;td&gt;üöß&lt;/td&gt; &#xA;   &lt;td&gt;See &lt;a href=&#34;https://github.com/AmbientRun/Ambient/issues/12&#34;&gt;this issue&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Audio&lt;/td&gt; &#xA;   &lt;td&gt;üöß&lt;/td&gt; &#xA;   &lt;td&gt;Audio is supported, but not currently exposed. See &lt;a href=&#34;https://github.com/AmbientRun/Ambient/issues/76&#34;&gt;this issue&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ECS save/load&lt;/td&gt; &#xA;   &lt;td&gt;üöß&lt;/td&gt; &#xA;   &lt;td&gt;For loading, &lt;a href=&#34;https://github.com/AmbientRun/Ambient/issues/71&#34;&gt;see this issue&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Each example in the &lt;a href=&#34;https://raw.githubusercontent.com/AmbientRun/Ambient/main/guest/rust/examples/&#34;&gt;examples&lt;/a&gt; directory can be run with Ambient as both client and server:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;cd guest/rust/examples/tictactoe&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ambient run&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Every example can also be run server-only. To do so:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;cd guest/rust/examples/tictactoe&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ambient serve&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This will start a server that other people, including yourself, can join (assuming that ports 8999 and 9000 are forwarded):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;ambient join [IP_OF_SERVER]&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that content is always streamed, so the only thing the joining user requires is Ambient itself to join the session.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome community contributions to this project.&lt;/p&gt; &#xA;&lt;p&gt;Please talk with us on Discord beforehand if you&#39;d like to contribute a larger piece of work.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under either of&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apache License, Version 2.0, (&lt;a href=&#34;https://raw.githubusercontent.com/AmbientRun/Ambient/main/LICENSE-APACHE&#34;&gt;LICENSE-APACHE&lt;/a&gt; or &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;MIT license (&lt;a href=&#34;https://raw.githubusercontent.com/AmbientRun/Ambient/main/LICENSE-MIT&#34;&gt;LICENSE-MIT&lt;/a&gt; or &lt;a href=&#34;http://opensource.org/licenses/MIT&#34;&gt;http://opensource.org/licenses/MIT&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;at your option.&lt;/p&gt; &#xA;&lt;h3&gt;Contribution&lt;/h3&gt; &#xA;&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Mikubill/sd-webui-controlnet</title>
    <updated>2023-03-05T01:46:52Z</updated>
    <id>tag:github.com,2023-03-05:/Mikubill/sd-webui-controlnet</id>
    <link href="https://github.com/Mikubill/sd-webui-controlnet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;WebUI extension for ControlNet&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;sd-webui-controlnet&lt;/h2&gt; &#xA;&lt;p&gt;(WIP) WebUI extension for ControlNet&lt;/p&gt; &#xA;&lt;p&gt;This extension is for AUTOMATIC1111&#39;s &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;Stable Diffusion web UI&lt;/a&gt;, allows the Web UI to add &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt; to the original Stable Diffusion model to generate images. The addition is on-the-fly, the merging is not required.&lt;/p&gt; &#xA;&lt;p&gt;ControlNet is a neural network structure to control diffusion models by adding extra conditions.&lt;/p&gt; &#xA;&lt;p&gt;Thanks &amp;amp; Inspired: kohya-ss/sd-webui-additional-networks&lt;/p&gt; &#xA;&lt;h3&gt;Limits&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dragging large file on the Web UI may freeze the entire page. It is better to use the upload file option instead.&lt;/li&gt; &#xA; &lt;li&gt;Just like WebUI&#39;s &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/raw/3715ece0adce7bf7c5e9c5ab3710b2fdc3848f39/modules/sd_hijack_unet.py#L27&#34;&gt;hijack&lt;/a&gt;, we used some interpolate to accept arbitrary size configure (see &lt;code&gt;scripts/cldm.py&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Install&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open &#34;Extensions&#34; tab.&lt;/li&gt; &#xA; &lt;li&gt;Open &#34;Install from URL&#34; tab in the tab.&lt;/li&gt; &#xA; &lt;li&gt;Enter URL of this repo to &#34;URL for extension&#39;s git repository&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Press &#34;Install&#34; button.&lt;/li&gt; &#xA; &lt;li&gt;Reload/Restart Web UI.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Upgrade gradio if any ui issues occured: &lt;code&gt;pip install gradio==3.16.2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Put the ControlNet models (&lt;code&gt;.pt&lt;/code&gt;, &lt;code&gt;.pth&lt;/code&gt;, &lt;code&gt;.ckpt&lt;/code&gt; or &lt;code&gt;.safetensors&lt;/code&gt;) inside the &lt;code&gt;models/ControlNet&lt;/code&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;Open &#34;txt2img&#34; or &#34;img2img&#34; tab, write your prompts.&lt;/li&gt; &#xA; &lt;li&gt;Press &#34;Refresh models&#34; and select the model you want to use. (If nothing appears, try reload/restart the webui)&lt;/li&gt; &#xA; &lt;li&gt;Upload your image and select preprocessor, done.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Currently it supports both full models and trimmed models. Use &lt;code&gt;extract_controlnet.py&lt;/code&gt; to extract controlnet from original &lt;code&gt;.pth&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;Pretrained Models: &lt;a href=&#34;https://huggingface.co/lllyasviel/ControlNet/tree/main/models&#34;&gt;https://huggingface.co/lllyasviel/ControlNet/tree/main/models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Extraction&lt;/h3&gt; &#xA;&lt;p&gt;Two methods can be used to reduce the model&#39;s filesize:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Directly extract controlnet from original .pth file using &lt;code&gt;extract_controlnet.py&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Transfer control from original checkpoint by making difference using &lt;code&gt;extract_controlnet_diff.py&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;All type of models can be correctly recognized and loaded. The results of different extraction methods are discussed in &lt;a href=&#34;https://github.com/lllyasviel/ControlNet/discussions/12&#34;&gt;https://github.com/lllyasviel/ControlNet/discussions/12&lt;/a&gt; and &lt;a href=&#34;https://github.com/Mikubill/sd-webui-controlnet/issues/73&#34;&gt;https://github.com/Mikubill/sd-webui-controlnet/issues/73&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Pre-extracted model: &lt;a href=&#34;https://huggingface.co/webui/ControlNet-modules-safetensors&#34;&gt;https://huggingface.co/webui/ControlNet-modules-safetensors&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Pre-extracted difference model: &lt;a href=&#34;https://huggingface.co/kohya-ss/ControlNet-diff-modules&#34;&gt;https://huggingface.co/kohya-ss/ControlNet-diff-modules&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;T2I-Adapter Support (Experimental)&lt;/h3&gt; &#xA;&lt;p&gt;Currently support both sketch Adapter and image Adapter. Note that the impl is experimental, result may differ from original repo. See &lt;code&gt;Adapter Examples&lt;/code&gt; for reference.&lt;/p&gt; &#xA;&lt;p&gt;To use these models:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download files from &lt;a href=&#34;https://huggingface.co/TencentARC/T2I-Adapter&#34;&gt;https://huggingface.co/TencentARC/T2I-Adapter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Setup correct config in settings panel - &lt;code&gt;sketch_adapter_v14.yaml&lt;/code&gt; for sketch model and &lt;code&gt;image_adapter_v14.yaml&lt;/code&gt; for keypose and segmentation model.&lt;/li&gt; &#xA; &lt;li&gt;It&#39;s better to use a slightly lower strength (t) when generating images with sketch model, such as 0.6-0.8. (ref: &lt;a href=&#34;https://github.com/TencentARC/T2I-Adapter/raw/5f41a0e38fc6eac90d04bc4cede85a2bc4570653/ldm/models/diffusion/plms.py#L158&#34;&gt;ldm/models/diffusion/plms.py&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Tips&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Don&#39;t forget to add some negative prompt, default negative prompt in ControlNet repo is &#34;longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Regarding canvas height/width: they are designed for canvas generation. If you want to upload images directly, you can safely ignore them.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Source&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Input&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Output&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;(no preprocessor)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/bal-source.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/bal-gen.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;(no preprocessor)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/dog_rel.jpg?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/dog_rel.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/mahiro_input.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/mahiro_canny.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/mahiro-out.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/evt_source.jpg?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/evt_hed.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/evt_gen.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/an-source.jpg?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/an-pose.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/an-gen.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/sk-b-src.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/sk-b-dep.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/sk-b-out.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/nm-src.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/nm-gen.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/nm-out.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Adapter Examples&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Input&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Output&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/dog_sk-2.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/dog_out-2.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/cat_sk-2.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/cat_out-2.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/kp_a-2.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/kp_o-2.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/kp_o2-2.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/samples/kp_a2-2.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Minimum Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(Windows) (NVIDIA: Ampere) 4gb - with &lt;code&gt;--xformers&lt;/code&gt; enabled, and &lt;code&gt;Low VRAM&lt;/code&gt; mode ticked in the UI, goes up to 768x832&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;CFG Based ControlNet (Experimental)&lt;/h3&gt; &#xA;&lt;p&gt;The original ControlNet applies control to both conditional (cond) and unconditional (uncond) parts. Enabling this option will make the control only apply to the cond part. Some experiments indicate that this approach improves image quality.&lt;/p&gt; &#xA;&lt;p&gt;To enable this option, tick &lt;code&gt;Enable CFG-Based guidance for ControlNet&lt;/code&gt; in the settings.&lt;/p&gt; &#xA;&lt;p&gt;Note that you need to use a low cfg scale/guidance scale (such as 3-5) and proper weight tuning to get good result.&lt;/p&gt; &#xA;&lt;h3&gt;Guess Mode (Non-Prompt Mode, Experimental)&lt;/h3&gt; &#xA;&lt;p&gt;Guess Mode is CFG Based ControlNet + Exponential decay in weighting.&lt;/p&gt; &#xA;&lt;p&gt;See issue &lt;a href=&#34;https://github.com/Mikubill/sd-webui-controlnet/issues/236&#34;&gt;https://github.com/Mikubill/sd-webui-controlnet/issues/236&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;Original introduction from controlnet:&lt;/p&gt; &#xA;&lt;p&gt;The &#34;guess mode&#34; (or called non-prompt mode) will completely unleash all the power of the very powerful ControlNet encoder.&lt;/p&gt; &#xA;&lt;p&gt;In this mode, you can just remove all prompts, and then the ControlNet encoder will recognize the content of the input control map, like depth map, edge map, scribbles, etc.&lt;/p&gt; &#xA;&lt;p&gt;This mode is very suitable for comparing different methods to control stable diffusion because the non-prompted generating task is significantly more difficult than prompted task. In this mode, different methods&#39; performance will be very salient.&lt;/p&gt; &#xA;&lt;p&gt;For this mode, we recommend to &lt;strong&gt;use 50 steps and guidance scale between 3 and 5.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Multi-ControlNet / Joint Conditioning (Experimental)&lt;/h3&gt; &#xA;&lt;p&gt;This option allows multiple ControlNet inputs for a single generation. To enable this option, change &lt;code&gt;Multi ControlNet: Max models amount (requires restart)&lt;/code&gt; in the settings. Note that you will need to restart the WebUI for changes to take effect.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Guess Mode will apply to all ControlNet if any of them are enabled.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Source A&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Source B&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Output&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://user-images.githubusercontent.com/31246794/220448620-cd3ede92-8d3f-43d5-b771-32dd8417618f.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://user-images.githubusercontent.com/31246794/220448619-beed9bdb-f6bb-41c2-a7df-aa3ef1f653c5.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;&#34; src=&#34;https://user-images.githubusercontent.com/31246794/220448613-c99a9e04-0450-40fd-bc73-a9122cefaa2c.png&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Weight and Guidance Strength/Start/End&lt;/h3&gt; &#xA;&lt;p&gt;Weight is the weight of the controlnet &#34;influence&#34;. It&#39;s analogous to prompt attention/emphasis. E.g. (myprompt: 1.2). Technically, it&#39;s the factor by which to multiply the ControlNet outputs before merging them with original SD Unet.&lt;/p&gt; &#xA;&lt;p&gt;Guidance Start/End is the percentage of total steps the controlnet applies (guidance strength = guidance end). It&#39;s analogous to prompt editing/shifting. E.g. [myprompt::0.8] (It applies from the beginning until 80% of total steps)&lt;/p&gt; &#xA;&lt;h3&gt;API/Script Access&lt;/h3&gt; &#xA;&lt;p&gt;This extension can accept txt2img or img2img tasks via API or external extension call. Note that you may need to enable &lt;code&gt;Allow other scripts to control this extension&lt;/code&gt; in settings for external calls.&lt;/p&gt; &#xA;&lt;p&gt;To use the API: start WebUI with argument &lt;code&gt;--api&lt;/code&gt; and go to &lt;code&gt;http://webui-address/docs&lt;/code&gt; for documents or checkout &lt;a href=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/main/example/api_txt2img.ipynb&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To use external extension call: Pass your config to p(pipeline). For more details see &lt;a href=&#34;https://github.com/Mikubill/sd-webui-controlnet/raw/8ca06ba8eb2989cbd434063c9a7c0e7a3bdfabea/scripts/controlnet.py#L459-L462&#34;&gt;scripts/controlnet.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;MacOS Support&lt;/h3&gt; &#xA;&lt;p&gt;Tested with pytorch nightly: &lt;a href=&#34;https://github.com/Mikubill/sd-webui-controlnet/pull/143#issuecomment-1435058285&#34;&gt;https://github.com/Mikubill/sd-webui-controlnet/pull/143#issuecomment-1435058285&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To use this extension with mps and normal pytorch, currently you may need to start WebUI with &lt;code&gt;--no-half&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>