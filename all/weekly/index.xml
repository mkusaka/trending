<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-02T01:40:22Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>medusajs/medusa</title>
    <updated>2022-10-02T01:40:22Z</updated>
    <id>tag:github.com,2022-10-02:/medusajs/medusa</id>
    <link href="https://github.com/medusajs/medusa" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The open-source Shopify alternative ⚡️&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.medusajs.com&#34;&gt; &lt;img alt=&#34;Medusa&#34; src=&#34;https://user-images.githubusercontent.com/7554214/153162406-bf8fd16f-aa98-4604-b87b-e13ab4baf604.png&#34; width=&#34;100&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; Medusa &lt;/h1&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.medusajs.com&#34;&gt;Documentation&lt;/a&gt; | &lt;a href=&#34;https://demo.medusajs.com/&#34;&gt;Medusa Admin Demo&lt;/a&gt; | &lt;a href=&#34;https://www.medusajs.com&#34;&gt;Website&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt; An open-source composable commerce engine built for developers. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/medusajs/medusa/raw/master/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;Medusa is released under the MIT license.&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://circleci.com/gh/medusajs/medusa&#34;&gt; &lt;img src=&#34;https://circleci.com/gh/medusajs/medusa.svg?style=shield&#34; alt=&#34;Current CircleCI build status.&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/medusajs/medusa/raw/master/CONTRIBUTING.md&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PRs welcome!&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.producthunt.com/posts/medusa&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Product%20Hunt-%231%20Product%20of%20the%20Day-%23DA552E&#34; alt=&#34;Product Hunt&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/xpCwq3Kfn8&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/chat-on%20discord-7289DA.svg?sanitize=true&#34; alt=&#34;Discord Chat&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=medusajs&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/medusajs.svg?label=Follow%20@medusajs&#34; alt=&#34;Follow @medusajs&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;You can install Medusa by either following our &lt;a href=&#34;https://docs.medusajs.com/quickstart/quick-start&#34;&gt;Quickstart guide&lt;/a&gt; or the following steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Medusa CLI&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install -g @medusajs/medusa-cli&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create a new Medusa project&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;medusa new my-medusa-store --seed&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start your Medusa engine&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;medusa develop&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Node v14.0 or higher.&lt;/li&gt; &#xA; &lt;li&gt;SQLite or PostgreSQL (SQLite is only for getting started; PostgreSQL is recommended)&lt;/li&gt; &#xA; &lt;li&gt;Redis&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can check out &lt;a href=&#34;https://docs.medusajs.com/tutorial/set-up-your-development-environment&#34;&gt;this documentation for more details about setting up your environment&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;What is Medusa?&lt;/h2&gt; &#xA;&lt;p&gt;Medusa is an open source composable commerce engine built with Node.js. Medusa enables developers to build scalable and sophisticated commerce setups with low effort and great developer experience.&lt;/p&gt; &#xA;&lt;p&gt;You can learn more about &lt;a href=&#34;https://docs.medusajs.com/introduction&#34;&gt;Medusa’s architecture in our documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Orders, Exchanges, and Returns APIs:&lt;/strong&gt; Aside from the standard order management that comes with ecommerce platforms, Medusa also provides an easy and automated way to manage swaps, returns, and claims.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Products and Collections APIs:&lt;/strong&gt; Add products with extensive customization settings and sort them into collections.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Region API:&lt;/strong&gt; Configure and manage multiple regions and currencies all from one platform.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Plugin API:&lt;/strong&gt; Easily integrate fulfillment providers, payment providers, notification services, and many other custom tools and third-party services.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;strong&gt;PriceList and Promotions APIs:&lt;/strong&gt;&lt;/strong&gt; Advanced pricing for products with conditions based on its amount in the cart or promotions and discounts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tax API:&lt;/strong&gt; Advanced tax configurations specific to multiple regions, with capability of specifying taxes for specific products.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See more of the &lt;a href=&#34;https://docs.medusajs.com/#features&#34;&gt;ecommerce features on our documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Write-ups for all features will be made available in&amp;nbsp;&lt;a href=&#34;https://github.com/medusajs/medusa/discussions&#34;&gt;Github discussions&lt;/a&gt;&amp;nbsp;prior to starting the implementation process.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;2022&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Admin revamp&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Tax API&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Tax Calculation Strategy&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Cart Calculation Strategy&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Customer Groups API&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Promotions API&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Price Lists API&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Price Selection Strategy&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Import / Export API&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Sales Channel API&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Extended Product API (custom fields, publishing control, and more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Extended Order API (managing placed orders, improved inventory control, and more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Multi-warehouse support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; GraphQL API&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Plugins&lt;/h2&gt; &#xA;&lt;p&gt;As a headless and extendible solution, Medusa allows you to integrate third-party services or add custom features into Medusa by installing Plugins.&lt;/p&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://github.com/medusajs/medusa/tree/master/packages&#34;&gt;our available plugins&lt;/a&gt; that you can install and use instantly on your Medusa server.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;Medusa is all about the community. Therefore, we would love for you to help us build the most robust and powerful commerce engine on the market.&lt;/p&gt; &#xA;&lt;p&gt;Whether it is fixing bugs, improving our documentation or simply spreading the word, please feel free to join in. Please check&amp;nbsp;&lt;a href=&#34;https://github.com/medusajs/medusa/raw/master/CONTRIBUTING.md&#34;&gt;our contribution guide&lt;/a&gt;&amp;nbsp;for further details about how to contribute.&lt;/p&gt; &#xA;&lt;h2&gt;Community &amp;amp; Support&lt;/h2&gt; &#xA;&lt;p&gt;Use these channels to be part of the community, ask for help while using Medusa, or just learn more about Medusa:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/medusajs&#34;&gt;Discord&lt;/a&gt;: This is the main channel to join the community. You can ask for help, showcase your work with Medusa, and stay up to date with everything Medusa.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/medusajs/medusa/issues&#34;&gt;GitHub Issues&lt;/a&gt;: for sending in any issues you face or bugs you find while using Medusa.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/medusajs/medusa/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;: for joining discussions and submitting your ideas.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medusajs.com/blog/&#34;&gt;Medusa Blog&lt;/a&gt;: find diverse tutorials and company news.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/medusajs&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/company/medusajs&#34;&gt;LinkedIn&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Upgrade Guides&lt;/h2&gt; &#xA;&lt;p&gt;Follow our&amp;nbsp;&lt;a href=&#34;https://docs.medusajs.com/advanced/backend/upgrade-guides/&#34;&gt;upgrade guides&lt;/a&gt; on the documentation to keep your Medusa project up-to-date.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the&amp;nbsp;&lt;a href=&#34;https://github.com/medusajs/medusa/raw/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>XavierXiao/Dreambooth-Stable-Diffusion</title>
    <updated>2022-10-02T01:40:22Z</updated>
    <id>tag:github.com,2022-10-02:/XavierXiao/Dreambooth-Stable-Diffusion</id>
    <link href="https://github.com/XavierXiao/Dreambooth-Stable-Diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Dreambooth (https://arxiv.org/abs/2208.12242) with Stable Diffusion&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dreambooth on Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;This is an implementtaion of Google&#39;s &lt;a href=&#34;https://arxiv.org/abs/2208.12242&#34;&gt;Dreambooth&lt;/a&gt; with &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;. The original Dreambooth is based on &lt;a href=&#34;https://imagen.research.google/&#34;&gt;Imagen&lt;/a&gt; text-to-image model. However, neither the model nor the pre-trained weights of Imagen is available. To enable people to fine-tune a text-to-image model with a few examples, I implemented the idea of Dreambooth on Stable diffusion.&lt;/p&gt; &#xA;&lt;p&gt;This code repository is based on that of &lt;a href=&#34;https://github.com/rinongal/textual_inversion&#34;&gt;Textual Inversion&lt;/a&gt;. Note that Textual Inversion only optimizes word ebedding, while dreambooth fine-tunes the whole diffusion model.&lt;/p&gt; &#xA;&lt;p&gt;The implementation makes minimum changes over the official codebase of Textual Inversion. In fact, due to lazyness, some components in Textual Inversion, such as the embedding manager, are not deleted, although they will never be used here.&lt;/p&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;9/20/2022&lt;/strong&gt;: I just found a way to reduce the GPU memory a bit. Remember that this code is based on Textual Inversion, and TI&#39;s code base has &lt;a href=&#34;https://github.com/rinongal/textual_inversion/raw/main/ldm/modules/diffusionmodules/util.py#L112&#34;&gt;this line&lt;/a&gt;, which disable gradient checkpointing in a hard-code way. This is because in TI, the Unet is not optimized. However, in Dreambooth we optimize the Unet, so we can turn on the gradient checkpoint pointing trick, as in the original SD repo &lt;a href=&#34;https://github.com/CompVis/stable-diffusion/raw/main/ldm/modules/diffusionmodules/util.py#L112&#34;&gt;here&lt;/a&gt;. The gradient checkpoint is default to be True in &lt;a href=&#34;https://github.com/XavierXiao/Dreambooth-Stable-Diffusion/raw/main/configs/stable-diffusion/v1-finetune_unfrozen.yaml#L47&#34;&gt;config&lt;/a&gt;. I have updated the codes.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Preparation&lt;/h3&gt; &#xA;&lt;p&gt;First set-up the &lt;code&gt;ldm&lt;/code&gt; enviroment following the instruction from textual inversion repo, or the original Stable Diffusion repo.&lt;/p&gt; &#xA;&lt;p&gt;To fine-tune a stable diffusion model, you need to obtain the pre-trained stable diffusion models following their &lt;a href=&#34;https://github.com/CompVis/stable-diffusion#stable-diffusion-v1&#34;&gt;instructions&lt;/a&gt;. Weights can be downloaded on &lt;a href=&#34;https://huggingface.co/CompVis&#34;&gt;HuggingFace&lt;/a&gt;. You can decide which version of checkpoint to use, but I use &lt;code&gt;sd-v1-4-full-ema.ckpt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also need to create a set of images for regularization, as the fine-tuning algorithm of Dreambooth requires that. Details of the algorithm can be found in the paper. Note that in the original paper, the regularization images seem to be generated on-the-fly. However, here I generated a set of regularization images before the training. The text prompt for generating regularization images can be &lt;code&gt;photo of a &amp;lt;class&amp;gt;&lt;/code&gt;, where &lt;code&gt;&amp;lt;class&amp;gt;&lt;/code&gt; is a word that describes the class of your object, such as &lt;code&gt;dog&lt;/code&gt;. The command is&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/stable_txt2img.py --ddim_eta 0.0 --n_samples 8 --n_iter 1 --scale 10.0 --ddim_steps 50  --ckpt /path/to/original/stable-diffusion/sd-v1-4-full-ema.ckpt --prompt &#34;a photo of a &amp;lt;class&amp;gt;&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;I generate 8 images for regularization, but more regularization images may lead to stronger regularization and better editability. After that, save the generated images (separately, one image per &lt;code&gt;.png&lt;/code&gt; file) at &lt;code&gt;/root/to/regularization/images&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Updates on 9/9&lt;/strong&gt; We should definitely use more images for regularization. Please try 100 or 200, to better align with the original paper. To acomodate this, I shorten the &#34;repeat&#34; of reg dataset in the &lt;a href=&#34;https://github.com/XavierXiao/Dreambooth-Stable-Diffusion/raw/main/configs/stable-diffusion/v1-finetune_unfrozen.yaml#L96&#34;&gt;config file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For some cases, if the generated regularization images are highly unrealistic (happens when you want to generate &#34;man&#34; or &#34;woman&#34;), you can find a diverse set of images (of man/woman) online, and use them as regularization images.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Training can be done by running the following command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --base configs/stable-diffusion/v1-finetune_unfrozen.yaml &#xA;                -t &#xA;                --actual_resume /path/to/original/stable-diffusion/sd-v1-4-full-ema.ckpt  &#xA;                -n &amp;lt;job name&amp;gt; &#xA;                --gpus 0, &#xA;                --data_root /root/to/training/images &#xA;                --reg_data_root /root/to/regularization/images &#xA;                --class_word &amp;lt;xxx&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Detailed configuration can be found in &lt;code&gt;configs/stable-diffusion/v1-finetune_unfrozen.yaml&lt;/code&gt;. In particular, the default learning rate is &lt;code&gt;1.0e-6&lt;/code&gt; as I found the &lt;code&gt;1.0e-5&lt;/code&gt; in the Dreambooth paper leads to poor editability. The parameter &lt;code&gt;reg_weight&lt;/code&gt; corresponds to the weight of regularization in the Dreambooth paper, and the default is set to &lt;code&gt;1.0&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Dreambooth requires a placeholder word &lt;code&gt;[V]&lt;/code&gt;, called identifier, as in the paper. This identifier needs to be a relatively rare tokens in the vocabulary. The original paper approaches this by using a rare word in T5-XXL tokenizer. For simplicity, here I just use a random word &lt;code&gt;sks&lt;/code&gt; and hard coded it.. If you want to change that, simply make a change in &lt;a href=&#34;https://github.com/XavierXiao/Dreambooth-Stable-Diffusion/raw/main/ldm/data/personalized.py#L10&#34;&gt;this file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Training will be run for 800 steps, and two checkpoints will be saved at &lt;code&gt;./logs/&amp;lt;job_name&amp;gt;/checkpoints&lt;/code&gt;, one at 500 steps and one at final step. Typically the one at 500 steps works well enough. I train the model use two A6000 GPUs and it takes ~15 mins.&lt;/p&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;p&gt;After training, personalized samples can be obtained by running the command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/stable_txt2img.py --ddim_eta 0.0 &#xA;                                 --n_samples 8 &#xA;                                 --n_iter 1 &#xA;                                 --scale 10.0 &#xA;                                 --ddim_steps 100  &#xA;                                 --ckpt /path/to/saved/checkpoint/from/training&#xA;                                 --prompt &#34;photo of a sks &amp;lt;class&amp;gt;&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In particular, &lt;code&gt;sks&lt;/code&gt; is the identifier, which should be replaced by your choice if you happen to change the identifier, and &lt;code&gt;&amp;lt;class&amp;gt;&lt;/code&gt; is the class word &lt;code&gt;--class_word&lt;/code&gt; for training.&lt;/p&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;Here I show some qualitative results. The training images are obtained from the &lt;a href=&#34;https://github.com/rinongal/textual_inversion/issues/8&#34;&gt;issue&lt;/a&gt; in the Textual Inversion repository, and they are 3 images of a large trash container. Regularization images are generated by prompt &lt;code&gt;photo of a container&lt;/code&gt;. Regularization images are shown here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/a-container-0038.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;After training, generated images with prompt &lt;code&gt;photo of a sks container&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/photo-of-a-sks-container-0018.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Generated images with prompt &lt;code&gt;photo of a sks container on the beach&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/photo-of-a-sks-container-on-the-beach-0017.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Generated images with prompt &lt;code&gt;photo of a sks container on the moon&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/photo-of-a-sks-container-on-the-moon-0016.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Some not-so-perfect but still interesting results:&lt;/p&gt; &#xA;&lt;p&gt;Generated images with prompt &lt;code&gt;photo of a red sks container&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/a-red-sks-container-0021.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Generated images with prompt &lt;code&gt;a dog on top of sks container&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/a-dog-on-top-of-sks-container-0023.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openai/whisper</title>
    <updated>2022-10-02T01:40:22Z</updated>
    <id>tag:github.com,2022-10-02:/openai/whisper</id>
    <link href="https://github.com/openai/whisper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Robust Speech Recognition via Large-Scale Weak Supervision&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Whisper&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openai.com/blog/whisper&#34;&gt;[Blog]&lt;/a&gt; &lt;a href=&#34;https://cdn.openai.com/papers/whisper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/openai/whisper/main/model-card.md&#34;&gt;[Model card]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb&#34;&gt;[Colab example]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.&lt;/p&gt; &#xA;&lt;h2&gt;Approach&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openai/whisper/main/approach.png&#34; alt=&#34;Approach&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. All of these tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing for a single model to replace many different stages of a traditional speech processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;We used Python 3.9.9 and &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; 1.10.1 to train and test our models, but the codebase is expected to be compatible with Python 3.7 or later and recent PyTorch versions. The codebase also depends on a few Python packages, most notably &lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;HuggingFace Transformers&lt;/a&gt; for their fast tokenizer implementation and &lt;a href=&#34;https://github.com/kkroening/ffmpeg-python&#34;&gt;ffmpeg-python&lt;/a&gt; for reading audio files. The following command will pull and install the latest commit from this repository, along with its Python dependencies&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/openai/whisper.git &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It also requires the command-line tool &lt;a href=&#34;https://ffmpeg.org/&#34;&gt;&lt;code&gt;ffmpeg&lt;/code&gt;&lt;/a&gt; to be installed on your system, which is available from most package managers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# on Ubuntu or Debian&#xA;sudo apt update &amp;amp;&amp;amp; sudo apt install ffmpeg&#xA;&#xA;# on Arch Linux&#xA;sudo pacman -S ffmpeg&#xA;&#xA;# on MacOS using Homebrew (https://brew.sh/)&#xA;brew install ffmpeg&#xA;&#xA;# on Windows using Chocolatey (https://chocolatey.org/)&#xA;choco install ffmpeg&#xA;&#xA;# on Windows using Scoop (https://scoop.sh/)&#xA;scoop install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may need &lt;a href=&#34;http://rust-lang.org&#34;&gt;&lt;code&gt;rust&lt;/code&gt;&lt;/a&gt; installed as well, in case &lt;a href=&#34;https://pypi.org/project/tokenizers/&#34;&gt;tokenizers&lt;/a&gt; does not provide a pre-built wheel for your platform. If you see installation errors during the &lt;code&gt;pip install&lt;/code&gt; command above, please follow the &lt;a href=&#34;https://www.rust-lang.org/learn/get-started&#34;&gt;Getting started page&lt;/a&gt; to install Rust development environment. Additionally, you may need to configure the &lt;code&gt;PATH&lt;/code&gt; environment variable, e.g. &lt;code&gt;export PATH=&#34;$HOME/.cargo/bin:$PATH&#34;&lt;/code&gt;. If the installation fails with &lt;code&gt;No module named &#39;setuptools_rust&#39;&lt;/code&gt;, you need to install &lt;code&gt;setuptools_rust&lt;/code&gt;, e.g. by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install setuptools-rust&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Available models and languages&lt;/h2&gt; &#xA;&lt;p&gt;There are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. Below are the names of the available models and their approximate memory requirements and relative speed.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Parameters&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;English-only model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Multilingual model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Required VRAM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Relative speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tiny&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;tiny.en&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;tiny&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~1 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~32x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;base.en&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~1 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~16x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;small&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;244 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;small.en&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;small&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~2 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~6x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;medium&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;769 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;medium.en&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;medium&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~5 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~2x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1550 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;large&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;~10 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For English-only applications, the &lt;code&gt;.en&lt;/code&gt; models tend to perform better, especially for the &lt;code&gt;tiny.en&lt;/code&gt; and &lt;code&gt;base.en&lt;/code&gt; models. We observed that the difference becomes less significant for the &lt;code&gt;small.en&lt;/code&gt; and &lt;code&gt;medium.en&lt;/code&gt; models.&lt;/p&gt; &#xA;&lt;p&gt;Whisper&#39;s performance varies widely depending on the language. The figure below shows a WER breakdown by languages of Fleurs dataset, using the &lt;code&gt;large&lt;/code&gt; model. More WER and BLEU scores corresponding to the other models and datasets can be found in Appendix D in &lt;a href=&#34;https://cdn.openai.com/papers/whisper.pdf&#34;&gt;the paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openai/whisper/main/language-breakdown.svg?sanitize=true&#34; alt=&#34;WER breakdown by language&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Command-line usage&lt;/h2&gt; &#xA;&lt;p&gt;The following command will transcribe speech in audio files, using the &lt;code&gt;medium&lt;/code&gt; model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisper audio.flac audio.mp3 audio.wav --model medium&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default setting (which selects the &lt;code&gt;small&lt;/code&gt; model) works well for transcribing English. To transcribe an audio file containing non-English speech, you can specify the language using the &lt;code&gt;--language&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisper japanese.wav --language Japanese&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Adding &lt;code&gt;--task translate&lt;/code&gt; will translate the speech into English:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisper japanese.wav --language Japanese --task translate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the following to view all available options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisper --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/openai/whisper/main/whisper/tokenizer.py&#34;&gt;tokenizer.py&lt;/a&gt; for the list of all available languages.&lt;/p&gt; &#xA;&lt;h2&gt;Python usage&lt;/h2&gt; &#xA;&lt;p&gt;Transcription can also be performed within Python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import whisper&#xA;&#xA;model = whisper.load_model(&#34;base&#34;)&#xA;result = model.transcribe(&#34;audio.mp3&#34;)&#xA;print(result[&#34;text&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Internally, the &lt;code&gt;transcribe()&lt;/code&gt; method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window.&lt;/p&gt; &#xA;&lt;p&gt;Below is an example usage of &lt;code&gt;whisper.detect_language()&lt;/code&gt; and &lt;code&gt;whisper.decode()&lt;/code&gt; which provide lower-level access to the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import whisper&#xA;&#xA;model = whisper.load_model(&#34;base&#34;)&#xA;&#xA;# load audio and pad/trim it to fit 30 seconds&#xA;audio = whisper.load_audio(&#34;audio.mp3&#34;)&#xA;audio = whisper.pad_or_trim(audio)&#xA;&#xA;# make log-Mel spectrogram and move to the same device as the model&#xA;mel = whisper.log_mel_spectrogram(audio).to(model.device)&#xA;&#xA;# detect the spoken language&#xA;_, probs = model.detect_language(mel)&#xA;print(f&#34;Detected language: {max(probs, key=probs.get)}&#34;)&#xA;&#xA;# decode the audio&#xA;options = whisper.DecodingOptions()&#xA;result = whisper.decode(model, mel, options)&#xA;&#xA;# print the recognized text&#xA;print(result.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;More examples&lt;/h2&gt; &#xA;&lt;p&gt;Please use the &lt;a href=&#34;https://github.com/openai/whisper/discussions/categories/show-and-tell&#34;&gt;🙌 Show and tell&lt;/a&gt; category in Discussions for sharing more example usages of Whisper and third-party extensions such as web demos, integrations with other tools, ports for different platforms, etc.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code and the model weights of Whisper are released under the MIT License. See &lt;a href=&#34;https://raw.githubusercontent.com/openai/whisper/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for further details.&lt;/p&gt;</summary>
  </entry>
</feed>