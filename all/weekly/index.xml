<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-11T01:40:28Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AykutSarac/jsoncrack.com</title>
    <updated>2022-09-11T01:40:28Z</updated>
    <id>tag:github.com,2022-09-11:/AykutSarac/jsoncrack.com</id>
    <link href="https://github.com/AykutSarac/jsoncrack.com" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üîÆ Seamlessly visualize your JSON data instantly into graphs; paste, import or fetch!&lt;/p&gt;&lt;hr&gt;&lt;center&gt; &#xA; &lt;a href=&#34;https://jsoncrack.com&#34;&gt; &lt;img width=&#34;1080&#34; alt=&#34;jsoncrack&#34; src=&#34;https://user-images.githubusercontent.com/47941171/187418000-8edea92b-b3ac-4b07-9c4c-e42f6763817d.png&#34;&gt; &lt;/a&gt; &#xA;&lt;/center&gt; &#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/yVyTtCRueq&#34;&gt; &lt;img alt=&#34;github sponsors&#34; src=&#34;https://dcbadge.vercel.app/api/server/yVyTtCRueq?style=flat-square&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://app.travis-ci.com/github/AykutSarac/jsoncrack.com&#34;&gt; &lt;img alt=&#34;travis ci badge&#34; src=&#34;https://img.shields.io/travis/com/AykutSarac/jsoncrack.com/main?style=flat-square&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/AykutSarac/jsoncrack.com/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;license badge&#34; src=&#34;https://img.shields.io/github/license/AykutSarac/jsoncrack.com?style=flat-square&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/AykutSarac/jsoncrack.com/releases&#34;&gt; &lt;img alt=&#34;version badge&#34; src=&#34;https://img.shields.io/github/package-json/v/AykutSarac/jsoncrack.com?color=brightgreen&amp;amp;style=flat-square&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/AykutSarac&#34;&gt; &lt;img alt=&#34;github sponsors&#34; src=&#34;https://img.shields.io/github/sponsors/AykutSarac?style=flat-square&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;i&gt;Simple json visualization tool for your data.&lt;/i&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.producthunt.com/posts/json-crack?utm_source=badge-featured&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-json-crack&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=332281&amp;amp;theme=light&#34; alt=&#34;JSON Crack - Simple visualization tool for your JSON data. | Product Hunt&#34; style=&#34;width: 250px; height: 54px;&#34; width=&#34;250&#34; height=&#34;54&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;800&#34; src=&#34;https://raw.githubusercontent.com/AykutSarac/jsoncrack.com/main/public/jsoncrack-screenshot.webp&#34; alt=&#34;preview 1&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;JSON Crack (jsoncrack.com)&lt;/h1&gt; &#xA;&lt;p&gt;JSON Crack is a tool that generates graph diagrams from JSON objects. These diagrams are much easier to navigate than the textual format and to make it even more convenient, the tool also allows you to search the nodes. Additionally, the generated diagrams can also be downloaded or clipboard as image.&lt;/p&gt; &#xA;&lt;p&gt;You can use the web version at &lt;a href=&#34;https://jsoncrack.com&#34;&gt;jsoncrack.com&lt;/a&gt; or also run it locally as &lt;a href=&#34;https://github.com/AykutSarac/jsoncrack.com#-docker&#34;&gt;Docker container&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;b&gt;&lt;a href=&#34;https://jsoncrack.com&#34;&gt;JSON Crack - Crack your data into pieces&lt;/a&gt;&lt;/b&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;‚ö°Ô∏è Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Search Nodes&lt;/li&gt; &#xA; &lt;li&gt;Share links &amp;amp; Create Embed Widgets&lt;/li&gt; &#xA; &lt;li&gt;Download/Clipboard as image&lt;/li&gt; &#xA; &lt;li&gt;Upload JSON locally or fetch from URL&lt;/li&gt; &#xA; &lt;li&gt;Great UI/UX&lt;/li&gt; &#xA; &lt;li&gt;Light/Dark Mode&lt;/li&gt; &#xA; &lt;li&gt;Advanced Error Messages&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üõ† Development Setup&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;  npm install --legacy-peer-deps&#xA;  npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üê≥ Docker&lt;/h2&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://raw.githubusercontent.com/AykutSarac/jsoncrack.com/main/Dockerfile&#34;&gt;&lt;code&gt;Dockerfile&lt;/code&gt;&lt;/a&gt; is provided in the root of the repository. If you want to run JSON Crack locally:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build a Docker image with &lt;code&gt;docker build -t jsoncrack .&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run locally with &lt;code&gt;docker run -p 8888:8080 jsoncrack&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Go to &lt;a href=&#34;http://localhost:8888&#34;&gt;http://localhost:8888&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is open source and available under the &lt;a href=&#34;https://raw.githubusercontent.com/AykutSarac/jsoncrack.com/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sd-webui/stable-diffusion-webui</title>
    <updated>2022-09-11T01:40:28Z</updated>
    <id>tag:github.com,2022-09-11:/sd-webui/stable-diffusion-webui</id>
    <link href="https://github.com/sd-webui/stable-diffusion-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stable Diffusion web UI&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/altryne/sd-webui-colab/blob/main/Stable_Diffusion_WebUi_Altryne.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installation instructions for &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/wiki/Installation&#34;&gt;Windows&lt;/a&gt;, &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/wiki/Linux-Automated-Setup-Guide&#34;&gt;Linux&lt;/a&gt;, or &lt;a href=&#34;https://colab.research.google.com/github/altryne/sd-webui-colab/blob/main/Stable_Diffusion_WebUi_Altryne.ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h3&gt;Have an &lt;strong&gt;issue&lt;/strong&gt;?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If the issue involves &lt;em&gt;a bug&lt;/em&gt; in &lt;strong&gt;textual-inversion&lt;/strong&gt; create the issue on &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui&#34;&gt;sd-webui/stable-diffusion-webui&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you want to know how to &lt;strong&gt;activate&lt;/strong&gt; or &lt;strong&gt;use&lt;/strong&gt; textual-inversion see &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/hlky/sd-enable-textual-inversion&#34;&gt;hlky/sd-enable-textual-inversion&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;. Activation not working? create the issue on &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui&#34;&gt;sd-webui/stable-diffusion-webui&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Want to contribute?&lt;/h3&gt; &#xA;&lt;p&gt;Gradio version (stable)&lt;/p&gt; &#xA;&lt;p&gt;Open new Pull Requests on &lt;code&gt;dev&lt;/code&gt; branch!&lt;/p&gt; &#xA;&lt;p&gt;for Gradio check out the &lt;a href=&#34;https://gradio.app/docs/&#34;&gt;docs&lt;/a&gt; to contribute&lt;/p&gt; &#xA;&lt;p&gt;Have an issue or feature request with Gradio? open a issue/feature request on github for support: &lt;a href=&#34;https://github.com/gradio-app/gradio/issues&#34;&gt;https://github.com/gradio-app/gradio/issues&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Need more support with Gradio? We have a discord channel called &lt;code&gt;gradio-stable-diffusion&lt;/code&gt; for Q&amp;amp;A with the gradio authors, to join use this link &lt;a href=&#34;https://discord.gg/Qs8AsnX7Jd&#34;&gt;https://discord.gg/Qs8AsnX7Jd&lt;/a&gt;, then go to &lt;code&gt;role-assignment&lt;/code&gt; and click gradio to join the &lt;code&gt;gradio&lt;/code&gt; channels.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;New features can be added to Gradio or Streamlit versions&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;More documentation about features, troubleshooting, common issues very soon&lt;/h2&gt; &#xA;&lt;h3&gt;Want to help with documentation? Documented something? Use &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/discussions&#34;&gt;Discussions&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Important&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;üî• NEW! webui.cmd updates with any changes in environment.yaml file so the environment will always be up to date as long as you get the new environment.yaml file üî•&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üî•&lt;/span&gt; no need to remove environment, delete src folder and create again, MUCH simpler! üî•&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Questions about &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/wiki/Upscalers&#34;&gt;Upscalers&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;?&lt;/h3&gt; &#xA;&lt;h3&gt;Questions about &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/wiki/Optimized-mode&#34;&gt;Optimized mode&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;?&lt;/h3&gt; &#xA;&lt;h3&gt;Questions about &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/wiki/Command-line-options&#34;&gt;Command line options&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;?&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gradio GUI: Idiot-proof, fully featured frontend for both txt2img and img2img generation&lt;/li&gt; &#xA; &lt;li&gt;No more manually typing parameters, now all you have to do is write your prompt and adjust sliders&lt;/li&gt; &#xA; &lt;li&gt;GFPGAN Face Correction üî•: &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui#gfpgan&#34;&gt;Download the model&lt;/a&gt;Automatically correct distorted faces with a built-in GFPGAN option, fixes them in less than half a second&lt;/li&gt; &#xA; &lt;li&gt;RealESRGAN Upscaling üî•: &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui#realesrgan&#34;&gt;Download the models&lt;/a&gt; Boosts the resolution of images with a built-in RealESRGAN option&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üíª&lt;/span&gt; esrgan/gfpgan on cpu support &lt;span&gt;üíª&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Textual inversion üî•: &lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;info&lt;/a&gt; - requires enabling, see &lt;a href=&#34;https://github.com/hlky/sd-enable-textual-inversion&#34;&gt;here&lt;/a&gt;, script works as usual without it enabled&lt;/li&gt; &#xA; &lt;li&gt;Advanced img2img editor &lt;span&gt;üé®&lt;/span&gt; &lt;span&gt;üî•&lt;/span&gt; &lt;span&gt;üé®&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üî•&lt;/span&gt;&lt;span&gt;üî•&lt;/span&gt; Mask and crop &lt;span&gt;üî•&lt;/span&gt;&lt;span&gt;üî•&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mask painting (NEW) üñåÔ∏è: Powerful tool for re-generating only specific parts of an image you want to change&lt;/li&gt; &#xA; &lt;li&gt;More k_diffusion samplers üî•üî• : Far greater quality outputs than the default sampler, less distortion and more accurate&lt;/li&gt; &#xA; &lt;li&gt;txt2img samplers: &#34;DDIM&#34;, &#34;PLMS&#34;, &#39;k_dpm_2_a&#39;, &#39;k_dpm_2&#39;, &#39;k_euler_a&#39;, &#39;k_euler&#39;, &#39;k_heun&#39;, &#39;k_lms&#39;&lt;/li&gt; &#xA; &lt;li&gt;img2img samplers: &#34;DDIM&#34;, &#39;k_dpm_2_a&#39;, &#39;k_dpm_2&#39;, &#39;k_euler_a&#39;, &#39;k_euler&#39;, &#39;k_heun&#39;, &#39;k_lms&#39;&lt;/li&gt; &#xA; &lt;li&gt;Loopback (NEW) ‚ûø: Automatically feed the last generated sample back into img2img&lt;/li&gt; &#xA; &lt;li&gt;Prompt Weighting (NEW) üèãÔ∏è: Adjust the strength of different terms in your prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üî•&lt;/span&gt; gpu device selectable with --gpu &#xA;  &lt;id&gt; &#xA;   &lt;span&gt;üî•&lt;/span&gt;&#xA;  &lt;/id&gt;&lt;/li&gt; &#xA; &lt;li&gt;Memory Monitoring üî•: Shows Vram usage and generation time after outputting.&lt;/li&gt; &#xA; &lt;li&gt;Word Seeds üî•: Use words instead of seed numbers&lt;/li&gt; &#xA; &lt;li&gt;CFG: Classifier free guidance scale, a feature for fine-tuning your output&lt;/li&gt; &#xA; &lt;li&gt;Launcher Automatic üëëüî• shortcut to load the model, no more typing in Conda&lt;/li&gt; &#xA; &lt;li&gt;Lighter on Vram: 512x512 img2img &amp;amp; txt2img tested working on 6gb&lt;/li&gt; &#xA; &lt;li&gt;and ????&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Stable Diffusion web UI&lt;/h1&gt; &#xA;&lt;p&gt;A browser interface based on Gradio library for Stable Diffusion.&lt;/p&gt; &#xA;&lt;p&gt;Original script with Gradio UI was written by a kind anonymous user. This is a modification.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/sd-webui/stable-diffusion-webui/raw/master/images/txt2img.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://github.com/sd-webui/stable-diffusion-webui/raw/master/images/img2img.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://github.com/sd-webui/stable-diffusion-webui/raw/master/images/gfpgan.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://github.com/sd-webui/stable-diffusion-webui/raw/master/images/esrgan.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;GFPGAN&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use GFPGAN to improve generated faces, you need to install it separately. Download &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth&#34;&gt;GFPGANv1.3.pth&lt;/a&gt; and put it into the &lt;code&gt;/stable-diffusion-webui/src/gfpgan/experiments/pretrained_models&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;RealESRGAN&lt;/h3&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth&#34;&gt;RealESRGAN_x4plus.pth&lt;/a&gt; and &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth&#34;&gt;RealESRGAN_x4plus_anime_6B.pth&lt;/a&gt;. Put them into the &lt;code&gt;stable-diffusion-webui/src/realesrgan/experiments/pretrained_models&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;LDSR&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download &lt;strong&gt;LDSR&lt;/strong&gt; &lt;a href=&#34;https://heibox.uni-heidelberg.de/f/31a76b13ea27482981b4/?dl=1&#34;&gt;project.yaml&lt;/a&gt; and &lt;a href=&#34;https://heibox.uni-heidelberg.de/f/578df07c8fc04ffbadf3/?dl=1&#34;&gt; model last.cpkt&lt;/a&gt;. Rename last.ckpt to model.ckpt and place both under stable-diffusion-webui/src/latent-diffusion/experiments/pretrained_models/&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;p&gt;When launching, you may get a very long warning message related to some weights not being used. You may freely ignore it. After a while, you will get a message like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Running on local URL:  http://127.0.0.1:7860/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open the URL in browser, and you are good to go.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;The script creates a web UI for Stable Diffusion&#39;s txt2img and img2img scripts. Following are features added that are not in original script.&lt;/p&gt; &#xA;&lt;h3&gt;GFPGAN&lt;/h3&gt; &#xA;&lt;p&gt;Lets you improve faces in pictures using the GFPGAN model. There is a checkbox in every tab to use GFPGAN at 100%, and also a separate tab that just allows you to use GFPGAN on any picture, with a slider that controls how strongthe effect is.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/GFPGAN.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;RealESRGAN&lt;/h3&gt; &#xA;&lt;p&gt;Lets you double the resolution of generated images. There is a checkbox in every tab to use RealESRGAN, and you can choose between the regular upscaler and the anime version. There is also a separate tab for using RealESRGAN on any picture.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/RealESRGAN.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Sampling method selection&lt;/h3&gt; &#xA;&lt;p&gt;txt2img samplers: &#34;DDIM&#34;, &#34;PLMS&#34;, &#39;k_dpm_2_a&#39;, &#39;k_dpm_2&#39;, &#39;k_euler_a&#39;, &#39;k_euler&#39;, &#39;k_heun&#39;, &#39;k_lms&#39; img2img samplers: &#34;DDIM&#34;, &#39;k_dpm_2_a&#39;, &#39;k_dpm_2&#39;, &#39;k_euler_a&#39;, &#39;k_euler&#39;, &#39;k_heun&#39;, &#39;k_lms&#39;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/sampling.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Prompt matrix&lt;/h3&gt; &#xA;&lt;p&gt;Separate multiple prompts using the &lt;code&gt;|&lt;/code&gt; character, and the system will produce an image for every combination of them. For example, if you use &lt;code&gt;a busy city street in a modern city|illustration|cinematic lighting&lt;/code&gt; prompt, there are four combinations possible (first part of prompt is always kept):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;a busy city street in a modern city&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;a busy city street in a modern city, illustration&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;a busy city street in a modern city, cinematic lighting&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;a busy city street in a modern city, illustration, cinematic lighting&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Four images will be produced, in this order, all with same seed and each with corresponding prompt: &lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/prompt-matrix.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Another example, this time with 5 prompts and 16 variations: &lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/prompt_matrix.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you use this feature, batch count will be ignored, because the number of pictures to produce depends on your prompts, but batch size will still work (generating multiple pictures at the same time for a small speed boost).&lt;/p&gt; &#xA;&lt;h3&gt;Flagging (Broken after UI changed to gradio.Blocks() see &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/issues/50&#34;&gt;Flag button missing from new UI&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;Click the Flag button under the output section, and generated images will be saved to &lt;code&gt;log/images&lt;/code&gt; directory, and generation parameters will be appended to a csv file &lt;code&gt;log/log.csv&lt;/code&gt; in the &lt;code&gt;/sd&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;but every image is saved, why would I need this?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you&#39;re like me, you experiment a lot with prompts and settings, and only few images are worth saving. You can just save them using right click in browser, but then you won&#39;t be able to reproduce them later because you will not know what exact prompt created the image. If you use the flag button, generation paramerters will be written to csv file, and you can easily find parameters for an image by searching for its filename.&lt;/p&gt; &#xA;&lt;h3&gt;Copy-paste generation parameters&lt;/h3&gt; &#xA;&lt;p&gt;A text output provides generation parameters in an easy to copy-paste form for easy sharing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/kopipe.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you generate multiple pictures, the displayed seed will be the seed of the first one.&lt;/p&gt; &#xA;&lt;h3&gt;Correct seeds for batches&lt;/h3&gt; &#xA;&lt;p&gt;If you use a seed of 1000 to generate two batches of two images each, four generated images will have seeds: &lt;code&gt;1000, 1001, 1002, 1003&lt;/code&gt;. Previous versions of the UI would produce &lt;code&gt;1000, x, 1001, x&lt;/code&gt;, where x is an iamge that can&#39;t be generated by any seed.&lt;/p&gt; &#xA;&lt;h3&gt;Resizing&lt;/h3&gt; &#xA;&lt;p&gt;There are three options for resizing input images in img2img mode:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Just resize - simply resizes source image to target resolution, resulting in incorrect aspect ratio&lt;/li&gt; &#xA; &lt;li&gt;Crop and resize - resize source image preserving aspect ratio so that entirety of target resolution is occupied by it, and crop parts that stick out&lt;/li&gt; &#xA; &lt;li&gt;Resize and fill - resize source image preserving aspect ratio so that it entirely fits target resolution, and fill empty space by rows/columns from source image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example: &lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/resizing.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Loading&lt;/h3&gt; &#xA;&lt;p&gt;Gradio&#39;s loading graphic has a very negative effect on the processing speed of the neural network. My RTX 3090 makes images about 10% faster when the tab with gradio is not active. By default, the UI now hides loading progress animation and replaces it with static &#34;Loading...&#34; text, which achieves the same effect. Use the --no-progressbar-hiding commandline option to revert this and show loading animations.&lt;/p&gt; &#xA;&lt;h3&gt;Prompt validation&lt;/h3&gt; &#xA;&lt;p&gt;Stable Diffusion has a limit for input text length. If your prompt is too long, you will get a warning in the text output field, showing which parts of your text were truncated and ignored by the model.&lt;/p&gt; &#xA;&lt;h3&gt;Loopback&lt;/h3&gt; &#xA;&lt;p&gt;A checkbox for img2img allowing to automatically feed output image as input for the next batch. Equivalent to saving output image, and replacing input image with it. Batch count setting controls how many iterations of this you get.&lt;/p&gt; &#xA;&lt;p&gt;Usually, when doing this, you would choose one of many images for the next iteration yourself, so the usefulness of this feature may be questionable, but I&#39;ve managed to get some very nice outputs with it that I wasn&#39;t abble to get otherwise.&lt;/p&gt; &#xA;&lt;p&gt;Example: (cherrypicked result; original picture by anon)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/loopback.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;--help&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --outdir [OUTDIR]     dir to write results to&#xA;  --outdir_txt2img [OUTDIR_TXT2IMG]&#xA;                        dir to write txt2img results to (overrides --outdir)&#xA;  --outdir_img2img [OUTDIR_IMG2IMG]&#xA;                        dir to write img2img results to (overrides --outdir)&#xA;  --save-metadata       Whether to embed the generation parameters in the sample images&#xA;  --skip-grid           do not save a grid, only individual samples. Helpful when evaluating lots of samples&#xA;  --skip-save           do not save indiviual samples. For speed measurements.&#xA;  --n_rows N_ROWS       rows in the grid; use -1 for autodetect and 0 for n_rows to be same as batch_size (default:&#xA;                        -1)&#xA;  --config CONFIG       path to config which constructs model&#xA;  --ckpt CKPT           path to checkpoint of model&#xA;  --precision {full,autocast}&#xA;                        evaluate at this precision&#xA;  --gfpgan-dir GFPGAN_DIR&#xA;                        GFPGAN directory&#xA;  --realesrgan-dir REALESRGAN_DIR&#xA;                        RealESRGAN directory&#xA;  --realesrgan-model REALESRGAN_MODEL&#xA;                        Upscaling model for RealESRGAN&#xA;  --no-verify-input     do not verify input to check if it&#39;s too long&#xA;  --no-half             do not switch the model to 16-bit floats&#xA;  --no-progressbar-hiding&#xA;                        do not hide progressbar in gradio UI (we hide it because it slows down ML if you have hardware&#xA;                        accleration in browser)&#xA;  --defaults DEFAULTS   path to configuration file providing UI defaults, uses same format as cli parameter&#xA;  --gpu GPU             choose which GPU to use if you have multiple&#xA;  --extra-models-cpu    run extra models (GFGPAN/ESRGAN) on cpu&#xA;  --esrgan-cpu          run ESRGAN on cpu&#xA;  --gfpgan-cpu          run GFPGAN on cpu&#xA;  --cli CLI             don&#39;t launch web server, take Python function kwargs from this file.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Stable Diffusion was made possible thanks to a collaboration with &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt; and &lt;a href=&#34;https://runwayml.com/&#34;&gt;Runway&lt;/a&gt; and builds upon our previous work:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/rromb&#34;&gt;Robin Rombach&lt;/a&gt;*, &lt;a href=&#34;https://github.com/ablattmann&#34;&gt;Andreas Blattmann&lt;/a&gt;*, &lt;a href=&#34;https://github.com/qp-qp&#34;&gt;Dominik Lorenz&lt;/a&gt;, &lt;a href=&#34;https://github.com/pesser&#34;&gt;Patrick Esser&lt;/a&gt;, &lt;a href=&#34;https://hci.iwr.uni-heidelberg.de/Staff/bommer&#34;&gt;Bj√∂rn Ommer&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CVPR &#39;22 Oral&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;which is available on &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;GitHub&lt;/a&gt;. PDF at &lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;arXiv&lt;/a&gt;. Please also visit our &lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;Project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/assets/stable-samples/txt2img/merged-0006.png&#34; alt=&#34;txt2img-stable2&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/#stable-diffusion-v1&#34;&gt;Stable Diffusion&lt;/a&gt; is a latent text-to-image diffusion model. Thanks to a generous compute donation from &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt; and support from &lt;a href=&#34;https://laion.ai/&#34;&gt;LAION&lt;/a&gt;, we were able to train a Latent Diffusion Model on 512x512 images from a subset of the &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-5B&lt;/a&gt; database. Similar to Google&#39;s &lt;a href=&#34;https://arxiv.org/abs/2205.11487&#34;&gt;Imagen&lt;/a&gt;, this model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. See &lt;a href=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/#stable-diffusion-v1&#34;&gt;this section&lt;/a&gt; below and the &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion&#34;&gt;model card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Stable Diffusion v1&lt;/h2&gt; &#xA;&lt;p&gt;Stable Diffusion v1 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 860M UNet and CLIP ViT-L/14 text encoder for the diffusion model. The model was pretrained on 256x256 images and then finetuned on 512x512 images.&lt;/p&gt; &#xA;&lt;p&gt;*Note: Stable Diffusion v1 is a general text-to-image diffusion model and therefore mirrors biases and (mis-)conceptions that are present in its training data. Details on the training procedure and data, as well as the intended use of the model can be found in the corresponding &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion&#34;&gt;model card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Comments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Our codebase for the diffusion models builds heavily on &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;OpenAI&#39;s ADM codebase&lt;/a&gt; and &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;https://github.com/lucidrains/denoising-diffusion-pytorch&lt;/a&gt;. Thanks for open-sourcing!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The implementation of the transformer encoder is from &lt;a href=&#34;https://github.com/lucidrains/x-transformers&#34;&gt;x-transformers&lt;/a&gt; by &lt;a href=&#34;https://github.com/lucidrains?tab=repositories&#34;&gt;lucidrains&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;      title={High-Resolution Image Synthesis with Latent Diffusion Models}, &#xA;      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Bj√∂rn Ommer},&#xA;      year={2021},&#xA;      eprint={2112.10752},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>AUTOMATIC1111/stable-diffusion-webui</title>
    <updated>2022-09-11T01:40:28Z</updated>
    <id>tag:github.com,2022-09-11:/AUTOMATIC1111/stable-diffusion-webui</id>
    <link href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stable Diffusion web UI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stable Diffusion web UI&lt;/h1&gt; &#xA;&lt;p&gt;A browser interface based on Gradio library for Stable Diffusion.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/screenshot.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Feature showcase&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui-feature-showcase&#34;&gt;Detailed feature showcase with images, art by Greg Rutkowski&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Original txt2img and img2img modes&lt;/li&gt; &#xA; &lt;li&gt;One click install and run script (but you still must install python and git)&lt;/li&gt; &#xA; &lt;li&gt;Outpainting&lt;/li&gt; &#xA; &lt;li&gt;Inpainting&lt;/li&gt; &#xA; &lt;li&gt;Prompt matrix&lt;/li&gt; &#xA; &lt;li&gt;Stable Diffusion upscale&lt;/li&gt; &#xA; &lt;li&gt;Attention&lt;/li&gt; &#xA; &lt;li&gt;Loopback&lt;/li&gt; &#xA; &lt;li&gt;X/Y plot&lt;/li&gt; &#xA; &lt;li&gt;Textual Inversion&lt;/li&gt; &#xA; &lt;li&gt;Extras tab with: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GFPGAN, neural network that fixes faces&lt;/li&gt; &#xA;   &lt;li&gt;RealESRGAN, neural network upscaler&lt;/li&gt; &#xA;   &lt;li&gt;ESRGAN, neural network with a lot of third party models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Resizing aspect ratio options&lt;/li&gt; &#xA; &lt;li&gt;Sampling method selection&lt;/li&gt; &#xA; &lt;li&gt;Interrupt processing at any time&lt;/li&gt; &#xA; &lt;li&gt;4GB videocard support&lt;/li&gt; &#xA; &lt;li&gt;Correct seeds for batches&lt;/li&gt; &#xA; &lt;li&gt;Prompt length validation&lt;/li&gt; &#xA; &lt;li&gt;Generation parameters added as text to PNG&lt;/li&gt; &#xA; &lt;li&gt;Tab to view an existing picture&#39;s generation parameters&lt;/li&gt; &#xA; &lt;li&gt;Settings page&lt;/li&gt; &#xA; &lt;li&gt;Running custom code from UI&lt;/li&gt; &#xA; &lt;li&gt;Mouseover hints fo most UI elements&lt;/li&gt; &#xA; &lt;li&gt;Possible to change defaults/mix/max/step values for UI elements via text config&lt;/li&gt; &#xA; &lt;li&gt;Random artist button&lt;/li&gt; &#xA; &lt;li&gt;Tiling support: UI checkbox to create images that can be tiled like textures&lt;/li&gt; &#xA; &lt;li&gt;Progress bar and live image generation preview&lt;/li&gt; &#xA; &lt;li&gt;Negative prompt&lt;/li&gt; &#xA; &lt;li&gt;Styles&lt;/li&gt; &#xA; &lt;li&gt;Variations&lt;/li&gt; &#xA; &lt;li&gt;Seed resizing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installing and running&lt;/h2&gt; &#xA;&lt;p&gt;You need &lt;a href=&#34;https://www.python.org/downloads/windows/&#34;&gt;python&lt;/a&gt; and &lt;a href=&#34;https://git-scm.com/download/win&#34;&gt;git&lt;/a&gt; installed to run this, and an NVidia videocard.&lt;/p&gt; &#xA;&lt;p&gt;You need &lt;code&gt;model.ckpt&lt;/code&gt;, Stable Diffusion model checkpoint, a big file containing the neural network weights. You can obtain it from the following places:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v-1-4-original&#34;&gt;official download&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.yerf.org/wl/?id=EBfTrmcCCUAGaQBXVIj5lJmEhjoP1tgl&#34;&gt;file storage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;magnet:?xt=urn:btih:3a4a612d75ed088ea542acac52f9f45987488d1c&amp;amp;dn=sd-v1-4.ckpt&amp;amp;tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&amp;amp;tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You optionally can use GFPGAN to improve faces, then you&#39;ll need to download the model from &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To use ESRGAN models, put them into ESRGAN directory in the same location as webui.py. A file will be loaded as model if it has .pth extension. Grab models from the &lt;a href=&#34;https://upscale.wiki/wiki/Model_Database&#34;&gt;Model Database&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Automatic installation/launch&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;install &lt;a href=&#34;https://www.python.org/downloads/windows/&#34;&gt;Python 3.10.6&lt;/a&gt; and check &#34;Add Python to PATH&#34; during installation. You must install this exact version.&lt;/li&gt; &#xA; &lt;li&gt;install &lt;a href=&#34;https://git-scm.com/download/win&#34;&gt;git&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;place &lt;code&gt;model.ckpt&lt;/code&gt; into webui directory, next to &lt;code&gt;webui.bat&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;em&gt;(optional)&lt;/em&gt;&lt;/em&gt; place &lt;code&gt;GFPGANv1.3.pth&lt;/code&gt; into webui directory, next to &lt;code&gt;webui.bat&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;run &lt;code&gt;webui-user.bat&lt;/code&gt; from Windows Explorer. Run it as normal user, &lt;em&gt;&lt;strong&gt;not&lt;/strong&gt;&lt;/em&gt; as administrator.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Troubleshooting&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;if your version of Python is not in PATH (or if another version is), edit &lt;code&gt;webui-user.bat&lt;/code&gt;, and modify the line &lt;code&gt;set PYTHON=python&lt;/code&gt; to say the full path to your python executable, for example: &lt;code&gt;set PYTHON=B:\soft\Python310\python.exe&lt;/code&gt;. You can do this for python, but not for git.&lt;/li&gt; &#xA; &lt;li&gt;if you get out of memory errors and your video-card has a low amount of VRAM (4GB), use custom parameter &lt;code&gt;set COMMANDLINE_ARGS&lt;/code&gt; (see section below) to enable appropriate optimization according to low VRAM guide below (for example, &lt;code&gt;set COMMANDLINE_ARGS=--medvram --opt-split-attention&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;to prevent the creation of virtual environment and use your system python, use custom parameter replacing &lt;code&gt;set VENV_DIR=-&lt;/code&gt; (see below).&lt;/li&gt; &#xA; &lt;li&gt;webui.bat installs requirements from files &lt;code&gt;requirements_versions.txt&lt;/code&gt;, which lists versions for modules specifically compatible with Python 3.10.6. If you choose to install for a different version of python, using custom parameter &lt;code&gt;set REQS_FILE=requirements.txt&lt;/code&gt; may help (but I still recommend you to just use the recommended version of python).&lt;/li&gt; &#xA; &lt;li&gt;if you feel you broke something and want to reinstall from scratch, delete directories: &lt;code&gt;venv&lt;/code&gt;, &lt;code&gt;repositories&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;if you get a green or black screen instead of generated pictures, you have a card that doesn&#39;t support half precision floating point numbers (Known issue with 16xx cards). You must use &lt;code&gt;--precision full --no-half&lt;/code&gt; in addition to command line arguments (set them using &lt;code&gt;set COMMANDLINE_ARGS&lt;/code&gt;, see below), and the model will take much more space in VRAM (you will likely have to also use at least &lt;code&gt;--medvram&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;installer creates python virtual environment, so none of installed modules will affect your system installation of python if you had one prior to installing this.&lt;/li&gt; &#xA; &lt;li&gt;About &lt;em&gt;&#34;You must install this exact version&#34;&lt;/em&gt; from the instructions above: you can use any version of python you like, and it will likely work, but if you want to seek help about things not working, I will not offer help unless you this exact version for my sanity.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;How to run with custom parameters&lt;/h4&gt; &#xA;&lt;p&gt;It&#39;s possible to edit &lt;code&gt;set COMMANDLINE_ARGS=&lt;/code&gt; line in &lt;code&gt;webui.bat&lt;/code&gt; to run the program with different command line arguments, but that may lead to inconveniences when the file is updated in the repository.&lt;/p&gt; &#xA;&lt;p&gt;The recommndended way is to use another .bat file named anything you like, set the parameters you want in it, and run webui.bat from it. A &lt;code&gt;webui-user.bat&lt;/code&gt; file included into the repository does exactly this.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example that runs the prgoram with &lt;code&gt;--opt-split-attention&lt;/code&gt; argument:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;@echo off&#xA;&#xA;set COMMANDLINE_ARGS=--opt-split-attention&#xA;&#xA;call webui.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Another example, this file will run the program with custom python path, a different model named &lt;code&gt;a.ckpt&lt;/code&gt; and without virtual environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;@echo off&#xA;&#xA;set PYTHON=b:/soft/Python310/Python.exe&#xA;set VENV_DIR=-&#xA;set COMMANDLINE_ARGS=--ckpt a.ckpt&#xA;&#xA;call webui.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;What options to use for low VRAM video-cards?&lt;/h3&gt; &#xA;&lt;p&gt;You can, through command line arguments, enable the various optimizations which sacrifice some/a lot of speed in favor of using less VRAM. Those arguments are added to the &lt;code&gt;COMMANDLINE_ARGS&lt;/code&gt; parameter, see section above.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s a list of optimization arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have 4GB VRAM and want to make 512x512 (or maybe up to 640x640) images, use &lt;code&gt;--medvram&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you have 4GB VRAM and want to make 512x512 images, but you get an out of memory error with &lt;code&gt;--medvram&lt;/code&gt;, use &lt;code&gt;--medvram --opt-split-attention&lt;/code&gt; instead.&lt;/li&gt; &#xA; &lt;li&gt;If you have 4GB VRAM and want to make 512x512 images, and you still get an out of memory error, use &lt;code&gt;--lowvram --always-batch-cond-uncond --opt-split-attention&lt;/code&gt; instead.&lt;/li&gt; &#xA; &lt;li&gt;If you have 4GB VRAM and want to make images larger than you can with &lt;code&gt;--medvram&lt;/code&gt;, use &lt;code&gt;--lowvram --opt-split-attention&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you have more VRAM and want to make larger images than you can usually make (for example 1024x1024 instead of 512x512), use &lt;code&gt;--medvram --opt-split-attention&lt;/code&gt;. You can use &lt;code&gt;--lowvram&lt;/code&gt; also but the effect will likely be barely noticeable.&lt;/li&gt; &#xA; &lt;li&gt;Otherwise, do not use any of those.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Running online&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;code&gt;--share&lt;/code&gt; option to run online. You will get a xxx.app.gradio link. This is the intended way to use the program in collabs.&lt;/p&gt; &#xA;&lt;p&gt;Use &lt;code&gt;--listen&lt;/code&gt; to make the server listen to network connections. This will allow computers on local newtork to access the UI, and if you configure port forwarding, also computers on the internet.&lt;/p&gt; &#xA;&lt;p&gt;Use &lt;code&gt;--port xxxx&lt;/code&gt; to make the server listen on a specific port, xxxx being the wanted port. Remember that all ports below 1024 needs root/admin rights, for this reason it is advised to use a port above 1024. Defaults to port 7860 if available.&lt;/p&gt; &#xA;&lt;h3&gt;Google collab&lt;/h3&gt; &#xA;&lt;p&gt;If you don&#39;t want or can&#39;t run locally, here is google collab that allows you to run the webui:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Iy-xW9t1-OQWhb0hNxueGij8phCyluOh&#34;&gt;https://colab.research.google.com/drive/1Iy-xW9t1-OQWhb0hNxueGij8phCyluOh&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Textual Inversion&lt;/h3&gt; &#xA;&lt;p&gt;To make use of pretrained embeddings, create &lt;code&gt;embeddings&lt;/code&gt; directory (in the same palce as &lt;code&gt;webui.py&lt;/code&gt;) and put your embeddings into it. They must be .pt files, each with only one trained embedding, and the filename (without .pt) will be the term you&#39;d use in prompt to get that embedding.&lt;/p&gt; &#xA;&lt;p&gt;As an example, I trained one for about 5000 steps: &lt;a href=&#34;https://files.catbox.moe/e2ui6r.pt&#34;&gt;https://files.catbox.moe/e2ui6r.pt&lt;/a&gt;; it does not produce very good results, but it does work. Download and rename it to Usada Pekora.pt, and put it into embeddings dir and use Usada Pekora in prompt.&lt;/p&gt; &#xA;&lt;h3&gt;How to change UI defaults?&lt;/h3&gt; &#xA;&lt;p&gt;After running once, a &lt;code&gt;ui-config.json&lt;/code&gt; file appears in webui directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;txt2img/Sampling Steps/value&#34;: 20,&#xA;    &#34;txt2img/Sampling Steps/minimum&#34;: 1,&#xA;    &#34;txt2img/Sampling Steps/maximum&#34;: 150,&#xA;    &#34;txt2img/Sampling Steps/step&#34;: 1,&#xA;    &#34;txt2img/Batch count/value&#34;: 1,&#xA;    &#34;txt2img/Batch count/minimum&#34;: 1,&#xA;    &#34;txt2img/Batch count/maximum&#34;: 32,&#xA;    &#34;txt2img/Batch count/step&#34;: 1,&#xA;    &#34;txt2img/Batch size/value&#34;: 1,&#xA;    &#34;txt2img/Batch size/minimum&#34;: 1,&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Edit values to your liking and the next time you launch the program they will be applied.&lt;/p&gt; &#xA;&lt;h3&gt;Manual installation&lt;/h3&gt; &#xA;&lt;p&gt;Alternatively, if you don&#39;t want to run webui.bat, here are instructions for installing everything by hand. This can run on both Windows and Linux (if you&#39;re on linux, use &lt;code&gt;ls&lt;/code&gt; instead of &lt;code&gt;dir&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# install torch with CUDA support. See https://pytorch.org/get-started/locally/ for more instructions if this fails.&#xA;pip install torch --extra-index-url https://download.pytorch.org/whl/cu113&#xA;&#xA;# check if torch supports GPU; this must output &#34;True&#34;. You need CUDA 11. installed for this. You might be able to use&#xA;# a different version, but this is what I tested.&#xA;python -c &#34;import torch; print(torch.cuda.is_available())&#34;&#xA;&#xA;# clone web ui and go into its directory&#xA;git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git&#xA;cd stable-diffusion-webui&#xA;&#xA;# clone repositories for Stable Diffusion and (optionally) CodeFormer&#xA;mkdir repositories&#xA;git clone https://github.com/CompVis/stable-diffusion.git repositories/stable-diffusion&#xA;git clone https://github.com/CompVis/taming-transformers.git repositories/taming-transformers&#xA;git clone https://github.com/sczhou/CodeFormer.git repositories/CodeFormer&#xA;&#xA;# install requirements of Stable Diffusion&#xA;pip install transformers==4.19.2 diffusers invisible-watermark --prefer-binary&#xA;&#xA;# install k-diffusion&#xA;pip install git+https://github.com/crowsonkb/k-diffusion.git --prefer-binary&#xA;&#xA;# (optional) install GFPGAN (face resoration)&#xA;pip install git+https://github.com/TencentARC/GFPGAN.git --prefer-binary&#xA;&#xA;# (optional) install requirements for CodeFormer (face resoration)&#xA;pip install -r repositories/CodeFormer/requirements.txt --prefer-binary&#xA;&#xA;# install requirements of web ui&#xA;pip install -r requirements.txt  --prefer-binary&#xA;&#xA;# update numpy to latest version&#xA;pip install -U numpy  --prefer-binary&#xA;&#xA;# (outside of command line) put stable diffusion model into web ui directory&#xA;# the command below must output something like: 1 File(s) 4,265,380,512 bytes&#xA;dir model.ckpt&#xA;&#xA;# (outside of command line) put the GFPGAN model into web ui directory&#xA;# the command below must output something like: 1 File(s) 348,632,874 bytes&#xA;dir GFPGANv1.3.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: the directory structure for manual instruction has been changed on 2022-09-09 to match automatic installation: previosuly webui was in a subdirectory of stable diffusion, now it&#39;s the reverse. If you followed manual installation before the chage, you can still use the program with you existing directory sctructure.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;After that the installation is finished.&lt;/p&gt; &#xA;&lt;p&gt;Run the command to start web ui:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python webui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have a 4GB video card, run the command with either &lt;code&gt;--lowvram&lt;/code&gt; or &lt;code&gt;--medvram&lt;/code&gt; argument:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python webui.py --medvram&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After a while, you will get a message like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Running on local URL:  http://127.0.0.1:7860/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open the URL in browser, and you are good to go.&lt;/p&gt; &#xA;&lt;h3&gt;Windows 11 WSL2 instructions&lt;/h3&gt; &#xA;&lt;p&gt;Alternatively, here are instructions for installing under Windows 11 WSL2 Linux distro, everything by hand:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# install conda (if not already done)&#xA;wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh&#xA;chmod +x Anaconda3-2022.05-Linux-x86_64.sh &#xA;./Anaconda3-2022.05-Linux-x86_64.sh&#xA;&#xA;# Clone webui repo&#xA;git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git&#xA;cd stable-diffusion-webui&#xA;&#xA;# Create and activate conda env&#xA;conda env create -f environment-wsl2.yaml&#xA;conda activate automatic&#xA;&#xA;# (optional) install requirements for GFPGAN (upscaling)&#xA;wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After that follow the instructions in the &lt;code&gt;Manual instructions&lt;/code&gt; section starting at step &lt;code&gt;:: clone repositories for Stable Diffusion and (optionally) CodeFormer&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stable Diffusion - &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;https://github.com/CompVis/stable-diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;https://github.com/CompVis/taming-transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;k-diffusion - &lt;a href=&#34;https://github.com/crowsonkb/k-diffusion.git&#34;&gt;https://github.com/crowsonkb/k-diffusion.git&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GFPGAN - &lt;a href=&#34;https://github.com/TencentARC/GFPGAN.git&#34;&gt;https://github.com/TencentARC/GFPGAN.git&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ESRGAN - &lt;a href=&#34;https://github.com/xinntao/ESRGAN&#34;&gt;https://github.com/xinntao/ESRGAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ideas for optimizations - &lt;a href=&#34;https://github.com/basujindal/stable-diffusion&#34;&gt;https://github.com/basujindal/stable-diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Cross Attention layer optimization - &lt;a href=&#34;https://github.com/Doggettx/stable-diffusion&#34;&gt;https://github.com/Doggettx/stable-diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Idea for SD upscale - &lt;a href=&#34;https://github.com/jquesnelle/txt2imghd&#34;&gt;https://github.com/jquesnelle/txt2imghd&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.&lt;/li&gt; &#xA; &lt;li&gt;(You)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>