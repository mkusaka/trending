<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-04T01:38:48Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>juspay/hyperswitch</title>
    <updated>2025-05-04T01:38:48Z</updated>
    <id>tag:github.com,2025-05-04:/juspay/hyperswitch</id>
    <link href="https://github.com/juspay/hyperswitch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open source payments switch written in Rust to make payments fast, reliable and affordable&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/hyperswitch-logo-dark.svg#gh-dark-mode-only&#34; alt=&#34;Hyperswitch-Logo&#34; width=&#34;40%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/hyperswitch-logo-light.svg#gh-light-mode-only&#34; alt=&#34;Hyperswitch-Logo&#34; width=&#34;40%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Open-Source Payments Orchestration&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  Single API to access the payments ecosystem and its features &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/juspay/hyperswitch/actions?query=workflow%3ACI+branch%3Amain&#34;&gt; &lt;img src=&#34;https://github.com/juspay/hyperswitch/workflows/CI-push/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/juspay/hyperswitch/raw/main/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/juspay/hyperswitch&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/juspay/hyperswitch/raw/main/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Made_in-Rust-orange&#34;&gt; &lt;/a&gt; &#xA; &lt;!-- Uncomment when we reach &gt;50% coverage --&gt; &#xA; &lt;!-- &lt;a href=&#34;https://codecov.io/github/juspay/hyperswitch&#34; &gt;&#xA;    &lt;img src=&#34;https://codecov.io/github/juspay/hyperswitch/graph/badge.svg&#34;/&gt;&#xA;  &lt;/a&gt; --&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.linkedin.com/company/hyperswitch/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/follow-hyperswitch-blue?logo=linkedin&amp;amp;labelColor=grey&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://x.com/hyperswitchio&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/follow-%40hyperswitchio-white?logo=x&amp;amp;labelColor=grey&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/hyperswitch-io/shared_invite/zt-2jqxmpsbm-WXUENx022HjNEy~Ark7Orw&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/chat-on_slack-blue?logo=slack&amp;amp;labelColor=grey&amp;amp;color=%233f0e40&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/#try-hyperswitch&#34;&gt;Try Hyperswitch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/#architectural-overview&#34;&gt;Architectural Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/#support-feature-requests&#34;&gt;Support, Feature requests &amp;amp; Bugs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/#our-vision&#34;&gt;Our Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/#versioning&#34;&gt;Versioning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/#copyright-and-license&#34;&gt;Copyright and License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/#introduction&#34;&gt; &lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt; &lt;/a&gt; Juspay, founded in 2012, is a global leader in payment orchestration and checkout solutions, trusted by 400+ leading enterprises and brands worldwide. Hyperswitch is Juspay&#39;s new generation of composable, commercial open-source payments platform for merchant and brands. It is an enterprise-grade, transparent and modular payments platform designed to provide digital businesses access to the best payments infrastructure. &#xA;&lt;p&gt;Here are the key components of Hyperswitch that deliver the whole solution:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/juspay/hyperswitch&#34;&gt;Hyperswitch Backend&lt;/a&gt;: Hyperswitch backend enables seamless payment processing with comprehensive support for various payment flows - authorization, authentication, void and capture workflows along with robust management of post-payment processes like refunds and chargeback handling. Additionally, Hyperswitch supports non-payment use cases by enabling connections with external FRM or authentication providers as part of the payment flow. The backend optimizes payment routing with customizable workflows, including success rate-based routing, rule-based routing, volume distribution, fallback handling, and intelligent retry mechanisms for failed payments based on specific error codes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/juspay/hyperswitch-web&#34;&gt;SDK (Frontend)&lt;/a&gt;: The SDK, available for web, &lt;a href=&#34;https://github.com/juspay/hyperswitch-client-core&#34;&gt;Android, and iOS&lt;/a&gt;, unifies the payment experience across various methods such as cards, wallets, BNPL, bank transfers, and more, while supporting the diverse payment flows of underlying PSPs. When paired with the locker, it surfaces the user&#39;s saved payment methods.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/juspay/hyperswitch-control-center&#34;&gt;Control Center&lt;/a&gt;: The Control Center enables users to manage the entire payments stack without any coding. It allows the creation of workflows for routing, payment retries, and defining conditions to invoke 3DS, fraud risk management (FRM), and surcharge modules. The Control Center provides access to transaction, refund, and chargeback operations across all integrated PSPs, transaction-level logs for initial debugging, and detailed analytics and insights into payment performance.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Read more at &lt;a href=&#34;https://docs.hyperswitch.io/&#34;&gt;Hyperswitch docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/#try-hyperswitch&#34;&gt; &lt;h2 id=&#34;try-hyperswitch&#34;&gt;Try Hyperswitch&lt;/h2&gt; &lt;/a&gt; &#xA;&lt;h3&gt;1. Local Setup&lt;/h3&gt; &#xA;&lt;h4&gt;One-Click Setup (Recommended)&lt;/h4&gt; &#xA;&lt;p&gt;You can run Hyperswitch on your system with a single command using our one-click setup script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone --depth 1 --branch latest https://github.com/juspay/hyperswitch&#xA;cd hyperswitch&#xA;scripts/setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above script will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check for prerequisites (Docker Compose/Podman)&lt;/li&gt; &#xA; &lt;li&gt;Set up necessary configurations&lt;/li&gt; &#xA; &lt;li&gt;Let you select a deployment profile: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Standard&lt;/strong&gt;: Recommended - App server + Control Center + Web SDK.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Full&lt;/strong&gt;: Standard + Monitoring + Scheduler.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Standalone App Server&lt;/strong&gt;: Core services only (Hyperswitch server, PostgreSQL, Redis)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Start the selected services&lt;/li&gt; &#xA; &lt;li&gt;Check service health&lt;/li&gt; &#xA; &lt;li&gt;Provide access information&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The next step is to &lt;a href=&#34;https://docs.hyperswitch.io/hyperswitch-open-source/account-setup/using-hyperswitch-control-center#add-a-payment-processor&#34;&gt;configure a connector&lt;/a&gt; with the Hyperswitch Control Center and &lt;a href=&#34;https://docs.hyperswitch.io/hyperswitch-open-source/account-setup/test-a-payment&#34;&gt;try a payment&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/try_local_system.md&#34;&gt;local setup guide&lt;/a&gt; for more details on setting up the entire stack or component wise.&lt;/p&gt; &#xA;&lt;h3&gt;2. Deployment on cloud&lt;/h3&gt; &#xA;&lt;p&gt;The fastest and easiest way to try Hyperswitch on AWS is via our CDK scripts&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Click on the following button for a quick standalone deployment on AWS, suitable for prototyping. No code or setup is required in your system and the deployment is covered within the AWS free-tier setup.&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=HyperswitchBootstarp&amp;amp;templateURL=https://hyperswitch-synth.s3.eu-central-1.amazonaws.com/hs-starter-config.yaml&#34;&gt;&lt;img src=&#34;https://github.com/juspay/hyperswitch/raw/main/docs/imgs/aws_button.png?raw=true&#34; height=&#34;35&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Sign-in to your AWS console.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Follow the instructions provided on the console to successfully deploy Hyperswitch. This takes 30-45mins and gives the following output&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Service&lt;/th&gt; &#xA;   &lt;th&gt;Host&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;App server running on&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;http://hyperswitch-&amp;lt;host-id.region&amp;gt;.elb.amazonaws.com&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HyperloaderJS Hosted at&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;http://&amp;lt;cloudfront.host-id&amp;gt;/0.103.1/v0/HyperLoader.js&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Control center server running on&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;http://hyperswitch-control-center-&amp;lt;host-id.region&amp;gt;.elb.amazonaws.com&lt;/code&gt;, Login with Email: &lt;code&gt;test@gmail.com&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hyperswitch Demo Store running on&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;http://hyperswitch-sdk-demo-&amp;lt;host-id.region&amp;gt;.elb.amazonaws.com&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Logs server running on&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;http://hyperswitch-logs-&amp;lt;host-id.region&amp;gt;.elb.amazonaws.com&lt;/code&gt;, Login with username: &lt;code&gt;admin&lt;/code&gt;, password: &lt;code&gt;admin&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We support deployment on GCP and Azure via Helm charts which takes 30-45mins. You can read more at &lt;a href=&#34;https://docs.hyperswitch.io/hyperswitch-open-source/deploy-on-kubernetes-using-helm&#34;&gt;Hyperswitch docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;3. Hosted Sandbox&lt;/h3&gt; &#xA;&lt;p&gt;You can experience the product by signing up for our &lt;a href=&#34;https://app.hyperswitch.io/&#34;&gt;hosted sandbox&lt;/a&gt;. The signup process accepts any email ID and provides access to the entire Control Center. You can set up connectors, define workflows for routing and retries, and even try payments from the dashboard.&lt;/p&gt; &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/#architectural-overview&#34;&gt; &lt;h2 id=&#34;architectural-overview&#34;&gt;Architectural Overview&lt;/h2&gt; &lt;/a&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/features.png&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/non-functional-features.png&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/hyperswitch-architecture-v1.png&#34;&gt; &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/support-feature-requests&#34;&gt; &lt;h2 id=&#34;support-feature-requests&#34;&gt;Support, Feature requests &amp;amp; Bugs&lt;/h2&gt; &lt;/a&gt; &#xA;&lt;p&gt;For any support, join the conversation in &lt;a href=&#34;https://join.slack.com/t/hyperswitch-io/shared_invite/zt-2jqxmpsbm-WXUENx022HjNEy~Ark7Orw&#34;&gt;Slack&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For new product features, enhancements, roadmap discussions, or to share queries and ideas, visit our &lt;a href=&#34;https://github.com/juspay/hyperswitch/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For reporting a bug, please read the issue guidelines and search for &lt;a href=&#34;https://github.com/juspay/hyperswitch/issues&#34;&gt;existing and closed issues&lt;/a&gt;. If your problem or idea is not addressed yet, please &lt;a href=&#34;https://github.com/juspay/hyperswitch/issues/new/choose&#34;&gt;open a new issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/our-vision&#34;&gt; &lt;h2 id=&#34;our-vision&#34;&gt;Our Vision&lt;/h2&gt; &lt;/a&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Linux for Payments&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Payments are evolving rapidly worldwide, with hundreds of processors, fraud detection systems, authentication modules, and new payment methods and flows emerging. Businesses building or managing their own payment stacks often face similar challenges, struggle with comparable issues, and find it hard to innovate at the desired pace.&lt;/p&gt; &#xA;&lt;p&gt;Hyperswitch serves as a well-architected designed reference platform, built on best-in-class design principles, empowering businesses to own and customize their payment stack. It provides a reusable core payments stack that can be tailored to specific requirements while relying on the Hyperswitch team for enhancements, support, and continuous innovation.&lt;/p&gt; &#xA;&lt;h3&gt;Our Values&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Embrace Payments Diversity: It will drive innovation in the ecosystem in multiple ways.&lt;/li&gt; &#xA; &lt;li&gt;Make it Open Source: Increases trust; Improves the quality and reusability of software.&lt;/li&gt; &#xA; &lt;li&gt;Be community driven: It enables participatory design and development.&lt;/li&gt; &#xA; &lt;li&gt;Build it like Systems Software: This sets a high bar for Reliability, Security and Performance SLAs.&lt;/li&gt; &#xA; &lt;li&gt;Maximise Value Creation: For developers, customers &amp;amp; partners.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This project is being created and maintained by &lt;a href=&#34;https://juspay.io&#34;&gt;Juspay&lt;/a&gt;&lt;/p&gt; &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/#versioning&#34;&gt; &lt;h2 id=&#34;versioning&#34;&gt;Versioning&lt;/h2&gt; &lt;/a&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/#copyright-and-license&#34;&gt; &lt;h2 id=&#34;copyright-and-license&#34;&gt;Copyright and License&lt;/h2&gt; &lt;/a&gt; &#xA;&lt;p&gt;This product is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/juspay/hyperswitch/main/team-behind-hyperswitch&#34;&gt; &lt;h2 id=&#34;team-behind-hyperswitch&#34;&gt;Team behind Hyperswitch&lt;/h2&gt; &lt;/a&gt; &#xA;&lt;p&gt;The core team of 150+ engineers building Hyperswitch. Keep up the great work! ü•Ç&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/juspay/hyperswitch/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contributors-img.web.app/image?repo=juspay/hyperswitch&#34; alt=&#34;Contributors&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>simular-ai/Agent-S</title>
    <updated>2025-05-04T01:38:48Z</updated>
    <id>tag:github.com,2025-05-04:/simular-ai/Agent-S</id>
    <link href="https://github.com/simular-ai/Agent-S" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Agent S: an open agentic framework that uses computers like a human&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/agent_s.png&#34; alt=&#34;Logo&#34; style=&#34;vertical-align:middle&#34; width=&#34;60&#34;&gt; Agent S2: &lt;small&gt;A Compositional Generalist-Specialist Framework for Computer Use Agents&lt;/small&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&amp;nbsp; üåê &lt;a href=&#34;https://www.simular.ai/articles/agent-s2-technical-review&#34;&gt;[S2 blog]&lt;/a&gt;&amp;nbsp; üìÑ &lt;a href=&#34;https://arxiv.org/abs/2504.00906&#34;&gt;[S2 Paper]&lt;/a&gt;&amp;nbsp; üé• &lt;a href=&#34;https://www.youtube.com/watch?v=wUGVQl7c0eg&#34;&gt;[S2 Video]&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&amp;nbsp; üåê &lt;a href=&#34;https://www.simular.ai/agent-s&#34;&gt;[S1 blog]&lt;/a&gt;&amp;nbsp; üìÑ &lt;a href=&#34;https://arxiv.org/abs/2410.08164&#34;&gt;[S1 Paper (ICLR 2025)]&lt;/a&gt;&amp;nbsp; üé• &lt;a href=&#34;https://www.youtube.com/watch?v=OBDE3Knte0g&#34;&gt;[S1 Video]&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&amp;nbsp; &lt;a href=&#34;https://trendshift.io/repositories/13151&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/13151&#34; alt=&#34;simular-ai%2FAgent-S | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/E2XfsK9fPV&#34;&gt; &lt;img src=&#34;https://dcbadge.limes.pink/api/server/https://discord.gg/E2XfsK9fPV?style=flat&#34; alt=&#34;Discord&#34;&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://pepy.tech/projects/gui-agents&#34;&gt; &lt;img src=&#34;https://static.pepy.tech/badge/gui-agents&#34; alt=&#34;PyPI Downloads&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;ü•≥ Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;2025/04/01&lt;/strong&gt;: Released &lt;a href=&#34;https://arxiv.org/abs/2504.00906&#34;&gt;Agent S2 paper&lt;/a&gt; with new SOTA results on OSWorld, WindowsAgentArena, and AndroidWorld!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;2025/03/12&lt;/strong&gt;: Released Agent S2 along with v0.2.0 of &lt;a href=&#34;https://github.com/simular-ai/Agent-S&#34;&gt;gui-agents&lt;/a&gt;, the new state-of-the-art for computer use agents (CUA), outperforming OpenAI&#39;s CUA/Operator and Anthropic&#39;s Claude 3.7 Sonnet Computer-Use!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;2025/01/22&lt;/strong&gt;: The &lt;a href=&#34;https://arxiv.org/abs/2410.08164&#34;&gt;Agent S paper&lt;/a&gt; is accepted to ICLR 2025!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;2025/01/21&lt;/strong&gt;: Released v0.1.2 of &lt;a href=&#34;https://github.com/simular-ai/Agent-S&#34;&gt;gui-agents&lt;/a&gt; library, with support for Linux and Windows!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;2024/12/05&lt;/strong&gt;: Released v0.1.0 of &lt;a href=&#34;https://github.com/simular-ai/Agent-S&#34;&gt;gui-agents&lt;/a&gt; library, allowing you to use Agent-S for Mac, OSWorld, and WindowsAgentArena with ease!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;2024/10/10&lt;/strong&gt;: Released the &lt;a href=&#34;https://arxiv.org/abs/2410.08164&#34;&gt;Agent S paper&lt;/a&gt; and codebase!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-introduction&#34;&gt;üí° Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-current-results&#34;&gt;üéØ Current Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#%EF%B8%8F-installation--setup&#34;&gt;üõ†Ô∏è Installation &amp;amp; Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-usage&#34;&gt;üöÄ Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-acknowledgements&#34;&gt;ü§ù Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-citation&#34;&gt;üí¨ Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üí° Introduction&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/agent_s2_teaser.png&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Welcome to &lt;strong&gt;Agent S&lt;/strong&gt;, an open-source framework designed to enable autonomous interaction with computers through Agent-Computer Interface. Our mission is to build intelligent GUI agents that can learn from past experiences and perform complex tasks autonomously on your computer.&lt;/p&gt; &#xA;&lt;p&gt;Whether you&#39;re interested in AI, automation, or contributing to cutting-edge agent-based systems, we&#39;re excited to have you here!&lt;/p&gt; &#xA;&lt;h2&gt;üéØ Current Results&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/agent_s2_osworld_result.png&#34; width=&#34;600&#34;&gt; &lt;br&gt; Results of Agent S2&#39;s Successful Rate (%) on the OSWorld full test set using Screenshot input only. &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table border=&#34;0&#34; cellspacing=&#34;0&#34; cellpadding=&#34;5&#34;&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Benchmark&lt;/th&gt; &#xA;    &lt;th&gt;Agent S2&lt;/th&gt; &#xA;    &lt;th&gt;Previous SOTA&lt;/th&gt; &#xA;    &lt;th&gt;Œî improve&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;OSWorld (15 step)&lt;/td&gt; &#xA;    &lt;td&gt;27.0%&lt;/td&gt; &#xA;    &lt;td&gt;22.7% (UI-TARS)&lt;/td&gt; &#xA;    &lt;td&gt;+4.3%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;OSWorld (50 step)&lt;/td&gt; &#xA;    &lt;td&gt;34.5%&lt;/td&gt; &#xA;    &lt;td&gt;32.6% (OpenAI CUA)&lt;/td&gt; &#xA;    &lt;td&gt;+1.9%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;WindowsAgentArena&lt;/td&gt; &#xA;    &lt;td&gt;29.8%&lt;/td&gt; &#xA;    &lt;td&gt;19.5% (NAVI)&lt;/td&gt; &#xA;    &lt;td&gt;+10.3%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;AndroidWorld&lt;/td&gt; &#xA;    &lt;td&gt;54.3%&lt;/td&gt; &#xA;    &lt;td&gt;46.8% (UI-TARS)&lt;/td&gt; &#xA;    &lt;td&gt;+7.5%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Installation &amp;amp; Setup&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ùó&lt;strong&gt;Warning&lt;/strong&gt;‚ùó: If you are on a Linux machine, creating a &lt;code&gt;conda&lt;/code&gt; environment will interfere with &lt;code&gt;pyatspi&lt;/code&gt;. As of now, there&#39;s no clean solution for this issue. Proceed through the installation without using &lt;code&gt;conda&lt;/code&gt; or any virtual environment.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ö†Ô∏è&lt;strong&gt;Disclaimer&lt;/strong&gt;‚ö†Ô∏è: To leverage the full potential of Agent S2, we utilize &lt;a href=&#34;https://github.com/bytedance/UI-TARS&#34;&gt;UI-TARS&lt;/a&gt; as a grounding model (7B-DPO or 72B-DPO for better performance). They can be hosted locally, or on Hugging Face Inference Endpoints. Our code supports Hugging Face Inference Endpoints. Check out &lt;a href=&#34;https://huggingface.co/learn/cookbook/en/enterprise_dedicated_endpoints&#34;&gt;Hugging Face Inference Endpoints&lt;/a&gt; for more information on how to set up and query this endpoint. However, running Agent S2 does not require this model, and you can use alternative API based models for visual grounding, such as Claude.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Install the package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install gui-agents&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set your LLM API Keys and other environment variables. You can do this by adding the following line to your .bashrc (Linux), or .zshrc (MacOS) file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY=&amp;lt;YOUR_API_KEY&amp;gt;&#xA;export ANTHROPIC_API_KEY=&amp;lt;YOUR_ANTHROPIC_API_KEY&amp;gt;&#xA;export HF_TOKEN=&amp;lt;YOUR_HF_TOKEN&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can set the environment variable in your Python script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import os&#xA;os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;&amp;lt;YOUR_API_KEY&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also support Azure OpenAI, Anthropic, Gemini, Open Router, and vLLM inference. For more information refer to &lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/models.md&#34;&gt;models.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Setup Retrieval from Web using Perplexica&lt;/h3&gt; &#xA;&lt;p&gt;Agent S works best with web-knowledge retrieval. To enable this feature, you need to setup Perplexica:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure Docker Desktop is installed and running on your system.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the directory containing the project files.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; cd Perplexica&#xA; git submodule update --init&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Rename the &lt;code&gt;sample.config.toml&lt;/code&gt; file to &lt;code&gt;config.toml&lt;/code&gt;. For Docker setups, you need only fill in the following fields:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;OPENAI&lt;/code&gt;: Your OpenAI API key. &lt;strong&gt;You only need to fill this if you wish to use OpenAI&#39;s models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;OLLAMA&lt;/code&gt;: Your Ollama API URL. You should enter it as &lt;code&gt;http://host.docker.internal:PORT_NUMBER&lt;/code&gt;. If you installed Ollama on port 11434, use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;. For other ports, adjust accordingly. &lt;strong&gt;You need to fill this if you wish to use Ollama&#39;s models instead of OpenAI&#39;s&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;GROQ&lt;/code&gt;: Your Groq API key. &lt;strong&gt;You only need to fill this if you wish to use Groq&#39;s hosted models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;ANTHROPIC&lt;/code&gt;: Your Anthropic API key. &lt;strong&gt;You only need to fill this if you wish to use Anthropic models&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can change these after starting Perplexica from the settings dialog.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;SIMILARITY_MEASURE&lt;/code&gt;: The similarity measure to use (This is filled by default; you can leave it as is if you are unsure about it.)&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure you are in the directory containing the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file and execute:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Next, export your Perplexica URL. This URL is used to interact with the Perplexica API backend. The port is given by the &lt;code&gt;config.toml&lt;/code&gt; in your Perplexica directory.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PERPLEXICA_URL=http://localhost:{port}/api/search&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Our implementation of Agent S incorporates the Perplexica API to integrate a search engine capability, which allows for a more convenient and responsive user experience. If you want to tailor the API to your settings and specific requirements, you may modify the URL and the message of request parameters in &lt;code&gt;agent_s/query_perplexica.py&lt;/code&gt;. For a comprehensive guide on configuring the Perplexica API, please refer to &lt;a href=&#34;https://github.com/ItzCrazyKns/Perplexica/raw/master/docs/API/SEARCH.md&#34;&gt;Perplexica Search API Documentation&lt;/a&gt;. For a more detailed setup and usage guide, please refer to the &lt;a href=&#34;https://github.com/ItzCrazyKns/Perplexica.git&#34;&gt;Perplexica Repository&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ùó&lt;strong&gt;Warning&lt;/strong&gt;‚ùó: The agent will directly run python code to control your computer. Please use with care.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üöÄ Usage&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Our best configuration uses Claude 3.7 with extended thinking and UI-TARS-72B-DPO. If you are unable to run UI-TARS-72B-DPO due to resource constraints, UI-TARS-7B-DPO can be used as a lighter alternative with minimal performance degradation.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;CLI&lt;/h3&gt; &#xA;&lt;p&gt;Run Agent S2 with a specific model (default is &lt;code&gt;gpt-4o&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;agent_s2 \&#xA;  --provider &#34;anthropic&#34; \&#xA;  --model &#34;claude-3-7-sonnet-20250219&#34; \&#xA;  --grounding_model_provider &#34;anthropic&#34; \&#xA;  --grounding_model &#34;claude-3-7-sonnet-20250219&#34; \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or use a custom endpoint:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;agent_s2 \&#xA;  --provider &#34;anthropic&#34; \&#xA;  --model &#34;claude-3-7-sonnet-20250219&#34; \&#xA;  --endpoint_provider &#34;huggingface&#34; \&#xA;  --endpoint_url &#34;&amp;lt;endpoint_url&amp;gt;/v1/&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Main Model Settings&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;--provider&lt;/code&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Purpose: Specifies the main generation model&lt;/li&gt; &#xA;   &lt;li&gt;Supports: all model providers in &lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/models.md&#34;&gt;models.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Default: &lt;code&gt;--provider &#34;anthropic&#34; --model &#34;claude-3-7-sonnet-20250219&#34;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Grounding Configuration Options&lt;/h4&gt; &#xA;&lt;p&gt;You can use either Configuration 1 or Configuration 2:&lt;/p&gt; &#xA;&lt;h5&gt;&lt;strong&gt;(Default) Configuration 1: API-Based Models&lt;/strong&gt;&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;--grounding_model_provider&lt;/code&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;code&gt;--grounding_model&lt;/code&gt;&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Purpose: Specifies the model for visual grounding (coordinate prediction)&lt;/li&gt; &#xA;   &lt;li&gt;Supports: all model providers in &lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/models.md&#34;&gt;models.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Default: &lt;code&gt;--grounding_model_provider &#34;anthropic&#34; --grounding_model &#34;claude-3-7-sonnet-20250219&#34;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;‚ùó&lt;strong&gt;Important&lt;/strong&gt;‚ùó &lt;strong&gt;&lt;code&gt;--grounding_model_resize_width&lt;/code&gt;&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Purpose: Some API providers automatically rescale images. Therefore, the generated (x, y) will be relative to the rescaled image dimensions, instead of the original image dimensions.&lt;/li&gt; &#xA;   &lt;li&gt;Supports: &lt;a href=&#34;https://docs.anthropic.com/en/docs/build-with-claude/vision#&#34;&gt;Anthropic rescaling&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Tips: If your grounding is inaccurate even for very simple queries, double check your rescaling width is correct for your machine&#39;s resolution.&lt;/li&gt; &#xA;   &lt;li&gt;Default: &lt;code&gt;--grounding_model_resize_width 1366&lt;/code&gt; (Anthropic)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;&lt;strong&gt;Configuration 2: Custom Endpoint&lt;/strong&gt;&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;--endpoint_provider&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Purpose: Specifies the endpoint provider&lt;/li&gt; &#xA;   &lt;li&gt;Supports: HuggingFace TGI, vLLM, Open Router&lt;/li&gt; &#xA;   &lt;li&gt;Default: None&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;--endpoint_url&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Purpose: The URL for your custom endpoint&lt;/li&gt; &#xA;   &lt;li&gt;Default: None&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Configuration 2 takes precedence over Configuration 1.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This will show a user query prompt where you can enter your query and interact with Agent S2. You can use any model from the list of supported models in &lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/models.md&#34;&gt;models.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;gui_agents&lt;/code&gt; SDK&lt;/h3&gt; &#xA;&lt;p&gt;First, we import the necessary modules. &lt;code&gt;AgentS2&lt;/code&gt; is the main agent class for Agent S2. &lt;code&gt;OSWorldACI&lt;/code&gt; is our grounding agent that translates agent actions into executable python code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import pyautogui&#xA;import io&#xA;from gui_agents.s2.agents.agent_s import AgentS2&#xA;from gui_agents.s2.agents.grounding import OSWorldACI&#xA;&#xA;# Load in your API keys.&#xA;from dotenv import load_dotenv&#xA;load_dotenv()&#xA;&#xA;current_platform = &#34;linux&#34;  # &#34;darwin&#34;, &#34;windows&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, we define our engine parameters. &lt;code&gt;engine_params&lt;/code&gt; is used for the main agent, and &lt;code&gt;engine_params_for_grounding&lt;/code&gt; is for grounding. For &lt;code&gt;engine_params_for_grounding&lt;/code&gt;, we support the Claude, GPT series, and Hugging Face Inference Endpoints.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;engine_type_for_grounding = &#34;huggingface&#34;&#xA;&#xA;engine_params = {&#xA;    &#34;engine_type&#34;: &#34;openai&#34;,&#xA;    &#34;model&#34;: &#34;gpt-4o&#34;,&#xA;}&#xA;&#xA;if engine_type_for_grounding == &#34;huggingface&#34;:&#xA;  engine_params_for_grounding = {&#xA;      &#34;engine_type&#34;: &#34;huggingface&#34;,&#xA;      &#34;endpoint_url&#34;: &#34;&amp;lt;endpoint_url&amp;gt;/v1/&#34;,&#xA;  }&#xA;elif engine_type_for_grounding == &#34;claude&#34;:&#xA;  engine_params_for_grounding = {&#xA;      &#34;engine_type&#34;: &#34;claude&#34;,&#xA;      &#34;model&#34;: &#34;claude-3-7-sonnet-20250219&#34;,&#xA;  }&#xA;elif engine_type_for_grounding == &#34;gpt&#34;:&#xA;  engine_params_for_grounding = {&#xA;    &#34;engine_type&#34;: &#34;gpt&#34;,&#xA;    &#34;model&#34;: &#34;gpt-4o&#34;,&#xA;  }&#xA;else:&#xA;  raise ValueError(&#34;Invalid engine type for grounding&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, we define our grounding agent and Agent S2.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;grounding_agent = OSWorldACI(&#xA;    platform=current_platform,&#xA;    engine_params_for_generation=engine_params,&#xA;    engine_params_for_grounding=engine_params_for_grounding&#xA;)&#xA;&#xA;agent = AgentS2(&#xA;  engine_params,&#xA;  grounding_agent,&#xA;  platform=current_platform,&#xA;  action_space=&#34;pyautogui&#34;,&#xA;  observation_type=&#34;mixed&#34;,&#xA;  search_engine=&#34;Perplexica&#34;  # Assuming you have set up Perplexica.&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, let&#39;s query the agent!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Get screenshot.&#xA;screenshot = pyautogui.screenshot()&#xA;buffered = io.BytesIO() &#xA;screenshot.save(buffered, format=&#34;PNG&#34;)&#xA;screenshot_bytes = buffered.getvalue()&#xA;&#xA;obs = {&#xA;  &#34;screenshot&#34;: screenshot_bytes,&#xA;}&#xA;&#xA;instruction = &#34;Close VS Code&#34;&#xA;info, action = agent.predict(instruction=instruction, observation=obs)&#xA;&#xA;exec(action[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;code&gt;gui_agents/s2/cli_app.py&lt;/code&gt; for more details on how the inference loop works.&lt;/p&gt; &#xA;&lt;h4&gt;Downloading the Knowledge Base&lt;/h4&gt; &#xA;&lt;p&gt;Agent S2 uses a knowledge base that continually updates with new knowledge during inference. The knowledge base is initially downloaded when initializing &lt;code&gt;AgentS2&lt;/code&gt;. The knowledge base is stored as assets under our &lt;a href=&#34;https://github.com/simular-ai/Agent-S/releases&#34;&gt;GitHub Releases&lt;/a&gt;. The &lt;code&gt;AgentS2&lt;/code&gt; initialization will only download the knowledge base for your specified platform and agent version (e.g s1, s2). If you&#39;d like to download the knowledge base programmatically, you can use the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;download_kb_data(&#xA;    version=&#34;s2&#34;,&#xA;    release_tag=&#34;v0.2.2&#34;,&#xA;    download_dir=&#34;kb_data&#34;,&#xA;    platform=&#34;linux&#34;  # &#34;darwin&#34;, &#34;windows&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will download Agent S2&#39;s knowledge base for Linux from release tag &lt;code&gt;v0.2.2&lt;/code&gt; to the &lt;code&gt;kb_data&lt;/code&gt; directory. Refer to our &lt;a href=&#34;https://github.com/simular-ai/Agent-S/releases&#34;&gt;GitHub Releases&lt;/a&gt; or release tags that include the knowledge bases.&lt;/p&gt; &#xA;&lt;h3&gt;OSWorld&lt;/h3&gt; &#xA;&lt;p&gt;To deploy Agent S2 in OSWorld, follow the &lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/OSWorld.md&#34;&gt;OSWorld Deployment instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We extend our sincere thanks to Tianbao Xie for developing OSWorld and discussing computer use challenges. We also appreciate the engaging discussions with Yujia Qin and Shihao Liang regarding UI-TARS.&lt;/p&gt; &#xA;&lt;h2&gt;üí¨ Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you find this codebase useful, please cite&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{Agent-S2,&#xA;      title={Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents}, &#xA;      author={Saaket Agashe and Kyle Wong and Vincent Tu and Jiachen Yang and Ang Li and Xin Eric Wang},&#xA;      year={2025},&#xA;      eprint={2504.00906},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI},&#xA;      url={https://arxiv.org/abs/2504.00906}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{Agent-S,&#xA;    title={{Agent S: An Open Agentic Framework that Uses Computers Like a Human}},&#xA;    author={Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang},&#xA;    booktitle={International Conference on Learning Representations (ICLR)},&#xA;    year={2025},&#xA;    url={https://arxiv.org/abs/2410.08164}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Tencent/Hunyuan3D-2</title>
    <updated>2025-05-04T01:38:48Z</updated>
    <id>tag:github.com,2025-05-04:/Tencent/Hunyuan3D-2</id>
    <link href="https://github.com/Tencent/Hunyuan3D-2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High-Resolution 3D Assets Generation with Large Scale Hunyuan3D Diffusion Models.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/README_zh_cn.md&#34;&gt;‰∏≠ÊñáÈòÖËØª&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/README_ja_jp.md&#34;&gt;Êó•Êú¨Ë™û„ÅßË™≠„ÇÄ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/efb402a1-0b09-41e0-a6cb-259d442e76aa&#34;&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://3d.hunyuan.tencent.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage&#34; height=&#34;22px&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/spaces/tencent/Hunyuan3D-2&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Demo-276cb4.svg?sanitize=true&#34; height=&#34;22px&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg?sanitize=true&#34; height=&#34;22px&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://3d-models.hunyuan.tencent.com/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Page-bb8a2e.svg?logo=github&#34; height=&#34;22px&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://discord.gg/dNBrdrGGMa&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-white.svg?logo=discord&#34; height=&#34;22px&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2501.12202&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv&#34; height=&#34;22px&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://x.com/txhunyuan&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hunyuan-black.svg?logo=x&#34; height=&#34;22px&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/#community-resources&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Community-lavender.svg?logo=homeassistantcommunitystore&#34; height=&#34;22px&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ‚Äú Living out everyone‚Äôs imagination on creating and manipulating 3D assets.‚Äù &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/a2cbc5b8-be22-49d7-b1c3-7aa2b20ba460&#34;&gt;https://github.com/user-attachments/assets/a2cbc5b8-be22-49d7-b1c3-7aa2b20ba460&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üî• News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apr 1, 2025: ü§ó Release turbo paint model &lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0-turbo&#34;&gt;Hunyuan3D-Paint-v2-0-Turbo&lt;/a&gt;, and multiview texture generation pipeline, try it &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/examples/fast_texture_gen_multiview.py&#34;&gt;here&lt;/a&gt;! Stay tuned for our new texture generation model &lt;a href=&#34;https://github.com/oakshy/RomanTex&#34;&gt;RomanTex&lt;/a&gt; and PBR material generation &lt;a href=&#34;https://github.com/ZebinHe/MaterialMVP/&#34;&gt;MaterialMVP&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;Mar 19, 2025: ü§ó Release turbo model &lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2/&#34;&gt;Hunyuan3D-2-Turbo&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2mini/&#34;&gt;Hunyuan3D-2mini-Turbo&lt;/a&gt; and &lt;a href=&#34;https://github.com/Tencent/FlashVDM&#34;&gt;FlashVDM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Mar 18, 2025: ü§ó Release multiview shape model &lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2mv&#34;&gt;Hunyuan3D-2mv&lt;/a&gt; and 0.6B shape model &lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2mini&#34;&gt;Hunyuan3D-2mini&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Feb 14, 2025: üõ†Ô∏è Release texture enhancement module, please obtain high-definition textures via &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/minimal_demo.py&#34;&gt;here&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;Feb 3, 2025: üêé Release &lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast&#34;&gt;Hunyuan3D-DiT-v2-0-Fast&lt;/a&gt;, our guidance distillation model that could half the dit inference time, see &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/minimal_demo.py&#34;&gt;here&lt;/a&gt; for usage.&lt;/li&gt; &#xA; &lt;li&gt;Jan 27, 2025: üõ†Ô∏è Release Blender addon for Hunyuan3D 2.0, Check it out &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/#blender-addon&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Jan 23, 2025: üí¨ We thank community members for creating &lt;a href=&#34;https://github.com/YanWenKun/Hunyuan3D-2-WinPortable&#34;&gt;Windows installation tool&lt;/a&gt;, ComfyUI support with &lt;a href=&#34;https://github.com/kijai/ComfyUI-Hunyuan3DWrapper&#34;&gt;ComfyUI-Hunyuan3DWrapper&lt;/a&gt; and &lt;a href=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack&#34;&gt;ComfyUI-3D-Pack&lt;/a&gt; and other awesome &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/#community-resources&#34;&gt;extensions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Jan 21, 2025: üí¨ Enjoy exciting 3D generation on our website &lt;a href=&#34;https://3d.hunyuan.tencent.com&#34;&gt;Hunyuan3D Studio&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;Jan 21, 2025: ü§ó Release inference code and pretrained models of &lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2&#34;&gt;Hunyuan3D 2.0&lt;/a&gt;. Please give it a try via &lt;a href=&#34;https://huggingface.co/spaces/tencent/Hunyuan3D-2&#34;&gt;huggingface space&lt;/a&gt; and our &lt;a href=&#34;https://3d.hunyuan.tencent.com&#34;&gt;official site&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Join our &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/#&#34;&gt;Wechat&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://discord.gg/dNBrdrGGMa&#34;&gt;Discord&lt;/a&gt;&lt;/strong&gt; group to discuss and find help from us.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Wechat Group&lt;/th&gt; &#xA;   &lt;th&gt;Xiaohongshu&lt;/th&gt; &#xA;   &lt;th&gt;X&lt;/th&gt; &#xA;   &lt;th&gt;Discord&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/qrcode/wechat.png&#34; height=&#34;140&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/qrcode/xiaohongshu.png&#34; height=&#34;140&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/qrcode/x.png&#34; height=&#34;140&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/qrcode/discord.png&#34; height=&#34;140&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets. This system includes two foundation components: a large-scale shape generation model - Hunyuan3D-DiT, and a large-scale texture synthesis model - Hunyuan3D-Paint. The shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly aligns with a given condition image, laying a solid foundation for downstream applications. The texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant texture maps for either generated or hand-crafted meshes. Furthermore, we build Hunyuan3D-Studio - a versatile, user-friendly production platform that simplifies the re-creation process of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes efficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models, including the open-source models and closed-source models in geometry details, condition alignment, texture quality, and e.t.c.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/system.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;‚òØÔ∏è &lt;strong&gt;Hunyuan3D 2.0&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Architecture&lt;/h3&gt; &#xA;&lt;p&gt;Hunyuan3D 2.0 features a two-stage generation pipeline, starting with the creation of a bare mesh, followed by the synthesis of a texture map for that mesh. This strategy is effective for decoupling the difficulties of shape and texture generation and also provides flexibility for texturing either generated or handcrafted meshes.&lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/arch.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Performance&lt;/h3&gt; &#xA;&lt;p&gt;We have evaluated Hunyuan3D 2.0 with other open-source as well as close-source 3d-generation methods. The numerical results indicate that Hunyuan3D 2.0 surpasses all baselines in the quality of generated textured 3D assets and the condition following ability.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;CMMD(‚¨á)&lt;/th&gt; &#xA;   &lt;th&gt;FID_CLIP(‚¨á)&lt;/th&gt; &#xA;   &lt;th&gt;FID(‚¨á)&lt;/th&gt; &#xA;   &lt;th&gt;CLIP-score(‚¨Ü)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Top Open-source Model1&lt;/td&gt; &#xA;   &lt;td&gt;3.591&lt;/td&gt; &#xA;   &lt;td&gt;54.639&lt;/td&gt; &#xA;   &lt;td&gt;289.287&lt;/td&gt; &#xA;   &lt;td&gt;0.787&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Top Close-source Model1&lt;/td&gt; &#xA;   &lt;td&gt;3.600&lt;/td&gt; &#xA;   &lt;td&gt;55.866&lt;/td&gt; &#xA;   &lt;td&gt;305.922&lt;/td&gt; &#xA;   &lt;td&gt;0.779&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Top Close-source Model2&lt;/td&gt; &#xA;   &lt;td&gt;3.368&lt;/td&gt; &#xA;   &lt;td&gt;49.744&lt;/td&gt; &#xA;   &lt;td&gt;294.628&lt;/td&gt; &#xA;   &lt;td&gt;0.806&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Top Close-source Model3&lt;/td&gt; &#xA;   &lt;td&gt;3.218&lt;/td&gt; &#xA;   &lt;td&gt;51.574&lt;/td&gt; &#xA;   &lt;td&gt;295.691&lt;/td&gt; &#xA;   &lt;td&gt;0.799&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hunyuan3D 2.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.193&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;49.165&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;282.429&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.809&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Generation results of Hunyuan3D 2.0:&lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-1.gif&#34; height=&#34;250&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-2.gif&#34; height=&#34;250&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üéÅ Models Zoo&lt;/h2&gt; &#xA;&lt;p&gt;It takes 6 GB VRAM for shape generation and 16 GB for shape and texture generation in total.&lt;/p&gt; &#xA;&lt;p&gt;Hunyuan3D-2mini Series&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Date&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Huggingface&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hunyuan3D-DiT-v2-mini-Turbo&lt;/td&gt; &#xA;   &lt;td&gt;Step Distillation Version&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-19&lt;/td&gt; &#xA;   &lt;td&gt;0.6B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini-turbo&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hunyuan3D-DiT-v2-mini-Fast&lt;/td&gt; &#xA;   &lt;td&gt;Guidance Distillation Version&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-18&lt;/td&gt; &#xA;   &lt;td&gt;0.6B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini-fast&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hunyuan3D-DiT-v2-mini&lt;/td&gt; &#xA;   &lt;td&gt;Mini Image to Shape Model&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-18&lt;/td&gt; &#xA;   &lt;td&gt;0.6B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Hunyuan3D-2mv Series&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Date&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Huggingface&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hunyuan3D-DiT-v2-mv-Turbo&lt;/td&gt; &#xA;   &lt;td&gt;Step Distillation Version&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-19&lt;/td&gt; &#xA;   &lt;td&gt;1.1B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-turbo&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hunyuan3D-DiT-v2-mv-Fast&lt;/td&gt; &#xA;   &lt;td&gt;Guidance Distillation Version&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-18&lt;/td&gt; &#xA;   &lt;td&gt;1.1B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hunyuan3D-DiT-v2-mv&lt;/td&gt; &#xA;   &lt;td&gt;Multiview Image to Shape Model&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-18&lt;/td&gt; &#xA;   &lt;td&gt;1.1B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Hunyuan3D-2 Series&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Date&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Huggingface&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hunyuan3D-DiT-v2-0-Turbo&lt;/td&gt; &#xA;   &lt;td&gt;Step Distillation Model&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-19&lt;/td&gt; &#xA;   &lt;td&gt;1.1B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-turbo&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hunyuan3D-DiT-v2-0-Fast&lt;/td&gt; &#xA;   &lt;td&gt;Guidance Distillation Model&lt;/td&gt; &#xA;   &lt;td&gt;2025-02-03&lt;/td&gt; &#xA;   &lt;td&gt;1.1B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hunyuan3D-DiT-v2-0&lt;/td&gt; &#xA;   &lt;td&gt;Image to Shape Model&lt;/td&gt; &#xA;   &lt;td&gt;2025-01-21&lt;/td&gt; &#xA;   &lt;td&gt;1.1B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hunyuan3D-Paint-v2-0&lt;/td&gt; &#xA;   &lt;td&gt;Texture Generation Model&lt;/td&gt; &#xA;   &lt;td&gt;2025-01-21&lt;/td&gt; &#xA;   &lt;td&gt;1.3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hunyuan3D-Paint-v2-0-Turbo&lt;/td&gt; &#xA;   &lt;td&gt;Distillation Texure Model&lt;/td&gt; &#xA;   &lt;td&gt;2025-04-01&lt;/td&gt; &#xA;   &lt;td&gt;1.3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0-turbo&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hunyuan3D-Delight-v2-0&lt;/td&gt; &#xA;   &lt;td&gt;Image Delight Model&lt;/td&gt; &#xA;   &lt;td&gt;2025-01-21&lt;/td&gt; &#xA;   &lt;td&gt;1.3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ü§ó Get Started with Hunyuan3D 2.0&lt;/h2&gt; &#xA;&lt;p&gt;Hunyuan3D 2.0 supports Macos, Windows, Linux. You may follow the next steps to use Hunyuan3D 2.0 via:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/#code-usage&#34;&gt;Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/#gradio-app&#34;&gt;Gradio App&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/#api-server&#34;&gt;API Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/#blender-addon&#34;&gt;Blender Addon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/#official-site&#34;&gt;Official Site&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Install Requirements&lt;/h3&gt; &#xA;&lt;p&gt;Please install Pytorch via the &lt;a href=&#34;https://pytorch.org/&#34;&gt;official&lt;/a&gt; site. Then install the other requirements via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;pip install -e .&#xA;# for texture&#xA;cd hy3dgen/texgen/custom_rasterizer&#xA;python3 setup.py install&#xA;cd ../../..&#xA;cd hy3dgen/texgen/differentiable_renderer&#xA;python3 setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Code Usage&lt;/h3&gt; &#xA;&lt;p&gt;We designed a diffusers-like API to use our shape generation model - Hunyuan3D-DiT and texture synthesis model - Hunyuan3D-Paint.&lt;/p&gt; &#xA;&lt;p&gt;You could assess &lt;strong&gt;Hunyuan3D-DiT&lt;/strong&gt; via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline&#xA;&#xA;pipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained(&#39;tencent/Hunyuan3D-2&#39;)&#xA;mesh = pipeline(image=&#39;assets/demo.png&#39;)[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output mesh is a &lt;a href=&#34;https://trimesh.org/trimesh.html&#34;&gt;trimesh object&lt;/a&gt;, which you could save to glb/obj (or other format) file.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;strong&gt;Hunyuan3D-Paint&lt;/strong&gt;, do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from hy3dgen.texgen import Hunyuan3DPaintPipeline&#xA;from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline&#xA;&#xA;# let&#39;s generate a mesh first&#xA;pipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained(&#39;tencent/Hunyuan3D-2&#39;)&#xA;mesh = pipeline(image=&#39;assets/demo.png&#39;)[0]&#xA;&#xA;pipeline = Hunyuan3DPaintPipeline.from_pretrained(&#39;tencent/Hunyuan3D-2&#39;)&#xA;mesh = pipeline(mesh, image=&#39;assets/demo.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please visit &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/examples&#34;&gt;examples&lt;/a&gt; folder for more advanced usage, such as &lt;strong&gt;multiview image to 3D generation&lt;/strong&gt; and * &lt;em&gt;texture generation for handcrafted mesh&lt;/em&gt;*.&lt;/p&gt; &#xA;&lt;h3&gt;Gradio App&lt;/h3&gt; &#xA;&lt;p&gt;You could also host a &lt;a href=&#34;https://www.gradio.app/&#34;&gt;Gradio&lt;/a&gt; App in your own computer via:&lt;/p&gt; &#xA;&lt;p&gt;Standard Version&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Hunyuan3D-2mini&#xA;python3 gradio_app.py --model_path tencent/Hunyuan3D-2mini --subfolder hunyuan3d-dit-v2-mini --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode&#xA;# Hunyuan3D-2mv&#xA;python3 gradio_app.py --model_path tencent/Hunyuan3D-2mv --subfolder hunyuan3d-dit-v2-mv --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode&#xA;# Hunyuan3D-2&#xA;python3 gradio_app.py --model_path tencent/Hunyuan3D-2 --subfolder hunyuan3d-dit-v2-0 --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Turbo Version&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Hunyuan3D-2mini&#xA;python3 gradio_app.py --model_path tencent/Hunyuan3D-2mini --subfolder hunyuan3d-dit-v2-mini-turbo --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode --enable_flashvdm&#xA;# Hunyuan3D-2mv&#xA;python3 gradio_app.py --model_path tencent/Hunyuan3D-2mv --subfolder hunyuan3d-dit-v2-mv-turbo --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode --enable_flashvdm&#xA;# Hunyuan3D-2&#xA;python3 gradio_app.py --model_path tencent/Hunyuan3D-2 --subfolder hunyuan3d-dit-v2-0-turbo --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode --enable_flashvdm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;API Server&lt;/h3&gt; &#xA;&lt;p&gt;You could launch an API server locally, which you could post web request for Image/Text to 3D, Texturing existing mesh, and e.t.c.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python api_server.py --host 0.0.0.0 --port 8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A demo post request for image to 3D without texture.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;img_b64_str=$(base64 -i assets/demo.png)&#xA;curl -X POST &#34;http://localhost:8080/generate&#34; \&#xA;     -H &#34;Content-Type: application/json&#34; \&#xA;     -d &#39;{&#xA;           &#34;image&#34;: &#34;&#39;&#34;$img_b64_str&#34;&#39;&#34;,&#xA;         }&#39; \&#xA;     -o test2.glb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Blender Addon&lt;/h3&gt; &#xA;&lt;p&gt;With an API server launched, you could also directly use Hunyuan3D 2.0 in your blender with our &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/blender_addon.py&#34;&gt;Blender Addon&lt;/a&gt;. Please follow our tutorial to install and use.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/8230bfb5-32b1-4e48-91f4-a977c54a4f3e&#34;&gt;https://github.com/user-attachments/assets/8230bfb5-32b1-4e48-91f4-a977c54a4f3e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Official Site&lt;/h3&gt; &#xA;&lt;p&gt;Don&#39;t forget to visit &lt;a href=&#34;https://3d.hunyuan.tencent.com&#34;&gt;Hunyuan3D&lt;/a&gt; for quick use, if you don&#39;t want to host yourself.&lt;/p&gt; &#xA;&lt;h2&gt;üìë Open-Source Plan&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Model Checkpoints&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Technical Report&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ComfyUI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TensorRT Version&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Finetuning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîó BibTeX&lt;/h2&gt; &#xA;&lt;p&gt;If you found this repository helpful, please cite our reports:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{hunyuan3d22025tencent,&#xA;    title={Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation},&#xA;    author={Tencent Hunyuan3D Team},&#xA;    year={2025},&#xA;    eprint={2501.12202},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CV}&#xA;}&#xA;&#xA;@misc{yang2024hunyuan3d,&#xA;    title={Hunyuan3D 1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation},&#xA;    author={Tencent Hunyuan3D Team},&#xA;    year={2024},&#xA;    eprint={2411.02293},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Community Resources&lt;/h2&gt; &#xA;&lt;p&gt;Thanks for the contributions of community members, here we have these great extensions of Hunyuan3D 2.0:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MrForExample/ComfyUI-3D-Pack&#34;&gt;ComfyUI-3D-Pack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kijai/ComfyUI-Hunyuan3DWrapper&#34;&gt;ComfyUI-Hunyuan3DWrapper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sdbds/Hunyuan3D-2-for-windows&#34;&gt;Hunyuan3D-2-for-windows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YanWenKun/Hunyuan3D-2-WinPortable&#34;&gt;üì¶ A bundle for running on Windows | Êï¥ÂêàÂåÖ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepbeepmeep/Hunyuan3D-2GP&#34;&gt;Hunyuan3D-2GP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/darkon12/Hunyuan3D-2GP_Kaggle&#34;&gt;Kaggle Notebook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank the contributors to the &lt;a href=&#34;https://github.com/microsoft/TRELLIS&#34;&gt;Trellis&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/dinov2&#34;&gt;DINOv2&lt;/a&gt;, &lt;a href=&#34;https://github.com/Stability-AI/stablediffusion&#34;&gt;Stable Diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/black-forest-labs/flux&#34;&gt;FLUX&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co&#34;&gt;HuggingFace&lt;/a&gt;, &lt;a href=&#34;https://github.com/wyysf-98/CraftsMan3D&#34;&gt;CraftsMan3D&lt;/a&gt;, and &lt;a href=&#34;https://github.com/NeuralCarver/Michelangelo/tree/main&#34;&gt;Michelangelo&lt;/a&gt; repositories, for their open research and exploration.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;a href=&#34;https://star-history.com/#Tencent/Hunyuan3D-2&amp;amp;Date&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&amp;amp;type=Date&#34;&gt; &#xA;  &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&amp;amp;type=Date&#34;&gt; &#xA; &lt;/picture&gt; &lt;/a&gt;</summary>
  </entry>
</feed>