<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-26T01:48:11Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tloen/alpaca-lora</title>
    <updated>2023-03-26T01:48:11Z</updated>
    <id>tag:github.com,2023-03-26:/tloen/alpaca-lora</id>
    <link href="https://github.com/tloen/alpaca-lora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Instruct-tune LLaMA on consumer hardware&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;ü¶ôüå≤ü§è Alpaca-LoRA: Low-Rank LLaMA Instruct-Tuning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ü§ó &lt;strong&gt;Try the pretrained model out &lt;a href=&#34;https://huggingface.co/spaces/tloen/alpaca-lora&#34;&gt;here&lt;/a&gt;, courtesy of a GPU grant from Huggingface!&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Users have created a Discord server for discussion and support &lt;a href=&#34;https://discord.gg/prbq284xX5&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This repository contains code for reproducing the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; results using &lt;a href=&#34;https://arxiv.org/pdf/2106.09685.pdf&#34;&gt;low-rank adaptation (LoRA)&lt;/a&gt;. We provide an Instruct model of similar quality to &lt;code&gt;text-davinci-003&lt;/code&gt; that can run &lt;a href=&#34;https://twitter.com/miolini/status/1634982361757790209&#34;&gt;on a Raspberry Pi&lt;/a&gt; (for research), and the code is easily extended to the &lt;code&gt;13b&lt;/code&gt;, &lt;code&gt;30b&lt;/code&gt;, and &lt;code&gt;65b&lt;/code&gt; models.&lt;/p&gt; &#xA;&lt;p&gt;In addition to the training code, which runs within five hours on a single RTX 4090, we publish a script for downloading and inference on the foundation model and LoRA, as well as the resulting &lt;a href=&#34;https://huggingface.co/tloen/alpaca-lora-7b/tree/main&#34;&gt;LoRA weights themselves&lt;/a&gt;. To fine-tune cheaply and efficiently, we use Hugging Face&#39;s &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt; as well as Tim Dettmers&#39; &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Without hyperparameter tuning, the LoRA model produces outputs comparable to the Stanford Alpaca model. (Please see the outputs included below.) Further tuning might be able to achieve better performance; I invite interested users to give it a try and report their results.&lt;/p&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;If bitsandbytes doesn&#39;t work, &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes/raw/main/compile_from_source.md&#34;&gt;install it from source.&lt;/a&gt; Windows users can follow &lt;a href=&#34;https://github.com/tloen/alpaca-lora/issues/17&#34;&gt;these instructions&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Training (&lt;code&gt;finetune.py&lt;/code&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;This file contains a straightforward application of PEFT to the LLaMA model, as well as some code related to prompt construction and tokenization. PRs adapting this code to support larger models are always welcome.&lt;/p&gt; &#xA;&lt;p&gt;Example usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune.py \&#xA;    --base_model &#39;decapoda-research/llama-7b-hf&#39; \&#xA;    --data_path &#39;./alpaca_data_cleaned.json&#39; \&#xA;    --output_dir &#39;./lora-alpaca&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can also tweak our hyperparameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune.py \&#xA;    --base_model &#39;decapoda-research/llama-7b-hf&#39; \&#xA;    --data_path &#39;./alpaca_data_cleaned.json&#39; \&#xA;    --output_dir &#39;./lora-alpaca&#39; \&#xA;    --batch_size 128 \&#xA;    --micro_batch_size 4 \&#xA;    --num_epochs 3 \&#xA;    --learning_rate 1e-4 \&#xA;    --cutoff_len 512 \&#xA;    --val_set_size 2000 \&#xA;    --lora_r 8 \&#xA;    --lora_alpha 16 \&#xA;    --lora_dropout 0.05 \&#xA;    --lora_target_modules &#39;[q_proj,v_proj]&#39; \&#xA;    --train_on_inputs \&#xA;    --group_by_length&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference (&lt;code&gt;generate.py&lt;/code&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;This file reads the foundation model from the Hugging Face model hub and the LoRA weights from &lt;code&gt;tloen/alpaca-lora-7b&lt;/code&gt;, and runs a Gradio interface for inference on a specified input. Users should treat this as example code for the use of the model, and modify it as needed.&lt;/p&gt; &#xA;&lt;p&gt;Example usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py \&#xA;    --load_8bit \&#xA;    --base_model &#39;decapoda-research/llama-7b-hf&#39; \&#xA;    --lora_weights &#39;tloen/alpaca-lora-7b&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Checkpoint export (&lt;code&gt;export_*_checkpoint.py&lt;/code&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;These files contain scripts that merge the LoRA weights back into the base model for export to Hugging Face format and to PyTorch &lt;code&gt;state_dicts&lt;/code&gt;. They should help users who want to run inference in projects like &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; or &lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;alpaca.cpp&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Dataset&lt;/h3&gt; &#xA;&lt;p&gt;In addition to &lt;code&gt;alpaca_data.json&lt;/code&gt;, which contains the original Stanford Alpaca dataset, we also include &lt;code&gt;alpaca_data_cleaned.json&lt;/code&gt;, which has been &lt;a href=&#34;https://github.com/tloen/alpaca-lora/pull/32&#34;&gt;stripped of various tokenization artifacts&lt;/a&gt; with the help of @gururise. This file is now used by default in the training script.&lt;/p&gt; &#xA;&lt;p&gt;@AndriyMulyar has also provided interactive, embedding-based visualizations of the original dataset&#39;s &lt;a href=&#34;https://atlas.nomic.ai/map/alpaca_instructions&#34;&gt;instructions&lt;/a&gt; and &lt;a href=&#34;https://atlas.nomic.ai/map/alpaca_outputs&#34;&gt;outputs&lt;/a&gt;, as well as &lt;a href=&#34;https://atlas.nomic.ai/map/d2139cc3-bc1c-441c-8d6f-3e6ffbbc2eda/838019ff-8fe2-42ba-809a-d86d2b98cd50/-18.11668742841587/-11.348087116836096/-20.88850316347706/-17.680468640801223/774455612&#34;&gt;clusters of bad examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We can likely improve our model performance significantly if we had a better dataset. Consider supporting the &lt;a href=&#34;https://open-assistant.io/&#34;&gt;LAION Open Assistant&lt;/a&gt; effort to produce a high-quality dataset for supervised fine-tuning (or bugging them to release their data).&lt;/li&gt; &#xA; &lt;li&gt;We&#39;re continually fixing bugs and conducting training runs, and the weights on the Hugging Face Hub are being updated accordingly. In particular, those facing issues with response lengths should make sure that they have the latest version of the weights and code.&lt;/li&gt; &#xA; &lt;li&gt;Users with multiple GPUs should take a look &lt;a href=&#34;https://github.com/tloen/alpaca-lora/issues/8#issuecomment-1477490259&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Resources&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;alpaca.cpp&lt;/a&gt;, a native client for running Alpaca models on the CPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deep-diver/Alpaca-LoRA-Serve&#34;&gt;Alpaca-LoRA-Serve&lt;/a&gt;, a ChatGPT-style interface for Alpaca models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gururise/AlpacaDataCleaned&#34;&gt;AlpacaDataCleaned&lt;/a&gt;, a project to improve the quality of the Alpaca dataset&lt;/li&gt; &#xA; &lt;li&gt;Various adapter weights (download at own risk): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;7B: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tloen/alpaca-lora-7b&#34;&gt;https://huggingface.co/tloen/alpaca-lora-7b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/samwit/alpaca7B-lora&#34;&gt;https://huggingface.co/samwit/alpaca7B-lora&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üáßüá∑ &lt;a href=&#34;https://huggingface.co/22h/cabrita-lora-v0-1&#34;&gt;https://huggingface.co/22h/cabrita-lora-v0-1&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üá®üá≥ &lt;a href=&#34;https://huggingface.co/qychen/luotuo-lora-7b-0.1&#34;&gt;https://huggingface.co/qychen/luotuo-lora-7b-0.1&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üáØüáµ &lt;a href=&#34;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-7b-v0&#34;&gt;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-7b-v0&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üá´üá∑ &lt;a href=&#34;https://huggingface.co/bofenghuang/vigogne-lora-7b&#34;&gt;https://huggingface.co/bofenghuang/vigogne-lora-7b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üáπüá≠ &lt;a href=&#34;https://huggingface.co/Thaweewat/thai-buffala-lora-7b-v0-1&#34;&gt;https://huggingface.co/Thaweewat/thai-buffala-lora-7b-v0-1&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üá©üá™ &lt;a href=&#34;https://huggingface.co/thisserand/alpaca_lora_german&#34;&gt;https://huggingface.co/thisserand/alpaca_lora_german&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üáÆüáπ &lt;a href=&#34;https://huggingface.co/teelinsan/camoscio-7b-llama&#34;&gt;https://huggingface.co/teelinsan/camoscio-7b-llama&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;13B: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/chansung/alpaca-lora-13b&#34;&gt;https://huggingface.co/chansung/alpaca-lora-13b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/mattreid/alpaca-lora-13b&#34;&gt;https://huggingface.co/mattreid/alpaca-lora-13b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/samwit/alpaca13B-lora&#34;&gt;https://huggingface.co/samwit/alpaca13B-lora&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üáØüáµ &lt;a href=&#34;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-13b-v0&#34;&gt;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-13b-v0&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üá∞üá∑ &lt;a href=&#34;https://huggingface.co/chansung/koalpaca-lora-13b&#34;&gt;https://huggingface.co/chansung/koalpaca-lora-13b&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;30B: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/baseten/alpaca-30b&#34;&gt;https://huggingface.co/baseten/alpaca-30b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/chansung/alpaca-lora-30b&#34;&gt;https://huggingface.co/chansung/alpaca-lora-30b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;üáØüáµ &lt;a href=&#34;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-30b-v0&#34;&gt;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-30b-v0&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/chavinlo/alpaca-native&#34;&gt;alpaca-native&lt;/a&gt;, a replication using the original Alpaca code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Example outputs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Tell me about alpacas.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: Alpacas are members of the camelid family and are native to the Andes Mountains of South America. They are known for their soft, luxurious fleece, which is used to make clothing, blankets, and other items. Alpacas are herbivores and graze on grasses and other plants. They are social animals and live in herds of up to 20 individuals.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: Alpacas are small, fluffy animals related to camels and llamas. They are native to Peru and Bolivia, and were first domesticated around 5,000 years ago. They are kept mainly for their fine, soft fleece, which is used to make knitwear and other garments. Alpacas are herd animals and live in small family groups, led by an older male. They are highly social animals and can form strong bonds with their owners.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: Alpacas are a domesticated species of South American camelid, similar to llamas. They are native to the Andes Mountains in South America and are kept as livestock for their soft, luxurious wool. Alpacas are social animals and live in herds of up to 20 individuals. They are typically kept in small herds of two to five animals, and are relatively easy to care for. Alpacas are herbivores and feed on grass, hay, and other vegetation. They are also known for their gentle and friendly nature, making them popular as pets.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Tell me about the president of Mexico in 2019.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: The president of Mexico in 2019 was Andr√©s Manuel L√≥pez Obrador, who took office on December 1, 2018. He is a member of the National Regeneration Movement (MORENA) political party and is the first left-wing president of Mexico since 1946. He is known for his anti-corruption and anti-neoliberal policies, as well as his commitment to improving the living conditions of the Mexican people.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: In 2019, Andres Manuel Lopez Obrador was sworn in as President of Mexico. He is the country‚Äôs 58th president and the first from the left-wing Morena party. He was elected in 2018 on a promise to combat corruption and reduce poverty and has faced criticism for his handling of the economy, crime and the country‚Äôs relationship with the United States. He is also known for his ambitious social welfare programs and has been praised for raising the minimum wage and providing aid to low-income families. He is seen as a champion of democracy and human rights in Mexico.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: The president of Mexico in 2019 is Andr√©s Manuel L√≥pez Obrador, commonly known as AMLO. He is a Mexican politician who has been the President of Mexico since December 1, 2018. He is a member of the National Regeneration Movement (MORENA) party and previously served as the Head of Government of Mexico City from 2000 to 2005. He is a left-wing populist and has promised to reduce corruption and inequality in Mexico. He has also promised to increase public spending on social programs and infrastructure projects.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Tell me about the king of France in 2019.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: The King of France in 2019 is Emmanuel Macron, who was elected in May 2017. He is the youngest president in the history of the Fifth Republic and the first president to be born after World War II. He is also the first president to have never held elected office before.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: In 2019, the King of France was Emmanuel Macron. He was sworn in as President of France on May 14, 2017, and has been leading the country ever since. He is a member of the centrist La R√©publique En Marche! Party. He is known for his reformist agenda and has faced a number of protests from both the right and left of the political spectrum. He is the youngest French president in history.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: The King of France is a largely symbolic role, as France is a republic. The current President of France is Emmanuel Macron, who was elected in 2017.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: List all Canadian provinces in alphabetical order.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island, Quebec, Saskatchewan&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island, Quebec, Saskatchewan, Northwest Territories, Nunavut, Yukon.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Alberta&lt;/li&gt; &#xA; &lt;li&gt;British Columbia&lt;/li&gt; &#xA; &lt;li&gt;Manitoba&lt;/li&gt; &#xA; &lt;li&gt;New Brunswick&lt;/li&gt; &#xA; &lt;li&gt;Newfoundland and Labrador&lt;/li&gt; &#xA; &lt;li&gt;Northwest Territories&lt;/li&gt; &#xA; &lt;li&gt;Nova Scotia&lt;/li&gt; &#xA; &lt;li&gt;Nunavut&lt;/li&gt; &#xA; &lt;li&gt;Ontario&lt;/li&gt; &#xA; &lt;li&gt;Prince Edward Island&lt;/li&gt; &#xA; &lt;li&gt;Quebec&lt;/li&gt; &#xA; &lt;li&gt;Saskatchewan&lt;/li&gt; &#xA; &lt;li&gt;Yukon&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Write a Python program that prints the first 10 Fibonacci numbers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def fibonacci(n):&#xA;    if n == 0:&#xA;        return 0&#xA;    elif n == 1:&#xA;        return 1&#xA;    else:&#xA;        return fibonacci(n-1) + fibonacci(n-2)&#xA;&#xA;print(fibonacci(10))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;numbers = [0, 1]&#xA;&#xA;for i in range(2, 11):&#xA;    numbers.append(numbers[i-2] + numbers[i-1])&#xA;&#xA;print(numbers)&#xA;# Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fibonacci numbers are the numbers in the following integer sequence, called the Fibonacci sequence, and characterized by the fact that every number after the first two is the sum of the two preceding ones.&#xA;&#xA;# Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...&#xA;&#xA;# Program:&#xA;&#xA;# Initialize the first two numbers of the Fibonacci sequence&#xA;a = 0&#xA;b = 1&#xA;&#xA;# Print the first 10 numbers of the Fibonacci sequence&#xA;print(a)&#xA;print(b)&#xA;&#xA;for i in range(2,10):&#xA;    c = a + b&#xA;    a = b&#xA;    b = c&#xA;    print(c)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Write a program that prints the numbers from 1 to 100. But for multiples of three print &#39;Fizz&#39; instead of the number and for the multiples of five print &#39;Buzz&#39;. For numbers which are multiples of both three and five print &#39;FizzBuzz&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(1, 101):&#xA;    if i % 3 == 0 and i % 5 == 0:&#xA;        print(&#34;FizzBuzz&#34;)&#xA;    elif i % 3 == 0:&#xA;        print(&#34;Fizz&#34;)&#xA;    elif i % 5 == 0:&#xA;        print(&#34;Buzz&#34;)&#xA;    else:&#xA;        print(i)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(1, 101):&#xA;    if i % 3 == 0 and i % 5 == 0:&#xA;        print(&#39;FizzBuzz&#39;)&#xA;    elif i % 3 == 0:&#xA;        print(&#39;Fizz&#39;)&#xA;    elif i % 5 == 0:&#xA;        print(&#39;Buzz&#39;)&#xA;    else:&#xA;        print(i)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;for (let i = 1; i &amp;lt;= 100; i++) {&#xA;  if (i % 3 === 0 &amp;amp;&amp;amp; i % 5 === 0) {&#xA;    console.log(&#34;FizzBuzz&#34;);&#xA;  } else if (i % 3 === 0) {&#xA;    console.log(&#34;Fizz&#34;);&#xA;  } else if (i % 5 === 0) {&#xA;    console.log(&#34;Buzz&#34;);&#xA;  } else {&#xA;    console.log(i);&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Tell me five words that rhyme with &#39;shock&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: Flock, lock, rock, stock, and sock.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: Five words that rhyme with shock are: rock, pop, shock, cook, and snock.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Block&lt;/li&gt; &#xA; &lt;li&gt;Lock&lt;/li&gt; &#xA; &lt;li&gt;Dock&lt;/li&gt; &#xA; &lt;li&gt;Clock&lt;/li&gt; &#xA; &lt;li&gt;Rock&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Translate the sentence &#39;I have no mouth but I must scream&#39; into Spanish.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: No tengo boca pero tengo que gritar.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: No tengo boca, pero debo gritar.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: No tengo boca pero debo gritar.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cocktailpeanut/dalai</title>
    <updated>2023-03-26T01:48:11Z</updated>
    <id>tag:github.com,2023-03-26:/cocktailpeanut/dalai</id>
    <link href="https://github.com/cocktailpeanut/dalai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The simplest way to run LLaMA on your local machine&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dalai&lt;/h1&gt; &#xA;&lt;p&gt;Run LLaMA and Alpaca on your computer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cocktailpeanut/dalai&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-github&#34;&gt;&lt;/i&gt; GitHub&lt;/a&gt; &lt;a href=&#34;https://twitter.com/cocktailpeanut&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-twitter&#34;&gt;&lt;/i&gt; Twitter&lt;/a&gt; &lt;a href=&#34;https://discord.gg/WWfgrzzkCT&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-discord&#34;&gt;&lt;/i&gt; Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;JUST RUN THIS&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/npx2.png&#34; class=&#34;round&#34;&gt; &#xA;&lt;h2&gt;TO GET&lt;/h2&gt; &#xA;&lt;p&gt;Both alpaca and llama working on your computer!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/alpaca.gif&#34; alt=&#34;alpaca.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Powered by &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;, &lt;a href=&#34;https://github.com/shawwn/llama-dl&#34;&gt;llama-dl CDN&lt;/a&gt;, and &lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;alpaca.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hackable web app included&lt;/li&gt; &#xA; &lt;li&gt;Ships with JavaScript API&lt;/li&gt; &#xA; &lt;li&gt;Ships with &lt;a href=&#34;https://socket.io/&#34;&gt;Socket.io&lt;/a&gt; API&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Intro&lt;/h1&gt; &#xA;&lt;h2&gt;1. Cross platform&lt;/h2&gt; &#xA;&lt;p&gt;Dalai runs on all of the following operating systems:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Linux&lt;/li&gt; &#xA; &lt;li&gt;Mac&lt;/li&gt; &#xA; &lt;li&gt;Windows&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;2. Memory Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Runs on most modern computers. Unless your computer is very very old, it should work.&lt;/p&gt; &#xA;&lt;p&gt;According to &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/issues/13&#34;&gt;a llama.cpp discussion thread&lt;/a&gt;, here are the memory requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;7B =&amp;gt; ~4 GB&lt;/li&gt; &#xA; &lt;li&gt;13B =&amp;gt; ~8 GB&lt;/li&gt; &#xA; &lt;li&gt;30B =&amp;gt; ~16 GB&lt;/li&gt; &#xA; &lt;li&gt;65B =&amp;gt; ~32 GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;3. Disk Space Requirements&lt;/h2&gt; &#xA;&lt;h3&gt;Alpaca&lt;/h3&gt; &#xA;&lt;p&gt;Currently 7B and 13B models are available via &lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;alpaca.cpp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;7B&lt;/h4&gt; &#xA;&lt;p&gt;Alpaca comes fully quantized (compressed), and the only space you need for the 7B model is 4.21GB:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/alpaca_7b.png&#34; alt=&#34;alpaca_7b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;13B&lt;/h4&gt; &#xA;&lt;p&gt;Alpaca comes fully quantized (compressed), and the only space you need for the 13B model is 8.14GB:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/alpaca_13b.png&#34; alt=&#34;alpaca_13b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;LLaMA&lt;/h3&gt; &#xA;&lt;p&gt;You need a lot of space for storing the models. &lt;strong&gt;The model name must be one of: 7B, 13B, 30B, and 65B.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You do NOT have to install all models, you can install one by one. Let&#39;s take a look at how much space each model takes up:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTE&lt;/p&gt; &#xA; &lt;p&gt;The following numbers assume that you DO NOT touch the original model files and keep BOTH the original model files AND the quantized versions.&lt;/p&gt; &#xA; &lt;p&gt;You can optimize this if you delete the original models (which are much larger) after installation and keep only the quantized versions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;7B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 31.17GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 4.21GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/7b.png&#34; alt=&#34;7b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;13B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 60.21GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 4.07GB * 2 = 8.14GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/13b.png&#34; alt=&#34;13b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;30B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 150.48GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 5.09GB * 4 = 20.36GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/30b.png&#34; alt=&#34;30b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;65B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 432.64GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 5.11GB * 8 = 40.88GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/65b.png&#34; alt=&#34;65b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Quickstart&lt;/h1&gt; &#xA;&lt;h2&gt;Docker compose&lt;/h2&gt; &#xA;&lt;p&gt;Requires that you have docker installed and running.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose build&#xA;docker compose run dalai npx dalai alpaca install 7B # or a different model&#xA;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will dave the models in the &lt;code&gt;./models&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;p&gt;View the site at &lt;a href=&#34;http://127.0.0.1:3000/&#34;&gt;http://127.0.0.1:3000/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Mac&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install node.js &amp;gt;= 18&lt;/h3&gt; &#xA;&lt;p&gt;If your mac doesn&#39;t have node.js installed yet, make sure to install node.js &amp;gt;= 18&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en/download/&#34; class=&#34;btn&#34;&gt;Install Node.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Step 2.1. Install models&lt;/h3&gt; &#xA;&lt;p&gt;Currently supported engines are &lt;code&gt;llama&lt;/code&gt; and &lt;code&gt;alpaca&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Add alpaca models&lt;/h4&gt; &#xA;&lt;p&gt;To download alpaca models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai alpaca install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Add llama models&lt;/h4&gt; &#xA;&lt;p&gt;To download llama models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to download multiple models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B 13B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now go to step 3.&lt;/p&gt; &#xA;&lt;h3&gt;Step 2.2. Troubleshoot&lt;/h3&gt; &#xA;&lt;p&gt;Normally you don&#39;t need this step, but if running the commands above don&#39;t do anything and immediately end, it means something went wrong because some of the required modules are not installed on your system.&lt;/p&gt; &#xA;&lt;p&gt;In that case, try the following steps:&lt;/p&gt; &#xA;&lt;h4&gt;1. Install homebrew&lt;/h4&gt; &#xA;&lt;p&gt;In case homebrew is not installed on your computer, install it by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;/bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Or you can find the same instruction on the homebrew hompage: &lt;a href=&#34;https://brew.sh/&#34;&gt;https://brew.sh/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2. Install dependencies&lt;/h4&gt; &#xA;&lt;p&gt;Once homebrew is installed, install these dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install cmake&#xA;brew install pkg-config&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Update NPM&lt;/h4&gt; &#xA;&lt;p&gt;Just to make sure we cover every vector, let&#39;s update NPM as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install -g npm@latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now go back to step 2.1 and try running the &lt;code&gt;npx dalai&lt;/code&gt; commands again.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, run the following command to launch the web UI server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Windows&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install Visual Studio&lt;/h3&gt; &#xA;&lt;p&gt;On windows, you need to install Visual Studio before installing Dalai.&lt;/p&gt; &#xA;&lt;p&gt;Press the button below to visit the Visual Studio downloads page and download:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34; class=&#34;btn&#34;&gt;Download Microsoft Visual Studio&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT!!!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;When installing Visual Studio, make sure to check the 3 options as highlighted below:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python development&lt;/li&gt; &#xA; &lt;li&gt;Node.js development&lt;/li&gt; &#xA; &lt;li&gt;Desktop development with C++&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/vs.png&#34; alt=&#34;vs.png&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Step 2.1. Install models&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;On Windows, make sure to run all commands in &lt;strong&gt;cmd&lt;/strong&gt;.&lt;/p&gt; &#xA; &lt;p&gt;DO NOT run in &lt;strong&gt;powershell&lt;/strong&gt;. Powershell has unnecessarily strict permissions and makes the script fail silently.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Currently supported engines are &lt;code&gt;llama&lt;/code&gt; and &lt;code&gt;alpaca&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Install alpaca&lt;/h4&gt; &#xA;&lt;p&gt;To download alpaca models. Open your &lt;code&gt;cmd&lt;/code&gt; application and enter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai alpaca install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Add llama models&lt;/h4&gt; &#xA;&lt;p&gt;To download llama models. Open your &lt;code&gt;cmd&lt;/code&gt; application and enter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to download multiple models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B 13B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Step 2.2. Troubleshoot (optional)&lt;/h3&gt; &#xA;&lt;p&gt;In case above steps fail, try installing Node.js and Python separately.&lt;/p&gt; &#xA;&lt;p&gt;Install Python:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/ftp/python/3.10.10/python-3.10.10-embed-amd64.zip&#34; class=&#34;btn&#34;&gt;Download Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install Node.js &amp;gt;= 18:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en/download/&#34; class=&#34;btn&#34;&gt;Download Node.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;After both have been installed, open powershell and type &lt;code&gt;python&lt;/code&gt; to see if the application exists. And also type &lt;code&gt;node&lt;/code&gt; to see if the application exists as well.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;ve checked that they both exist, try again.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, run the following command to launch the web UI server (Make sure to run in &lt;code&gt;cmd&lt;/code&gt; and not powershell!):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Linux&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;You need to make sure you have the correct version of Python and Node.js installed.&lt;/p&gt; &#xA;&lt;h4&gt;Step 1.1. Python &amp;lt;= 3.10&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pimylifeup.com/installing-python-on-linux/&#34; class=&#34;btn&#34;&gt;Download Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Make sure the version is 3.10 or lower (not 3.11) Python must be 3.10 or below (pytorch and other libraries are not supported yet on the latest)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Step 1.2. Node.js &amp;gt;= 18&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en/download/package-manager/&#34; class=&#34;btn&#34;&gt;Download node.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Make sure the version is 18 or higher&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Step 2.1. Install models&lt;/h3&gt; &#xA;&lt;p&gt;Currently supported engines are &lt;code&gt;llama&lt;/code&gt; and &lt;code&gt;alpaca&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Add alpaca models&lt;/h4&gt; &#xA;&lt;p&gt;To download alpaca models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai alpaca install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Add llama models&lt;/h4&gt; &#xA;&lt;p&gt;To download llama models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to download multiple models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B 13B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2.2. Troubleshoot&lt;/h3&gt; &#xA;&lt;p&gt;In case the model install silently fails or hangs forever, try the following command, and try running the npx command again:&lt;/p&gt; &#xA;&lt;p&gt;On ubuntu/debian/etc.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install build-essential python3-venv -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On fedora/etc.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dnf install make automake gcc gcc-c++ kernel-devel python3-virtualenv -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 3. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, run the following command to launch the web UI server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;API&lt;/h1&gt; &#xA;&lt;p&gt;Dalai is also an NPM package:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;programmatically install&lt;/li&gt; &#xA; &lt;li&gt;locally make requests to the model&lt;/li&gt; &#xA; &lt;li&gt;run a dalai server (powered by socket.io)&lt;/li&gt; &#xA; &lt;li&gt;programmatically make requests to a remote dalai server (via socket.io)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Dalai is an NPM package. You can install it using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install dalai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;1. constructor()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai(home)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;home&lt;/code&gt;: (optional) manually specify the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By default, Dalai automatically stores the entire &lt;code&gt;llama.cpp&lt;/code&gt; repository under &lt;code&gt;~/llama.cpp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;However, often you may already have a &lt;code&gt;llama.cpp&lt;/code&gt; repository somewhere else on your machine and want to just use that folder. In this case you can pass in the &lt;code&gt;home&lt;/code&gt; attribute.&lt;/p&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;h4&gt;Basic&lt;/h4&gt; &#xA;&lt;p&gt;Creates a workspace at &lt;code&gt;~/llama.cpp&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Custom path&lt;/h4&gt; &#xA;&lt;p&gt;Manually set the &lt;code&gt;llama.cpp&lt;/code&gt; path:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai(&#34;/Documents/llama.cpp&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;2. request()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.request(req, callback)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;req&lt;/code&gt;: a request object. made up of the following attributes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;prompt&lt;/code&gt;: &lt;strong&gt;(required)&lt;/strong&gt; The prompt string&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: &lt;strong&gt;(required)&lt;/strong&gt; The model type + model name to query. Takes the following form: &lt;code&gt;&amp;lt;model_type&amp;gt;.&amp;lt;model_name&amp;gt;&lt;/code&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Example: &lt;code&gt;alpaca.7B&lt;/code&gt;, &lt;code&gt;llama.13B&lt;/code&gt;, ...&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;url&lt;/code&gt;: only needed if connecting to a remote dalai server &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;if unspecified, it uses the node.js API to directly run dalai locally&lt;/li&gt; &#xA;     &lt;li&gt;if specified (for example &lt;code&gt;ws://localhost:3000&lt;/code&gt;) it looks for a socket.io endpoint at the URL and connects to it.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;threads&lt;/code&gt;: The number of threads to use (The default is 8 if unspecified)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;n_predict&lt;/code&gt;: The number of tokens to return (The default is 128 if unspecified)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;seed&lt;/code&gt;: The seed. The default is -1 (none)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;top_k&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;top_p&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;repeat_last_n&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;repeat_penalty&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;temp&lt;/code&gt;: temperature&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;batch_size&lt;/code&gt;: batch size&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;skip_end&lt;/code&gt;: by default, every session ends with &lt;code&gt;\n\n&amp;lt;end&amp;gt;&lt;/code&gt;, which can be used as a marker to know when the full response has returned. However sometimes you may not want this suffix. Set &lt;code&gt;skip_end: true&lt;/code&gt; and the response will no longer end with &lt;code&gt;\n\n&amp;lt;end&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;callback&lt;/code&gt;: the streaming callback function that gets called every time the client gets any token response back from the model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;h4&gt;1. Node.js&lt;/h4&gt; &#xA;&lt;p&gt;Using node.js, you just need to initialize a Dalai object with &lt;code&gt;new Dalai()&lt;/code&gt; and then use it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#39;dalai&#39;)&#xA;new Dalai().request({&#xA;  model: &#34;7B&#34;,&#xA;  prompt: &#34;The following is a conversation between a boy and a girl:&#34;,&#xA;}, (token) =&amp;gt; {&#xA;  process.stdout.write(token)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Non node.js (socket.io)&lt;/h4&gt; &#xA;&lt;p&gt;To make use of this in a browser or any other language, you can use thie socket.io API.&lt;/p&gt; &#xA;&lt;h5&gt;Step 1. start a server&lt;/h5&gt; &#xA;&lt;p&gt;First you need to run a Dalai socket server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// server.js&#xA;const Dalai = require(&#39;dalai&#39;)&#xA;new Dalai().serve(3000)     // port 3000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Step 2. connect to the server&lt;/h5&gt; &#xA;&lt;p&gt;Then once the server is running, simply make requests to it by passing the &lt;code&gt;ws://localhost:3000&lt;/code&gt; socket url when initializing the Dalai object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;)&#xA;new Dalai().request({&#xA;  url: &#34;ws://localhost:3000&#34;,&#xA;  model: &#34;7B&#34;,&#xA;  prompt: &#34;The following is a conversation between a boy and a girl:&#34;,&#xA;}, (token) =&amp;gt; {&#xA;  console.log(&#34;token&#34;, token)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;3. serve()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;p&gt;Starts a socket.io server at &lt;code&gt;port&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.serve(port)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;)&#xA;new Dalai().serve(3000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;4. http()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;p&gt;connect with an existing &lt;code&gt;http&lt;/code&gt; instance (The &lt;code&gt;http&lt;/code&gt; npm package)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.http(http)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;http&lt;/code&gt;: The &lt;a href=&#34;https://nodejs.org/api/http.html&#34;&gt;http&lt;/a&gt; object&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;This is useful when you&#39;re trying to plug dalai into an existing node.js web app&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const app = require(&#39;express&#39;)();&#xA;const http = require(&#39;http&#39;).Server(app);&#xA;dalai.http(http)&#xA;http.listen(3000, () =&amp;gt; {&#xA;  console.log(&#34;server started&#34;)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;5. install()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;await dalai.install(model_type, model_name1, model_name2, ...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model_type&lt;/code&gt;: the name of the model. currently supports: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;alpaca&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&#34;llama&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model1&lt;/code&gt;, &lt;code&gt;model2&lt;/code&gt;, ...: the model names to install (&#34;7B&#34;`, &#34;13B&#34;, &#34;30B&#34;, &#34;65B&#34;, etc)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;Install Llama &#34;7B&#34; and &#34;13B&#34; models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;await dalai.install(&#34;llama&#34;, &#34;7B&#34;, &#34;13B&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install alpaca 7B model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;await dalai.install(&#34;alpaca&#34;, &#34;7B&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;6. installed()&lt;/h2&gt; &#xA;&lt;p&gt;returns the array of installed models&lt;/p&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const models = await dalai.installed()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;const models = await dalai.installed()&#xA;console.log(models)     // prints [&#34;7B&#34;, &#34;13B&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!--&#xA;&#xA;---&#xA;&#xA;## 7. download()&#xA;&#xA;Download models.&#xA;&#xA;There are two download options:&#xA;&#xA;1. **LLaMA:** Download the original LLaMA model, convert it, and quantize (compress) it&#xA;2. **LLaMA.zip:** Download the compressed version (generated from step 1 and published on HuggingFace)&#xA;&#xA;### Syntax&#xA;&#xA;```javascript&#xA;await dalai.download(model1, model2, model3, ...)&#xA;```&#xA;&#xA;- `models`: the model names to install. Can be: &#34;7B&#34;`, &#34;13B&#34;, &#34;30B&#34;, &#34;65B&#34;, &#34;7B.zip&#34;, &#34;13B.zip&#34;, &#34;30B.zip&#34;, &#34;65B.zip&#34;&#xA;  - &#34;7B&#34;, &#34;13B&#34;, &#34;30B&#34;, &#34;65B&#34;: download the raw model, convert, and quantize&#xA;  - &#34;7B.zip&#34;, &#34;13B.zip&#34;, &#34;30B.zip&#34;, &#34;65B.zip&#34;: download the quantized model (no need to waste time downloading huge files)&#xA;&#xA;### Examples&#xA;&#xA;Install the &#34;7B&#34; and &#34;13B&#34; models:&#xA;&#xA;&#xA;```javascript&#xA;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;await dalai.install(&#34;7B&#34;, &#34;13B&#34;)&#xA;```&#xA;&#xA;--&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h2&gt;Using a different home folder&lt;/h2&gt; &#xA;&lt;p&gt;By default Dalai uses your home directory to store the entire repository (&lt;code&gt;~/dalai&lt;/code&gt;). However sometimes you may want to store the archive elsewhere.&lt;/p&gt; &#xA;&lt;p&gt;In this case you can call all CLI methods using the &lt;code&gt;--home&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;h3&gt;1. Installing models to a custom path&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B --home ~/test_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Serving from the custom path&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve --home ~/test_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Updating to the latest&lt;/h2&gt; &#xA;&lt;p&gt;To make sure you update to the latest, first find the latest version at &lt;a href=&#34;https://www.npmjs.com/package/dalai&#34;&gt;https://www.npmjs.com/package/dalai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s say the latest version is &lt;code&gt;0.3.0&lt;/code&gt;. To update the dalai version, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai@0.3.0 setup&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Staying up to date&lt;/h2&gt; &#xA;&lt;p&gt;Have questions or feedback? Follow the project through the following outlets:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cocktailpeanut/dalai&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-github&#34;&gt;&lt;/i&gt; GitHub&lt;/a&gt; &lt;a href=&#34;https://twitter.com/cocktailpeanut&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-twitter&#34;&gt;&lt;/i&gt; Twitter&lt;/a&gt; &lt;a href=&#34;https://discord.gg/WWfgrzzkCT&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-discord&#34;&gt;&lt;/i&gt; Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>GaiZhenbiao/ChuanhuChatGPT</title>
    <updated>2023-03-26T01:48:11Z</updated>
    <id>tag:github.com,2023-03-26:/GaiZhenbiao/ChuanhuChatGPT</id>
    <link href="https://github.com/GaiZhenbiao/ChuanhuChatGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GUI for ChatGPT API&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;Â∑ùËôé ChatGPT üêØ Chuanhu ChatGPT&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/GaiZhenBiao/ChuanhuChatGPT&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/70903329/227087087-93b37d64-7dc3-4738-a518-c1cf05591c8a.png&#34; alt=&#34;Logo&#34; height=&#34;156&#34;&gt; &lt;/a&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA; &lt;h3&gt;‰∏∫ChatGPT APIÊèê‰æõ‰∫Ü‰∏Ä‰∏™ËΩªÂø´Â•ΩÁî®ÁöÑWebÂõæÂΩ¢ÁïåÈù¢&lt;/h3&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;Tests Passing&#34; src=&#34;https://img.shields.io/github/license/GaiZhenbiao/ChuanhuChatGPT&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://gradio.app/&#34;&gt; &lt;img alt=&#34;GitHub Contributors&#34; src=&#34;https://img.shields.io/badge/Base-Gradio-fb7d1a?style=flat&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/GaiZhenBiao/ChuanhuChatGPT/graphs/contributors&#34;&gt; &lt;img alt=&#34;GitHub Contributors&#34; src=&#34;https://img.shields.io/github/contributors/GaiZhenBiao/ChuanhuChatGPT&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/GaiZhenBiao/ChuanhuChatGPT/issues&#34;&gt; &lt;img alt=&#34;Issues&#34; src=&#34;https://img.shields.io/github/issues/GaiZhenBiao/ChuanhuChatGPT?color=0088ff&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/GaiZhenBiao/ChuanhuChatGPT/pulls&#34;&gt; &lt;img alt=&#34;GitHub pull requests&#34; src=&#34;https://img.shields.io/github/issues-pr/GaiZhenBiao/ChuanhuChatGPT?color=0088ff&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA; &lt;p&gt; ÂÆûÊó∂ÂõûÂ§ç / Êó†ÈôêÂØπËØù / ‰øùÂ≠òÂØπËØùËÆ∞ÂΩï / È¢ÑËÆæPromptÈõÜ / ËÅîÁΩëÊêúÁ¥¢ / Ê†πÊçÆÊñá‰ª∂ÂõûÁ≠î &lt;br&gt; Ê∏≤ÊüìLaTex / Ê∏≤ÊüìË°®Ê†º / Ê∏≤Êüì‰ª£Á†Å / ‰ª£Á†ÅÈ´ò‰∫Æ / Ëá™ÂÆö‰πâapi-URL / ‚ÄúÂ∞èËÄåÁæé‚ÄùÁöÑ‰ΩìÈ™å / Ready for GPT-4 &lt;/p&gt; &#xA; &lt;a href=&#34;https://www.bilibili.com/video/BV1mo4y1r7eE&#34;&gt;&lt;strong&gt;ËßÜÈ¢ëÊïôÁ®ã&lt;/strong&gt;&lt;/a&gt; ¬∑ &#xA; &lt;a href=&#34;https://www.bilibili.com/video/BV1184y1w7aP&#34;&gt;&lt;strong&gt;2.0‰ªãÁªçËßÜÈ¢ë&lt;/strong&gt;&lt;/a&gt; || &#xA; &lt;a href=&#34;https://huggingface.co/spaces/JohnSmith9982/ChuanhuChatGPT&#34;&gt;&lt;strong&gt;Âú®Á∫ø‰ΩìÈ™å&lt;/strong&gt;&lt;/a&gt; ¬∑ &#xA; &lt;a href=&#34;https://huggingface.co/login?next=%2Fspaces%2FJohnSmith9982%2FChuanhuChatGPT%3Fduplicate%3Dtrue&#34;&gt;&lt;strong&gt;‰∏ÄÈîÆÈÉ®ÁΩ≤&lt;/strong&gt;&lt;/a&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Animation Demo&#34; src=&#34;https://user-images.githubusercontent.com/51039745/226255695-6b17ff1f-ea8d-464f-b69b-a7b6b68fffe8.gif&#34;&gt; &lt;/p&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;ÁõÆÂΩï&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GaiZhenbiao/ChuanhuChatGPT/main/#%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7&#34;&gt;‰ΩøÁî®ÊäÄÂ∑ß&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GaiZhenbiao/ChuanhuChatGPT/main/#%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F&#34;&gt;ÂÆâË£ÖÊñπÂºè&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GaiZhenbiao/ChuanhuChatGPT/main/#%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87%E8%A7%A3%E5%86%B3&#34;&gt;ÁñëÈöæÊùÇÁóáËß£ÂÜ≥&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GaiZhenbiao/ChuanhuChatGPT/main/#%E6%8D%90%E6%AC%BE&#34;&gt;Áªô‰ΩúËÄÖ‰π∞ÂèØ‰πêü•§&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;‰ΩøÁî®ÊäÄÂ∑ß&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‰ΩøÁî®System PromptÂèØ‰ª•ÂæàÊúâÊïàÂú∞ËÆæÂÆöÂâçÊèêÊù°‰ª∂„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;‰ΩøÁî®PromptÊ®°ÊùøÂäüËÉΩÊó∂ÔºåÈÄâÊã©PromptÊ®°ÊùøÈõÜÂêàÊñá‰ª∂ÔºåÁÑ∂Âêé‰ªé‰∏ãÊãâËèúÂçï‰∏≠ÈÄâÊã©ÊÉ≥Ë¶ÅÁöÑprompt„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;Â¶ÇÊûúÂõûÁ≠î‰∏çÊª°ÊÑèÔºåÂèØ‰ª•‰ΩøÁî®&lt;code&gt;ÈáçÊñ∞ÁîüÊàê&lt;/code&gt;ÊåâÈíÆÂÜçËØï‰∏ÄÊ¨°&lt;/li&gt; &#xA; &lt;li&gt;ÂØπ‰∫éÈïøÂØπËØùÔºåÂèØ‰ª•‰ΩøÁî®&lt;code&gt;‰ºòÂåñTokens&lt;/code&gt;ÊåâÈíÆÂáèÂ∞ëTokensÂç†Áî®„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ËæìÂÖ•Ê°ÜÊîØÊåÅÊç¢Ë°åÔºåÊåâ&lt;code&gt;shift enter&lt;/code&gt;Âç≥ÂèØ„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ÈÉ®ÁΩ≤Âà∞ÊúçÂä°Âô®ÔºöÂ∞ÜÁ®ãÂ∫èÊúÄÂêé‰∏ÄÂè•ÊîπÊàê&lt;code&gt;demo.launch(server_name=&#34;0.0.0.0&#34;, server_port=&amp;lt;‰Ω†ÁöÑÁ´ØÂè£Âè∑&amp;gt;)&lt;/code&gt;„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;Ëé∑ÂèñÂÖ¨ÂÖ±ÈìæÊé•ÔºöÂ∞ÜÁ®ãÂ∫èÊúÄÂêé‰∏ÄÂè•ÊîπÊàê&lt;code&gt;demo.launch(share=True)&lt;/code&gt;„ÄÇÊ≥®ÊÑèÁ®ãÂ∫èÂøÖÈ°ªÂú®ËøêË°åÔºåÊâçËÉΩÈÄöËøáÂÖ¨ÂÖ±ÈìæÊé•ËÆøÈóÆ„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;Âú®Hugging Face‰∏ä‰ΩøÁî®ÔºöÂª∫ËÆÆÂú®Âè≥‰∏äËßí &lt;strong&gt;Â§çÂà∂Space&lt;/strong&gt; ÂÜç‰ΩøÁî®ÔºåËøôÊ†∑AppÂèçÂ∫îÂèØËÉΩ‰ºöÂø´‰∏ÄÁÇπ„ÄÇ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÂÆâË£ÖÊñπÂºè&lt;/h2&gt; &#xA;&lt;h3&gt;Áõ¥Êé•Âú®Hugging Face‰∏äÈÉ®ÁΩ≤&lt;/h3&gt; &#xA;&lt;p&gt;ËÆøÈóÆ&lt;a href=&#34;https://huggingface.co/spaces/JohnSmith9982/ChuanhuChatGPT&#34;&gt;Êú¨È°πÁõÆÁöÑHugging FaceÈ°µÈù¢&lt;/a&gt;ÔºåÁÇπÂáªÂè≥‰∏äËßíÁöÑ &lt;strong&gt;Duplicate Space&lt;/strong&gt; ÔºàÂ§çÂà∂Á©∫Èó¥ÔºâÔºåÊñ∞Âª∫‰∏Ä‰∏™ÁßÅ‰∫∫Á©∫Èó¥„ÄÇÁÑ∂ÂêéÂ∞±Áõ¥Êé•ÂèØ‰ª•ÂºÄÂßã‰ΩøÁî®Âï¶ÔºÅÊîæÂøÉÔºåËøôÊòØÂÖçË¥πÁöÑ„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;ÊÇ®ÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®ÊàëÁöÑÁ©∫Èó¥ÔºåËøôÊ†∑ËÉΩÂÆûÊó∂‰∫´ÂèóÂà∞ÊúÄÊñ∞ÂäüËÉΩ„ÄÇÊÇ®‰πüÂèØ‰ª•Â∞ÜÈ°πÁõÆÂ§çÂà∂‰∏∫ÁßÅ‰∫∫Á©∫Èó¥Èáå‰ΩøÁî®ÔºåËøôÊ†∑AppÂèçÂ∫îÂèØËÉΩ‰ºöÂø´‰∏ÄÁÇπ„ÄÇ&lt;/p&gt; &#xA;&lt;img width=&#34;300&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/51039745/223447310-e098a1f2-0dcf-48d6-bcc5-49472dd7ca0d.png&#34;&gt; &#xA;&lt;p&gt;Hugging FaceÁöÑ‰ºòÁÇπÔºöÂÖçË¥πÔºåÊó†ÈúÄÈÖçÁΩÆ‰ª£ÁêÜÔºåÈÉ®ÁΩ≤ÂÆπÊòìÔºàÁîöËá≥‰∏çÈúÄË¶ÅÁîµËÑëÔºâ„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Hugging FaceÁöÑÁº∫ÁÇπÔºö‰∏çÊîØÊåÅÊüê‰∫õÁïåÈù¢Ê†∑Âºè„ÄÇ&lt;/p&gt; &#xA;&lt;h3&gt;ÊâãÂä®Êú¨Âú∞ÈÉ®ÁΩ≤&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;‰∏ãËΩΩÊú¨È°πÁõÆ&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/GaiZhenbiao/ChuanhuChatGPT.git&#xA;cd ChuanhuChatGPT&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;ÊàñËÄÖÔºåÁÇπÂáªÁΩëÈ°µÂè≥‰∏äËßíÁöÑ &lt;code&gt;Download ZIP&lt;/code&gt;Ôºå‰∏ãËΩΩÂπ∂Ëß£ÂéãÂÆåÊàêÂêéËøõÂÖ•Êñá‰ª∂Â§πÔºåËøõÂÖ•&lt;code&gt;ÁªàÁ´Ø&lt;/code&gt;Êàñ&lt;code&gt;ÂëΩ‰ª§ÊèêÁ§∫Á¨¶&lt;/code&gt;„ÄÇ&lt;/p&gt; &lt;p&gt;Â¶ÇÊûú‰Ω†‰ΩøÁî®WindowsÔºåÂ∫îËØ•Âú®Êñá‰ª∂Â§πÈáåÊåâ‰Ωè&lt;code&gt;shift&lt;/code&gt;Âè≥ÈîÆÔºåÈÄâÊã©‚ÄúÂú®ÁªàÁ´Ø‰∏≠ÊâìÂºÄ‚Äù„ÄÇÂ¶ÇÊûúÊ≤°ÊúâËøô‰∏™ÈÄâÈ°πÔºåÈÄâÊã©‚ÄúÂú®Ê≠§Â§ÑÊâìÂºÄPowershellÁ™óÂè£‚Äù„ÄÇÂ¶ÇÊûú‰Ω†‰ΩøÁî®macOSÔºåÂèØ‰ª•Âú®FinderÂ∫ïÈÉ®ÁöÑË∑ØÂæÑÊ†è‰∏≠Âè≥ÈîÆÂΩìÂâçÊñá‰ª∂Â§πÔºåÈÄâÊã©&lt;code&gt;ÊúçÂä°-Êñ∞Âª∫‰Ωç‰∫éÊñá‰ª∂Â§π‰ΩçÁΩÆÁöÑÁªàÁ´ØÊ†áÁ≠æÈ°µ&lt;/code&gt;„ÄÇ&lt;/p&gt; &lt;img width=&#34;200&#34; alt=&#34;downloadZIP&#34; src=&#34;https://user-images.githubusercontent.com/23137268/223696317-b89d2c71-c74d-4c6d-8060-a21406cfb8c8.png&#34;&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Â°´ÂÜôAPIÂØÜÈí•&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‰ª•‰∏ã3ÁßçÊñπÊ≥ï‰ªªÈÄâÂÖ∂‰∏ÄÔºö&lt;/p&gt; &#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;1. Âú®ÂõæÂΩ¢ÁïåÈù¢‰∏≠Â°´ÂÜô‰Ω†ÁöÑAPIÂØÜÈí•&lt;/summary&gt; &#xA;   &lt;p&gt;ËøôÊ†∑ËÆæÁΩÆÁöÑÂØÜÈí•‰ºöÂú®È°µÈù¢Âà∑Êñ∞ÂêéË¢´Ê∏ÖÈô§„ÄÇ&lt;/p&gt; &#xA;   &lt;p&gt;&lt;img width=&#34;760&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/51039745/222873756-3858bb82-30b9-49bc-9019-36e378ee624d.png&#34;&gt;&lt;/p&gt;&#xA;  &lt;/details&gt;&lt;p&gt;&lt;/p&gt; &#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;2. Âú®Áõ¥Êé•‰ª£Á†Å‰∏≠Â°´ÂÖ•‰Ω†ÁöÑ OpenAI API ÂØÜÈí•&lt;/summary&gt; &#xA;   &lt;p&gt;ËøôÊ†∑ËÆæÁΩÆÁöÑÂØÜÈí•‰ºöÊàê‰∏∫ÈªòËÆ§ÂØÜÈí•„ÄÇÂú®ËøôÈáåËøòÂèØ‰ª•ÈÄâÊã©ÊòØÂê¶Âú®UI‰∏≠ÈöêËóèÂØÜÈí•ËæìÂÖ•Ê°Ü„ÄÇ&lt;/p&gt; &#xA;   &lt;p&gt;&lt;img width=&#34;525&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/51039745/223440375-d472de4b-aa7f-4eae-9170-6dc2ed9f5480.png&#34;&gt;&lt;/p&gt;&#xA;  &lt;/details&gt;&lt;p&gt;&lt;/p&gt; &#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;3. Âú®Êñá‰ª∂‰∏≠ËÆæÂÆöÈªòËÆ§ÂØÜÈí•„ÄÅÁî®Êà∑ÂêçÂØÜÁ†Å&lt;/summary&gt; &#xA;   &lt;p&gt;ËøôÊ†∑ËÆæÁΩÆÁöÑÂØÜÈí•ÂèØ‰ª•Âú®ÊãâÂèñÈ°πÁõÆÊõ¥Êñ∞‰πãÂêé‰øùÁïô„ÄÇ&lt;/p&gt; &#xA;   &lt;p&gt;Âú®È°πÁõÆÊñá‰ª∂Â§π‰∏≠Êñ∞Âª∫Ëøô‰∏§‰∏™Êñá‰ª∂Ôºö&lt;code&gt;api_key.txt&lt;/code&gt; Âíå &lt;code&gt;auth.json&lt;/code&gt;„ÄÇ&lt;/p&gt; &#xA;   &lt;p&gt;Âú®&lt;code&gt;api_key.txt&lt;/code&gt;‰∏≠Â°´ÂÜô‰Ω†ÁöÑAPI-KeyÔºåÊ≥®ÊÑè‰∏çË¶ÅÂ°´ÂÜô‰ªª‰ΩïÊó†ÂÖ≥ÂÜÖÂÆπ„ÄÇ&lt;/p&gt; &#xA;   &lt;p&gt;Âú®&lt;code&gt;auth.json&lt;/code&gt;‰∏≠Â°´ÂÜô‰Ω†ÁöÑÁî®Êà∑ÂêçÂíåÂØÜÁ†Å„ÄÇ&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code&gt;{&#xA;&#34;username&#34;: &#34;Áî®Êà∑Âêç&#34;,&#xA;&#34;password&#34;: &#34;ÂØÜÁ†Å&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂÆâË£Ö‰æùËµñ&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Âú®ÁªàÁ´Ø‰∏≠ËæìÂÖ•‰∏ãÈù¢ÁöÑÂëΩ‰ª§ÔºåÁÑ∂ÂêéÂõûËΩ¶„ÄÇ&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Â¶ÇÊûúÊä•ÈîôÔºåËØïËØï&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Â¶ÇÊûúËøòÊòØ‰∏çË°åÔºåËØ∑ÂÖà&lt;a href=&#34;https://www.runoob.com/python/python-install.html&#34;&gt;ÂÆâË£ÖPython&lt;/a&gt;„ÄÇ&lt;/p&gt; &lt;p&gt;Â¶ÇÊûú‰∏ãËΩΩÊÖ¢ÔºåÂª∫ËÆÆ&lt;a href=&#34;https://mirrors.tuna.tsinghua.edu.cn/help/pypi/&#34;&gt;ÈÖçÁΩÆÊ∏ÖÂçéÊ∫ê&lt;/a&gt;ÔºåÊàñËÄÖÁßëÂ≠¶‰∏äÁΩë„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂêØÂä®&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ËØ∑‰ΩøÁî®‰∏ãÈù¢ÁöÑÂëΩ‰ª§„ÄÇ&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python ChuanhuChatbot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Â¶ÇÊûúÊä•ÈîôÔºåËØïËØï&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 ChuanhuChatbot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Â¶ÇÊûúËøòÊòØ‰∏çË°åÔºåËØ∑ÂÖà&lt;a href=&#34;https://www.runoob.com/python/python-install.html&#34;&gt;ÂÆâË£ÖPython&lt;/a&gt;„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Â¶ÇÊûú‰∏ÄÂàáÈ°∫Âà©ÔºåÁé∞Âú®Ôºå‰Ω†Â∫îËØ•Â∑≤ÁªèÂèØ‰ª•Âú®ÊµèËßàÂô®Âú∞ÂùÄÊ†è‰∏≠ËæìÂÖ• &lt;a href=&#34;http://localhost:7860&#34;&gt;&lt;code&gt;http://localhost:7860&lt;/code&gt;&lt;/a&gt; Êü•ÁúãÂπ∂‰ΩøÁî® ChuanhuChatGPT ‰∫Ü„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Â¶ÇÊûú‰Ω†Âú®ÂÆâË£ÖËøáÁ®ã‰∏≠Á¢∞Âà∞‰∫ÜÈóÆÈ¢òÔºåËØ∑ÂÖàÊü•Áúã&lt;a href=&#34;https://raw.githubusercontent.com/GaiZhenbiao/ChuanhuChatGPT/main/#%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87%E8%A7%A3%E5%86%B3&#34;&gt;ÁñëÈöæÊùÇÁóáËß£ÂÜ≥&lt;/a&gt;ÈÉ®ÂàÜ„ÄÇ&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Ëá™Âä®Êõ¥Êñ∞&lt;/h3&gt; &#xA;&lt;p&gt;‰Ω†ÂèØ‰ª•ÈÄöËøáÊú¨È°πÁõÆÊèê‰æõÁöÑËÑöÊú¨Ê£ÄÊµã‰ªìÂ∫ìÊòØÂê¶ÊúâÊõ¥Êñ∞ÔºåÂ¶ÇÊûúÊúâÔºåÂàôÊãâÂèñÊúÄÊñ∞ËÑöÊú¨„ÄÅÂÆâË£Ö‰æùËµñ„ÄÅÈáçÂêØÊúçÂä°Âô®„ÄÇÊ≠§ÂäüËÉΩÊîØÊåÅ&lt;code&gt;Linux&lt;/code&gt;Âíå&lt;code&gt;macOS&lt;/code&gt;Á≥ªÁªü„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Â¶ÇÊûú‰Ω†ÊÉ≥ËøêË°åÔºåÂè™ÈúÄË¶ÅËøêË°å&lt;code&gt;run_Linux.sh&lt;/code&gt;ÊàñËÄÖ&lt;code&gt;run_macOS.command&lt;/code&gt;„ÄÇÂ¶ÇÊûú‰Ω†ËøòÊÉ≥‰øùÊåÅÊúÄÊñ∞ÁâàÊú¨ÔºåÂè™ÈúÄË¶ÅÂÆöÊó∂ËøêË°åËÑöÊú¨„ÄÇ‰æãÂ¶ÇÔºåÂú®crontab‰∏≠Âä†ÂÖ•‰∏ãÈù¢ÁöÑÂÜÖÂÆπÔºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;*/20 * * * * /path/to/ChuanhuChatGPT/run_Linux.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Â∞±ÂèØ‰ª•ÊØè20ÂàÜÈíüÊ£ÄÊü•‰∏ÄÊ¨°ËÑöÊú¨Êõ¥Êñ∞ÔºåÂ¶ÇÊûúÊúâÊõ¥Êñ∞ÔºåÂàôËá™Âä®ÊãâÂèñÂπ∂ÈáçÂêØÊúçÂä°Âô®„ÄÇ&lt;/p&gt; &#xA;&lt;h3&gt;‰ΩøÁî®DockerËøêË°å&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Â¶ÇÊûúËßâÂæó‰ª•‰∏äÊñπÊ≥ïÊØîËæÉÈ∫ªÁÉ¶ÔºåÊàë‰ª¨Êèê‰æõ‰∫ÜDockerÈïúÂÉè&lt;/summary&gt; &#xA; &lt;h4&gt;ÊãâÂèñÈïúÂÉè&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker pull tuchuanhuhuhu/chuanhuchatgpt:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;ËøêË°å&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -d --name chatgpt \&#xA;&#x9;-e my_api_key=&#34;ÊõøÊç¢ÊàêAPI&#34; \&#xA;&#x9;-e USERNAME=&#34;ÊõøÊç¢ÊàêÁî®Êà∑Âêç&#34; \&#xA;&#x9;-e PASSWORD=&#34;ÊõøÊç¢ÊàêÂØÜÁ†Å&#34; \&#xA;&#x9;-v ~/chatGPThistory:/app/history \&#xA;&#x9;-p 7860:7860 \&#xA;&#x9;tuchuanhuhuhu/chuanhuchatgpt:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Ê≥®Ôºö&lt;code&gt;USERNAME&lt;/code&gt; Âíå &lt;code&gt;PASSWORD&lt;/code&gt; ‰∏§Ë°åÂèØÁúÅÁï•„ÄÇËã•ÁúÅÁï•Âàô‰∏ç‰ºöÂêØÁî®ËÆ§ËØÅ„ÄÇ&lt;/p&gt; &#xA; &lt;h4&gt;Êü•ÁúãËøêË°åÁä∂ÊÄÅ&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker logs chatgpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;‰πüÂèØ‰øÆÊîπËÑöÊú¨ÂêéÊâãÂä®ÊûÑÂª∫ÈïúÂÉè&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker build -t chuanhuchatgpt:latest .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;ËøúÁ®ãÈÉ®ÁΩ≤&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Â¶ÇÊûúÈúÄË¶ÅÂú®ÂÖ¨ÁΩëÊúçÂä°Âô®ÈÉ®ÁΩ≤Êú¨È°πÁõÆÔºåËØ∑ÈòÖËØªËØ•ÈÉ®ÂàÜ&lt;/summary&gt; &#xA; &lt;h4&gt;ÈÉ®ÁΩ≤Âà∞ÂÖ¨ÁΩëÊúçÂä°Âô®&lt;/h4&gt; &#xA; &lt;p&gt;Â∞ÜÊúÄÂêé‰∏ÄÂè•‰øÆÊîπ‰∏∫&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;demo.queue().launch(server_name=&#34;0.0.0.0&#34;, server_port=7860, share=False) # ÂèØËá™ÂÆö‰πâÁ´ØÂè£&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Áî®Ë¥¶Âè∑ÂØÜÁ†Å‰øùÊä§È°µÈù¢&lt;/h4&gt; &#xA; &lt;p&gt;Â∞ÜÊúÄÂêé‰∏ÄÂè•‰øÆÊîπ‰∏∫&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;demo.queue().launch(server_name=&#34;0.0.0.0&#34;, server_port=7860,auth=(&#34;Âú®ËøôÈáåÂ°´ÂÜôÁî®Êà∑Âêç&#34;, &#34;Âú®ËøôÈáåÂ°´ÂÜôÂØÜÁ†Å&#34;)) # ÂèØËÆæÁΩÆÁî®Êà∑Âêç‰∏éÂØÜÁ†Å&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;ÈÖçÁΩÆ Nginx ÂèçÂêë‰ª£ÁêÜ&lt;/h4&gt; &#xA; &lt;p&gt;Ê≥®ÊÑèÔºöÈÖçÁΩÆÂèçÂêë‰ª£ÁêÜ‰∏çÊòØÂøÖÈ°ªÁöÑ„ÄÇÂ¶ÇÊûúÈúÄË¶Å‰ΩøÁî®ÂüüÂêçÔºåÂàôÈúÄË¶ÅÈÖçÁΩÆ Nginx ÂèçÂêë‰ª£ÁêÜ„ÄÇ&lt;/p&gt; &#xA; &lt;p&gt;ÂèàÂèäÔºöÁõÆÂâçÈÖçÁΩÆËÆ§ËØÅÂêéÔºåNginx ÂøÖÈ°ªÈÖçÁΩÆ SSLÔºåÂê¶Âàô‰ºöÂá∫Áé∞ &lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/issues/89&#34;&gt;Cookie ‰∏çÂåπÈÖçÈóÆÈ¢ò&lt;/a&gt;„ÄÇ&lt;/p&gt; &#xA; &lt;p&gt;Ê∑ªÂä†Áã¨Á´ãÈÖçÁΩÆÊñá‰ª∂Ôºö&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;server {&#xA;&#x9;listen 80;&#xA;&#x9;server_name /ÂüüÂêç/;   # ËØ∑Â°´ÂÖ•‰Ω†ËÆæÂÆöÁöÑÂüüÂêç&#xA;&#x9;access_log off;&#xA;&#x9;error_log off;&#xA;&#x9;location / {&#xA;&#x9;&#x9;proxy_pass http://127.0.0.1:7860;   # Ê≥®ÊÑèÁ´ØÂè£Âè∑&#xA;&#x9;&#x9;proxy_redirect off;&#xA;&#x9;&#x9;proxy_set_header Host $host;&#xA;&#x9;&#x9;proxy_set_header X-Real-IP $remote_addr;&#xA;&#x9;&#x9;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;&#xA;&#x9;&#x9;proxy_set_header Upgrade $http_upgrade;&#x9;&#x9;# WebsocketÈÖçÁΩÆ&#xA;&#x9;&#x9;proxy_set_header Connection $connection_upgrade;&#x9;&#x9;#WebsocketÈÖçÁΩÆ&#xA;&#x9;&#x9;proxy_max_temp_file_size 0;&#xA;&#x9;&#x9;client_max_body_size 10m;&#xA;&#x9;&#x9;client_body_buffer_size 128k;&#xA;&#x9;&#x9;proxy_connect_timeout 90;&#xA;&#x9;&#x9;proxy_send_timeout 90;&#xA;&#x9;&#x9;proxy_read_timeout 90;&#xA;&#x9;&#x9;proxy_buffer_size 4k;&#xA;&#x9;&#x9;proxy_buffers 4 32k;&#xA;&#x9;&#x9;proxy_busy_buffers_size 64k;&#xA;&#x9;&#x9;proxy_temp_file_write_size 64k;&#xA;&#x9;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;‰øÆÊîπ&lt;code&gt;nginx.conf&lt;/code&gt;ÈÖçÁΩÆÊñá‰ª∂ÔºàÈÄöÂ∏∏Âú®&lt;code&gt;/etc/nginx/nginx.conf&lt;/code&gt;ÔºâÔºåÂêëhttpÈÉ®ÂàÜÊ∑ªÂä†Â¶Ç‰∏ãÈÖçÁΩÆÔºö ÔºàËøô‰∏ÄÊ≠•ÊòØ‰∏∫‰∫ÜÈÖçÁΩÆwebsocketËøûÊé•ÔºåÂ¶Ç‰πãÂâçÈÖçÁΩÆËøáÂèØÂøΩÁï•Ôºâ&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;map $http_upgrade $connection_upgrade {&#xA;  default upgrade;&#xA;  &#39;&#39;      close;&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;‰∏∫‰∫ÜÂêåÊó∂ÈÖçÁΩÆÂüüÂêçËÆøÈóÆÂíåË∫´‰ªΩËÆ§ËØÅÔºåÈúÄË¶ÅÈÖçÁΩÆSSLÁöÑËØÅ‰π¶ÔºåÂèØ‰ª•ÂèÇËÄÉ&lt;a href=&#34;https://www.gzblog.tech/2020/12/25/how-to-config-hexo/#%E9%85%8D%E7%BD%AEHTTPS&#34;&gt;ËøôÁØáÂçöÂÆ¢&lt;/a&gt;‰∏ÄÈîÆÈÖçÁΩÆ&lt;/p&gt; &#xA; &lt;h4&gt;ÂÖ®Á®ã‰ΩøÁî®Docker ‰∏∫ChuanhuChatGPT ÂºÄÂêØHTTPS&lt;/h4&gt; &#xA; &lt;p&gt;Â¶ÇÊûú‰Ω†ÁöÑVPS 80Á´ØÂè£‰∏é443Á´ØÂè£Ê≤°ÊúâË¢´Âç†Áî®ÔºåÂàôÂèØ‰ª•ËÄÉËôëÂ¶Ç‰∏ãÁöÑÊñπÊ≥ïÔºåÂè™ÈúÄË¶ÅÂ∞Ü‰Ω†ÁöÑÂüüÂêçÊèêÂâçÁªëÂÆöÂà∞‰Ω†ÁöÑVPS ÁöÑIPÂç≥ÂèØ„ÄÇÊ≠§ÊñπÊ≥ïÁî±&lt;a href=&#34;https://github.com/iskoldt-X&#34;&gt;@iskoldt-X&lt;/a&gt; Êèê‰æõ„ÄÇ&lt;/p&gt; &#xA; &lt;p&gt;È¶ñÂÖàÔºåËøêË°å&lt;a href=&#34;https://github.com/nginx-proxy/nginx-proxy&#34;&gt;nginx-proxy&lt;/a&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;docker run --detach \&#xA;    --name nginx-proxy \&#xA;    --publish 80:80 \&#xA;    --publish 443:443 \&#xA;    --volume certs:/etc/nginx/certs \&#xA;    --volume vhost:/etc/nginx/vhost.d \&#xA;    --volume html:/usr/share/nginx/html \&#xA;    --volume /var/run/docker.sock:/tmp/docker.sock:ro \&#xA;    nginxproxy/nginx-proxy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Êé•ÁùÄÔºåËøêË°å&lt;a href=&#34;https://github.com/nginx-proxy/acme-companion&#34;&gt;acme-companion&lt;/a&gt;ÔºåËøôÊòØÁî®Êù•Ëá™Âä®Áî≥ËØ∑TLS ËØÅ‰π¶ÁöÑÂÆπÂô®&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;docker run --detach \&#xA;    --name nginx-proxy-acme \&#xA;    --volumes-from nginx-proxy \&#xA;    --volume /var/run/docker.sock:/var/run/docker.sock:ro \&#xA;    --volume acme:/etc/acme.sh \&#xA;    --env &#34;DEFAULT_EMAIL=‰Ω†ÁöÑÈÇÆÁÆ±ÔºàÁî®‰∫éÁî≥ËØ∑TLS ËØÅ‰π¶Ôºâ&#34; \&#xA;    nginxproxy/acme-companion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;ÊúÄÂêéÔºåÂèØ‰ª•ËøêË°åChuanhuChatGPT&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;docker run -d --name chatgpt \&#xA;&#x9;-e my_api_key=&#34;‰Ω†ÁöÑAPI&#34; \&#xA;&#x9;-e USERNAME=&#34;ÊõøÊç¢ÊàêÁî®Êà∑Âêç&#34; \&#xA;&#x9;-e PASSWORD=&#34;ÊõøÊç¢ÊàêÂØÜÁ†Å&#34; \&#xA;&#x9;-v ~/chatGPThistory:/app/history \&#xA;&#x9;-e VIRTUAL_HOST=‰Ω†ÁöÑÂüüÂêç \&#xA;&#x9;-e VIRTUAL_PORT=7860 \&#xA;&#x9;-e LETSENCRYPT_HOST=‰Ω†ÁöÑÂüüÂêç \&#xA;&#x9;tuchuanhuhuhu/chuanhuchatgpt:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Â¶ÇÊ≠§Âç≥ÂèØ‰∏∫ChuanhuChatGPTÂÆûÁé∞Ëá™Âä®Áî≥ËØ∑TLSËØÅ‰π¶Âπ∂‰∏îÂºÄÂêØHTTPS&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ÁñëÈöæÊùÇÁóáËß£ÂÜ≥&lt;/h2&gt; &#xA;&lt;p&gt;Âú®ÈÅáÂà∞ÂêÑÁßçÈóÆÈ¢òÊü•ÈòÖÁõ∏ÂÖ≥‰ø°ÊÅØÂâçÔºåÊÇ®ÂèØ‰ª•ÂÖàÂ∞ùËØïÊâãÂä®ÊãâÂèñÊú¨È°πÁõÆÁöÑÊúÄÊñ∞Êõ¥ÊîπÂπ∂Êõ¥Êñ∞ gradioÔºåÁÑ∂ÂêéÈáçËØï„ÄÇÊ≠•È™§‰∏∫Ôºö&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ÁÇπÂáªÁΩëÈ°µ‰∏äÁöÑ &lt;code&gt;Download ZIP&lt;/code&gt; ‰∏ãËΩΩÊúÄÊñ∞‰ª£Á†ÅÔºåÊàñ &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git pull https://github.com/GaiZhenbiao/ChuanhuChatGPT.git main -f&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Â∞ùËØïÂÜçÊ¨°ÂÆâË£Ö‰æùËµñÔºàÂèØËÉΩÊú¨È°πÁõÆÂºïÂÖ•‰∫ÜÊñ∞ÁöÑ‰æùËµñÔºâ &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Êõ¥Êñ∞gradio &lt;pre&gt;&lt;code&gt;pip install gradio --upgrade --force-reinstall&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;ÂæàÂ§öÊó∂ÂÄôÔºåËøôÊ†∑Â∞±ÂèØ‰ª•Ëß£ÂÜ≥ÈóÆÈ¢ò„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Â¶ÇÊûúÈóÆÈ¢ò‰ªçÁÑ∂Â≠òÂú®ÔºåËØ∑Êü•ÈòÖËØ•È°µÈù¢Ôºö&lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98&#34;&gt;Â∏∏ËßÅÈóÆÈ¢ò&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ËØ•È°µÈù¢ÂàóÂá∫‰∫Ü&lt;strong&gt;Âá†‰πéÊâÄÊúâ&lt;/strong&gt;ÊÇ®ÂèØËÉΩÈÅáÂà∞ÁöÑÂêÑÁßçÈóÆÈ¢òÔºåÂåÖÊã¨Â¶Ç‰ΩïÈÖçÁΩÆ‰ª£ÁêÜÔºå‰ª•ÂèäÈÅáÂà∞ÈóÆÈ¢òÂêéÊÇ®ËØ•ÈááÂèñÁöÑÊé™ÊñΩÔºå&lt;strong&gt;ËØ∑Âä°ÂøÖËÆ§ÁúüÈòÖËØª&lt;/strong&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;Starchart&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#GaiZhenbiao/ChuanhuChatGPT&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=GaiZhenbiao/ChuanhuChatGPT&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=GaiZhenbiao/ChuanhuChatGPT&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;ÊçêÊ¨æ&lt;/h2&gt; &#xA;&lt;p&gt;üêØÂ¶ÇÊûúËßâÂæóËøô‰∏™ËΩØ‰ª∂ÂØπ‰Ω†ÊúâÊâÄÂ∏ÆÂä©ÔºåÊ¨¢ËøéËØ∑‰ΩúËÄÖÂñùÂèØ‰πê„ÄÅÂñùÂíñÂï°ÔΩû&lt;/p&gt; &#xA;&lt;img width=&#34;250&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/51039745/226920291-e8ec0b0a-400f-4c20-ac13-dafac0c3aeeb.JPG&#34;&gt;</summary>
  </entry>
</feed>