<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-03T01:44:32Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Pythagora-io/gpt-pilot</title>
    <updated>2023-09-03T01:44:32Z</updated>
    <id>tag:github.com,2023-09-03:/Pythagora-io/gpt-pilot</id>
    <link href="https://github.com/Pythagora-io/gpt-pilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PoC for a scalable dev tool that writes entire apps from scratch while the developer oversees the implementation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üßë‚Äç‚úàÔ∏è GPT PILOT&lt;/h1&gt; &#xA;&lt;h3&gt;GPT Pilot codes the entire app as you oversee the code being written&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This is a research project to see how can GPT-4 be utilized to generate fully working, production-ready, apps. &lt;strong&gt;The main idea is that AI can write most of the code for an app (maybe 95%) but for the rest 5%, a developer is and will be needed until we get full AGI&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;I&#39;ve broken down the idea behind GPT Pilot and how it works in the following blog posts:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://blog.pythagora.ai/2023/08/23/430/&#34;&gt;[Part 1/3] High-level concepts + GPT Pilot workflow until the coding part&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Part 2/3] GPT Pilot coding workflow (COMING UP)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Part 3/3] Other important concepts and future plans (COMING UP)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/#-examples&#34;&gt;üëâ Examples of apps written by GPT Pilot üëà&lt;/a&gt;&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Pythagora-io/gpt-pilot/assets/10895136/0495631b-511e-451b-93d5-8a42acf22d3d&#34;&gt;https://github.com/Pythagora-io/gpt-pilot/assets/10895136/0495631b-511e-451b-93d5-8a42acf22d3d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Main pillars of GPT Pilot:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;For AI to create a fully working app, &lt;strong&gt;a developer needs to be involved&lt;/strong&gt; in the process of app creation. They need to be able to change the code at any moment and GPT Pilot needs to continue working with those changes (eg. add an API key or fix an issue if an AI gets stuck) &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The app needs to be written step by step as a developer would write it&lt;/strong&gt; - Let&#39;s say you want to create a simple app and you know everything you need to code and have the entire architecture in your head. Even then, you won&#39;t code it out entirely, then run it for the first time and debug all the issues at once. Rather, you will implement something simple, like add routes, run it, see how it works, and then move on to the next task. This way, you can debug issues as they arise. The same should be in the case when AI codes. It will make mistakes for sure so in order for it to have an easier time debugging issues and for the developer to understand what is happening, the AI shouldn&#39;t just spit out the entire codebase at once. Rather, the app should be developed step by step just like a developer would code it - eg. setup routes, add database connection, etc. &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The approach needs to be scalable&lt;/strong&gt; so that AI can create a production ready app &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Context rewinding&lt;/strong&gt; - for solving each development task, the context size of the first message to the LLM has to be relatively the same. For example, the context size of the first LLM message while implementing development task #5 has to be more or less the same as the first message while developing task #50. Because of this, the conversation needs to be rewound to the first message upon each task. &lt;a href=&#34;https://blogpythagora.files.wordpress.com/2023/08/pythagora-product-development-frame-3-1.jpg?w=1714&#34;&gt;See the diagram here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Recursive conversations&lt;/strong&gt; are LLM conversations that are set up in a way that they can be used ‚Äúrecursively‚Äù. For example, if GPT Pilot detects an error, it needs to debug it but let‚Äôs say that, during the debugging process, another error happens. Then, GPT Pilot needs to stop debugging the first issue, fix the second one, and then get back to fixing the first issue. This is a very important concept that, I believe, needs to work to make AI build large and scalable apps by itself. It works by rewinding the context and explaining each error in the recursion separately. Once the deepest level error is fixed, we move up in the recursion and continue fixing that error. We do this until the entire recursion is completed.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;TDD (Test Driven Development)&lt;/strong&gt; - for GPT Pilot to be able to scale the codebase, it will need to be able to create new code without breaking previously written code. There is no better way to do this than working with TDD methodology. For each code that GPT Pilot writes, it needs to write tests that check if the code works as intended so that whenever new changes are made, all previous tests can be run.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The idea is that AI won&#39;t be able to (at least in the near future) create apps from scratch without the developer being involved. That&#39;s why we created an interactive tool that generates code but also requires the developer to check each step so that they can understand what&#39;s going on and so that the AI can have a better overview of the entire codebase.&lt;/p&gt; &#xA;&lt;p&gt;Obviously, it still can&#39;t create any production-ready app but the general concept of how this could work is there.&lt;/p&gt; &#xA;&lt;h1&gt;üîå Requirements&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PostgreSQL&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;DB is needed for multiple reasons like continuing app development if you had to stop at any point or app crashed, going back to specific step so you can change some later steps in development, easier debugging, for future we will add functionality to update project (change some things in existing project or add new features to the project and so on)...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üö¶How to start using gpt-pilot?&lt;/h1&gt; &#xA;&lt;p&gt;After you have Python and PostgreSQL installed, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/Pythagora-io/gpt-pilot.git&lt;/code&gt; (clone the repo)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd gpt-pilot&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python -m venv pilot-env&lt;/code&gt; (create a virtual environment)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;source pilot-env/bin/activate&lt;/code&gt; (activate the virtual environment)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt; (install the dependencies)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd pilot&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mv .env.example .env&lt;/code&gt; (create the .env file)&lt;/li&gt; &#xA; &lt;li&gt;Add your OpenAI API key and the PostgreSQL database info to the &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python db_init.py&lt;/code&gt; (initialize the database)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python main.py&lt;/code&gt; (start GPT Pilot)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;After, this, you can just follow the instructions in the terminal.&lt;/p&gt; &#xA;&lt;p&gt;All generated code will be stored in the folder &lt;code&gt;workspace&lt;/code&gt; inside the folder named after the app name you enter upon starting the pilot.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT: To run GPT Pilot, you need to have PostgreSQL set up on your machine&lt;/strong&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üßë‚ÄçüíªÔ∏è Other arguments&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;continue working on an existing app&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py app_id=&amp;lt;ID_OF_THE_APP&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;continue working on an existing app from a specific step&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py app_id=&amp;lt;ID_OF_THE_APP&amp;gt; step=&amp;lt;STEP_FROM_CONST_COMMON&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;continue working on an existing app from a specific development step&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py app_id=&amp;lt;ID_OF_THE_APP&amp;gt; skip_until_dev_step=&amp;lt;DEV_STEP&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is basically the same as &lt;code&gt;step&lt;/code&gt; but during the actual development process. If you want to play around with gpt-pilot, this is likely the flag you will often use &lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üîé Examples&lt;/h1&gt; &#xA;&lt;p&gt;Here are a couple of example apps GPT Pilot created by itself:&lt;/p&gt; &#xA;&lt;h3&gt;Real-time chat app&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üí¨ Prompt: &lt;code&gt;A simple chat app with real time communication&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://youtu.be/bUj9DbMRYhA&#34;&gt;Video of the app creation process&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üíªÔ∏è &lt;a href=&#34;https://github.com/Pythagora-io/gpt-pilot-chat-app-demo&#34;&gt;GitHub repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://github.com/Pythagora-io/gpt-pilot/assets/10895136/85bc705c-be88-4ca1-9a3b-033700b97a22&#34; alt=&#34;gpt-pilot demo chat app&#34; width=&#34;500px&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Markdown editor&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üí¨ Prompt: &lt;code&gt;Build a simple markdown editor using HTML, CSS, and JavaScript. Allow users to input markdown text and display the formatted output in real-time.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://youtu.be/uZeA1iX9dgg&#34;&gt;Video of the app creation process&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üíªÔ∏è &lt;a href=&#34;https://github.com/Pythagora-io/gpt-pilot-demo-markdown-editor.git&#34;&gt;GitHub repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://github.com/Pythagora-io/gpt-pilot/assets/10895136/dbe1ccc3-b126-4df0-bddb-a524d6a386a8&#34; alt=&#34;gpt-pilot demo markdown editor&#34; width=&#34;500px&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Timer app&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üí¨ Prompt: &lt;code&gt;Create a simple timer app using HTML, CSS, and JavaScript that allows users to set a countdown timer and receive an alert when the time is up.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://youtu.be/CMN3W18zfiE&#34;&gt;Video of the app creation process&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üíªÔ∏è &lt;a href=&#34;https://github.com/Pythagora-io/gpt-pilot-timer-app-demo&#34;&gt;GitHub repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://github.com/Pythagora-io/gpt-pilot/assets/10895136/93bed40b-b769-4c8b-b16d-b80fb6fc73e0&#34; alt=&#34;gpt-pilot demo markdown editor&#34; width=&#34;500px&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;üèó How GPT Pilot works?&lt;/h1&gt; &#xA;&lt;p&gt;Here are the steps GPT Pilot takes to create an app:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Pythagora-io/gpt-pilot/assets/10895136/d89ba1d4-1208-4b7f-b3d4-76e3ccea584e&#34; alt=&#34;GPT Pilot workflow&#34;&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You enter the app name and the description&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Product Owner agent&lt;/strong&gt; asks a couple of questions to understand the requirements better&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Product Owner agent&lt;/strong&gt; writes user stories and asks you if they are all correct (this helps it create code later on)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Architect agent&lt;/strong&gt; writes up technologies that will be used for the app&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DevOps agent&lt;/strong&gt; checks if all technologies are installed on the machine and installs them if they are not&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tech Lead agent&lt;/strong&gt; writes up development tasks that Developer will need to implement. This is an important part because, for each step, Tech Lead needs to specify how the user (real world developer) can review if the task is done (eg. open localhost:3000 and do something)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Developer agent&lt;/strong&gt; takes each task and writes up what needs to be done to implement it. The description is in human readable form.&lt;/li&gt; &#xA; &lt;li&gt;Finally, &lt;strong&gt;Code Monkey agent&lt;/strong&gt; takes the Developer&#39;s description and the currently implement file and implements the changes into it. We realized this works much better than giving it to Developer right away to implement changes.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Pythagora-io/gpt-pilot/assets/10895136/54a8ec24-a2ea-43a6-a494-03139d4e43f5&#34; alt=&#34;GPT Pilot Coding Workflow&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;üï¥How&#39;s GPT Pilot different from &lt;em&gt;Smol developer&lt;/em&gt; and &lt;em&gt;GPT engineer&lt;/em&gt;?&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Human developer is involved throughout the process&lt;/strong&gt; - I don&#39;t think that AI can (at least in the near future) create apps without a developer being involved. Also, I think it&#39;s hard for a developer to get into a big codebase and try debugging it. That&#39;s why my idea was for AI to develop the app step by step where each step is reviewed by the developer. If you want to change some code yourself, you can just change it and GPT Pilot will continue developing on top of those changes. &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continuous development loops&lt;/strong&gt; - The goal behind this project was to see how we can create recursive conversations with GPT so that it can debug any issue and implement any feature. For example, after the app is generated, you can always add more instructions about what you want to implement or debug. I wanted to see if this can be so flexible that, regardless of the app&#39;s size, it can just iterate and build bigger and bigger apps &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Auto debugging&lt;/strong&gt; - when it detects an error, it debugs it by itself. I still haven&#39;t implemented writing automated tests which should make this fully autonomous but for now, you can input the error that&#39;s happening (eg. within a UI) and GPT Pilot will debug it from there. The plan is to make it write automated tests in Cypress as well so that it can test it by itself and debug without the developer&#39;s explanation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üçª Contributing&lt;/h1&gt; &#xA;&lt;p&gt;If you are interested in contributing to GPT Pilot, I would be more than happy to have you on board but also help you get started. Feel free to ping &lt;a href=&#34;mailto:zvonimir@pythagora.ai&#34;&gt;zvonimir@pythagora.ai&lt;/a&gt; and I&#39;ll help you get started.&lt;/p&gt; &#xA;&lt;h2&gt;üî¨Ô∏è Research&lt;/h2&gt; &#xA;&lt;p&gt;Since this is a research project, there are many areas that need to be researched on both practical and theoretical levels. We&#39;re happy to hear how can the entire GPT Pilot concept be improved. For example, maybe it would work better if we structured functional requirements differently or maybe technical requirements need to be specified in a different way.&lt;/p&gt; &#xA;&lt;h2&gt;üñ• Development&lt;/h2&gt; &#xA;&lt;p&gt;Other than the research, GPT Pilot needs to be debugged to work in different scenarios. For example, we realized that the quality of the code generated is very sensitive to the size of the development task. When the task is too broad, the code has too many bugs that are hard to fix but when the development task is too narrow, GPT also seems to struggle in getting the task implemented into the existing code.&lt;/p&gt; &#xA;&lt;h1&gt;üîó Connect with us&lt;/h1&gt; &#xA;&lt;p&gt;üåü As an open source tool, it would mean the world to us if you starred the GPT-pilot repo üåü&lt;/p&gt; &#xA;&lt;p&gt;üí¨ Join &lt;a href=&#34;https://discord.gg/FWnRZdCb&#34;&gt;the Discord server&lt;/a&gt; to get in touch. &lt;br&gt;&lt;br&gt; &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openchatai/OpenCopilot</title>
    <updated>2023-09-03T01:44:32Z</updated>
    <id>tag:github.com,2023-09-03:/openchatai/OpenCopilot</id>
    <link href="https://github.com/openchatai/OpenCopilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü§ñ üî• AI Copilot for your own SaaS product. Open source AI sidekick for everyone.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://www.producthunt.com/posts/opencopilot?utm_source=badge-featured&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-opencopilot&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=410719&amp;amp;theme=light&#34; alt=&#34;OpenCopilot - Build AI copilots for your own SaaS product | Product Hunt&#34; style=&#34;width: 250px; height: 54px;&#34; width=&#34;250&#34; height=&#34;54&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt; &lt;img alt=&#34;GitHub Contributors&#34; src=&#34;https://img.shields.io/github/contributors/openchatai/opencopilot&#34;&gt; &lt;img alt=&#34;GitHub Last Commit&#34; src=&#34;https://img.shields.io/github/last-commit/openchatai/opencopilot&#34;&gt; &lt;img alt=&#34;&#34; src=&#34;https://img.shields.io/github/repo-size/openchatai/opencopilot&#34;&gt; &lt;img alt=&#34;GitHub Issues&#34; src=&#34;https://img.shields.io/github/issues/openchatai/opencopilot&#34;&gt; &lt;img alt=&#34;GitHub Pull Requests&#34; src=&#34;https://img.shields.io/github/issues-pr/openchatai/opencopilot&#34;&gt; &lt;img alt=&#34;Github License&#34; src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/1110910277110743103?label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;style=plastic&amp;amp;color=d7b023)%5D(https://discord.gg/Q8hHfdav&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/openchatai/OpenCopilot/assets/32633162/a0cdc888-d2de-46b7-8c0b-96e876050b6e&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Documentation &lt;a href=&#34;https://docs.opencopilot.so&#34;&gt;available here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üî• OpenCopilot [early beta]&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;OpenCopilot allows you to have your own product&#39;s AI copilot. It integrates with your underlying APIs and is able to execute API calls whenever needed. It uses LLMs to determine if the user&#39;s request requires calling an API endpoint. Then, it decides which endpoint to call and passes the appropriate payload based on the given API definition.&lt;/p&gt; &#xA;&lt;h2&gt;How does it work?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Provide your API/backend definition, including your public endpoints and how to call them. Currently, OpenCopilot supports Swagger OpenAPI 3.0. We&#39;re also working on a UI to allow you to dynamically add endpoints.&lt;/li&gt; &#xA; &lt;li&gt;OpenCopilot validates your schema to achieve the best results.&lt;/li&gt; &#xA; &lt;li&gt;We feed the API definition to an LLM.&lt;/li&gt; &#xA; &lt;li&gt;Finally, you can integrate our user-friendly chat bubble into your SaaS app.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Try it out:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;You can try it out on &lt;a href=&#34;http://opencopilot.so/&#34;&gt;opencopilot.so&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/openchatai/OpenCopilot/assets/32633162/3bf5c24d-572c-4a42-9e45-40f05e5a16b2&#34;&gt;https://github.com/openchatai/OpenCopilot/assets/32633162/3bf5c24d-572c-4a42-9e45-40f05e5a16b2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;AI Copilot: growing trend&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=HVvbY7A7lIQ&amp;amp;ab_channel=Shopify&#34;&gt;Shopify is developing &#34;Shopify Sidekick.&#34;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=FCfwc-NNo30&amp;amp;ab_channel=MicrosoftDeveloper&#34;&gt;Microsoft is working on &#34;Windows Copilot.&#34;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/features/copilot&#34;&gt;GitHub is in the process of creating &#34;GitHub Copilot.&#34;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/bing?form=MA13FV&#34;&gt;Microsoft is also developing &#34;Bing Copilot.&#34;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And our goal is to empower every SaaS product with the ability to have their own AI copilots tailored for their unique products.&lt;/p&gt; &#xA;&lt;h2&gt;üèÅ What OpenCopilot can and can&#39;t do now?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It is capable of calling your underlying APIs.&lt;/li&gt; &#xA; &lt;li&gt;It can transform the response into meaningful text.&lt;/li&gt; &#xA; &lt;li&gt;It can automatically populate certain request payload fields based on the context. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For instance, you can request actions like: &#34;Initiate a new case about X problem,&#34; and the title field will be automatically filled with the appropriate name.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Currently, it does not support calling multiple endpoints simultaneously (feature coming soon).&lt;/li&gt; &#xA; &lt;li&gt;It is not suitable for handling large APIs.&lt;/li&gt; &#xA; &lt;li&gt;It is not equipped to handle complex APIs.&lt;/li&gt; &#xA; &lt;li&gt;It can not remember the chat history (every message is agnostic from previous messages.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üõ£Ô∏è Roadmap:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Create unlimited copilots.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Embed the copilot on your SaaS product using standard JS calls.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; TypeScript chat bubble.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Provide Swagger definitions for your APIs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Swagger definition validator + recommender.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; [in progress] UI endpoints editor.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Chat memory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Vector DB support for large Swagger files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Plugins system to support different types of authentications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Offline LLMs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Ability to ingest text data, PDF files, websites, and extra data sources.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We love hearing from you! Got any cool ideas or requests? We&#39;re all ears! So, if you have something in mind, give us a shout!&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure you have docker installed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To begin, clone this Git repository:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:openchatai/OpenCopilot.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Update llm-server/Dockerfile with your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; key:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;ENV OPENAI_API_KEY YOUR_TOKEN_HERE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Navigate to the repository folder and run the following command (for MacOS or Linux):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the installation is complete, you can access the OpenCopilot console at: &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project follows the &lt;a href=&#34;https://github.com/all-contributors/all-contributors&#34;&gt;all-contributors&lt;/a&gt; specification. Contributions of any kind welcome!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/codellama</title>
    <updated>2023-09-03T01:44:32Z</updated>
    <id>tag:github.com,2023-09-03:/facebookresearch/codellama</id>
    <link href="https://github.com/facebookresearch/codellama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Inference code for CodeLlama models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Introducing Code Llama&lt;/h1&gt; &#xA;&lt;p&gt;Code Llama is a family of large language models for code based on &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama 2&lt;/a&gt; providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama was developed by fine-tuning Llama 2 using a higher sampling of code. As with Llama 2, we applied considerable safety mitigations to the fine-tuned versions of the model. For detailed information on model training, architecture and parameters, evaluations, responsible AI and safety refer to our &lt;a href=&#34;https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/&#34;&gt;research paper&lt;/a&gt;. Output generated by code generation features of the Llama Materials, including Code Llama, may be subject to third party licenses, including, without limitation, open source licenses.&lt;/p&gt; &#xA;&lt;p&gt;We are unlocking the power of large language models and our latest version of Code Llama is now accessible to individuals, creators, researchers and businesses of all sizes so that they can experiment, innovate and scale their ideas responsibly. This release includes model weights and starting code for pretrained and fine-tuned Llama language models ‚Äî ranging from 7B to 34B parameters.&lt;/p&gt; &#xA;&lt;p&gt;This repository is intended as a minimal example to load &lt;a href=&#34;https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/&#34;&gt;Code Llama&lt;/a&gt; models and run inference.&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;In order to download the model weights and tokenizers, please visit the &lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;Meta AI website&lt;/a&gt; and accept our License.&lt;/p&gt; &#xA;&lt;p&gt;Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, &lt;strong&gt;do not use the &#39;Copy link address&#39; option&lt;/strong&gt; when you right click the URL. If the copied URL text starts with: &lt;a href=&#34;https://download.llamameta.net&#34;&gt;https://download.llamameta.net&lt;/a&gt;, you copied it correctly. If the copied URL text starts with: &lt;a href=&#34;https://l.facebook.com&#34;&gt;https://l.facebook.com&lt;/a&gt;, you copied it the wrong way.&lt;/p&gt; &#xA;&lt;p&gt;Pre-requisites: make sure you have &lt;code&gt;wget&lt;/code&gt; and &lt;code&gt;md5sum&lt;/code&gt; installed. Then to run the script: &lt;code&gt;bash download.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Keep in mind that the links expire after 24 hours and a certain amount of downloads. If you start seeing errors such as &lt;code&gt;403: Forbidden&lt;/code&gt;, you can always re-request a link.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;In a conda env with PyTorch / CUDA available, clone the repo and run in the top-level directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Different models require different model-parallel (MP) values:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;MP&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;34B&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;All models support sequence lengths up to 100,000 tokens, but we pre-allocate the cache according to &lt;code&gt;max_seq_len&lt;/code&gt; and &lt;code&gt;max_batch_size&lt;/code&gt; values. So set those according to your hardware and use-case.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Code Models&lt;/h3&gt; &#xA;&lt;p&gt;The Code Llama and Code Llama - Python models are not fine-tuned to follow instructions. They should be prompted so that the expected answer is the natural continuation of the prompt.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;example_completion.py&lt;/code&gt; for some examples. To illustrate, see command below to run it with the &lt;code&gt;CodeLlama-7b&lt;/code&gt; model (&lt;code&gt;nproc_per_node&lt;/code&gt; needs to be set to the &lt;code&gt;MP&lt;/code&gt; value):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_completion.py \&#xA;    --ckpt_dir CodeLlama-7b/ \&#xA;    --tokenizer_path CodeLlama-7b/tokenizer.model \&#xA;    --max_seq_len 128 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pretrained code models are: the Code Llama models &lt;code&gt;CodeLlama-7b&lt;/code&gt;, &lt;code&gt;CodeLlama-13b&lt;/code&gt;, &lt;code&gt;CodeLlama-34b&lt;/code&gt; and the Code Llama - Python models &lt;code&gt;CodeLlama-7b-Python&lt;/code&gt;, &lt;code&gt;CodeLlama-13b-Python&lt;/code&gt;, &lt;code&gt;CodeLlama-34b-Python&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Code Infilling&lt;/h3&gt; &#xA;&lt;p&gt;Code Llama and Code Llama - Instruct 7B and 13B models are capable of filling in code given the surrounding context.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;example_infilling.py&lt;/code&gt; for some examples. The &lt;code&gt;CodeLlama-7b&lt;/code&gt; model can be run for infilling with the command below (&lt;code&gt;nproc_per_node&lt;/code&gt; needs to be set to the &lt;code&gt;MP&lt;/code&gt; value):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_infilling.py \&#xA;    --ckpt_dir CodeLlama-7b/ \&#xA;    --tokenizer_path CodeLlama-7b/tokenizer.model \&#xA;    --max_seq_len 192 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pretrained infilling models are: the Code Llama models &lt;code&gt;CodeLlama-7b&lt;/code&gt; and &lt;code&gt;CodeLlama-13b&lt;/code&gt; and the Code Llama - Instruct models &lt;code&gt;CodeLlama-7b-Instruct&lt;/code&gt;, &lt;code&gt;CodeLlama-13b-Instruct&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Fine-tuned Instruction Models&lt;/h3&gt; &#xA;&lt;p&gt;Code Llama - Instruct models are fine-tuned to follow instructions. To get the expected features and performance for them, a specific formatting defined in &lt;a href=&#34;https://github.com/facebookresearch/codellama/raw/main/llama/generation.py#L212&#34;&gt;&lt;code&gt;chat_completion&lt;/code&gt;&lt;/a&gt; needs to be followed, including the &lt;code&gt;INST&lt;/code&gt; and &lt;code&gt;&amp;lt;&amp;lt;SYS&amp;gt;&amp;gt;&lt;/code&gt; tags, &lt;code&gt;BOS&lt;/code&gt; and &lt;code&gt;EOS&lt;/code&gt; tokens, and the whitespaces and linebreaks in between (we recommend calling &lt;code&gt;strip()&lt;/code&gt; on inputs to avoid double-spaces).&lt;/p&gt; &#xA;&lt;p&gt;You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for &lt;a href=&#34;https://github.com/facebookresearch/llama-recipes/raw/main/inference/inference.py&#34;&gt;an example&lt;/a&gt; of how to add a safety checker to the inputs and outputs of your inference code.&lt;/p&gt; &#xA;&lt;p&gt;Examples using &lt;code&gt;CodeLlama-7b-Instruct&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_instructions.py \&#xA;    --ckpt_dir CodeLlama-7b-Instruct/ \&#xA;    --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \&#xA;    --max_seq_len 512 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Fine-tuned instruction-following models are: the Code Llama - Instruct models &lt;code&gt;CodeLlama-7b-Instruct&lt;/code&gt;, &lt;code&gt;CodeLlama-13b-Instruct&lt;/code&gt;, &lt;code&gt;CodeLlama-34b-Instruct&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Code Llama is a new technology that carries potential risks with use. Testing conducted to date has not ‚Äî and could not ‚Äî cover all scenarios. In order to help developers address these risks, we have created the &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/Responsible-Use-Guide.pdf&#34;&gt;Responsible Use Guide&lt;/a&gt;. More details can be found in our research papers as well.&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please report any software ‚Äúbug‚Äù, or other problems with the models through one of the following means:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reporting issues with the model: &lt;a href=&#34;http://github.com/facebookresearch/codellama&#34;&gt;github.com/facebookresearch/codellama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting risky content generated by the model: &lt;a href=&#34;http://developers.facebook.com/llama_output_feedback&#34;&gt;developers.facebook.com/llama_output_feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting bugs and security concerns: &lt;a href=&#34;http://facebook.com/whitehat/info&#34;&gt;facebook.com/whitehat/info&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Card&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/codellama/main/MODEL_CARD.md&#34;&gt;MODEL_CARD.md&lt;/a&gt; for the model card of Code Llama.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Our model and weights are licensed for both researchers and commercial entities, upholding the principles of openness. Our mission is to empower individuals, and industry through this opportunity, while fostering an environment of discovery and ethical AI advancements.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file, as well as our accompanying &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/USE_POLICY.md&#34;&gt;Acceptable Use Policy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/&#34;&gt;Code Llama Research Paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/blog/code-llama-large-language-model-coding/&#34;&gt;Code Llama Blog Post&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>