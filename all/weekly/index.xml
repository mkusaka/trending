<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-18T01:42:21Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>conductor-oss/conductor</title>
    <updated>2024-02-18T01:42:21Z</updated>
    <id>tag:github.com,2024-02-18:/conductor-oss/conductor</id>
    <link href="https://github.com/conductor-oss/conductor" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Conductor is a microservices orchestration engine.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/conductor-oss/conductor/main/docs/img/logo.svg?sanitize=true&#34; alt=&#34;Conductor&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/Netflix/conductor/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/Netflix/conductor.svg?sanitize=true&#34; alt=&#34;Github release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/conductor-oss/conductor.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Conductor is a platform &lt;em&gt;originally&lt;/em&gt; created at Netflix to orchestrate microservices and events. Conductor OSS is maintained by the team of developers at &lt;a href=&#34;https://orkes.io/&#34;&gt;Orkes&lt;/a&gt; along with the members of the open source community.&lt;/p&gt; &#xA;&lt;h2&gt;Conductor OSS&lt;/h2&gt; &#xA;&lt;p&gt;This is the new home for the Conductor open source going forward (previously hosted at Netflix/Conductor).&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;br&gt; Going forward, all the bug fixes, feature requests and security patches will be applied and released from this repository.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The last published version of Netflix Conductor will be &lt;strong&gt;3.15.0&lt;/strong&gt; which we will continue to support.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to participate in the roadmap and development, &lt;a href=&#34;https://forms.gle/P2i1xHrxPQLrjzTB7&#34;&gt;please reach out&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;‚≠ê This repository&lt;/h2&gt; &#xA;&lt;p&gt;Show support for the Conductor OSS. Please help spread the awareness by starring this repo.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/conductor-oss/conductor/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/conductor-oss/conductor.svg?style=social&amp;amp;label=Star&amp;amp;maxAge=&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Update your local forks/clones&lt;/h2&gt; &#xA;&lt;p&gt;Please update your forks to point to this repo. This will ensure your commits and PRs can be send against this repository&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git remote set-url origin https://github.com/conductor-oss/conductor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;br&gt; &lt;strong&gt;Follow the steps below if you have an active PR against the Netflix/Conductor repository&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Fork &lt;strong&gt;this&lt;/strong&gt; repository&lt;/li&gt; &#xA;  &lt;li&gt;Update your local repository to change the remote to this repository&lt;/li&gt; &#xA;  &lt;li&gt;Send a PR against the &lt;code&gt;main&lt;/code&gt; branch&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;The latest version is &lt;a href=&#34;https://GitHub.com/conductor-oss/conductor/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/conductor-oss/conductor.svg?sanitize=true&#34; alt=&#34;Github release&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The next scheduled release is in Feb 2024.&lt;/p&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://join.slack.com/t/orkes-conductor/shared_invite/zt-xyxqyseb-YZ3hwwAgHJH97bsrYRnSZg&#34;&gt;Slack Community&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;We have an active &lt;a href=&#34;https://join.slack.com/t/orkes-conductor/shared_invite/zt-xyxqyseb-YZ3hwwAgHJH97bsrYRnSZg&#34;&gt;community&lt;/a&gt; of Conductor users and contributors on the channel.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://docs.conductor-oss.org/&#34;&gt;Documentation Site&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.conductor-oss.org/&#34;&gt;Documentation&lt;/a&gt; and tutorial on how to use Conductor&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/conductor-oss/conductor/discussions&#34;&gt;Discussion Forum&lt;/a&gt;: Please use the forum for questions and discussing ideas and join the community.&lt;/p&gt; &#xA;&lt;h3&gt;Conductor SDKs&lt;/h3&gt; &#xA;&lt;p&gt;Conductor supports creating workflows using JSON and Code.&lt;br&gt; SDK support for creating workflows using code is available in multiple languages and can be found at &lt;a href=&#34;https://github.com/conductor-sdk&#34;&gt;https://github.com/conductor-sdk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started - Building &amp;amp; Running Conductor&lt;/h2&gt; &#xA;&lt;h3&gt;From Source:&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to build your own distribution, you can run &lt;code&gt;./gradlew build&lt;/code&gt; from this project that products the runtime artifacts. The runnable server is in server/ module.&lt;/p&gt; &#xA;&lt;h3&gt;Using Docker (Recommended)&lt;/h3&gt; &#xA;&lt;p&gt;Follow the steps below to launch the docker container:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&#xA;# Create volumes for persistent stores&#xA;# Used to create a persistent volume that will preserve the &#xA;docker volume create postgres&#xA;docker volume create redis&#xA;&#xA;docker run --init -p 8080:8080 -p 1234:5000 --mount source=redis,target=/redis \&#xA;--mount source=postgres,target=/pgdata conductoross/conductor-standalone:3.15.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Navigate to &lt;a href=&#34;http://localhost:1234&#34;&gt;http://localhost:1234&lt;/a&gt; once the container starts to launch UI.&lt;/p&gt; &#xA;&lt;h2&gt;Docker Containers for production usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker pull conductoross/conductor:3.15.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Database Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The default persistence used is Redis&lt;/li&gt; &#xA; &lt;li&gt;The indexing backend is &lt;a href=&#34;https://www.elastic.co/&#34;&gt;Elasticsearch&lt;/a&gt; (6.x)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Other Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;JDK 17+&lt;/li&gt; &#xA; &lt;li&gt;UI requires Node 14 to build. Earlier Node versions may work but are untested.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Get Support&lt;/h2&gt; &#xA;&lt;p&gt;There are several ways to get in touch with us:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/orkes-conductor/shared_invite/zt-xyxqyseb-YZ3hwwAgHJH97bsrYRnSZg&#34;&gt;Slack Community&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/conductor-oss/conductor/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=conductor-oss/conductor&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>lllyasviel/stable-diffusion-webui-forge</title>
    <updated>2024-02-18T01:42:21Z</updated>
    <id>tag:github.com,2024-02-18:/lllyasviel/stable-diffusion-webui-forge</id>
    <link href="https://github.com/lllyasviel/stable-diffusion-webui-forge" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stable Diffusion WebUI Forge&lt;/h1&gt; &#xA;&lt;p&gt;Stable Diffusion WebUI Forge is a platform on top of &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;Stable Diffusion WebUI&lt;/a&gt; (based on &lt;a href=&#34;https://www.gradio.app/&#34;&gt;Gradio&lt;/a&gt;) to make development easier, optimize resource management, and speed up inference.&lt;/p&gt; &#xA;&lt;p&gt;The name &#34;Forge&#34; is inspired from &#34;Minecraft Forge&#34;. This project is aimed at becoming SD WebUI&#39;s Forge.&lt;/p&gt; &#xA;&lt;p&gt;Compared to original WebUI (for SDXL inference at 1024px), you can expect the below speed-ups:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;If you use common GPU like 8GB vram, you can expect to get about &lt;strong&gt;30~45% speed up&lt;/strong&gt; in inference speed (it/s), the GPU memory peak (in task manager) will drop about 700MB to 1.3GB, the maximum diffusion resolution (that will not OOM) will increase about 2x to 3x, and the maximum diffusion batch size (that will not OOM) will increase about 4x to 6x.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you use less powerful GPU like 6GB vram, you can expect to get about &lt;strong&gt;60~75% speed up&lt;/strong&gt; in inference speed (it/s), the GPU memory peak (in task manager) will drop about 800MB to 1.5GB, the maximum diffusion resolution (that will not OOM) will increase about 3x, and the maximum diffusion batch size (that will not OOM) will increase about 4x.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you use powerful GPU like 4090 with 24GB vram, you can expect to get about &lt;strong&gt;3~6% speed up&lt;/strong&gt; in inference speed (it/s), the GPU memory peak (in task manager) will drop about 1GB to 1.4GB, the maximum diffusion resolution (that will not OOM) will increase about 1.6x, and the maximum diffusion batch size (that will not OOM) will increase about 2x.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you use ControlNet for SDXL, the maximum ControlNet count (that will not OOM) will increase about 2x, the speed with SDXL+ControlNet will &lt;strong&gt;speed up about 30~45%&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Another very important change that Forge brings is &lt;strong&gt;Unet Patcher&lt;/strong&gt;. Using Unet Patcher, methods like Self-Attention Guidance, Kohya High Res Fix, FreeU, StyleAlign, Hypertile can all be implemented in about 100 lines of codes.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to Unet Patcher, many new things are possible now and supported in Forge, including SVD, Z123, masked Ip-adapter, masked controlnet, photomaker, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;No need to monkeypatch UNet and conflict other extensions anymore!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Forge also adds a few samplers, including but not limited to DDPM, DDPM Karras, DPM++ 2M Turbo, DPM++ 2M SDE Turbo, LCM Karras, Euler A Turbo, etc. (LCM is already in original webui since 1.7.0).&lt;/p&gt; &#xA;&lt;p&gt;Finally, Forge promise that we will only do our jobs. Forge will never add unnecessary opinioned changes to the user interface. You are still using 100% Automatic1111 WebUI.&lt;/p&gt; &#xA;&lt;h1&gt;Installing Forge&lt;/h1&gt; &#xA;&lt;p&gt;If you are proficient in Git and you want to install Forge as another branch of SD-WebUI, please see &lt;a href=&#34;https://github.com/continue-revolution/sd-webui-animatediff/raw/forge/master/docs/how-to-use.md#you-have-a1111-and-you-know-git&#34;&gt;here&lt;/a&gt;. In this way, you can reuse all SD checkpoints and all extensions you installed previously in your OG SD-WebUI, but you should know what you are doing.&lt;/p&gt; &#xA;&lt;p&gt;If you know what you are doing, you can install Forge using same method as SD-WebUI. (Install Git, Python, Git Clone the forge repo &lt;code&gt;https://github.com/lllyasviel/stable-diffusion-webui-forge.git&lt;/code&gt; and then run webui-user.bat).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Or you can just use this one-click installation package (with git and python included).&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/releases/download/latest/webui_forge_cu121_torch21.7z&#34;&gt;&amp;gt;&amp;gt;&amp;gt; Click Here to Download One-Click Package&amp;lt;&amp;lt;&amp;lt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;After you download, you uncompress, use &lt;code&gt;update.bat&lt;/code&gt; to update, and use &lt;code&gt;run.bat&lt;/code&gt; to run.&lt;/p&gt; &#xA;&lt;p&gt;Note that running &lt;code&gt;update.bat&lt;/code&gt; is important, otherwise you may be using a previous version with potential bugs unfixed.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/c49bd60d-82bd-4086-9859-88d472582b94&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Screenshots of Comparison&lt;/h1&gt; &#xA;&lt;p&gt;I tested with several devices, and this is a typical result from 8GB VRAM (3070ti laptop) with SDXL.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This is original WebUI:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/16893937-9ed9-4f8e-b960-70cd5d1e288f&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/7bbc16fe-64ef-49e2-a595-d91bb658bd94&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/de1747fd-47bc-482d-a5c6-0728dd475943&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/96e5e171-2d74-41ba-9dcc-11bf68be7e16&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(average about 7.4GB/8GB, peak at about 7.9GB/8GB)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This is WebUI Forge:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/ca5e05ed-bd86-4ced-8662-f41034648e8c&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/3629ee36-4a99-4d9b-b371-12efb260a283&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/6d13ebb7-c30d-4aa8-9242-c0b5a1af8c95&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/c4f723c3-6ea7-4539-980b-0708ed2a69aa&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(average and peak are all 6.3GB/8GB)&lt;/p&gt; &#xA;&lt;p&gt;You can see that Forge does not change WebUI results. Installing Forge is not a seed breaking change.&lt;/p&gt; &#xA;&lt;p&gt;Forge can perfectly keep WebUI unchanged even for most complicated prompts like &lt;code&gt;fantasy landscape with a [mountain:lake:0.25] and [an oak:a christmas tree:0.75][ in foreground::0.6][ in background:0.25] [shoddy:masterful:0.5]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All your previous works still work in Forge!&lt;/p&gt; &#xA;&lt;h1&gt;Forge Backend&lt;/h1&gt; &#xA;&lt;p&gt;Forge backend removes all WebUI&#39;s codes related to resource management and reworked everything. All previous CMD flags like &lt;code&gt;medvram, lowvram, medvram-sdxl, precision full, no half, no half vae, attention_xxx, upcast unet&lt;/code&gt;, ... are all &lt;strong&gt;REMOVED&lt;/strong&gt;. Adding these flags will not cause error but they will not do anything now. &lt;strong&gt;We highly encourage Forge users to remove all cmd flags and let Forge to decide how to load models.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Without any cmd flag, Forge can run SDXL with 4GB vram and SD1.5 with 2GB vram.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The only one flag that you may still need&lt;/strong&gt; is &lt;code&gt;--always-offload-from-vram&lt;/code&gt; (This flag will make things &lt;strong&gt;slower&lt;/strong&gt;). This option will let Forge always unload models from VRAM. This can be useful if you use multiple software together and want Forge to use less VRAM and give some vram to other software, or when you are using some old extensions that will compete vram with Forge, or (very rarely) when you get OOM.&lt;/p&gt; &#xA;&lt;p&gt;If you really want to play with cmd flags, you can additionally control the GPU with:&lt;/p&gt; &#xA;&lt;p&gt;(extreme VRAM cases)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--always-gpu&#xA;--always-cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(rare attention cases)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--attention-split&#xA;--attention-quad&#xA;--attention-pytorch&#xA;--disable-xformers&#xA;--disable-attention-upcast&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(float point type)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--all-in-fp32&#xA;--all-in-fp16&#xA;--unet-in-bf16&#xA;--unet-in-fp16&#xA;--unet-in-fp8-e4m3fn&#xA;--unet-in-fp8-e5m2&#xA;--vae-in-fp16&#xA;--vae-in-fp32&#xA;--vae-in-bf16&#xA;--clip-in-fp8-e4m3fn&#xA;--clip-in-fp8-e5m2&#xA;--clip-in-fp16&#xA;--clip-in-fp32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(rare platforms)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--directml&#xA;--disable-ipex-hijack&#xA;--pytorch-deterministic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Again, Forge do not recommend users to use any cmd flags unless you are very sure that you really need these.&lt;/p&gt; &#xA;&lt;h1&gt;UNet Patcher&lt;/h1&gt; &#xA;&lt;p&gt;Note that &lt;a href=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/169&#34;&gt;Forge does not use any other software as backend&lt;/a&gt;. The full name of the backend is &lt;code&gt;Stable Diffusion WebUI with Forge backend&lt;/code&gt;, or for simplicity, the &lt;code&gt;Forge backend&lt;/code&gt;. The API and python symbols are made similar to previous software only for reducing the learning cost of developers.&lt;/p&gt; &#xA;&lt;p&gt;Now developing an extension is super simple. We finally have a patchable UNet.&lt;/p&gt; &#xA;&lt;p&gt;Below is using one single file with 80 lines of codes to support FreeU:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;extensions-builtin/sd_forge_freeu/scripts/forge_freeu.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import gradio as gr&#xA;from modules import scripts&#xA;&#xA;&#xA;def Fourier_filter(x, threshold, scale):&#xA;    x_freq = torch.fft.fftn(x.float(), dim=(-2, -1))&#xA;    x_freq = torch.fft.fftshift(x_freq, dim=(-2, -1))&#xA;    B, C, H, W = x_freq.shape&#xA;    mask = torch.ones((B, C, H, W), device=x.device)&#xA;    crow, ccol = H // 2, W //2&#xA;    mask[..., crow - threshold:crow + threshold, ccol - threshold:ccol + threshold] = scale&#xA;    x_freq = x_freq * mask&#xA;    x_freq = torch.fft.ifftshift(x_freq, dim=(-2, -1))&#xA;    x_filtered = torch.fft.ifftn(x_freq, dim=(-2, -1)).real&#xA;    return x_filtered.to(x.dtype)&#xA;&#xA;&#xA;def set_freeu_v2_patch(model, b1, b2, s1, s2):&#xA;    model_channels = model.model.model_config.unet_config[&#34;model_channels&#34;]&#xA;    scale_dict = {model_channels * 4: (b1, s1), model_channels * 2: (b2, s2)}&#xA;&#xA;    def output_block_patch(h, hsp, *args, **kwargs):&#xA;        scale = scale_dict.get(h.shape[1], None)&#xA;        if scale is not None:&#xA;            hidden_mean = h.mean(1).unsqueeze(1)&#xA;            B = hidden_mean.shape[0]&#xA;            hidden_max, _ = torch.max(hidden_mean.view(B, -1), dim=-1, keepdim=True)&#xA;            hidden_min, _ = torch.min(hidden_mean.view(B, -1), dim=-1, keepdim=True)&#xA;            hidden_mean = (hidden_mean - hidden_min.unsqueeze(2).unsqueeze(3)) / \&#xA;                          (hidden_max - hidden_min).unsqueeze(2).unsqueeze(3)&#xA;            h[:, :h.shape[1] // 2] = h[:, :h.shape[1] // 2] * ((scale[0] - 1) * hidden_mean + 1)&#xA;            hsp = Fourier_filter(hsp, threshold=1, scale=scale[1])&#xA;        return h, hsp&#xA;&#xA;    m = model.clone()&#xA;    m.set_model_output_block_patch(output_block_patch)&#xA;    return m&#xA;&#xA;&#xA;class FreeUForForge(scripts.Script):&#xA;    def title(self):&#xA;        return &#34;FreeU Integrated&#34;&#xA;&#xA;    def show(self, is_img2img):&#xA;        # make this extension visible in both txt2img and img2img tab.&#xA;        return scripts.AlwaysVisible&#xA;&#xA;    def ui(self, *args, **kwargs):&#xA;        with gr.Accordion(open=False, label=self.title()):&#xA;            freeu_enabled = gr.Checkbox(label=&#39;Enabled&#39;, value=False)&#xA;            freeu_b1 = gr.Slider(label=&#39;B1&#39;, minimum=0, maximum=2, step=0.01, value=1.01)&#xA;            freeu_b2 = gr.Slider(label=&#39;B2&#39;, minimum=0, maximum=2, step=0.01, value=1.02)&#xA;            freeu_s1 = gr.Slider(label=&#39;S1&#39;, minimum=0, maximum=4, step=0.01, value=0.99)&#xA;            freeu_s2 = gr.Slider(label=&#39;S2&#39;, minimum=0, maximum=4, step=0.01, value=0.95)&#xA;&#xA;        return freeu_enabled, freeu_b1, freeu_b2, freeu_s1, freeu_s2&#xA;&#xA;    def process_before_every_sampling(self, p, *script_args, **kwargs):&#xA;        # This will be called before every sampling.&#xA;        # If you use highres fix, this will be called twice.&#xA;        &#xA;        freeu_enabled, freeu_b1, freeu_b2, freeu_s1, freeu_s2 = script_args&#xA;&#xA;        if not freeu_enabled:&#xA;            return&#xA;&#xA;        unet = p.sd_model.forge_objects.unet&#xA;&#xA;        unet = set_freeu_v2_patch(unet, freeu_b1, freeu_b2, freeu_s1, freeu_s2)&#xA;&#xA;        p.sd_model.forge_objects.unet = unet&#xA;&#xA;        # Below codes will add some logs to the texts below the image outputs on UI.&#xA;        # The extra_generation_params does not influence results.&#xA;        p.extra_generation_params.update(dict(&#xA;            freeu_enabled=freeu_enabled,&#xA;            freeu_b1=freeu_b1,&#xA;            freeu_b2=freeu_b2,&#xA;            freeu_s1=freeu_s1,&#xA;            freeu_s2=freeu_s2,&#xA;        ))&#xA;&#xA;        return&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It looks like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/277bac6e-5ea7-4bff-b71a-e55a60cfc03c&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Similar components like HyperTile, KohyaHighResFix, SAG, can all be implemented within 100 lines of codes (see also the codes).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/06472b03-b833-4816-ab47-70712ac024d3&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ControlNets can finally be called by different extensions.&lt;/p&gt; &#xA;&lt;p&gt;Implementing Stable Video Diffusion and Zero123 are also super simple now (see also the codes).&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Stable Video Diffusion:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;extensions-builtin/sd_forge_svd/scripts/forge_svd.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import gradio as gr&#xA;import os&#xA;import pathlib&#xA;&#xA;from modules import script_callbacks&#xA;from modules.paths import models_path&#xA;from modules.ui_common import ToolButton, refresh_symbol&#xA;from modules import shared&#xA;&#xA;from modules_forge.forge_util import numpy_to_pytorch, pytorch_to_numpy&#xA;from ldm_patched.modules.sd import load_checkpoint_guess_config&#xA;from ldm_patched.contrib.external_video_model import VideoLinearCFGGuidance, SVD_img2vid_Conditioning&#xA;from ldm_patched.contrib.external import KSampler, VAEDecode&#xA;&#xA;&#xA;opVideoLinearCFGGuidance = VideoLinearCFGGuidance()&#xA;opSVD_img2vid_Conditioning = SVD_img2vid_Conditioning()&#xA;opKSampler = KSampler()&#xA;opVAEDecode = VAEDecode()&#xA;&#xA;svd_root = os.path.join(models_path, &#39;svd&#39;)&#xA;os.makedirs(svd_root, exist_ok=True)&#xA;svd_filenames = []&#xA;&#xA;&#xA;def update_svd_filenames():&#xA;    global svd_filenames&#xA;    svd_filenames = [&#xA;        pathlib.Path(x).name for x in&#xA;        shared.walk_files(svd_root, allowed_extensions=[&#34;.pt&#34;, &#34;.ckpt&#34;, &#34;.safetensors&#34;])&#xA;    ]&#xA;    return svd_filenames&#xA;&#xA;&#xA;@torch.inference_mode()&#xA;@torch.no_grad()&#xA;def predict(filename, width, height, video_frames, motion_bucket_id, fps, augmentation_level,&#xA;            sampling_seed, sampling_steps, sampling_cfg, sampling_sampler_name, sampling_scheduler,&#xA;            sampling_denoise, guidance_min_cfg, input_image):&#xA;    filename = os.path.join(svd_root, filename)&#xA;    model_raw, _, vae, clip_vision = \&#xA;        load_checkpoint_guess_config(filename, output_vae=True, output_clip=False, output_clipvision=True)&#xA;    model = opVideoLinearCFGGuidance.patch(model_raw, guidance_min_cfg)[0]&#xA;    init_image = numpy_to_pytorch(input_image)&#xA;    positive, negative, latent_image = opSVD_img2vid_Conditioning.encode(&#xA;        clip_vision, init_image, vae, width, height, video_frames, motion_bucket_id, fps, augmentation_level)&#xA;    output_latent = opKSampler.sample(model, sampling_seed, sampling_steps, sampling_cfg,&#xA;                                      sampling_sampler_name, sampling_scheduler, positive,&#xA;                                      negative, latent_image, sampling_denoise)[0]&#xA;    output_pixels = opVAEDecode.decode(vae, output_latent)[0]&#xA;    outputs = pytorch_to_numpy(output_pixels)&#xA;    return outputs&#xA;&#xA;&#xA;def on_ui_tabs():&#xA;    with gr.Blocks() as svd_block:&#xA;        with gr.Row():&#xA;            with gr.Column():&#xA;                input_image = gr.Image(label=&#39;Input Image&#39;, source=&#39;upload&#39;, type=&#39;numpy&#39;, height=400)&#xA;&#xA;                with gr.Row():&#xA;                    filename = gr.Dropdown(label=&#34;SVD Checkpoint Filename&#34;,&#xA;                                           choices=svd_filenames,&#xA;                                           value=svd_filenames[0] if len(svd_filenames) &amp;gt; 0 else None)&#xA;                    refresh_button = ToolButton(value=refresh_symbol, tooltip=&#34;Refresh&#34;)&#xA;                    refresh_button.click(&#xA;                        fn=lambda: gr.update(choices=update_svd_filenames),&#xA;                        inputs=[], outputs=filename)&#xA;&#xA;                width = gr.Slider(label=&#39;Width&#39;, minimum=16, maximum=8192, step=8, value=1024)&#xA;                height = gr.Slider(label=&#39;Height&#39;, minimum=16, maximum=8192, step=8, value=576)&#xA;                video_frames = gr.Slider(label=&#39;Video Frames&#39;, minimum=1, maximum=4096, step=1, value=14)&#xA;                motion_bucket_id = gr.Slider(label=&#39;Motion Bucket Id&#39;, minimum=1, maximum=1023, step=1, value=127)&#xA;                fps = gr.Slider(label=&#39;Fps&#39;, minimum=1, maximum=1024, step=1, value=6)&#xA;                augmentation_level = gr.Slider(label=&#39;Augmentation Level&#39;, minimum=0.0, maximum=10.0, step=0.01,&#xA;                                               value=0.0)&#xA;                sampling_steps = gr.Slider(label=&#39;Sampling Steps&#39;, minimum=1, maximum=200, step=1, value=20)&#xA;                sampling_cfg = gr.Slider(label=&#39;CFG Scale&#39;, minimum=0.0, maximum=50.0, step=0.1, value=2.5)&#xA;                sampling_denoise = gr.Slider(label=&#39;Sampling Denoise&#39;, minimum=0.0, maximum=1.0, step=0.01, value=1.0)&#xA;                guidance_min_cfg = gr.Slider(label=&#39;Guidance Min Cfg&#39;, minimum=0.0, maximum=100.0, step=0.5, value=1.0)&#xA;                sampling_sampler_name = gr.Radio(label=&#39;Sampler Name&#39;,&#xA;                                                 choices=[&#39;euler&#39;, &#39;euler_ancestral&#39;, &#39;heun&#39;, &#39;heunpp2&#39;, &#39;dpm_2&#39;,&#xA;                                                          &#39;dpm_2_ancestral&#39;, &#39;lms&#39;, &#39;dpm_fast&#39;, &#39;dpm_adaptive&#39;,&#xA;                                                          &#39;dpmpp_2s_ancestral&#39;, &#39;dpmpp_sde&#39;, &#39;dpmpp_sde_gpu&#39;,&#xA;                                                          &#39;dpmpp_2m&#39;, &#39;dpmpp_2m_sde&#39;, &#39;dpmpp_2m_sde_gpu&#39;,&#xA;                                                          &#39;dpmpp_3m_sde&#39;, &#39;dpmpp_3m_sde_gpu&#39;, &#39;ddpm&#39;, &#39;lcm&#39;, &#39;ddim&#39;,&#xA;                                                          &#39;uni_pc&#39;, &#39;uni_pc_bh2&#39;], value=&#39;euler&#39;)&#xA;                sampling_scheduler = gr.Radio(label=&#39;Scheduler&#39;,&#xA;                                              choices=[&#39;normal&#39;, &#39;karras&#39;, &#39;exponential&#39;, &#39;sgm_uniform&#39;, &#39;simple&#39;,&#xA;                                                       &#39;ddim_uniform&#39;], value=&#39;karras&#39;)&#xA;                sampling_seed = gr.Number(label=&#39;Seed&#39;, value=12345, precision=0)&#xA;&#xA;                generate_button = gr.Button(value=&#34;Generate&#34;)&#xA;&#xA;                ctrls = [filename, width, height, video_frames, motion_bucket_id, fps, augmentation_level,&#xA;                         sampling_seed, sampling_steps, sampling_cfg, sampling_sampler_name, sampling_scheduler,&#xA;                         sampling_denoise, guidance_min_cfg, input_image]&#xA;&#xA;            with gr.Column():&#xA;                output_gallery = gr.Gallery(label=&#39;Gallery&#39;, show_label=False, object_fit=&#39;contain&#39;,&#xA;                                            visible=True, height=1024, columns=4)&#xA;&#xA;        generate_button.click(predict, inputs=ctrls, outputs=[output_gallery])&#xA;    return [(svd_block, &#34;SVD&#34;, &#34;svd&#34;)]&#xA;&#xA;&#xA;update_svd_filenames()&#xA;script_callbacks.on_ui_tabs(on_ui_tabs)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that although the above codes look like independent codes, they actually will automatically offload/unload any other models. For example, below is me opening webui, load SDXL, generated an image, then go to SVD, then generated image frames. You can see that the GPU memory is perfectly managed and the SDXL is moved to RAM then SVD is moved to GPU.&lt;/p&gt; &#xA;&lt;p&gt;Note that this management is fully automatic. This makes writing extensions super simple.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/de1a2d05-344a-44d7-bab8-9ecc0a58a8d3&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/14bcefcf-599f-42c3-bce9-3fd5e428dd91&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Similarly, Zero123:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/7685019c-7239-47fb-9cb5-2b7b33943285&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Write a simple ControlNet:&lt;/h3&gt; &#xA;&lt;p&gt;Below is a simple extension to have a completely independent pass of ControlNet that never conflicts any other extensions:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;extensions-builtin/sd_forge_controlnet_example/scripts/sd_forge_controlnet_example.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that this extension is hidden because it is only for developers. To see it in UI, use &lt;code&gt;--show-controlnet-example&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The memory optimization in this example is fully automatic. You do not need to care about memory and inference speed, but you may want to cache objects if you wish.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Use --show-controlnet-example to see this extension.&#xA;&#xA;import cv2&#xA;import gradio as gr&#xA;import torch&#xA;&#xA;from modules import scripts&#xA;from modules.shared_cmd_options import cmd_opts&#xA;from modules_forge.shared import supported_preprocessors&#xA;from modules.modelloader import load_file_from_url&#xA;from ldm_patched.modules.controlnet import load_controlnet&#xA;from modules_forge.controlnet import apply_controlnet_advanced&#xA;from modules_forge.forge_util import numpy_to_pytorch&#xA;from modules_forge.shared import controlnet_dir&#xA;&#xA;&#xA;class ControlNetExampleForge(scripts.Script):&#xA;    model = None&#xA;&#xA;    def title(self):&#xA;        return &#34;ControlNet Example for Developers&#34;&#xA;&#xA;    def show(self, is_img2img):&#xA;        # make this extension visible in both txt2img and img2img tab.&#xA;        return scripts.AlwaysVisible&#xA;&#xA;    def ui(self, *args, **kwargs):&#xA;        with gr.Accordion(open=False, label=self.title()):&#xA;            gr.HTML(&#39;This is an example controlnet extension for developers.&#39;)&#xA;            gr.HTML(&#39;You see this extension because you used --show-controlnet-example&#39;)&#xA;            input_image = gr.Image(source=&#39;upload&#39;, type=&#39;numpy&#39;)&#xA;            funny_slider = gr.Slider(label=&#39;This slider does nothing. It just shows you how to transfer parameters.&#39;,&#xA;                                     minimum=0.0, maximum=1.0, value=0.5)&#xA;&#xA;        return input_image, funny_slider&#xA;&#xA;    def process(self, p, *script_args, **kwargs):&#xA;        input_image, funny_slider = script_args&#xA;&#xA;        # This slider does nothing. It just shows you how to transfer parameters.&#xA;        del funny_slider&#xA;&#xA;        if input_image is None:&#xA;            return&#xA;&#xA;        # controlnet_canny_path = load_file_from_url(&#xA;        #     url=&#39;https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/sai_xl_canny_256lora.safetensors&#39;,&#xA;        #     model_dir=model_dir,&#xA;        #     file_name=&#39;sai_xl_canny_256lora.safetensors&#39;&#xA;        # )&#xA;        controlnet_canny_path = load_file_from_url(&#xA;            url=&#39;https://huggingface.co/lllyasviel/fav_models/resolve/main/fav/control_v11p_sd15_canny_fp16.safetensors&#39;,&#xA;            model_dir=controlnet_dir,&#xA;            file_name=&#39;control_v11p_sd15_canny_fp16.safetensors&#39;&#xA;        )&#xA;        print(&#39;The model [control_v11p_sd15_canny_fp16.safetensors] download finished.&#39;)&#xA;&#xA;        self.model = load_controlnet(controlnet_canny_path)&#xA;        print(&#39;Controlnet loaded.&#39;)&#xA;&#xA;        return&#xA;&#xA;    def process_before_every_sampling(self, p, *script_args, **kwargs):&#xA;        # This will be called before every sampling.&#xA;        # If you use highres fix, this will be called twice.&#xA;&#xA;        input_image, funny_slider = script_args&#xA;&#xA;        if input_image is None or self.model is None:&#xA;            return&#xA;&#xA;        B, C, H, W = kwargs[&#39;noise&#39;].shape  # latent_shape&#xA;        height = H * 8&#xA;        width = W * 8&#xA;        batch_size = p.batch_size&#xA;&#xA;        preprocessor = supported_preprocessors[&#39;canny&#39;]&#xA;&#xA;        # detect control at certain resolution&#xA;        control_image = preprocessor(&#xA;            input_image, resolution=512, slider_1=100, slider_2=200, slider_3=None)&#xA;&#xA;        # here we just use nearest neighbour to align input shape.&#xA;        # You may want crop and resize, or crop and fill, or others.&#xA;        control_image = cv2.resize(&#xA;            control_image, (width, height), interpolation=cv2.INTER_NEAREST)&#xA;&#xA;        # Output preprocessor result. Now called every sampling. Cache in your own way.&#xA;        p.extra_result_images.append(control_image)&#xA;&#xA;        print(&#39;Preprocessor Canny finished.&#39;)&#xA;&#xA;        control_image_bchw = numpy_to_pytorch(control_image).movedim(-1, 1)&#xA;&#xA;        unet = p.sd_model.forge_objects.unet&#xA;&#xA;        # Unet has input, middle, output blocks, and we can give different weights&#xA;        # to each layers in all blocks.&#xA;        # Below is an example for stronger control in middle block.&#xA;        # This is helpful for some high-res fix passes. (p.is_hr_pass)&#xA;        positive_advanced_weighting = {&#xA;            &#39;input&#39;: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2],&#xA;            &#39;middle&#39;: [1.0],&#xA;            &#39;output&#39;: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2]&#xA;        }&#xA;        negative_advanced_weighting = {&#xA;            &#39;input&#39;: [0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95, 1.05, 1.15, 1.25],&#xA;            &#39;middle&#39;: [1.05],&#xA;            &#39;output&#39;: [0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95, 1.05, 1.15, 1.25]&#xA;        }&#xA;&#xA;        # The advanced_frame_weighting is a weight applied to each image in a batch.&#xA;        # The length of this list must be same with batch size&#xA;        # For example, if batch size is 5, the below list is [0.2, 0.4, 0.6, 0.8, 1.0]&#xA;        # If you view the 5 images as 5 frames in a video, this will lead to&#xA;        # progressively stronger control over time.&#xA;        advanced_frame_weighting = [float(i + 1) / float(batch_size) for i in range(batch_size)]&#xA;&#xA;        # The advanced_sigma_weighting allows you to dynamically compute control&#xA;        # weights given diffusion timestep (sigma).&#xA;        # For example below code can softly make beginning steps stronger than ending steps.&#xA;        sigma_max = unet.model.model_sampling.sigma_max&#xA;        sigma_min = unet.model.model_sampling.sigma_min&#xA;        advanced_sigma_weighting = lambda s: (s - sigma_min) / (sigma_max - sigma_min)&#xA;&#xA;        # You can even input a tensor to mask all control injections&#xA;        # The mask will be automatically resized during inference in UNet.&#xA;        # The size should be B 1 H W and the H and W are not important&#xA;        # because they will be resized automatically&#xA;        advanced_mask_weighting = torch.ones(size=(1, 1, 512, 512))&#xA;&#xA;        # But in this simple example we do not use them&#xA;        positive_advanced_weighting = None&#xA;        negative_advanced_weighting = None&#xA;        advanced_frame_weighting = None&#xA;        advanced_sigma_weighting = None&#xA;        advanced_mask_weighting = None&#xA;&#xA;        unet = apply_controlnet_advanced(unet=unet, controlnet=self.model, image_bchw=control_image_bchw,&#xA;                                         strength=0.6, start_percent=0.0, end_percent=0.8,&#xA;                                         positive_advanced_weighting=positive_advanced_weighting,&#xA;                                         negative_advanced_weighting=negative_advanced_weighting,&#xA;                                         advanced_frame_weighting=advanced_frame_weighting,&#xA;                                         advanced_sigma_weighting=advanced_sigma_weighting,&#xA;                                         advanced_mask_weighting=advanced_mask_weighting)&#xA;&#xA;        p.sd_model.forge_objects.unet = unet&#xA;&#xA;        # Below codes will add some logs to the texts below the image outputs on UI.&#xA;        # The extra_generation_params does not influence results.&#xA;        p.extra_generation_params.update(dict(&#xA;            controlnet_info=&#39;You should see these texts below output images!&#39;,&#xA;        ))&#xA;&#xA;        return&#xA;&#xA;&#xA;# Use --show-controlnet-example to see this extension.&#xA;if not cmd_opts.show_controlnet_example:&#xA;    del ControlNetExampleForge&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/822fa2fc-c9f4-4f58-8669-4b6680b91063&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Add a preprocessor&lt;/h3&gt; &#xA;&lt;p&gt;Below is the full codes to add a normalbae preprocessor with perfect memory managements.&lt;/p&gt; &#xA;&lt;p&gt;You can use arbitrary independent extensions to add a preprocessor.&lt;/p&gt; &#xA;&lt;p&gt;Your preprocessor will be read by all other extensions using &lt;code&gt;modules_forge.shared.preprocessors&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Below codes are in &lt;code&gt;extensions-builtin\forge_preprocessor_normalbae\scripts\preprocessor_normalbae.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from modules_forge.supported_preprocessor import Preprocessor, PreprocessorParameter&#xA;from modules_forge.shared import preprocessor_dir, add_supported_preprocessor&#xA;from modules_forge.forge_util import resize_image_with_pad&#xA;from modules.modelloader import load_file_from_url&#xA;&#xA;import types&#xA;import torch&#xA;import numpy as np&#xA;&#xA;from einops import rearrange&#xA;from annotator.normalbae.models.NNET import NNET&#xA;from annotator.normalbae import load_checkpoint&#xA;from torchvision import transforms&#xA;&#xA;&#xA;class PreprocessorNormalBae(Preprocessor):&#xA;    def __init__(self):&#xA;        super().__init__()&#xA;        self.name = &#39;normalbae&#39;&#xA;        self.tags = [&#39;NormalMap&#39;]&#xA;        self.model_filename_filters = [&#39;normal&#39;]&#xA;        self.slider_resolution = PreprocessorParameter(&#xA;            label=&#39;Resolution&#39;, minimum=128, maximum=2048, value=512, step=8, visible=True)&#xA;        self.slider_1 = PreprocessorParameter(visible=False)&#xA;        self.slider_2 = PreprocessorParameter(visible=False)&#xA;        self.slider_3 = PreprocessorParameter(visible=False)&#xA;        self.show_control_mode = True&#xA;        self.do_not_need_model = False&#xA;        self.sorting_priority = 100  # higher goes to top in the list&#xA;&#xA;    def load_model(self):&#xA;        if self.model_patcher is not None:&#xA;            return&#xA;&#xA;        model_path = load_file_from_url(&#xA;            &#34;https://huggingface.co/lllyasviel/Annotators/resolve/main/scannet.pt&#34;,&#xA;            model_dir=preprocessor_dir)&#xA;&#xA;        args = types.SimpleNamespace()&#xA;        args.mode = &#39;client&#39;&#xA;        args.architecture = &#39;BN&#39;&#xA;        args.pretrained = &#39;scannet&#39;&#xA;        args.sampling_ratio = 0.4&#xA;        args.importance_ratio = 0.7&#xA;        model = NNET(args)&#xA;        model = load_checkpoint(model_path, model)&#xA;        self.norm = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])&#xA;&#xA;        self.model_patcher = self.setup_model_patcher(model)&#xA;&#xA;    def __call__(self, input_image, resolution, slider_1=None, slider_2=None, slider_3=None, **kwargs):&#xA;        input_image, remove_pad = resize_image_with_pad(input_image, resolution)&#xA;&#xA;        self.load_model()&#xA;&#xA;        self.move_all_model_patchers_to_gpu()&#xA;&#xA;        assert input_image.ndim == 3&#xA;        image_normal = input_image&#xA;&#xA;        with torch.no_grad():&#xA;            image_normal = self.send_tensor_to_model_device(torch.from_numpy(image_normal))&#xA;            image_normal = image_normal / 255.0&#xA;            image_normal = rearrange(image_normal, &#39;h w c -&amp;gt; 1 c h w&#39;)&#xA;            image_normal = self.norm(image_normal)&#xA;&#xA;            normal = self.model_patcher.model(image_normal)&#xA;            normal = normal[0][-1][:, :3]&#xA;            normal = ((normal + 1) * 0.5).clip(0, 1)&#xA;&#xA;            normal = rearrange(normal[0], &#39;c h w -&amp;gt; h w c&#39;).cpu().numpy()&#xA;            normal_image = (normal * 255.0).clip(0, 255).astype(np.uint8)&#xA;&#xA;        return remove_pad(normal_image)&#xA;&#xA;&#xA;add_supported_preprocessor(PreprocessorNormalBae())&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;New features (that are not available in original WebUI)&lt;/h1&gt; &#xA;&lt;p&gt;Thanks to Unet Patcher, many new things are possible now and supported in Forge, including SVD, Z123, masked Ip-adapter, masked controlnet, photomaker, etc.&lt;/p&gt; &#xA;&lt;p&gt;Masked Ip-Adapter&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/d26630f9-922d-4483-8bf9-f364dca5fd50&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/03580ef7-235c-4b03-9ca6-a27677a5a175&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/d9ed4a01-70d4-45b4-a6a7-2f765f158fae&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Masked ControlNet&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/872d4785-60e4-4431-85c7-665c781dddaa&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/335a3b33-1ef8-46ff-a462-9f1b4f2c49fc&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/b3684a15-8895-414e-8188-487269dfcada&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;PhotoMaker&lt;/p&gt; &#xA;&lt;p&gt;(Note that photomaker is a special control that need you to add the trigger word &#34;photomaker&#34;. Your prompt should be like &#34;a photo of photomaker&#34;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/07b0b626-05b5-473b-9d69-3657624d59be&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Marigold Depth&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/bdf54148-892d-410d-8ed9-70b4b121b6e7&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;New Samplers (that are not in origin)&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;DDPM&#xA;DDPM Karras&#xA;DPM++ 2M Turbo&#xA;DPM++ 2M SDE Turbo&#xA;LCM Karras&#xA;Euler A Turbo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;About Extensions&lt;/h1&gt; &#xA;&lt;p&gt;ControlNet and TiledVAE are integrated, and you should uninstall these two extensions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sd-webui-controlnet&#xA;multidiffusion-upscaler-for-automatic1111&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;strong&gt;AnimateDiff&lt;/strong&gt; is under construction by &lt;a href=&#34;https://github.com/continue-revolution&#34;&gt;continue-revolution&lt;/a&gt; at &lt;a href=&#34;https://github.com/continue-revolution/sd-webui-animatediff/tree/forge/master&#34;&gt;sd-webui-animatediff forge/master branch&lt;/a&gt; and &lt;a href=&#34;https://github.com/continue-revolution/sd-forge-animatediff&#34;&gt;sd-forge-animatediff&lt;/a&gt; (they are in sync). (continue-revolution original words: prompt travel, inf t2v, controlnet v2v have been proven to work well; motion lora, i2i batch still under construction and may be finished in a week&#34;)&lt;/p&gt; &#xA;&lt;p&gt;Other extensions should work without problems, like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;canvas-zoom&#xA;translations/localizations&#xA;Dynamic Prompts&#xA;Adetailer&#xA;Ultimate SD Upscale&#xA;Reactor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, if newer extensions use Forge, their codes can be much shorter.&lt;/p&gt; &#xA;&lt;p&gt;Usually if an old extension rework using Forge&#39;s unet patcher, 80% codes can be removed, especially when they need to call controlnet.&lt;/p&gt; &#xA;&lt;h1&gt;Contribution&lt;/h1&gt; &#xA;&lt;p&gt;Forge uses a bot to get commits and codes from &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/tree/dev&#34;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui/tree/dev&lt;/a&gt; every afternoon (if merge is automatically successful by a git bot, or by my compiler, or by my ChatGPT bot) or mid-night (if my compiler and my ChatGPT bot both failed to merge and I review it manually).&lt;/p&gt; &#xA;&lt;p&gt;All PRs that can be implemented in &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/tree/dev&#34;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui/tree/dev&lt;/a&gt; should submit PRs there.&lt;/p&gt; &#xA;&lt;p&gt;Feel free to submit PRs related to the functionality of Forge here.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>awslabs/llrt</title>
    <updated>2024-02-18T01:42:21Z</updated>
    <id>tag:github.com,2024-02-18:/awslabs/llrt</id>
    <link href="https://github.com/awslabs/llrt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLRT (Low Latency Runtime) is an experimental, lightweight JavaScript runtime designed to address the growing demand for fast and efficient Serverless applications.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/awslabs/llrt/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/awslabs/llrt/actions/workflows/ci.yml/badge.svg?branch=main&#34; alt=&#34;LLRT CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/awslabs/llrt/actions/workflows/release.yml&#34;&gt;&lt;img src=&#34;https://github.com/awslabs/llrt/actions/workflows/release.yml/badge.svg?sanitize=true&#34; alt=&#34;LLRT Release&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;LLRT (&lt;strong&gt;L&lt;/strong&gt;ow &lt;strong&gt;L&lt;/strong&gt;atency &lt;strong&gt;R&lt;/strong&gt;un&lt;strong&gt;t&lt;/strong&gt;ime) is a lightweight JavaScript runtime designed to address the growing demand for fast and efficient Serverless applications. LLRT offers up to over &lt;strong&gt;10x&lt;/strong&gt; faster startup and up to &lt;strong&gt;2x&lt;/strong&gt; overall lower cost compared to other JavaScript runtimes running on &lt;strong&gt;AWS Lambda&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s built in Rust, utilizing QuickJS as JavaScript engine, ensuring efficient memory usage and swift startup.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] LLRT is an &lt;strong&gt;experimental&lt;/strong&gt; package. It is subject to change and intended only for evaluation purposes.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;sub&gt;LLRT - &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/llrt/main/example/functions/src/v3-lib.mjs&#34;&gt;DynamoDB Put, ARM, 128MB&lt;/a&gt;:&lt;sub&gt; &lt;img src=&#34;https://raw.githubusercontent.com/awslabs/llrt/main/benchmarks/llrt-ddb-put.png&#34; alt=&#34;DynamoDB Put LLRT&#34; title=&#34;LLRT DynamoDB Put&#34;&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;sub&gt;Node.js 20 - &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/llrt/main/example/functions/src/v3-lib.mjs&#34;&gt;DynamoDB Put, ARM, 128MB&lt;/a&gt;:&lt;sub&gt; &lt;img src=&#34;https://raw.githubusercontent.com/awslabs/llrt/main/benchmarks/node20-ddb-put.png&#34; alt=&#34;DynamoDB Put Node20&#34; title=&#34;Node20 DynamoDB Put&#34;&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;p&gt;HTTP benchmarks measured in &lt;strong&gt;round trip time&lt;/strong&gt; for a cold start (&lt;a href=&#34;https://raw.githubusercontent.com/awslabs/llrt/main/#benchmark-methodology&#34;&gt;why?&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Configure Lambda functions to use LLRT&lt;/h2&gt; &#xA;&lt;p&gt;Download the last LLRT release from &lt;a href=&#34;https://github.com/awslabs/llrt/releases&#34;&gt;https://github.com/awslabs/llrt/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Option 1: Custom runtime (recommended)&lt;/h3&gt; &#xA;&lt;p&gt;Choose &lt;code&gt;Custom Runtime on Amazon Linux 2023&lt;/code&gt; and package the LLRT &lt;code&gt;bootstrap&lt;/code&gt; binary together with your JS code.&lt;/p&gt; &#xA;&lt;h3&gt;Option 2: Use a layer&lt;/h3&gt; &#xA;&lt;p&gt;Choose &lt;code&gt;Custom Runtime on Amazon Linux 2023&lt;/code&gt;, upload &lt;code&gt;llrt-lambda-arm64.zip&lt;/code&gt; or &lt;code&gt;llrt-lambda-x86.zip&lt;/code&gt; as a layer and add to your function&lt;/p&gt; &#xA;&lt;p&gt;That&#39;s it üéâ&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Even though LLRT supports &lt;a href=&#34;https://262.ecma-international.org/11.0/&#34;&gt;ES2020&lt;/a&gt; it&#39;s &lt;strong&gt;NOT&lt;/strong&gt; a drop in replacement for Node.js. Consult &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/llrt/main/#compatibility-matrix&#34;&gt;Compatibility matrix&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/llrt/main/API.md&#34;&gt;API&lt;/a&gt; for more details. All dependencies should be bundled for a &lt;code&gt;browser&lt;/code&gt; platform and mark included &lt;code&gt;@aws-sdk&lt;/code&gt; packages as external.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Option 3: AWS SAM&lt;/h3&gt; &#xA;&lt;p&gt;The following &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/llrt/main/example/llrt-sam/&#34;&gt;example project&lt;/a&gt; sets up a lambda instrumented with a layer containing the llrt runtime.&lt;/p&gt; &#xA;&lt;h2&gt;Testing &amp;amp; ensuring compatibility&lt;/h2&gt; &#xA;&lt;p&gt;The best way to ensure that your code is compatible with LLRT is to write tests and executing them via the built in test runner&lt;/p&gt; &#xA;&lt;h3&gt;Test runner&lt;/h3&gt; &#xA;&lt;p&gt;Test runner uses a lightweight Jest-like API and uses the &lt;a href=&#34;https://nodejs.org/api/assert.html&#34;&gt;assert module&lt;/a&gt; from Node.js for test assertions. For examples how to implement tests for LLRT see the &lt;code&gt;/tests&lt;/code&gt; folder of this repository.&lt;/p&gt; &#xA;&lt;p&gt;To run tests, execute the &lt;code&gt;llrt test&lt;/code&gt; command. LLRT scans the current directory and sub-directories for files that ends with &lt;code&gt;*.test.js&lt;/code&gt; or &lt;code&gt;*.test.mjs&lt;/code&gt;. You can also provide a specific test directory to scan by using the &lt;code&gt;llrt test -d &amp;lt;directory&amp;gt;&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;p&gt;The test runner also has support for filters. Using filters is as simple as adding additional command line arguments, i.e: &lt;code&gt;llrt test crypto&lt;/code&gt; will only run tests that match the filename containing &lt;code&gt;crypto&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Compatibility matrix&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] LLRT only support a fraction of the Node.js APIs. It is &lt;strong&gt;NOT&lt;/strong&gt; a drop in replacement for Node.js, nor will it ever be. Below is a high level overview of partially supported APIs and modules. For more details consult the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/llrt/main/API.md&#34;&gt;API&lt;/a&gt; documentation&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Node.js&lt;/th&gt; &#xA;   &lt;th&gt;LLRT ‚ö†Ô∏è&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;buffer&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏éÔ∏è&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;streams&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é*&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;child_process&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é‚è±&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;net:sockets&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é‚è±&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;net:server&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tls&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úò‚è±&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;fetch&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;http&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úò‚è±**&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;https&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úò‚è±**&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;fs/promises&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;fs&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úò‚è±&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;path&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;timers&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;uuid&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;crypto&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;process&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;encoding&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;console&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;events&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ESM&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CJS&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;async/await&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Other modules&lt;/td&gt; &#xA;   &lt;td&gt;‚úîÔ∏é&lt;/td&gt; &#xA;   &lt;td&gt;‚úò&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;‚ö†Ô∏è = partially supported in LLRT&lt;/em&gt; &lt;em&gt;‚è± = planned partial support&lt;/em&gt; &lt;em&gt;* = Not native&lt;/em&gt; &lt;em&gt;** = Use fetch instead&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Using node_modules (dependencies) with LLRT&lt;/h2&gt; &#xA;&lt;p&gt;Since LLRT is meant for performance critical application it&#39;s not recommended to deploy &lt;code&gt;node_modules&lt;/code&gt; without bundling, minification and tree-shaking.&lt;/p&gt; &#xA;&lt;p&gt;LLRT can work with any bundler of your choice. Below are some configurations for popular bundlers:&lt;/p&gt; &#xA;&lt;h3&gt;ESBuild&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;esbuild index.js --platform=node --target=es2020 --format=esm --bundle --minify --external:@aws-sdk --external:@smithy --external:uuid&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Rollup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;import resolve from &#39;rollup-plugin-node-resolve&#39;;&#xA;import commonjs from &#39;rollup-plugin-commonjs&#39;;&#xA;import { terser } from &#39;rollup-plugin-terser&#39;;&#xA;&#xA;export default {&#xA;  input: &#39;index.js&#39;,&#xA;  output: {&#xA;    file: &#39;dist/bundle.js&#39;,&#xA;    format: &#39;esm&#39;,&#xA;    sourcemap: true,&#xA;    target: &#39;es2020&#39;,&#xA;  },&#xA;  plugins: [&#xA;    resolve(),&#xA;    commonjs(),&#xA;    terser(),&#xA;  ],&#xA;  external: [&#34;@aws-sdk&#34;,&#34;@smithy&#34;,&#34;uuid&#34;],&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Webpack&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;import TerserPlugin from &#39;terser-webpack-plugin&#39;;&#xA;import nodeExternals from &#39;webpack-node-externals&#39;;&#xA;&#xA;export default {&#xA;  entry: &#39;./index.js&#39;,&#xA;  output: {&#xA;    path: &#34;dist&#34;,&#xA;    filename: &#39;bundle.js&#39;,&#xA;    libraryTarget: &#39;module&#39;,&#xA;  },&#xA;  target: &#39;web&#39;,&#xA;  mode: &#39;production&#39;,&#xA;  resolve: {&#xA;    extensions: [&#39;.js&#39;],&#xA;  },&#xA;  externals: [nodeExternals(),&#34;@aws-sdk&#34;,&#34;@smithy&#34;,&#34;uuid&#34;],&#xA;  optimization: {&#xA;    minimize: true,&#xA;    minimizer: [&#xA;      new TerserPlugin({&#xA;        terserOptions: {&#xA;          ecma: 2020,&#xA;        },&#xA;      }),&#xA;    ],&#xA;  },&#xA;};&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using AWS SDK (v3) with LLRT&lt;/h2&gt; &#xA;&lt;p&gt;LLRT includes many AWS SDK clients and utils as part of the runtime, built into the executable. These SDK Clients have been specifically fine-tuned to offer best performance while not compromising on compatibility. LLRT replaces some JavaScript dependencies used by the AWS SDK by native ones such as Hash calculations and XML parsing. V3 SDK packages not included in the list below have to be bundled with your source code while marking the following packages as external:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Bundled AWS SDK packages&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-dynamodb&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/lib-dynamodb&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-kms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-lambda&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-s3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-secrets-manager&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-ses&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-sns&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-sqs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-sts&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-ssm&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-cloudwatch-logs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-cloudwatch-events&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-eventbridge&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-sfn&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-xray&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/client-cognito-identity&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/util-dynamodb&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@aws-sdk/credential-providers&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@smithy&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] LLRT currently does not support returning streams from SDK responses. Use &lt;code&gt;response.Body.transformToString();&lt;/code&gt; or &lt;code&gt;response.Body.transformToByteArray();&lt;/code&gt; as shown below.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const response = await client.send(command);&#xA;// or &#39;transformToByteArray()&#39;&#xA;const str = await response.Body.transformToString();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Running TypeScript with LLRT&lt;/h2&gt; &#xA;&lt;p&gt;Same principle as dependencies applies when using TypeScript. TypeScript must be bundled and transpiled into ES2020 JavaScript.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] LLRT will not support running TypeScript without transpilation. This is by design for performance reasons. Transpiling requires CPU and memory that adds latency and cost during execution. This can be avoided if done ahead of time during deployment.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Rationale&lt;/h2&gt; &#xA;&lt;p&gt;What justifies the introduction of another JavaScript runtime in light of existing options such as &lt;a href=&#34;https://nodejs.org/en&#34;&gt;Node.js&lt;/a&gt;, &lt;a href=&#34;https://bun.sh&#34;&gt;Bun&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://deno.com/&#34;&gt;Deno&lt;/a&gt;?&lt;/p&gt; &#xA;&lt;p&gt;Node.js, Bun, and Deno represent highly proficient JavaScript runtimes. However, they are designed with general-purpose applications in mind. These runtimes were not specifically tailored for the demands of a Serverless environment, characterized by short-lived runtime instances. They each depend on a (&lt;a href=&#34;https://en.wikipedia.org/wiki/Just-in-time_compilation&#34;&gt;Just-In-Time compiler (JIT)&lt;/a&gt; for dynamic code compilation and optimization during execution. While JIT compilation offers substantial long-term performance advantages, it carries a computational and memory overhead.&lt;/p&gt; &#xA;&lt;p&gt;In contrast, LLRT distinguishes itself by not incorporating a JIT compiler, a strategic decision that yields two significant advantages:&lt;/p&gt; &#xA;&lt;p&gt;A) JIT compilation is a notably sophisticated technological component, introducing increased system complexity and contributing substantially to the runtime&#39;s overall size.&lt;/p&gt; &#xA;&lt;p&gt;B) Without the JIT overhead, LLRT conserves both CPU and memory resources that can be more efficiently allocated to code execution tasks, thereby reducing application startup times.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;p&gt;There are many cases where LLRT shows notable performance drawbacks compared with JIT-powered runtimes, such as large data processing, Monte Carlo simulations or performing tasks with hundreds of thousands or millions of iterations. LLRT is most effective when applied to smaller Serverless functions dedicated to tasks such as data transformation, real time processing, AWS service integrations, authorization, validation etc. It is designed to complement existing components rather than serve as a comprehensive replacement for everything. Notably, given its supported APIs are based on Node.js specification, transitioning back to alternative solutions requires minimal code adjustments.&lt;/p&gt; &#xA;&lt;h2&gt;Building from source&lt;/h2&gt; &#xA;&lt;p&gt;Clone code and cd to directory&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:awslabs/llrt.git --recursive&#xA;cd llrt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install rust&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | bash -s -- -y&#xA;source &#34;$HOME/.cargo/env&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependencies&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# MacOS&#xA;brew install zig make zstd node corepack&#xA;&#xA;# Ubuntu&#xA;sudo apt -y install make zstd&#xA;sudo snap install zig --classic --beta&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install Node.js packages&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;corepack enable&#xA;yarn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install generate libs and setup rust targets &amp;amp; toolchains&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make stdlib &amp;amp;&amp;amp; make libs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build release for Lambda&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make release-arm64&#xA;# or for x86-64, use&#xA;make release-x64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optionally build for your local machine (Mac or Linux)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should now have a &lt;code&gt;llrt-lambda-arm64.zip&lt;/code&gt; or &lt;code&gt;llrt-lambda-x86.zip&lt;/code&gt;. You can manually upload this as a Lambda layer or use it via your Infrastructure-as-code pipeline&lt;/p&gt; &#xA;&lt;h2&gt;Running Lambda emulator&lt;/h2&gt; &#xA;&lt;p&gt;Please note that in order to run the example you will need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Valid AWS credentials via a &lt;code&gt;~/.aws/credentials&lt;/code&gt; or via environment variables.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export AWS_ACCESS_KEY_ID=XXX&#xA;export AWS_SECRET_ACCESS_KEY=YYY&#xA;export AWS_REGION=us-east-1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A DynamoDB table (with &lt;code&gt;id&lt;/code&gt; as the partition key) on &lt;code&gt;us-east-1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;dynamodb:PutItem&lt;/code&gt; IAM permission on this table. You can use this policy (don&#39;t forget to modify &amp;lt;YOUR_ACCOUNT_ID&amp;gt;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;&#x9;&#34;Version&#34;: &#34;2012-10-17&#34;,&#xA;&#x9;&#34;Statement&#34;: [&#xA;&#x9;&#x9;{&#xA;&#x9;&#x9;&#x9;&#34;Sid&#34;: &#34;putItem&#34;,&#xA;&#x9;&#x9;&#x9;&#34;Effect&#34;: &#34;Allow&#34;,&#xA;&#x9;&#x9;&#x9;&#34;Action&#34;: &#34;dynamodb:PutItem&#34;,&#xA;&#x9;&#x9;&#x9;&#34;Resource&#34;: &#34;arn:aws:dynamodb:us-east-1:&amp;lt;YOUR_ACCOUNT_ID&amp;gt;:table/quickjs-table&#34;&#xA;&#x9;&#x9;}&#xA;&#x9;]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start the &lt;code&gt;lambda-server.js&lt;/code&gt; in a separate terminal&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;node lambda-server.js&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run llrt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Benchmark Methodology&lt;/h2&gt; &#xA;&lt;p&gt;Although Init Duration &lt;a href=&#34;https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtime-environment.html&#34;&gt;reported by Lambda&lt;/a&gt; is commonly used to understand cold start impact on overall request latency, this metric does not include the time needed to copy code into the Lambda sandbox.&lt;/p&gt; &#xA;&lt;p&gt;The technical definition of Init Duration (&lt;a href=&#34;https://docs.aws.amazon.com/lambda/latest/dg/nodejs-logging.html#node-logging-output&#34;&gt;source&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For the first request served, the amount of time it took the runtime to load the function and run code outside of the handler method.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Measuring round-trip request duration provides a more complete picture of user facing cold-start latency.&lt;/p&gt; &#xA;&lt;p&gt;Lambda invocation results (Œª-labeled row) report the sum total of Init Duration + Function Duration.&lt;/p&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/llrt/main/CONTRIBUTING.md#security-issue-notifications&#34;&gt;CONTRIBUTING&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This library is licensed under the Apache-2.0 License. See the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/llrt/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
</feed>