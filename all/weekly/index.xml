<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-23T01:48:21Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>guillaumekln/faster-whisper</title>
    <updated>2023-07-23T01:48:21Z</updated>
    <id>tag:github.com,2023-07-23:/guillaumekln/faster-whisper</id>
    <link href="https://github.com/guillaumekln/faster-whisper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Faster Whisper transcription with CTranslate2&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/guillaumekln/faster-whisper/actions?query=workflow%3ACI&#34;&gt;&lt;img src=&#34;https://github.com/guillaumekln/faster-whisper/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/faster-whisper&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/faster-whisper.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Faster Whisper transcription with CTranslate2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;faster-whisper&lt;/strong&gt; is a reimplementation of OpenAI&#39;s Whisper model using &lt;a href=&#34;https://github.com/OpenNMT/CTranslate2/&#34;&gt;CTranslate2&lt;/a&gt;, which is a fast inference engine for Transformer models.&lt;/p&gt; &#xA;&lt;p&gt;This implementation is up to 4 times faster than &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;openai/whisper&lt;/a&gt; for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;For reference, here&#39;s the time and memory usage that are required to transcribe &lt;a href=&#34;https://www.youtube.com/watch?v=0u7tTptBo9I&#34;&gt;&lt;strong&gt;13 minutes&lt;/strong&gt;&lt;/a&gt; of audio using different implementations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;openai/whisper&lt;/a&gt;@&lt;a href=&#34;https://github.com/openai/whisper/commit/6dea21fd7f7253bfe450f1e2512a0fe47ee2d258&#34;&gt;6dea21fd&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;whisper.cpp&lt;/a&gt;@&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/commit/3b010f9bed9a6068609e9faf52383aea792b0362&#34;&gt;3b010f9&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/guillaumekln/faster-whisper&#34;&gt;faster-whisper&lt;/a&gt;@&lt;a href=&#34;https://github.com/guillaumekln/faster-whisper/commit/cce6b53e4554f71172dad188c45f10fb100f6e3e&#34;&gt;cce6b53e&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Large-v2 model on GPU&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Implementation&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;Beam size&lt;/th&gt; &#xA;   &lt;th&gt;Time&lt;/th&gt; &#xA;   &lt;th&gt;Max. GPU memory&lt;/th&gt; &#xA;   &lt;th&gt;Max. CPU memory&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openai/whisper&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;4m30s&lt;/td&gt; &#xA;   &lt;td&gt;11325MB&lt;/td&gt; &#xA;   &lt;td&gt;9439MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;faster-whisper&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;54s&lt;/td&gt; &#xA;   &lt;td&gt;4755MB&lt;/td&gt; &#xA;   &lt;td&gt;3244MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;faster-whisper&lt;/td&gt; &#xA;   &lt;td&gt;int8&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;59s&lt;/td&gt; &#xA;   &lt;td&gt;3091MB&lt;/td&gt; &#xA;   &lt;td&gt;3117MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Executed with CUDA 11.7.1 on a NVIDIA Tesla V100S.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Small model on CPU&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Implementation&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;Beam size&lt;/th&gt; &#xA;   &lt;th&gt;Time&lt;/th&gt; &#xA;   &lt;th&gt;Max. memory&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openai/whisper&lt;/td&gt; &#xA;   &lt;td&gt;fp32&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;10m31s&lt;/td&gt; &#xA;   &lt;td&gt;3101MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;whisper.cpp&lt;/td&gt; &#xA;   &lt;td&gt;fp32&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;17m42s&lt;/td&gt; &#xA;   &lt;td&gt;1581MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;whisper.cpp&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;12m39s&lt;/td&gt; &#xA;   &lt;td&gt;873MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;faster-whisper&lt;/td&gt; &#xA;   &lt;td&gt;fp32&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;2m44s&lt;/td&gt; &#xA;   &lt;td&gt;1675MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;faster-whisper&lt;/td&gt; &#xA;   &lt;td&gt;int8&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;2m04s&lt;/td&gt; &#xA;   &lt;td&gt;995MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Executed with 8 threads on a Intel(R) Xeon(R) Gold 6226R.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The module can be installed from &lt;a href=&#34;https://pypi.org/project/faster-whisper/&#34;&gt;PyPI&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install faster-whisper&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Other installation methods:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install the master branch:&#xA;pip install --force-reinstall &#34;faster-whisper @ https://github.com/guillaumekln/faster-whisper/archive/refs/heads/master.tar.gz&#34;&#xA;&#xA;# Install a specific commit:&#xA;pip install --force-reinstall &#34;faster-whisper @ https://github.com/guillaumekln/faster-whisper/archive/a4f1cc8f11433e454c3934442b5e1a4ed5e865c3.tar.gz&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;GPU support&lt;/h3&gt; &#xA;&lt;p&gt;GPU execution requires the NVIDIA libraries cuBLAS 11.x and cuDNN 8.x to be installed on the system. Please refer to the &lt;a href=&#34;https://opennmt.net/CTranslate2/installation.html&#34;&gt;CTranslate2 documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from faster_whisper import WhisperModel&#xA;&#xA;model_size = &#34;large-v2&#34;&#xA;&#xA;# Run on GPU with FP16&#xA;model = WhisperModel(model_size, device=&#34;cuda&#34;, compute_type=&#34;float16&#34;)&#xA;&#xA;# or run on GPU with INT8&#xA;# model = WhisperModel(model_size, device=&#34;cuda&#34;, compute_type=&#34;int8_float16&#34;)&#xA;# or run on CPU with INT8&#xA;# model = WhisperModel(model_size, device=&#34;cpu&#34;, compute_type=&#34;int8&#34;)&#xA;&#xA;segments, info = model.transcribe(&#34;audio.mp3&#34;, beam_size=5)&#xA;&#xA;print(&#34;Detected language &#39;%s&#39; with probability %f&#34; % (info.language, info.language_probability))&#xA;&#xA;for segment in segments:&#xA;    print(&#34;[%.2fs -&amp;gt; %.2fs] %s&#34; % (segment.start, segment.end, segment.text))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; &lt;code&gt;segments&lt;/code&gt; is a &lt;em&gt;generator&lt;/em&gt; so the transcription only starts when you iterate over it. The transcription can be run to completion by gathering the segments in a list or a &lt;code&gt;for&lt;/code&gt; loop:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;segments, _ = model.transcribe(&#34;audio.mp3&#34;)&#xA;segments = list(segments)  # The transcription will actually run here.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Word-level timestamps&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;segments, _ = model.transcribe(&#34;audio.mp3&#34;, word_timestamps=True)&#xA;&#xA;for segment in segments:&#xA;    for word in segment.words:&#xA;        print(&#34;[%.2fs -&amp;gt; %.2fs] %s&#34; % (word.start, word.end, word.word))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;VAD filter&lt;/h3&gt; &#xA;&lt;p&gt;The library integrates the &lt;a href=&#34;https://github.com/snakers4/silero-vad&#34;&gt;Silero VAD&lt;/a&gt; model to filter out parts of the audio without speech:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;segments, _ = model.transcribe(&#34;audio.mp3&#34;, vad_filter=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default behavior is conservative and only removes silence longer than 2 seconds. See the available VAD parameters and default values in the &lt;a href=&#34;https://github.com/guillaumekln/faster-whisper/raw/master/faster_whisper/vad.py&#34;&gt;source code&lt;/a&gt;. They can be customized with the dictionary argument &lt;code&gt;vad_parameters&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;segments, _ = model.transcribe(&#xA;    &#34;audio.mp3&#34;,&#xA;    vad_filter=True,&#xA;    vad_parameters=dict(min_silence_duration_ms=500),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Logging&lt;/h3&gt; &#xA;&lt;p&gt;The library logging level can be configured like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging&#xA;&#xA;logging.basicConfig()&#xA;logging.getLogger(&#34;faster_whisper&#34;).setLevel(logging.DEBUG)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Going further&lt;/h3&gt; &#xA;&lt;p&gt;See more model and transcription options in the &lt;a href=&#34;https://github.com/guillaumekln/faster-whisper/raw/master/faster_whisper/transcribe.py&#34;&gt;&lt;code&gt;WhisperModel&lt;/code&gt;&lt;/a&gt; class implementation.&lt;/p&gt; &#xA;&lt;h2&gt;Community integrations&lt;/h2&gt; &#xA;&lt;p&gt;Here is a non exhaustive list of open-source projects using faster-whisper. Feel free to add your project to the list!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Softcatala/whisper-ctranslate2&#34;&gt;whisper-ctranslate2&lt;/a&gt; is a command line client based on faster-whisper and compatible with the original client from openai/whisper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MahmoudAshraf97/whisper-diarization&#34;&gt;whisper-diarize&lt;/a&gt; is a speaker diarization tool that is based on faster-whisper and NVIDIA NeMo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Purfview/whisper-standalone-win&#34;&gt;whisper-standalone-win&lt;/a&gt; contains the portable ready to run binaries of faster-whisper for Windows.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hedrergudene/asr-sd-pipeline&#34;&gt;asr-sd-pipeline&lt;/a&gt; provides a scalable, modular, end to end multi-speaker speech to text solution implemented using AzureML pipelines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zh-plus/Open-Lyrics&#34;&gt;Open-Lyrics&lt;/a&gt; is a Python library that transcribes voice files using faster-whisper, and translates/polishes the resulting text into &lt;code&gt;.lrc&lt;/code&gt; files in the desired language using OpenAI-GPT.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model conversion&lt;/h2&gt; &#xA;&lt;p&gt;When loading a model from its size such as &lt;code&gt;WhisperModel(&#34;large-v2&#34;)&lt;/code&gt;, the correspondig CTranslate2 model is automatically downloaded from the &lt;a href=&#34;https://huggingface.co/guillaumekln&#34;&gt;Hugging Face Hub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a script to convert any Whisper models compatible with the Transformers library. They could be the original OpenAI models or user fine-tuned models.&lt;/p&gt; &#xA;&lt;p&gt;For example the command below converts the &lt;a href=&#34;https://huggingface.co/openai/whisper-large-v2&#34;&gt;original &#34;large-v2&#34; Whisper model&lt;/a&gt; and saves the weights in FP16:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install transformers[torch]&amp;gt;=4.23&#xA;&#xA;ct2-transformers-converter --model openai/whisper-large-v2 --output_dir whisper-large-v2-ct2 \&#xA;    --copy_files tokenizer.json --quantization float16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The option &lt;code&gt;--model&lt;/code&gt; accepts a model name on the Hub or a path to a model directory.&lt;/li&gt; &#xA; &lt;li&gt;If the option &lt;code&gt;--copy_files tokenizer.json&lt;/code&gt; is not used, the tokenizer configuration is automatically downloaded when the model is loaded later.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Models can also be converted from the code. See the &lt;a href=&#34;https://opennmt.net/CTranslate2/python/ctranslate2.converters.TransformersConverter.html&#34;&gt;conversion API&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Load a converted model&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Directly load the model from a local directory:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = faster_whisper.WhisperModel(&#34;whisper-large-v2-ct2&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/model_sharing#upload-with-the-web-interface&#34;&gt;Upload your model to the Hugging Face Hub&lt;/a&gt; and load it from its name:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = faster_whisper.WhisperModel(&#34;username/whisper-large-v2-ct2&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Comparing performance against other implementations&lt;/h2&gt; &#xA;&lt;p&gt;If you are comparing the performance against other Whisper implementations, you should make sure to run the comparison with similar settings. In particular:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Verify that the same transcription options are used, especially the same beam size. For example in openai/whisper, &lt;code&gt;model.transcribe&lt;/code&gt; uses a default beam size of 1 but here we use a default beam size of 5.&lt;/li&gt; &#xA; &lt;li&gt;When running on CPU, make sure to set the same number of threads. Many frameworks will read the environment variable &lt;code&gt;OMP_NUM_THREADS&lt;/code&gt;, which can be set when running your script:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OMP_NUM_THREADS=4 python3 my_script.py&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>minimaxir/simpleaichat</title>
    <updated>2023-07-23T01:48:21Z</updated>
    <id>tag:github.com,2023-07-23:/minimaxir/simpleaichat</id>
    <link href="https://github.com/minimaxir/simpleaichat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python package for easily interfacing with chat apps, with robust features and minimal code complexity.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;simpleaichat&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;from simpleaichat import AIChat&#xA;&#xA;ai = AIChat(system=&#34;Write a fancy GitHub README based on the user-provided project name.&#34;)&#xA;ai(&#34;simpleaichat&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;simpleaichat is a Python package for easily interfacing with chat apps like ChatGPT and GPT-4 with robust features and minimal code complexity. This tool has many features optimized for working with ChatGPT as fast and as cheap as possible, but still much more capable of modern AI tricks than most implementations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create and run chats with only a few lines of code!&lt;/li&gt; &#xA; &lt;li&gt;Optimized workflows which minimize the amount of tokens used, reducing costs and latency.&lt;/li&gt; &#xA; &lt;li&gt;Run multiple independent chats at once.&lt;/li&gt; &#xA; &lt;li&gt;Minimal codebase: no code dives to figure out what&#39;s going on under the hood needed!&lt;/li&gt; &#xA; &lt;li&gt;Chat streaming responses and the ability to use tools.&lt;/li&gt; &#xA; &lt;li&gt;Async support, including for streaming and tools.&lt;/li&gt; &#xA; &lt;li&gt;Ability to create more complex yet clear workflows if needed, such as Agents. (Demo soon!)&lt;/li&gt; &#xA; &lt;li&gt;Coming soon: more chat model support (PaLM, Claude)!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here&#39;s some fun, hackable examples on how simpleaichat works:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creating a &lt;a href=&#34;https://raw.githubusercontent.com/minimaxir/simpleaichat/main/examples/notebooks/simpleaichat_coding.ipynb&#34;&gt;Python coding assistant&lt;/a&gt; without any unnecessary accompanying output, allowing 5x faster generation at 1/3rd the cost. (&lt;a href=&#34;https://colab.research.google.com/github/minimaxir/simpleaichat/blob/main/examples/notebooks/simpleaichat_coding.ipynb&#34;&gt;Colab&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Allowing simpleaichat to &lt;a href=&#34;https://raw.githubusercontent.com/minimaxir/simpleaichat/main/examples/notebooks/chatgpt_inline_tips.ipynb&#34;&gt;provide inline tips&lt;/a&gt; following ChatGPT usage guidelines. (&lt;a href=&#34;https://colab.research.google.com/github/minimaxir/simpleaichat/blob/main/examples/notebooks/chatgpt_inline_tips.ipynb&#34;&gt;Colab&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Async interface for &lt;a href=&#34;https://raw.githubusercontent.com/minimaxir/simpleaichat/main/examples/notebooks/simpleaichat_async.ipynb&#34;&gt;conducting many chats&lt;/a&gt; in the time it takes to receive one AI message. (&lt;a href=&#34;https://colab.research.google.com/github/minimaxir/simpleaichat/blob/main/examples/notebooks/simpleaichat_async.ipynb&#34;&gt;Colab&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Create your own Tabletop RPG (TTRPG) setting and campaign by using &lt;a href=&#34;https://raw.githubusercontent.com/minimaxir/simpleaichat/main/examples/notebooks/schema_ttrpg.ipynb&#34;&gt;advanced structured data models&lt;/a&gt;. (&lt;a href=&#34;https://colab.research.google.com/github/minimaxir/simpleaichat/blob/main/examples/notebooks/schema_ttrpg.ipynb&#34;&gt;Colab&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;simpleaichat can be installed &lt;a href=&#34;https://pypi.org/project/simpleaichat/&#34;&gt;from PyPI&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip3 install simpleaichat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick, Fun Demo&lt;/h2&gt; &#xA;&lt;p&gt;You can demo chat-apps very quickly with simpleaichat! First, you will need to get an OpenAI API key, and then with one line of code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;from simpleaichat import AIChat&#xA;&#xA;AIChat(api_key=&#34;sk-...&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And with that, you&#39;ll be thrust directly into an interactive chat!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/minimaxir/simpleaichat/main/docs/helloworld.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This AI chat will mimic the behavior of OpenAI&#39;s webapp, but on your local computer!&lt;/p&gt; &#xA;&lt;p&gt;You can also pass the API key by storing it in an &lt;code&gt;.env&lt;/code&gt; file with a &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; field in the working directory (recommended), or by setting the environment variable of &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; directly to the API key.&lt;/p&gt; &#xA;&lt;p&gt;But what about creating your own custom conversations? That&#39;s where things get fun. Just input whatever person, place or thing, fictional or nonfictional, that you want to chat with!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;AIChat(&#34;GLaDOS&#34;)  # assuming API key loaded via methods above&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/minimaxir/simpleaichat/main/docs/glados.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;But that&#39;s not all! You can customize exactly how they behave too with additional commands!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;AIChat(&#34;GLaDOS&#34;, &#34;Speak in the style of a Seinfeld monologue&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/minimaxir/simpleaichat/main/docs/gladoseinfeld.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;AIChat(&#34;Ronald McDonald&#34;, &#34;Speak using only emoji&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/minimaxir/simpleaichat/main/docs/clownemoji.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Need some socialization immediately? Once simpleaichat is installed, you can also start these chats directly from the command line!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;simpleaichat&#xA;simpleaichat &#34;GlaDOS&#34;&#xA;simpleaichat &#34;GLaDOS&#34; &#34;Speak in the style of a Seinfeld monologue&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building AI-based Apps&lt;/h2&gt; &#xA;&lt;p&gt;The trick with working with new chat-based apps that wasn&#39;t readily available with earlier iterations of GPT-3 is the addition of the system prompt: a different class of prompt that guides the AI behavior throughout the entire conversation. In fact, the chat demos above are actually using &lt;a href=&#34;https://github.com/minimaxir/simpleaichat/raw/main/PROMPTS.md#interactive-chat&#34;&gt;system prompt tricks&lt;/a&gt; behind the scenes! OpenAI has also released an official guide for &lt;a href=&#34;https://platform.openai.com/docs/guides/gpt-best-practices&#34;&gt;system prompt best practices&lt;/a&gt; to building AI apps.&lt;/p&gt; &#xA;&lt;p&gt;For developers, you can instantiate a programmatic instance of &lt;code&gt;AIChat&lt;/code&gt; by explicitly specifying a system prompt, or by disabling the console.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;ai = AIChat(system=&#34;You are a helpful assistant.&#34;)&#xA;ai = AIChat(console=False)  # same as above&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also pass in a &lt;code&gt;model&lt;/code&gt; parameter, such as &lt;code&gt;model=&#34;gpt-4&#34;&lt;/code&gt; if you have access to GPT-4, or &lt;code&gt;model=&#34;gpt-3.5-turbo-16k&#34;&lt;/code&gt; for a larger-context-window ChatGPT.&lt;/p&gt; &#xA;&lt;p&gt;You can then feed the new &lt;code&gt;ai&lt;/code&gt; class with user input, and it will return and save the response from ChatGPT:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;response = ai(&#34;What is the capital of California?&#34;)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;The capital of California is Sacramento.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can stream responses by token with a generator if the text generation itself is too slow:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;for chunk in ai.stream(&#34;What is the capital of California?&#34;, params={&#34;max_tokens&#34;: 5}):&#xA;    response_td = chunk[&#34;response&#34;]  # dict contains &#34;delta&#34; for the new token and &#34;response&#34;&#xA;    print(response_td)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;The&#xA;The capital&#xA;The capital of&#xA;The capital of California&#xA;The capital of California is&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Further calls to the &lt;code&gt;ai&lt;/code&gt; object will continue the chat, automatically incorporating previous information from the conversation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;response = ai(&#34;When was it founded?&#34;)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Sacramento was founded on February 27, 1850.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also save chat sessions (as CSV or JSON) and load them later. The API key is not saved so you will have to provide that when loading.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;ai.save_session()  # CSV, will only save messages&#xA;ai.save_session(format=&#34;json&#34;, minify=True)  # JSON&#xA;&#xA;ai.load_session(&#34;my.csv&#34;)&#xA;ai.load_session(&#34;my.json&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Functions&lt;/h3&gt; &#xA;&lt;p&gt;A large number of popular venture-capital-funded ChatGPT apps don&#39;t actually use the &#34;chat&#34; part of the model. Instead, they just use the system prompt/first user prompt as a form of natural language programming. You can emulate this behavior by passing a new system prompt when generating text, and not saving the resulting messages.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;AIChat&lt;/code&gt; class is a manager of chat &lt;em&gt;sessions&lt;/em&gt;, which means you can have multiple independent chats or functions happening! The examples above use a default session, but you can create new ones by specifying a &lt;code&gt;id&lt;/code&gt; when calling &lt;code&gt;ai&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;json = &#39;{&#34;title&#34;: &#34;An array of integers.&#34;, &#34;array&#34;: [-1, 0, 1]}&#39;&#xA;functions = [&#xA;             &#34;Format the user-provided JSON as YAML.&#34;,&#xA;             &#34;Write a limerick based on the user-provided JSON.&#34;,&#xA;             &#34;Translate the user-provided JSON from English to French.&#34;&#xA;            ]&#xA;params = {&#34;temperature&#34;: 0.0, &#34;max_tokens&#34;: 100}  # a temperature of 0.0 is deterministic&#xA;&#xA;# We namespace the function by `id` so it doesn&#39;t affect other chats.&#xA;# Settings set during session creation will apply to all generations from the session,&#xA;# but you can change them per-generation, as is the case with the `system` prompt here.&#xA;ai = AIChat(id=&#34;function&#34;, params=params, save_messages=False)&#xA;for function in functions:&#xA;    output = ai(json, id=&#34;function&#34;, system=function)&#xA;    print(output)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;title: &#34;An array of integers.&#34;&#xA;array:&#xA;  - -1&#xA;  - 0&#xA;  - 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;An array of integers so neat,&#xA;With values that can&#39;t be beat,&#xA;From negative to positive one,&#xA;It&#39;s a range that&#39;s quite fun,&#xA;This JSON is really quite sweet!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;{&#34;titre&#34;: &#34;Un tableau d&#39;entiers.&#34;, &#34;tableau&#34;: [-1, 0, 1]}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Newer versions of ChatGPT also support &#34;&lt;a href=&#34;https://platform.openai.com/docs/guides/gpt/function-calling&#34;&gt;function calling&lt;/a&gt;&#34;, but the real benefit of that feature is the ability for ChatGPT to support structured input and/or output, which now opens up a wide variety of applications! simpleaichat streamlines the workflow to allow you to just pass an &lt;code&gt;input_schema&lt;/code&gt; and/or an &lt;code&gt;output_schema&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can construct a schema using a &lt;a href=&#34;https://docs.pydantic.dev/latest/&#34;&gt;pydantic&lt;/a&gt; BaseModel.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;from pydantic import BaseModel, Field&#xA;&#xA;ai = AIChat(&#xA;    console=False,&#xA;    save_messages=False,  # with schema I/O, messages are never saved&#xA;    model=&#34;gpt-3.5-turbo-0613&#34;,&#xA;    params={&#34;temperature&#34;: 0.0},&#xA;)&#xA;&#xA;class get_event_metadata(BaseModel):&#xA;    &#34;&#34;&#34;Event information&#34;&#34;&#34;&#xA;&#xA;    description: str = Field(description=&#34;Description of event&#34;)&#xA;    city: str = Field(description=&#34;City where event occured&#34;)&#xA;    year: int = Field(description=&#34;Year when event occured&#34;)&#xA;    month: str = Field(description=&#34;Month when event occured&#34;)&#xA;&#xA;# returns a dict, with keys ordered as in the schema&#xA;ai(&#34;First iPhone announcement&#34;, output_schema=get_event_metadata)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;{&#39;description&#39;: &#39;The first iPhone was announced by Apple Inc.&#39;,&#xA; &#39;city&#39;: &#39;San Francisco&#39;,&#xA; &#39;year&#39;: 2007,&#xA; &#39;month&#39;: &#39;January&#39;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/minimaxir/simpleaichat/main/examples/notebooks/schema_ttrpg.ipynb&#34;&gt;TTRPG Generator Notebook&lt;/a&gt; for a more elaborate demonstration of schema capabilities.&lt;/p&gt; &#xA;&lt;h3&gt;Tools&lt;/h3&gt; &#xA;&lt;p&gt;One of the most recent aspects of interacting with ChatGPT is the ability for the model to use &#34;tools.&#34; As popularized by &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;, tools allow the model to decide when to use custom functions, which can extend beyond just the chat AI itself, for example retrieving recent information from the internet not present in the chat AI&#39;s training data. This workflow is analogous to ChatGPT Plugins.&lt;/p&gt; &#xA;&lt;p&gt;Parsing the model output to invoke tools typically requires a number of shennanigans, but simpleaichat uses &lt;a href=&#34;https://github.com/minimaxir/simpleaichat/raw/main/PROMPTS.md#tools&#34;&gt;a neat trick&lt;/a&gt; to make it fast and reliable! Additionally, the specified tools return a &lt;code&gt;context&lt;/code&gt; for ChatGPT to draw from for its final response, and tools you specify can return a dictionary which you can also populate with arbitrary metadata for debugging and postprocessing. Each generation returns a dictionary with the &lt;code&gt;response&lt;/code&gt; and the &lt;code&gt;tool&lt;/code&gt; function used, which can be used to set up workflows akin to &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;-style Agents, e.g. recursively feed input to the model until it determines it does not need to use any more tools.&lt;/p&gt; &#xA;&lt;p&gt;You will need to specify functions with docstrings which provide hints for the AI to select them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;from simpleaichat.utils import wikipedia_search, wikipedia_search_lookup&#xA;&#xA;# This uses the Wikipedia Search API.&#xA;# Results from it are nondeterministic, your mileage will vary.&#xA;def search(query):&#xA;    &#34;&#34;&#34;Search the internet.&#34;&#34;&#34;&#xA;    wiki_matches = wikipedia_search(query, n=3)&#xA;    return {&#34;context&#34;: &#34;, &#34;.join(wiki_matches), &#34;titles&#34;: wiki_matches}&#xA;&#xA;def lookup(query):&#xA;    &#34;&#34;&#34;Lookup more information about a topic.&#34;&#34;&#34;&#xA;    page = wikipedia_search_lookup(query, sentences=3)&#xA;    return page&#xA;&#xA;params = {&#34;temperature&#34;: 0.0, &#34;max_tokens&#34;: 100}&#xA;ai = AIChat(params=params, console=False)&#xA;&#xA;ai(&#34;San Francisco tourist attractions&#34;, tools=[search, lookup])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;{&#39;context&#39;: &#34;Fisherman&#39;s Wharf, San Francisco, Tourist attractions in the United States, Lombard Street (San Francisco)&#34;,&#xA; &#39;titles&#39;: [&#34;Fisherman&#39;s Wharf, San Francisco&#34;,&#xA;  &#39;Tourist attractions in the United States&#39;,&#xA;  &#39;Lombard Street (San Francisco)&#39;],&#xA; &#39;tool&#39;: &#39;search&#39;,&#xA; &#39;response&#39;: &#34;There are many popular tourist attractions in San Francisco, including Fisherman&#39;s Wharf and Lombard Street. Fisherman&#39;s Wharf is a bustling waterfront area known for its seafood restaurants, souvenir shops, and sea lion sightings. Lombard Street, on the other hand, is a famous winding street with eight hairpin turns that attract visitors from all over the world. Both of these attractions are must-sees for anyone visiting San Francisco.&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;ai(&#34;Lombard Street?&#34;, tools=[search, lookup])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#39;context&#39;: &#39;Lombard Street is an east‚Äìwest street in San Francisco, California that is famous for a steep, one-block section with eight hairpin turns. Stretching from The Presidio east to The Embarcadero (with a gap on Telegraph Hill), most of the street\&#39;s western segment is a major thoroughfare designated as part of U.S. Route 101. The famous one-block section, claimed to be &#34;the crookedest street in the world&#34;, is located along the eastern segment in the Russian Hill neighborhood.&#39;,&#xA; &#39;tool&#39;: &#39;lookup&#39;,&#xA; &#39;response&#39;: &#39;Lombard Street is a famous street in San Francisco, California known for its steep, one-block section with eight hairpin turns. It stretches from The Presidio to The Embarcadero, with a gap on Telegraph Hill. The western segment of the street is a major thoroughfare designated as part of U.S. Route 101, while the famous one-block section, claimed to be &#34;the crookedest street in the world&#34;, is located along the eastern segment in the Russian Hill&#39;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py3&#34;&gt;ai(&#34;Thanks for your help!&#34;, tools=[search, lookup])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;{&#39;response&#39;: &#34;You&#39;re welcome! If you have any more questions or need further assistance, feel free to ask.&#34;,&#xA; &#39;tool&#39;: None}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Miscellaneous Notes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Like &lt;a href=&#34;https://github.com/minimaxir/gpt-2-simple&#34;&gt;gpt-2-simple&lt;/a&gt; before it, the primary motivation behind releasing simpleaichat is to both democratize access to ChatGPT even more and also offer more transparency for non-engineers into how Chat AI-based apps work under the hood given the disproportionate amount of media misinformation about their capabilities. This is inspired by real-world experience from &lt;a href=&#34;https://tech.buzzfeed.com/the-right-tools-for-the-job-c05de96e949e&#34;&gt;my work with BuzzFeed&lt;/a&gt; in the domain, where after spending a long time working with the popular &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;, a more-simple implementation was both much easier to maintain and resulted in much better generations. I began focusing development on simpleaichat after reading a &lt;a href=&#34;https://news.ycombinator.com/item?id=35820931&#34;&gt;Hacker News thread&lt;/a&gt; filled with many similar complaints, indicating value for an easier-to-use interface for modern AI tricks. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;simpleaichat very intentionally avoids coupling features with common use cases where possible (e.g. Tools) in order to avoid software lock-in due to the difficulty implementing anything not explicitly mentioned in the project&#39;s documentation. The philosophy behind simpleaichat is to provide good demos, and let the user&#39;s creativity and business needs take priority instead of having to fit a round peg into a square hole like with LangChain.&lt;/li&gt; &#xA;   &lt;li&gt;simpleaichat makes it easier to interface with Chat AIs, but it does not attempt to solve common technical and ethical problems inherent to large language models trained on the internet, including prompt injection and unintended plagiarism. The user should exercise good judgment when implementing simpleaichat. Use cases of simpleaichat which go against OpenAI&#39;s &lt;a href=&#34;https://openai.com/policies/usage-policies&#34;&gt;usage policies&lt;/a&gt; (including jailbreaking) will not be endorsed.&lt;/li&gt; &#xA;   &lt;li&gt;simpleaichat intentionally does not use the &#34;Agent&#34; logical metaphor for tool workflows because it&#39;s become an AI hype buzzword heavily divorced from its origins. If needed be, you can emulate the Agent workflow with a &lt;code&gt;while&lt;/code&gt; loop without much additional code, plus with the additional benefit of much more flexibility such as debugging.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The session manager implements some sensible security defaults, such as using UUIDs as session ids by default, storing authentication information in a way to minimize unintentional leakage, and type enforcement via Pydantic. Your end-user application should still be aware of potential security issues, however.&lt;/li&gt; &#xA; &lt;li&gt;Although OpenAI&#39;s documentation says that system prompts are less effective than a user prompt constructed in a similar manner, in my experience it still does perform better for maintaining rules/a persona.&lt;/li&gt; &#xA; &lt;li&gt;Many examples of popular prompts use more conversational prompts, while the example prompts here use more consise and imperative prompts. This aspect of prompt engineering is still evolving, but in my experience commands do better with ChatGPT and with greater token efficieny. That&#39;s also why simpleaichat allows users to specify system prompts (and explicitly highlights what the default use) instead of relying on historical best practices.&lt;/li&gt; &#xA; &lt;li&gt;Token counts for async is not supported as OpenAI doesn&#39;t return token counts when streaming responses. In general, there may be some desync in token counts and usage for various use cases; I&#39;m working on categorizing them.&lt;/li&gt; &#xA; &lt;li&gt;Outside of the explicit examples, none of this README uses AI-generated text. The introduction code example is just a joke, but it was too good of a real-world use case!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PaLM Chat (Bard) and Anthropic Claude support&lt;/li&gt; &#xA; &lt;li&gt;More fun/feature-filled CLI chat app based on Textual&lt;/li&gt; &#xA; &lt;li&gt;Simple example of using simpleaichat in a webapp&lt;/li&gt; &#xA; &lt;li&gt;Simple of example of using simpleaichat in a stateless manner (e.g. AWS Lambda functions)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Maintainer/Creator&lt;/h2&gt; &#xA;&lt;p&gt;Max Woolf (&lt;a href=&#34;https://minimaxir.com&#34;&gt;@minimaxir&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Max&#39;s open-source projects are supported by his &lt;a href=&#34;https://www.patreon.com/minimaxir&#34;&gt;Patreon&lt;/a&gt; and &lt;a href=&#34;https://github.com/sponsors/minimaxir&#34;&gt;GitHub Sponsors&lt;/a&gt;. If you found this project helpful, any monetary contributions to the Patreon are appreciated and will be put to good creative use.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LazyVim/LazyVim</title>
    <updated>2023-07-23T01:48:21Z</updated>
    <id>tag:github.com,2023-07-23:/LazyVim/LazyVim</id>
    <link href="https://github.com/LazyVim/LazyVim" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Neovim config for the lazy&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/292349/213446185-2db63fd5-8c84-459c-9f04-e286382d6e80.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;https://lazyvim.github.io/installation&#34;&gt;Install&lt;/a&gt; ¬∑ &lt;a href=&#34;https://lazyvim.github.io/configuration&#34;&gt;Configure&lt;/a&gt; ¬∑ &lt;a href=&#34;https://lazyvim.github.io&#34;&gt;Docs&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;p&gt; &lt;a href=&#34;https://github.com/LazyVim/LazyVim/releases/latest&#34;&gt; &lt;img alt=&#34;Latest release&#34; src=&#34;https://img.shields.io/github/v/release/LazyVim/LazyVim?style=for-the-badge&amp;amp;logo=starship&amp;amp;color=C9CBFF&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41&amp;amp;include_prerelease&amp;amp;sort=semver&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/LazyVim/LazyVim/pulse&#34;&gt; &lt;img alt=&#34;Last commit&#34; src=&#34;https://img.shields.io/github/last-commit/LazyVim/LazyVim?style=for-the-badge&amp;amp;logo=starship&amp;amp;color=8bd5ca&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/LazyVim/LazyVim/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/github/license/LazyVim/LazyVim?style=for-the-badge&amp;amp;logo=starship&amp;amp;color=ee999f&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/LazyVim/LazyVim/stargazers&#34;&gt; &lt;img alt=&#34;Stars&#34; src=&#34;https://img.shields.io/github/stars/LazyVim/LazyVim?style=for-the-badge&amp;amp;logo=starship&amp;amp;color=c69ff5&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/LazyVim/LazyVim/issues&#34;&gt; &lt;img alt=&#34;Issues&#34; src=&#34;https://img.shields.io/github/issues/LazyVim/LazyVim?style=for-the-badge&amp;amp;logo=bilibili&amp;amp;color=F5E0DC&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/LazyVim/LazyVim&#34;&gt; &lt;img alt=&#34;Repo Size&#34; src=&#34;https://img.shields.io/github/repo-size/LazyVim/LazyVim?color=%23DDB6F2&amp;amp;label=SIZE&amp;amp;logo=codesandbox&amp;amp;style=for-the-badge&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=folke&#34;&gt; &lt;img alt=&#34;follow on Twitter&#34; src=&#34;https://img.shields.io/twitter/follow/folke?style=for-the-badge&amp;amp;logo=twitter&amp;amp;color=8aadf3&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;/div&gt; &#xA;&lt;p&gt;LazyVim is a Neovim setup powered by &lt;a href=&#34;https://github.com/folke/lazy.nvim&#34;&gt;üí§ lazy.nvim&lt;/a&gt; to make it easy to customize and extend your config. Rather than having to choose between starting from scratch or using a pre-made distro, LazyVim offers the best of both worlds - the flexibility to tweak your config as needed, along with the convenience of a pre-configured setup.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/292349/211285846-0b7bb3bf-0462-4029-b64c-4ee1d037fc1c.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/292349/213447056-92290767-ea16-430c-8727-ce994c93e9cc.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚ú® Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üî• Transform your Neovim into a full-fledged IDE&lt;/li&gt; &#xA; &lt;li&gt;üí§ Easily customize and extend your config with &lt;a href=&#34;https://github.com/folke/lazy.nvim&#34;&gt;lazy.nvim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöÄ Blazingly fast&lt;/li&gt; &#xA; &lt;li&gt;üßπ Sane default settings for options, autocmds, and keymaps&lt;/li&gt; &#xA; &lt;li&gt;üì¶ Comes with a wealth of plugins pre-configured and ready to use&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚ö°Ô∏è Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Neovim &amp;gt;= &lt;strong&gt;0.8.0&lt;/strong&gt; (needs to be built with &lt;strong&gt;LuaJIT&lt;/strong&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Git &amp;gt;= &lt;strong&gt;2.19.0&lt;/strong&gt; (for partial clones support)&lt;/li&gt; &#xA; &lt;li&gt;a &lt;a href=&#34;https://www.nerdfonts.com/&#34;&gt;Nerd Font&lt;/a&gt; &lt;strong&gt;&lt;em&gt;(optional)&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;a &lt;strong&gt;C&lt;/strong&gt; compiler for &lt;code&gt;nvim-treesitter&lt;/code&gt;. See &lt;a href=&#34;https://github.com/nvim-treesitter/nvim-treesitter#requirements&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;You can find a starter template for &lt;strong&gt;LazyVim&lt;/strong&gt; &lt;a href=&#34;https://github.com/LazyVim/starter&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Try it with Docker&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker run -w /root -it --rm alpine:edge sh -uelic &#39;&#xA;  apk add git lazygit neovim ripgrep alpine-sdk --update&#xA;  git clone https://github.com/LazyVim/starter ~/.config/nvim&#xA;  cd ~/.config/nvim&#xA;  nvim&#xA;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Install the &lt;a href=&#34;https://github.com/LazyVim/starter&#34;&gt;LazyVim Starter&lt;/a&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Make a backup of your current Neovim files:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mv ~/.config/nvim ~/.config/nvim.bak&#xA;mv ~/.local/share/nvim ~/.local/share/nvim.bak&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Clone the starter&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/LazyVim/starter ~/.config/nvim&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Remove the &lt;code&gt;.git&lt;/code&gt; folder, so you can add it to your own repo later&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;rm -rf ~/.config/nvim/.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Start Neovim!&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;nvim&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Refer to the comments in the files on how to customize &lt;strong&gt;LazyVim&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;There&#39;s a great video created by &lt;a href=&#34;https://github.com/elijahmanor&#34;&gt;@elijahmanor&lt;/a&gt; with a walkthrough to get started.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=N93cTbtLCIM&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/N93cTbtLCIM/hqdefault.jpg&#34; alt=&#34;Watch the video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìÇ File Structure&lt;/h2&gt; &#xA;&lt;p&gt;The files under config will be automatically loaded at the appropriate time, so you don&#39;t need to require those files manually. &lt;strong&gt;LazyVim&lt;/strong&gt; comes with a set of default config files that will be loaded &lt;strong&gt;&lt;em&gt;before&lt;/em&gt;&lt;/strong&gt; your own. See &lt;a href=&#34;https://github.com/LazyVim/LazyVim/tree/main/lua/lazyvim/config&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can add your custom plugin specs under &lt;code&gt;lua/plugins/&lt;/code&gt;. All files there will be automatically loaded by &lt;a href=&#34;https://github.com/folke/lazy.nvim&#34;&gt;lazy.nvim&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&#xA;~/.config/nvim&#xA;‚îú‚îÄ‚îÄ lua&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ config&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ autocmds.lua&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ keymaps.lua&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ lazy.lua&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ options.lua&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ plugins&#xA;‚îÇ&amp;nbsp;&amp;nbsp;     ‚îú‚îÄ‚îÄ spec1.lua&#xA;‚îÇ&amp;nbsp;&amp;nbsp;     ‚îú‚îÄ‚îÄ **&#xA;‚îÇ&amp;nbsp;&amp;nbsp;     ‚îî‚îÄ‚îÄ spec2.lua&#xA;‚îî‚îÄ‚îÄ init.lua&#xA;&lt;/pre&gt; &#xA;&lt;h2&gt;‚öôÔ∏è Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://lazyvim.github.io&#34;&gt;docs&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>