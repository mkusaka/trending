<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-31T01:42:47Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>GaParmar/img2img-turbo</title>
    <updated>2024-03-31T01:42:47Z</updated>
    <id>tag:github.com,2024-03-31:/GaParmar/img2img-turbo</id>
    <link href="https://github.com/GaParmar/img2img-turbo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;One-step image-to-image with Stable Diffusion turbo: sketch2image, day2night, and more&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;img2img-turbo&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.12036&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/gparmar/img2img-turbo-sketch&#34;&gt;&lt;strong&gt;Sketch2Image Demo&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;strong&gt;Quick start:&lt;/strong&gt; &lt;a href=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/#getting-started&#34;&gt;&lt;strong&gt;Running Locally&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/#gradio-demo&#34;&gt;&lt;strong&gt;Gradio (locally hosted)&lt;/strong&gt;&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h3&gt;Cat Sketching&lt;/h3&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/cat_2x.gif&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Fish Sketching&lt;/h3&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/fish_2x.gif&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We propose a general method for adapting a single-step diffusion model, such as SD-Turbo, to new tasks and domains through adversarial learning. This enables us to leverage the internal knowledge of pre-trained diffusion models while achieving efficient inference (e.g., for 512x512 images, 0.29 seconds on A6000 and 0.11 seconds on A100).&lt;/p&gt; &#xA;&lt;p&gt;Our one-step conditional models &lt;strong&gt;CycleGAN-Turbo&lt;/strong&gt; and &lt;strong&gt;pix2pix-turbo&lt;/strong&gt; can perform various image-to-image translation tasks for both unpaired and paired settings. CycleGAN-Turbo outperforms existing GAN-based and diffusion-based methods, while pix2pix-turbo is on par with recent works such as ControlNet for Sketch2Photo and Edge2Image, but with one-step inference.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.12036&#34;&gt;One-Step Image Translation with Text-to-Image Models&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://gauravparmar.com/&#34;&gt;Gaurav Parmar&lt;/a&gt;, &lt;a href=&#34;https://taesung.me/&#34;&gt;Taesung Park&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~srinivas/&#34;&gt;Srinivasa Narasimhan&lt;/a&gt;, &lt;a href=&#34;https://github.com/junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;&lt;br&gt; CMU and Adobe, arXiv 2403.12036&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/teaser_results.jpg&#34; align=&#34;center&#34; width=&#34;1000px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Paired Translation with pix2pix-turbo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Edge to Image&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/edge_to_image_results.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- **Sketch to Image**&#xA;TODO --&gt; &#xA;&lt;h3&gt;Generating Diverse Outputs&lt;/h3&gt; &#xA;&lt;p&gt;By varying the input noise map, our method can generate diverse outputs from the same input conditioning. The output style can be controlled by changing the text prompt.&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/gen_variations.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Unpaired Translation with CycleGAN-Turbo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Day to Night&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/day2night_results.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Night to Day&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div&gt;&#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/night2day_results.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Clear to Rainy&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/clear2rainy_results.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rainy to Clear&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/rainy2clear.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Method&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Our Generator Architecture:&lt;/strong&gt; We tightly integrate three separate modules in the original latent diffusion models into a single end-to-end network with small trainable weights. This architecture allows us to translate the input image x to the output y, while retaining the input scene structure. We use LoRA adapters in each module, introduce skip connections and Zero-Convs between input and output, and retrain the first layer of the U-Net. Blue boxes indicate trainable layers. Semi-transparent layers are frozen. The same generator can be used for various GAN objectives.&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/method.jpg&#34; align=&#34;center&#34; width=&#34;900px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Environment Setup&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We provide a &lt;a href=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/environment.yml&#34;&gt;conda env file&lt;/a&gt; that contains all the required dependencies. &lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Following this, you can activate the conda environment with the command below. &lt;pre&gt;&lt;code&gt;conda activate img2img-turbo&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paired Image Translation (pix2pix-turbo)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The following command takes an image file and a prompt as inputs, extracts the canny edges, and saves the results in the directory specified.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/inference_paired.py --model_name &#34;edge_to_image&#34; \&#xA;    --input_image &#34;assets/examples/bird.png&#34; \&#xA;    --prompt &#34;a blue bird&#34; \&#xA;    --output_dir &#34;outputs&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;table&gt; &#xA;   &lt;tbody&gt;&#xA;    &lt;tr&gt;&#xA;     &lt;th&gt;Input Image&lt;/th&gt; &#xA;     &lt;th&gt;Canny Edges&lt;/th&gt; &#xA;     &lt;th&gt;Model Output&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/examples/bird.png&#34; width=&#34;200px&#34;&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/examples/bird_canny.png&#34; width=&#34;200px&#34;&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/examples/bird_canny_blue.png&#34; width=&#34;200px&#34;&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt;&#xA;  &lt;/table&gt; &lt;br&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The following command takes a sketch and a prompt as inputs, and saves the results in the directory specified.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/inference_paired.py --model_name &#34;sketch_to_image_stochastic&#34; \&#xA;--input_image &#34;assets/examples/sketch_input.png&#34; --gamma 0.4 \&#xA;--prompt &#34;ethereal fantasy concept art of an asteroid. magnificent, celestial, ethereal, painterly, epic, majestic, magical, fantasy art, cover art, dreamy&#34; \&#xA;--output_dir &#34;outputs&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;table&gt; &#xA;   &lt;tbody&gt;&#xA;    &lt;tr&gt;&#xA;     &lt;th&gt;Input&lt;/th&gt; &#xA;     &lt;th&gt;Model Output&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/examples/sketch_input.png&#34; width=&#34;400px&#34;&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/examples/sketch_output.png&#34; width=&#34;400px&#34;&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt;&#xA;  &lt;/table&gt; &lt;br&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unpaired Image Translation (CycleGAN-Turbo)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The following command takes a &lt;strong&gt;day&lt;/strong&gt; image file as input, and saves the output &lt;strong&gt;night&lt;/strong&gt; in the directory specified.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python src/inference_unpaired.py --model &#34;day_to_night&#34; \&#xA;    --input_image &#34;assets/examples/day2night_input.png&#34; --output_dir &#34;outputs&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;table&gt; &#xA;   &lt;tbody&gt;&#xA;    &lt;tr&gt;&#xA;     &lt;th&gt;Input (day)&lt;/th&gt; &#xA;     &lt;th&gt;Model Output (night)&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/examples/day2night_input.png&#34; width=&#34;400px&#34;&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/examples/day2night_output.png&#34; width=&#34;400px&#34;&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt;&#xA;  &lt;/table&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The following command takes a &lt;strong&gt;night&lt;/strong&gt; image file as input, and saves the output &lt;strong&gt;day&lt;/strong&gt; in the directory specified.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python src/inference_unpaired.py --model &#34;night_to_day&#34; \&#xA;    --input_image &#34;assets/examples/night2day_input.png&#34; --output_dir &#34;outputs&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;table&gt; &#xA;   &lt;tbody&gt;&#xA;    &lt;tr&gt;&#xA;     &lt;th&gt;Input (night)&lt;/th&gt; &#xA;     &lt;th&gt;Model Output (day)&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/examples/night2day_input.png&#34; width=&#34;400px&#34;&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/examples/night2day_output.png&#34; width=&#34;400px&#34;&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt;&#xA;  &lt;/table&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The following command takes a &lt;strong&gt;clear&lt;/strong&gt; image file as input, and saves the output &lt;strong&gt;rainy&lt;/strong&gt; in the directory specified.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python src/inference_unpaired.py --model &#34;clear_to_rainy&#34; \&#xA;    --input_image &#34;assets/examples/clear2rainy_input.png&#34; --output_dir &#34;outputs&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;table&gt; &#xA;   &lt;tbody&gt;&#xA;    &lt;tr&gt;&#xA;     &lt;th&gt;Input (clear)&lt;/th&gt; &#xA;     &lt;th&gt;Model Output (rainy)&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/examples/clear2rainy_input.png&#34; width=&#34;400px&#34;&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/examples/clear2rainy_output.png&#34; width=&#34;400px&#34;&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt;&#xA;  &lt;/table&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The following command takes a &lt;strong&gt;rainy&lt;/strong&gt; image file as input, and saves the output &lt;strong&gt;clear&lt;/strong&gt; in the directory specified.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python src/inference_unpaired.py --model &#34;rainy_to_clear&#34; \&#xA;    --input_image &#34;assets/examples/rainy2clear_input.png&#34; --output_dir &#34;outputs&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;table&gt; &#xA;   &lt;tbody&gt;&#xA;    &lt;tr&gt;&#xA;     &lt;th&gt;Input (rainy)&lt;/th&gt; &#xA;     &lt;th&gt;Model Output (clear)&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/examples/rainy2clear_input.png&#34; width=&#34;400px&#34;&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/examples/rainy2clear_output.png&#34; width=&#34;400px&#34;&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt;&#xA;  &lt;/table&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Gradio Demo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We provide a Gradio demo for the paired image translation tasks.&lt;/li&gt; &#xA; &lt;li&gt;The following command will launch the sketch to image locally using gradio. &lt;pre&gt;&lt;code&gt;gradio gradio_sketch2image.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgment&lt;/h2&gt; &#xA;&lt;p&gt;Our work uses the Stable Diffusion-Turbo as the base model with the following &lt;a href=&#34;https://huggingface.co/stabilityai/sd-turbo/blob/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>snipe-it-com/solana-sniper-bot</title>
    <updated>2024-03-31T01:42:47Z</updated>
    <id>tag:github.com,2024-03-31:/snipe-it-com/solana-sniper-bot</id>
    <link href="https://github.com/snipe-it-com/solana-sniper-bot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Solana Sniper Bot - Proof of Concept&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Solana Sniper Bot (Poc)&lt;/h1&gt; &#xA;&lt;p&gt;This code is written as proof of concept to demonstrate how we can buy new tokens immediately after the liquidity pool is open for trading.&lt;/p&gt; &#xA;&lt;p&gt;Script listens to new Raydium USDC or SOL pools and buys tokens for a fixed amount in USDC/SOL.&lt;br&gt; Depending on the speed of the RPC node, the purchase usually happens before the token is available on Raydium UI for swapping.&lt;/p&gt; &#xA;&lt;p&gt;This is provided as is, for learning purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;To run the script you need to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a new empty Solana wallet&lt;/li&gt; &#xA; &lt;li&gt;Transfer some SOL to it.&lt;/li&gt; &#xA; &lt;li&gt;Convert some SOL to USDC or WSOL. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You need USDC or WSOL depending on the configuration set below.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Configure the script by updating &lt;code&gt;.env.copy&lt;/code&gt; file (remove the .copy from the file name when done). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;PRIVATE_KEY (your wallet private key)&lt;/li&gt; &#xA;   &lt;li&gt;RPC_ENDPOINT (https RPC endpoint)&lt;/li&gt; &#xA;   &lt;li&gt;RPC_WEBSOCKET_ENDPOINT (websocket RPC endpoint)&lt;/li&gt; &#xA;   &lt;li&gt;QUOTE_MINT (which pools to snipe, USDC or WSOL)&lt;/li&gt; &#xA;   &lt;li&gt;QUOTE_AMOUNT (amount used to buy each new token)&lt;/li&gt; &#xA;   &lt;li&gt;COMMITMENT_LEVEL&lt;/li&gt; &#xA;   &lt;li&gt;USE_SNIPE_LIST (buy only tokens listed in snipe-list.txt)&lt;/li&gt; &#xA;   &lt;li&gt;SNIPE_LIST_REFRESH_INTERVAL (how often snipe list should be refreshed in milliseconds)&lt;/li&gt; &#xA;   &lt;li&gt;CHECK_IF_MINT_IS_RENOUNCED (script will buy only if mint is renounced)&lt;/li&gt; &#xA;   &lt;li&gt;MIN_POOL_SIZE (EXPERIMENTAL) (script will buy only if pool size is greater than specified amount) &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;set to 0 to disable pool size check&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Install dependencies by typing: &lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run the script by typing: &lt;code&gt;npm run buy&lt;/code&gt; in terminal&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You should see the following output:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/snipe-it-com/solana-sniper-bot/master/readme/output.png&#34; alt=&#34;output&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Snipe list&lt;/h2&gt; &#xA;&lt;p&gt;By default, script buys each token which has a new liquidity pool created and open for trading. There are scenarios when you want to buy one specific token as soon as possible during the launch event. To achieve this, you&#39;ll have to use snipe list.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change variable &lt;code&gt;USE_SNIPE_LIST&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add token mint addresses you wish to buy in &lt;code&gt;snipe-list.txt&lt;/code&gt; file &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add each address as a new line&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This will prevent script from buying everything, and instead it will buy just listed tokens. You can update the list while script is running. Script will check for new values in specified interval (&lt;code&gt;SNIPE_LIST_REFRESH_INTERVAL&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Pool must not exist before the script starts. It will buy only when new pool is open for trading. If you want to buy token that will be launched in the future, make sure that script is running before the launch.&lt;/p&gt; &#xA;&lt;h2&gt;Auto Sell&lt;/h2&gt; &#xA;&lt;p&gt;By default, auto sell is enabled. If you want to disable it, you need to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change variable &lt;code&gt;AUTO_SELL&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Update &lt;code&gt;MAX_SELL_RETRIES&lt;/code&gt; to set the maximum number of retries for selling token&lt;/li&gt; &#xA; &lt;li&gt;Update &lt;code&gt;AUTO_SELL_DELAY&lt;/code&gt; to the number of milliseconds you want to wait before selling the token &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This will sell the token after the specified delay. (+- RPC node speed)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you set AUTO_SELL_DELAY to 0, token will be sold immediately after it is bought.&lt;/p&gt; &#xA;&lt;p&gt;There is no guarantee that the token will be sold at a profit or even sold at all. The developer is not responsible for any losses incurred by using this feature.&lt;/p&gt; &#xA;&lt;h2&gt;Common issues&lt;/h2&gt; &#xA;&lt;p&gt;If you have an error which is not listed here, please create a new issue in this repository. To collect more information on an issue, please change &lt;code&gt;LOG_LEVEL&lt;/code&gt; to &lt;code&gt;debug&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Empty transaction&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you see empty transactions on SolScan most likely fix is to change commitment level to &lt;code&gt;finalized&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Unsupported RPC node&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you see following error in your log file:&lt;br&gt; &lt;code&gt;Error: 410 Gone: {&#34;jsonrpc&#34;:&#34;2.0&#34;,&#34;error&#34;:{&#34;code&#34;: 410, &#34;message&#34;:&#34;The RPC call or parameters have been disabled.&#34;}, &#34;id&#34;: &#34;986f3599-b2b7-47c4-b951-074c19842bad&#34; }&lt;/code&gt;&lt;br&gt; it means your RPC node doesn&#39;t support methods needed to execute script. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;FIX: Change your RPC node. You can use Helius or Quicknode.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;No token account&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you see following error in your log file:&lt;br&gt; &lt;code&gt;Error: No SOL token account found in wallet: &lt;/code&gt;&lt;br&gt; it means that wallet you provided doesn&#39;t have USDC/WSOL token account. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;FIX: Go to dex and swap some SOL to USDC/WSOL. For example when you swap sol to wsol you should see it in wallet as shown below:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/snipe-it-com/solana-sniper-bot/master/readme/wsol.png&#34; alt=&#34;wsol&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/xYUETCA2aP&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1201826085655023616?color=5865F2&amp;amp;logo=Discord&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Use this script at your own risk.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/retina</title>
    <updated>2024-03-31T01:42:47Z</updated>
    <id>tag:github.com,2024-03-31:/microsoft/retina</id>
    <link href="https://github.com/microsoft/retina" rel="alternate"></link>
    <summary type="html">&lt;p&gt;eBPF distributed networking observability tool for Kubernetes&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Retina&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/codespaces/new?hide_repo_select=true&amp;amp;ref=main&amp;amp;repo=746962176&#34;&gt;&lt;img src=&#34;https://github.com/codespaces/badge.svg?sanitize=true&#34; alt=&#34;Open in GitHub Codespaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://goreportcard.com/report/github.com/microsoft/retina&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/github.com/microsoft/retina&#34; alt=&#34;goreport&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/microsoft/retina.svg?sanitize=true&#34; alt=&#34;GitHub release&#34;&gt; &lt;a href=&#34;https://godoc.org/github.com/microsoft/retina&#34;&gt;&lt;img src=&#34;https://godoc.org/github.com/microsoft/retina?status.svg?sanitize=true&#34; alt=&#34;retina-publish&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue?link=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fretina%2Fblob%2Fmain%2FLICENSE&#34; alt=&#34;license&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/retina/actions/workflows/test.yaml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/retina/actions/workflows/test.yaml/badge.svg?branch=main&#34; alt=&#34;retina-test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://retina.sh/&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/retina/actions/workflows/docs.yaml/badge.svg?branch=main&#34; alt=&#34;retinash&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/retina/actions/workflows/images.yaml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/retina/actions/workflows/images.yaml/badge.svg?branch=main&#34; alt=&#34;retina-publish&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/microsoft/retina/actions/workflows/codeql.yaml/badge.svg?branch=main&#34; alt=&#34;retina-codeql-img&#34;&gt; &lt;img src=&#34;https://github.com/microsoft/retina/actions/workflows/golangci-lint.yaml/badge.svg?branch=main&#34; alt=&#34;retina-golangci-lint-img&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Retina is a cloud-agnostic, open-source &lt;strong&gt;Kubernetes network observability platform&lt;/strong&gt; that provides a &lt;strong&gt;centralized hub for monitoring application health, network health, and security&lt;/strong&gt;. It provides actionable insights to cluster network administrators, cluster security administrators, and DevOps engineers navigating DevOps, SecOps, and compliance use cases.&lt;/p&gt; &#xA;&lt;p&gt;Retina &lt;strong&gt;collects customizable telemetry&lt;/strong&gt;, which can be exported to &lt;strong&gt;multiple storage options&lt;/strong&gt; (such as Prometheus, Azure Monitor, and other vendors) and &lt;strong&gt;visualized in a variety of ways&lt;/strong&gt; (like Grafana, Azure Log Analytics, and other vendors).&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://ebpf.io/what-is-ebpf#what-is-ebpf&#34;&gt;eBPF&lt;/a&gt;-based&lt;/strong&gt; Network Observability platform for Kubernetes workloads.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;On-Demand&lt;/strong&gt; and &lt;strong&gt;Configurable&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Actionable, industry-standard &lt;strong&gt;Prometheus metrics&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Streamlined &lt;strong&gt;Packet Captures&lt;/strong&gt; for deep dives.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cloud-agnostic&lt;/strong&gt;, supporting multiple OS (like Linux, Windows, Azure Linux).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why Retina?&lt;/h2&gt; &#xA;&lt;p&gt;Retina lets you &lt;strong&gt;investigate network issues on-demand&lt;/strong&gt; and &lt;strong&gt;continuously monitor your clusters&lt;/strong&gt;. For scenarios where Retina shines, see the intro docs &lt;a href=&#34;https://retina.sh/docs/intro&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;http://retina.sh&#34;&gt;retina.sh&lt;/a&gt; for documentation and examples.&lt;/p&gt; &#xA;&lt;h2&gt;Capabilities&lt;/h2&gt; &#xA;&lt;p&gt;Retina has two major features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://retina.sh/docs/metrics/modes&#34;&gt;Metrics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://retina.sh/docs/captures&#34;&gt;Captures&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Metrics Quick Install Guide&lt;/h3&gt; &#xA;&lt;p&gt;Retina can be installed using the Helm chart from GHCR:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set the version to a specific version here or get latest version from GitHub API.&#xA;VERSION=$( curl -sL https://api.github.com/repos/microsoft/retina/releases/latest | jq -r .name)&#xA;helm upgrade --install retina oci://ghcr.io/microsoft/retina/charts/retina \&#xA;    --version $VERSION \&#xA;&#x9;&#x9;--set image.tag=$VERSION \&#xA;&#x9;&#x9;--set operator.tag=$VERSION \&#xA;&#x9;&#x9;--set logLevel=info \&#xA;&#x9;&#x9;--set enabledPlugin_linux=&#34;\[dropreason\,packetforward\,linuxutil\,dns\]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set the &lt;code&gt;version&lt;/code&gt; and image &lt;code&gt;tag&lt;/code&gt; arguments to the desired version, if different.&lt;/p&gt; &#xA;&lt;p&gt;After Helm install, follow steps in &lt;a href=&#34;https://retina.sh/docs/installation/prometheus-unmanaged&#34;&gt;Using Prometheus and Grafana&lt;/a&gt; to set up metrics collection and visualization.&lt;/p&gt; &#xA;&lt;h3&gt;Captures Quick Start Guide&lt;/h3&gt; &#xA;&lt;h4&gt;Captures via CLI&lt;/h4&gt; &#xA;&lt;p&gt;The preferred way to install the Retina CLI using &lt;a href=&#34;https://krew.sigs.k8s.io/&#34;&gt;Krew&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl krew install retina&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other installation options are documented in &lt;a href=&#34;https://retina.sh/docs/installation/cli&#34;&gt;CLI Installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Verify installation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl retina version&#xA;v0.0.4 # or latest version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To quickly start creating a capture:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl retina capture create --name &amp;lt;my-capture&amp;gt; --namespace &amp;lt;my-namespace&amp;gt; --selector &amp;lt;app=my-app&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For further CLI documentation, see &lt;a href=&#34;https://retina.sh/docs/captures/cli&#34;&gt;Capture with Retina CLI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Captures via CRD&lt;/h4&gt; &#xA;&lt;p&gt;Install Retina using Helm:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;VERSION=$( curl -sL https://api.github.com/repos/microsoft/retina/releases/latest | jq -r .name)&#xA;helm upgrade --install retina oci://ghcr.io/microsoft/retina/charts/retina \&#xA;    --version $VERSION \&#xA;    --set image.tag=$VERSION \&#xA;    --set operator.tag=$VERSION \&#xA;    --set image.pullPolicy=Always \&#xA;    --set logLevel=info \&#xA;    --set os.windows=true \&#xA;    --set operator.enabled=true \&#xA;    --set operator.enableRetinaEndpoint=true \&#xA;    --skip-crds \&#xA;    --set enabledPlugin_linux=&#34;\[dropreason\,packetforward\,linuxutil\,dns\,packetparser\]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then follow steps in &lt;a href=&#34;https://retina.sh/docs/captures/#option-2-capture-crd-custom-resource-definition&#34;&gt;Capture CRD&lt;/a&gt; for documentation of the CRD and examples for setting up Captures.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://retina.sh/docs/contributing&#34;&gt;Read more about how to begin contributing here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Office Hours and Community Meetings&lt;/h3&gt; &#xA;&lt;p&gt;We host a periodic open community meeting. &lt;a href=&#34;https://retina.sh/docs/contributing/#office-hours-and-community-meetings&#34;&gt;Find the details here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/retina/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;For bugs or feature requests, open an &lt;a href=&#34;https://github.com/microsoft/retina/issues&#34;&gt;issue&lt;/a&gt;. For security or vulnerability concerns, see &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/retina/main/SECURITY.md&#34;&gt;SECURITY.md&lt;/a&gt;. For other communication, contact the maintainers at &lt;a href=&#34;mailto:retina@microsoft.com&#34;&gt;retina@microsoft.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>