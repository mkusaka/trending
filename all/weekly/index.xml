<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-09T01:38:59Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>n4ze3m/page-assist</title>
    <updated>2025-02-09T01:38:59Z</updated>
    <id>tag:github.com,2025-02-09:/n4ze3m/page-assist</id>
    <link href="https://github.com/n4ze3m/page-assist" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Use your locally running AI models to assist you in your web browsing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Page Assist&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/bu54382uBd&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/discord-join%20chat-blue.svg?sanitize=true&#34; alt=&#34;Join dialoqbase #welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Page Assist is an open-source browser extension that provides a sidebar and web UI for your local AI model. It allows you to interact with your model from any webpage.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Page Assist supports Chromium-based browsers like Chrome, Brave, and Edge, as well as Firefox.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chrome.google.com/webstore/detail/page-assist/jfgfiigpkhlkbnfnbobbkinehhfdhndo&#34;&gt;&lt;img src=&#34;https://pub-35424b4473484be483c0afa08c69e7da.r2.dev/UV4C4ybeBTsZt43U4xis.png&#34; alt=&#34;Chrome Web Store&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://addons.mozilla.org/en-US/firefox/addon/page-assist/&#34;&gt;&lt;img src=&#34;https://pub-35424b4473484be483c0afa08c69e7da.r2.dev/get-the-addon.png&#34; alt=&#34;Firefox Add-on&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Checkout the Demo (v1.0.0):&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8VTjlLGXA4s&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/8VTjlLGXA4s/0.jpg&#34; alt=&#34;Page Assist Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sidebar&lt;/strong&gt;: A sidebar that can be opened on any webpage. It allows you to interact with your model and see the results.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Web UI&lt;/strong&gt;: A web UI that allows you to interact with your model like a ChatGPT Website.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chat With Webpage&lt;/strong&gt;: You can chat with the webpage and ask questions about the content.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;want more features? Create an issue and let me know.&lt;/p&gt; &#xA;&lt;h3&gt;Manual Installation&lt;/h3&gt; &#xA;&lt;h4&gt;Pre-requisites&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bun - &lt;a href=&#34;https://bun.sh/&#34;&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ollama (Local AI Provider) - &lt;a href=&#34;https://ollama.com&#34;&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Any OpenAI API Compatible Endpoint (like LM Studio, llamafile etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/n4ze3m/page-assist.git&#xA;cd page-assist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install the dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bun install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Build the extension (by default it will build for Chrome)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bun run build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or you can build for Firefox&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bun build:firefox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Load the extension (chrome)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the Extension Management page by navigating to &lt;code&gt;chrome://extensions&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enable Developer Mode by clicking the toggle switch next to Developer mode.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click the &lt;code&gt;Load unpacked&lt;/code&gt; button and select the &lt;code&gt;build&lt;/code&gt; directory.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Load the extension (firefox)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open the Add-ons page by navigating to &lt;code&gt;about:addons&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Click the &lt;code&gt;Extensions&lt;/code&gt; tab.&lt;/li&gt; &#xA; &lt;li&gt;Click the &lt;code&gt;Manage Your Extensions&lt;/code&gt; button.&lt;/li&gt; &#xA; &lt;li&gt;Click the &lt;code&gt;Load Temporary Add-on&lt;/code&gt; button and select the &lt;code&gt;manifest.json&lt;/code&gt; file from the &lt;code&gt;build&lt;/code&gt; directory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Sidebar&lt;/h3&gt; &#xA;&lt;p&gt;Once the extension is installed, you can open the sidebar via context menu or keyboard shortcut.&lt;/p&gt; &#xA;&lt;p&gt;Default Keyboard Shortcut: &lt;code&gt;Ctrl+Shift+Y&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;p&gt;You can open the Web UI by clicking on the extension icon which will open a new tab with the Web UI.&lt;/p&gt; &#xA;&lt;p&gt;Default Keyboard Shortcut: &lt;code&gt;Ctrl+Shift+L&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: You can change the keyboard shortcuts from the extension settings on the Chrome Extension Management page.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;You can run the extension in development mode to make changes and test them.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bun dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will start a development server and watch for changes in the source files. You can load the extension in your browser and test the changes.&lt;/p&gt; &#xA;&lt;h2&gt;Browser Support&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Browser&lt;/th&gt; &#xA;   &lt;th&gt;Sidebar&lt;/th&gt; &#xA;   &lt;th&gt;Chat With Webpage&lt;/th&gt; &#xA;   &lt;th&gt;Web UI&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chrome&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Brave&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Firefox&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vivaldi&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Edge&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LibreWolf&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Zen Browser&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Opera&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Arc&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Local AI Provider&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;Ollama&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Chrome AI (Gemini Nano)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;OpenAI API Compatible endpoints (like LM Studio, llamafile etc.)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Firefox Support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; More Local AI Providers&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; More Customization Options&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Better UI/UX&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Privacy&lt;/h2&gt; &#xA;&lt;p&gt;Page Assist does not collect any personal data. The only time the extension communicates with the server is when you are using the share feature, which can be disabled from the settings.&lt;/p&gt; &#xA;&lt;p&gt;All the data is stored locally in the browser storage. You can view the source code and verify it yourself.&lt;/p&gt; &#xA;&lt;p&gt;You learn more about the privacy policy &lt;a href=&#34;https://raw.githubusercontent.com/n4ze3m/page-assist/main/PRIVACY.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome. If you have any feature requests, bug reports, or questions, feel free to create an issue.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;If you like the project and want to support it, you can buy me a coffee. It will help me to keep working on the project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ko-fi.com/M4M3EMCLL&#34; target=&#34;_blank&#34;&gt;&lt;img height=&#34;36&#34; style=&#34;border:0px;height:36px;&#34; src=&#34;https://storage.ko-fi.com/cdn/kofi2.png?v=3&#34; border=&#34;0&#34; alt=&#34;Buy Me a Coffee at ko-fi.com&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;or you can sponsor me on GitHub.&lt;/p&gt; &#xA;&lt;h2&gt;Blogs and Videos About Page Assist&lt;/h2&gt; &#xA;&lt;p&gt;This are some of the blogs and videos about Page Assist. If you have written a blog or made a video about Page Assist, feel free to create a PR and add it here.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://note.com/lucas_san/n/nf00d01a02c3a&#34;&gt;Ollama„ÇíChromeAddon„ÅÆPage Assist„ÅßÁ∞°ÂçòÊìç‰Ωú&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/LucasChatGPT&#34;&gt;LucasChatGPT&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=IvLTlDy9G8c&#34;&gt;This Chrome Extension Surprised Me&lt;/a&gt; by &lt;a href=&#34;https://www.youtube.com/@technovangelist&#34;&gt;Matt Williams&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=61uN5jtj2wo&#34;&gt;Ollama With 1 Click&lt;/a&gt; by &lt;a href=&#34;https://www.youtube.com/@ecomxfactor-YaronBeen&#34;&gt;Yaron Been From EcomXFactor&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt; &#xA;&lt;h2&gt;Last but not least&lt;/h2&gt; &#xA;&lt;p&gt;Made in &lt;a href=&#34;https://en.wikipedia.org/wiki/Alappuzha&#34;&gt;Alappuzha&lt;/a&gt; with ‚ù§Ô∏è&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>langgenius/dify</title>
    <updated>2025-02-09T01:38:59Z</updated>
    <id>tag:github.com,2025-02-09:/langgenius/dify</id>
    <link href="https://github.com/langgenius/dify" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Dify is an open-source LLM app development platform. Dify&#39;s intuitive interface combines AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/langgenius/dify/assets/13230914/f9e19af5-61ba-4119-b926-d10c4c06ebab&#34; alt=&#34;cover-v5-optimized&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; üìå &lt;a href=&#34;https://dify.ai/blog/introducing-dify-workflow-file-upload-a-demo-on-ai-podcast&#34;&gt;Introducing Dify Workflow File Upload: Recreate Google NotebookLM Podcast&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://cloud.dify.ai&#34;&gt;Dify Cloud&lt;/a&gt; ¬∑ &lt;a href=&#34;https://docs.dify.ai/getting-started/install-self-hosted&#34;&gt;Self-hosting&lt;/a&gt; ¬∑ &lt;a href=&#34;https://docs.dify.ai&#34;&gt;Documentation&lt;/a&gt; ¬∑ &lt;a href=&#34;https://udify.app/chat/22L1zSxg6yW1cWQg&#34;&gt;Enterprise inquiry&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://dify.ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Static Badge&#34; src=&#34;https://img.shields.io/badge/Product-F04438&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dify.ai/pricing&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Static Badge&#34; src=&#34;https://img.shields.io/badge/free-pricing?logo=free&amp;amp;color=%20%23155EEF&amp;amp;label=pricing&amp;amp;labelColor=%20%23528bff&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/FngNHpbcY7&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/discord/1082486657678311454?logo=discord&amp;amp;labelColor=%20%235462eb&amp;amp;logoColor=%20%23f5f5f5&amp;amp;color=%20%235462eb&#34; alt=&#34;chat on Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://reddit.com/r/difyai&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/reddit/subreddit-subscribers/difyai?style=plastic&amp;amp;logo=reddit&amp;amp;label=r%2Fdifyai&amp;amp;labelColor=white&#34; alt=&#34;join Reddit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=dify_ai&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/dify_ai?logo=X&amp;amp;color=%20%23f5f5f5&#34; alt=&#34;follow on X(Twitter)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/company/langgenius/&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&amp;amp;logoColor=fff&#34; alt=&#34;follow on LinkedIn&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/u/langgenius&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Docker Pulls&#34; src=&#34;https://img.shields.io/docker/pulls/langgenius/dify-web?labelColor=%20%23FDB062&amp;amp;color=%20%23f79009&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/langgenius/dify/graphs/commit-activity&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Commits last month&#34; src=&#34;https://img.shields.io/github/commit-activity/m/langgenius/dify?labelColor=%20%2332b583&amp;amp;color=%20%2312b76a&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/langgenius/dify/&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Issues closed&#34; src=&#34;https://img.shields.io/github/issues-search?query=repo%3Alanggenius%2Fdify%20is%3Aclosed&amp;amp;label=issues%20closed&amp;amp;labelColor=%20%237d89b0&amp;amp;color=%20%235d6b98&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/langgenius/dify/discussions/&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Discussion posts&#34; src=&#34;https://img.shields.io/github/discussions/langgenius/dify?labelColor=%20%239b8afb&amp;amp;color=%20%237a5af8&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/README.md&#34;&gt;&lt;img alt=&#34;README in English&#34; src=&#34;https://img.shields.io/badge/English-d9d9d9&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/README_CN.md&#34;&gt;&lt;img alt=&#34;ÁÆÄ‰Ωì‰∏≠ÊñáÁâàËá™Ëø∞Êñá‰ª∂&#34; src=&#34;https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-d9d9d9&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/README_JA.md&#34;&gt;&lt;img alt=&#34;Êó•Êú¨Ë™û„ÅÆREADME&#34; src=&#34;https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9E-d9d9d9&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/README_ES.md&#34;&gt;&lt;img alt=&#34;README en Espa√±ol&#34; src=&#34;https://img.shields.io/badge/Espa%C3%B1ol-d9d9d9&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/README_FR.md&#34;&gt;&lt;img alt=&#34;README en Fran√ßais&#34; src=&#34;https://img.shields.io/badge/Fran%C3%A7ais-d9d9d9&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/README_KL.md&#34;&gt;&lt;img alt=&#34;README tlhIngan Hol&#34; src=&#34;https://img.shields.io/badge/Klingon-d9d9d9&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/README_KR.md&#34;&gt;&lt;img alt=&#34;README in Korean&#34; src=&#34;https://img.shields.io/badge/%ED%95%9C%EA%B5%AD%EC%96%B4-d9d9d9&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/README_AR.md&#34;&gt;&lt;img alt=&#34;README ÿ®ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&#34; src=&#34;https://img.shields.io/badge/%D8%A7%D9%84%D8%B9%D8%B1%D8%A8%D9%8A%D8%A9-d9d9d9&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/README_TR.md&#34;&gt;&lt;img alt=&#34;T√ºrk√ße README&#34; src=&#34;https://img.shields.io/badge/T%C3%BCrk%C3%A7e-d9d9d9&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/README_VI.md&#34;&gt;&lt;img alt=&#34;README Ti·∫øng Vi·ªát&#34; src=&#34;https://img.shields.io/badge/Ti%E1%BA%BFng%20Vi%E1%BB%87t-d9d9d9&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Dify is an open-source LLM app development platform. Its intuitive interface combines agentic AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production.&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Before installing Dify, make sure your machine meets the following minimum system requirements:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;CPU &amp;gt;= 2 Core&lt;/li&gt; &#xA;  &lt;li&gt;RAM &amp;gt;= 4 GiB&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;The easiest way to start the Dify server is through &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/docker/docker-compose.yaml&#34;&gt;docker compose&lt;/a&gt;. Before running Dify with the following commands, make sure that &lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;Docker&lt;/a&gt; and &lt;a href=&#34;https://docs.docker.com/compose/install/&#34;&gt;Docker Compose&lt;/a&gt; are installed on your machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd dify&#xA;cd docker&#xA;cp .env.example .env&#xA;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running, you can access the Dify dashboard in your browser at &lt;a href=&#34;http://localhost/install&#34;&gt;http://localhost/install&lt;/a&gt; and start the initialization process.&lt;/p&gt; &#xA;&lt;h4&gt;Seeking help&lt;/h4&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://docs.dify.ai/getting-started/install-self-hosted/faqs&#34;&gt;FAQ&lt;/a&gt; if you encounter problems setting up Dify. Reach out to &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/#community--contact&#34;&gt;the community and us&lt;/a&gt; if you are still having issues.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you&#39;d like to contribute to Dify or do additional development, refer to our &lt;a href=&#34;https://docs.dify.ai/getting-started/install-self-hosted/local-source-code&#34;&gt;guide to deploying from source code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Key features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Workflow&lt;/strong&gt;: Build and test powerful AI workflows on a visual canvas, leveraging all the following features and beyond.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/langgenius/dify/assets/13230914/356df23e-1604-483d-80a6-9517ece318aa&#34;&gt;https://github.com/langgenius/dify/assets/13230914/356df23e-1604-483d-80a6-9517ece318aa&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Comprehensive model support&lt;/strong&gt;: Seamless integration with hundreds of proprietary / open-source LLMs from dozens of inference providers and self-hosted solutions, covering GPT, Mistral, Llama3, and any OpenAI API-compatible models. A full list of supported model providers can be found &lt;a href=&#34;https://docs.dify.ai/getting-started/readme/model-providers&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/langgenius/dify/assets/13230914/5a17bdbe-097a-4100-8363-40255b70f6e3&#34; alt=&#34;providers-v5&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Prompt IDE&lt;/strong&gt;: Intuitive interface for crafting prompts, comparing model performance, and adding additional features such as text-to-speech to a chat-based app.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;4. RAG Pipeline&lt;/strong&gt;: Extensive RAG capabilities that cover everything from document ingestion to retrieval, with out-of-box support for text extraction from PDFs, PPTs, and other common document formats.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;5. Agent capabilities&lt;/strong&gt;: You can define agents based on LLM Function Calling or ReAct, and add pre-built or custom tools for the agent. Dify provides 50+ built-in tools for AI agents, such as Google Search, DALL¬∑E, Stable Diffusion and WolframAlpha.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;6. LLMOps&lt;/strong&gt;: Monitor and analyze application logs and performance over time. You could continuously improve prompts, datasets, and models based on production data and annotations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;7. Backend-as-a-Service&lt;/strong&gt;: All of Dify&#39;s offerings come with corresponding APIs, so you could effortlessly integrate Dify into your own business logic.&lt;/p&gt; &#xA;&lt;h2&gt;Using Dify&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cloud &lt;br&gt;&lt;/strong&gt; We host a &lt;a href=&#34;https://dify.ai&#34;&gt;Dify Cloud&lt;/a&gt; service for anyone to try with zero setup. It provides all the capabilities of the self-deployed version, and includes 200 free GPT-4 calls in the sandbox plan.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Self-hosting Dify Community Edition&lt;br&gt;&lt;/strong&gt; Quickly get Dify running in your environment with this &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/#quick-start&#34;&gt;starter guide&lt;/a&gt;. Use our &lt;a href=&#34;https://docs.dify.ai&#34;&gt;documentation&lt;/a&gt; for further references and more in-depth instructions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dify for enterprise / organizations&lt;br&gt;&lt;/strong&gt; We provide additional enterprise-centric features. &lt;a href=&#34;https://udify.app/chat/22L1zSxg6yW1cWQg&#34;&gt;Log your questions for us through this chatbot&lt;/a&gt; or &lt;a href=&#34;mailto:business@dify.ai?subject=%5BGitHub%5DBusiness%20License%20Inquiry&#34;&gt;send us an email&lt;/a&gt; to discuss enterprise needs. &lt;br&gt;&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;For startups and small businesses using AWS, check out &lt;a href=&#34;https://aws.amazon.com/marketplace/pp/prodview-t22mebxzwjhu6&#34;&gt;Dify Premium on AWS Marketplace&lt;/a&gt; and deploy it to your own AWS VPC with one-click. It&#39;s an affordable AMI offering with the option to create apps with custom logo and branding.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Staying ahead&lt;/h2&gt; &#xA;&lt;p&gt;Star Dify on GitHub and be instantly notified of new releases.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/langgenius/dify/assets/13230914/b823edc1-6388-4e25-ad45-2f6b187adbb4&#34; alt=&#34;star-us&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Advanced Setup&lt;/h2&gt; &#xA;&lt;p&gt;If you need to customize the configuration, please refer to the comments in our &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/docker/.env.example&#34;&gt;.env.example&lt;/a&gt; file and update the corresponding values in your &lt;code&gt;.env&lt;/code&gt; file. Additionally, you might need to make adjustments to the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file itself, such as changing image versions, port mappings, or volume mounts, based on your specific deployment environment and requirements. After making any changes, please re-run &lt;code&gt;docker-compose up -d&lt;/code&gt;. You can find the full list of available environment variables &lt;a href=&#34;https://docs.dify.ai/getting-started/install-self-hosted/environments&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to configure a highly-available setup, there are community-contributed &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm Charts&lt;/a&gt; and YAML files which allow Dify to be deployed on Kubernetes.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/douban/charts/tree/master/charts/dify&#34;&gt;Helm Chart by @LeoQuote&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BorisPolonsky/dify-helm&#34;&gt;Helm Chart by @BorisPolonsky&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Winson-030/dify-kubernetes&#34;&gt;YAML file by @Winson-030&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Using Terraform for Deployment&lt;/h4&gt; &#xA;&lt;p&gt;Deploy Dify to Cloud Platform with a single click using &lt;a href=&#34;https://www.terraform.io/&#34;&gt;terraform&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h5&gt;Azure Global&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nikawang/dify-azure-terraform&#34;&gt;Azure Terraform by @nikawang&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Google Cloud&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DeNA/dify-google-cloud-terraform&#34;&gt;Google Cloud Terraform by @sotazum&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Using AWS CDK for Deployment&lt;/h4&gt; &#xA;&lt;p&gt;Deploy Dify to AWS with &lt;a href=&#34;https://aws.amazon.com/cdk/&#34;&gt;CDK&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h5&gt;AWS&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aws-samples/solution-for-deploying-dify-on-aws&#34;&gt;AWS CDK by @KevinZhao&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;For those who&#39;d like to contribute code, see our &lt;a href=&#34;https://github.com/langgenius/dify/raw/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt;. At the same time, please consider supporting Dify by sharing it on social media and at events and conferences.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We are looking for contributors to help with translating Dify to languages other than Mandarin or English. If you are interested in helping, please see the &lt;a href=&#34;https://github.com/langgenius/dify/raw/main/web/i18n/README.md&#34;&gt;i18n README&lt;/a&gt; for more information, and leave us a comment in the &lt;code&gt;global-users&lt;/code&gt; channel of our &lt;a href=&#34;https://discord.gg/8Tpq4AcN9c&#34;&gt;Discord Community Server&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Community &amp;amp; contact&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langgenius/dify/discussions&#34;&gt;Github Discussion&lt;/a&gt;. Best for: sharing feedback and asking questions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langgenius/dify/issues&#34;&gt;GitHub Issues&lt;/a&gt;. Best for: bugs you encounter using Dify.AI, and feature proposals. See our &lt;a href=&#34;https://github.com/langgenius/dify/raw/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/FngNHpbcY7&#34;&gt;Discord&lt;/a&gt;. Best for: sharing your applications and hanging out with the community.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/dify_ai&#34;&gt;X(Twitter)&lt;/a&gt;. Best for: sharing your applications and hanging out with the community.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contributors&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/langgenius/dify/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=langgenius/dify&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Star history&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#langgenius/dify&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=langgenius/dify&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Security disclosure&lt;/h2&gt; &#xA;&lt;p&gt;To protect your privacy, please avoid posting security issues on GitHub. Instead, send your questions to &lt;a href=&#34;mailto:security@dify.ai&#34;&gt;security@dify.ai&lt;/a&gt; and we will provide you with a more detailed answer.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is available under the &lt;a href=&#34;https://raw.githubusercontent.com/langgenius/dify/main/LICENSE&#34;&gt;Dify Open Source License&lt;/a&gt;, which is essentially Apache 2.0 with a few additional restrictions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ollama/ollama</title>
    <updated>2025-02-09T01:38:59Z</updated>
    <id>tag:github.com,2025-02-09:/ollama/ollama</id>
    <link href="https://github.com/ollama/ollama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 2, and other large language models.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt;&#xA;  &amp;nbsp; &#xA; &lt;a href=&#34;https://ollama.com&#34;&gt;&lt;/a&gt; &#xA; &lt;img alt=&#34;ollama&#34; height=&#34;200px&#34; src=&#34;https://github.com/ollama/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7&#34;&gt;  &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Ollama&lt;/h1&gt; &#xA;&lt;p&gt;Get up and running with large language models.&lt;/p&gt; &#xA;&lt;h3&gt;macOS&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ollama.com/download/Ollama-darwin.zip&#34;&gt;Download&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ollama.com/download/OllamaSetup.exe&#34;&gt;Download&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl -fsSL https://ollama.com/install.sh | sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ollama/ollama/raw/main/docs/linux.md&#34;&gt;Manual install instructions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;The official &lt;a href=&#34;https://hub.docker.com/r/ollama/ollama&#34;&gt;Ollama Docker image&lt;/a&gt; &lt;code&gt;ollama/ollama&lt;/code&gt; is available on Docker Hub.&lt;/p&gt; &#xA;&lt;h3&gt;Libraries&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ollama/ollama-python&#34;&gt;ollama-python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ollama/ollama-js&#34;&gt;ollama-js&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Community&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/ollama&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://reddit.com/r/ollama&#34;&gt;Reddit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;To run and chat with &lt;a href=&#34;https://ollama.com/library/llama3.2&#34;&gt;Llama 3.2&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama run llama3.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model library&lt;/h2&gt; &#xA;&lt;p&gt;Ollama supports a list of models available on &lt;a href=&#34;https://ollama.com/library&#34; title=&#34;ollama model library&#34;&gt;ollama.com/library&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here are some example models that can be downloaded:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Parameters&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek-R1&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;4.7GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run deepseek-r1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek-R1&lt;/td&gt; &#xA;   &lt;td&gt;671B&lt;/td&gt; &#xA;   &lt;td&gt;404GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run deepseek-r1:671b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.3&lt;/td&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;43GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run llama3.3&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.2&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;2.0GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run llama3.2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.2&lt;/td&gt; &#xA;   &lt;td&gt;1B&lt;/td&gt; &#xA;   &lt;td&gt;1.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run llama3.2:1b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.2 Vision&lt;/td&gt; &#xA;   &lt;td&gt;11B&lt;/td&gt; &#xA;   &lt;td&gt;7.9GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run llama3.2-vision&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.2 Vision&lt;/td&gt; &#xA;   &lt;td&gt;90B&lt;/td&gt; &#xA;   &lt;td&gt;55GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run llama3.2-vision:90b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.1&lt;/td&gt; &#xA;   &lt;td&gt;8B&lt;/td&gt; &#xA;   &lt;td&gt;4.7GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run llama3.1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.1&lt;/td&gt; &#xA;   &lt;td&gt;405B&lt;/td&gt; &#xA;   &lt;td&gt;231GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run llama3.1:405b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi 4&lt;/td&gt; &#xA;   &lt;td&gt;14B&lt;/td&gt; &#xA;   &lt;td&gt;9.1GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run phi4&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi 3 Mini&lt;/td&gt; &#xA;   &lt;td&gt;3.8B&lt;/td&gt; &#xA;   &lt;td&gt;2.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run phi3&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma 2&lt;/td&gt; &#xA;   &lt;td&gt;2B&lt;/td&gt; &#xA;   &lt;td&gt;1.6GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run gemma2:2b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma 2&lt;/td&gt; &#xA;   &lt;td&gt;9B&lt;/td&gt; &#xA;   &lt;td&gt;5.5GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run gemma2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma 2&lt;/td&gt; &#xA;   &lt;td&gt;27B&lt;/td&gt; &#xA;   &lt;td&gt;16GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run gemma2:27b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;4.1GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run mistral&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Moondream 2&lt;/td&gt; &#xA;   &lt;td&gt;1.4B&lt;/td&gt; &#xA;   &lt;td&gt;829MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run moondream&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Neural Chat&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;4.1GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run neural-chat&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Starling&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;4.1GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run starling-lm&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Code Llama&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run codellama&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 Uncensored&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run llama2-uncensored&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;4.5GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run llava&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Solar&lt;/td&gt; &#xA;   &lt;td&gt;10.7B&lt;/td&gt; &#xA;   &lt;td&gt;6.1GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run solar&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Customize a model&lt;/h2&gt; &#xA;&lt;h3&gt;Import from GGUF&lt;/h3&gt; &#xA;&lt;p&gt;Ollama supports importing GGUF models in the Modelfile:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a file named &lt;code&gt;Modelfile&lt;/code&gt;, with a &lt;code&gt;FROM&lt;/code&gt; instruction with the local filepath to the model you want to import.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM ./vicuna-33b.Q4_0.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create the model in Ollama&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama create example -f Modelfile&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the model&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama run example&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Import from Safetensors&lt;/h3&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/ollama/ollama/main/docs/import.md&#34;&gt;guide&lt;/a&gt; on importing models for more information.&lt;/p&gt; &#xA;&lt;h3&gt;Customize a prompt&lt;/h3&gt; &#xA;&lt;p&gt;Models from the Ollama library can be customized with a prompt. For example, to customize the &lt;code&gt;llama3.2&lt;/code&gt; model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama pull llama3.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a &lt;code&gt;Modelfile&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;FROM llama3.2&#xA;&#xA;# set the temperature to 1 [higher is more creative, lower is more coherent]&#xA;PARAMETER temperature 1&#xA;&#xA;# set the system message&#xA;SYSTEM &#34;&#34;&#34;&#xA;You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, create and run the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama create mario -f ./Modelfile&#xA;ollama run mario&#xA;&amp;gt;&amp;gt;&amp;gt; hi&#xA;Hello! It&#39;s your friend Mario.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information on working with a Modelfile, see the &lt;a href=&#34;https://raw.githubusercontent.com/ollama/ollama/main/docs/modelfile.md&#34;&gt;Modelfile&lt;/a&gt; documentation.&lt;/p&gt; &#xA;&lt;h2&gt;CLI Reference&lt;/h2&gt; &#xA;&lt;h3&gt;Create a model&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;ollama create&lt;/code&gt; is used to create a model from a Modelfile.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama create mymodel -f ./Modelfile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pull a model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama pull llama3.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This command can also be used to update a local model. Only the diff will be pulled.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Remove a model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama rm llama3.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Copy a model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama cp llama3.2 my-model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Multiline input&lt;/h3&gt; &#xA;&lt;p&gt;For multiline input, you can wrap text with &lt;code&gt;&#34;&#34;&#34;&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; &#34;&#34;&#34;Hello,&#xA;... world!&#xA;... &#34;&#34;&#34;&#xA;I&#39;m a basic program that prints the famous &#34;Hello, world!&#34; message to the console.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Multimodal models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama run llava &#34;What&#39;s in this image? /Users/jmorgan/Desktop/smile.png&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;: The image features a yellow smiley face, which is likely the central focus of the picture.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Pass the prompt as an argument&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama run llama3.2 &#34;Summarize this file: $(cat README.md)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;: Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Show model information&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama show llama3.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;List models on your computer&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;List which models are currently loaded&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama ps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Stop a model which is currently running&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama stop llama3.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Start Ollama&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;ollama serve&lt;/code&gt; is used when you want to start ollama without running the desktop application.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/ollama/ollama/raw/main/docs/development.md&#34;&gt;developer guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Running local builds&lt;/h3&gt; &#xA;&lt;p&gt;Next, start the server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./ollama serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, in a separate shell, run a model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./ollama run llama3.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;REST API&lt;/h2&gt; &#xA;&lt;p&gt;Ollama has a REST API for running and managing models.&lt;/p&gt; &#xA;&lt;h3&gt;Generate a response&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl http://localhost:11434/api/generate -d &#39;{&#xA;  &#34;model&#34;: &#34;llama3.2&#34;,&#xA;  &#34;prompt&#34;:&#34;Why is the sky blue?&#34;&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Chat with a model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl http://localhost:11434/api/chat -d &#39;{&#xA;  &#34;model&#34;: &#34;llama3.2&#34;,&#xA;  &#34;messages&#34;: [&#xA;    { &#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;why is the sky blue?&#34; }&#xA;  ]&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/ollama/ollama/main/docs/api.md&#34;&gt;API documentation&lt;/a&gt; for all endpoints.&lt;/p&gt; &#xA;&lt;h2&gt;Community Integrations&lt;/h2&gt; &#xA;&lt;h3&gt;Web &amp;amp; Desktop&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-webui/open-webui&#34;&gt;Open WebUI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AugustDev/enchanted&#34;&gt;Enchanted (macOS native)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fmaclen/hollama&#34;&gt;Hollama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ParisNeo/lollms-webui&#34;&gt;Lollms-Webui&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/danny-avila/LibreChat&#34;&gt;LibreChat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bionic-gpt/bionic-gpt&#34;&gt;Bionic GPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rtcfirefly/ollama-ui&#34;&gt;HTML UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jikkuatwork/saddle&#34;&gt;Saddle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ivanfioravanti/chatbot-ollama&#34;&gt;Chatbot UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mckaywrigley/chatbot-ui&#34;&gt;Chatbot UI v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ollama-interface/Ollama-Gui?tab=readme-ov-file&#34;&gt;Typescript UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/richawo/minimal-llm-ui&#34;&gt;Minimalistic React UI for Ollama Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kevinhermawan/Ollamac&#34;&gt;Ollamac&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/enricoros/big-AGI/raw/main/docs/config-local-ollama.md&#34;&gt;big-AGI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cheshire-cat-ai/core&#34;&gt;Cheshire Cat assistant framework&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/semperai/amica&#34;&gt;Amica&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BruceMacD/chatd&#34;&gt;chatd&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kghandour/Ollama-SwiftUI&#34;&gt;Ollama-SwiftUI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langgenius/dify&#34;&gt;Dify.AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mindmac.app&#34;&gt;MindMac&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jakobhoeg/nextjs-ollama-llm-ui&#34;&gt;NextJS Web Interface for Ollama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://msty.app&#34;&gt;Msty&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Bin-Huang/Chatbox&#34;&gt;Chatbox&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tgraupmann/WinForm_Ollama_Copilot&#34;&gt;WinForm Ollama Copilot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web&#34;&gt;NextChat&lt;/a&gt; with &lt;a href=&#34;https://docs.nextchat.dev/models/ollama&#34;&gt;Get Started Doc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mmo80/alpaca-webui&#34;&gt;Alpaca WebUI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/enoch1118/ollamaGUI&#34;&gt;OllamaGUI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/InternLM/OpenAOE&#34;&gt;OpenAOE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/leonid20000/OdinRunes&#34;&gt;Odin Runes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdjohnson/llm-x&#34;&gt;LLM-X&lt;/a&gt; (Progressive Web App)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mintplex-Labs/anything-llm&#34;&gt;AnythingLLM (Docker + MacOs/Windows/Linux native app)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rapidarchitect/ollama_basic_chat&#34;&gt;Ollama Basic Chat: Uses HyperDiv Reactive UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/drazdra/ollama-chats&#34;&gt;Ollama-chats RPG&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://intellibar.app/&#34;&gt;IntelliBar&lt;/a&gt; (AI-powered assistant for macOS)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/reid41/QA-Pilot&#34;&gt;QA-Pilot&lt;/a&gt; (Interactive chat tool that can leverage Ollama models for rapid understanding and navigation of GitHub code repositories)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sugarforever/chat-ollama&#34;&gt;ChatOllama&lt;/a&gt; (Open Source Chatbot based on Ollama with Knowledge Bases)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Nagi-ovo/CRAG-Ollama-Chat&#34;&gt;CRAG Ollama Chat&lt;/a&gt; (Simple Web Search with Corrective RAG)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/infiniflow/ragflow&#34;&gt;RAGFlow&lt;/a&gt; (Open-source Retrieval-Augmented Generation engine based on deep document understanding)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/StreamDeploy-DevRel/streamdeploy-llm-app-scaffold&#34;&gt;StreamDeploy&lt;/a&gt; (LLM Application Scaffold)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/swuecho/chat&#34;&gt;chat&lt;/a&gt; (chat web app for teams)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lobehub/lobe-chat&#34;&gt;Lobe Chat&lt;/a&gt; with &lt;a href=&#34;https://lobehub.com/docs/self-hosting/examples/ollama&#34;&gt;Integrating Doc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datvodinh/rag-chatbot.git&#34;&gt;Ollama RAG Chatbot&lt;/a&gt; (Local Chat with multiple PDFs using Ollama and RAG)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nurgo-software.com/products/brainsoup&#34;&gt;BrainSoup&lt;/a&gt; (Flexible native client with RAG &amp;amp; multi-agent automation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Renset/macai&#34;&gt;macai&lt;/a&gt; (macOS client for Ollama, ChatGPT, and other compatible API back-ends)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/josStorer/RWKV-Runner&#34;&gt;RWKV-Runner&lt;/a&gt; (RWKV offline LLM deployment tool, also usable as a client for ChatGPT and Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dezoito/ollama-grid-search&#34;&gt;Ollama Grid Search&lt;/a&gt; (app to evaluate and compare models)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Otacon/olpaka&#34;&gt;Olpaka&lt;/a&gt; (User-friendly Flutter Web App for Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CrazyNeil/OllamaSpring&#34;&gt;OllamaSpring&lt;/a&gt; (Ollama Client for macOS)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kartikm7/llocal&#34;&gt;LLocal.in&lt;/a&gt; (Easy to use Electron Desktop Client for Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dcSpark/shinkai-apps&#34;&gt;Shinkai Desktop&lt;/a&gt; (Two click install Local AI using Ollama + Files + RAG)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zeyoyt/ailama&#34;&gt;AiLama&lt;/a&gt; (A Discord User App that allows you to interact with Ollama anywhere in discord )&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rapidarchitect/ollama_mesop/&#34;&gt;Ollama with Google Mesop&lt;/a&gt; (Mesop Chat Client implementation with Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SciPhi-AI/R2R&#34;&gt;R2R&lt;/a&gt; (Open-source RAG engine)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/elearningshow/ollama-kis&#34;&gt;Ollama-Kis&lt;/a&gt; (A simple easy to use GUI with sample custom LLM for Drivers Education)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opengpa.org&#34;&gt;OpenGPA&lt;/a&gt; (Open-source offline-first Enterprise Agentic Application)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mateuszmigas/painting-droid&#34;&gt;Painting Droid&lt;/a&gt; (Painting app with AI integrations)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kerlig.com/&#34;&gt;Kerlig AI&lt;/a&gt; (AI writing assistant for macOS)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MindWorkAI/AI-Studio&#34;&gt;AI Studio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gyopak/sidellama&#34;&gt;Sidellama&lt;/a&gt; (browser-based LLM client)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/trypromptly/LLMStack&#34;&gt;LLMStack&lt;/a&gt; (No-code multi-agent framework to build LLM agents and workflows)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://boltai.com&#34;&gt;BoltAI for Mac&lt;/a&gt; (AI Chat Client for Mac)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/av/harbor&#34;&gt;Harbor&lt;/a&gt; (Containerized LLM Toolkit with Ollama as default backend)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/szczyglis-dev/py-gpt&#34;&gt;PyGPT&lt;/a&gt; (AI desktop assistant for Linux, Windows and Mac)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Jeffser/Alpaca&#34;&gt;Alpaca&lt;/a&gt; (An Ollama client application for linux and macos made with GTK4 and Adwaita)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Significant-Gravitas/AutoGPT/raw/master/docs/content/platform/ollama.md&#34;&gt;AutoGPT&lt;/a&gt; (AutoGPT Ollama integration)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jonathanhecl.com/go-crew/&#34;&gt;Go-CREW&lt;/a&gt; (Powerful Offline RAG in Golang)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvmp/partcad/&#34;&gt;PartCAD&lt;/a&gt; (CAD model generation with OpenSCAD and CadQuery)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ollama4j/ollama4j-web-ui&#34;&gt;Ollama4j Web UI&lt;/a&gt; - Java-based Web UI for Ollama built with Vaadin, Spring Boot and Ollama4j&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kspviswa/pyOllaMx&#34;&gt;PyOllaMx&lt;/a&gt; - macOS application capable of chatting with both Ollama and Apple MLX models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/saoudrizwan/claude-dev&#34;&gt;Claude Dev&lt;/a&gt; - VSCode extension for multi-file/whole-repo coding&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kangfenmao/cherry-studio&#34;&gt;Cherry Studio&lt;/a&gt; (Desktop client with Ollama support)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/1runeberg/confichat&#34;&gt;ConfiChat&lt;/a&gt; (Lightweight, standalone, multi-platform, and privacy focused LLM chat interface with optional encryption)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nickthecook/archyve&#34;&gt;Archyve&lt;/a&gt; (RAG-enabling document library)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rapidarchitect/ollama-crew-mesop&#34;&gt;crewAI with Mesop&lt;/a&gt; (Mesop Web Interface to run crewAI with Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chyok/ollama-gui&#34;&gt;Tkinter-based client&lt;/a&gt; (Python tkinter-based Client for Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/trendy-design/llmchat&#34;&gt;LLMChat&lt;/a&gt; (Privacy focused, 100% local, intuitive all-in-one chat interface)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Leon-Sander/Local-Multimodal-AI-Chat&#34;&gt;Local Multimodal AI Chat&lt;/a&gt; (Ollama-based LLM Chat with support for multiple features, including PDF RAG, voice chat, image-based interactions, and integration with OpenAI.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xark-argo/argo&#34;&gt;ARGO&lt;/a&gt; (Locally download and run Ollama and Huggingface models with RAG on Mac/Windows/Linux)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EliasPereirah/OrionChat&#34;&gt;OrionChat&lt;/a&gt; - OrionChat is a web interface for chatting with different AI providers&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bklieger-groq/g1&#34;&gt;G1&lt;/a&gt; (Prototype of using prompting strategies to improve the LLM&#39;s reasoning through o1-like reasoning chains.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lemonit-eric-mao/ollama-web-management&#34;&gt;Web management&lt;/a&gt; (Web management page)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/promptery/promptery&#34;&gt;Promptery&lt;/a&gt; (desktop client for Ollama.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JHubi1/ollama-app&#34;&gt;Ollama App&lt;/a&gt; (Modern and easy-to-use multi-platform client for Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/annilq/chat-ollama&#34;&gt;chat-ollama&lt;/a&gt; (a React Native client for Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tcsenpai/spacellama&#34;&gt;SpaceLlama&lt;/a&gt; (Firefox and Chrome extension to quickly summarize web pages with ollama in a sidebar)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tcsenpai/youlama&#34;&gt;YouLama&lt;/a&gt; (Webapp to quickly summarize any YouTube video, supporting Invidious as well)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tcsenpai/dualmind&#34;&gt;DualMind&lt;/a&gt; (Experimental app allowing two models to talk to each other in the terminal or in a web interface)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/h1ddenpr0cess20/ollamarama-matrix&#34;&gt;ollamarama-matrix&lt;/a&gt; (Ollama chatbot for the Matrix chat protocol)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/anan1213095357/ollama-chat-app&#34;&gt;ollama-chat-app&lt;/a&gt; (Flutter-based chat app)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.perfectmemory.ai/&#34;&gt;Perfect Memory AI&lt;/a&gt; (Productivity AI assists personalized by what you have seen on your screen, heard and said in the meetings)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hexastack/hexabot&#34;&gt;Hexabot&lt;/a&gt; (A conversational AI builder)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rapidarchitect/reddit_analyzer&#34;&gt;Reddit Rate&lt;/a&gt; (Search and Rate Reddit topics with a weighted summation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/adarshM84/OpenTalkGpt&#34;&gt;OpenTalkGpt&lt;/a&gt; (Chrome Extension to manage open-source models supported by Ollama, create custom models, and chat with models from a user-friendly UI)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vinhnx/vt.ai&#34;&gt;VT&lt;/a&gt; (A minimal multimodal AI chat app, with dynamic conversation routing. Supports local models via Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nosia-ai/nosia&#34;&gt;Nosia&lt;/a&gt; (Easy to install and use RAG platform based on Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nbonamy/witsy&#34;&gt;Witsy&lt;/a&gt; (An AI Desktop application available for Mac/Windows/Linux)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/US-Artificial-Intelligence/abbey&#34;&gt;Abbey&lt;/a&gt; (A configurable AI interface server with notebooks, document storage, and YouTube support)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dmayboroda/minima&#34;&gt;Minima&lt;/a&gt; (RAG with on-premises or fully local workflow)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AidfulAI/aidful-ollama-model-delete&#34;&gt;aidful-ollama-model-delete&lt;/a&gt; (User interface for simplified model cleanup)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ItzCrazyKns/Perplexica&#34;&gt;Perplexica&lt;/a&gt; (An AI-powered search engine &amp;amp; an open-source alternative to Perplexity AI)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oslook/ollama-webui&#34;&gt;Ollama Chat WebUI for Docker &lt;/a&gt; (Support for local docker deployment, lightweight ollama webui)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-tooklit/ollama-docs&#34;&gt;AI Toolkit for Visual Studio Code&lt;/a&gt; (Microsoft-official VSCode extension to chat, test, evaluate models with Ollama support, and use them in your AI applications.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/anilkay/MinimalNextOllamaChat&#34;&gt;MinimalNextOllamaChat&lt;/a&gt; (Minimal Web UI for Chat and Model Control)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TilmanGriesel/chipper&#34;&gt;Chipper&lt;/a&gt; AI interface for tinkerers (Ollama, Haystack RAG, Python)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CosmicEventHorizon/ChibiChat&#34;&gt;ChibiChat&lt;/a&gt; (Kotlin-based Android app to chat with Ollama and Koboldcpp API endpoints)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/qusaismael/localllm&#34;&gt;LocalLLM&lt;/a&gt; (Minimal Web-App to run ollama models on it with a GUI)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Cloud&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.google.com/run/docs/tutorials/gpu-gemma2-with-ollama&#34;&gt;Google Cloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fly.io/docs/python/do-more/add-ollama/&#34;&gt;Fly.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.koyeb.com/deploy/ollama&#34;&gt;Koyeb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Terminal&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggozad/oterm&#34;&gt;oterm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/s-kostyaev/ellama&#34;&gt;Ellama Emacs client&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zweifisch/ollama&#34;&gt;Emacs client&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/paradoxical-dev/neollama&#34;&gt;neollama&lt;/a&gt; UI client for interacting with models from within Neovim&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/David-Kunz/gen.nvim&#34;&gt;gen.nvim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nomnivore/ollama.nvim&#34;&gt;ollama.nvim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/marco-souza/ollero.nvim&#34;&gt;ollero.nvim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gerazov/ollama-chat.nvim&#34;&gt;ollama-chat.nvim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huynle/ogpt.nvim&#34;&gt;ogpt.nvim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karthink/gptel&#34;&gt;gptel Emacs client&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dustinblackman/oatmeal&#34;&gt;Oatmeal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pgibler/cmdh&#34;&gt;cmdh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/npahlfer/ooo&#34;&gt;ooo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/reid41/shell-pilot&#34;&gt;shell-pilot&lt;/a&gt;(Interact with models via pure shell scripts on Linux or macOS)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pythops/tenere&#34;&gt;tenere&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/taketwo/llm-ollama&#34;&gt;llm-ollama&lt;/a&gt; for &lt;a href=&#34;https://llm.datasette.io/en/stable/&#34;&gt;Datasette&#39;s LLM CLI&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/anaisbetts/typechat-cli&#34;&gt;typechat-cli&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/djcopley/ShellOracle&#34;&gt;ShellOracle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yusufcanb/tlm&#34;&gt;tlm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ericcurtin/podman-ollama&#34;&gt;podman-ollama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sammcj/gollama&#34;&gt;gollama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/paulrobello/parllama&#34;&gt;ParLlama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cognitivetech/ollama-ebook-summary/&#34;&gt;Ollama eBook Summary&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rapidarchitect/ollama_moe&#34;&gt;Ollama Mixture of Experts (MOE) in 50 lines of code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pepo-ec/vim-intelligence-bridge&#34;&gt;vim-intelligence-bridge&lt;/a&gt; Simple interaction of &#34;Ollama&#34; with the Vim editor&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://x-cmd.com/mod/ollama&#34;&gt;x-cmd ollama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/drunkwcodes/bb7&#34;&gt;bb7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/marcusziade/Swollama&#34;&gt;SwollamaCLI&lt;/a&gt; bundled with the Swollama Swift package. &lt;a href=&#34;https://github.com/marcusziade/Swollama?tab=readme-ov-file#cli-usage&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sigoden/aichat&#34;&gt;aichat&lt;/a&gt; All-in-one LLM CLI tool featuring Shell Assistant, Chat-REPL, RAG, AI tools &amp;amp; agents, with access to OpenAI, Claude, Gemini, Ollama, Groq, and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rrg92/powershai&#34;&gt;PowershAI&lt;/a&gt; PowerShell module that brings AI to terminal on Windows, including support for Ollama&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xyproto/orbiton&#34;&gt;orbiton&lt;/a&gt; Configuration-free text editor and IDE with support for tab completion with Ollama.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Apple Vision Pro&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AugustDev/enchanted&#34;&gt;Enchanted&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Database&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/timescale/pgai&#34;&gt;pgai&lt;/a&gt; - PostgreSQL as a vector database (Create and search embeddings from Ollama models using pgvector) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/timescale/pgai/raw/main/docs/vectorizer-quick-start.md&#34;&gt;Get started guide&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mindsdb/mindsdb/raw/staging/mindsdb/integrations/handlers/ollama_handler/README.md&#34;&gt;MindsDB&lt;/a&gt; (Connects Ollama models with nearly 200 data platforms and apps)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/philippgille/chromem-go/raw/v0.5.0/embed_ollama.go&#34;&gt;chromem-go&lt;/a&gt; with &lt;a href=&#34;https://github.com/philippgille/chromem-go/tree/v0.5.0/examples/rag-wikipedia-ollama&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dbkangaroo/kangaroo&#34;&gt;Kangaroo&lt;/a&gt; (AI-powered SQL client and admin tool for popular databases)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Package managers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://archlinux.org/packages/extra/x86_64/ollama/&#34;&gt;Pacman&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gentoo/guru/tree/master/app-misc/ollama&#34;&gt;Gentoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://artifacthub.io/packages/helm/ollama-helm/ollama&#34;&gt;Helm Chart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codeberg.org/tusharhero/ollama-guix&#34;&gt;Guix channel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://search.nixos.org/packages?channel=24.05&amp;amp;show=ollama&amp;amp;from=0&amp;amp;size=50&amp;amp;sort=relevance&amp;amp;type=packages&amp;amp;query=ollama&#34;&gt;Nix package&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://flox.dev/blog/ollama-part-one&#34;&gt;Flox&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Libraries&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/docs/integrations/llms/ollama&#34;&gt;LangChain&lt;/a&gt; and &lt;a href=&#34;https://js.langchain.com/docs/integrations/chat/ollama/&#34;&gt;LangChain.js&lt;/a&gt; with &lt;a href=&#34;https://js.langchain.com/docs/tutorials/local_rag/&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://firebase.google.com/docs/genkit/plugins/ollama&#34;&gt;Firebase Genkit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/crewAIInc/crewAI&#34;&gt;crewAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://remembersoftwares.github.io/yacana/&#34;&gt;Yacana&lt;/a&gt; (User-friendly multi-agent framework for brainstorming and executing predetermined flows with built-in tool integration)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/spring-projects/spring-ai&#34;&gt;Spring AI&lt;/a&gt; with &lt;a href=&#34;https://docs.spring.io/spring-ai/reference/api/chat/ollama-chat.html&#34;&gt;reference&lt;/a&gt; and &lt;a href=&#34;https://github.com/tzolov/ollama-tools&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tmc/langchaingo/&#34;&gt;LangChainGo&lt;/a&gt; with &lt;a href=&#34;https://github.com/tmc/langchaingo/tree/main/examples/ollama-completion-example&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langchain4j/langchain4j&#34;&gt;LangChain4j&lt;/a&gt; with &lt;a href=&#34;https://github.com/langchain4j/langchain4j-examples/tree/main/ollama-examples/src/main/java&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Abraxas-365/langchain-rust&#34;&gt;LangChainRust&lt;/a&gt; with &lt;a href=&#34;https://github.com/Abraxas-365/langchain-rust/raw/main/examples/llm_ollama.rs&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tryAGI/LangChain&#34;&gt;LangChain for .NET&lt;/a&gt; with &lt;a href=&#34;https://github.com/tryAGI/LangChain/raw/main/examples/LangChain.Samples.OpenAI/Program.cs&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/theodo-group/LLPhant?tab=readme-ov-file#ollama&#34;&gt;LLPhant&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.llamaindex.ai/en/stable/examples/llm/ollama/&#34;&gt;LlamaIndex&lt;/a&gt; and &lt;a href=&#34;https://ts.llamaindex.ai/modules/llms/available_llms/ollama&#34;&gt;LlamaIndexTS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BerriAI/litellm&#34;&gt;LiteLLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/presbrey/ollamafarm&#34;&gt;OllamaFarm for Go&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awaescher/OllamaSharp&#34;&gt;OllamaSharp for .NET&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gbaptista/ollama-ai&#34;&gt;Ollama for Ruby&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pepperoni21/ollama-rs&#34;&gt;Ollama-rs for Rust&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jmont-dev/ollama-hpp&#34;&gt;Ollama-hpp for C++&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ollama4j/ollama4j&#34;&gt;Ollama4j for Java&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelfusion.dev/integration/model-provider/ollama&#34;&gt;ModelFusion Typescript Library&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kevinhermawan/OllamaKit&#34;&gt;OllamaKit for Swift&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/breitburg/dart-ollama&#34;&gt;Ollama for Dart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cloudstudio/ollama-laravel&#34;&gt;Ollama for Laravel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/davidmigloz/langchain_dart&#34;&gt;LangChainDart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/semantic-kernel/tree/main/python/semantic_kernel/connectors/ai/ollama&#34;&gt;Semantic Kernel - Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepset-ai/haystack-integrations/raw/main/integrations/ollama.md&#34;&gt;Haystack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/brainlid/langchain&#34;&gt;Elixir LangChain&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JBGruber/rollama&#34;&gt;Ollama for R - rollama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hauselin/ollama-r&#34;&gt;Ollama for R - ollama-r&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lebrunel/ollama-ex&#34;&gt;Ollama-ex for Elixir&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/b-tocs/abap_btocs_ollama&#34;&gt;Ollama Connector for SAP ABAP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://testcontainers.com/modules/ollama/&#34;&gt;Testcontainers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://portkey.ai/docs/welcome/integration-guides/ollama&#34;&gt;Portkey&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/svilupp/PromptingTools.jl&#34;&gt;PromptingTools.jl&lt;/a&gt; with an &lt;a href=&#34;https://svilupp.github.io/PromptingTools.jl/dev/examples/working_with_ollama&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Project-Llama/llamascript&#34;&gt;LlamaScript&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/emirsahin1/llm-axe&#34;&gt;llm-axe&lt;/a&gt; (Python Toolkit for Building LLM Powered Apps)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.gollm.co/examples/ollama-example&#34;&gt;Gollm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jonathanhecl/gollama&#34;&gt;Gollama for Golang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xyproto/ollamaclient&#34;&gt;Ollamaclient for Golang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gitlab.com/tozd/go/fun&#34;&gt;High-level function abstraction in Go&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ArdaGnsrn/ollama-php&#34;&gt;Ollama PHP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/agents-flex/agents-flex&#34;&gt;Agents-Flex for Java&lt;/a&gt; with &lt;a href=&#34;https://github.com/agents-flex/agents-flex/tree/main/agents-flex-llm/agents-flex-llm-ollama/src/test/java/com/agentsflex/llm/ollama&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/parakeet-nest/parakeet&#34;&gt;Parakeet&lt;/a&gt; is a GoLang library, made to simplify the development of small generative AI applications with Ollama.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andygill/haverscript&#34;&gt;Haverscript&lt;/a&gt; with &lt;a href=&#34;https://github.com/andygill/haverscript/tree/main/examples&#34;&gt;examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mattt/ollama-swift&#34;&gt;Ollama for Swift&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/marcusziade/Swollama&#34;&gt;Swollama for Swift&lt;/a&gt; with &lt;a href=&#34;https://marcusziade.github.io/Swollama/documentation/swollama/&#34;&gt;DocC&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/prasad89/golamify&#34;&gt;GoLamify&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tusharad/ollama-haskell&#34;&gt;Ollama for Haskell&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nbonamy/multi-llm-ts&#34;&gt;multi-llm-ts&lt;/a&gt; (A Typescript/JavaScript library allowing access to different LLM in unified API)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lofcz/llmtornado&#34;&gt;LlmTornado&lt;/a&gt; (C# library providing a unified interface for major FOSS &amp;amp; Commercial inference APIs)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dravenk/ollama-zig&#34;&gt;Ollama for Zig&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Mobile&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AugustDev/enchanted&#34;&gt;Enchanted&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mobile-Artificial-Intelligence/maid&#34;&gt;Maid&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JHubi1/ollama-app&#34;&gt;Ollama App&lt;/a&gt; (Modern and easy-to-use multi-platform client for Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/1runeberg/confichat&#34;&gt;ConfiChat&lt;/a&gt; (Lightweight, standalone, multi-platform, and privacy focused LLM chat interface with optional encryption)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Extensions &amp;amp; Plugins&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MassimilianoPasquini97/raycast_ollama&#34;&gt;Raycast extension&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mxyng/discollama&#34;&gt;Discollama&lt;/a&gt; (Discord bot inside the Ollama discord channel)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/continuedev/continue&#34;&gt;Continue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thewh1teagle/vibe&#34;&gt;Vibe&lt;/a&gt; (Transcribe and analyze meetings with Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hinterdupfinger/obsidian-ollama&#34;&gt;Obsidian Ollama plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omagdy7/ollama-logseq&#34;&gt;Logseq Ollama plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andersrex/notesollama&#34;&gt;NotesOllama&lt;/a&gt; (Apple Notes Ollama plugin)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/samalba/dagger-chatbot&#34;&gt;Dagger Chatbot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mekb-turtle/discord-ai-bot&#34;&gt;Discord AI Bot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ruecat/ollama-telegram&#34;&gt;Ollama Telegram Bot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ej52/hass-ollama-conversation&#34;&gt;Hass Ollama Conversation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/abrenneke/rivet-plugin-ollama&#34;&gt;Rivet plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/longy2k/obsidian-bmo-chatbot&#34;&gt;Obsidian BMO Chatbot plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/herval/cliobot&#34;&gt;Cliobot&lt;/a&gt; (Telegram bot with Ollama support)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/logancyang/obsidian-copilot&#34;&gt;Copilot for Obsidian plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pfrankov/obsidian-local-gpt&#34;&gt;Obsidian Local GPT plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openinterpreter.com/language-model-setup/local-models/ollama&#34;&gt;Open Interpreter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ex3ndr/llama-coder&#34;&gt;Llama Coder&lt;/a&gt; (Copilot alternative using Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bernardo-bruning/ollama-copilot&#34;&gt;Ollama Copilot&lt;/a&gt; (Proxy that allows you to use ollama as a copilot like Github copilot)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rjmacarthy/twinny&#34;&gt;twinny&lt;/a&gt; (Copilot and Copilot chat alternative using Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/RussellCanfield/wingman-ai&#34;&gt;Wingman-AI&lt;/a&gt; (Copilot code and chat alternative using Ollama and Hugging Face)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/n4ze3m/page-assist&#34;&gt;Page Assist&lt;/a&gt; (Chrome Extension)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/imoize/plasmoid-ollamacontrol&#34;&gt;Plasmoid Ollama Control&lt;/a&gt; (KDE Plasma extension that allows you to quickly manage/control Ollama model)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tusharhero/aitelegrambot&#34;&gt;AI Telegram Bot&lt;/a&gt; (Telegram bot using Ollama in backend)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yaroslavyaroslav/OpenAI-sublime-text&#34;&gt;AI ST Completion&lt;/a&gt; (Sublime Text 4 AI assistant plugin with Ollama support)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kevinthedang/discord-ollama&#34;&gt;Discord-Ollama Chat Bot&lt;/a&gt; (Generalized TypeScript Discord Bot w/ Tuning Documentation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/josStorer/chatGPTBox&#34;&gt;ChatGPTBox: All in one browser extension&lt;/a&gt; with &lt;a href=&#34;https://github.com/josStorer/chatGPTBox/issues/616#issuecomment-1975186467&#34;&gt;Integrating Tutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rapmd73/Companion&#34;&gt;Discord AI chat/moderation bot&lt;/a&gt; Chat/moderation bot written in python. Uses Ollama to create personalities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nischalj10/headless-ollama&#34;&gt;Headless Ollama&lt;/a&gt; (Scripts to automatically install ollama client &amp;amp; models on any OS for apps that depends on ollama server)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xuyangbocn/terraform-aws-self-host-llm&#34;&gt;Terraform AWS Ollama &amp;amp; Open WebUI&lt;/a&gt; (A Terraform module to deploy on AWS a ready-to-use Ollama service, together with its front end Open WebUI service.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jakubburkiewicz/node-red-contrib-ollama&#34;&gt;node-red-contrib-ollama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ivostoykov/localAI&#34;&gt;Local AI Helper&lt;/a&gt; (Chrome and Firefox extensions that enable interactions with the active tab and customisable API endpoints. Includes secure storage for user prompts.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jake83741/vnc-lm&#34;&gt;vnc-lm&lt;/a&gt; (Discord bot for messaging with LLMs through Ollama and LiteLLM. Seamlessly move between local and flagship models.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SilasMarvin/lsp-ai&#34;&gt;LSP-AI&lt;/a&gt; (Open-source language server for AI-powered functionality)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Palm1r/QodeAssist&#34;&gt;QodeAssist&lt;/a&gt; (AI-powered coding assistant plugin for Qt Creator)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ECuiDev/obsidian-quiz-generator&#34;&gt;Obsidian Quiz Generator plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/philffm/ai-summary-helper&#34;&gt;AI Summmary Helper plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/suncloudsmoon/TextCraft&#34;&gt;TextCraft&lt;/a&gt; (Copilot in Word alternative using Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zeitlings/alfred-ollama&#34;&gt;Alfred Ollama&lt;/a&gt; (Alfred Workflow)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/adarshM84/TextLLaMA&#34;&gt;TextLLaMA&lt;/a&gt; A Chrome Extension that helps you write emails, correct grammar, and translate into any language&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zyphixor/simple-discord-ai&#34;&gt;Simple-Discord-AI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Supported backends&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; project founded by Georgi Gerganov.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Observability&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openlit/openlit&#34;&gt;OpenLIT&lt;/a&gt; is an OpenTelemetry-native tool for monitoring Ollama Applications &amp;amp; GPUs using traces and metrics.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.honeyhive.ai/integrations/ollama&#34;&gt;HoneyHive&lt;/a&gt; is an AI observability and evaluation platform for AI agents. Use HoneyHive to evaluate agent performance, interrogate failures, and monitor quality in production.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://langfuse.com/docs/integrations/ollama&#34;&gt;Langfuse&lt;/a&gt; is an open source LLM observability platform that enables teams to collaboratively monitor, evaluate and debug AI applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing&#34;&gt;MLflow Tracing&lt;/a&gt; is an open source LLM observability tool with a convenient API to log and visualize traces, making it easy to debug and evaluate GenAI applications.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>