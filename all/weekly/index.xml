<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-16T01:49:42Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>imClumsyPanda/langchain-ChatGLM</title>
    <updated>2023-04-16T01:49:42Z</updated>
    <id>tag:github.com,2023-04-16:/imClumsyPanda/langchain-ChatGLM</id>
    <link href="https://github.com/imClumsyPanda/langchain-ChatGLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;langchain-ChatGLM, local knowledge based ChatGLM with langchain ï½œ åŸºäºæœ¬åœ°çŸ¥è¯†çš„ ChatGLM é—®ç­”&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;åŸºäºæœ¬åœ°çŸ¥è¯†çš„ ChatGLM åº”ç”¨å®ç°&lt;/h1&gt; &#xA;&lt;h2&gt;ä»‹ç»&lt;/h2&gt; &#xA;&lt;p&gt;ğŸŒ &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/README_en.md&#34;&gt;&lt;em&gt;READ THIS IN ENGLISH&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ğŸ¤–ï¸ ä¸€ç§åˆ©ç”¨ &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt; + &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;langchain&lt;/a&gt; å®ç°çš„åŸºäºæœ¬åœ°çŸ¥è¯†çš„ ChatGLM åº”ç”¨ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ğŸ’¡ å— &lt;a href=&#34;https://github.com/GanymedeNil&#34;&gt;GanymedeNil&lt;/a&gt; çš„é¡¹ç›® &lt;a href=&#34;https://github.com/GanymedeNil/document.ai&#34;&gt;document.ai&lt;/a&gt; å’Œ &lt;a href=&#34;https://github.com/AlexZhangji&#34;&gt;AlexZhangji&lt;/a&gt; åˆ›å»ºçš„ &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B/pull/216&#34;&gt;ChatGLM-6B Pull Request&lt;/a&gt; å¯å‘ï¼Œå»ºç«‹äº†å…¨éƒ¨åŸºäºå¼€æºæ¨¡å‹å®ç°çš„æœ¬åœ°çŸ¥è¯†é—®ç­”åº”ç”¨ã€‚&lt;/p&gt; &#xA;&lt;p&gt;âœ… æœ¬é¡¹ç›®ä¸­ Embedding é€‰ç”¨çš„æ˜¯ &lt;a href=&#34;https://huggingface.co/GanymedeNil/text2vec-large-chinese/tree/main&#34;&gt;GanymedeNil/text2vec-large-chinese&lt;/a&gt;ï¼ŒLLM é€‰ç”¨çš„æ˜¯ &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt;ã€‚ä¾æ‰˜ä¸Šè¿°æ¨¡å‹ï¼Œæœ¬é¡¹ç›®å¯å®ç°å…¨éƒ¨ä½¿ç”¨&lt;strong&gt;å¼€æº&lt;/strong&gt;æ¨¡å‹&lt;strong&gt;ç¦»çº¿ç§æœ‰éƒ¨ç½²&lt;/strong&gt;ã€‚&lt;/p&gt; &#xA;&lt;p&gt;â›“ï¸ æœ¬é¡¹ç›®å®ç°åŸç†å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿‡ç¨‹åŒ…æ‹¬åŠ è½½æ–‡ä»¶ -&amp;gt; è¯»å–æ–‡æœ¬ -&amp;gt; æ–‡æœ¬åˆ†å‰² -&amp;gt; æ–‡æœ¬å‘é‡åŒ– -&amp;gt; é—®å¥å‘é‡åŒ– -&amp;gt; åœ¨æ–‡æœ¬å‘é‡ä¸­åŒ¹é…å‡ºä¸é—®å¥å‘é‡æœ€ç›¸ä¼¼çš„&lt;code&gt;top k&lt;/code&gt;ä¸ª -&amp;gt; åŒ¹é…å‡ºçš„æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡å’Œé—®é¢˜ä¸€èµ·æ·»åŠ åˆ°&lt;code&gt;prompt&lt;/code&gt;ä¸­ -&amp;gt; æäº¤ç»™&lt;code&gt;LLM&lt;/code&gt;ç”Ÿæˆå›ç­”ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/img/langchain+chatglm.png&#34; alt=&#34;å®ç°åŸç†å›¾&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ğŸš© æœ¬é¡¹ç›®æœªæ¶‰åŠå¾®è°ƒã€è®­ç»ƒè¿‡ç¨‹ï¼Œä½†å¯åˆ©ç”¨å¾®è°ƒæˆ–è®­ç»ƒå¯¹æœ¬é¡¹ç›®æ•ˆæœè¿›è¡Œä¼˜åŒ–ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;æ›´æ–°ä¿¡æ¯&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/04/15]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;é‡æ„é¡¹ç›®ç»“æ„ï¼Œåœ¨æ ¹ç›®å½•ä¸‹ä¿ç•™å‘½ä»¤è¡Œ Demo &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/cli_demo.py&#34;&gt;cli_demo.py&lt;/a&gt; å’Œ Web UI Demo &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/webui.py&#34;&gt;webui.py&lt;/a&gt;ï¼›&lt;/li&gt; &#xA; &lt;li&gt;å¯¹ Web UI è¿›è¡Œæ”¹è¿›ï¼Œä¿®æ”¹ä¸ºè¿è¡Œ Web UI åé¦–å…ˆæŒ‰ç…§ &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/configs/model_config.py&#34;&gt;configs/model_config.py&lt;/a&gt; é»˜è®¤é€‰é¡¹åŠ è½½æ¨¡å‹ï¼Œå¹¶å¢åŠ æŠ¥é”™æç¤ºä¿¡æ¯ç­‰ï¼›&lt;/li&gt; &#xA; &lt;li&gt;å¯¹å¸¸è§é—®é¢˜è¿›è¡Œè¡¥å……è¯´æ˜ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/04/12]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;æ›¿æ¢ Web UI ä¸­çš„æ ·ä¾‹æ–‡ä»¶ï¼Œé¿å…å‡ºç° Ubuntu ä¸­å‡ºç°å› æ–‡ä»¶ç¼–ç æ— æ³•è¯»å–çš„é—®é¢˜ï¼›&lt;/li&gt; &#xA; &lt;li&gt;æ›¿æ¢&lt;code&gt;knowledge_based_chatglm.py&lt;/code&gt;ä¸­çš„ prompt æ¨¡ç‰ˆï¼Œé¿å…å‡ºç°å›  prompt æ¨¡ç‰ˆåŒ…å«ä¸­è‹±åŒè¯­å¯¼è‡´ chatglm è¿”å›å†…å®¹é”™ä¹±çš„é—®é¢˜ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/04/11]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;åŠ å…¥ Web UI V0.1 ç‰ˆæœ¬ï¼ˆæ„Ÿè°¢ &lt;a href=&#34;https://github.com/liangtongt&#34;&gt;@liangtongt&lt;/a&gt;ï¼‰ï¼›&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;README.md&lt;/code&gt;ä¸­å¢åŠ å¸¸è§é—®é¢˜ï¼ˆæ„Ÿè°¢ &lt;a href=&#34;https://github.com/calcitem&#34;&gt;@calcitem&lt;/a&gt; å’Œ &lt;a href=&#34;https://github.com/bolongliu&#34;&gt;@bolongliu&lt;/a&gt;ï¼‰ï¼›&lt;/li&gt; &#xA; &lt;li&gt;å¢åŠ  LLM å’Œ Embedding æ¨¡å‹è¿è¡Œè®¾å¤‡æ˜¯å¦å¯ç”¨&lt;code&gt;cuda&lt;/code&gt;ã€&lt;code&gt;mps&lt;/code&gt;ã€&lt;code&gt;cpu&lt;/code&gt;çš„è‡ªåŠ¨åˆ¤æ–­ã€‚&lt;/li&gt; &#xA; &lt;li&gt;åœ¨&lt;code&gt;knowledge_based_chatglm.py&lt;/code&gt;ä¸­å¢åŠ å¯¹&lt;code&gt;filepath&lt;/code&gt;çš„åˆ¤æ–­ï¼Œåœ¨ä¹‹å‰æ”¯æŒå•ä¸ªæ–‡ä»¶å¯¼å…¥çš„åŸºç¡€ä¸Šï¼Œç°æ”¯æŒå•ä¸ªæ–‡ä»¶å¤¹è·¯å¾„ä½œä¸ºè¾“å…¥ï¼Œè¾“å…¥åå°†ä¼šéå†æ–‡ä»¶å¤¹ä¸­å„ä¸ªæ–‡ä»¶ï¼Œå¹¶åœ¨å‘½ä»¤è¡Œä¸­æ˜¾ç¤ºæ¯ä¸ªæ–‡ä»¶æ˜¯å¦æˆåŠŸåŠ è½½ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/04/09]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ä½¿ç”¨&lt;code&gt;langchain&lt;/code&gt;ä¸­çš„&lt;code&gt;RetrievalQA&lt;/code&gt;æ›¿ä»£ä¹‹å‰é€‰ç”¨çš„&lt;code&gt;ChatVectorDBChain&lt;/code&gt;ï¼Œæ›¿æ¢åå¯ä»¥æœ‰æ•ˆå‡å°‘æé—® 2-3 æ¬¡åå› æ˜¾å­˜ä¸è¶³è€Œåœæ­¢è¿è¡Œçš„é—®é¢˜ï¼›&lt;/li&gt; &#xA; &lt;li&gt;åœ¨&lt;code&gt;knowledge_based_chatglm.py&lt;/code&gt;ä¸­å¢åŠ &lt;code&gt;EMBEDDING_MODEL&lt;/code&gt;ã€&lt;code&gt;VECTOR_SEARCH_TOP_K&lt;/code&gt;ã€&lt;code&gt;LLM_MODEL&lt;/code&gt;ã€&lt;code&gt;LLM_HISTORY_LEN&lt;/code&gt;ã€&lt;code&gt;REPLY_WITH_SOURCE&lt;/code&gt;å‚æ•°å€¼è®¾ç½®ï¼›&lt;/li&gt; &#xA; &lt;li&gt;å¢åŠ  GPU æ˜¾å­˜éœ€æ±‚æ›´å°çš„&lt;code&gt;chatglm-6b-int4&lt;/code&gt;ã€&lt;code&gt;chatglm-6b-int4-qe&lt;/code&gt;ä½œä¸º LLM æ¨¡å‹å¤‡é€‰é¡¹ï¼›&lt;/li&gt; &#xA; &lt;li&gt;æ›´æ­£&lt;code&gt;README.md&lt;/code&gt;ä¸­çš„ä»£ç é”™è¯¯ï¼ˆæ„Ÿè°¢ &lt;a href=&#34;https://github.com/calcitem&#34;&gt;@calcitem&lt;/a&gt;ï¼‰ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/04/07]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;è§£å†³åŠ è½½ ChatGLM æ¨¡å‹æ—¶å‘ç”Ÿæ˜¾å­˜å ç”¨ä¸ºåŒå€çš„é—®é¢˜ (æ„Ÿè°¢ &lt;a href=&#34;https://github.com/suc16&#34;&gt;@suc16&lt;/a&gt; å’Œ &lt;a href=&#34;https://github.com/myml&#34;&gt;@myml&lt;/a&gt;) ï¼›&lt;/li&gt; &#xA; &lt;li&gt;æ–°å¢æ¸…ç†æ˜¾å­˜æœºåˆ¶ï¼›&lt;/li&gt; &#xA; &lt;li&gt;æ–°å¢&lt;code&gt;nghuyong/ernie-3.0-nano-zh&lt;/code&gt;å’Œ&lt;code&gt;nghuyong/ernie-3.0-base-zh&lt;/code&gt;ä½œä¸º Embedding æ¨¡å‹å¤‡é€‰é¡¹ï¼Œç›¸æ¯”&lt;code&gt;GanymedeNil/text2vec-large-chinese&lt;/code&gt;å ç”¨æ˜¾å­˜èµ„æºæ›´å°‘ (æ„Ÿè°¢ &lt;a href=&#34;https://github.com/lastrei&#34;&gt;@lastrei&lt;/a&gt;)ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ä½¿ç”¨æ–¹å¼&lt;/h2&gt; &#xA;&lt;h3&gt;ç¡¬ä»¶éœ€æ±‚&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ChatGLM-6B æ¨¡å‹ç¡¬ä»¶éœ€æ±‚&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;&lt;strong&gt;é‡åŒ–ç­‰çº§&lt;/strong&gt;&lt;/th&gt; &#xA;     &lt;th&gt;&lt;strong&gt;æœ€ä½ GPU æ˜¾å­˜&lt;/strong&gt;ï¼ˆæ¨ç†ï¼‰&lt;/th&gt; &#xA;     &lt;th&gt;&lt;strong&gt;æœ€ä½ GPU æ˜¾å­˜&lt;/strong&gt;ï¼ˆé«˜æ•ˆå‚æ•°å¾®è°ƒï¼‰&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;FP16ï¼ˆæ— é‡åŒ–ï¼‰&lt;/td&gt; &#xA;     &lt;td&gt;13 GB&lt;/td&gt; &#xA;     &lt;td&gt;14 GB&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;INT8&lt;/td&gt; &#xA;     &lt;td&gt;8 GB&lt;/td&gt; &#xA;     &lt;td&gt;9 GB&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;INT4&lt;/td&gt; &#xA;     &lt;td&gt;6 GB&lt;/td&gt; &#xA;     &lt;td&gt;7 GB&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Embedding æ¨¡å‹ç¡¬ä»¶éœ€æ±‚&lt;/p&gt; &lt;p&gt;æœ¬é¡¹ç›®ä¸­é»˜è®¤é€‰ç”¨çš„ Embedding æ¨¡å‹ &lt;a href=&#34;https://huggingface.co/GanymedeNil/text2vec-large-chinese/tree/main&#34;&gt;GanymedeNil/text2vec-large-chinese&lt;/a&gt; çº¦å ç”¨æ˜¾å­˜ 3GBï¼Œä¹Ÿå¯ä¿®æ”¹ä¸ºåœ¨ CPU ä¸­è¿è¡Œã€‚&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;è½¯ä»¶éœ€æ±‚&lt;/h3&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®å·²åœ¨ Python 3.8ï¼ŒCUDA 11.7 ç¯å¢ƒä¸‹å®Œæˆæµ‹è¯•ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;1. å®‰è£…ç¯å¢ƒ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç¯å¢ƒæ£€æŸ¥&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# é¦–å…ˆï¼Œç¡®ä¿¡ä½ çš„æœºå™¨å®‰è£…äº† Python 3.8 åŠä»¥ä¸Šç‰ˆæœ¬&#xA;$ python --version&#xA;Python 3.8.13&#xA;&#xA;# å¦‚æœä½äºè¿™ä¸ªç‰ˆæœ¬ï¼Œå¯ä½¿ç”¨condaå®‰è£…ç¯å¢ƒ&#xA;$ conda create -p /your_path/env_name python=3.8&#xA;&#xA;# æ¿€æ´»ç¯å¢ƒ&#xA;$ source activate /your_path/env_name&#xA;&#xA;# å…³é—­ç¯å¢ƒ&#xA;$ source deactivate /your_path/env_name&#xA;&#xA;# åˆ é™¤ç¯å¢ƒ&#xA;$ conda env remove -p  /your_path/env_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;é¡¹ç›®ä¾èµ–&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# æ‹‰å–ä»“åº“&#xA;$ git clone https://github.com/imClumsyPanda/langchain-ChatGLM.git&#xA;&#xA;# å®‰è£…ä¾èµ–&#xA;$ pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;æ³¨ï¼šä½¿ç”¨ langchain.document_loaders.UnstructuredFileLoader è¿›è¡Œéç»“æ„åŒ–æ–‡ä»¶æ¥å…¥æ—¶ï¼Œå¯èƒ½éœ€è¦ä¾æ®æ–‡æ¡£è¿›è¡Œå…¶ä»–ä¾èµ–åŒ…çš„å®‰è£…ï¼Œè¯·å‚è€ƒ &lt;a href=&#34;https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/unstructured_file.html&#34;&gt;langchain æ–‡æ¡£&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;2. è®¾ç½®æ¨¡å‹é»˜è®¤å‚æ•°&lt;/h3&gt; &#xA;&lt;p&gt;åœ¨å¼€å§‹æ‰§è¡Œ Web UI æˆ–å‘½ä»¤è¡Œäº¤äº’å‰ï¼Œè¯·å…ˆæ£€æŸ¥ &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/configs/model_config.py&#34;&gt;configs/model_config.py&lt;/a&gt; ä¸­çš„å„é¡¹æ¨¡å‹å‚æ•°è®¾è®¡æ˜¯å¦ç¬¦åˆéœ€æ±‚ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;3. æ‰§è¡Œè„šæœ¬ä½“éªŒ Web UI æˆ–å‘½ä»¤è¡Œäº¤äº’&lt;/h3&gt; &#xA;&lt;p&gt;æ‰§è¡Œ &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/webui.py&#34;&gt;webui.py&lt;/a&gt; è„šæœ¬ä½“éªŒ &lt;strong&gt;Web äº¤äº’&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python webui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;æ³¨ï¼šå¦‚æœªå°†æ¨¡å‹ä¸‹è½½è‡³æœ¬åœ°ï¼Œè¯·æ‰§è¡Œå‰æ£€æŸ¥&lt;code&gt;$HOME/.cache/huggingface/&lt;/code&gt;æ–‡ä»¶å¤¹å‰©ä½™ç©ºé—´ï¼Œè‡³å°‘15G&lt;/p&gt; &#xA;&lt;p&gt;æ‰§è¡Œåæ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š &lt;img src=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/img/ui1.png&#34; alt=&#34;webui&#34;&gt; Web UI å¯ä»¥å®ç°å¦‚ä¸‹åŠŸèƒ½ï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;è¿è¡Œå‰è‡ªåŠ¨è¯»å–&lt;code&gt;configs/model_config.py&lt;/code&gt;ä¸­&lt;code&gt;LLM&lt;/code&gt;åŠ&lt;code&gt;Embedding&lt;/code&gt;æ¨¡å‹æšä¸¾åŠé»˜è®¤æ¨¡å‹è®¾ç½®è¿è¡Œæ¨¡å‹ï¼Œå¦‚éœ€é‡æ–°åŠ è½½æ¨¡å‹ï¼Œå¯åœ¨ç•Œé¢é‡æ–°é€‰æ‹©åç‚¹å‡»&lt;code&gt;é‡æ–°åŠ è½½æ¨¡å‹&lt;/code&gt;è¿›è¡Œæ¨¡å‹åŠ è½½ï¼›&lt;/li&gt; &#xA; &lt;li&gt;å¯æ‰‹åŠ¨è°ƒèŠ‚ä¿ç•™å¯¹è¯å†å²é•¿åº¦ï¼Œå¯æ ¹æ®æ˜¾å­˜å¤§å°è‡ªè¡Œè°ƒèŠ‚&lt;/li&gt; &#xA; &lt;li&gt;æ·»åŠ ä¸Šä¼ æ–‡ä»¶åŠŸèƒ½ï¼Œé€šè¿‡ä¸‹æ‹‰æ¡†é€‰æ‹©å·²ä¸Šä¼ çš„æ–‡ä»¶ï¼Œç‚¹å‡»&lt;code&gt;åŠ è½½æ–‡ä»¶&lt;/code&gt;æŒ‰é’®ï¼Œè¿‡ç¨‹ä¸­å¯éšæ—¶æ›´æ¢åŠ è½½çš„æ–‡ä»¶&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;æˆ–æ‰§è¡Œ &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/cli_demo.py&#34;&gt;knowledge_based_chatglm.py&lt;/a&gt; è„šæœ¬ä½“éªŒ&lt;strong&gt;å‘½ä»¤è¡Œäº¤äº’&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python knowledge_based_chatglm.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;å¸¸è§é—®é¢˜&lt;/h3&gt; &#xA;&lt;p&gt;Q1: æœ¬é¡¹ç›®æ”¯æŒå“ªäº›æ–‡ä»¶æ ¼å¼ï¼Ÿ&lt;/p&gt; &#xA;&lt;p&gt;A1: ç›®å‰å·²æµ‹è¯•æ”¯æŒ txtã€docxã€mdã€pdf æ ¼å¼æ–‡ä»¶ï¼Œæ›´å¤šæ–‡ä»¶æ ¼å¼è¯·å‚è€ƒ &lt;a href=&#34;https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/unstructured_file.html&#34;&gt;langchain æ–‡æ¡£&lt;/a&gt;ã€‚ç›®å‰å·²çŸ¥æ–‡æ¡£ä¸­è‹¥å«æœ‰ç‰¹æ®Šå­—ç¬¦ï¼Œå¯èƒ½å­˜åœ¨æ–‡ä»¶æ— æ³•åŠ è½½çš„é—®é¢˜ã€‚&lt;/p&gt; &#xA;&lt;p&gt;Q3: ä½¿ç”¨è¿‡ç¨‹ä¸­ Python åŒ…&lt;code&gt;nltk&lt;/code&gt;å‘ç”Ÿäº†&lt;code&gt;Resource punkt not found.&lt;/code&gt;æŠ¥é”™ï¼Œè¯¥å¦‚ä½•è§£å†³ï¼Ÿ&lt;/p&gt; &#xA;&lt;p&gt;A3: &lt;a href=&#34;https://github.com/nltk/nltk_data/raw/gh-pages/packages/tokenizers/punkt.zip&#34;&gt;https://github.com/nltk/nltk_data/raw/gh-pages/packages/tokenizers/punkt.zip&lt;/a&gt; ä¸­çš„ &lt;code&gt;packages/tokenizers&lt;/code&gt; è§£å‹ï¼Œæ”¾åˆ° &lt;code&gt;nltk_data/tokenizers&lt;/code&gt; å­˜å‚¨è·¯å¾„ä¸‹ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;nltk_data&lt;/code&gt; å­˜å‚¨è·¯å¾„å¯ä»¥é€šè¿‡ &lt;code&gt;nltk.data.path&lt;/code&gt; æŸ¥è¯¢ã€‚&lt;/p&gt; &#xA;&lt;p&gt;Q4: ä½¿ç”¨è¿‡ç¨‹ä¸­ Python åŒ…&lt;code&gt;nltk&lt;/code&gt;å‘ç”Ÿäº†&lt;code&gt;Resource averaged_perceptron_tagger not found.&lt;/code&gt;æŠ¥é”™ï¼Œè¯¥å¦‚ä½•è§£å†³ï¼Ÿ&lt;/p&gt; &#xA;&lt;p&gt;A4: å°† &lt;a href=&#34;https://github.com/nltk/nltk_data/raw/gh-pages/packages/taggers/averaged_perceptron_tagger.zip&#34;&gt;https://github.com/nltk/nltk_data/blob/gh-pages/packages/taggers/averaged_perceptron_tagger.zip&lt;/a&gt; ä¸‹è½½ï¼Œè§£å‹æ”¾åˆ° &lt;code&gt;nltk_data/taggers&lt;/code&gt; å­˜å‚¨è·¯å¾„ä¸‹ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;nltk_data&lt;/code&gt; å­˜å‚¨è·¯å¾„å¯ä»¥é€šè¿‡ &lt;code&gt;nltk.data.path&lt;/code&gt; æŸ¥è¯¢ã€‚&lt;/p&gt; &#xA;&lt;p&gt;Q5: æœ¬é¡¹ç›®å¯å¦åœ¨ colab ä¸­è¿è¡Œï¼Ÿ&lt;/p&gt; &#xA;&lt;p&gt;A5: å¯ä»¥å°è¯•ä½¿ç”¨ chatglm-6b-int4 æ¨¡å‹åœ¨ colab ä¸­è¿è¡Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¦‚éœ€åœ¨ colab ä¸­è¿è¡Œ Web UIï¼Œéœ€å°†&lt;code&gt;webui.py&lt;/code&gt;ä¸­&lt;code&gt;demo.queue(concurrency_count=3).launch( server_name=&#39;0.0.0.0&#39;, share=False, inbrowser=False)&lt;/code&gt;ä¸­å‚æ•°&lt;code&gt;share&lt;/code&gt;è®¾ç½®ä¸º&lt;code&gt;True&lt;/code&gt;ã€‚&lt;/p&gt; &#xA;&lt;p&gt;Q6: åœ¨ Anaconda ä¸­ä½¿ç”¨ pip å®‰è£…åŒ…æ— æ•ˆå¦‚ä½•è§£å†³ï¼Ÿ&lt;/p&gt; &#xA;&lt;p&gt;A6: æ­¤é—®é¢˜æ˜¯ç³»ç»Ÿç¯å¢ƒé—®é¢˜ï¼Œè¯¦ç»†è§ &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/docs/%E5%9C%A8Anaconda%E4%B8%AD%E4%BD%BF%E7%94%A8pip%E5%AE%89%E8%A3%85%E5%8C%85%E6%97%A0%E6%95%88%E9%97%AE%E9%A2%98.md&#34;&gt;åœ¨Anacondaä¸­ä½¿ç”¨pipå®‰è£…åŒ…æ— æ•ˆé—®é¢˜&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Q7: æœ¬é¡¹ç›®ä¸­æ‰€éœ€æ¨¡å‹å¦‚ä½•ä¸‹è½½è‡³æœ¬åœ°ï¼Ÿ&lt;/p&gt; &#xA;&lt;p&gt;A7: æœ¬é¡¹ç›®ä¸­ä½¿ç”¨çš„æ¨¡å‹å‡ä¸º&lt;code&gt;huggingface.com&lt;/code&gt;ä¸­å¯ä¸‹è½½çš„å¼€æºæ¨¡å‹ï¼Œä»¥é»˜è®¤é€‰æ‹©çš„&lt;code&gt;chatglm-6b&lt;/code&gt;å’Œ&lt;code&gt;text2vec-large-chinese&lt;/code&gt;æ¨¡å‹ä¸ºä¾‹ï¼Œä¸‹è½½æ¨¡å‹å¯æ‰§è¡Œå¦‚ä¸‹ä»£ç ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# å®‰è£… git lfs&#xA;$ git lfs install&#xA;&#xA;# ä¸‹è½½ LLM æ¨¡å‹&#xA;$ git clone https://huggingface.co/THUDM/chatglm-6b /your_path/chatglm-6b&#xA;&#xA;# ä¸‹è½½ Embedding æ¨¡å‹&#xA;$ git clone https://huggingface.co/GanymedeNil/text2vec-large-chinese /your_path/text2vec&#xA;&#xA;# æ¨¡å‹éœ€è¦æ›´æ–°æ—¶ï¼Œå¯æ‰“å¼€æ¨¡å‹æ‰€åœ¨æ–‡ä»¶å¤¹åæ‹‰å–æœ€æ–°æ¨¡å‹æ–‡ä»¶/ä»£ç &#xA;$ git pull&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Q8: &lt;code&gt;huggingface.com&lt;/code&gt;ä¸­æ¨¡å‹ä¸‹è½½é€Ÿåº¦è¾ƒæ…¢æ€ä¹ˆåŠï¼Ÿ&lt;/p&gt; &#xA;&lt;p&gt;A8: å¯ä½¿ç”¨æœ¬é¡¹ç›®ç”¨åˆ°çš„æ¨¡å‹æƒé‡æ–‡ä»¶ç™¾åº¦ç½‘ç›˜åœ°å€ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ernie-3.0-base-zh.zip é“¾æ¥: &lt;a href=&#34;https://pan.baidu.com/s/1CIvKnD3qzE-orFouA8qvNQ?pwd=4wih&#34;&gt;https://pan.baidu.com/s/1CIvKnD3qzE-orFouA8qvNQ?pwd=4wih&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ernie-3.0-nano-zh.zip é“¾æ¥: &lt;a href=&#34;https://pan.baidu.com/s/1Fh8fgzVdavf5P1omAJJ-Zw?pwd=q6s5&#34;&gt;https://pan.baidu.com/s/1Fh8fgzVdavf5P1omAJJ-Zw?pwd=q6s5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;text2vec-large-chinese.zip é“¾æ¥: &lt;a href=&#34;https://pan.baidu.com/s/1sMyPzBIXdEzHygftEoyBuA?pwd=4xs7&#34;&gt;https://pan.baidu.com/s/1sMyPzBIXdEzHygftEoyBuA?pwd=4xs7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;chatglm-6b-int4-qe.zip é“¾æ¥: &lt;a href=&#34;https://pan.baidu.com/s/1DDKMOMHtNZccOOBGWIOYww?pwd=22ji&#34;&gt;https://pan.baidu.com/s/1DDKMOMHtNZccOOBGWIOYww?pwd=22ji&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;chatglm-6b-int4.zip é“¾æ¥: &lt;a href=&#34;https://pan.baidu.com/s/1pvZ6pMzovjhkA6uPcRLuJA?pwd=3gjd&#34;&gt;https://pan.baidu.com/s/1pvZ6pMzovjhkA6uPcRLuJA?pwd=3gjd&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;chatglm-6b.zip é“¾æ¥: &lt;a href=&#34;https://pan.baidu.com/s/1B-MpsVVs1GHhteVBetaquw?pwd=djay&#34;&gt;https://pan.baidu.com/s/1B-MpsVVs1GHhteVBetaquw?pwd=djay&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Q9: ä¸‹è½½å®Œæ¨¡å‹åï¼Œå¦‚ä½•ä¿®æ”¹ä»£ç ä»¥æ‰§è¡Œæœ¬åœ°æ¨¡å‹ï¼Ÿ&lt;/p&gt; &#xA;&lt;p&gt;A9: æ¨¡å‹ä¸‹è½½å®Œæˆåï¼Œè¯·åœ¨ &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/configs/model_config.py&#34;&gt;configs/model_config.py&lt;/a&gt; æ–‡ä»¶ä¸­ï¼Œå¯¹&lt;code&gt;embedding_model_dict&lt;/code&gt;å’Œ&lt;code&gt;llm_model_dict&lt;/code&gt;å‚æ•°è¿›è¡Œä¿®æ”¹ï¼Œå¦‚æŠŠ&lt;code&gt;llm_model_dict&lt;/code&gt;ä»&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;embedding_model_dict = {&#xA;    &#34;ernie-tiny&#34;: &#34;nghuyong/ernie-3.0-nano-zh&#34;,&#xA;    &#34;ernie-base&#34;: &#34;nghuyong/ernie-3.0-base-zh&#34;,&#xA;    &#34;text2vec&#34;: &#34;GanymedeNil/text2vec-large-chinese&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ä¿®æ”¹ä¸º&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;embedding_model_dict = {&#xA;                        &#34;ernie-tiny&#34;: &#34;nghuyong/ernie-3.0-nano-zh&#34;,&#xA;                        &#34;ernie-base&#34;: &#34;nghuyong/ernie-3.0-base-zh&#34;,&#xA;                        &#34;text2vec&#34;: &#34;/Users/liuqian/Downloads/ChatGLM-6B/text2vec-large-chinese&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;DEMO&lt;/h2&gt; &#xA;&lt;p&gt;ä»¥é—®é¢˜&lt;code&gt;chatglm-6b çš„å±€é™æ€§å…·ä½“ä½“ç°åœ¨å“ªé‡Œï¼Œå¦‚ä½•å®ç°æ”¹è¿›&lt;/code&gt;ä¸ºä¾‹&lt;/p&gt; &#xA;&lt;p&gt;æœªä½¿ç”¨ langchain æ¥å…¥æœ¬åœ°æ–‡æ¡£æ—¶ï¼š&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ChatGLM-6B æ˜¯ä¸€ä¸ªåŸºäº GPT-3 çš„èŠå¤©æœºå™¨äºº,å®ƒçš„å±€é™æ€§ä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;è¯­è¨€æ¨¡å‹çš„ç²¾åº¦:GPT-3 æ˜¯ä¸€ç§å¤§å‹çš„è¯­è¨€æ¨¡å‹,å®ƒçš„ç²¾åº¦æ¯”è¾ƒé«˜,ä½†æ˜¯ä»ç„¶æœ‰ä¸€äº›å±€é™æ€§ã€‚ä¾‹å¦‚,å®ƒå¯èƒ½æ— æ³•å‡†ç¡®åœ°ç†è§£æŸäº›æ–¹è¨€ã€å£éŸ³æˆ–è€…å¤æ‚çš„å¥å­ç»“æ„ã€‚&lt;/li&gt; &#xA;  &lt;li&gt;çŸ¥è¯†å›¾è°±çš„å®Œæ•´æ€§:GPT-3 å¯ä»¥ç”Ÿæˆä¸€äº›çŸ¥è¯†å›¾è°±,ä½†æ˜¯å…¶å®Œæ•´æ€§å’Œå‡†ç¡®æ€§å¯èƒ½ä¸å¤Ÿã€‚ä¾‹å¦‚,å®ƒå¯èƒ½æ— æ³•å‡†ç¡®åœ°ç†è§£æŸäº›é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†å’Œæœ¯è¯­ã€‚&lt;/li&gt; &#xA;  &lt;li&gt;å¯¹è¯ç”Ÿæˆçš„è¿è´¯æ€§:GPT-3 å¯ä»¥ç”Ÿæˆä¸€äº›è¿è´¯çš„å¯¹è¯,ä½†æ˜¯å…¶ç”Ÿæˆçš„å¯¹è¯å¯èƒ½ä¸å¤Ÿè‡ªç„¶ã€‚ä¾‹å¦‚,å®ƒå¯èƒ½ä¼šç”Ÿæˆä¸€äº›è™šå‡çš„ã€ä¸è¿è´¯çš„å¯¹è¯æˆ–è€…è¯¯è§£ç”¨æˆ·çš„æ„å›¾ã€‚&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;ä¸ºäº†æ”¹è¿› ChatGLM-6B,å¯ä»¥è€ƒè™‘ä»¥ä¸‹å‡ ä¸ªæ–¹é¢:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;æé«˜è¯­è¨€æ¨¡å‹çš„ç²¾åº¦:å¯ä»¥é€šè¿‡å¢åŠ è®­ç»ƒæ•°æ®ã€ä½¿ç”¨æ›´å¥½çš„è¯­è¨€æ¨¡å‹æ¶æ„ã€ä¼˜åŒ–æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ç­‰æ–¹å¼æ¥æé«˜è¯­è¨€æ¨¡å‹çš„ç²¾åº¦ã€‚&lt;/li&gt; &#xA;  &lt;li&gt;å¢å¼ºçŸ¥è¯†å›¾è°±çš„å®Œæ•´æ€§:å¯ä»¥é€šè¿‡å¢åŠ çŸ¥è¯†å›¾è°±çš„ç»´åº¦ã€ä½¿ç”¨æ›´å¥½çš„çŸ¥è¯†å›¾è°±æ„å»ºæ–¹æ³•ã€ä¼˜åŒ–çŸ¥è¯†å›¾è°±çš„åŒ¹é…ç®—æ³•ç­‰æ–¹å¼æ¥å¢å¼ºçŸ¥è¯†å›¾è°±çš„å®Œæ•´æ€§ã€‚&lt;/li&gt; &#xA;  &lt;li&gt;æé«˜å¯¹è¯ç”Ÿæˆçš„è¿è´¯æ€§:å¯ä»¥é€šè¿‡å¢åŠ å¯¹è¯ç”Ÿæˆæ¨¡å‹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€ä½¿ç”¨æ›´å¥½çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æå–æ–¹æ³•ã€ä¼˜åŒ–å¯¹è¯ç”Ÿæˆæ¨¡å‹çš„ç®—æ³•ç­‰æ–¹å¼æ¥æé«˜å¯¹è¯ç”Ÿæˆçš„è¿è´¯æ€§ã€‚&lt;/li&gt; &#xA;  &lt;li&gt;å¼•å…¥æ›´å¤šçš„äººç±»åé¦ˆ:äººç±»åé¦ˆå¯ä»¥å¸®åŠ© ChatGLM-6B æ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„æ„å›¾å’Œéœ€æ±‚,ä»è€Œæé«˜å¯¹è¯ç”Ÿæˆçš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚å¯ä»¥é€šè¿‡å¼•å…¥æ›´å¤šçš„äººç±»å®¢æœã€ä½¿ç”¨äººç±»åé¦ˆæ”¶é›†å™¨ç­‰æ–¹å¼æ¥å¼•å…¥æ›´å¤šçš„äººç±»åé¦ˆã€‚&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;ä½¿ç”¨ langchain æ¥å…¥ ChatGLM-6B é¡¹ç›®çš„ README.md æ–‡ä»¶å ChatGLM çš„å›ç­”ï¼š&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ChatGLM-6B æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹,å®ƒåœ¨å›ç­”é—®é¢˜æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä½†æ˜¯,å®ƒä¹Ÿæœ‰ä¸€äº›å±€é™æ€§,å…·ä½“å¦‚ä¸‹:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;è¯­è¨€ç†è§£æœ‰é™:ChatGLM-6B åªå­¦ä¹ äº†ä¸­æ–‡è‡ªç„¶è¯­è¨€,å¯¹äºå…¶ä»–è¯­è¨€å¯èƒ½ä¼šå­˜åœ¨ç†è§£ä¸Šçš„å±€é™æ€§ã€‚&lt;/li&gt; &#xA;  &lt;li&gt;çŸ¥è¯†å‚¨å¤‡ä¸è¶³:ChatGLM-6B çš„è®­ç»ƒæ•°æ®é›†åªåŒ…å«äº†ä¸­æ–‡è¯­æ–™,å› æ­¤å®ƒå¯èƒ½æ— æ³•å›ç­”ä¸€äº›éä¸­æ–‡çš„é—®é¢˜æˆ–è€…æŸäº›ç‰¹å®šé¢†åŸŸçš„é—®é¢˜ã€‚&lt;/li&gt; &#xA;  &lt;li&gt;æ•°æ®é‡æœ‰é™:ChatGLM-6B çš„è®­ç»ƒæ•°æ®é›†åªæœ‰å‡ ç™¾ä¸‡æ¡è®°å½•,å› æ­¤å®ƒå¯èƒ½æ— æ³•å›ç­”ä¸€äº›éå¸¸å…·ä½“æˆ–è€…å¤æ‚çš„é—®é¢˜ã€‚&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;ä¸ºäº†æ”¹è¿› ChatGLM-6B,å¯ä»¥è€ƒè™‘ä»¥ä¸‹å‡ ä¸ªæ–¹é¢:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;å­¦ä¹ æ›´å¤šçš„è¯­è¨€çŸ¥è¯†:å¯ä»¥å­¦ä¹ å…¶ä»–è¯­è¨€çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯,æ‰©å¤§è¯­è¨€ç†è§£çš„èŒƒå›´ã€‚&lt;/li&gt; &#xA;  &lt;li&gt;æ‰©å¤§çŸ¥è¯†å‚¨å¤‡:å¯ä»¥æ”¶é›†æ›´å¤šçš„ä¸­æ–‡è¯­æ–™,æˆ–è€…ä½¿ç”¨å…¶ä»–è¯­è¨€çš„æ•°æ®é›†æ¥æ‰©å……çŸ¥è¯†å‚¨å¤‡ã€‚&lt;/li&gt; &#xA;  &lt;li&gt;å¢åŠ æ•°æ®é‡:å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†æ¥è®­ç»ƒ ChatGLM-6B,æé«˜æ¨¡å‹çš„è¡¨ç°ã€‚&lt;/li&gt; &#xA;  &lt;li&gt;å¼•å…¥æ›´å¤šçš„è¯„ä¼°æŒ‡æ ‡:å¯ä»¥å¼•å…¥æ›´å¤šçš„è¯„ä¼°æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çš„è¡¨ç°,ä»è€Œå‘ç° ChatGLM-6B å­˜åœ¨çš„ä¸è¶³å’Œå±€é™æ€§ã€‚&lt;/li&gt; &#xA;  &lt;li&gt;æ”¹è¿›æ¨¡å‹æ¶æ„:å¯ä»¥æ”¹è¿› ChatGLM-6B çš„æ¨¡å‹æ¶æ„,æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œè¡¨ç°ã€‚ä¾‹å¦‚,å¯ä»¥ä½¿ç”¨æ›´å¤§çš„ç¥ç»ç½‘ç»œæˆ–è€…æ”¹è¿›çš„å·ç§¯ç¥ç»ç½‘ç»œç»“æ„ã€‚&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;è·¯çº¿å›¾&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; å®ç° langchain + ChatGLM-6B æœ¬åœ°çŸ¥è¯†åº”ç”¨&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; åŸºäº langchain å®ç°éç»“æ„åŒ–æ–‡ä»¶æ¥å…¥ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; .md&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; .pdf(éœ€è¦æŒ‰ç…§å¸¸è§é—®é¢˜ Q2 ä¸­æè¿°è¿›è¡Œ&lt;code&gt;detectron2&lt;/code&gt;çš„å®‰è£…)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; .docx&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; .txt&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; æœç´¢å¼•æ“ä¸æœ¬åœ°ç½‘é¡µ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; å¢åŠ æ›´å¤š LLM æ¨¡å‹æ”¯æŒ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; THUDM/chatglm-6b&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; THUDM/chatglm-6b-int4&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; THUDM/chatglm-6b-int4-qe&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; å¢åŠ  Web UI DEMO &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; åˆ©ç”¨ gradio å®ç° Web UI DEMO&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ·»åŠ è¾“å‡ºå†…å®¹åŠé”™è¯¯æç¤º&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; å¼•ç”¨æ ‡æ³¨&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; åˆ©ç”¨ fastapi å®ç° API éƒ¨ç½²æ–¹å¼ï¼Œå¹¶å®ç°è°ƒç”¨ API çš„ web ui DEMO&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;é¡¹ç›®äº¤æµç¾¤&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/img/qr_code.jpg&#34; alt=&#34;äºŒç»´ç &#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ğŸ‰ langchain-ChatGLM é¡¹ç›®äº¤æµç¾¤ï¼Œå¦‚æœä½ ä¹Ÿå¯¹æœ¬é¡¹ç›®æ„Ÿå…´è¶£ï¼Œæ¬¢è¿åŠ å…¥ç¾¤èŠå‚ä¸è®¨è®ºäº¤æµã€‚&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yoheinakajima/babyagi</title>
    <updated>2023-04-16T01:49:42Z</updated>
    <id>tag:github.com,2023-04-16:/yoheinakajima/babyagi</id>
    <link href="https://github.com/yoheinakajima/babyagi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Translations:&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-fr.md&#34;&gt;&lt;img title=&#34;FranÃ§ais&#34; alt=&#34;FranÃ§ais&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/fr.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-pt-br.md&#34;&gt;&lt;img title=&#34;Portuguese&#34; alt=&#34;Portuguese&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/br.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-ro.md&#34;&gt;&lt;img title=&#34;Romanian&#34; alt=&#34;Romanian&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/ro.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-ru.md&#34;&gt;&lt;img title=&#34;Russian&#34; alt=&#34;Russian&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/ru.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-si.md&#34;&gt;&lt;img title=&#34;Slovenian&#34; alt=&#34;Slovenian&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/si.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-es.md&#34;&gt;&lt;img title=&#34;Spanish&#34; alt=&#34;Spanish&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/es.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-tr.md&#34;&gt;&lt;img title=&#34;Turkish&#34; alt=&#34;Turkish&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/tr.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-ua.md&#34;&gt;&lt;img title=&#34;Ukrainian&#34; alt=&#34;Ukrainian&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/ua.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-cn.md&#34;&gt;&lt;img title=&#34;ç®€ä½“ä¸­æ–‡&#34; alt=&#34;Simplified Chinese&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/cn.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-zh-tw.md&#34;&gt;&lt;img title=&#34;ç¹é«”ä¸­æ–‡ (Traditional Chinese)&#34; alt=&#34;ç¹é«”ä¸­æ–‡ (Traditional Chinese)&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/tw.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Objective&lt;/h1&gt; &#xA;&lt;p&gt;This Python script is an example of an AI-powered task management system. The system uses OpenAI and Pinecone APIs to create, prioritize, and execute tasks. The main idea behind this system is that it creates tasks based on the result of previous tasks and a predefined objective. The script then uses OpenAI&#39;s natural language processing (NLP) capabilities to create new tasks based on the objective, and Pinecone to store and retrieve task results for context. This is a pared-down version of the original &lt;a href=&#34;https://twitter.com/yoheinakajima/status/1640934493489070080?s=20&#34;&gt;Task-Driven Autonomous Agent&lt;/a&gt; (Mar 28, 2023).&lt;/p&gt; &#xA;&lt;p&gt;This README will cover the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/#how-it-works&#34;&gt;How the script works&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/#how-to-use&#34;&gt;How to use the script&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/#supported-models&#34;&gt;Supported Models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/#continous-script-warning&#34;&gt;Warning about running the script continuously&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;How It Works&lt;a name=&#34;how-it-works&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;The script works by running an infinite loop that does the following steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pulls the first task from the task list.&lt;/li&gt; &#xA; &lt;li&gt;Sends the task to the execution agent, which uses OpenAI&#39;s API to complete the task based on the context.&lt;/li&gt; &#xA; &lt;li&gt;Enriches the result and stores it in Pinecone.&lt;/li&gt; &#xA; &lt;li&gt;Creates new tasks and reprioritizes the task list based on the objective and the result of the previous task. &lt;br&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The execution_agent() function is where the OpenAI API is used. It takes two parameters: the objective and the task. It then sends a prompt to OpenAI&#39;s API, which returns the result of the task. The prompt consists of a description of the AI system&#39;s task, the objective, and the task itself. The result is then returned as a string. &lt;br&gt; The task_creation_agent() function is where OpenAI&#39;s API is used to create new tasks based on the objective and the result of the previous task. The function takes four parameters: the objective, the result of the previous task, the task description, and the current task list. It then sends a prompt to OpenAI&#39;s API, which returns a list of new tasks as strings. The function then returns the new tasks as a list of dictionaries, where each dictionary contains the name of the task. &lt;br&gt; The prioritization_agent() function is where OpenAI&#39;s API is used to reprioritize the task list. The function takes one parameter, the ID of the current task. It sends a prompt to OpenAI&#39;s API, which returns the reprioritized task list as a numbered list.&lt;/p&gt; &#xA;&lt;p&gt;Finally, the script uses Pinecone to store and retrieve task results for context. The script creates a Pinecone index based on the table name specified in the YOUR_TABLE_NAME variable. Pinecone is then used to store the results of the task in the index, along with the task name and any additional metadata.&lt;/p&gt; &#xA;&lt;h1&gt;How to Use&lt;a name=&#34;how-to-use&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;To use the script, you will need to follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository via &lt;code&gt;git clone https://github.com/yoheinakajima/babyagi.git&lt;/code&gt; and &lt;code&gt;cd&lt;/code&gt; into the cloned repository.&lt;/li&gt; &#xA; &lt;li&gt;Install the required packages: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copy the .env.example file to .env: &lt;code&gt;cp .env.example .env&lt;/code&gt;. This is where you will set the following variables.&lt;/li&gt; &#xA; &lt;li&gt;Set your OpenAI and Pinecone API keys in the OPENAI_API_KEY, OPENAPI_API_MODEL, and PINECONE_API_KEY variables.&lt;/li&gt; &#xA; &lt;li&gt;Set the Pinecone environment in the PINECONE_ENVIRONMENT variable.&lt;/li&gt; &#xA; &lt;li&gt;Set the name of the table where the task results will be stored in the TABLE_NAME variable.&lt;/li&gt; &#xA; &lt;li&gt;(Optional) Set the objective of the task management system in the OBJECTIVE variable.&lt;/li&gt; &#xA; &lt;li&gt;(Optional) Set the first task of the system in the INITIAL_TASK variable.&lt;/li&gt; &#xA; &lt;li&gt;Run the script.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;All optional values above can also be specified on the command line.&lt;/p&gt; &#xA;&lt;h1&gt;Running inside a docker container&lt;/h1&gt; &#xA;&lt;p&gt;As a prerequisite, you will need docker and docker-compose installed. Docker desktop is the simplest option &lt;a href=&#34;https://www.docker.com/products/docker-desktop/&#34;&gt;https://www.docker.com/products/docker-desktop/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run the system inside a docker container, setup your .env file as per steps above and then run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Supported Models&lt;a name=&#34;supported-models&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This script works with all OpenAI models, as well as Llama through Llama.cpp. Default model is &lt;strong&gt;gpt-3.5-turbo&lt;/strong&gt;. To use a different model, specify it through OPENAI_API_MODEL or use the command line.&lt;/p&gt; &#xA;&lt;h2&gt;Llama&lt;/h2&gt; &#xA;&lt;p&gt;Download the latest version of &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;Llama.cpp&lt;/a&gt; and follow instructions to make it. You will also need the Llama model weights.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Under no circumstances share IPFS, magnet links, or any other links to model downloads anywhere in this repository, including in issues, discussions or pull requests. They will be immediately deleted.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After that link &lt;code&gt;llama/main&lt;/code&gt; to llama.cpp/main and &lt;code&gt;models&lt;/code&gt; to the folder where you have the Llama model weights. Then run the script with &lt;code&gt;OPENAI_API_MODEL=llama&lt;/code&gt; or &lt;code&gt;-l&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;h1&gt;Warning&lt;a name=&#34;continous-script-warning&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This script is designed to be run continuously as part of a task management system. Running this script continuously can result in high API usage, so please use it responsibly. Additionally, the script requires the OpenAI and Pinecone APIs to be set up correctly, so make sure you have set up the APIs before running the script.&lt;/p&gt; &#xA;&lt;h1&gt;Contribution&lt;/h1&gt; &#xA;&lt;p&gt;Needless to say, BabyAGI is still in its infancy and thus we are still determining its direction and the steps to get there. Currently, a key design goal for BabyAGI is to be &lt;em&gt;simple&lt;/em&gt; such that it&#39;s easy to understand and build upon. To maintain this simplicity, we kindly request that you adhere to the following guidelines when submitting PRs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Focus on small, modular modifications rather than extensive refactoring.&lt;/li&gt; &#xA; &lt;li&gt;When introducing new features, provide a detailed description of the specific use case you are addressing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A note from @yoheinakajima (Apr 5th, 2023):&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;I know there are a growing number of PRs, appreciate your patience - as I am both new to GitHub/OpenSource, and did not plan my time availability accordingly this week. Re:direction, I&#39;ve been torn on keeping it simple vs expanding - currently leaning towards keeping a core Baby AGI simple, and using this as a platform to support and promote different approaches to expanding this (eg. BabyAGIxLangchain as one direction). I believe there are various opinionated approaches that are worth exploring, and I see value in having a central place to compare and discuss. More updates coming shortly.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;I am new to GitHub and open source, so please be patient as I learn to manage this project properly. I run a VC firm by day, so I will generally be checking PRs and issues at night after I get my kids down - which may not be every night. Open to the idea of bringing in support, will be updating this section soon (expectations, visions, etc). Talking to lots of people and learning - hang tight for updates!&lt;/p&gt; &#xA;&lt;h1&gt;Backstory&lt;/h1&gt; &#xA;&lt;p&gt;BabyAGI is a pared-down version of the original &lt;a href=&#34;https://twitter.com/yoheinakajima/status/1640934493489070080?s=20&#34;&gt;Task-Driven Autonomous Agent&lt;/a&gt; (Mar 28, 2023) shared on Twitter. This version is down to 140 lines: 13 comments, 22 blanks, and 105 code. The name of the repo came up in the reaction to the original autonomous agent - the author does not mean to imply that this is AGI.&lt;/p&gt; &#xA;&lt;p&gt;Made with love by &lt;a href=&#34;https://twitter.com/yoheinakajima&#34;&gt;@yoheinakajima&lt;/a&gt;, who happens to be a VC (would love to see what you&#39;re building!)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databrickslabs/dolly</title>
    <updated>2023-04-16T01:49:42Z</updated>
    <id>tag:github.com,2023-04-16:/databrickslabs/dolly</id>
    <link href="https://github.com/databrickslabs/dolly" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Databricksâ€™ Dolly, a large language model trained on the Databricks Machine Learning Platform&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dolly&lt;/h1&gt; &#xA;&lt;p&gt;Databricksâ€™ &lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-12b&#34;&gt;Dolly&lt;/a&gt; is an instruction-following large language model trained on the Databricks machine learning platform that is licensed for commercial use. Based on &lt;code&gt;pythia-12b&lt;/code&gt;, Dolly is trained on ~15k instruction/response fine tuning records &lt;a href=&#34;https://github.com/databrickslabs/dolly/tree/master/data&#34;&gt;&lt;code&gt;databricks-dolly-15k&lt;/code&gt;&lt;/a&gt; generated by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA and summarization. &lt;code&gt;dolly-v2-12b&lt;/code&gt; is not a state-of-the-art model, but does exhibit surprisingly high quality instruction following behavior not characteristic of the foundation model on which it is based.&lt;/p&gt; &#xA;&lt;p&gt;Databricks is committed to ensuring that every organization and individual benefits from the transformative power of artificial intelligence. The Dolly model family represents our first steps along this journey, and weâ€™re excited to share this technology with the world.&lt;/p&gt; &#xA;&lt;p&gt;The model is available on Hugging Face as &lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-12b&#34;&gt;databricks/dolly-v2-12b&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Model Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;dolly-v2-12b&lt;/code&gt; is a 12 billion parameter causal language model created by &lt;a href=&#34;https://databricks.com/&#34;&gt;Databricks&lt;/a&gt; that is derived from &lt;a href=&#34;https://www.eleuther.ai/&#34;&gt;EleutherAIâ€™s&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-12b&#34;&gt;Pythia-12b&lt;/a&gt; and fine-tuned on a &lt;a href=&#34;https://github.com/databrickslabs/dolly/tree/master/data&#34;&gt;~15K record instruction corpus&lt;/a&gt; generated by Databricks employees and released under a permissive license (CC-BY-SA)&lt;/p&gt; &#xA;&lt;h2&gt;Known Limitations&lt;/h2&gt; &#xA;&lt;h3&gt;Performance Limitations&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;dolly-v2-12b&lt;/code&gt; is not a state-of-the-art generative language model&lt;/strong&gt; and, though quantitative benchmarking is ongoing, is not designed to perform competitively with more modern model architectures or models subject to larger pretraining corpuses.&lt;/p&gt; &#xA;&lt;p&gt;The Dolly model family is under active development, and so any list of shortcomings is unlikely to be exhaustive, but we include known limitations and misfires here as a means to document and share our preliminary findings with the community. In particular, &lt;code&gt;dolly-v2-12b&lt;/code&gt; struggles with: syntactically complex prompts, programming problems, mathematical operations, factual errors, dates and times, open-ended question answering, hallucination, enumerating lists of specific length, stylistic mimicry, having a sense of humor, etc. Moreover, we find that &lt;code&gt;dolly-v2-12b&lt;/code&gt; does not have some capabilities, such as well-formatted letter writing, present in the original model.&lt;/p&gt; &#xA;&lt;h3&gt;Dataset Limitations&lt;/h3&gt; &#xA;&lt;p&gt;Like all language models, &lt;code&gt;dolly-v2-12b&lt;/code&gt; reflects the content and limitations of its training corpuses.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;The Pile&lt;/strong&gt;: GPT-Jâ€™s pre-training corpus contains content mostly collected from the public internet, and like most web-scale datasets, it contains content many users would find objectionable. As such, the model is likely to reflect these shortcomings, potentially overtly in the case it is explicitly asked to produce objectionable content, and sometimes subtly, as in the case of biased or harmful implicit associations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;databricks-dolly-15k&lt;/code&gt;&lt;/strong&gt;: The training data on which &lt;code&gt;dolly-v2-12b&lt;/code&gt; is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization. To our knowledge it does not contain obscenity, intellectual property or personally identifying information about non-public figures, but it may contain typos and factual errors. The dataset may also reflect biases found in Wikipedia. Finally, the dataset likely reflects the interests and semantic choices of Databricks employees, a demographic which is not representative of the global population at large.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Databricks is committed to ongoing research and development efforts to develop helpful, honest and harmless AI technologies that maximize the potential of all individuals and organizations.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started with Response Generation&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to simply test the model without training, the model is available on Hugging Face as &lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-12b&#34;&gt;databricks/dolly-v2-12b&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To use the model with the &lt;code&gt;transformers&lt;/code&gt; library on a machine with GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from transformers import pipeline&#xA;&#xA;instruct_pipeline = pipeline(model=&#34;databricks/dolly-v2-12b&#34;, trust_remote_code=True, device_map=&#34;auto&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then use the pipeline to answer instructions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;instruct_pipeline(&#34;Explain to me the difference between nuclear fission and fusion.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To reduce memory usage you can load the model with &lt;code&gt;bfloat16&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;from transformers import pipeline&#xA;&#xA;instruct_pipeline = pipeline(model=&#34;databricks/dolly-v2-12b&#34;, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=&#34;auto&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started with Training&lt;/h2&gt; &#xA;&lt;p&gt;The following instructions refer to Dolly v1 and still need to be updated for v2 training.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add the &lt;code&gt;dolly&lt;/code&gt; repo to Databricks (under Repos click Add Repo, enter &lt;code&gt;https://github.com/databrickslabs/dolly.git&lt;/code&gt;, then click Create Repo).&lt;/li&gt; &#xA; &lt;li&gt;Start a &lt;code&gt;12.2 LTS ML (includes Apache Spark 3.3.2, GPU, Scala 2.12)&lt;/code&gt; single-node cluster with node type having 8 A100 GPUs (e.g. &lt;code&gt;Standard_ND96asr_v4&lt;/code&gt; or &lt;code&gt;p4d.24xlarge&lt;/code&gt;). Note that these instance types may not be available in all regions, or may be difficult to provision. In Databricks, note that you must select the GPU runtime first, and unselect &#34;Use Photon&#34;, for these instance types to appear (where supported).&lt;/li&gt; &#xA; &lt;li&gt;Open the &lt;code&gt;train_dolly&lt;/code&gt; notebook in the Repo (which is the &lt;code&gt;train_dolly.py&lt;/code&gt; file in the Github &lt;code&gt;dolly&lt;/code&gt; repo), attach to your GPU cluster, and run all cells. When training finishes, the notebook will save the model under &lt;code&gt;/dbfs/dolly_training&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Training on Other Instances&lt;/h2&gt; &#xA;&lt;p&gt;A100 instance types are not available in all cloud regions, or can be hard to provision. Training is possible on other GPU instance types, with small modifications to reduce memory usage. Training will take longer on these instances. These modifications are not necessarily optimal, but are simple to make.&lt;/p&gt; &#xA;&lt;h3&gt;A10 GPUs&lt;/h3&gt; &#xA;&lt;p&gt;To run on A10 instances (ex: &lt;code&gt;g5.24xlarge&lt;/code&gt;, 4 x A10 24GB; &lt;code&gt;Standard_NV72ads_A10_v5&lt;/code&gt;, 2 x A10), make the following changes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modify the deepspeed config file &lt;code&gt;ds_z3_bf16_config.json&lt;/code&gt; to configure optimizer offload. Within the &lt;code&gt;&#34;zero_optimization&#34;&lt;/code&gt; section, add: &lt;pre&gt;&lt;code&gt;&#34;offload_optimizer&#34;: {&#xA;  &#34;device&#34;: &#34;cpu&#34;,&#xA;  &#34;pin_memory&#34;: true&#xA;},&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Set the &lt;code&gt;num_gpus&lt;/code&gt; widget in &lt;code&gt;train_dolly&lt;/code&gt; to the number of GPUs in your instance, such as 2 or 4, before running&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;With 4 A10s, an epoch completes in about 7 hours.&lt;/p&gt; &#xA;&lt;h3&gt;V100 GPUs&lt;/h3&gt; &#xA;&lt;p&gt;To run on V100 instances with 32GB of GPU memory (ex: &lt;code&gt;p3dn.24xlarge&lt;/code&gt; or &lt;code&gt;Standard_ND40rs_v2&lt;/code&gt;), make the following changes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modify the deepspeed config to enable optimizer offload, as above&lt;/li&gt; &#xA; &lt;li&gt;Modify &lt;code&gt;trainer.py&lt;/code&gt; to disable &lt;code&gt;bf16&lt;/code&gt; and enable &lt;code&gt;fp16&lt;/code&gt; in &lt;code&gt;TrainingArguments&lt;/code&gt;: &lt;pre&gt;&lt;code&gt;...&#xA;fp16=True,&#xA;bf16=False,&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Set the &lt;code&gt;num_gpus&lt;/code&gt; widget in &lt;code&gt;train_dolly&lt;/code&gt; to the number of GPUs in your instance, typically 8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;With 8 V100s, an epoch completes in about 3.5 hours. Note that the resulting model may be slightly different when trained with &lt;code&gt;fp16&lt;/code&gt; versus &lt;code&gt;bf16&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Running Unit Tests Locally&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pyenv local 3.8.13&#xA;python -m venv .venv&#xA;. .venv/bin/activate&#xA;pip install -r requirements_dev.txt&#xA;./run_pytest.sh&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>