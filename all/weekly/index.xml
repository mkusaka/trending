<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-05T02:02:48Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>protocolbuffers/protobuf</title>
    <updated>2022-06-05T02:02:48Z</updated>
    <id>tag:github.com,2022-06-05:/protocolbuffers/protobuf</id>
    <link href="https://github.com/protocolbuffers/protobuf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/h1&gt; &#xA;&lt;p&gt;Copyright 2008 Google Inc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;https://developers.google.com/protocol-buffers/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Protocol Buffers (a.k.a., protobuf) are Google&#39;s language-neutral, platform-neutral, extensible mechanism for serializing structured data. You can find &lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;protobuf&#39;s documentation on the Google Developers site&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This README file contains protobuf installation instructions. To install protobuf, you need to install the protocol compiler (used to compile .proto files) and the protobuf runtime for your chosen programming language.&lt;/p&gt; &#xA;&lt;h2&gt;Protocol Compiler Installation&lt;/h2&gt; &#xA;&lt;p&gt;The protocol compiler is written in C++. If you are using C++, please follow the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&#34;&gt;C++ Installation Instructions&lt;/a&gt; to install protoc along with the C++ runtime.&lt;/p&gt; &#xA;&lt;p&gt;For non-C++ users, the simplest way to install the protocol compiler is to download a pre-built binary from our release page:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf/releases&#34;&gt;https://github.com/protocolbuffers/protobuf/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the downloads section of each release, you can find pre-built binaries in zip packages: protoc-$VERSION-$PLATFORM.zip. It contains the protoc binary as well as a set of standard .proto files distributed along with protobuf.&lt;/p&gt; &#xA;&lt;p&gt;If you are looking for an old version that is not available in the release page, check out the maven repo here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://repo1.maven.org/maven2/com/google/protobuf/protoc/&#34;&gt;https://repo1.maven.org/maven2/com/google/protobuf/protoc/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;These pre-built binaries are only provided for released versions. If you want to use the github main version at HEAD, or you need to modify protobuf code, or you are using C++, it&#39;s recommended to build your own protoc binary from source.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to build protoc binary from source, see the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&#34;&gt;C++ Installation Instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Protobuf Runtime Installation&lt;/h2&gt; &#xA;&lt;p&gt;Protobuf supports several different programming languages. For each programming language, you can find instructions in the corresponding source directory about how to install protobuf runtime for that specific language:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C++ (include C++ runtime and protoc)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src&#34;&gt;src&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Java&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/java&#34;&gt;java&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/python&#34;&gt;python&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Objective-C&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/objectivec&#34;&gt;objectivec&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C#&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/csharp&#34;&gt;csharp&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ruby&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/ruby&#34;&gt;ruby&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Go&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf-go&#34;&gt;protocolbuffers/protobuf-go&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PHP&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/php&#34;&gt;php&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dart&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dart-lang/protobuf&#34;&gt;dart-lang/protobuf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The best way to learn how to use protobuf is to follow the tutorials in our developer guide:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/tutorials&#34;&gt;https://developers.google.com/protocol-buffers/docs/tutorials&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to learn from code examples, take a look at the examples in the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The complete documentation for Protocol Buffers is available via the web at:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;https://developers.google.com/protocol-buffers/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lucidrains/imagen-pytorch</title>
    <updated>2022-06-05T02:02:48Z</updated>
    <id>tag:github.com,2022-06-05:/lucidrains/imagen-pytorch</id>
    <link href="https://github.com/lucidrains/imagen-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Imagen, Google&#39;s Text-to-Image Neural Network, in Pytorch&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/imagen-pytorch/main/imagen.png&#34; width=&#34;450px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Imagen - Pytorch (wip)&lt;/h2&gt; &#xA;&lt;p&gt;Implementation of &lt;a href=&#34;https://gweb-research-imagen.appspot.com/&#34;&gt;Imagen&lt;/a&gt;, Google&#39;s Text-to-Image Neural Network that beats DALL-E2, in Pytorch. It is the new SOTA for text-to-image synthesis.&lt;/p&gt; &#xA;&lt;p&gt;Architecturally, it is actually much simpler than DALL-E2. It consists of a cascading DDPM conditioned on text embeddings from a large pretrained T5 model (attention network). It also contains dynamic clipping for improved classifier free guidance, noise level conditioning, and a memory efficient unet design.&lt;/p&gt; &#xA;&lt;p&gt;It appears neither CLIP nor prior network is needed after all. And so research continues.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xqDeAz0U-R4&#34;&gt;AI Coffee Break with Letitia&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please join &lt;a href=&#34;https://discord.gg/xBPBXfcFHd&#34;&gt;&lt;img alt=&#34;Join us on Discord&#34; src=&#34;https://img.shields.io/discord/823813159592001537?color=5865F2&amp;amp;logo=discord&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt; if you are interested in helping out with the replication with the &lt;a href=&#34;https://laion.ai/&#34;&gt;LAION&lt;/a&gt; community&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install imagen-pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from imagen_pytorch import Unet, Imagen&#xA;&#xA;# unet for imagen&#xA;&#xA;unet1 = Unet(&#xA;    dim = 32,&#xA;    cond_dim = 512,&#xA;    dim_mults = (1, 2, 4, 8),&#xA;    num_resnet_blocks = 3,&#xA;    layer_attns = (False, True, True, True),&#xA;    layer_cross_attns = (False, True, True, True)&#xA;)&#xA;&#xA;unet2 = Unet(&#xA;    dim = 32,&#xA;    cond_dim = 512,&#xA;    dim_mults = (1, 2, 4, 8),&#xA;    num_resnet_blocks = (2, 4, 8, 8),&#xA;    layer_attns = (False, False, False, True),&#xA;    layer_cross_attns = (False, False, False, True)&#xA;)&#xA;&#xA;# imagen, which contains the unets above (base unet and super resoluting ones)&#xA;&#xA;imagen = Imagen(&#xA;    unets = (unet1, unet2),&#xA;    image_sizes = (64, 256),&#xA;    beta_schedules = (&#39;cosine&#39;, &#39;linear&#39;),&#xA;    timesteps = 1000,&#xA;    cond_drop_prob = 0.5&#xA;).cuda()&#xA;&#xA;# mock images (get a lot of this) and text encodings from large T5&#xA;&#xA;text_embeds = torch.randn(4, 256, 768).cuda()&#xA;text_masks = torch.ones(4, 256).bool().cuda()&#xA;images = torch.randn(4, 3, 256, 256).cuda()&#xA;&#xA;# feed images into imagen, training each unet in the cascade&#xA;&#xA;for i in (1, 2):&#xA;    loss = imagen(images, text_embeds = text_embeds, text_masks = text_masks, unet_number = i)&#xA;    loss.backward()&#xA;&#xA;# do the above for many many many many steps&#xA;# now you can sample an image based on the text embeddings from the cascading ddpm&#xA;&#xA;images = imagen.sample(texts = [&#xA;    &#39;a whale breaching from afar&#39;,&#xA;    &#39;young girl blowing out candles on her birthday cake&#39;,&#xA;    &#39;fireworks with blue and green sparkles&#39;&#xA;], cond_scale = 2.)&#xA;&#xA;images.shape # (3, 3, 256, 256)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With the &lt;code&gt;ImagenTrainer&lt;/code&gt; wrapper class, the exponential moving averages for all of the U-nets in the cascading DDPM will be automatically taken care of when calling &lt;code&gt;update&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from imagen_pytorch import Unet, Imagen, ImagenTrainer&#xA;&#xA;# unet for imagen&#xA;&#xA;unet1 = Unet(&#xA;    dim = 32,&#xA;    cond_dim = 512,&#xA;    dim_mults = (1, 2, 4, 8),&#xA;    num_resnet_blocks = 3,&#xA;    layer_attns = (False, True, True, True),&#xA;)&#xA;&#xA;unet2 = Unet(&#xA;    dim = 32,&#xA;    cond_dim = 512,&#xA;    dim_mults = (1, 2, 4, 8),&#xA;    num_resnet_blocks = (2, 4, 8, 8),&#xA;    layer_attns = (False, False, False, True),&#xA;    layer_cross_attns = (False, False, False, True)&#xA;)&#xA;&#xA;# imagen, which contains the unets above (base unet and super resoluting ones)&#xA;&#xA;imagen = Imagen(&#xA;    unets = (unet1, unet2),&#xA;    text_encoder_name = &#39;t5-large&#39;,&#xA;    image_sizes = (64, 256),&#xA;    beta_schedules = (&#39;cosine&#39;, &#39;linear&#39;),&#xA;    timesteps = 1000,&#xA;    cond_drop_prob = 0.5&#xA;).cuda()&#xA;&#xA;# wrap imagen with the trainer class&#xA;&#xA;trainer = ImagenTrainer(imagen)&#xA;&#xA;# mock images (get a lot of this) and text encodings from large T5&#xA;&#xA;text_embeds = torch.randn(64, 256, 1024).cuda()&#xA;text_masks = torch.ones(64, 256).bool().cuda()&#xA;images = torch.randn(64, 3, 256, 256).cuda()&#xA;&#xA;# feed images into imagen, training each unet in the cascade&#xA;&#xA;for i in (1, 2):&#xA;    loss = trainer(&#xA;        images,&#xA;        text_embeds = text_embeds,&#xA;        text_masks = text_masks,&#xA;        unet_number = i,&#xA;        max_batch_size = 4        # auto divide the batch of 64 up into batch size of 4 and accumulate gradients, so it all fits in memory&#xA;    )&#xA;&#xA;    trainer.update(unet_number = i)&#xA;&#xA;# do the above for many many many many steps&#xA;# now you can sample an image based on the text embeddings from the cascading ddpm&#xA;&#xA;images = trainer.sample(texts = [&#xA;    &#39;a puppy looking anxiously at a giant donut on the table&#39;,&#xA;    &#39;the milky way galaxy in the style of monet&#39;&#xA;], cond_scale = 2.)&#xA;&#xA;images.shape # (2, 3, 256, 256)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Shoutouts&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://stability.ai/&#34;&gt;StabilityAI&lt;/a&gt; for the generous sponsorship, as well as my other sponsors out there&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/&#34;&gt;ü§ó Huggingface&lt;/a&gt; for their amazing transformers library. The text encoder portion is pretty much taken care of because of them&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/jorgemcgomes&#34;&gt;Jorge Gomes&lt;/a&gt; for helping out with the T5 loading code and advice on the correct T5 version&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;Katherine Crowson&lt;/a&gt;, for her &lt;a href=&#34;https://github.com/crowsonkb/v-diffusion-jax/raw/master/diffusion/utils.py&#34;&gt;beautiful code&lt;/a&gt;, which helped me understand the continuous time version of gaussian diffusion&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You? It isn&#39;t done yet, chip in if you are a researcher or skilled ML engineer&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Todo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; use huggingface transformers for T5-small text embeddings&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add dynamic thresholding&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add dynamic thresholding DALLE2 and video-diffusion repository as well&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; allow for one to set T5-large (and perhaps small factory method to take in any huggingface transformer)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add the lowres noise level with the pseudocode in appendix, and figure out what is this sweep they do at inference time&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; port over some training code from DALLE2&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; need to be able to use a different noise schedule per unet (cosine was used for base, but linear for SR)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; just make one master-configurable unet&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; complete resnet block (biggan inspired? but with groupnorm) - complete self attention&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; complete conditioning embedding block (and make it completely configurable, whether it be attention, film etc)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; consider using perceiver-resampler from &lt;a href=&#34;https://github.com/lucidrains/flamingo-pytorch&#34;&gt;https://github.com/lucidrains/flamingo-pytorch&lt;/a&gt; in place of attention pooling&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add attention pooling option, in addition to cross attention and film&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add optional cosine decay schedule with warmup, for each unet, to trainer&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; figure out if learned variance was used at all, and remove it if it was inconsequential&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; switch to continuous timesteps instead of discretized, as it seems that is what they used for all stages - first figure out the linear noise schedule case from the variational ddpm paper &lt;a href=&#34;https://openreview.net/forum?id=2LdBqxc1Yv&#34;&gt;https://openreview.net/forum?id=2LdBqxc1Yv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; exercise efficient attention expertise + explore skip layer excitation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; try out grid attention&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Saharia2022PhotorealisticTD,&#xA;    title   = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},&#xA;    author  = {Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily L. Denton and Seyed Kamyar Seyed Ghasemipour and Burcu Karagol Ayan and Seyedeh Sara Mahdavi and Raphael Gontijo Lopes and Tim Salimans and Jonathan Ho and David Fleet and Mohammad Norouzi},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Tu2022MaxViTMV,&#xA;    title   = {MaxViT: Multi-Axis Vision Transformer},&#xA;    author  = {Zhengzhong Tu and Hossein Talebi and Han Zhang and Feng Yang and Peyman Milanfar and Alan Conrad Bovik and Yinxiao Li},&#xA;    year    = {2022},&#xA;    url     = {https://arxiv.org/abs/2204.01697}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Alayrac2022Flamingo,&#xA;    title   = {Flamingo: a Visual Language Model for Few-Shot Learning},&#xA;    author  = {Jean-Baptiste Alayrac et al},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>iperov/DeepFaceLive</title>
    <updated>2022-06-05T02:02:48Z</updated>
    <id>tag:github.com,2022-06-05:/iperov/DeepFaceLive</id>
    <link href="https://github.com/iperov/DeepFaceLive" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Real-time face swap for PC streaming or video calls&lt;/p&gt;&lt;hr&gt;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/deepfacelive_intro.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/logo_onnx.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/logo_directx.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/logo_python.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Face Swapper&lt;/h2&gt; &lt;p&gt;You can swap your face from a webcam or the face in the video using trained face models.&lt;/p&gt; &lt;p&gt;Here is a list of available ready-to-use public face models.&lt;/p&gt; &lt;p&gt;These persons do not exists. Similarities with real people are accidental. Except Keanu Reeves. He exists, and he&#39;s breathtaking!&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &#xA;    &lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA;     &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Keanu Reeves &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Keanu_Reeves/Keanu_Reeves.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Keanu_Reeves/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;     &lt;/tbody&gt;&#xA;    &lt;/table&gt; &#xA;    &lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA;     &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Ava de Addario &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Ava_de_Addario/Ava_de_Addario.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Ava_de_Addario/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Dilraba Dilmurat &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Dilraba_Dilmurat/Dilraba_Dilmurat.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;examples&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Ewon Spice &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Ewon_Spice/Ewon_Spice.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Ewon_Spice/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Yohanna Coralson &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Yohanna_Coralson/Yohanna_Coralson.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Yohanna_Coralson/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;     &lt;/tbody&gt;&#xA;    &lt;/table&gt; &#xA;    &lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA;     &lt;tbody&gt;&#xA;      &lt;tr align=&#34;center&#34;&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Kim Jarrey &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Kim_Jarrey/Kim_Jarrey.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Kim_Jarrey/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td&gt; David Kovalniy &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/David_Kovalniy/David_Kovalniy.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/David_Kovalniy/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td&gt; Matilda Bobbie &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Matilda_Bobbie/Matilda_Bobbie.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Matilda_Bobbie/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td&gt; Bryan Greynolds &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Bryan_Greynolds/Bryan_Greynolds.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Bryan_Greynolds/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td&gt; Nicola Badge &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Nicola_Badge/Nicola_Badge.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Nicola_Badge/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;     &lt;/tbody&gt;&#xA;    &lt;/table&gt; &#xA;    &lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA;     &lt;tbody&gt;&#xA;      &lt;tr align=&#34;center&#34;&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Silwan Stillwone &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Silwan_Stillwone/Silwan_Stillwone.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Silwan_Stillwone/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td&gt; Tim Chrys &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tim_Chrys/Tim_Chrys.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tim_Chrys/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td&gt; Zahar Lupin &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Zahar_Lupin/Zahar_Lupin.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Zahar_Lupin/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td&gt; Tim Norland &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tim_Norland/Tim_Norland.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tim_Norland/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;     &lt;/tbody&gt;&#xA;    &lt;/table&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; If you want a higher quality or better face match, you can train your own face model using &lt;a href=&#34;https://github.com/iperov/DeepFaceLab&#34;&gt;DeepFaceLab&lt;/a&gt; &lt;p&gt;Here is an &lt;a href=&#34;https://www.tiktok.com/@arnoldschwarzneggar/video/6995538782204300545&#34;&gt;example&lt;/a&gt; of Arnold Schwarzneggar trained on a particular face and used in a video call. Read the FAQ for more information.&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Face Animator&lt;/h2&gt; &lt;p&gt;There is also a Face Animator module in DeepFaceLive app. You can control a static face picture using video or your own face from the camera. The quality is not the best, and requires fine face matching and tuning parameters for every face pair, but enough for funny videos and memes or real-time streaming at 25 fps using 35 TFLOPS GPU.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/face_animator_example.gif&#34;&gt;&lt;/p&gt; &lt;p&gt;Here is a &lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/FaceAnimator_tutor.webm?raw=true&#34;&gt;mini video&lt;/a&gt; showing the process of setting up the Face Animator for Obama controlling Kim Chen&#39;s face.&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;System requirements&lt;/h2&gt; &lt;p&gt;any DirectX12 compatible graphics card&lt;/p&gt; &lt;p&gt;(Recommended RTX 2070+ / Radeon RX 5700 XT+ )&lt;/p&gt; &lt;p&gt;Modern CPU with AVX instructions&lt;/p&gt; &lt;p&gt;4GB RAM, 32GB+ paging file&lt;/p&gt; &lt;p&gt;Windows 10&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Setup tutorial&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/setup_tutorial_windows/index.md&#34;&gt;Windows 10 x64&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/build/linux&#34;&gt;Linux build info&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Documentation&lt;/h2&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/user_faq/user_faq.md&#34;&gt;User FAQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/developer_faq/developer_faq.md&#34;&gt;Developer FAQ&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Releases&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;p&gt;&lt;a href=&#34;https://disk.yandex.ru/d/7i5XTKIKVg5UUg&#34;&gt;Windows 10 x64 (yandex.ru)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://mega.nz/folder/m10iELBK#Y0H6BflF9C4k_clYofC7yA&#34;&gt;Windows 10 x64 (mega.nz)&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt; Contains stand-alone zero-dependency all-in-one ready-to-use portable self-extracting folder! You don&#39;t need to install anything other than video drivers. &lt;br&gt;&lt;br&gt; DirectX12 build : NVIDIA, AMD, Intel videocards. &lt;br&gt;&lt;br&gt; NVIDIA build : NVIDIA cards only, GT730 and higher. Works faster than DX12. FaceMerger can work also on AMD/Intel. &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Communication groups&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://discord.gg/S2h7kPySQp&#34;&gt;Discord&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;Official discord channel. English / Russian.&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://mrdeepfakes.com/forums/&#34;&gt;mrdeepfakes&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;the biggest NSFW English deepfake community&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://www.dfldata.xyz&#34;&gt;dfldata.xyz&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;‰∏≠Êñá‰∫§ÊµÅËÆ∫ÂùõÔºåÂÖçË¥πËΩØ‰ª∂ÊïôÁ®ã„ÄÅÊ®°Âûã„ÄÅ‰∫∫ËÑ∏Êï∞ÊçÆ&lt;/td&gt;&#xA;  &lt;/tr&gt;  &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;How can I help the project?&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; I need the computing power to train models. &lt;br&gt; If you have a free computer with 2080TI or better card with 12GB+ VRAM, you can give me remote access to it. I will train 1 model in a month. Contact me(iperov#6528) in Discord channel. &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; Register github account and push &#34;Star&#34; button. &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;!--&lt;tr&gt;&lt;td colspan=2 align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;https://www.paypal.com/paypalme/DeepFaceLab&#34;&gt;Donate via Paypal&lt;/a&gt;&#xA;&lt;/td&gt;&lt;/tr&gt;--&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;a href=&#34;https://money.yandex.ru/to/41001142318065&#34;&gt;Donate via Yandex.Money&lt;/a&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; bitcoin:bc1qewl062v70rszulml3f0mjdjrys8uxdydw3v6rq &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &#xA;    &lt;!--&#xA;    &lt;a href=&#34;https://br-stone.online&#34;&gt;&lt;img src=&#34;doc/logo_barclay_stone.png&#34;&gt;&lt;/img&gt;&lt;/a&gt;&lt;a href=&#34;https://exmo.com&#34;&gt;&lt;img src=&#34;doc/logo_exmo.png&#34;&gt;&lt;/img&gt;&lt;/a&gt;&#xA;&#xA;    presents&#xA;&#xA;    &lt;tr&gt;&lt;td align=&#34;right&#34;&gt;&#xA;&#xA;&#xA;    &lt;a href=&#34;&#34;&gt;Windows (magnet link)&lt;/a&gt;&#xA;    &lt;/td&gt;&lt;td align=&#34;center&#34;&gt;Latest release. Use torrent client to download.&lt;/td&gt;&lt;/tr&gt;&#xA;    &lt;/tr&gt;&#xA;--&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt;</summary>
  </entry>
</feed>