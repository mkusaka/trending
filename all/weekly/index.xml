<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-29T01:38:41Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AtsushiSakai/PythonRobotics</title>
    <updated>2024-09-29T01:38:41Z</updated>
    <id>tag:github.com,2024-09-29:/AtsushiSakai/PythonRobotics</id>
    <link href="https://github.com/AtsushiSakai/PythonRobotics" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python sample codes for robotics algorithms.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true&#34; align=&#34;right&#34; width=&#34;300&#34; alt=&#34;header pic&#34;&gt; &#xA;&lt;h1&gt;PythonRobotics&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRobotics/workflows/Linux_CI/badge.svg?sanitize=true&#34; alt=&#34;GitHub_Action_Linux_CI&#34;&gt; &lt;img src=&#34;https://github.com/AtsushiSakai/PythonRobotics/workflows/MacOS_CI/badge.svg?sanitize=true&#34; alt=&#34;GitHub_Action_MacOS_CI&#34;&gt; &lt;img src=&#34;https://github.com/AtsushiSakai/PythonRobotics/workflows/Windows_CI/badge.svg?sanitize=true&#34; alt=&#34;GitHub_Action_Windows_CI&#34;&gt; &lt;a href=&#34;https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/AtsushiSakai/PythonRobotics&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/AtsushiSakai/PythonRobotics/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Python codes for robotics algorithm.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#what-is-this&#34;&gt;What is this?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#how-to-use&#34;&gt;How to use&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#localization&#34;&gt;Localization&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#extended-kalman-filter-localization&#34;&gt;Extended Kalman Filter localization&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#particle-filter-localization&#34;&gt;Particle filter localization&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#histogram-filter-localization&#34;&gt;Histogram filter localization&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#mapping&#34;&gt;Mapping&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#gaussian-grid-map&#34;&gt;Gaussian grid map&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#ray-casting-grid-map&#34;&gt;Ray casting grid map&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#lidar-to-grid-map&#34;&gt;Lidar to grid map&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#k-means-object-clustering&#34;&gt;k-means object clustering&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rectangle-fitting&#34;&gt;Rectangle fitting&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#slam&#34;&gt;SLAM&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#iterative-closest-point-icp-matching&#34;&gt;Iterative Closest Point (ICP) Matching&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#fastslam-10&#34;&gt;FastSLAM 1.0&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#path-planning&#34;&gt;Path Planning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#dynamic-window-approach&#34;&gt;Dynamic Window Approach&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#grid-based-search&#34;&gt;Grid based search&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#dijkstra-algorithm&#34;&gt;Dijkstra algorithm&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#a-algorithm&#34;&gt;A* algorithm&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#d-algorithm&#34;&gt;D* algorithm&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#d-lite-algorithm&#34;&gt;D* Lite algorithm&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#potential-field-algorithm&#34;&gt;Potential Field algorithm&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#grid-based-coverage-path-planning&#34;&gt;Grid based coverage path planning&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#state-lattice-planning&#34;&gt;State Lattice Planning&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#biased-polar-sampling&#34;&gt;Biased polar sampling&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#lane-sampling&#34;&gt;Lane sampling&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#probabilistic-road-map-prm-planning&#34;&gt;Probabilistic Road-Map (PRM) planning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rapidly-exploring-random-trees-rrt&#34;&gt;Rapidly-Exploring Random Trees (RRT)&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rrt&#34;&gt;RRT*&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rrt-with-reeds-shepp-path&#34;&gt;RRT* with reeds-shepp path&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#lqr-rrt&#34;&gt;LQR-RRT*&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#quintic-polynomials-planning&#34;&gt;Quintic polynomials planning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#reeds-shepp-planning&#34;&gt;Reeds Shepp planning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#lqr-based-path-planning&#34;&gt;LQR based path planning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#optimal-trajectory-in-a-frenet-frame&#34;&gt;Optimal Trajectory in a Frenet Frame&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#path-tracking&#34;&gt;Path Tracking&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#move-to-a-pose-control&#34;&gt;move to a pose control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#stanley-control&#34;&gt;Stanley control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rear-wheel-feedback-control&#34;&gt;Rear wheel feedback control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#linearquadratic-regulator-lqr-speed-and-steering-control&#34;&gt;Linear–quadratic regulator (LQR) speed and steering control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#model-predictive-speed-and-steering-control&#34;&gt;Model predictive speed and steering control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#nonlinear-model-predictive-control-with-c-gmres&#34;&gt;Nonlinear Model predictive control with C-GMRES&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#arm-navigation&#34;&gt;Arm Navigation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#n-joint-arm-to-point-control&#34;&gt;N joint arm to point control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#arm-navigation-with-obstacle-avoidance&#34;&gt;Arm navigation with obstacle avoidance&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#aerial-navigation&#34;&gt;Aerial Navigation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#drone-3d-trajectory-following&#34;&gt;drone 3d trajectory following&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#rocket-powered-landing&#34;&gt;rocket powered landing&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#bipedal&#34;&gt;Bipedal&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#bipedal-planner-with-inverted-pendulum&#34;&gt;bipedal planner with inverted pendulum&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#use-case&#34;&gt;Use-case&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#contribution&#34;&gt;Contribution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#citing&#34;&gt;Citing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#support&#34;&gt;Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#sponsors&#34;&gt;Sponsors&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#JetBrains&#34;&gt;JetBrains&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#1password&#34;&gt;1Password&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AtsushiSakai/PythonRobotics/master/#authors&#34;&gt;Authors&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;What is this?&lt;/h1&gt; &#xA;&lt;p&gt;This is a Python code collection of robotics algorithms.&lt;/p&gt; &#xA;&lt;p&gt;Features:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Easy to read for understanding each algorithm&#39;s basic idea.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Widely used and practical algorithms are selected.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Minimum dependency.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;See this paper for more details:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1808.10703&#34;&gt;[1808.10703] PythonRobotics: a Python code collection of robotics algorithms&lt;/a&gt; (&lt;a href=&#34;https://github.com/AtsushiSakai/PythonRoboticsPaper/raw/master/python_robotics.bib&#34;&gt;BibTeX&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Requirements&lt;/h1&gt; &#xA;&lt;p&gt;For running each sample code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python 3.12.x&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://numpy.org/&#34;&gt;NumPy&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://scipy.org/&#34;&gt;SciPy&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://matplotlib.org/&#34;&gt;Matplotlib&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.cvxpy.org/&#34;&gt;cvxpy&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For development:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://pytest.org/&#34;&gt;pytest&lt;/a&gt; (for unit tests)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/pytest-xdist/&#34;&gt;pytest-xdist&lt;/a&gt; (for parallel unit tests)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://mypy-lang.org/&#34;&gt;mypy&lt;/a&gt; (for type check)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.sphinx-doc.org/&#34;&gt;sphinx&lt;/a&gt; (for document generation)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/pycodestyle/&#34;&gt;pycodestyle&lt;/a&gt; (for code style check)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;This README only shows some examples of this project.&lt;/p&gt; &#xA;&lt;p&gt;If you are interested in other examples or mathematical backgrounds of each algorithm,&lt;/p&gt; &#xA;&lt;p&gt;You can check the full documentation online: &lt;a href=&#34;https://atsushisakai.github.io/PythonRobotics/index.html&#34;&gt;Welcome to PythonRobotics’s documentation! — PythonRobotics documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All animation gifs are stored here: &lt;a href=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs&#34;&gt;AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;How to use&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone this repo.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-terminal&#34;&gt;git clone https://github.com/AtsushiSakai/PythonRobotics.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the required libraries.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;using conda :&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-terminal&#34;&gt;conda env create -f requirements/environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;using pip :&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-terminal&#34;&gt;pip install -r requirements/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Execute python script in each directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add star to this repo if you like it &lt;span&gt;😃&lt;/span&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Localization&lt;/h1&gt; &#xA;&lt;h2&gt;Extended Kalman Filter localization&lt;/h2&gt; &#xA;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif&#34; width=&#34;640&#34; alt=&#34;EKF pic&#34;&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://atsushisakai.github.io/PythonRobotics/modules/localization/extended_kalman_filter_localization_files/extended_kalman_filter_localization.html&#34;&gt;documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Particle filter localization&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a sensor fusion localization with Particle Filter(PF).&lt;/p&gt; &#xA;&lt;p&gt;The blue line is true trajectory, the black line is dead reckoning trajectory,&lt;/p&gt; &#xA;&lt;p&gt;and the red line is an estimated trajectory with PF.&lt;/p&gt; &#xA;&lt;p&gt;It is assumed that the robot can measure a distance from landmarks (RFID).&lt;/p&gt; &#xA;&lt;p&gt;These measurements are used for PF localization.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.probabilistic-robotics.org/&#34;&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Histogram filter localization&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a 2D localization example with Histogram filter.&lt;/p&gt; &#xA;&lt;p&gt;The red cross is true position, black points are RFID positions.&lt;/p&gt; &#xA;&lt;p&gt;The blue grid shows a position probability of histogram filter.&lt;/p&gt; &#xA;&lt;p&gt;In this simulation, x,y are unknown, yaw is known.&lt;/p&gt; &#xA;&lt;p&gt;The filter integrates speed input and range observations from RFID for localization.&lt;/p&gt; &#xA;&lt;p&gt;Initial position is not needed.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.probabilistic-robotics.org/&#34;&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Mapping&lt;/h1&gt; &#xA;&lt;h2&gt;Gaussian grid map&lt;/h2&gt; &#xA;&lt;p&gt;This is a 2D Gaussian grid mapping example.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Ray casting grid map&lt;/h2&gt; &#xA;&lt;p&gt;This is a 2D ray casting grid mapping example.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Lidar to grid map&lt;/h2&gt; &#xA;&lt;p&gt;This example shows how to convert a 2D range measurement to a grid map.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/lidar_to_grid_map/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;k-means object clustering&lt;/h2&gt; &#xA;&lt;p&gt;This is a 2D object clustering with k-means algorithm.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Rectangle fitting&lt;/h2&gt; &#xA;&lt;p&gt;This is a 2D rectangle fitting for vehicle detection.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;SLAM&lt;/h1&gt; &#xA;&lt;p&gt;Simultaneous Localization and Mapping(SLAM) examples&lt;/p&gt; &#xA;&lt;h2&gt;Iterative Closest Point (ICP) Matching&lt;/h2&gt; &#xA;&lt;p&gt;This is a 2D ICP matching example with singular value decomposition.&lt;/p&gt; &#xA;&lt;p&gt;It can calculate a rotation matrix, and a translation vector between points and points.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf&#34;&gt;Introduction to Mobile Robotics: Iterative Closest Point Algorithm&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FastSLAM 1.0&lt;/h2&gt; &#xA;&lt;p&gt;This is a feature based SLAM example using FastSLAM 1.0.&lt;/p&gt; &#xA;&lt;p&gt;The blue line is ground truth, the black line is dead reckoning, the red line is the estimated trajectory with FastSLAM.&lt;/p&gt; &#xA;&lt;p&gt;The red points are particles of FastSLAM.&lt;/p&gt; &#xA;&lt;p&gt;Black points are landmarks, blue crosses are estimated landmark positions by FastSLAM.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://www.probabilistic-robotics.org/&#34;&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://www-personal.acfr.usyd.edu.au/tbailey/software/slam_simulations.htm&#34;&gt;SLAM simulations by Tim Bailey&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Path Planning&lt;/h1&gt; &#xA;&lt;h2&gt;Dynamic Window Approach&lt;/h2&gt; &#xA;&lt;p&gt;This is a 2D navigation sample code with Dynamic Window Approach.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf&#34;&gt;The Dynamic Window Approach to Collision Avoidance&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Grid based search&lt;/h2&gt; &#xA;&lt;h3&gt;Dijkstra algorithm&lt;/h3&gt; &#xA;&lt;p&gt;This is a 2D grid based the shortest path planning with Dijkstra&#39;s algorithm.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif&#34; alt=&#34;PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt; &#xA;&lt;h3&gt;A* algorithm&lt;/h3&gt; &#xA;&lt;p&gt;This is a 2D grid based the shortest path planning with A star algorithm.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif&#34; alt=&#34;PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt; &#xA;&lt;p&gt;Its heuristic is 2D Euclid distance.&lt;/p&gt; &#xA;&lt;h3&gt;D* algorithm&lt;/h3&gt; &#xA;&lt;p&gt;This is a 2D grid based the shortest path planning with D star algorithm.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DStar/animation.gif&#34; alt=&#34;figure at master · nirnayroy/intelligentrobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The animation shows a robot finding its path avoiding an obstacle using the D* search algorithm.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/D*&#34;&gt;D* Algorithm Wikipedia&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;D* Lite algorithm&lt;/h3&gt; &#xA;&lt;p&gt;This algorithm finds the shortest path between two points while rerouting when obstacles are discovered. It has been implemented here for a 2D grid.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DStarLite/animation.gif&#34; alt=&#34;D* Lite&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The animation shows a robot finding its path and rerouting to avoid obstacles as they are discovered using the D* Lite search algorithm.&lt;/p&gt; &#xA;&lt;p&gt;Refs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://idm-lab.org/bib/abstracts/papers/aaai02b.pd&#34;&gt;D* Lite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.cmu.edu/~maxim/files/dlite_icra02.pdf&#34;&gt;Improved Fast Replanning for Robot Navigation in Unknown Terrain&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Potential Field algorithm&lt;/h3&gt; &#xA;&lt;p&gt;This is a 2D grid based path planning with Potential Field algorithm.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif&#34; alt=&#34;PotentialField&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the animation, the blue heat map shows potential value on each grid.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf&#34;&gt;Robotic Motion Planning:Potential Functions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Grid based coverage path planning&lt;/h3&gt; &#xA;&lt;p&gt;This is a 2D grid based coverage path planning simulation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif&#34; alt=&#34;PotentialField&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;State Lattice Planning&lt;/h2&gt; &#xA;&lt;p&gt;This script is a path planning code with state lattice planning.&lt;/p&gt; &#xA;&lt;p&gt;This code uses the model predictive trajectory generator to solve boundary problem.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://journals.sagepub.com/doi/pdf/10.1177/0278364906075328&#34;&gt;Optimal rough terrain trajectory generation for wheeled mobile robots&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://www.frc.ri.cmu.edu/~alonzo/pubs/papers/JFR_08_SS_Sampling.pdf&#34;&gt;State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Biased polar sampling&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif&#34; alt=&#34;PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Lane sampling&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif&#34; alt=&#34;PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Probabilistic Road-Map (PRM) planning&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif&#34; alt=&#34;PRM&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This PRM planner uses Dijkstra method for graph search.&lt;/p&gt; &#xA;&lt;p&gt;In the animation, blue points are sampled points,&lt;/p&gt; &#xA;&lt;p&gt;Cyan crosses means searched points with Dijkstra method,&lt;/p&gt; &#xA;&lt;p&gt;The red line is the final path of PRM.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Probabilistic_roadmap&#34;&gt;Probabilistic roadmap - Wikipedia&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;　　&lt;/p&gt; &#xA;&lt;h2&gt;Rapidly-Exploring Random Trees (RRT)&lt;/h2&gt; &#xA;&lt;h3&gt;RRT*&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif&#34; alt=&#34;PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a path planning code with RRT*&lt;/p&gt; &#xA;&lt;p&gt;Black circles are obstacles, green line is a searched tree, red crosses are start and goal positions.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1005.0416&#34;&gt;Incremental Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.419.5503&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RRT* with reeds-shepp path&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif&#34; alt=&#34;Robotics/animation.gif at master · AtsushiSakai/PythonRobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Path planning for a car robot with RRT* and reeds shepp path planner.&lt;/p&gt; &#xA;&lt;h3&gt;LQR-RRT*&lt;/h3&gt; &#xA;&lt;p&gt;This is a path planning simulation with LQR-RRT*.&lt;/p&gt; &#xA;&lt;p&gt;A double integrator motion model is used for LQR local planner.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif&#34; alt=&#34;LQR_RRT&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://lis.csail.mit.edu/pubs/perez-icra12.pdf&#34;&gt;LQR-RRT*: Optimal Sampling-Based Motion Planning with Automatically Derived Extension Heuristics&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/MahanFathi/LQR-RRTstar&#34;&gt;MahanFathi/LQR-RRTstar: LQR-RRT* method is used for random motion planning of a simple pendulum in its phase plot&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quintic polynomials planning&lt;/h2&gt; &#xA;&lt;p&gt;Motion planning with quintic polynomials.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;It can calculate a 2D path, velocity, and acceleration profile based on quintic polynomials.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/document/637936/&#34;&gt;Local Path Planning And Motion Control For Agv In Positioning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reeds Shepp planning&lt;/h2&gt; &#xA;&lt;p&gt;A sample code with Reeds Shepp path planning.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true&#34; alt=&#34;RSPlanning&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://planning.cs.uiuc.edu/node822.html&#34;&gt;15.3.2 Reeds-Shepp Curves&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://pdfs.semanticscholar.org/932e/c495b1d0018fd59dee12a0bf74434fac7af4.pdf&#34;&gt;optimal paths for a car that goes both forwards and backwards&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ghliu/pyReedsShepp&#34;&gt;ghliu/pyReedsShepp: Implementation of Reeds Shepp curve.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LQR based path planning&lt;/h2&gt; &#xA;&lt;p&gt;A sample code using LQR based path planning for double integrator model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true&#34; alt=&#34;RSPlanning&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Optimal Trajectory in a Frenet Frame&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is optimal trajectory generation in a Frenet Frame.&lt;/p&gt; &#xA;&lt;p&gt;The cyan line is the target course and black crosses are obstacles.&lt;/p&gt; &#xA;&lt;p&gt;The red line is the predicted path.&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf&#34;&gt;Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Cj6tAQe7UCY&#34;&gt;Optimal trajectory generation for dynamic street scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Path Tracking&lt;/h1&gt; &#xA;&lt;h2&gt;move to a pose control&lt;/h2&gt; &#xA;&lt;p&gt;This is a simulation of moving to a pose control&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/move_to_pose/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://link.springer.com/book/10.1007/978-3-642-20144-8&#34;&gt;P. I. Corke, &#34;Robotics, Vision and Control&#34; | SpringerLink p102&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Stanley control&lt;/h2&gt; &#xA;&lt;p&gt;Path tracking simulation with Stanley steering control and PID speed control.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif&#34; alt=&#34;2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://robots.stanford.edu/papers/thrun.stanley05.pdf&#34;&gt;Stanley: The robot that won the DARPA grand challenge&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf&#34;&gt;Automatic Steering Methods for Autonomous Automobile Path Tracking&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Rear wheel feedback control&lt;/h2&gt; &#xA;&lt;p&gt;Path tracking simulation with rear wheel feedback steering control and PID speed control.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif&#34; alt=&#34;PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1604.07446&#34;&gt;A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Linear–quadratic regulator (LQR) speed and steering control&lt;/h2&gt; &#xA;&lt;p&gt;Path tracking simulation with LQR speed and steering control.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ieeexplore.ieee.org/document/5940562/&#34;&gt;Towards fully autonomous driving: Systems and algorithms - IEEE Conference Publication&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model predictive speed and steering control&lt;/h2&gt; &#xA;&lt;p&gt;Path tracking simulation with iterative linear model predictive speed and steering control.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif&#34; width=&#34;640&#34; alt=&#34;MPC pic&#34;&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://atsushisakai.github.io/PythonRobotics/modules/path_tracking/model_predictive_speed_and_steering_control/model_predictive_speed_and_steering_control.html&#34;&gt;documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://grauonline.de/wordpress/?page_id=3244&#34;&gt;Real-time Model Predictive Control (MPC), ACADO, Python | Work-is-Playing&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Nonlinear Model predictive control with C-GMRES&lt;/h2&gt; &#xA;&lt;p&gt;A motion planning and path tracking simulation with NMPC of C-GMRES&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://atsushisakai.github.io/PythonRobotics/modules/path_tracking/cgmres_nmpc/cgmres_nmpc.html&#34;&gt;documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Arm Navigation&lt;/h1&gt; &#xA;&lt;h2&gt;N joint arm to point control&lt;/h2&gt; &#xA;&lt;p&gt;N joint arm to a point control simulation.&lt;/p&gt; &#xA;&lt;p&gt;This is an interactive simulation.&lt;/p&gt; &#xA;&lt;p&gt;You can set the goal position of the end effector with left-click on the plotting area.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In this simulation N = 10, however, you can change it.&lt;/p&gt; &#xA;&lt;h2&gt;Arm navigation with obstacle avoidance&lt;/h2&gt; &#xA;&lt;p&gt;Arm navigation with obstacle avoidance simulation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Aerial Navigation&lt;/h1&gt; &#xA;&lt;h2&gt;drone 3d trajectory following&lt;/h2&gt; &#xA;&lt;p&gt;This is a 3d trajectory following simulation for a quadrotor.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;rocket powered landing&lt;/h2&gt; &#xA;&lt;p&gt;This is a 3d trajectory generation simulation for a rocket powered landing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ref:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://atsushisakai.github.io/PythonRobotics/modules/aerial_navigation/rocket_powered_landing/rocket_powered_landing.html&#34;&gt;documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Bipedal&lt;/h1&gt; &#xA;&lt;h2&gt;bipedal planner with inverted pendulum&lt;/h2&gt; &#xA;&lt;p&gt;This is a bipedal planner for modifying footsteps for an inverted pendulum.&lt;/p&gt; &#xA;&lt;p&gt;You can set the footsteps, and the planner will modify those automatically.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt; &#xA;&lt;h1&gt;Use-case&lt;/h1&gt; &#xA;&lt;p&gt;If this project helps your robotics project, please let me know with creating an issue.&lt;/p&gt; &#xA;&lt;p&gt;Your robot&#39;s video, which is using PythonRobotics, is very welcome!!&lt;/p&gt; &#xA;&lt;p&gt;This is a list of user&#39;s comment and references:&lt;a href=&#34;https://github.com/AtsushiSakai/PythonRobotics/raw/master/users_comments.md&#34;&gt;users_comments&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contribution&lt;/h1&gt; &#xA;&lt;p&gt;Any contribution is welcome!!&lt;/p&gt; &#xA;&lt;p&gt;Please check this document:&lt;a href=&#34;https://atsushisakai.github.io/PythonRobotics/how_to_contribute.html&#34;&gt;How To Contribute — PythonRobotics documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Citing&lt;/h1&gt; &#xA;&lt;p&gt;If you use this project&#39;s code for your academic work, we encourage you to cite &lt;a href=&#34;https://arxiv.org/abs/1808.10703&#34;&gt;our papers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you use this project&#39;s code in industry, we&#39;d love to hear from you as well; feel free to reach out to the developers directly.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a id=&#34;support&#34;&gt;&lt;/a&gt;Supporting this project&lt;/h1&gt; &#xA;&lt;p&gt;If you or your company would like to support this project, please consider:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/sponsors/AtsushiSakai&#34;&gt;Sponsor @AtsushiSakai on GitHub Sponsors&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.patreon.com/myenigma&#34;&gt;Become a backer or sponsor on Patreon&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.paypal.me/myenigmapay/&#34;&gt;One-time donation via PayPal&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you would like to support us in some other way, please contact with creating an issue.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a id=&#34;sponsors&#34;&gt;&lt;/a&gt;Sponsors&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a id=&#34;JetBrains&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://www.jetbrains.com/&#34;&gt;JetBrains&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;They are providing a free license of their IDEs for this OSS development.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/1Password/1password-teams-open-source&#34;&gt;1Password&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;They are providing a free license of their 1Password team license for this OSS project.&lt;/p&gt; &#xA;&lt;h1&gt;Authors&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AtsushiSakai/PythonRobotics/graphs/contributors&#34;&gt;Contributors to AtsushiSakai/PythonRobotics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>graviraja/MLOps-Basics</title>
    <updated>2024-09-29T01:38:41Z</updated>
    <id>tag:github.com,2024-09-29:/graviraja/MLOps-Basics</id>
    <link href="https://github.com/graviraja/MLOps-Basics" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLOps-Basics&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;There is nothing magic about magic. The magician merely understands something simple which doesn’t appear to be simple or natural to the untrained audience. Once you learn how to hold a card while making your hand look empty, you only need practice before you, too, can “do magic.” – Jeffrey Friedl in the book Mastering Regular Expressions&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: Please raise an issue for any suggestions, corrections, and feedback.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The goal of the series is to understand the basics of MLOps like model building, monitoring, configurations, testing, packaging, deployment, cicd, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/summary.png&#34; alt=&#34;pl&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Week 0: Project Setup&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-project-setup-part1&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The project I have implemented is a simple classification problem. The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;How to get the data?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to process the data?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to define dataloaders?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to declare the model?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to train the model?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to do the inference?&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/pl.jpeg&#34; alt=&#34;pl&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/datasets&#34;&gt;Huggingface Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Huggingface Transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/&#34;&gt;Pytorch Lightning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 1: Model monitoring - Weights and Biases&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-wandb-integration&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tracking all the experiments like tweaking hyper-parameters, trying different models to test their performance and seeing the connection between model and the input data will help in developing a better model.&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;How to configure basic logging with W&amp;amp;B?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to compute metrics and log them in W&amp;amp;B?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to add plots in W&amp;amp;B?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to add data samples to W&amp;amp;B?&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/wandb.png&#34; alt=&#34;wannb&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wandb.ai/site&#34;&gt;Weights and Biases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://torchmetrics.readthedocs.io/&#34;&gt;torchmetrics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;References:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hUXQm46TAKc&#34;&gt;Tutorial on Pytorch Lightning + Weights &amp;amp; Bias&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://docs.wandb.ai/&#34;&gt;WandB Documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 2: Configurations - Hydra&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-hydra-config&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Configuration management is a necessary for managing complex software systems. Lack of configuration management can cause serious problems with reliability, uptime, and the ability to scale a system.&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Basics of Hydra&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Overridding configurations&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Splitting configuration across multiple files&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Variable Interpolation&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to run model with different parameter combinations?&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/hydra.png&#34; alt=&#34;hydra&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hydra.cc/&#34;&gt;Hydra&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://hydra.cc/docs/intro&#34;&gt;Hydra Documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.sscardapane.it/tutorials/hydra-tutorial/#executing-multiple-runs&#34;&gt;Simone Tutorial on Hydra&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 3: Data Version Control - DVC&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-dvc&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Classical code version control systems are not designed to handle large files, which make cloning and storing the history impractical. Which are very common in Machine Learning.&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Basics of DVC&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Initialising DVC&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Configuring Remote Storage&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Saving Model to the Remote Storage&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Versioning the models&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/dvc.png&#34; alt=&#34;dvc&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dvc.org/&#34;&gt;DVC&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dvc.org/doc&#34;&gt;DVC Documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kLKBcPonMYw&#34;&gt;DVC Tutorial on Versioning data&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 4: Model Packaging - ONNX&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-onnx&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Why do we need model packaging? Models can be built using any machine learning framework available out there (sklearn, tensorflow, pytorch, etc.). We might want to deploy models in different environments like (mobile, web, raspberry pi) or want to run in a different framework (trained in pytorch, inference in tensorflow). A common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers will help a lot.&lt;/p&gt; &#xA;&lt;p&gt;This is acheived by a community project &lt;code&gt;ONNX&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;What is ONNX?&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;How to convert a trained model to ONNX format?&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;What is ONNX Runtime?&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;How to run ONNX converted model in ONNX Runtime?&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Comparisions&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/onnx.jpeg&#34; alt=&#34;ONNX&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://onnx.ai/&#34;&gt;ONNX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.onnxruntime.ai/&#34;&gt;ONNXRuntime&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=7nutT3Aacyw&#34;&gt;Abhishek Thakur tutorial on onnx model conversion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/stable/common/production_inference.html&#34;&gt;Pytorch Lightning documentation on onnx conversion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333&#34;&gt;Huggingface Blog on ONNXRuntime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tugot17.github.io/data-science-blog/onnx/tutorial/2020/09/21/Exporting-lightning-model-to-onnx.html&#34;&gt;Piotr Blog on onnx conversion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 5: Model Packaging - Docker&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-docker&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Why do we need packaging? We might have to share our application with others, and when they try to run the application most of the time it doesn’t run due to dependencies issues / OS related issues and for that, we say (famous quote across engineers) that &lt;code&gt;It works on my laptop/system&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;So for others to run the applications they have to set up the same environment as it was run on the host side which means a lot of manual configuration and installation of components.&lt;/p&gt; &#xA;&lt;p&gt;The solution to these limitations is a technology called Containers.&lt;/p&gt; &#xA;&lt;p&gt;By containerizing/packaging the application, we can run the application on any cloud platform to get advantages of managed services and autoscaling and reliability, and many more.&lt;/p&gt; &#xA;&lt;p&gt;The most prominent tool to do the packaging of application is Docker 🛳&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;FastAPI wrapper&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Basics of Docker&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Building Docker Container&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Docker Compose&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/docker_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2021/06/a-hands-on-guide-to-containerized-your-machine-learning-workflow-with-docker/&#34;&gt;Analytics vidhya blog&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 6: CI/CD - GitHub Actions&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-github-actions&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;CI/CD is a coding philosophy and set of practices with which you can continuously build, test, and deploy iterative code changes.&lt;/p&gt; &#xA;&lt;p&gt;This iterative process helps reduce the chance that you develop new code based on a buggy or failed previous versions. With this method, you strive to have less human intervention or even no intervention at all, from the development of new code until its deployment.&lt;/p&gt; &#xA;&lt;p&gt;In this post, I will be going through the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basics of GitHub Actions&lt;/li&gt; &#xA; &lt;li&gt;First GitHub Action&lt;/li&gt; &#xA; &lt;li&gt;Creating Google Service Account&lt;/li&gt; &#xA; &lt;li&gt;Giving access to Service account&lt;/li&gt; &#xA; &lt;li&gt;Configuring DVC to use Google Service account&lt;/li&gt; &#xA; &lt;li&gt;Configuring Github Action&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/basic_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dvc.org/doc/user-guide/setup-google-drive-remote&#34;&gt;Configuring service account&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://docs.github.com/en/actions/quickstart&#34;&gt;Github actions&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 7: Container Registry - AWS ECR&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-container-registry&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A container registry is a place to store container images. A container image is a file comprised of multiple layers which can execute applications in a single instance. Hosting all the images in one stored location allows users to commit, identify and pull images when needed.&lt;/p&gt; &#xA;&lt;p&gt;Amazon Simple Storage Service (S3) is a storage for the internet. It is designed for large-capacity, low-cost storage provision across multiple geographical regions.&lt;/p&gt; &#xA;&lt;p&gt;In this week, I will be going through the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of S3&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Programmatic access to S3&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Configuring AWS S3 as remote storage in DVC&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of ECR&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Configuring GitHub Actions to use S3, ECR&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/ecr_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Week 8: Serverless Deployment - AWS Lambda&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-serverless&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A serverless architecture is a way to build and run applications and services without having to manage infrastructure. The application still runs on servers, but all the server management is done by third party service (AWS). We no longer have to provision, scale, and maintain servers to run the applications. By using a serverless architecture, developers can focus on their core product instead of worrying about managing and operating servers or runtimes, either in the cloud or on-premises.&lt;/p&gt; &#xA;&lt;p&gt;In this week, I will be going through the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of Serverless&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of AWS Lambda&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Triggering Lambda with API Gateway&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Deploying Container using Lambda&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Automating deployment to Lambda using Github Actions&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/lambda_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Week 9: Prediction Monitoring - Kibana&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://deep-learning-blogs.vercel.app/blog/mlops-monitoring&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Monitoring systems can help give us confidence that our systems are running smoothly and, in the event of a system failure, can quickly provide appropriate context when diagnosing the root cause.&lt;/p&gt; &#xA;&lt;p&gt;Things we want to monitor during and training and inference are different. During training we are concered about whether the loss is decreasing or not, whether the model is overfitting, etc.&lt;/p&gt; &#xA;&lt;p&gt;But, during inference, We like to have confidence that our model is making correct predictions.&lt;/p&gt; &#xA;&lt;p&gt;There are many reasons why a model can fail to make useful predictions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The underlying data distribution has shifted over time and the model has gone stale. i.e inference data characteristics is different from the data characteristics used to train the model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The inference data stream contains edge cases (not seen during model training). In this scenarios model might perform poorly or can lead to errors.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The model was misconfigured in its production deployment. (Configuration issues are common)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In all of these scenarios, the model could still make a &lt;code&gt;successful&lt;/code&gt; prediction from a service perspective, but the predictions will likely not be useful. Monitoring machine learning models can help us detect such scenarios and intervene (e.g. trigger a model retraining/deployment pipeline).&lt;/p&gt; &#xA;&lt;p&gt;In this week, I will be going through the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of Cloudwatch Logs&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Creating Elastic Search Cluster&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Configuring Cloudwatch Logs with Elastic Search&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Creating Index Patterns in Kibana&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Creating Kibana Visualisations&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Creating Kibana Dashboard&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/kibana_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>roboflow/supervision</title>
    <updated>2024-09-29T01:38:41Z</updated>
    <id>tag:github.com,2024-09-29:/roboflow/supervision</id>
    <link href="https://github.com/roboflow/supervision" rel="alternate"></link>
    <summary type="html">&lt;p&gt;We write your reusable computer vision tools. 💜&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;a align=&#34;center&#34; href=&#34;&#34; target=&#34;https://supervision.roboflow.com&#34;&gt; &lt;img width=&#34;100%&#34; src=&#34;https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/notebooks&#34;&gt;notebooks&lt;/a&gt; | &lt;a href=&#34;https://github.com/roboflow/inference&#34;&gt;inference&lt;/a&gt; | &lt;a href=&#34;https://github.com/autodistill/autodistill&#34;&gt;autodistill&lt;/a&gt; | &lt;a href=&#34;https://github.com/roboflow/multimodal-maestro&#34;&gt;maestro&lt;/a&gt;&lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/supervision&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/supervision.svg?sanitize=true&#34; alt=&#34;version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypistats.org/packages/supervision&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/supervision&#34; alt=&#34;downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://snyk.io/advisor/python/supervision&#34;&gt;&lt;img src=&#34;https://snyk.io/advisor/python/supervision/badge.svg?sanitize=true&#34; alt=&#34;snyk&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/roboflow/supervision/raw/main/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/supervision&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/supervision&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/supervision&#34; alt=&#34;python-version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/roboflow/supervision/blob/main/demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Roboflow/Annotators&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;gradio&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/GbfgXGJ8Bk&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1159501506232451173&#34; alt=&#34;discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://squidfunk.github.io/mkdocs-material/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Material_for_MkDocs-526CFE?logo=MaterialForMkDocs&amp;amp;logoColor=white&#34; alt=&#34;built-with-material-for-mkdocs&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://trendshift.io/repositories/124&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/124&#34; alt=&#34;roboflow%2Fsupervision | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;👋 hello&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;We write your reusable computer vision tools.&lt;/strong&gt; Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! 🤝&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/orgs/roboflow/projects&#34;&gt;&lt;img src=&#34;https://github.com/roboflow/supervision/assets/26109316/c05cc954-b9a6-4ed5-9a52-d0b4b619ff65&#34; alt=&#34;supervision-hackfest&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;💻 install&lt;/h2&gt; &#xA;&lt;p&gt;Pip install the supervision package in a &lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;strong&gt;Python&amp;gt;=3.8&lt;/strong&gt;&lt;/a&gt; environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install supervision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Read more about conda, mamba, and installing from source in our &lt;a href=&#34;https://roboflow.github.io/supervision/&#34;&gt;guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🔥 quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;models&lt;/h3&gt; &#xA;&lt;p&gt;Supervision was designed to be model agnostic. Just plug in any classification, detection, or segmentation model. For your convenience, we have created &lt;a href=&#34;https://supervision.roboflow.com/latest/detection/core/#detections&#34;&gt;connectors&lt;/a&gt; for the most popular libraries like Ultralytics, Transformers, or MMDetection.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import supervision as sv&#xA;from ultralytics import YOLO&#xA;&#xA;image = cv2.imread(...)&#xA;model = YOLO(&#34;yolov8s.pt&#34;)&#xA;result = model(image)[0]&#xA;detections = sv.Detections.from_ultralytics(result)&#xA;&#xA;len(detections)&#xA;# 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;👉 more model connectors&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;inference&lt;/p&gt; &lt;p&gt;Running with &lt;a href=&#34;https://github.com/roboflow/inference&#34;&gt;Inference&lt;/a&gt; requires a &lt;a href=&#34;https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key&#34;&gt;Roboflow API KEY&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import supervision as sv&#xA;from inference import get_model&#xA;&#xA;image = cv2.imread(...)&#xA;model = get_model(model_id=&#34;yolov8s-640&#34;, api_key=&amp;lt;ROBOFLOW API KEY&amp;gt;)&#xA;result = model.infer(image)[0]&#xA;detections = sv.Detections.from_inference(result)&#xA;&#xA;len(detections)&#xA;# 5&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;annotators&lt;/h3&gt; &#xA;&lt;p&gt;Supervision offers a wide range of highly customizable &lt;a href=&#34;https://supervision.roboflow.com/latest/detection/annotators/&#34;&gt;annotators&lt;/a&gt;, allowing you to compose the perfect visualization for your use case.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import supervision as sv&#xA;&#xA;image = cv2.imread(...)&#xA;detections = sv.Detections(...)&#xA;&#xA;box_annotator = sv.BoxAnnotator()&#xA;annotated_frame = box_annotator.annotate(&#xA;  scene=image.copy(),&#xA;  detections=detections)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce&#34;&gt;https://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;datasets&lt;/h3&gt; &#xA;&lt;p&gt;Supervision provides a set of &lt;a href=&#34;https://supervision.roboflow.com/latest/datasets/core/&#34;&gt;utils&lt;/a&gt; that allow you to load, split, merge, and save datasets in one of the supported formats.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import supervision as sv&#xA;from roboflow import Roboflow&#xA;&#xA;project = Roboflow().workspace(&amp;lt;WORKSPACE_ID&amp;gt;).project(&amp;lt;PROJECT_ID&amp;gt;)&#xA;dataset = project.version(&amp;lt;PROJECT_VERSION&amp;gt;).download(&#34;coco&#34;)&#xA;&#xA;ds = sv.DetectionDataset.from_coco(&#xA;    images_directory_path=f&#34;{dataset.location}/train&#34;,&#xA;    annotations_path=f&#34;{dataset.location}/train/_annotations.coco.json&#34;,&#xA;)&#xA;&#xA;path, image, annotation = ds[0]&#xA;    # loads image on demand&#xA;&#xA;for path, image, annotation in ds:&#xA;    # loads image on demand&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details close&gt; &#xA; &lt;summary&gt;👉 more dataset utils&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;load&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset = sv.DetectionDataset.from_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;)&#xA;&#xA;dataset = sv.DetectionDataset.from_pascal_voc(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...&#xA;)&#xA;&#xA;dataset = sv.DetectionDataset.from_coco(&#xA;    images_directory_path=...,&#xA;    annotations_path=...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;split&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_dataset, test_dataset = dataset.split(split_ratio=0.7)&#xA;test_dataset, valid_dataset = test_dataset.split(split_ratio=0.5)&#xA;&#xA;len(train_dataset), len(test_dataset), len(valid_dataset)&#xA;# (700, 150, 150)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;merge&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ds_1 = sv.DetectionDataset(...)&#xA;len(ds_1)&#xA;# 100&#xA;ds_1.classes&#xA;# [&#39;dog&#39;, &#39;person&#39;]&#xA;&#xA;ds_2 = sv.DetectionDataset(...)&#xA;len(ds_2)&#xA;# 200&#xA;ds_2.classes&#xA;# [&#39;cat&#39;]&#xA;&#xA;ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])&#xA;len(ds_merged)&#xA;# 300&#xA;ds_merged.classes&#xA;# [&#39;cat&#39;, &#39;dog&#39;, &#39;person&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;save&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset.as_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;)&#xA;&#xA;dataset.as_pascal_voc(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...&#xA;)&#xA;&#xA;dataset.as_coco(&#xA;    images_directory_path=...,&#xA;    annotations_path=...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;convert&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sv.DetectionDataset.from_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;).as_pascal_voc(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;🎬 tutorials&lt;/h2&gt; &#xA;&lt;p&gt;Want to learn how to use Supervision? Explore our &lt;a href=&#34;https://supervision.roboflow.com/develop/how_to/detect_and_annotate/&#34;&gt;how-to guides&lt;/a&gt;, &lt;a href=&#34;https://github.com/roboflow/supervision/tree/develop/examples&#34;&gt;end-to-end examples&lt;/a&gt;, &lt;a href=&#34;https://roboflow.github.io/cheatsheet-supervision/&#34;&gt;cheatsheet&lt;/a&gt;, and &lt;a href=&#34;https://supervision.roboflow.com/develop/cookbooks/&#34;&gt;cookbooks&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://youtu.be/hAWpsIuem10&#34; title=&#34;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&#34;&gt;&lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/assets/26109316/a742823d-c158-407d-b30f-063a5d11b4e1&#34; alt=&#34;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&#34; width=&#34;300px&#34; align=&#34;left&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/hAWpsIuem10&#34; title=&#34;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&#34;&gt;&lt;strong&gt;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;div&gt;&#xA; &lt;strong&gt;Created: 5 Apr 2024&lt;/strong&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt;Learn how to use computer vision to analyze wait times and optimize processes. This tutorial covers object detection, tracking, and calculating time spent in designated zones. Use these techniques to improve customer experience in retail, traffic management, or other scenarios.&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://youtu.be/uWP6UjDeZvY&#34; title=&#34;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&#34;&gt;&lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/assets/26109316/61a444c8-b135-48ce-b979-2a5ab47c5a91&#34; alt=&#34;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&#34; width=&#34;300px&#34; align=&#34;left&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/uWP6UjDeZvY&#34; title=&#34;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&#34;&gt;&lt;strong&gt;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;div&gt;&#xA; &lt;strong&gt;Created: 11 Jan 2024&lt;/strong&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt;Learn how to track and estimate the speed of vehicles using YOLO, ByteTrack, and Roboflow Inference. This comprehensive tutorial covers object detection, multi-object tracking, filtering detections, perspective transformation, speed estimation, visualization improvements, and more.&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;💜 built with supervision&lt;/h2&gt; &#xA;&lt;p&gt;Did you build something cool using supervision? &lt;a href=&#34;https://github.com/roboflow/supervision/discussions/categories/built-with-supervision&#34;&gt;Let us know!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4&#34;&gt;https://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900&#34;&gt;https://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f&#34;&gt;https://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📚 documentation&lt;/h2&gt; &#xA;&lt;p&gt;Visit our &lt;a href=&#34;https://roboflow.github.io/supervision&#34;&gt;documentation&lt;/a&gt; page to learn how supervision can help you build computer vision applications faster and more reliably.&lt;/p&gt; &#xA;&lt;h2&gt;🏆 contribution&lt;/h2&gt; &#xA;&lt;p&gt;We love your input! Please see our &lt;a href=&#34;https://github.com/roboflow/supervision/raw/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to get started. Thank you 🙏 to all our contributors!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/roboflow/supervision/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=roboflow/supervision&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://youtube.com/roboflow&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634652&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949746649&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://www.linkedin.com/company/roboflow-ai/&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633691&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://docs.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634511&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://discuss.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633584&#34; width=&#34;3%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &lt;/a&gt;&#xA;  &lt;a href=&#34;https://blog.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633605&#34; width=&#34;3%&#34;&gt; &lt;/a&gt;  &#xA; &lt;/div&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>