<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-05T01:36:23Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>DrewThomasson/ebook2audiobook</title>
    <updated>2025-01-05T01:36:23Z</updated>
    <id>tag:github.com,2025-01-05:/DrewThomasson/ebook2audiobook</id>
    <link href="https://github.com/DrewThomasson/ebook2audiobook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Convert ebooks to audiobooks with chapters and metadata using dynamic AI models and voice cloning. Supports 1,107+ languages!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üìö ebook2audiobook&lt;/h1&gt; &#xA;&lt;p&gt;CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br&gt; using Calibre, ffmpeg, XTTSv2, Fairseq and more. Supports voice cloning and 1124 languages!&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] &lt;strong&gt;This tool is intended for use with non-DRM, legally acquired eBooks only.&lt;/strong&gt; &lt;br&gt; The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br&gt; Use this tool responsibly and in accordance with all applicable laws.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/bg5Kx43c6w&#34;&gt;&lt;img src=&#34;https://dcbadge.limes.pink/api/server/https://discord.gg/bg5Kx43c6w&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;New v2.0 Web GUI Interface!&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/demo_web_gui.gif&#34; alt=&#34;demo_web_gui&#34;&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;GUI Screen 1&#34; src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_1.png&#34;&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;GUI Screen 2&#34; src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_2.png&#34;&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;GUI Screen 3&#34; src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_3.png&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;README.md&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ara &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/readme/README_AR.md&#34;&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;zho &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/readme/README_CN.md&#34;&gt;‰∏≠Êñá (Chinese)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;eng &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/README.md&#34;&gt;English&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;swe &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/readme/README_SWE.md&#34;&gt;Svenska (Swedish)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#ebook2audiobook&#34;&gt;ebook2audiobook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#new-v20-web-gui-interface&#34;&gt;New v2.0 Web GUI Interface&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#huggingface-space-demo&#34;&gt;Huggingface Space Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#free-google-colab&#34;&gt;Free Google Colab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#demos&#34;&gt;Pre-made Audio Demos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-languages&#34;&gt;Supported Languages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#installation-instructions&#34;&gt;Installation Instructions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface&#34;&gt;Launching Gradio Web Interface&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#basic-headless-usage&#34;&gt;Basic Headless Usage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#headless-custom-xtts-model-usage&#34;&gt;Headless Custom XTTS Model Usage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#renting-a-gpu&#34;&gt;Renting a GPU&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output&#34;&gt;Help command output&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-models&#34;&gt;Fine Tuned TTS models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-collection&#34;&gt;For Collection of Fine-Tuned TTS Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#using-docker&#34;&gt;Using Docker&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#running-the-docker-container&#34;&gt;Docker Run&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container&#34;&gt;Docker Build&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-compose&#34;&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-headless-guide&#34;&gt;Docker headless guide&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-container-file-locations&#34;&gt;Docker container file locations&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-docker-issues&#34;&gt;Common Docker issues&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-ebook-formats&#34;&gt;Supported eBook Formats&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#output&#34;&gt;Output&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-issues&#34;&gt;Common Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#special-thanks&#34;&gt;Special Thanks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#join-our-discord-server&#34;&gt;Join Our Discord Server!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#legacy-v10&#34;&gt;Legacy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#glossary-of-sections&#34;&gt;Glossary of Sections&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìñ Converts eBooks to text format with Calibre.&lt;/li&gt; &#xA; &lt;li&gt;üìö Splits eBook into chapters for organized audio.&lt;/li&gt; &#xA; &lt;li&gt;üéôÔ∏è High-quality text-to-speech with &lt;a href=&#34;https://huggingface.co/coqui/XTTS-v2&#34;&gt;Coqui XTTSv2&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/fairseq/tree/main/examples/mms&#34;&gt;Fairseq&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üó£Ô∏è Optional voice cloning with your own voice file.&lt;/li&gt; &#xA; &lt;li&gt;üåç Supports 1107 languages (English by default). &lt;a href=&#34;https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html&#34;&gt;List of Supported languages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üñ•Ô∏è Designed to run on 4GB RAM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://huggingface.co/spaces/drewThomasson/ebook2audiobook&#34;&gt;Huggingface space demo&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/drewThomasson/ebook2audiobook&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=for-the-badge&amp;amp;logo=huggingface&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Huggingface space is running on free cpu tier so expect very slow or timeout lol, just don&#39;t give it giant files is all&lt;/li&gt; &#xA; &lt;li&gt;Best to duplicate space or run locally.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Free Google Colab&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Free Google Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Supported Languages&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Arabic (ara)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chinese (zho)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Czech (ces)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dutch (nld)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;English (eng)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;French (fra)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;German (deu)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hindi (hin)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hungarian (hun)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Italian (ita)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Japanese (jpn)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Korean (kor)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Polish (pol)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Portuguese (por)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Russian (rus)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spanish (spa)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Turkish (tur)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vietnamese (vie)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html&#34;&gt;** + 1107 languages via Fairseq**&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;4gb ram&lt;/li&gt; &#xA; &lt;li&gt;Virtualization enabled if running on windows (Docker only)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation Instructions&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Clone repo&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Specify the language code when running the script in mode.&lt;/p&gt; &#xA;&lt;h3&gt;Launching Gradio Web Interface&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run ebook2audiobook&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./ebook2audiobook.sh  # Run Launch script&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.\ebook2audiobook.cmd  # Run launch script&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open the Web App&lt;/strong&gt;: Click the URL provided in the terminal to access the web app and convert eBooks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Public Link&lt;/strong&gt;: Add &lt;code&gt;--share&lt;/code&gt; to the end of it like this: &lt;code&gt;python app.py --share&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[For More Parameters]&lt;/strong&gt;: use the &lt;code&gt;--help&lt;/code&gt; parameter like this &lt;code&gt;python app.py --help&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Basic Usage&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./ebook2audiobook.sh  -- --ebook &amp;lt;path_to_ebook_file&amp;gt; --voice [path_to_voice_file] --language [language_code]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.\ebook2audiobook.cmd  -- --ebook &amp;lt;path_to_ebook_file&amp;gt; --voice [path_to_voice_file] --language [language_code]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;path_to_ebook_file&amp;gt;&lt;/strong&gt;: Path to your eBook file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[path_to_voice_file]&lt;/strong&gt;: Optional for voice cloning.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[language_code]&lt;/strong&gt;: Optional to specify ISO-639-3 3+ letters language code (default is eng). ISO-639-1 2 letters code is also supported&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[For More Parameters]&lt;/strong&gt;: use the &lt;code&gt;--help&lt;/code&gt; parameter like this &lt;code&gt;python app.py --help&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Custom XTTS Model Usage&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./ebook2audiobook.sh  -- --ebook &amp;lt;ebook_file_path&amp;gt; --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt; --custom_config &amp;lt;custom_config_path&amp;gt; --custom_vocab &amp;lt;custom_vocab_path&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.\ebook2audiobook.cmd  -- --ebook &amp;lt;ebook_file_path&amp;gt; --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt; --custom_config &amp;lt;custom_config_path&amp;gt; --custom_vocab &amp;lt;custom_vocab_path&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;ebook_file_path&amp;gt;&lt;/strong&gt;: Path to your eBook file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;target_voice_file_path&amp;gt;&lt;/strong&gt;: Optional for voice cloning.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&#xA;    &lt;language&gt;&lt;/language&gt;&lt;/strong&gt;: Optional to specify language.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;custom_model_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;model.pth&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;custom_config_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;config.json&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&amp;lt;custom_vocab_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;vocab.json&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[For More Parameters]&lt;/strong&gt;: use the &lt;code&gt;--help&lt;/code&gt; parameter like this &lt;code&gt;python app.py --help&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;For Detailed Guide with list of all Parameters to use&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./ebook2audiobook.sh  --help&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.\ebook2audiobook.cmd  --help&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a id=&#34;help-command-output&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This will output the following:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;usage: app.py [-h] [--script_mode SCRIPT_MODE] [--share] [-- []]&#xA;              [--session SESSION] [--ebook EBOOK] [--ebooks_dir [EBOOKS_DIR]]&#xA;              [--voice VOICE] [--language LANGUAGE] [--device {cpu,gpu}]&#xA;              [--custom_model CUSTOM_MODEL] [--temperature TEMPERATURE]&#xA;              [--length_penalty LENGTH_PENALTY]&#xA;              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K] [--top_p TOP_P]&#xA;              [--speed SPEED] [--enable_text_splitting] [--fine_tuned FINE_TUNED]&#xA;              [--version]&#xA;&#xA;Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in  mode for direct conversion.&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  --script_mode SCRIPT_MODE&#xA;                        Force the script to run in NATIVE or DOCKER_UTILS&#xA;  --share               Enable a public shareable Gradio link. Default to False.&#xA;  -- []&#xA;                        Run in  mode. Default to True if the flag is present without a value, False otherwise.&#xA;  --session SESSION     Session to reconnect in case of interruption ( mode only)&#xA;  --ebook EBOOK         Path to the ebook file for conversion. Required in  mode.&#xA;  --ebooks_dir [EBOOKS_DIR]&#xA;                        Path to the directory containing ebooks for batch conversion. Default to &#34;ebooks&#34; if &#34;default&#34; is provided.&#xA;  --voice VOICE         Path to the target voice file for TTS. Optional, must be 24khz for XTTS and 16khz for fairseq models, uses a default voice if not provided.&#xA;  --language LANGUAGE   Language for the audiobook conversion. Options: eng, zho, spa, fra, por, rus, ind, hin, ben, yor, ara, jav, jpn, kor, deu, ita, fas, tam, tel, tur, pol, hun, nld, zzzz, abi, ace, aca, acn, acr, ach, acu, guq, ade, adj, agd, agx, agn, aha, aka, knj, ake, aeu, ahk, bss, alj, sqi, alt, alp, alz, kab, amk, mmg, amh, ami, azg, agg, boj, cko, any, arl, atq, luc, hyw, apr, aia, msy, cni, cjo, cpu, cpb, asm, asa, teo, ati, djk, ava, avn, avu, awb, kwi, awa, agr, agu, ayr, ayo, abp, blx, sgb, azj-script_cyrillic, azj-script_latin, azb, bba, bhz, bvc, bfy, bgq, bdq, bdh, bqi, bjw, blz, ban, bcc-script_latin, bcc-script_arabic, bam, ptu, bcw, bqj, bno, bbb, bfa, bjz, bak, eus, bsq, akb, btd, btx, bts, bbc, bvz, bjv, bep, bkv, bzj, bem, bng, bom, btt, bha, bgw, bht, beh, sne, ubl, bcl, bim, bkd, bjr, bfo, biv, bib, bis, bzi, bqp, bpr, bps, bwq, bdv, bqc, bus, bnp, bmq, bdg, boa, ksr, bor, bru, box, bzh, bgt, sab, bul, bwu, bmv, mya, tte, cjp, cbv, kaq, cot, cbc, car, cat, ceb, cme, cbi, ceg, cly, cya, che, hne, nya, dig, dug, bgr, cek, cfm, cnh, hlt, mwq, ctd, tcz, zyp, cco, cnl, cle, chz, cpa, cso, cnt, cuc, hak, nan, xnj, cap, cax, ctg, ctu, chf, cce, crt, crq, cac-dialect_sansebasti√°ncoat√°n, cac-dialect_sanmateoixtat√°n, ckt, ncu, cdj, chv, caa, asg, con, crn, cok, crk-script_latin, crk-script_syllabics, crh, hrv, cui, ces, dan, dsh, dbq, dga, dgi, dgk, dnj-dialect_gweetaawueast, dnj-dialect_blowowest, daa, dnt, dnw, dar, tcc, dwr, ded, mzw, ntr, ddn, des, dso, nfa, dhi, gud, did, mhu, dip, dik, tbz, dts, dos, dgo, mvp, jen, dzo, idd, eka, cto, emp, enx, sja, myv, mcq, ese, evn, eza, ewe, fal, fao, far, fij, fin, fon, frd, ful, flr, gau, gbk, gag-script_cyrillic, gag-script_latin, gbi, gmv, lug, pwg, gbm, cab, grt, krs, gso, nlg, gej, gri, kik, acd, glk, gof-script_latin, gog, gkn, wsg, gjn, gqr, gor, gux, gbo, ell, grc, guh, gub, grn, gyr, guo, gde, guj, gvl, guk, rub, dah, gwr, gwi, hat, hlb, amf, hag, hnn, bgc, had, hau, hwc, hvn, hay, xed, heb, heh, hil, hif, hns, hoc, hoy, hus-dialect_westernpotosino, hus-dialect_centralveracruz, huv, hui, hap, iba, isl, dbj, ifa, ifb, ifu, ifk, ife, ign, ikk, iqw, ilb, ilo, imo, inb, ipi, irk, icr, itv, itl, atg, ixl-dialect_sanjuancotzal, ixl-dialect_sangasparchajul, ixl-dialect_santamarianebaj, nca, izr, izz, jac, jam, jvn, kac, dyo, csk, adh, jun, jbu, dyu, bex, juy, gna, urb, kbp, cwa, dtp, kbr, cgc, kki, kzf, lew, cbr, kkj, keo, kqe, kak, kyb, knb, kmd, kml, ify, xal, kbq, kay, ktb, hig, gam, cbu, xnr, kmu, kne, kan, kby, pam, cak-dialect_santamar√≠adejes√∫s, cak-dialect_southcentral, cak-dialect_yepocapa, cak-dialect_western, cak-dialect_santodomingoxenacoj, cak-dialect_central, xrb, krc, kaa, krl, pww, xsm, cbs, pss, kxf, kyz, kyu, txu, kaz, ndp, kbo, kyq, ken, ker, xte, kyg, kjh, kca, khm, kxm, kjg, nyf, kij, kia, kqr, kqp, krj, zga, kin, pkb, geb, gil, kje, kss, thk, klu, kyo, kog, kfb, kpv, bbo, xon, kma, kno, kxc, ozm, kqy, coe, kpq, kpy, kyf, kff-script_telugu, kri, rop, ktj, ted, krr, kdt, kez, cul, kle, kdi, kue, kum, kvn, cuk, kdn, xuo, key, kpz, knk, kmr-script_latin, kmr-script_arabic, kmr-script_cyrillic, xua, kru, kus, kub, kdc, kxv, blh, cwt, kwd, tnk, kwf, cwe, kyc, tye, kir, quc-dialect_north, quc-dialect_east, quc-dialect_central, lac, lsi, lbj, lhu, las, lam, lns, ljp, laj, lao, lat, lav, law, lcp, lzz, lln, lef, acf, lww, mhx, eip, lia, lif, onb, lis, loq, lob, yaz, lok, llg, ycl, lom, ngl, lon, lex, lgg, ruf, dop, lnd, ndy, lwo, lee, mev, mfz, jmc, myy, mbc, mda, mad, mag, ayz, mai, mca, mcp, mak, vmw, mgh, kde, mlg, zlm, pse, mkn, xmm, mal, xdy, div, mdy, mup, mam-dialect_central, mam-dialect_northern, mam-dialect_southern, mam-dialect_western, mqj, mcu, mzk, maw, mjl, mnk, mge, mbh, knf, mjv, mbt, obo, mbb, mzj, sjm, mrw, mar, mpg, mhr, enb, mah, myx, klv, mfh, met, mcb, mop, yua, mfy, maz, vmy, maq, mzi, maj, maa-dialect_sanantonio, maa-dialect_sanjer√≥nimo, mhy, mhi, zmz, myb, gai, mqb, mbu, med, men, mee, mwv, meq, zim, mgo, mej, mpp, min, gum, mpx, mco, mxq, pxm, mto, mim, xta, mbz, mip, mib, miy, mih, miz, xtd, mxt, xtm, mxv, xtn, mie, mil, mio, mdv, mza, mit, mxb, mpm, soy, cmo-script_latin, cmo-script_khmer, mfq, old, mfk, mif, mkl, mox, myl, mqf, mnw, mon, mog, mfe, mor, mqn, mgd, mtj, cmr, mtd, bmr, moz, mzm, mnb, mnf, unr, fmu, mur, tih, muv, muy, sur, moa, wmw, tnr, miq, mos, muh, nas, mbj, nfr, kfw, nst, nag, nch, nhe, ngu, azz, nhx, ncl, nhy, ncj, nsu, npl, nuz, nhw, nhi, nlc, nab, gld, nnb, npy, pbb, ntm, nmz, naw, nxq, ndj, ndz, ndv, new, nij, sba, gng, nga, nnq, ngp, gym, kdj, nia, nim, nin, nko, nog, lem, not, nhu, nob, bud, nus, yas, nnw, nwb, nyy, nyn, rim, lid, nuj, nyo, nzi, ann, ory, ojb-script_latin, ojb-script_syllabics, oku, bsc, bdu, orm, ury, oss, ote, otq, stn, sig, kfx, bfz, sey, pao, pau, pce, plw, pmf, pag, pap, prf, pab, pbi, pbc, pad, ata, pez, peg, pcm, pis, pny, pir, pjt, poy, pps, pls, poi, poh-dialect_eastern, poh-dialect_western, prt, pui, pan, tsz, suv, lme, quy, qvc, quz, qve, qub, qvh, qwh, qvw, quf, qvm, qul, qvn, qxn, qxh, qvs, quh, qxo, qxr, qvo, qvz, qxl, quw, kjb, kek, rah, rjs, rai, lje, rnl, rkt, rap, yea, raw, rej, rel, ril, iri, rgu, rhg, rmc-script_latin, rmc-script_cyrillic, rmo, rmy-script_latin, rmy-script_cyrillic, ron, rol, cla, rng, rug, run, lsm, spy, sck, saj, sch, sml, xsb, sbl, saq, sbd, smo, rav, sxn, sag, sbp, xsu, srm, sas, apb, sgw, tvw, lip, slu, snw, sea, sza, seh, crs, ksb, shn, sho, mcd, cbt, xsr, shk, shp, sna, cjs, jiv, snp, sya, sid, snn, sri, srx, sil, sld, akp, xog, som, bmu, khq, ses, mnx, srn, sxb, suc, tgo, suk, sun, suz, sgj, sus, swh, swe, syl, dyi, myk, spp, tap, tby, tna, shi, klw, tgl, tbk, tgj, blt, tbg, omw, tgk, tdj, tbc, tlj, tly, ttq-script_tifinagh, taj, taq, tpm, tgp, tnn, tac, rif-script_latin, rif-script_arabic, tat, tav, twb, tbl, kps, twe, ttc, kdh, tes, tex, tee, tpp, tpt, stp, tfr, twu, ter, tew, tha, nod, thl, tem, adx, bod, khg, tca, tir, txq, tik, dgr, tob, tmf, tng, tlb, ood, tpi, jic, lbw, txa, tom, toh, tnt, sda, tcs, toc, tos, neb, trn, trs, trc, tri, cof, tkr, kdl, cas, tso, tuo, iou, tmc, tuf, tuk-script_latin, tuk-script_arabic, bov, tue, kcg, tzh-dialect_bachaj√≥n, tzh-dialect_tenejapa, tzo-dialect_chenalh√≥, tzo-dialect_chamula, tzj-dialect_western, tzj-dialect_eastern, aoz, udm, udu, ukr, ppk, ubu, urk, ura, urt, urd-script_devanagari, urd-script_arabic, urd-script_latin, upv, usp, uig-script_arabic, uig-script_cyrillic, uzb-script_cyrillic, vag, bav, vid, vie, vif, vun, vut, prk, wwa, rro, bao, waw, lgl, wlx, cou, hub, gvc, mfi, wap, wba, war, way, guc, cym, kvw, tnp, hto, huu, wal-script_latin, wal-script_ethiopic, wlo, noa, wob, kao, xer, yad, yka, sah, yba, yli, nlk, yal, yam, yat, jmd, tao, yaa, ame, guu, yao, yre, yva, ybb, pib, byr, pil, ycn, ess, yuz, atb, zne, zaq, zpo, zad, zpc, zca, zpg, zai, zpl, zam, zaw, zpm, zac, zao, ztq, zar, zpt, zpi, zas, zaa, zpz, zab, zpu, zae, zty, zav, zza, zyb, ziw, zos, gnd. Default to English (eng).&#xA;  --device {cpu,gpu}    Type of processor unit for the audiobook conversion. If not specified: check first if gpu available, if not cpu is selected.&#xA;  --custom_model CUSTOM_MODEL&#xA;                        Path to the custom model (.zip file containing [&#39;config.json&#39;, &#39;vocab.json&#39;, &#39;model.pth&#39;, &#39;ref.wav&#39;]). Required if using a custom model.&#xA;  --temperature TEMPERATURE&#xA;                        Temperature for the model. Default to 0.65. Higher temperatures lead to more creative outputs.&#xA;  --length_penalty LENGTH_PENALTY&#xA;                        A length penalty applied to the autoregressive decoder. Default to 1.0. Not applied to custom models.&#xA;  --repetition_penalty REPETITION_PENALTY&#xA;                        A penalty that prevents the autoregressive decoder from repeating itself. Default to 2.5&#xA;  --top_k TOP_K         Top-k sampling. Lower values mean more likely outputs and increased audio generation speed. Default to 50&#xA;  --top_p TOP_P         Top-p sampling. Lower values mean more likely outputs and increased audio generation speed. Default to 0.8&#xA;  --speed SPEED         Speed factor for the speech generation. Default to 1.0&#xA;  --enable_text_splitting&#xA;                        Enable splitting text into sentences. Default to False.&#xA;  --fine_tuned FINE_TUNED&#xA;                        Name of the fine tuned model. Optional, uses the standard model according to the TTS engine and language.&#xA;  --version             Show the version of the script and exit&#xA;&#xA;Example usage:    &#xA;Windows:&#xA;    :&#xA;    ebook2audiobook.cmd -- --ebook &#39;path_to_ebook&#39;&#xA;    Graphic Interface:&#xA;    ebook2audiobook.cmd&#xA;Linux/Mac:&#xA;    :&#xA;    ./ebook2audiobook.sh -- --ebook &#39;path_to_ebook&#39;&#xA;    Graphic Interface:&#xA;    ./ebook2audiobook.sh&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Docker&lt;/h3&gt; &#xA;&lt;p&gt;You can also use Docker to run the eBook to Audiobook converter. This method ensures consistency across different environments and simplifies setup.&lt;/p&gt; &#xA;&lt;h4&gt;Running the Docker Container&lt;/h4&gt; &#xA;&lt;p&gt;To run the Docker container and start the Gradio interface, use the following command:&lt;/p&gt; &#xA;&lt;p&gt;-Run with CPU only&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;docker run -it --rm -p 7860:7860 --platform=linux/amd64 athomasson2/ebook2audiobook python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;-Run with GPU Speedup (Nvida graphics cards only)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;docker run -it --rm --gpus all -p 7860:7860 --platform=linux/amd64 athomasson2/ebook2audiobook python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Building the Docker Container&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can build the docker image with the command: &#39;&#39;&#39;powershell docker build --platform linux/amd64 -t athomasson2/ebook2audiobook . &#39;&#39;&#39;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This command will start the Gradio interface on port 7860.(localhost:7860)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For more options like running the docker in mode or making the gradio link public add the &lt;code&gt;--help&lt;/code&gt; parameter after the &lt;code&gt;app.py&lt;/code&gt; in the docker launch command&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker container file locations&lt;/h2&gt; &#xA;&lt;p&gt;All ebook2audiobooks will have the base dir of &lt;code&gt;/home/user/app/&lt;/code&gt; For example: &lt;code&gt;tmp&lt;/code&gt; = &lt;code&gt;/home/user/app/tmp&lt;/code&gt; &lt;code&gt;audiobooks&lt;/code&gt; = &lt;code&gt;/home/user/app/audiobooks&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Docker headless guide&lt;/h2&gt; &#xA;&lt;p&gt;first for a docker pull of the latest with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull athomasson2/ebook2audiobook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Before you do run this you need to create a dir named &#34;input-folder&#34; in your current dir which will be linked, This is where you can put your input files for the docker image to see&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir input-folder &amp;amp;&amp;amp; mkdir Audiobooks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In the command below swap out &lt;strong&gt;YOUR_INPUT_FILE.TXT&lt;/strong&gt; with the name of your input file&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm \&#xA;    -v $(pwd)/input-folder:/home/user/app/input_folder \&#xA;    -v $(pwd)/audiobooks:/home/user/app/audiobooks \&#xA;    --platform linux/amd64 \&#xA;    athomasson2/ebook2audiobook \&#xA;    python app.py --headless --ebook /input_folder/YOUR_INPUT_FILE.TXT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;And that should be it!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The output Audiobooks will be found in the Audiobook folder which will also be located in your local dir you ran this docker command in&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;To get the help command for the other parameters this program has you can run this&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm \&#xA;    --platform linux/amd64 \&#xA;    athomasson2/ebook2audiobook \&#xA;    python app.py --help&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and that will output this&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output&#34;&gt;Help command output&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker Compose&lt;/h3&gt; &#xA;&lt;p&gt;This project uses Docker Compose to run locally. You can enable or disable GPU support by setting either &lt;code&gt;*gpu-enabled&lt;/code&gt; or &lt;code&gt;*gpu-disabled&lt;/code&gt; in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Steps to Run&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt; (if you haven&#39;t already):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git&#xA;cd ebook2audiobook&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set GPU Support (disabled by default)&lt;/strong&gt; To enable GPU support, modify &lt;code&gt;docker-compose.yml&lt;/code&gt; and change &lt;code&gt;*gpu-disabled&lt;/code&gt; to &lt;code&gt;*gpu-enabled&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the service:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Access the service:&lt;/strong&gt; The service will be available at &lt;a href=&#34;http://localhost:7860&#34;&gt;http://localhost:7860&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;New v2.0 Docker Web GUI Interface!&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/demo_web_gui.gif&#34; alt=&#34;demo_web_gui&#34;&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;GUI Screen 1&#34; src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_1.png&#34;&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;GUI Screen 2&#34; src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_2.png&#34;&gt; &#xA; &lt;img width=&#34;1728&#34; alt=&#34;GUI Screen 3&#34; src=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_3.png&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Renting a GPU&lt;/h2&gt; &#xA;&lt;p&gt;Don&#39;t have the hardware to run it or you want to rent a GPU?&lt;/p&gt; &#xA;&lt;h4&gt;You can duplicate the hugginface space and rent a gpu for around $0.40 an hour&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#huggingface-space-demo&#34;&gt;Huggingface Space Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Or you can try using the google colab for free!&lt;/h4&gt; &#xA;&lt;p&gt;(Be aware it will time out after a bit of your not messing with the google colab) &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#free-google-colab&#34;&gt;Free Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Common Docker Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker gets stuck downloading Fine-Tuned models. (This does not happen for every computer but some appear to run into this issue) Disabling the progress bar appears to fix the issue, as discussed &lt;a href=&#34;https://github.com/DrewThomasson/ebook2audiobook/issues/191&#34;&gt;here in #191&lt;/a&gt; Example of adding this fix in the &lt;code&gt;docker run&lt;/code&gt; command&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Dockerfile&#34;&gt;docker run -it --rm --gpus all -e HF_HUB_DISABLE_PROGRESS_BARS=1 -e HF_HUB_ENABLE_HF_TRANSFER=0 -p 7860:7860 --platform=linux/amd64 athomasson2/ebook2audiobook python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fine Tuned TTS models&lt;/h2&gt; &#xA;&lt;p&gt;You can fine-tune your own xtts model easily with this repo &lt;a href=&#34;https://github.com/daswer123/xtts-finetune-webui&#34;&gt;xtts-finetune-webui&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to rent a GPU easily you can also duplicate this huggingface &lt;a href=&#34;https://huggingface.co/spaces/drewThomasson/xtts-finetune-webui-gpu&#34;&gt;xtts-finetune-webui-space&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A space you can use to de-noise the training data easily also &lt;a href=&#34;https://huggingface.co/spaces/drewThomasson/DeepFilterNet2_no_limit&#34;&gt;denoise-huggingface-space&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Fine Tuned TTS Collection&lt;/h3&gt; &#xA;&lt;p&gt;To find our collection of already fine-tuned TTS models, visit &lt;a href=&#34;https://huggingface.co/drewThomasson/fineTunedTTSModels/tree/main&#34;&gt;this Hugging Face link&lt;/a&gt; For an XTTS custom model a ref audio clip of the voice will also be needed:&lt;/p&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;p&gt;Rainy day voice&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/8486603c-38b1-43ce-9639-73757dfb1031&#34;&gt;https://github.com/user-attachments/assets/8486603c-38b1-43ce-9639-73757dfb1031&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;David Attenborough voice&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/47c846a7-9e51-4eb9-844a-7460402a20a8&#34;&gt;https://github.com/user-attachments/assets/47c846a7-9e51-4eb9-844a-7460402a20a8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Supported eBook Formats&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.mobi&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.chm&lt;/code&gt;, &lt;code&gt;.lit&lt;/code&gt;, &lt;code&gt;.pdb&lt;/code&gt;, &lt;code&gt;.fb2&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.cbr&lt;/code&gt;, &lt;code&gt;.cbz&lt;/code&gt;, &lt;code&gt;.prc&lt;/code&gt;, &lt;code&gt;.lrf&lt;/code&gt;, &lt;code&gt;.pml&lt;/code&gt;, &lt;code&gt;.snb&lt;/code&gt;, &lt;code&gt;.cbc&lt;/code&gt;, &lt;code&gt;.rb&lt;/code&gt;, &lt;code&gt;.tcr&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Best results&lt;/strong&gt;: &lt;code&gt;.epub&lt;/code&gt; or &lt;code&gt;.mobi&lt;/code&gt; for automatic chapter detection&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Output&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creates an &lt;code&gt;.m4b&lt;/code&gt; file with metadata and chapters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Example Output&lt;/strong&gt;: &lt;img src=&#34;https://github.com/DrewThomasson/VoxNovel/raw/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg&#34; alt=&#34;Example&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Common Issues:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;It&#39;s slow!&#34; - On CPU only this is very slow, and you can only get speedups though a NVIDIA GPU. &lt;a href=&#34;https://github.com/DrewThomasson/ebook2audiobook/discussions/19#discussioncomment-10879846&#34;&gt;Discussion about this&lt;/a&gt; For faster multilingual generation I would suggest my other &lt;a href=&#34;https://github.com/DrewThomasson/ebook2audiobookpiper-tts&#34;&gt;project that uses piper-tts&lt;/a&gt; instead(It doesn&#39;t have zero-shot voice cloning though, and is siri quality voices, but it is much faster on cpu.)&lt;/li&gt; &#xA; &lt;li&gt;&#34;I&#39;m having dependency issues&#34; - Just use the docker, its fully self contained and has a headless mode, add &lt;code&gt;-h&lt;/code&gt; parameter after the &lt;code&gt;app.py&lt;/code&gt; in the docker run command for more information.&lt;/li&gt; &#xA; &lt;li&gt;&#34;Im getting a truncated audio issue!&#34; - PLEASE MAKE AN ISSUE OF THIS, I don&#39;t speak every language and I need advise from each person to fine tune my sentense splitting function on any other languages.üòä&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What I need help with! üôå&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/DrewThomasson/ebook2audiobook/issues/32&#34;&gt;Full list of things can be found here&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Any help from people speaking any of the supported languages to help with proper sentence splitting methods&lt;/li&gt; &#xA; &lt;li&gt;Potentially creating readme Guides for Multiple languages(Becuase the only language I know is English üòî)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Special Thanks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Coqui TTS&lt;/strong&gt;: &lt;a href=&#34;https://github.com/idiap/coqui-ai-TTS&#34;&gt;Coqui TTS GitHub&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Calibre&lt;/strong&gt;: &lt;a href=&#34;https://calibre-ebook.com&#34;&gt;Calibre Website&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;FFmpeg&lt;/strong&gt;: &lt;a href=&#34;https://ffmpeg.org&#34;&gt;FFmpeg Website&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/DrewThomasson/ebook2audiobook/issues/8&#34;&gt;@shakenbake15 for better chapter saving method&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/legacy/v1.0&#34;&gt;Legacy V1.0&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;You can view the code &lt;a href=&#34;https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/legacy/v1.0&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Join Our Discord Server!&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/bg5Kx43c6w&#34;&gt;&lt;img src=&#34;https://dcbadge.limes.pink/api/server/https://discord.gg/bg5Kx43c6w&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>stanford-oval/storm</title>
    <updated>2025-01-05T01:36:23Z</updated>
    <id>tag:github.com,2025-01-05:/stanford-oval/storm</id>
    <link href="https://github.com/stanford-oval/storm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/logo.svg?sanitize=true&#34; style=&#34;width: 25%; height: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;http://storm.genie.stanford.edu&#34;&gt;&lt;b&gt;Research preview&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2402.14207&#34;&gt;&lt;b&gt;STORM Paper&lt;/b&gt;&lt;/a&gt;| &lt;a href=&#34;https://www.arxiv.org/abs/2408.15232&#34;&gt;&lt;b&gt;Co-STORM Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://storm-project.stanford.edu/&#34;&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latest News&lt;/strong&gt; üî•&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/09] Co-STORM codebase is now released and integrated into &lt;code&gt;knowledge-storm&lt;/code&gt; python package v1.0.0. Run &lt;code&gt;pip install knowledge-storm --upgrade&lt;/code&gt; to check it out.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/09] We introduce collaborative STORM (Co-STORM) to support human-AI collaborative knowledge curation! &lt;a href=&#34;https://www.arxiv.org/abs/2408.15232&#34;&gt;Co-STORM Paper&lt;/a&gt; has been accepted to EMNLP 2024 main conference.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/07] You can now install our package with &lt;code&gt;pip install knowledge-storm&lt;/code&gt;!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/07] We add &lt;code&gt;VectorRM&lt;/code&gt; to support grounding on user-provided documents, complementing existing support of search engines (&lt;code&gt;YouRM&lt;/code&gt;, &lt;code&gt;BingSearch&lt;/code&gt;). (check out &lt;a href=&#34;https://github.com/stanford-oval/storm/pull/58&#34;&gt;#58&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/07] We release demo light for developers a minimal user interface built with streamlit framework in Python, handy for local development and demo hosting (checkout &lt;a href=&#34;https://github.com/stanford-oval/storm/pull/54&#34;&gt;#54&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/06] We will present STORM at NAACL 2024! Find us at Poster Session 2 on June 17 or check our &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/storm_naacl2024_slides.pdf&#34;&gt;presentation material&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/05] We add Bing Search support in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/rm.py&#34;&gt;rm.py&lt;/a&gt;. Test STORM with &lt;code&gt;GPT-4o&lt;/code&gt; - we now configure the article generation part in our demo using &lt;code&gt;GPT-4o&lt;/code&gt; model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/04] We release refactored version of STORM codebase! We define &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/interface.py&#34;&gt;interface&lt;/a&gt; for STORM pipeline and reimplement STORM-wiki (check out &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/storm_wiki&#34;&gt;&lt;code&gt;src/storm_wiki&lt;/code&gt;&lt;/a&gt;) to demonstrate how to instantiate the pipeline. We provide API to support customization of different language models and retrieval/search integration.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview &lt;a href=&#34;https://storm.genie.stanford.edu/&#34;&gt;(Try STORM now!)&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/overview.svg?sanitize=true&#34; style=&#34;width: 90%; height: auto;&#34;&gt; &lt;/p&gt; STORM is a LLM system that writes Wikipedia-like articles from scratch based on Internet search. Co-STORM further enhanced its feature by enabling human to collaborative LLM system to support more aligned and preferred information seeking and knowledge curation. &#xA;&lt;p&gt;While the system cannot produce publication-ready articles that often require a significant number of edits, experienced Wikipedia editors have found it helpful in their pre-writing stage.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;More than 70,000 people have tried our &lt;a href=&#34;https://storm.genie.stanford.edu/&#34;&gt;live research preview&lt;/a&gt;. Try it out to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system üôè!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How STORM &amp;amp; Co-STORM works&lt;/h2&gt; &#xA;&lt;h3&gt;STORM&lt;/h3&gt; &#xA;&lt;p&gt;STORM breaks down generating long articles with citations into two steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pre-writing stage&lt;/strong&gt;: The system conducts Internet-based research to collect references and generates an outline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Writing stage&lt;/strong&gt;: The system uses the outline and references to generate the full-length article with citations.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/two_stages.jpg&#34; style=&#34;width: 60%; height: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;STORM identifies the core of automating the research process as automatically coming up with good questions to ask. Directly prompting the language model to ask questions does not work well. To improve the depth and breadth of the questions, STORM adopts two strategies:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Perspective-Guided Question Asking&lt;/strong&gt;: Given the input topic, STORM discovers different perspectives by surveying existing articles from similar topics and uses them to control the question-asking process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simulated Conversation&lt;/strong&gt;: STORM simulates a conversation between a Wikipedia writer and a topic expert grounded in Internet sources to enable the language model to update its understanding of the topic and ask follow-up questions.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;CO-STORM&lt;/h3&gt; &#xA;&lt;p&gt;Co-STORM proposes &lt;strong&gt;a collaborative discourse protocol&lt;/strong&gt; which implements a turn management policy to support smooth collaboration among&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Co-STORM LLM experts&lt;/strong&gt;: This type of agent generates answers grounded on external knowledge sources and/or raises follow-up questions based on the discourse history.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Moderator&lt;/strong&gt;: This agent generates thought-provoking questions inspired by information discovered by the retriever but not directly used in previous turns. Question generation can also be grounded!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Human user&lt;/strong&gt;: The human user will take the initiative to either (1) observe the discourse to gain deeper understanding of the topic, or (2) actively engage in the conversation by injecting utterances to steer the discussion focus.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/co-storm-workflow.jpg&#34; style=&#34;width: 60%; height: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Co-STORM also maintains a dynamic updated &lt;strong&gt;mind map&lt;/strong&gt;, which organize collected information into a hierarchical concept structure, aiming to &lt;strong&gt;build a shared conceptual space between the human user and the system&lt;/strong&gt;. The mind map has been proven to help reduce the mental load when the discourse goes long and in-depth.&lt;/p&gt; &#xA;&lt;p&gt;Both STORM and Co-STORM are implemented in a highly modular way using &lt;a href=&#34;https://github.com/stanfordnlp/dspy&#34;&gt;dspy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install the knowledge storm library, use &lt;code&gt;pip install knowledge-storm&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You could also install the source code which allows you to modify the behavior of STORM engine directly.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the git repository.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/stanford-oval/storm.git&#xA;cd storm&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the required packages.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n storm python=3.11&#xA;conda activate storm&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;API&lt;/h2&gt; &#xA;&lt;p&gt;Currently, our package support:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;OpenAIModel&lt;/code&gt;, &lt;code&gt;AzureOpenAIModel&lt;/code&gt;, &lt;code&gt;ClaudeModel&lt;/code&gt;, &lt;code&gt;VLLMClient&lt;/code&gt;, &lt;code&gt;TGIClient&lt;/code&gt;, &lt;code&gt;TogetherClient&lt;/code&gt;, &lt;code&gt;OllamaClient&lt;/code&gt;, &lt;code&gt;GoogleModel&lt;/code&gt;, &lt;code&gt;DeepSeekModel&lt;/code&gt;, &lt;code&gt;GroqModel&lt;/code&gt; as language model components&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;YouRM&lt;/code&gt;, &lt;code&gt;BingSearch&lt;/code&gt;, &lt;code&gt;VectorRM&lt;/code&gt;, &lt;code&gt;SerperRM&lt;/code&gt;, &lt;code&gt;BraveRM&lt;/code&gt;, &lt;code&gt;SearXNG&lt;/code&gt;, &lt;code&gt;DuckDuckGoSearchRM&lt;/code&gt;, &lt;code&gt;TavilySearchRM&lt;/code&gt;, &lt;code&gt;GoogleSearch&lt;/code&gt;, and &lt;code&gt;AzureAISearch&lt;/code&gt; as retrieval module components&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;üåü&lt;/span&gt; &lt;strong&gt;PRs for integrating more language models into &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/lm.py&#34;&gt;knowledge_storm/lm.py&lt;/a&gt; and search engines/retrievers into &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/rm.py&#34;&gt;knowledge_storm/rm.py&lt;/a&gt; are highly appreciated!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Both STORM and Co-STORM are working in the information curation layer, you need to set up the information retrieval module and language model module to create their &lt;code&gt;Runner&lt;/code&gt; classes respectively.&lt;/p&gt; &#xA;&lt;h3&gt;STORM&lt;/h3&gt; &#xA;&lt;p&gt;The STORM knowledge curation engine is defined as a simple Python &lt;code&gt;STORMWikiRunner&lt;/code&gt; class. Here is an example of using You.com search engine and OpenAI models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;from knowledge_storm import STORMWikiRunnerArguments, STORMWikiRunner, STORMWikiLMConfigs&#xA;from knowledge_storm.lm import OpenAIModel&#xA;from knowledge_storm.rm import YouRM&#xA;&#xA;lm_configs = STORMWikiLMConfigs()&#xA;openai_kwargs = {&#xA;    &#39;api_key&#39;: os.getenv(&#34;OPENAI_API_KEY&#34;),&#xA;    &#39;temperature&#39;: 1.0,&#xA;    &#39;top_p&#39;: 0.9,&#xA;}&#xA;# STORM is a LM system so different components can be powered by different models to reach a good balance between cost and quality.&#xA;# For a good practice, choose a cheaper/faster model for `conv_simulator_lm` which is used to split queries, synthesize answers in the conversation.&#xA;# Choose a more powerful model for `article_gen_lm` to generate verifiable text with citations.&#xA;gpt_35 = OpenAIModel(model=&#39;gpt-3.5-turbo&#39;, max_tokens=500, **openai_kwargs)&#xA;gpt_4 = OpenAIModel(model=&#39;gpt-4o&#39;, max_tokens=3000, **openai_kwargs)&#xA;lm_configs.set_conv_simulator_lm(gpt_35)&#xA;lm_configs.set_question_asker_lm(gpt_35)&#xA;lm_configs.set_outline_gen_lm(gpt_4)&#xA;lm_configs.set_article_gen_lm(gpt_4)&#xA;lm_configs.set_article_polish_lm(gpt_4)&#xA;# Check out the STORMWikiRunnerArguments class for more configurations.&#xA;engine_args = STORMWikiRunnerArguments(...)&#xA;rm = YouRM(ydc_api_key=os.getenv(&#39;YDC_API_KEY&#39;), k=engine_args.search_top_k)&#xA;runner = STORMWikiRunner(engine_args, lm_configs, rm)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;STORMWikiRunner&lt;/code&gt; instance can be evoked with the simple &lt;code&gt;run&lt;/code&gt; method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;topic = input(&#39;Topic: &#39;)&#xA;runner.run(&#xA;    topic=topic,&#xA;    do_research=True,&#xA;    do_generate_outline=True,&#xA;    do_generate_article=True,&#xA;    do_polish_article=True,&#xA;)&#xA;runner.post_run()&#xA;runner.summary()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;do_research&lt;/code&gt;: if True, simulate conversations with difference perspectives to collect information about the topic; otherwise, load the results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;do_generate_outline&lt;/code&gt;: if True, generate an outline for the topic; otherwise, load the results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;do_generate_article&lt;/code&gt;: if True, generate an article for the topic based on the outline and the collected information; otherwise, load the results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;do_polish_article&lt;/code&gt;: if True, polish the article by adding a summarization section and (optionally) removing duplicate content; otherwise, load the results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Co-STORM&lt;/h3&gt; &#xA;&lt;p&gt;The Co-STORM knowledge curation engine is defined as a simple Python &lt;code&gt;CoStormRunner&lt;/code&gt; class. Here is an example of using Bing search engine and OpenAI models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from knowledge_storm.collaborative_storm.engine import CollaborativeStormLMConfigs, RunnerArgument, CoStormRunner&#xA;from knowledge_storm.lm import OpenAIModel&#xA;from knowledge_storm.logging_wrapper import LoggingWrapper&#xA;from knowledge_storm.rm import BingSearch&#xA;&#xA;# Co-STORM adopts the same multi LM system paradigm as STORM &#xA;lm_config: CollaborativeStormLMConfigs = CollaborativeStormLMConfigs()&#xA;openai_kwargs = {&#xA;    &#34;api_key&#34;: os.getenv(&#34;OPENAI_API_KEY&#34;),&#xA;    &#34;api_provider&#34;: &#34;openai&#34;,&#xA;    &#34;temperature&#34;: 1.0,&#xA;    &#34;top_p&#34;: 0.9,&#xA;    &#34;api_base&#34;: None,&#xA;} &#xA;question_answering_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)&#xA;discourse_manage_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)&#xA;utterance_polishing_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=2000, **openai_kwargs)&#xA;warmstart_outline_gen_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)&#xA;question_asking_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=300, **openai_kwargs)&#xA;knowledge_base_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)&#xA;&#xA;lm_config.set_question_answering_lm(question_answering_lm)&#xA;lm_config.set_discourse_manage_lm(discourse_manage_lm)&#xA;lm_config.set_utterance_polishing_lm(utterance_polishing_lm)&#xA;lm_config.set_warmstart_outline_gen_lm(warmstart_outline_gen_lm)&#xA;lm_config.set_question_asking_lm(question_asking_lm)&#xA;lm_config.set_knowledge_base_lm(knowledge_base_lm)&#xA;&#xA;# Check out the Co-STORM&#39;s RunnerArguments class for more configurations.&#xA;topic = input(&#39;Topic: &#39;)&#xA;runner_argument = RunnerArgument(topic=topic, ...)&#xA;logging_wrapper = LoggingWrapper(lm_config)&#xA;bing_rm = BingSearch(bing_search_api_key=os.environ.get(&#34;BING_SEARCH_API_KEY&#34;),&#xA;                     k=runner_argument.retrieve_top_k)&#xA;costorm_runner = CoStormRunner(lm_config=lm_config,&#xA;                               runner_argument=runner_argument,&#xA;                               logging_wrapper=logging_wrapper,&#xA;                               rm=bing_rm)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;CoStormRunner&lt;/code&gt; instance can be evoked with the &lt;code&gt;warmstart()&lt;/code&gt; and &lt;code&gt;step(...)&lt;/code&gt; methods.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Warm start the system to build shared conceptual space between Co-STORM and users&#xA;costorm_runner.warm_start()&#xA;&#xA;# Step through the collaborative discourse &#xA;# Run either of the code snippets below in any order, as many times as you&#39;d like&#xA;# To observe the conversation:&#xA;conv_turn = costorm_runner.step()&#xA;# To inject your utterance to actively steer the conversation:&#xA;costorm_runner.step(user_utterance=&#34;YOUR UTTERANCE HERE&#34;)&#xA;&#xA;# Generate report based on the collaborative discourse&#xA;costorm_runner.knowledge_base.reorganize()&#xA;article = costorm_runner.generate_report()&#xA;print(article)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start with Example Scripts&lt;/h2&gt; &#xA;&lt;p&gt;We provide scripts in our &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/examples&#34;&gt;examples folder&lt;/a&gt; as a quick start to run STORM and Co-STORM with different configurations.&lt;/p&gt; &#xA;&lt;p&gt;We suggest using &lt;code&gt;secrets.toml&lt;/code&gt; to set up the API keys. Create a file &lt;code&gt;secrets.toml&lt;/code&gt; under the root directory and add the following content:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Set up OpenAI API key.&#xA;OPENAI_API_KEY=&#34;your_openai_api_key&#34;&#xA;# If you are using the API service provided by OpenAI, include the following line:&#xA;OPENAI_API_TYPE=&#34;openai&#34;&#xA;# If you are using the API service provided by Microsoft Azure, include the following lines:&#xA;OPENAI_API_TYPE=&#34;azure&#34;&#xA;AZURE_API_BASE=&#34;your_azure_api_base_url&#34;&#xA;AZURE_API_VERSION=&#34;your_azure_api_version&#34;&#xA;# Set up You.com search API key.&#xA;YDC_API_KEY=&#34;your_youcom_api_key&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;STORM examples&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;To run STORM with &lt;code&gt;gpt&lt;/code&gt; family models with default configurations:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python examples/storm_examples/run_storm_wiki_gpt.py \&#xA;    --output-dir $OUTPUT_DIR \&#xA;    --retriever you \&#xA;    --do-research \&#xA;    --do-generate-outline \&#xA;    --do-generate-article \&#xA;    --do-polish-article&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;To run STORM using your favorite language models or grounding on your own corpus:&lt;/strong&gt; Check out &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/examples/storm_examples/README.md&#34;&gt;examples/storm_examples/README.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Co-STORM examples&lt;/h3&gt; &#xA;&lt;p&gt;To run Co-STORM with &lt;code&gt;gpt&lt;/code&gt; family models with default configurations,&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Add &lt;code&gt;BING_SEARCH_API_KEY=&#34;xxx&#34;&lt;/code&gt; and &lt;code&gt;ENCODER_API_TYPE=&#34;xxx&#34;&lt;/code&gt; to &lt;code&gt;secrets.toml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run the following command&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python examples/costorm_examples/run_costorm_gpt.py \&#xA;    --output-dir $OUTPUT_DIR \&#xA;    --retriever bing&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Customization of the Pipeline&lt;/h2&gt; &#xA;&lt;h3&gt;STORM&lt;/h3&gt; &#xA;&lt;p&gt;If you have installed the source code, you can customize STORM based on your own use case. STORM engine consists of 4 modules:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Knowledge Curation Module: Collects a broad coverage of information about the given topic.&lt;/li&gt; &#xA; &lt;li&gt;Outline Generation Module: Organizes the collected information by generating a hierarchical outline for the curated knowledge.&lt;/li&gt; &#xA; &lt;li&gt;Article Generation Module: Populates the generated outline with the collected information.&lt;/li&gt; &#xA; &lt;li&gt;Article Polishing Module: Refines and enhances the written article for better presentation.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The interface for each module is defined in &lt;code&gt;knowledge_storm/interface.py&lt;/code&gt;, while their implementations are instantiated in &lt;code&gt;knowledge_storm/storm_wiki/modules/*&lt;/code&gt;. These modules can be customized according to your specific requirements (e.g., generating sections in bullet point format instead of full paragraphs).&lt;/p&gt; &#xA;&lt;h3&gt;Co-STORM&lt;/h3&gt; &#xA;&lt;p&gt;If you have installed the source code, you can customize Co-STORM based on your own use case&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Co-STORM introduces multiple LLM agent types (i.e. Co-STORM experts and Moderator). LLM agent interface is defined in &lt;code&gt;knowledge_storm/interface.py&lt;/code&gt; , while its implementation is instantiated in &lt;code&gt;knowledge_storm/collaborative_storm/modules/co_storm_agents.py&lt;/code&gt;. Different LLM agent policies can be customized.&lt;/li&gt; &#xA; &lt;li&gt;Co-STORM introduces a collaborative discourse protocol, with its core function centered on turn policy management. We provide an example implementation of turn policy management through &lt;code&gt;DiscourseManager&lt;/code&gt; in &lt;code&gt;knowledge_storm/collaborative_storm/engine.py&lt;/code&gt;. It can be customized and further improved.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;To facilitate the study of automatic knowledge curation and complex information seeking, our project releases the following datasets:&lt;/p&gt; &#xA;&lt;h3&gt;FreshWiki&lt;/h3&gt; &#xA;&lt;p&gt;The FreshWiki Dataset is a collection of 100 high-quality Wikipedia articles focusing on the most-edited pages from February 2022 to September 2023. See Section 2.1 in &lt;a href=&#34;https://arxiv.org/abs/2402.14207&#34;&gt;STORM paper&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;You can download the dataset from &lt;a href=&#34;https://huggingface.co/datasets/EchoShao8899/FreshWiki&#34;&gt;huggingface&lt;/a&gt; directly. To ease the data contamination issue, we archive the &lt;a href=&#34;https://github.com/stanford-oval/storm/tree/NAACL-2024-code-backup/FreshWiki&#34;&gt;source code&lt;/a&gt; for the data construction pipeline that can be repeated at future dates.&lt;/p&gt; &#xA;&lt;h3&gt;WildSeek&lt;/h3&gt; &#xA;&lt;p&gt;To study users‚Äô interests in complex information seeking tasks in the wild, we utilized data collected from the web research preview to create the WildSeek dataset. We downsampled the data to ensure the diversity of the topics and the quality of the data. Each data point is a pair comprising a topic and the user‚Äôs goal for conducting deep search on the topic. For more details, please refer to Section 2.2 and Appendix A of &lt;a href=&#34;https://www.arxiv.org/abs/2408.15232&#34;&gt;Co-STORM paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The WildSeek dataset is available &lt;a href=&#34;https://huggingface.co/datasets/YuchengJiang/WildSeek&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Replicate STORM &amp;amp; Co-STORM paper result&lt;/h2&gt; &#xA;&lt;p&gt;For STORM paper experiments, please switch to the branch &lt;code&gt;NAACL-2024-code-backup&lt;/code&gt; &lt;a href=&#34;https://github.com/stanford-oval/storm/tree/NAACL-2024-code-backup&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For Co-STORM paper experiments, please switch to the branch &lt;code&gt;EMNLP-2024-code-backup&lt;/code&gt; (placeholder for now, will be updated soon).&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap &amp;amp; Contributions&lt;/h2&gt; &#xA;&lt;p&gt;Our team is actively working on:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Human-in-the-Loop Functionalities: Supporting user participation in the knowledge curation process.&lt;/li&gt; &#xA; &lt;li&gt;Information Abstraction: Developing abstractions for curated information to support presentation formats beyond the Wikipedia-style report.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you have any questions or suggestions, please feel free to open an issue or pull request. We welcome contributions to improve the system and the codebase!&lt;/p&gt; &#xA;&lt;p&gt;Contact person: &lt;a href=&#34;mailto:shaoyj@stanford.edu&#34;&gt;Yijia Shao&lt;/a&gt; and &lt;a href=&#34;mailto:yuchengj@stanford.edu&#34;&gt;Yucheng Jiang&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank Wikipedia for its excellent open-source content. The FreshWiki dataset is sourced from Wikipedia, licensed under the Creative Commons Attribution-ShareAlike (CC BY-SA) license.&lt;/p&gt; &#xA;&lt;p&gt;We are very grateful to &lt;a href=&#34;https://michelle123lam.github.io/&#34;&gt;Michelle Lam&lt;/a&gt; for designing the logo for this project and &lt;a href=&#34;https://dekun.me&#34;&gt;Dekun Ma&lt;/a&gt; for leading the UI development.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite our paper if you use this code or part of it in your work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{jiang2024unknownunknowns,&#xA;      title={Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations}, &#xA;      author={Yucheng Jiang and Yijia Shao and Dekun Ma and Sina J. Semnani and Monica S. Lam},&#xA;      year={2024},&#xA;      eprint={2408.15232},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2408.15232}, &#xA;}&#xA;&#xA;@inproceedings{shao2024assisting,&#xA;      title={{Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}}, &#xA;      author={Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam},&#xA;      year={2024},&#xA;      booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>OpenSPG/KAG</title>
    <updated>2025-01-05T01:36:23Z</updated>
    <id>tag:github.com,2025-01-05:/OpenSPG/KAG</id>
    <link href="https://github.com/OpenSPG/KAG" rel="alternate"></link>
    <summary type="html">&lt;p&gt;KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs. It is used to build logical reasoning and factual Q&amp;A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;KAG: Knowledge Augmented Generation&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://spg.openkg.cn/en-US&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenSPG/KAG/master/_static/images/OpenSPG-1.png&#34; width=&#34;520&#34; alt=&#34;openspg logo&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenSPG/KAG/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/OpenSPG/KAG/master/README_cn.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/OpenSPG/KAG/master/README_ja.md&#34;&gt;Êó•Êú¨Ë™ûÁâà„Éâ„Ç≠„É•„É°„É≥„Éà&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/pdf/2409.13731&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2409.13731-b31b1b&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenSPG/KAG/releases/latest&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/OpenSPG/KAG?color=blue&amp;amp;label=Latest%20Release&#34; alt=&#34;Latest Release&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/OpenSPG/KAG/raw/main/LICENSE&#34;&gt; &lt;img height=&#34;21&#34; src=&#34;https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&amp;amp;color=2e6cc4&#34; alt=&#34;license&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;1. What is KAG?&lt;/h1&gt; &#xA;&lt;p&gt;KAG is a logical reasoning and Q&amp;amp;A framework based on the &lt;a href=&#34;https://github.com/OpenSPG/openspg&#34;&gt;OpenSPG&lt;/a&gt; engine and large language models, which is used to build logical reasoning and Q&amp;amp;A solutions for vertical domain knowledge bases. KAG can effectively overcome the ambiguity of traditional RAG vector similarity calculation and the noise problem of GraphRAG introduced by OpenIE. KAG supports logical reasoning and multi-hop fact Q&amp;amp;A, etc., and is significantly better than the current SOTA method.&lt;/p&gt; &#xA;&lt;p&gt;The goal of KAG is to build a knowledge-enhanced LLM service framework in professional domains, supporting logical reasoning, factual Q&amp;amp;A, etc. KAG fully integrates the logical and factual characteristics of the KGs. Its core features include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Knowledge and Chunk Mutual Indexing structure to integrate more complete contextual text information&lt;/li&gt; &#xA; &lt;li&gt;Knowledge alignment using conceptual semantic reasoning to alleviate the noise problem caused by OpenIE&lt;/li&gt; &#xA; &lt;li&gt;Schema-constrained knowledge construction to support the representation and construction of domain expert knowledge&lt;/li&gt; &#xA; &lt;li&gt;Logical form-guided hybrid reasoning and retrieval to support logical reasoning and multi-hop reasoning Q&amp;amp;A&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;‚≠êÔ∏è Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new releases! üåü&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenSPG/KAG/master/_static/images/star-kag.gif&#34; alt=&#34;Star KAG&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;2. Core Features&lt;/h1&gt; &#xA;&lt;h2&gt;2.1 Knowledge Representation&lt;/h2&gt; &#xA;&lt;p&gt;In the context of private knowledge bases, unstructured data, structured information, and business expert experience often coexist. KAG references the DIKW hierarchy to upgrade SPG to a version that is friendly to LLMs.&lt;/p&gt; &#xA;&lt;p&gt;For unstructured data such as news, events, logs, and books, as well as structured data like transactions, statistics, and approvals, along with business experience and domain knowledge rules, KAG employs techniques such as layout analysis, knowledge extraction, property normalization, and semantic alignment to integrate raw business data and expert rules into a unified business knowledge graph.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenSPG/KAG/master/_static/images/kag-diag.jpg&#34; alt=&#34;KAG Diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This makes it compatible with schema-free information extraction and schema-constrained expertise construction on the same knowledge type (e. G., entity type, event type), and supports the cross-index representation between the graph structure and the original text block.&lt;/p&gt; &#xA;&lt;p&gt;This mutual index representation is helpful to the construction of inverted index based on graph structure, and promotes the unified representation and reasoning of logical forms.&lt;/p&gt; &#xA;&lt;h2&gt;2.2 Mixed Reasoning Guided by Logic Forms&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenSPG/KAG/master/_static/images/kag-lf-solver.png&#34; alt=&#34;Logical Form Solver&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;KAG proposes a logically formal guided hybrid solution and inference engine.&lt;/p&gt; &#xA;&lt;p&gt;The engine includes three types of operators: planning, reasoning, and retrieval, which transform natural language problems into problem solving processes that combine language and notation.&lt;/p&gt; &#xA;&lt;p&gt;In this process, each step can use different operators, such as exact match retrieval, text retrieval, numerical calculation or semantic reasoning, so as to realize the integration of four different problem solving processes: Retrieval, Knowledge Graph reasoning, language reasoning and numerical calculation.&lt;/p&gt; &#xA;&lt;h1&gt;3. Release Notes&lt;/h1&gt; &#xA;&lt;h2&gt;3.1 Latest Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024.11.21 : Support Word docs upload, model invoke concurrency setting, User experience optimization, etc.&lt;/li&gt; &#xA; &lt;li&gt;2024.10.25 : KAG initial release&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;3.2 Future Plans&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;domain knowledge injection, domain schema customization, QFS tasks support, Visual query analysis, etc.&lt;/li&gt; &#xA; &lt;li&gt;Logical reasoning optimization, conversational tasks support&lt;/li&gt; &#xA; &lt;li&gt;kag-model release, kag solution for event reasoning knowledge graph and medical knowledge graph&lt;/li&gt; &#xA; &lt;li&gt;kag front-end open source, distributed build support, mathematical reasoning optimization&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;4. Quick Start&lt;/h1&gt; &#xA;&lt;h2&gt;4.1 product-based (for ordinary users)&lt;/h2&gt; &#xA;&lt;h3&gt;4.1.1 Engine &amp;amp; Dependent Image Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Recommend System Version:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;macOS UserÔºömacOS Monterey 12.6 or later&#xA;Linux UserÔºöCentOS 7 / Ubuntu 20.04 or later&#xA;Windows UserÔºöWindows 10 LTSC 2021 or later&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Software Requirements:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;macOS / Linux UserÔºöDockerÔºåDocker Compose&#xA;Windows UserÔºöWSL 2 / Hyper-VÔºåDockerÔºåDocker Compose&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Use the following commands to download the docker-compose.yml file and launch the services with Docker Compose.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# set the HOME environment variable (only Windows users need to execute this command)&#xA;# set HOME=%USERPROFILE%&#xA;&#xA;curl -sSL https://raw.githubusercontent.com/OpenSPG/openspg/refs/heads/master/dev/release/docker-compose-west.yml -o docker-compose-west.yml&#xA;docker compose -f docker-compose-west.yml up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4.1.2 Use the product&lt;/h3&gt; &#xA;&lt;p&gt;Navigate to the default url of the KAG product with your browser: &lt;a href=&#34;http://127.0.0.1:8887&#34;&gt;http://127.0.0.1:8887&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://openspg.yuque.com/ndx6g9/wc9oyq/yexegklu44bqqicm&#34;&gt;Quick Start for Product Mode&lt;/a&gt; for detailed introduction.&lt;/p&gt; &#xA;&lt;h2&gt;4.2 toolkit-based (for developers)&lt;/h2&gt; &#xA;&lt;h3&gt;4.2.1 Engine &amp;amp; Dependent Image Installation&lt;/h3&gt; &#xA;&lt;p&gt;Refer to the 3.1 section to complete the installation of the engine &amp;amp; dependent image.&lt;/p&gt; &#xA;&lt;h3&gt;4.2.2 Installation of KAG&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;macOS / Linux developers&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;# Create conda env: conda create -n kag-demo python=3.10 &amp;amp;&amp;amp; conda activate kag-demo&#xA;&#xA;# Clone code: git clone https://github.com/OpenSPG/KAG.git&#xA;&#xA;# Install KAG: cd KAG &amp;amp;&amp;amp; pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Windows developers&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;# Install the official Python 3.8.10 or later, install Git.&#xA;&#xA;# Create and activate Python venv: py -m venv kag-demo &amp;amp;&amp;amp; kag-demo\Scripts\activate&#xA;&#xA;# Clone code: git clone https://github.com/OpenSPG/KAG.git&#xA;&#xA;# Install KAG: cd KAG &amp;amp;&amp;amp; pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4.2.3 Use the toolkit&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://openspg.yuque.com/ndx6g9/wc9oyq/yexegklu44bqqicm#cikso&#34;&gt;Quick Start for Developer Mode&lt;/a&gt; guide for detailed introduction of the toolkit. Then you can use the built-in components to reproduce the performance results of the built-in datasets, and apply those components to new busineness scenarios.&lt;/p&gt; &#xA;&lt;h1&gt;5. Technical Architecture&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenSPG/KAG/master/_static/images/kag-arch.png&#34; alt=&#34;KAG technical architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The KAG framework includes three parts: kg-builder, kg-solver, and kag-model. This release only involves the first two parts, kag-model will be gradually open source release in the future.&lt;/p&gt; &#xA;&lt;p&gt;kg-builder implements a knowledge representation that is friendly to large-scale language models (LLM). Based on the hierarchical structure of DIKW (data, information, knowledge and wisdom), IT upgrades SPG knowledge representation ability, and is compatible with information extraction without schema constraints and professional knowledge construction with schema constraints on the same knowledge type (such as entity type and event type), it also supports the mutual index representation between the graph structure and the original text block, which supports the efficient retrieval of the reasoning question and answer stage.&lt;/p&gt; &#xA;&lt;p&gt;kg-solver uses a logical symbol-guided hybrid solving and reasoning engine that includes three types of operators: planning, reasoning, and retrieval, to transform natural language problems into a problem-solving process that combines language and symbols. In this process, each step can use different operators, such as exact match retrieval, text retrieval, numerical calculation or semantic reasoning, so as to realize the integration of four different problem solving processes: Retrieval, Knowledge Graph reasoning, language reasoning and numerical calculation.&lt;/p&gt; &#xA;&lt;h1&gt;6. Contact us&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=&#34;https://github.com/OpenSPG/KAG&#34;&gt;https://github.com/OpenSPG/KAG&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OpenSPG&lt;/strong&gt;: &lt;a href=&#34;https://spg.openkg.cn/&#34;&gt;https://spg.openkg.cn/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/OpenSPG/KAG/master/_static/images/openspg-qr.png&#34; alt=&#34;Contact Us: OpenSPG QR-code&#34; width=&#34;200&#34;&gt; &#xA;&lt;h1&gt;7. Differences between KAG, RAG, and GraphRAG&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;KAG introduction and applications&lt;/strong&gt;: &lt;a href=&#34;https://github.com/orgs/OpenSPG/discussions/52&#34;&gt;https://github.com/orgs/OpenSPG/discussions/52&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;8. Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use this software, please cite it as below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.13731&#34;&gt;KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;KGFabric: A Scalable Knowledge Graph Warehouse for Enterprise Data Interconnection&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{liang2024kag,&#xA;  title={KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation},&#xA;  author={Liang, Lei and Sun, Mengshu and Gui, Zhengke and Zhu, Zhongshu and Jiang, Zhouyu and Zhong, Ling and Qu, Yuan and Zhao, Peilong and Bo, Zhongpu and Yang, Jin and others},&#xA;  journal={arXiv preprint arXiv:2409.13731},&#xA;  year={2024}&#xA;}&#xA;&#xA;@article{yikgfabric,&#xA;  title={KGFabric: A Scalable Knowledge Graph Warehouse for Enterprise Data Interconnection},&#xA;  author={Yi, Peng and Liang, Lei and Da Zhang, Yong Chen and Zhu, Jinye and Liu, Xiangyu and Tang, Kun and Chen, Jialin and Lin, Hao and Qiu, Leijie and Zhou, Jun}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenSPG/KAG/master/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>