<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-07T01:49:57Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>GreyDGL/PentestGPT</title>
    <updated>2023-05-07T01:49:57Z</updated>
    <id>tag:github.com,2023-05-07:/GreyDGL/PentestGPT</id>
    <link href="https://github.com/GreyDGL/PentestGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A GPT-empowered penetration testing tool&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PentestGPT&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Update on 30/04/2023] The support to OpenAI API is available! I&#39;ll implement a input param parser for it soon. You can now freely configure the OpenAI model in &lt;code&gt;main.py&lt;/code&gt; (several examples are included).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;We&#39;re testing PentestGPT on HackTheBox&lt;/strong&gt;. You may follow &lt;a href=&#34;https://www.hackthebox.com/home/users/profile/1489431&#34;&gt;this link&lt;/a&gt;. More details will be released soon.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;We include a video of using PentestGPT for OSCP-like machine: &lt;a href=&#34;https://youtu.be/lAjLIj1JT3c&#34;&gt;HTB-Jarvis&lt;/a&gt;&lt;/strong&gt;. This is the first part only, and I&#39;ll complete the rest when I have time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Common Questions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: What is PentestGPT? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;: PentestGPT is a penetration testing tool empowered by ChatGPT. It is designed to automate the penetration testing process. It is built on top of ChatGPT and operate in an interactive mode to guide penetration testers in both overall progress and specific operations.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Do I need to be a ChatGPT plus member to use PentestGPT? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;: Yes. PentestGPT relies on GPT-4 model for high-quality reasoning. Since there is no public GPT-4 API yet, a wrapper is included to use ChatGPT session to support PentestGPT.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Why GPT-4? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;: After empirical evaluation, we found that GPT-4 performs better than GPT-3.5 in terms of penetration testing reasoning. In fact, GPT-3.5 leads to failed test in simple tasks.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Why not just use GPT-4 directly? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;: We found that GPT-4 suffers from losses of context as test goes deeper. It is essential to maintain a &#34;test status awareness&#34; in this process. You may check the PentestGPT design &lt;a href=&#34;https://raw.githubusercontent.com/GreyDGL/PentestGPT/main/PentestGPT_design.md&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: What about AutoGPT? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;: AutoGPT is not designed for pentest. It may perform malicious operations. Due to this consideration, we design PentestGPT in an interactive mode. Of course, our end goal is an automated pentest solution.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Future plan? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;: We&#39;re working on a paper to explore the tech details behind automated pentest. Meanwhile, please feel free to raise issues/discussions. I&#39;ll do my best to address all of them.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;PentestGPT&lt;/strong&gt; is a penetration testing tool empowered by &lt;strong&gt;ChatGPT&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;It is designed to automate the penetration testing process. It is built on top of ChatGPT and operate in an interactive mode to guide penetration testers in both overall progress and specific operations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PentestGPT&lt;/strong&gt; is able to solve easy to medium HackTheBox machines, and other CTF challenges. You can check &lt;a href=&#34;https://raw.githubusercontent.com/GreyDGL/PentestGPT/main/resources/README.md&#34;&gt;this&lt;/a&gt; example in &lt;code&gt;resources&lt;/code&gt; where we use it to solve HackTheBox challenge &lt;strong&gt;TEMPLATED&lt;/strong&gt; (web challenge).&lt;/li&gt; &#xA; &lt;li&gt;A sample testing process of &lt;strong&gt;PentestGPT&lt;/strong&gt; on a target VulnHub machine (Hackable II) is available at &lt;a href=&#34;https://raw.githubusercontent.com/GreyDGL/PentestGPT/main/resources/PentestGPT_Hackable2.pdf&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A sample usage video is below: (or available here: &lt;a href=&#34;https://youtu.be/h0k6kWWaCEU&#34;&gt;Demo&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/78410652/232327920-7318a0c4-bee0-4cb4-becb-6658b80180ff.mov&#34;&gt;https://user-images.githubusercontent.com/78410652/232327920-7318a0c4-bee0-4cb4-becb-6658b80180ff.mov&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The project is still in its early stage. Feel free to raise any issues when using the tool.&lt;/li&gt; &#xA; &lt;li&gt;Please help to contribute by submitting the vulnerabilities you identified or challenges you solved with &lt;strong&gt;PentestGPT&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;This project is for research purpose. Please contact me if you&#39;re interested in collaboration.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;code&gt;requirements.txt&lt;/code&gt; with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Configure the cookies in &lt;code&gt;config&lt;/code&gt;. You may follow a sample by &lt;code&gt;cp config/chatgpt_config_sample.py config/chatgpt_config.py&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you&#39;re using cookie: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Login to ChatGPT session page.&lt;/li&gt; &#xA;     &lt;li&gt;In &lt;code&gt;Inspect - Network&lt;/code&gt;, find the connections to the ChatGPT session page.&lt;/li&gt; &#xA;     &lt;li&gt;Find the cookie in the &lt;strong&gt;request header&lt;/strong&gt; in the request to &lt;code&gt;https://chat.openai.com/api/auth/session&lt;/code&gt; and paste it into the &lt;code&gt;cookie&lt;/code&gt; field of &lt;code&gt;config/chatgpt_config.py&lt;/code&gt;. (You may use Inspect-&amp;gt;Network, find session and copy the &lt;code&gt;cookie&lt;/code&gt; field in &lt;code&gt;request_headers&lt;/code&gt; to &lt;code&gt;https://chat.openai.com/api/auth/session&lt;/code&gt;)&lt;/li&gt; &#xA;     &lt;li&gt;Note that the other fields are temporarily deprecated due to the update of ChatGPT page.&lt;/li&gt; &#xA;     &lt;li&gt;Fill in &lt;code&gt;userAgent&lt;/code&gt; with your user agent.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;If you&#39;re using API: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Fill in the OpenAI API key in &lt;code&gt;chatgpt_config.py&lt;/code&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;To verify that the connection is configured properly, you may run &lt;code&gt;python3 test_connection.py&lt;/code&gt;. You should see some sample conversation with ChatGPT. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A sample output is below&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;1. You&#39;re connected with ChatGPT Plus cookie. &#xA;To start PentestGPT, please use &amp;lt;python3 main.py --reasoning_model=gpt-4&amp;gt;&#xA;## Test connection for OpenAI api (GPT-4)&#xA;2. You&#39;re connected with OpenAI API. You have GPT-4 access. To start PentestGPT, please use &amp;lt;python3 main.py --reasoning_model=gpt-4 --useAPI&amp;gt;&#xA;## Test connection for OpenAI api (GPT-3.5)&#xA;3. You&#39;re connected with OpenAI API. You have GPT-3.5 access. To start PentestGPT, please use &amp;lt;python3 main.py --reasoning_model=gpt-3.5-turbo --useAPI&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;(Notice) The above verification process for cookie. If you encounter errors after several trials, please try to refresh the page, repeat the above steps, and try again. You may also try with the cookie to &lt;code&gt;https://chat.openai.com/backend-api/conversations&lt;/code&gt;. Please submit an issue if you encounter any problem.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;To start, run &lt;code&gt;python3 main.py --args&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;--reasoning_model&lt;/code&gt; is the reasoning model you want to use.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--useAPI&lt;/code&gt; is whether you want to use OpenAI API.&lt;/li&gt; &#xA;   &lt;li&gt;You&#39;re recommended to use the combination as suggested by &lt;code&gt;test_connection.py&lt;/code&gt;, which are: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;python3 main.py --reasoning_model=gpt-4&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;python3 main.py --reasoning_model=gpt-4 --useAPI&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;python3 main.py --reasoning_model=gpt-3.5-turbo --useAPI&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The tool works similar to &lt;em&gt;msfconsole&lt;/em&gt;. Follow the guidance to perform penetration testing.&lt;/li&gt; &#xA; &lt;li&gt;In general, PentestGPT intakes commands similar to chatGPT. There are several basic commands. &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;The commands are: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;help&lt;/code&gt;: show the help message.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;next&lt;/code&gt;: key in the test execution result and get the next step.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;more&lt;/code&gt;: let &lt;strong&gt;PentestGPT&lt;/strong&gt; to explain more details of the current step. Also, a new sub-task solver will be created to guide the tester.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;todo&lt;/code&gt;: show the todo list.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;discuss&lt;/code&gt;: discuss with the &lt;strong&gt;PentestGPT&lt;/strong&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;google&lt;/code&gt;: search on Google. This function is still under development.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;quit&lt;/code&gt;: exit the tool and save the output as log file (see the &lt;strong&gt;reporting&lt;/strong&gt; section below).&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;You can use &amp;lt;SHIFT + right arrow&amp;gt; to end your input (and &#xA;    &lt;enter&gt;&#xA;      is for next line).&#xA;    &lt;/enter&gt;&lt;/li&gt; &#xA;   &lt;li&gt;You may always use &lt;code&gt;TAB&lt;/code&gt; to autocomplete the commands.&lt;/li&gt; &#xA;   &lt;li&gt;When you&#39;re given a drop-down selection list, you can use cursor or arrow key to navigate the list. Press &lt;code&gt;ENTER&lt;/code&gt; to select the item. Similarly, use &amp;lt;SHIFT + right arrow&amp;gt; to confirm selection.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;In the sub-task handler initiated by &lt;code&gt;more&lt;/code&gt;, users can execute more commands to investigate into a specific problem: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;The commands are: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;help&lt;/code&gt;: show the help message.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;brainstorm&lt;/code&gt;: let PentestGPT brainstorm on the local task for all the possible solutions.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;discuss&lt;/code&gt;: discuss with PentestGPT about this local task.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;google&lt;/code&gt;: search on Google. This function is still under development.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;continue&lt;/code&gt;: exit the subtask and continue the main testing session.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Report&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;After finishing the penetration testing, a report will be automatically generated in &lt;code&gt;logs&lt;/code&gt; folder (if you quit with &lt;code&gt;quit&lt;/code&gt; command).&lt;/li&gt; &#xA; &lt;li&gt;The report can be printed in a human-readable format by running &lt;code&gt;python3 utils/report_generator.py &amp;lt;log file&amp;gt;&lt;/code&gt;. A sample report &lt;code&gt;sample_pentestGPT_log.txt&lt;/code&gt; is also uploaded.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Design Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The current design is mainly for web penetration testing&lt;/p&gt; &#xA;&lt;h3&gt;General Design&lt;/h3&gt; &#xA;&lt;p&gt;PentestGPT provides a unified terminal input handler, and backed by three main components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A test generation module which generates the exact penetration testing commands or operations for the users to execute.&lt;/li&gt; &#xA; &lt;li&gt;A test reasoning module which conducts the reasoning of the test, guiding the penetration testers on what to do next.&lt;/li&gt; &#xA; &lt;li&gt;A parsing module which parses the output of the penetration tools and the contents on the webUI.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Function Design&lt;/h3&gt; &#xA;&lt;p&gt;The handler is the main entry point of the penetration testing tool. It allows pentesters to perform the following operations:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;(initialize itself with some pre-designed prompts.)&lt;/li&gt; &#xA; &lt;li&gt;Start a new penetration testing session by providing the target information.&lt;/li&gt; &#xA; &lt;li&gt;Ask for todo-list, and acquire the next step to perform.&lt;/li&gt; &#xA; &lt;li&gt;After completing the operation, pass the information to PentestGPT. &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Pass a tool output.&lt;/li&gt; &#xA;   &lt;li&gt;Pass a webpage content.&lt;/li&gt; &#xA;   &lt;li&gt;Pass a human description.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>AIGC-Audio/AudioGPT</title>
    <updated>2023-05-07T01:49:57Z</updated>
    <id>tag:github.com,2023-05-07:/AIGC-Audio/AudioGPT</id>
    <link href="https://github.com/AIGC-Audio/AudioGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.12995&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AIGC-Audio/AudioGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AIGC-Audio/AudioGPT?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://visitor-badge.glitch.me/badge?page_id=AIGC-Audio.AudioGPT&#34; alt=&#34;visitors&#34;&gt; &lt;a href=&#34;https://huggingface.co/spaces/AIGC-Audio/AudioGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We provide our implementation and pretrained models as open source in this repository.&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/AIGC-Audio/AudioGPT/main/run.md&#34;&gt;run.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Capabilities&lt;/h2&gt; &#xA;&lt;p&gt;Here we list the capability of AudioGPT at this time. More supported models and tasks are coming soon. For prompt examples, refer to &lt;a href=&#34;https://raw.githubusercontent.com/AIGC-Audio/AudioGPT/main/assets/README.md&#34;&gt;asset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Currently not every model has repository.&lt;/p&gt; &#xA;&lt;h3&gt;Speech&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Foundation Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-to-Speech&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ming024/FastSpeech2&#34;&gt;FastSpeech&lt;/a&gt;, &lt;a href=&#34;https://github.com/yerfor/SyntaSpeech&#34;&gt;SyntaSpeech&lt;/a&gt;, &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;VITS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Style Transfer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Rongjiehuang/GenerSpeech&#34;&gt;GenerSpeech&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speech Recognition&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;whisper&lt;/a&gt;, &lt;a href=&#34;https://github.com/sooftware/conformer&#34;&gt;Conformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speech Enhancement&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;ConvTasNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speech Separation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2211.12433.pdf&#34;&gt;TF-GridNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speech Translation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2109.12804.pdf&#34;&gt;Multi-decoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WIP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Mono-to-Binaural&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fdarmon/NeuralWarp&#34;&gt;NeuralWarp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Sing&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Foundation Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-to-Sing&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/MoonInTheRiver/DiffSinger&#34;&gt;DiffSinger&lt;/a&gt;, &lt;a href=&#34;https://github.com/jerryuhoo/VISinger&#34;&gt;VISinger&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Audio&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Foundation Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-to-Audio&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Make-An-Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Audio Inpainting&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Make-An-Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-to-Audio&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Make-An-Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Sound Detection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RetroCirce/HTS-Audio-Transformer&#34;&gt;Audio-transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Target Sound Detection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/gy65896/TSDNet&#34;&gt;TSDNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Sound Extraction&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/liuxubo717/LASS&#34;&gt;LASSNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Talking Head&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Foundation Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Talking Head Synthesis&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yerfor/GeneFace&#34;&gt;GeneFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate the open source of the following projects:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/espnet/espnet&#34;&gt;ESPNet&lt;/a&gt;   &lt;a href=&#34;https://github.com/NATSpeech/NATSpeech&#34;&gt;NATSpeech&lt;/a&gt;   &lt;a href=&#34;https://github.com/microsoft/visual-chatgpt&#34;&gt;Visual ChatGPT&lt;/a&gt;   &lt;a href=&#34;https://github.com/huggingface&#34;&gt;Hugging Face&lt;/a&gt;   &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;   &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;  &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Ryujinx/Ryujinx</title>
    <updated>2023-05-07T01:49:57Z</updated>
    <id>tag:github.com,2023-05-07:/Ryujinx/Ryujinx</id>
    <link href="https://github.com/Ryujinx/Ryujinx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Experimental Nintendo Switch Emulator written in C#&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://ryujinx.org/&#34;&gt;&lt;img src=&#34;https://i.imgur.com/WcCj6Rt.png&#34; alt=&#34;Ryujinx&#34; width=&#34;150&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;b&gt;Ryujinx&lt;/b&gt; &lt;br&gt; &lt;sub&gt;&lt;sup&gt;&lt;b&gt;(REE-YOU-JINX)&lt;/b&gt;&lt;/sup&gt;&lt;/sub&gt; &lt;br&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Ryujinx is an open-source Nintendo Switch emulator, created by gdkchan, written in C#. This emulator aims at providing excellent accuracy and performance, a user-friendly interface and consistent builds. It was written from scratch and development on the project began in September 2017. Ryujinx is available on Github under the &lt;a href=&#34;https://github.com/Ryujinx/Ryujinx/raw/master/LICENSE.txt&#34; target=&#34;_blank&#34;&gt;MIT license&lt;/a&gt;. &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Ryujinx/Ryujinx/actions/workflows/release.yml&#34;&gt; &lt;img src=&#34;https://github.com/Ryujinx/Ryujinx/actions/workflows/release.yml/badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://crwd.in/ryujinx&#34;&gt; &lt;img src=&#34;https://badges.crowdin.net/ryujinx/localized.svg?sanitize=true&#34; alt=&#34;&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/VkQYXAZ&#34;&gt; &lt;img src=&#34;https://img.shields.io/discord/410208534861447168?color=5865F2&amp;amp;label=Ryujinx&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt; &lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Ryujinx/Ryujinx-Website/master/public/assets/images/shell.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h5 align=&#34;center&#34;&gt; &lt;/h5&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;As of April 2023, Ryujinx has been tested on approximately 4,050 titles; over 4,000 boot past menus and into gameplay, with roughly 3,400 of those being considered playable. You can check out the compatibility list &lt;a href=&#34;https://github.com/Ryujinx/Ryujinx-Games-List/issues&#34;&gt;here&lt;/a&gt;. Anyone is free to submit a new game test or update an existing game test entry; simply follow the new issue template and testing guidelines, or post as a reply to the applicable game issue. Use the search function to see if a game has been tested already!&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To run this emulator, your PC must be equipped with at least 8GiB of RAM; failing to meet this requirement may result in a poor gameplay experience or unexpected crashes.&lt;/p&gt; &#xA;&lt;p&gt;See our &lt;a href=&#34;https://github.com/Ryujinx/Ryujinx/wiki/Ryujinx-Setup-&amp;amp;-Configuration-Guide&#34;&gt;Setup &amp;amp; Configuration Guide&lt;/a&gt; on how to set up the emulator.&lt;/p&gt; &#xA;&lt;p&gt;For our Local Wireless and LAN builds, see our &lt;a href=&#34;https://github.com/Ryujinx/Ryujinx/wiki/Multiplayer-(LDN-Local-Wireless)-Guide&#34;&gt;Multiplayer: Local Play/Local Wireless Guide &lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Avalonia UI comes with translations for various languages. See &lt;a href=&#34;https://crwd.in/ryujinx&#34;&gt;Crowdin&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Latest build&lt;/h2&gt; &#xA;&lt;p&gt;These builds are compiled automatically for each commit on the master branch. While we strive to ensure optimal stability and performance prior to pushing an update, our automated builds &lt;strong&gt;may be unstable or completely broken.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to see details on updates to the emulator, you can visit our &lt;a href=&#34;https://github.com/Ryujinx/Ryujinx/wiki/Changelog&#34;&gt;Changelog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The latest automatic build for Windows, macOS, and Linux can be found on the &lt;a href=&#34;https://ryujinx.org/download&#34;&gt;Official Website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;If you wish to build the emulator yourself, follow these steps:&lt;/p&gt; &#xA;&lt;h3&gt;Step 1&lt;/h3&gt; &#xA;&lt;p&gt;Install the X64 version of &lt;a href=&#34;https://dotnet.microsoft.com/download/dotnet/7.0&#34;&gt;.NET 7.0 (or higher) SDK&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Step 2&lt;/h3&gt; &#xA;&lt;p&gt;Either use &lt;code&gt;git clone https://github.com/Ryujinx/Ryujinx&lt;/code&gt; on the command line to clone the repository or use Code --&amp;gt; Download zip button to get the files.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3&lt;/h3&gt; &#xA;&lt;p&gt;To build Ryujinx, open a command prompt inside the project directory. You can quickly access it on Windows by holding shift in File Explorer, then right clicking and selecting &lt;code&gt;Open command window here&lt;/code&gt;. Then type the following command: &lt;code&gt;dotnet build -c Release -o build&lt;/code&gt; the built files will be found in the newly created build directory.&lt;/p&gt; &#xA;&lt;p&gt;Ryujinx system files are stored in the &lt;code&gt;Ryujinx&lt;/code&gt; folder. This folder is located in the user folder, which can be accessed by clicking &lt;code&gt;Open Ryujinx Folder&lt;/code&gt; under the File menu in the GUI.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Audio&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Audio output is entirely supported, audio input (microphone) isn&#39;t supported. We use C# wrappers for &lt;a href=&#34;https://openal-soft.org/&#34;&gt;OpenAL&lt;/a&gt;, and &lt;a href=&#34;https://www.libsdl.org/&#34;&gt;SDL2&lt;/a&gt; &amp;amp; &lt;a href=&#34;http://libsound.io/&#34;&gt;libsoundio&lt;/a&gt; as fallbacks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The CPU emulator, ARMeilleure, emulates an ARMv8 CPU and currently has support for most 64-bit ARMv8 and some of the ARMv7 (and older) instructions, including partial 32-bit support. It translates the ARM code to a custom IR, performs a few optimizations, and turns that into x86 code. There are three memory manager options available depending on the user&#39;s preference, leveraging both software-based (slower) and host-mapped modes (much faster). The fastest option (host, unchecked) is set by default. Ryujinx also features an optional Profiled Persistent Translation Cache, which essentially caches translated functions so that they do not need to be translated every time the game loads. The net result is a significant reduction in load times (the amount of time between launching a game and arriving at the title screen) for nearly every game. NOTE: this feature is enabled by default in the Options menu &amp;gt; System tab. You must launch the game at least twice to the title screen or beyond before performance improvements are unlocked on the third launch! These improvements are permanent and do not require any extra launches going forward.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The GPU emulator emulates the Switch&#39;s Maxwell GPU using either the OpenGL (version 4.5 minimum), Vulkan, or Metal (via MoltenVK) APIs through a custom build of OpenTK or Silk.NET respectively. There are currently six graphics enhancements available to the end user in Ryujinx: Disk Shader Caching, Resolution Scaling, Anti-Aliasing, Scaling Filters (including FSR), Anisotropic Filtering and Aspect Ratio Adjustment. These enhancements can be adjusted or toggled as desired in the GUI.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We currently have support for keyboard, mouse, touch input, JoyCon input support, and nearly all controllers. Motion controls are natively supported in most cases; for dual-JoyCon motion support, DS4Windows or BetterJoy are currently required. In all scenarios, you can set up everything inside the input configuration menu.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DLC &amp;amp; Modifications&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ryujinx is able to manage add-on content/downloadable content through the GUI. Mods (romfs, exefs, and runtime mods such as cheats) are also supported; the GUI contains a shortcut to open the respective mods folder for a particular game.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configuration&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The emulator has settings for enabling or disabling some logging, remapping controllers, and more. You can configure all of them through the graphical interface or manually through the config file, &lt;code&gt;Config.json&lt;/code&gt;, found in the user folder which can be accessed by clicking &lt;code&gt;Open Ryujinx Folder&lt;/code&gt; under the File menu in the GUI.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have contributions, suggestions, need emulator support or just want to get in touch with the team, join our &lt;a href=&#34;https://discord.com/invite/Ryujinx&#34;&gt;Discord server&lt;/a&gt;. You may also review our &lt;a href=&#34;https://github.com/Ryujinx/Ryujinx/wiki/Frequently-Asked-Questions&#34;&gt;FAQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Donations&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to support the project financially, Ryujinx has an active Patreon campaign.&lt;/p&gt; &#xA;&lt;a href=&#34;https://www.patreon.com/ryujinx&#34;&gt; &lt;img src=&#34;https://images.squarespace-cdn.com/content/v1/560c1d39e4b0b4fae0c9cf2a/1567548955044-WVD994WZP76EWF15T0L3/Patreon+Button.png?format=500w&#34; width=&#34;150&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;All developers working on the project do so in their free time, but the project has several expenses:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hackable Nintendo Switch consoles to reverse-engineer the hardware&lt;/li&gt; &#xA; &lt;li&gt;Additional computer hardware for testing purposes (e.g. GPUs to diagnose graphical bugs, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Licenses for various software development tools (e.g. Jetbrains, IDA)&lt;/li&gt; &#xA; &lt;li&gt;Web hosting and infrastructure maintenance (e.g. LDN servers)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All funds received through Patreon are considered a donation to support the project. Patrons receive early access to progress reports and exclusive access to developer interviews.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This software is licensed under the terms of the &lt;a href=&#34;https://github.com/Ryujinx/Ryujinx/raw/master/LICENSE.txt&#34; target=&#34;_blank&#34;&gt;MIT license.&lt;/a&gt;&lt;br&gt; This project makes use of code authored by the libvpx project, licensed under BSD and the ffmpeg project, licensed under LGPLv3. See &lt;a href=&#34;https://raw.githubusercontent.com/Ryujinx/Ryujinx/master/LICENSE.txt&#34;&gt;LICENSE.txt&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Ryujinx/Ryujinx/master/distribution/legal/THIRDPARTY.md&#34;&gt;THIRDPARTY.md&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Thealexbarney/LibHac&#34;&gt;LibHac&lt;/a&gt; is used for our file-system.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amiiboapi.com&#34;&gt;AmiiboAPI&lt;/a&gt; is used in our Amiibo emulation.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>