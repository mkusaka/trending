<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-26T01:49:24Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>shyamsn97/mario-gpt</title>
    <updated>2023-02-26T01:49:24Z</updated>
    <id>tag:github.com,2023-02-26:/shyamsn97/mario-gpt</id>
    <link href="https://github.com/shyamsn97/mario-gpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generating Mario Levels with GPT2. Code for the paper &#34;MarioGPT: Open-Ended Text2Level Generation through Large Language Models&#34; https://arxiv.org/abs/2302.05981&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;MarioGPT: Open-Ended Text2Level Generation through Large Language Models&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.05981&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/paper-arxiv.2302.05981-B31B1B.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/mario-gpt&#34;&gt;&lt;img src=&#34;https://badgen.net/pypi/v/mario-gpt/&#34; alt=&#34;PyPi version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/multimodalart/mariogpt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%20HuggingFace%20-Demo-blue.svg?sanitize=true&#34; alt=&#34;HuggingFace Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/16KR9idJUim6RAiyPASoQAaC768AvOGxP?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/shyamsn97/mario-gpt/main/#interacting-with-levels&#34;&gt;Playing Generated Level&lt;/a&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Generated Level&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shyamsn97/mario-gpt/main/static/example_interactive.gif&#34; alt=&#34;alt text&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shyamsn97/mario-gpt/main/static/test_level.png&#34; alt=&#34;alt text&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;How does it work?&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Architecture&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Example Prompt Generations&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shyamsn97/mario-gpt/main/static/architecture.png&#34; alt=&#34;alt text&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shyamsn97/mario-gpt/main/static/prompt-samples.png&#34; alt=&#34;alt text&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;MarioGPT is a finetuned GPT2 model (specifically, &lt;a href=&#34;https://huggingface.co/distilgpt2&#34;&gt;distilgpt2&lt;/a&gt;), that is trained on a subset Super Mario Bros and Super Mario Bros: The Lost Levels levels, provided by &lt;a href=&#34;https://github.com/TheVGLC/TheVGLC&#34;&gt;The Video Game Level Corpus&lt;/a&gt;. MarioGPT is able to generate levels, guided by a simple text prompt. This generation is not perfect, but we believe this is a great first step more controllable and diverse level / environment generation. Forward generation:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shyamsn97/mario-gpt/main/static/timelapse_0.gif&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python3.8+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;from pypi&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install mario-gpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or from source&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:shyamsn97/mario-gpt.git&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generating Levels&lt;/h2&gt; &#xA;&lt;p&gt;Since our models are built off of the amazing &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; library, we host our model in &lt;a href=&#34;https://huggingface.co/shyamsn97/Mario-GPT2-700-context-length&#34;&gt;https://huggingface.co/shyamsn97/Mario-GPT2-700-context-length&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This code snippet is the minimal code you need to generate a mario level!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mario_gpt import MarioLM, SampleOutput&#xA;&#xA;# pretrained_model = shyamsn97/Mario-GPT2-700-context-length&#xA;&#xA;mario_lm = MarioLM()&#xA;&#xA;# use cuda to speed stuff up&#xA;# import torch&#xA;# device = torch.device(&#39;cuda&#39;)&#xA;# mario_lm = mario_lm.to(device)&#xA;&#xA;prompts = [&#34;many pipes, many enemies, some blocks, high elevation&#34;]&#xA;&#xA;# generate level of size 1400, pump temperature up to ~2.4 for more stochastic but playable levels&#xA;generated_level = mario_lm.sample(&#xA;    prompts=prompts,&#xA;    num_steps=1400,&#xA;    temperature=2.0,&#xA;    use_tqdm=True&#xA;)&#xA;&#xA;# show string list&#xA;generated_level.level&#xA;&#xA;# show PIL image&#xA;generated_level.img&#xA;&#xA;# save image&#xA;generated_level.img.save(&#34;generated_level.png&#34;)&#xA;&#xA;# save text level to file&#xA;generated_level.save(&#34;generated_level.txt&#34;)&#xA;&#xA;# play in interactive&#xA;generated_level.play()&#xA;&#xA;# run Astar agent&#xA;generated_level.run_astar()&#xA;&#xA;# Continue generation&#xA;generated_level_continued = mario_lm.sample(&#xA;    seed=generated_level,&#xA;    prompts=prompts,&#xA;    num_steps=1400,&#xA;    temperature=2.0,&#xA;    use_tqdm=True&#xA;)&#xA;&#xA;# load from text file&#xA;loaded_level = SampleOutput.load(&#34;generated_level.txt&#34;)&#xA;&#xA;# play from loaded (should be the same level that we generated)&#xA;loaded_level.play()&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/shyamsn97/mario-gpt/main/notebooks/Sampling.ipynb&#34;&gt;notebook&lt;/a&gt; for a more in depth tutorial to generate levels&lt;/h5&gt; &#xA;&lt;h2&gt;Interacting with Levels&lt;/h2&gt; &#xA;&lt;p&gt;Right now there are two ways to interact with generated levels:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/multimodalart/mariogpt&#34;&gt;Huggingface demo&lt;/a&gt; -- Thanks to the amazing work by &lt;a href=&#34;https://github.com/multimodalart&#34;&gt;multimodalart&lt;/a&gt;, you can generate and play levels interactively in the browser! In addition, gpus are provided so you don&#39;t have to own one yourself.&lt;/li&gt; &#xA; &lt;li&gt;Using the &lt;a href=&#34;https://raw.githubusercontent.com/shyamsn97/mario-gpt/main/mario_gpt/simulator/simulator.py&#34;&gt;play and astar methods&lt;/a&gt;. These require you to have java installed on your computer (Java 8+ tested). For interactive, use the &lt;code&gt;play()&lt;/code&gt; method and for astar use the &lt;code&gt;run_astar&lt;/code&gt; method. Example:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mario_gpt import MarioLM&#xA;&#xA;mario_lm = MarioLM()&#xA;&#xA;prompts = [&#34;many pipes, many enemies, some blocks, high elevation&#34;]&#xA;&#xA;generated_level = mario_lm.sample(&#xA;    prompts=prompts,&#xA;    num_steps=1400,&#xA;    temperature=2.0,&#xA;    use_tqdm=True&#xA;)&#xA;&#xA;# play in interactive&#xA;generated_level.play()&#xA;&#xA;# run Astar agent&#xA;generated_level.run_astar()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Future Plans&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s a list of some stuff that will be added to the codebase!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Basic inference code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add MarioBert Model&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add Interactive simulator&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Inpainting functionality from paper&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Open-ended level generation code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training code from paper&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Different generation methods (eg. constrained beam search, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;p&gt;Shyam Sudhakaran &lt;a href=&#34;mailto:shyamsnair@protonmail.com&#34;&gt;shyamsnair@protonmail.com&lt;/a&gt;, &lt;a href=&#34;https://github.com/shyamsn97&#34;&gt;https://github.com/shyamsn97&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Miguel Gonz√°lez-Duque &lt;a href=&#34;mailto:migd@itu.dk&#34;&gt;migd@itu.dk&lt;/a&gt;, &lt;a href=&#34;https://github.com/miguelgondu&#34;&gt;https://github.com/miguelgondu&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Claire Glanois &lt;a href=&#34;mailto:clgl@itu.dk&#34;&gt;clgl@itu.dk&lt;/a&gt;, &lt;a href=&#34;https://github.com/claireaoi&#34;&gt;https://github.com/claireaoi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Matthias Freiberger &lt;a href=&#34;mailto:matfr@itu.dk&#34;&gt;matfr@itu.dk&lt;/a&gt;, &lt;a href=&#34;https://github.com/matfrei&#34;&gt;https://github.com/matfrei&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Elias Najarro &lt;a href=&#34;mailto:enaj@itu.dk&#34;&gt;enaj@itu.dk&lt;/a&gt;, &lt;a href=&#34;https://github.com/enajx&#34;&gt;https://github.com/enajx&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Sebastian Risi &lt;a href=&#34;mailto:sebr@itu.dk&#34;&gt;sebr@itu.dk&lt;/a&gt;, &lt;a href=&#34;https://github.com/sebastianrisi&#34;&gt;https://github.com/sebastianrisi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use the code for academic or commecial use, please cite the associated paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{https://doi.org/10.48550/arxiv.2302.05981,&#xA;  doi = {10.48550/ARXIV.2302.05981},&#xA;  &#xA;  url = {https://arxiv.org/abs/2302.05981},&#xA;  &#xA;  author = {Sudhakaran, Shyam and Gonz√°lez-Duque, Miguel and Glanois, Claire and Freiberger, Matthias and Najarro, Elias and Risi, Sebastian},&#xA;  &#xA;  keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},&#xA;  &#xA;  title = {MarioGPT: Open-Ended Text2Level Generation through Large Language Models},&#xA;  &#xA;  publisher = {arXiv},&#xA;  &#xA;  year = {2023},&#xA;  &#xA;  copyright = {arXiv.org perpetual, non-exclusive license}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>sczhou/CodeFormer</title>
    <updated>2023-02-26T01:49:24Z</updated>
    <id>tag:github.com,2023-02-26:/sczhou/CodeFormer</id>
    <link href="https://github.com/sczhou/CodeFormer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[NeurIPS 2022] Towards Robust Blind Face Restoration with Codebook Lookup Transformer&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sczhou/CodeFormer/master/assets/CodeFormer_logo.png&#34; height=&#34;110&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Towards Robust Blind Face Restoration with Codebook Lookup Transformer (NeurIPS 2022)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.11253&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://shangchenzhou.com/projects/CodeFormer/&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/d3VDpkXlueI&#34;&gt;Video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;google colab logo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/sczhou/CodeFormer&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-%F0%9F%A4%97%20Hugging%20Face-blue&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/sczhou/codeformer&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-%F0%9F%9A%80%20Replicate-blue&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://visitor-badge-sczhou.glitch.me/badge?page_id=sczhou/CodeFormer&#34; alt=&#34;visitors&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://shangchenzhou.com/&#34;&gt;Shangchen Zhou&lt;/a&gt;, &lt;a href=&#34;https://ckkelvinchan.github.io/&#34;&gt;Kelvin C.K. Chan&lt;/a&gt;, &lt;a href=&#34;https://li-chongyi.github.io/&#34;&gt;Chongyi Li&lt;/a&gt;, &lt;a href=&#34;https://www.mmlab-ntu.com/person/ccloy/&#34;&gt;Chen Change Loy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;S-Lab, Nanyang Technological University&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sczhou/CodeFormer/master/assets/network.jpg&#34; width=&#34;800px&#34;&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; If CodeFormer is helpful to your images or projects, please help star this repo. Thanks! &lt;span&gt;ü§ó&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[&lt;font color=&#34;#d1585d&#34;&gt;News&lt;/font&gt;]&lt;/strong&gt;: &lt;span&gt;üê≥&lt;/span&gt; &lt;em&gt;We regret to inform you that the release of our code will be postponed from its earlier plan. Nevertheless, we assure you that it will be made available &lt;strong&gt;by the end of this April&lt;/strong&gt;. Thank you for your understanding and patience. Our apologies for any inconvenience this may cause.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Update&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.02.10&lt;/strong&gt;: Include &lt;code&gt;dlib&lt;/code&gt; as a new face detector option, it produces more accurate face identity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2022.10.05&lt;/strong&gt;: Support video input &lt;code&gt;--input_path [YOUR_VIDOE.mp4]&lt;/code&gt;. Try it to enhance your videos! &lt;span&gt;üé¨&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2022.09.14&lt;/strong&gt;: Integrated to &lt;span&gt;ü§ó&lt;/span&gt; &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Hugging Face&lt;/a&gt;. Try out online demo! &lt;a href=&#34;https://huggingface.co/spaces/sczhou/CodeFormer&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-%F0%9F%A4%97%20Hugging%20Face-blue&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2022.09.09&lt;/strong&gt;: Integrated to &lt;span&gt;üöÄ&lt;/span&gt; &lt;a href=&#34;https://replicate.com/explore&#34;&gt;Replicate&lt;/a&gt;. Try out online demo! &lt;a href=&#34;https://replicate.com/sczhou/codeformer&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-%F0%9F%9A%80%20Replicate-blue&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2022.09.04&lt;/strong&gt;: Add face upsampling &lt;code&gt;--face_upsample&lt;/code&gt; for high-resolution AI-created face enhancement.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2022.08.23&lt;/strong&gt;: Some modifications on face detection and fusion for better AI-created face enhancement.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2022.08.07&lt;/strong&gt;: Integrate &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;Real-ESRGAN&lt;/a&gt; to support background image enhancement.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2022.07.29&lt;/strong&gt;: Integrate new face detectors of &lt;code&gt;[&#39;RetinaFace&#39;(default), &#39;YOLOv5&#39;]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2022.07.17&lt;/strong&gt;: Add Colab demo of CodeFormer. &lt;a href=&#34;https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;google colab logo&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2022.07.16&lt;/strong&gt;: Release inference code for face restoration. &lt;span&gt;üòä&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2022.06.21&lt;/strong&gt;: This repo is created.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TODO&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add checkpoint for face inpainting&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add checkpoint for face colorization&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add training code and config files&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;del&gt;Add background image enhancement&lt;/del&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;&lt;span&gt;üêº&lt;/span&gt; Try Enhancing Old Photos / Fixing AI-arts&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imgsli.com/MTI3NTE2&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sczhou/CodeFormer/master/assets/imgsli_1.jpg&#34; height=&#34;226px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MTI3NTE1&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sczhou/CodeFormer/master/assets/imgsli_2.jpg&#34; height=&#34;226px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imgsli.com/MTI3NTIw&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sczhou/CodeFormer/master/assets/imgsli_3.jpg&#34; height=&#34;226px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Face Restoration&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sczhou/CodeFormer/master/assets/restoration_result1.png&#34; width=&#34;400px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sczhou/CodeFormer/master/assets/restoration_result2.png&#34; width=&#34;400px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sczhou/CodeFormer/master/assets/restoration_result3.png&#34; width=&#34;400px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sczhou/CodeFormer/master/assets/restoration_result4.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Face Color Enhancement and Restoration&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sczhou/CodeFormer/master/assets/color_enhancement_result1.png&#34; width=&#34;400px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sczhou/CodeFormer/master/assets/color_enhancement_result2.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Face Inpainting&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sczhou/CodeFormer/master/assets/inpainting_result1.png&#34; width=&#34;400px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sczhou/CodeFormer/master/assets/inpainting_result2.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dependencies and Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pytorch &amp;gt;= 1.7.1&lt;/li&gt; &#xA; &lt;li&gt;CUDA &amp;gt;= 10.1&lt;/li&gt; &#xA; &lt;li&gt;Other required packages in &lt;code&gt;requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;# git clone this repository&#xA;git clone https://github.com/sczhou/CodeFormer&#xA;cd CodeFormer&#xA;&#xA;# create new anaconda env&#xA;conda create -n codeformer python=3.8 -y&#xA;conda activate codeformer&#xA;&#xA;# install python dependencies&#xA;pip3 install -r requirements.txt&#xA;python basicsr/setup.py develop&#xA;conda install -c conda-forge dlib (only for dlib face detector)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- conda install -c conda-forge dlib --&gt; &#xA;&lt;h3&gt;Quick Inference&lt;/h3&gt; &#xA;&lt;h4&gt;Download Pre-trained Models:&lt;/h4&gt; &#xA;&lt;p&gt;Download the facelib and dlib pretrained models from [&lt;a href=&#34;https://drive.google.com/drive/folders/1b_3qwrzY_kTQh0-SnBoGBgOrJ_PLZSKm?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; | &lt;a href=&#34;https://entuedu-my.sharepoint.com/:f:/g/personal/s200094_e_ntu_edu_sg/EvDxR7FcAbZMp_MA9ouq7aQB8XTppMb3-T0uGZ_2anI2mg?e=DXsJFo&#34;&gt;OneDrive&lt;/a&gt;] to the &lt;code&gt;weights/facelib&lt;/code&gt; folder. You can manually download the pretrained models OR download by running the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/download_pretrained_models.py facelib&#xA;python scripts/download_pretrained_models.py dlib (only for dlib face detector)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the CodeFormer pretrained models from [&lt;a href=&#34;https://drive.google.com/drive/folders/1CNNByjHDFt0b95q54yMVp6Ifo5iuU6QS?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; | &lt;a href=&#34;https://entuedu-my.sharepoint.com/:f:/g/personal/s200094_e_ntu_edu_sg/EoKFj4wo8cdIn2-TY2IV6CYBhZ0pIG4kUOeHdPR_A5nlbg?e=AO8UN9&#34;&gt;OneDrive&lt;/a&gt;] to the &lt;code&gt;weights/CodeFormer&lt;/code&gt; folder. You can manually download the pretrained models OR download by running the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/download_pretrained_models.py CodeFormer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Prepare Testing Data:&lt;/h4&gt; &#xA;&lt;p&gt;You can put the testing images in the &lt;code&gt;inputs/TestWhole&lt;/code&gt; folder. If you would like to test on cropped and aligned faces, you can put them in the &lt;code&gt;inputs/cropped_faces&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h4&gt;Testing on Face Restoration:&lt;/h4&gt; &#xA;&lt;p&gt;[Note] If you want to compare CodeFormer in your paper, please run the following command indicating &lt;code&gt;--has_aligned&lt;/code&gt; (for cropped and aligned face), as the command for the whole image will involve a process of face-background fusion that may damage hair texture on the boundary, which leads to unfair comparison.&lt;/p&gt; &#xA;&lt;p&gt;üßëüèª Face Restoration (cropped and aligned face)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# For cropped and aligned faces&#xA;python inference_codeformer.py -w 0.5 --has_aligned --input_path [image folder]|[image path]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;span&gt;üñº&lt;/span&gt; Whole Image Enhancement&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# For whole image&#xA;# Add &#39;--bg_upsampler realesrgan&#39; to enhance the background regions with Real-ESRGAN&#xA;# Add &#39;--face_upsample&#39; to further upsample restorated face with Real-ESRGAN&#xA;python inference_codeformer.py -w 0.7 --input_path [image folder]|[image path]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;span&gt;üé¨&lt;/span&gt; Video Enhancement&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# For Windows/Mac users, please install ffmpeg first&#xA;conda install -c conda-forge ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# For video clips&#xA;# video path should end with &#39;.mp4&#39;|&#39;.mov&#39;|&#39;.avi&#39;&#xA;python inference_codeformer.py --bg_upsampler realesrgan --face_upsample -w 1.0 --input_path [video path]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Fidelity weight &lt;em&gt;w&lt;/em&gt; lays in [0, 1]. Generally, smaller &lt;em&gt;w&lt;/em&gt; tends to produce a higher-quality result, while larger &lt;em&gt;w&lt;/em&gt; yields a higher-fidelity result.&lt;/p&gt; &#xA;&lt;p&gt;The results will be saved in the &lt;code&gt;results&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;If our work is useful for your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{zhou2022codeformer,&#xA;    author = {Zhou, Shangchen and Chan, Kelvin C.K. and Li, Chongyi and Loy, Chen Change},&#xA;    title = {Towards Robust Blind Face Restoration with Codebook Lookup TransFormer},&#xA;    booktitle = {NeurIPS},&#xA;    year = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;This project is licensed under &lt;a rel=&#34;license&#34; href=&#34;https://github.com/sczhou/CodeFormer/raw/master/LICENSE&#34;&gt;NTU S-Lab License 1.0&lt;/a&gt;. Redistribution and use should follow this license.&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgement&lt;/h3&gt; &#xA;&lt;p&gt;This project is based on &lt;a href=&#34;https://github.com/XPixelGroup/BasicSR&#34;&gt;BasicSR&lt;/a&gt;. Some codes are brought from &lt;a href=&#34;https://github.com/samb-t/unleashing-transformers&#34;&gt;Unleashing Transformers&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepcam-cn/yolov5-face&#34;&gt;YOLOv5-face&lt;/a&gt;, and &lt;a href=&#34;https://github.com/xinntao/facexlib&#34;&gt;FaceXLib&lt;/a&gt;. We also adopt &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;Real-ESRGAN&lt;/a&gt; to support background image enhancement. Thanks for their awesome works.&lt;/p&gt; &#xA;&lt;h3&gt;Contact&lt;/h3&gt; &#xA;&lt;p&gt;If you have any questions, please feel free to reach me out at &lt;code&gt;shangchenzhou@gmail.com&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jaymody/picoGPT</title>
    <updated>2023-02-26T01:49:24Z</updated>
    <id>tag:github.com,2023-02-26:/jaymody/picoGPT</id>
    <link href="https://github.com/jaymody/picoGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An unnecessarily tiny implementation of GPT-2 in NumPy.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PicoGPT&lt;/h1&gt; &#xA;&lt;p&gt;You&#39;ve seen &lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;openai/gpt-2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ve seen &lt;a href=&#34;https://github.com/karpathy/mingpt&#34;&gt;karpathy/minGPT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ve even seen &lt;a href=&#34;https://github.com/karpathy/nanogpt&#34;&gt;karpathy/nanoGPT&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;But have you seen &lt;a href=&#34;https://github.com/jaymody/picoGPT&#34;&gt;picoGPT&lt;/a&gt;??!?&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;picoGPT&lt;/code&gt; is an unnecessarily tiny and minimal implementation of &lt;a href=&#34;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;GPT-2&lt;/a&gt; in plain &lt;a href=&#34;https://numpy.org&#34;&gt;NumPy&lt;/a&gt;. The entire forward pass code is &lt;a href=&#34;https://github.com/jaymody/picoGPT/raw/main/gpt2_pico.py#L3-L41&#34;&gt;40 lines of code&lt;/a&gt;. I wrote a related &lt;a href=&#34;https://jaykmody.com/blog/gpt-from-scratch/&#34;&gt;blog post&lt;/a&gt; for picoGPT.&lt;/p&gt; &#xA;&lt;p&gt;picoGPT features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fast? ‚ùå Nah, picoGPT is megaSLOW üêå&lt;/li&gt; &#xA; &lt;li&gt;Training code? ‚ùå Error, 4Ô∏è‚É£0Ô∏è‚É£4Ô∏è‚É£ not found&lt;/li&gt; &#xA; &lt;li&gt;Batch inference? ‚ùå picoGPT is civilized, single file line, one at a time only&lt;/li&gt; &#xA; &lt;li&gt;top-p sampling? ‚ùå top-k? ‚ùå temperature? ‚ùå categorical sampling?! ‚ùå greedy? ‚úÖ&lt;/li&gt; &#xA; &lt;li&gt;Readable? &lt;code&gt;gpt2.py&lt;/code&gt; ‚úÖ &lt;code&gt;gpt2_pico.py&lt;/code&gt; ‚ùå&lt;/li&gt; &#xA; &lt;li&gt;Smol??? ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ YESS!!! TEENIE TINY in fact ü§è&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A quick breakdown of each of the files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;encoder.py&lt;/code&gt; contains the code for OpenAI&#39;s BPE Tokenizer, taken straight from their &lt;a href=&#34;https://github.com/openai/gpt-2/raw/master/src/encoder.py&#34;&gt;gpt-2 repo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;utils.py&lt;/code&gt; contains the code to download and load the GPT-2 model weights, tokenizer, and hyper-parameters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gpt2.py&lt;/code&gt; contains the actual GPT model and generation code which we can run as a python script.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gpt2_pico.py&lt;/code&gt; is the same as &lt;code&gt;gpt2.py&lt;/code&gt;, but in even fewer lines of code. Why? Because why not üòéüëç.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Dependencies&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Tested on &lt;code&gt;Python 3.9.10&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Usage&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gpt2.py &#34;Alan Turing theorized that computers would one day become&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which generates&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; the most powerful machines on the planet.&#xA;&#xA;The computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also control the number of tokens to generate, the model size (one of &lt;code&gt;[&#34;124M&#34;, &#34;355M&#34;, &#34;774M&#34;, &#34;1558M&#34;]&lt;/code&gt;), and the directory to save the models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gpt2.py \&#xA;    &#34;Alan Turing theorized that computers would one day become&#34; \&#xA;    --n_tokens_to_generate 40 \&#xA;    --model_size &#34;124M&#34; \&#xA;    --models_dir &#34;models&#34;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>