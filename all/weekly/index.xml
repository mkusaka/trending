<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-11T01:41:03Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lizongying/my-tv</title>
    <updated>2024-02-11T01:41:03Z</updated>
    <id>tag:github.com,2024-02-11:/lizongying/my-tv</id>
    <link href="https://github.com/lizongying/my-tv" rel="alternate"></link>
    <summary type="html">&lt;p&gt;我的电视 电视直播软件，安装即可使用&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;我的电视&lt;/h1&gt; &#xA;&lt;p&gt;电视直播软件，安装即可使用&lt;/p&gt; &#xA;&lt;h2&gt;使用&lt;/h2&gt; &#xA;&lt;p&gt;下载安装 &lt;a href=&#34;https://github.com/lizongying/my-tv/releases/&#34;&gt;releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;其他地址 &lt;a href=&#34;https://lyrics.run/my-tv.html&#34;&gt;my-tv&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizongying/my-tv/main/screenshots/img_3.png&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/lizongying/my-tv/main/screenshots/img_2.png&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/lizongying/my-tv/main/screenshots/img_1.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;更新日志&lt;/h2&gt; &#xA;&lt;h3&gt;v1.5.2（通用版）&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;修复APP恢复后频道号、频道列表不自动消失的问题&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1.5.1（高版本专用）&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;性能优化&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1.5.0（通用版）&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;修复部分情况下APP切换后无法继续播放的问题&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1.4.9（高版本专用）&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;同步v1.4.8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1.4.8（通用版）&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;频道号从1开始，CCTV5+为18&lt;/li&gt; &#xA; &lt;li&gt;提高CCTV6清晰度&lt;/li&gt; &#xA; &lt;li&gt;增加天津卫视、新疆卫视&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1.4.7（高版本专用）&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;修复部分用户CCTV13播放过程中卡住的问题&lt;/li&gt; &#xA; &lt;li&gt;调整CCTV的频道顺序&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1.4.6（通用版）&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;10以下频道不再需要先按0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1.4.5（高版本专用）&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;数字选台配置&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1.4.4（通用版）&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;优化图标显示&lt;/li&gt; &#xA; &lt;li&gt;增加换台反转&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1.3.3&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;部分错误会提示用户&lt;/li&gt; &#xA; &lt;li&gt;菜单3秒钟后自动关闭&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1.3.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;增加重试，减少因网络问题导致的播放失败&lt;/li&gt; &#xA; &lt;li&gt;优化横幅banner&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1.2.6&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;支持安卓4.2&lt;/li&gt; &#xA; &lt;li&gt;解决部分频道无法播放的问题&lt;/li&gt; &#xA; &lt;li&gt;修复切换时有时没有恢复播放的问题&lt;/li&gt; &#xA; &lt;li&gt;左右键不再切换源&lt;/li&gt; &#xA; &lt;li&gt;增大频道信息标题尺寸，缩小与节目信息的间隔&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1.2.5&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;美化频道信息显示&lt;/li&gt; &#xA; &lt;li&gt;优化节目单获取&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1.2.4&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;改变换台滑动方向，上一个频道下滑，下一个频道上滑&lt;/li&gt; &#xA; &lt;li&gt;软件退出时，退出播放器&lt;/li&gt; &#xA; &lt;li&gt;播放相同的频道，不再重复加载&lt;/li&gt; &#xA; &lt;li&gt;暂时移除部分频道&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;其他&lt;/h2&gt; &#xA;&lt;p&gt;小米电视可以使用小米电视助手进行安装&lt;/p&gt; &#xA;&lt;p&gt;如电视可以启用ADB，也可以通过ADB进行安装：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;adb install my-tv.apk&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;音量不同&lt;/li&gt; &#xA; &lt;li&gt;大湾区卫视、广东4k超高清、广东珠江&lt;/li&gt; &#xA; &lt;li&gt;CETV教育频道&lt;/li&gt; &#xA; &lt;li&gt;CHC高清三个电影频道&lt;/li&gt; &#xA; &lt;li&gt;地方频道&lt;/li&gt; &#xA; &lt;li&gt;收藏夹&lt;/li&gt; &#xA; &lt;li&gt;自定义源&lt;/li&gt; &#xA; &lt;li&gt;凤凰卫视、凤凰资讯台&lt;/li&gt; &#xA; &lt;li&gt;海外&lt;/li&gt; &#xA; &lt;li&gt;1.5.0 无法安装，1.5.1 可以安装&lt;/li&gt; &#xA; &lt;li&gt;获取系统时间&lt;/li&gt; &#xA; &lt;li&gt;选中的图标比例能否相差更大&lt;/li&gt; &#xA; &lt;li&gt;自动重连&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;赞赏&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizongying/my-tv/main/screenshots/zfb.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>allenai/OLMo</title>
    <updated>2024-02-11T01:41:03Z</updated>
    <id>tag:github.com,2024-02-11:/allenai/OLMo</id>
    <link href="https://github.com/allenai/OLMo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Modeling, training, eval, and inference code for OLMo&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;img src=&#34;https://github.com/allenai/OLMo/assets/8812459/774ac485-a535-4768-8f7c-db7be20f5cc3&#34; width=&#34;300&#34;/&gt; --&gt; &#xA; &lt;img src=&#34;https://allenai.org/olmo/olmo-7b-animation.gif&#34; alt=&#34;OLMo Logo&#34; width=&#34;800&#34; style=&#34;margin-left:&#39;auto&#39; margin-right:&#39;auto&#39; display:&#39;block&#39;&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;h1&gt;OLMo: Open Language Model&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;GitHub License&#34; src=&#34;https://img.shields.io/github/license/allenai/OLMo&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/OLMo/releases&#34;&gt; &lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/github/release/allenai/OLMo.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2402.00838.pdf&#34;&gt; &lt;img alt=&#34;Paper URL&#34; src=&#34;https://img.shields.io/badge/arxiv-2402.00838-blue&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;OLMo is a repository for training and using AI2&#39;s state-of-the-art open language models. It is built by scientists, for scientists.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;First install &lt;a href=&#34;https://pytorch.org&#34;&gt;PyTorch&lt;/a&gt; according to the instructions specific to your operating system.&lt;/p&gt; &#xA;&lt;p&gt;To install from source (recommended for training/fine-tuning) run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/allenai/OLMo.git&#xA;cd OLMo&#xA;pip install -e .[all]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Otherwise you can install the model code by itself directly from PyPI with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install ai2-olmo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Models overview&lt;/h2&gt; &#xA;&lt;p&gt;The core models in the OLMo family released so far are (all trained on the &lt;a href=&#34;https://huggingface.co/datasets/allenai/dolma&#34;&gt;Dolma dataset&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Training Tokens&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Context Length&lt;/th&gt; &#xA;   &lt;th&gt;Training Config&lt;/th&gt; &#xA;   &lt;th&gt;W&amp;amp;B Logs&lt;/th&gt; &#xA;   &lt;th&gt;Data Order File(s) ☨&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/allenai/OLMo-1B&#34;&gt;OLMo 1B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3 Trillion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/configs/official/OLMo-1B.yaml&#34;&gt;configs/official/OLMo-1B.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://wandb.ai/ai2-llm/OLMo-1B/reports/OLMo-1B--Vmlldzo2NzY1Njk1&#34;&gt;wandb.ai/…/OLMo-1B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://olmo-checkpoints.org/ai2-llm/olmo-small/46zc5fly/train_data/global_indices.npy&#34;&gt;epoch 1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/allenai/OLMo-7B&#34;&gt;OLMo 7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.5 Trillion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/configs/official/OLMo-7B.yaml&#34;&gt;configs/official/OLMo-7B.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5&#34;&gt;wandb.ai/…/OLMo-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://olmo-checkpoints.org/ai2-llm/olmo-medium/wvc30anm/train_data/global_indices.npy&#34;&gt;epoch 1&lt;/a&gt;, &lt;a href=&#34;https://olmo-checkpoints.org/ai2-llm/olmo-medium/wd2gxrza/train_data/global_indices.npy&#34;&gt;epoch 2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/allenai/OLMo-7B-Twin-2T&#34;&gt;OLMo 7B Twin 2T&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2 Trillion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/configs/official/OLMo-7B.yaml&#34;&gt;configs/official/OLMo-7B.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B-Twin-2T--Vmlldzo2NzU0NTIz&#34;&gt;wandb.ai/…/OLMo-7B-Twin-2T&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://olmo-checkpoints.org/ai2-llm/olmo-medium/wvc30anm/train_data/global_indices.npy&#34;&gt;epoch 1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;☨ &lt;em&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo/main/#inspecting-training-data&#34;&gt;Inspecting training data&lt;/a&gt; below for usage.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;You can utilize our Hugging Face integration to run inference on the olmo checkpoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from hf_olmo import * # registers the Auto* classes&#xA;&#xA;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;olmo = AutoModelForCausalLM.from_pretrained(&#34;allenai/OLMo-7B&#34;)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;allenai/OLMo-7B&#34;)&#xA;&#xA;message = [&#34;Language modeling is &#34;]&#xA;inputs = tokenizer(message, return_tensors=&#39;pt&#39;, return_token_type_ids=False)&#xA;response = olmo.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)&#xA;print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, with the Hugging Face pipeline abstraction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import pipeline&#xA;olmo_pipe = pipeline(&#34;text-generation&#34;, model=&#34;allenai/OLMo-7B&#34;)&#xA;print(olmo_pipe(&#34;Language modeling is&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference on finetuned checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;If you finetune the model using the code above, you can use the conversion script to convert a native OLMo checkpoint to a Hugging Face-compatible checkpoint&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python hf_olmo/convert_olmo_to_hf.py --checkpoint-dir /path/to/checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;olmo = AutoModelForCausalLM.from_pretrained(&#34;allenai/OLMo-7B&#34;, torch_dtype=torch.float16, load_in_8bit=True)  # requires bitsandbytes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The quantized model is more sensitive to typing / cuda, so it is recommended to pass the inputs as inputs.input_ids.to(&#39;cuda&#39;) to avoid potential issues.&lt;/p&gt; &#xA;&lt;h2&gt;Reproducibility&lt;/h2&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;The configs used to train the official OLMo models are provided in the &lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/configs/official&#34;&gt;&lt;code&gt;configs/official/&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;Note that while the training and validation data is public and free to download, the paths to the data within those configs are pointed at a CloudFlare R2 bucket, which requires an API key for programmatic access. So in order to use any of these configs to reproduce a training run you&#39;ll first have to download the corresponding data to a location of your choosing and then update the paths in the config accordingly.&lt;/p&gt; &#xA;&lt;p&gt;You can derive the public HTTP URL from an R2 URL by replacing &lt;code&gt;r2://olmo-data&lt;/code&gt; with &lt;code&gt;https://olmo-data.org&lt;/code&gt;. For example, if the R2 data URL is:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;r2://olmo-data/preprocessed/olmo-mix/v1_5/gpt-neox-20b-pii-special/part-000-00000.npy&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;then the corresponding public URL is:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;https://olmo-data.org/preprocessed/olmo-mix/v1_5/gpt-neox-20b-pii-special/part-000-00000.npy&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;ve updated the data paths in the config you can launch a training run via &lt;code&gt;torchrun&lt;/code&gt;. For example, to launch the 1B model training on a single 8x GPU node, you would run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node=8 scripts/train.py configs/official/OLMo-1B.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use the same method to launch multi-node jobs as well. See &lt;a href=&#34;https://pytorch.org/docs/stable/elastic/run.html&#34;&gt;the documentation&lt;/a&gt; for &lt;code&gt;torchrun&lt;/code&gt; to understand the additional arguments you&#39;ll need to configure the rendezvous backend / endpoint.&lt;/p&gt; &#xA;&lt;h3&gt;Inspecting training data&lt;/h3&gt; &#xA;&lt;p&gt;You may be interesting in inspecting the exact tokens that composed a particular batch during the training of one of the OLMo models. We provide tools to do this, but first you&#39;ll need to download the data as above (unless you have an R2 API key) and update the corresponding config accordingly.&lt;/p&gt; &#xA;&lt;p&gt;Then take note of the URL of the data order file you want, which can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo/main/#models-overview&#34;&gt;Models Overview&lt;/a&gt; table. For example, the data order file for the first epoch of the OLMo-7B model is &lt;a href=&#34;https://olmo-checkpoints.org/ai2-llm/olmo-small/46zc5fly/train_data/global_indices.npy&#34;&gt;https://olmo-checkpoints.org/ai2-llm/olmo-medium/wvc30anm/train_data/global_indices.npy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once you have that you can use this snippet to inspect the data within a particular batch:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;from cached_path import cached_path&#xA;&#xA;from olmo.config import TrainConfig&#xA;from olmo.data import build_memmap_dataset&#xA;&#xA;# Update these paths to what you want:&#xA;data_order_file_path = cached_path(&#34;https://olmo-checkpoints.org/ai2-llm/olmo-medium/wvc30anm/train_data/global_indices.npy&#34;)&#xA;train_config_path = &#34;configs/official/OLMo-7B.yaml&#34;&#xA;&#xA;&#xA;cfg = TrainConfig.load(train_config_path)&#xA;dataset = build_memmap_dataset(cfg, cfg.data)&#xA;batch_size = cfg.global_train_batch_size&#xA;global_indices = np.memmap(data_order_file_path, mode=&#34;r+&#34;, dtype=np.uint32)&#xA;&#xA;&#xA;def get_batch_instances(batch_idx: int) -&amp;gt; list[list[int]]:&#xA;    batch_start = batch_idx * batch_size&#xA;    batch_end = (batch_idx + 1) * batch_size&#xA;    batch_indices = global_indices[batch_start:batch_end]&#xA;    batch_instances = []&#xA;    for index in batch_indices:&#xA;        token_ids = dataset[index][&#34;input_ids&#34;].tolist()&#xA;        batch_instances.append(token_ids)&#xA;    return batch_instances&#xA;&#xA;&#xA;# Get all 2048 x 2048 token IDs in the first batch.&#xA;get_batch_instances(0)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;p&gt;To fine-tune an OLMo model using our trainer you&#39;ll first need to prepare your dataset by tokenizing it and saving the tokens IDs to a flat numpy memory-mapped array. See &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo/main/scripts/prepare_tulu_data.py&#34;&gt;&lt;code&gt;scripts/prepare_tulu_data.py&lt;/code&gt;&lt;/a&gt; for an example with the Tulu V2 dataset, which can be easily modified for other datasets.&lt;/p&gt; &#xA;&lt;p&gt;Next, prepare your training config. There are many examples in the &lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/configs&#34;&gt;&lt;code&gt;configs/&lt;/code&gt;&lt;/a&gt; directory that you can use as a starting point. The most important thing is to make sure the model parameters (the &lt;code&gt;model&lt;/code&gt; field in the config) match up with the checkpoint you&#39;re starting from. To be safe you can always start from the config that comes with the model checkpoint. At a minimum you&#39;ll need to make the following changes to the config or provide the corresponding overrides from the command line:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Update &lt;code&gt;load_path&lt;/code&gt; to point to the checkpoint you want to start from.&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;reset_trainer_state&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Update &lt;code&gt;data.paths&lt;/code&gt; to point to the &lt;code&gt;token_ids.npy&lt;/code&gt; file you generated.&lt;/li&gt; &#xA; &lt;li&gt;Optionally update &lt;code&gt;data.label_mask_paths&lt;/code&gt; to point to the &lt;code&gt;label_mask.npy&lt;/code&gt; file you generated, unless you don&#39;t need special masking for the loss.&lt;/li&gt; &#xA; &lt;li&gt;Update &lt;code&gt;evaluators&lt;/code&gt; to add/remove in-loop evaluations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once you&#39;re satisfied with your training config, you can launch the training job via &lt;code&gt;torchrun&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node=8 scripts/train.py {path_to_train_config} \&#xA;    --data.paths=[{path_to_data}/input_ids.npy] \&#xA;    --data.label_mask_paths=[{path_to_data}/label_mask.npy] \&#xA;    --load_path={path_to_checkpoint} \&#xA;    --reset_trainer_state&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: passing CLI overrides like &lt;code&gt;--reset_trainer_state&lt;/code&gt; is only necessary if you didn&#39;t update those fields in your config.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Additional tools for evaluating OLMo models are available at the &lt;a href=&#34;https://github.com/allenai/ai2-olmo-eval&#34;&gt;OLMo Eval&lt;/a&gt; repo.&lt;/p&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{OLMo,&#xA;  title={OLMo: Accelerating the Science of Language Models},&#xA;  author={Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and A. Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Daniel Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hanna Hajishirzi},&#xA;  year={2024},&#xA;  url={https://api.semanticscholar.org/CorpusID:267365485},&#xA;  journal={arXiv preprint},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>langchain-ai/opengpts</title>
    <updated>2024-02-11T01:41:03Z</updated>
    <id>tag:github.com,2024-02-11:/langchain-ai/opengpts</id>
    <link href="https://github.com/langchain-ai/opengpts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenGPTs&lt;/h1&gt; &#xA;&lt;p&gt;This is an open source effort to create a similar experience to OpenAI&#39;s GPTs and Assistants API. It is powered by &lt;a href=&#34;https://github.com/langchain-ai/langgraph&#34;&gt;LangGraph&lt;/a&gt; - a framework for creating agent runtimes. It also builds upon &lt;a href=&#34;https://github.com/langchain-ai/langchain&#34;&gt;LangChain&lt;/a&gt;, &lt;a href=&#34;https://github.com/langchain-ai/langserve&#34;&gt;LangServe&lt;/a&gt; and &lt;a href=&#34;https://smith.langchain.com/&#34;&gt;LangSmith&lt;/a&gt;. OpenGPTs gives you more control, allowing you to configure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The LLM you use (choose between the 60+ that LangChain offers)&lt;/li&gt; &#xA; &lt;li&gt;The prompts you use (use LangSmith to debug those)&lt;/li&gt; &#xA; &lt;li&gt;The tools you give it (choose from LangChain&#39;s 100+ tools, or easily write your own)&lt;/li&gt; &#xA; &lt;li&gt;The vector database you use (choose from LangChain&#39;s 60+ vector database integrations)&lt;/li&gt; &#xA; &lt;li&gt;The retrieval algorithm you use&lt;/li&gt; &#xA; &lt;li&gt;The chat history database you use&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Most importantly, it gives you full control over the &lt;strong&gt;cognitive architecture&lt;/strong&gt; of your application. Currently, there are three different architectures implemented:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Assistant&lt;/li&gt; &#xA; &lt;li&gt;RAG&lt;/li&gt; &#xA; &lt;li&gt;Chatbot&lt;/li&gt; &#xA; &lt;li&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See below for more details on those. Because this is open source, if you do not like those architectures or want to modify them, you can easily do that!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Configure&#34; src=&#34;https://raw.githubusercontent.com/langchain-ai/opengpts/main/_static/configure.png&#34; width=&#34;49%&#34;&gt; &lt;img alt=&#34;Chat&#34; src=&#34;https://raw.githubusercontent.com/langchain-ai/opengpts/main/_static/chat.png&#34; width=&#34;49%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Key Links&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opengpts-example-vz4y4ooboq-uc.a.run.app/&#34;&gt;GPTs: a simple hosted version&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/langchain-ai/opengpts/main/API.md&#34;&gt;Assistants API: a getting started guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Start the backend&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install requirements&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd backend&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Set up persistence layer&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The backend by default uses Redis for saving agent configurations and chat message history. In order to you use this, you need to a &lt;code&gt;REDIS_URL&lt;/code&gt; variable.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export REDIS_URL=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Set up vector database&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The backend by default also uses Redis as a vector database, although you can easily switch this out to use any of the 50+ vector databases in LangChain. If you are using Redis as a vectorstore, the above environment variable should work (assuming you&#39;ve enabled &lt;code&gt;redissearch&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Set up language models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;By default, this uses OpenAI, but there are also options for Azure OpenAI and Anthropic. If you are using those, you may need to set different environment variables.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export OPENAI_API_KEY=&#34;sk-...&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other language models can be used, and in order to use them you will need to set more environment variables. See the section below on &lt;code&gt;LLMs&lt;/code&gt; for how to configure Azure OpenAI, Anthropic, and Amazon Bedrock.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Set up tools&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;By default this uses a lot of tools. Some of these require additional environment variables. You do not need to use any of these tools, and the environment variables are not required to spin up the app (they are only required if that tool is called).&lt;/p&gt; &#xA;&lt;p&gt;For a full list of environment variables to enable, see the &lt;code&gt;Tools&lt;/code&gt; section below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Set up monitoring&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Set up &lt;a href=&#34;https://smith.langchain.com/&#34;&gt;LangSmith&lt;/a&gt;. This is optional, but it will help with debugging, logging, monitoring. Sign up at the link above and then set the relevant environment variables&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export LANGCHAIN_TRACING_V2=&#34;true&#34;&#xA;export LANGCHAIN_API_KEY=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start the backend server&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;langchain serve --port=8100&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Start the frontend&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd frontend&#xA;yarn&#xA;yarn dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Navigate to &lt;a href=&#34;http://localhost:5173/&#34;&gt;http://localhost:5173/&lt;/a&gt; and enjoy!&lt;/p&gt; &#xA;&lt;h2&gt;Installation and Running with Docker&lt;/h2&gt; &#xA;&lt;p&gt;This project supports a Docker-based setup, streamlining installation and execution. It automatically builds images for the frontend and backend and sets up Redis using docker-compose.&lt;/p&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the Repository:&lt;/strong&gt;&lt;br&gt; Obtain the project files by cloning the repository.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/langchain-ai/opengpts.git&#xA;cd opengpts&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run with Docker Compose:&lt;/strong&gt;&lt;br&gt; In the root directory of the project, execute:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command builds the Docker images for the frontend and backend from their respective Dockerfiles and starts all necessary services, including Redis.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Access the Application:&lt;/strong&gt;&lt;br&gt; With the services running, access the frontend at &lt;a href=&#34;http://localhost:5173&#34;&gt;http://localhost:5173&lt;/a&gt;, substituting &lt;code&gt;5173&lt;/code&gt; with the designated port number.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rebuilding After Changes:&lt;/strong&gt;&lt;br&gt; If you make changes to either the frontend or backend, rebuild the Docker images to reflect these changes. Run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker-compose up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command rebuilds the images with your latest changes and restarts the services.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Note&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ensure Docker and docker-compose are installed on your system.&lt;/li&gt; &#xA; &lt;li&gt;Adjust the &lt;code&gt;.env&lt;/code&gt; file as required for specific environment configurations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;As much as possible, we are striving for feature parity with OpenAI.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Sandbox - Provides an environment to import, test, and modify existing chatbots. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The chatbots used are all in code, so are easily editable&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Custom Actions - Define additional functionality for your chatbot using OpenAPI specifications &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Supported by adding tools&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Knowledge Files - attach additional files that your chatbot can reference &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Upload files from the UI or API, used by Retrieval tool&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Tools - Provides basic tools for web browsing, image creation, etc. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Basic DuckDuckGo and PythonREPL tools enabled by default&lt;/li&gt; &#xA;   &lt;li&gt;Image creation coming soon&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Analytics - View and analyze chatbot usage data &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use LangSmith for this&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Drafts - Save and share drafts of chatbots you&#39;re creating &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Supports saving of configurations&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Publishing - publicly distribute your completed chatbot &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Can do by deploying via LangServe&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Sharing - Set up and manage chatbot sharing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Can do by deploying via LangServe&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Marketplace - Search and deploy chatbots created by other users &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Coming soon&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Repo Structure&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;frontend&lt;/code&gt;: Code for the frontend&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;backend&lt;/code&gt;: Code for the backend &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;app&lt;/code&gt;: LangServe code (for exposing APIs)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;packages&lt;/code&gt;: Core logic &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;agent-executor&lt;/code&gt;: Runtime for the agent&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;gizmo-agent&lt;/code&gt;: Configuration for the agent&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Customization&lt;/h2&gt; &#xA;&lt;p&gt;The big appeal of OpenGPTs as compared to using OpenAI directly is that it is more customizable. Specifically, you can choose which language models to use as well as more easily add custom tools. You can also use the underlying APIs directly and build a custom UI yourself should you choose.&lt;/p&gt; &#xA;&lt;h3&gt;Cognitive Architecture&lt;/h3&gt; &#xA;&lt;p&gt;This refers to the logic of how the GPT works. There are currently three different architectures supported, but because they are all written in LangGraph, it is very easy to modify them or add your own.&lt;/p&gt; &#xA;&lt;p&gt;The three different architectures supported are assistants, RAG, and chatbots.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Assistants&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Assistants can be equipped with arbitrary amount of tools and use an LLM to decide when to use them. This makes them the most flexible choice, but they work well with fewer models and can be less reliable.&lt;/p&gt; &#xA;&lt;p&gt;When creating an assistant, you specify a few things.&lt;/p&gt; &#xA;&lt;p&gt;First, you choose the language model to use. Only a few language models can be used reliably well: GPT-3.5, GPT-4, Claude, and Gemini.&lt;/p&gt; &#xA;&lt;p&gt;Second, you choose the tools to use. These can be predefined tools OR a retriever constructed from uploaded files. You can choose however many you want.&lt;/p&gt; &#xA;&lt;p&gt;The cognitive architecture can then be thought of as a loop. First, the LLM is called to determine what (if any) actions to take. If it decides to take actions, then those actions are executed and it loops back. If no actions are decided to take, then the response of the LLM is the final response, and it finishes the loop.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/langchain-ai/opengpts/main/_static/agent.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This can be a really powerful and flexible architecture. This is probably closest to how us humans operate. However, these also can be not super reliable, and generally only work with the more performant models (and even then they can mess up). Therefore, we introduced a few simpler architecures.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RAGBot&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;One of the big use cases of the GPT store is uploading files and giving the bot knowledge of those files. What would it mean to make an architecture more focused on that use case?&lt;/p&gt; &#xA;&lt;p&gt;We added RAGBot - a retrieval-focused GPT with a straightforward architecture. First, a set of documents are retrieved. Then, those documents are passed in the system message to a separate call to the language model so it can respond.&lt;/p&gt; &#xA;&lt;p&gt;Compared to assistants, it is more structured (but less powerful). It ALWAYS looks up something - which is good if you know you want to look things up, but potentially wasteful if the user is just trying to have a normal conversation. Also importantly, this only looks up things once - so if it doesn’t find the right results then it will yield a bad result (compared to an assistant, which could decide to look things up again).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/langchain-ai/opengpts/main/_static/rag.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Despite this being a more simple architecture, it is good for a few reasons. First, because it is simpler it can work pretty well with a wider variety of models (including lots of open source models). Second, if you have a use case where you don’t NEED the flexibility of an assistant (eg you know users will be looking up information every time) then it can be more focused. And third, compared to the final architecture below it can use external knowledge.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ChatBot&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The final architecture is dead simple - just a call to a language model, parameterized by a system message. This allows the GPT to take on different personas and characters. This is clearly far less powerful than Assistants or RAGBots (which have access to external sources of data/computation) - but it’s still valuable! A lot of popular GPTs are just system messages at the end of the day, and CharacterAI is crushing it despite largely just being system messages as well.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/langchain-ai/opengpts/main/_static/chatbot.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;LLMs&lt;/h3&gt; &#xA;&lt;p&gt;You can choose between different LLMs to use. This takes advantage of LangChain&#39;s many integrations. It is important to note that depending on which LLM you use, you may need to change how you are prompting it.&lt;/p&gt; &#xA;&lt;p&gt;We have exposed four agent types by default:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;GPT 3.5 Turbo&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;GPT 4&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Azure OpenAI&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Claude 2&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We will work to add more when we have confidence they can work well.&lt;/p&gt; &#xA;&lt;p&gt;If you want to add your own LLM or agent configuration, or want to edit the existing ones, you can find them in &lt;code&gt;backend/packages/gizmo-agent/gizmo_agent/agent_types&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Claude 2&lt;/h4&gt; &#xA;&lt;p&gt;If using Claude 2, you will need to set the following environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export ANTHROPIC_API_KEY=sk-...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Azure OpenAI&lt;/h4&gt; &#xA;&lt;p&gt;If using Azure OpenAI, you will need to set the following environment variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export AZURE_OPENAI_API_BASE=...&#xA;export AZURE_OPENAI_API_VERSION=...&#xA;export AZURE_OPENAI_API_KEY=...&#xA;export AZURE_OPENAI_DEPLOYMENT_NAME=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Amazon Bedrock&lt;/h4&gt; &#xA;&lt;p&gt;If using Amazon Bedrock, you either have valid credentials in &lt;code&gt;~/.aws/credentials&lt;/code&gt; or set the following environment variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export AWS_ACCESS_KEY_ID=...&#xA;export AWS_SECRET_ACCESS_KEY=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tools&lt;/h3&gt; &#xA;&lt;p&gt;One of the big benefits of having this be open source is that you can more easily add tools (directly in Python).&lt;/p&gt; &#xA;&lt;p&gt;In practice, most teams we see define their own tools. This is easy to do within LangChain. See &lt;a href=&#34;https://python.langchain.com/docs/modules/agents/tools/custom_tools&#34;&gt;this guide&lt;/a&gt; for details on how to best do this.&lt;/p&gt; &#xA;&lt;p&gt;If you want to use some preconfigured tools, these include:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Robocorp Action Server&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run AI Python based actions with &lt;a href=&#34;https://github.com/robocorp/robocorp&#34;&gt;Robocorp Action Server&lt;/a&gt;. Does not require a service API key, but it requires the credentials for a running Action Server instance to be defined:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ROBOCORP_ACTION_SERVER_URL=https://dummy-action-server.robocorp.link&#xA;ROBOCORP_ACTION_SERVER_KEY=dummy-api-key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Connery Actions&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Connect OpenGPTs to the real world with &lt;a href=&#34;https://github.com/connery-io/connery&#34;&gt;Connery&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Requires setting an environment variable, which you get during the &lt;a href=&#34;https://docs.connery.io/docs/runner/quick-start/&#34;&gt;Connery Runner setup&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CONNERY_RUNNER_URL=https://your-personal-connery-runner-url&#xA;CONNERY_RUNNER_API_KEY=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;DuckDuckGo Search&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Search the web with &lt;a href=&#34;https://pypi.org/project/duckduckgo-search/&#34;&gt;DuckDuckGo&lt;/a&gt;. Does not require any API keys.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tavily Search&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Uses the &lt;a href=&#34;https://app.tavily.com/&#34;&gt;Tavily&lt;/a&gt; search engine. Requires setting an environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export TAVILY_API_KEY=tvly-...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sign up for an API key &lt;a href=&#34;https://app.tavily.com/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tavily Search (Answer Only)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Uses the &lt;a href=&#34;https://app.tavily.com/&#34;&gt;Tavily&lt;/a&gt; search engine. This returns only the answer, no supporting evidence. Good when you need a short response (small context windows). Requires setting an environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export TAVILY_API_KEY=tvly-...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sign up for an API key &lt;a href=&#34;https://app.tavily.com/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;You.com Search&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Uses &lt;a href=&#34;https://you.com/&#34;&gt;You.com&lt;/a&gt; search, optimized responses for LLMs. Requires setting an environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export YDC_API_KEY=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sign up for an API key &lt;a href=&#34;https://you.com/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SEC Filings (Kay.ai)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Searches through SEC filings using &lt;a href=&#34;https://www.kay.ai/&#34;&gt;Kay.ai&lt;/a&gt;. Requires setting an environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export KAY_API_KEY=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sign up for an API key &lt;a href=&#34;https://www.kay.ai/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Press Releases (Kay.ai)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Searches through press releases using &lt;a href=&#34;https://www.kay.ai/&#34;&gt;Kay.ai&lt;/a&gt;. Requires setting an environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export KAY_API_KEY=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sign up for an API key &lt;a href=&#34;https://www.kay.ai/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Arxiv&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Searches &lt;a href=&#34;https://arxiv.org/&#34;&gt;Arxiv&lt;/a&gt;. Does not require any API keys.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PubMed&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Searches &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/&#34;&gt;PubMed&lt;/a&gt;. Does not require any API keys.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Wikipedia&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Searches &lt;a href=&#34;https://pypi.org/project/wikipedia/&#34;&gt;Wikipedia&lt;/a&gt;. Does not require any API keys.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;h3&gt;Deploy via Cloud Run&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Build the frontend&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd frontend&#xA;yarn&#xA;yarn build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Deploy to Google Cloud Run&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can deploy to GCP Cloud Run using the following command:&lt;/p&gt; &#xA;&lt;p&gt;First create a &lt;code&gt;.env.gcp.yaml&lt;/code&gt; file with the contents from &lt;code&gt;.env.gcp.yaml.example&lt;/code&gt; and fill in the values. Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;gcloud run deploy opengpts --source . --port 8000 --env-vars-file .env.gcp.yaml --allow-unauthenticated --region us-central1 --min-instances 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deploy in Kubernetes&lt;/h3&gt; &#xA;&lt;p&gt;We have a Helm chart for deploying the backend to Kubernetes. You can find more information here: &lt;a href=&#34;https://github.com/langchain-ai/helm/tree/main/charts/open-gpts&#34;&gt;README.md&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>