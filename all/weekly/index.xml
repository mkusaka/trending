<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-04T02:01:43Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lucidrains/imagen-pytorch</title>
    <updated>2022-06-04T02:01:43Z</updated>
    <id>tag:github.com,2022-06-04:/lucidrains/imagen-pytorch</id>
    <link href="https://github.com/lucidrains/imagen-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Imagen, Google&#39;s Text-to-Image Neural Network, in Pytorch&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/imagen-pytorch/main/imagen.png&#34; width=&#34;450px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Imagen - Pytorch (wip)&lt;/h2&gt; &#xA;&lt;p&gt;Implementation of &lt;a href=&#34;https://gweb-research-imagen.appspot.com/&#34;&gt;Imagen&lt;/a&gt;, Google&#39;s Text-to-Image Neural Network that beats DALL-E2, in Pytorch. It is the new SOTA for text-to-image synthesis.&lt;/p&gt; &#xA;&lt;p&gt;Architecturally, it is actually much simpler than DALL-E2. It consists of a cascading DDPM conditioned on text embeddings from a large pretrained T5 model (attention network). It also contains dynamic clipping for improved classifier free guidance, noise level conditioning, and a memory efficient unet design.&lt;/p&gt; &#xA;&lt;p&gt;It appears neither CLIP nor prior network is needed after all. And so research continues.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xqDeAz0U-R4&#34;&gt;AI Coffee Break with Letitia&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please join &lt;a href=&#34;https://discord.gg/xBPBXfcFHd&#34;&gt;&lt;img alt=&#34;Join us on Discord&#34; src=&#34;https://img.shields.io/discord/823813159592001537?color=5865F2&amp;amp;logo=discord&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt; if you are interested in helping out with the replication with the &lt;a href=&#34;https://laion.ai/&#34;&gt;LAION&lt;/a&gt; community&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install imagen-pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from imagen_pytorch import Unet, Imagen&#xA;&#xA;# unet for imagen&#xA;&#xA;unet1 = Unet(&#xA;    dim = 32,&#xA;    cond_dim = 512,&#xA;    dim_mults = (1, 2, 4, 8),&#xA;    num_resnet_blocks = 3,&#xA;    layer_attns = (False, True, True, True),&#xA;    layer_cross_attns = (False, True, True, True)&#xA;)&#xA;&#xA;unet2 = Unet(&#xA;    dim = 32,&#xA;    cond_dim = 512,&#xA;    dim_mults = (1, 2, 4, 8),&#xA;    num_resnet_blocks = (2, 4, 8, 8),&#xA;    layer_attns = (False, False, False, True),&#xA;    layer_cross_attns = (False, False, False, True)&#xA;)&#xA;&#xA;# imagen, which contains the unets above (base unet and super resoluting ones)&#xA;&#xA;imagen = Imagen(&#xA;    unets = (unet1, unet2),&#xA;    image_sizes = (64, 256),&#xA;    beta_schedules = (&#39;cosine&#39;, &#39;linear&#39;),&#xA;    timesteps = 1000,&#xA;    cond_drop_prob = 0.5&#xA;).cuda()&#xA;&#xA;# mock images (get a lot of this) and text encodings from large T5&#xA;&#xA;text_embeds = torch.randn(4, 256, 768).cuda()&#xA;text_masks = torch.ones(4, 256).bool().cuda()&#xA;images = torch.randn(4, 3, 256, 256).cuda()&#xA;&#xA;# feed images into imagen, training each unet in the cascade&#xA;&#xA;for i in (1, 2):&#xA;    loss = imagen(images, text_embeds = text_embeds, text_masks = text_masks, unet_number = i)&#xA;    loss.backward()&#xA;&#xA;# do the above for many many many many steps&#xA;# now you can sample an image based on the text embeddings from the cascading ddpm&#xA;&#xA;images = imagen.sample(texts = [&#xA;    &#39;a whale breaching from afar&#39;,&#xA;    &#39;young girl blowing out candles on her birthday cake&#39;,&#xA;    &#39;fireworks with blue and green sparkles&#39;&#xA;], cond_scale = 2.)&#xA;&#xA;images.shape # (3, 3, 256, 256)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With the &lt;code&gt;ImagenTrainer&lt;/code&gt; wrapper class, the exponential moving averages for all of the U-nets in the cascading DDPM will be automatically taken care of when calling &lt;code&gt;update&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from imagen_pytorch import Unet, Imagen, ImagenTrainer&#xA;&#xA;# unet for imagen&#xA;&#xA;unet1 = Unet(&#xA;    dim = 32,&#xA;    cond_dim = 512,&#xA;    dim_mults = (1, 2, 4, 8),&#xA;    num_resnet_blocks = 3,&#xA;    layer_attns = (False, True, True, True),&#xA;)&#xA;&#xA;unet2 = Unet(&#xA;    dim = 32,&#xA;    cond_dim = 512,&#xA;    dim_mults = (1, 2, 4, 8),&#xA;    num_resnet_blocks = (2, 4, 8, 8),&#xA;    layer_attns = (False, False, False, True),&#xA;    layer_cross_attns = (False, False, False, True)&#xA;)&#xA;&#xA;# imagen, which contains the unets above (base unet and super resoluting ones)&#xA;&#xA;imagen = Imagen(&#xA;    unets = (unet1, unet2),&#xA;    text_encoder_name = &#39;t5-large&#39;,&#xA;    image_sizes = (64, 256),&#xA;    beta_schedules = (&#39;cosine&#39;, &#39;linear&#39;),&#xA;    timesteps = 1000,&#xA;    cond_drop_prob = 0.5&#xA;).cuda()&#xA;&#xA;# wrap imagen with the trainer class&#xA;&#xA;trainer = ImagenTrainer(imagen)&#xA;&#xA;# mock images (get a lot of this) and text encodings from large T5&#xA;&#xA;text_embeds = torch.randn(64, 256, 1024).cuda()&#xA;text_masks = torch.ones(64, 256).bool().cuda()&#xA;images = torch.randn(64, 3, 256, 256).cuda()&#xA;&#xA;# feed images into imagen, training each unet in the cascade&#xA;&#xA;for i in (1, 2):&#xA;    loss = trainer(&#xA;        images,&#xA;        text_embeds = text_embeds,&#xA;        text_masks = text_masks,&#xA;        unet_number = i,&#xA;        max_batch_size = 4        # auto divide the batch of 64 up into batch size of 4 and accumulate gradients, so it all fits in memory&#xA;    )&#xA;&#xA;    trainer.update(unet_number = i)&#xA;&#xA;# do the above for many many many many steps&#xA;# now you can sample an image based on the text embeddings from the cascading ddpm&#xA;&#xA;images = trainer.sample(texts = [&#xA;    &#39;a puppy looking anxiously at a giant donut on the table&#39;,&#xA;    &#39;the milky way galaxy in the style of monet&#39;&#xA;], cond_scale = 2.)&#xA;&#xA;images.shape # (2, 3, 256, 256)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Shoutouts&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://stability.ai/&#34;&gt;StabilityAI&lt;/a&gt; for the generous sponsorship, as well as my other sponsors out there&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/&#34;&gt;ü§ó Huggingface&lt;/a&gt; for their amazing transformers library. The text encoder portion is pretty much taken care of because of them&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/jorgemcgomes&#34;&gt;Jorge Gomes&lt;/a&gt; for helping out with the T5 loading code and advice on the correct T5 version&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;Katherine Crowson&lt;/a&gt;, for her &lt;a href=&#34;https://github.com/crowsonkb/v-diffusion-jax/raw/master/diffusion/utils.py&#34;&gt;beautiful code&lt;/a&gt;, which helped me understand the continuous time version of gaussian diffusion&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You? It isn&#39;t done yet, chip in if you are a researcher or skilled ML engineer&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Todo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; use huggingface transformers for T5-small text embeddings&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add dynamic thresholding&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add dynamic thresholding DALLE2 and video-diffusion repository as well&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; allow for one to set T5-large (and perhaps small factory method to take in any huggingface transformer)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add the lowres noise level with the pseudocode in appendix, and figure out what is this sweep they do at inference time&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; port over some training code from DALLE2&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; need to be able to use a different noise schedule per unet (cosine was used for base, but linear for SR)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; just make one master-configurable unet&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; complete resnet block (biggan inspired? but with groupnorm) - complete self attention&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; complete conditioning embedding block (and make it completely configurable, whether it be attention, film etc)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; consider using perceiver-resampler from &lt;a href=&#34;https://github.com/lucidrains/flamingo-pytorch&#34;&gt;https://github.com/lucidrains/flamingo-pytorch&lt;/a&gt; in place of attention pooling&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add attention pooling option, in addition to cross attention and film&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add optional cosine decay schedule with warmup, for each unet, to trainer&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; figure out if learned variance was used at all, and remove it if it was inconsequential&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; switch to continuous timesteps instead of discretized, as it seems that is what they used for all stages - first figure out the linear noise schedule case from the variational ddpm paper &lt;a href=&#34;https://openreview.net/forum?id=2LdBqxc1Yv&#34;&gt;https://openreview.net/forum?id=2LdBqxc1Yv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; exercise efficient attention expertise + explore skip layer excitation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; try out grid attention&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Saharia2022PhotorealisticTD,&#xA;    title   = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},&#xA;    author  = {Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily L. Denton and Seyed Kamyar Seyed Ghasemipour and Burcu Karagol Ayan and Seyedeh Sara Mahdavi and Raphael Gontijo Lopes and Tim Salimans and Jonathan Ho and David Fleet and Mohammad Norouzi},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Tu2022MaxViTMV,&#xA;    title   = {MaxViT: Multi-Axis Vision Transformer},&#xA;    author  = {Zhengzhong Tu and Hossein Talebi and Han Zhang and Feng Yang and Peyman Milanfar and Alan Conrad Bovik and Yinxiao Li},&#xA;    year    = {2022},&#xA;    url     = {https://arxiv.org/abs/2204.01697}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Alayrac2022Flamingo,&#xA;    title   = {Flamingo: a Visual Language Model for Few-Shot Learning},&#xA;    author  = {Jean-Baptiste Alayrac et al},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>facebook/flipper</title>
    <updated>2022-06-04T02:01:43Z</updated>
    <id>tag:github.com,2022-06-04:/facebook/flipper</id>
    <link href="https://github.com/facebook/flipper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A desktop debugging platform for mobile developers.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://fbflipper.com/img/icon.png&#34; alt=&#34;logo&#34; width=&#34;20%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; Flipper &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://search.maven.org/artifact/com.facebook.flipper/flipper&#34;&gt; &lt;img src=&#34;https://img.shields.io/maven-central/v/com.facebook.flipper/flipper&#34; alt=&#34;Android Maven Badge&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://cocoapods.org/pods/Flipper&#34;&gt; &lt;img src=&#34;https://img.shields.io/cocoapods/v/FlipperKit.svg?label=iOS&amp;amp;color=blue&#34; alt=&#34;iOS&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Flipper (formerly Sonar) is a platform for debugging mobile apps on iOS and Android and, recently, even JS apps in your browser or in Node.js. Visualize, inspect, and control your apps from a simple desktop interface. Use Flipper as is or extend it using the plugin API. &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebook/flipper/main/website/static/img/layout.png&#34; alt=&#34;Flipper&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#in-this-repo&#34;&gt;In this repo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#getting-started&#34;&gt;Getting started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#building-from-source&#34;&gt;Building from Source&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#desktop&#34;&gt;Desktop&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#running-from-source&#34;&gt;Running from source&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#building-standalone-application&#34;&gt;Building standalone application&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#ios-sdk--sample-app&#34;&gt;iOS SDK + Sample App&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#android-sdk--sample-app&#34;&gt;Android SDK + Sample app&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#react-native-sdk--sample-app&#34;&gt;React Native SDK + Sample app&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#js-sdk--sample-react-app&#34;&gt;JS SDK + Sample React app&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#troubleshooting&#34;&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#documentation&#34;&gt;Documentation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Mobile development&lt;/h2&gt; &#xA;&lt;p&gt;Flipper aims to be your number one companion for mobile app development on iOS and Android. Therefore, we provide a bunch of useful tools, including a log viewer, interactive layout inspector, and network inspector.&lt;/p&gt; &#xA;&lt;h2&gt;Extending Flipper&lt;/h2&gt; &#xA;&lt;p&gt;Flipper is built as a platform. In addition to using the tools already included, you can create your own plugins to visualize and debug data from your mobile apps. Flipper takes care of sending data back and forth, calling functions, and listening for events on the mobile app.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing to Flipper&lt;/h2&gt; &#xA;&lt;p&gt;Both Flipper&#39;s desktop app, native mobile SDKs, JS SDKs are open-source and MIT licensed. This enables you to see and understand how we are building plugins, and of course, join the community and help to improve Flipper. We are excited to see what you will build on this platform.&lt;/p&gt; &#xA;&lt;h1&gt;In this repo&lt;/h1&gt; &#xA;&lt;p&gt;This repository includes all parts of Flipper. This includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Flipper&#39;s desktop app built using &lt;a href=&#34;https://electronjs.org&#34;&gt;Electron&lt;/a&gt; (&lt;code&gt;/desktop&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;native Flipper SDKs for iOS (&lt;code&gt;/iOS&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;native Flipper SDKs for Android (&lt;code&gt;/android&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;React Native Flipper SDK (&lt;code&gt;/react-native&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;JS Flipper SDK (&lt;code&gt;/js&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Plugins: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Logs (&lt;code&gt;/desktop/plugins/public/logs&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Layout inspector (&lt;code&gt;/desktop/plugins/public/layout&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Network inspector (&lt;code&gt;/desktop/plugins/public/network&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Shared Preferences/NSUserDefaults inspector (&lt;code&gt;/desktop/plugins/public/shared_preferences&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;website and documentation (&lt;code&gt;/website&lt;/code&gt; / &lt;code&gt;/docs&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting started&lt;/h1&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://fbflipper.com/docs/getting-started/index&#34;&gt;Getting Started guide&lt;/a&gt; to set up Flipper. Or, (still experimental) run &lt;code&gt;npx flipper-server&lt;/code&gt; for a browser based version of Flipper.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;node &amp;gt;= 8&lt;/li&gt; &#xA; &lt;li&gt;yarn &amp;gt;= 1.5&lt;/li&gt; &#xA; &lt;li&gt;iOS developer tools (for developing iOS plugins)&lt;/li&gt; &#xA; &lt;li&gt;Android SDK and adb&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Building from Source&lt;/h1&gt; &#xA;&lt;h2&gt;Desktop&lt;/h2&gt; &#xA;&lt;h3&gt;Running from source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/facebook/flipper.git&#xA;cd flipper/desktop&#xA;yarn&#xA;yarn start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: If you&#39;re on Windows, you need to use Yarn 1.5.1 until &lt;a href=&#34;https://github.com/yarnpkg/yarn/issues/6048&#34;&gt;this issue&lt;/a&gt; is resolved.&lt;/p&gt; &#xA;&lt;h3&gt;Building standalone application&lt;/h3&gt; &#xA;&lt;p&gt;Provide either &lt;code&gt;--mac&lt;/code&gt;, &lt;code&gt;--win&lt;/code&gt;, &lt;code&gt;--linux&lt;/code&gt; or any combination of them to &lt;code&gt;yarn build&lt;/code&gt; to build a release zip file for the given platform(s). E.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yarn build --mac --version $buildNumber&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find the resulting artifact in the &lt;code&gt;dist/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;iOS SDK + Sample App&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd iOS/Sample&#xA;rm -f Podfile.lock&#xA;pod install --repo-update&#xA;open Sample.xcworkspace&#xA;&amp;lt;Run app from xcode&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can omit &lt;code&gt;--repo-update&lt;/code&gt; to speed up the installation, but watch out as you may be building against outdated dependencies.&lt;/p&gt; &#xA;&lt;h2&gt;Android SDK + Sample app&lt;/h2&gt; &#xA;&lt;p&gt;Start up an android emulator and run the following in the project root:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./gradlew :sample:installDebug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;React Native SDK + Sample app&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd react-native/ReactNativeFlipperExample&#xA;yarn&#xA;yarn android&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the first 2 steps need to be done only once.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, the app can be started on &lt;code&gt;iOS&lt;/code&gt; by running &lt;code&gt;yarn ios&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If this is the first time running, you will also need to run &lt;code&gt;pod install --repo-update&lt;/code&gt; from the &lt;code&gt;react-native/ReactNativeFlipperExample/ios&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;JS SDK + Sample React app&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd js/react-flipper-example&#xA;yarn&#xA;yarn start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Troubleshooting&lt;/h4&gt; &#xA;&lt;p&gt;Older yarn versions might show an error / hang with the message &#39;Waiting for the other yarn instance to finish&#39;. If that happens, run the command &lt;code&gt;yarn&lt;/code&gt; first separately in the directory &lt;code&gt;react-native/react-native-flipper&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;Find the full documentation for this project at &lt;a href=&#34;https://fbflipper.com/&#34;&gt;fbflipper.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Our documentation is built with &lt;a href=&#34;https://docusaurus.io/&#34;&gt;Docusaurus&lt;/a&gt;. You can build it locally by running this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd website&#xA;yarn&#xA;yarn start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; file for how to help out.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Flipper is MIT licensed, as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebook/flipper/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>phyver/GameShell</title>
    <updated>2022-06-04T02:01:43Z</updated>
    <id>tag:github.com,2022-06-04:/phyver/GameShell</id>
    <link href="https://github.com/phyver/GameShell" rel="alternate"></link>
    <summary type="html">&lt;p&gt;a game to learn (or teach) how to use standard commands in a Unix shell&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GameShell: a &#34;game&#34; to teach the Unix shell&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phyver/GameShell/master/Images/illustration-small.png&#34; alt=&#34;Illustration inspired by the game&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Teaching first-year university students or high schoolers to use a Unix shell is not always the easiest or most entertaining of tasks. GameShell was devised as a tool to help students at the &lt;a href=&#34;https://univ-smb.fr&#34;&gt;Universit√© Savoie Mont Blanc&lt;/a&gt; to engage with a &lt;em&gt;real&lt;/em&gt; shell, in a way that encourages learning while also having fun.&lt;/p&gt; &#xA;&lt;p&gt;The original idea, due to Rodolphe Lepigre, was to run a standard bash session with an appropriate configuration file that defined &#34;missions&#34; which would be &#34;checked&#34; in order to progress through the game.&lt;/p&gt; &#xA;&lt;p&gt;Here is the result...&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/phyver/GameShell/master/Images/gameshell_first_mission_small.gif&#34; alt=&#34;GameShell&#39;s first mission&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Feel free to send us your remarks, questions or suggestions by opening &lt;a href=&#34;https://github.com/phyver/GameShell/issues&#34;&gt;issues&lt;/a&gt; or submitting &lt;a href=&#34;https://github.com/phyver/GameShell/pulls&#34;&gt;pull requests&lt;/a&gt;. We are particularly interested in any new missions you might create!&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; GameShell is currently undergoing heavy development: the current version has not been field tested by students. Do not hesitate to report any problems you might encounter or suggestions you might have by &lt;a href=&#34;https://github.com/phyver/GameShell/issues/new&#34;&gt;opening an issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;GameShell should work on any standard Linux system, and also on macOS and BSD (but we have run fewer tests on the latter systems). On Debian or Ubuntu, the only dependencies (besides &lt;code&gt;bash&lt;/code&gt;) are the &lt;code&gt;gettext-base&lt;/code&gt; and &lt;code&gt;awk&lt;/code&gt; packages (the latter is generally installed by default). Some missions have additional dependencies: these missions will be skipped if the dependencies are not met. On Debian or Ubuntu, run the following command to install all game and mission dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ sudo apt install gettext man-db procps psmisc nano tree bsdmainutils x11-apps wget&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://raw.githubusercontent.com/phyver/GameShell/master/doc/user_manual.md&#34;&gt;user manual&lt;/a&gt; to see how to install the game dependencies on other systems (macOS, BSD, ...).&lt;/p&gt; &#xA;&lt;p&gt;Assuming all the dependencies are installed, you can try the latest version of the game by running the following two commands in a terminal.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ wget https://github.com/phyver/GameShell/releases/download/latest/gameshell.sh&#xA;$ bash gameshell.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first command will download the latest version of the game in the form of a self-extracting archive, and the second command will initialise and run the game from the downloaded archive. Instructions on how to play are provided in the game directly.&lt;/p&gt; &#xA;&lt;p&gt;Note that when you quit the game (with &lt;code&gt;control-d&lt;/code&gt; or the command &lt;code&gt;gsh exit&lt;/code&gt;) your progression will be saved in a new archive (called &lt;code&gt;GameShell-save.sh&lt;/code&gt;). Run this archive to resume the game where you left it.&lt;/p&gt; &#xA;&lt;p&gt;If you prefer not running foreign shell scripts on your computer, you can generate a Docker image with the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ mkdir GameShell; cd GameShell&#xA;$ wget --quiet https://github.com/phyver/GameShell/releases/download/latest/Dockerfile&#xA;$ docker build -t gsh .&#xA;$ docker run -it gsh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The game will NOT be saved when you exit, and additional flags are required if you want to run X programs from inside GameShell. Refer to &lt;a href=&#34;https://raw.githubusercontent.com/phyver/GameShell/master/doc/deps.md#running-GameShell-from-a-docker-container&#34;&gt;this section&lt;/a&gt; of the user manual.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;To find out more about GameShell, refer to the following documents:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/phyver/GameShell/master/doc/user_manual.md&#34;&gt;user manual&lt;/a&gt; provides information on how to run the game on all supported platforms (Linux, macOS, BSD), explains how to run the game from the sources, tells you how to generate custom game archives (which is useful if you want to use GameShell for teaching a class), and more.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/phyver/GameShell/master/doc/dev_manual.md&#34;&gt;developer manual&lt;/a&gt; provides information on how to create new missions, how to translate missions, and how to participate in the development of the game.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Who is developing GameShell?&lt;/h2&gt; &#xA;&lt;h3&gt;Developers&lt;/h3&gt; &#xA;&lt;p&gt;The game is currently being developed by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.lama.univ-smb.fr/~hyvernat&#34;&gt;Pierre Hyvernat&lt;/a&gt; (main developer, &lt;a href=&#34;mailto:pierre.hyvernat@univ-smb.fr&#34;&gt;pierre.hyvernat@univ-smb.fr&lt;/a&gt;),&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lepigre.fr&#34;&gt;Rodolphe Lepigre&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Mission contributors&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pierre Hyvernat&lt;/li&gt; &#xA; &lt;li&gt;Rodolphe Lepigre&lt;/li&gt; &#xA; &lt;li&gt;Christophe Raffalli&lt;/li&gt; &#xA; &lt;li&gt;Xavier Provencal&lt;/li&gt; &#xA; &lt;li&gt;Clovis Eberhart&lt;/li&gt; &#xA; &lt;li&gt;S√©bastien Tavenas&lt;/li&gt; &#xA; &lt;li&gt;Tiemen Duvillard&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Special thanks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All the students who found &lt;em&gt;many&lt;/em&gt; bugs in the early versions.&lt;/li&gt; &#xA; &lt;li&gt;Joan Stark (a.k.a, jgs), who designed hundreds of ASCII-art pieces in the late 90s. Most of the ASCII-art encountered in GameShell are due to her.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Licence&lt;/h2&gt; &#xA;&lt;p&gt;GameShell is released under the &lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.en.html&#34;&gt;GPLv3&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please link to this repository if you use GameShell.&lt;/p&gt; &#xA;&lt;p&gt;GameShell is open source and free to use. One way you can acknowledge the work it required is by sending an actual postcard to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  Pierre Hyvernat&#xA;  Laboratoire de Math√©matiques, CNRS UMR 5127&#xA;  Universit√© de Savoie&#xA;  73376 Le Bourget du Lac&#xA;  FRANCE&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>