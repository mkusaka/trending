<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-03T01:38:22Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bluesky-social/social-app</title>
    <updated>2024-11-03T01:38:22Z</updated>
    <id>tag:github.com,2024-11-03:/bluesky-social/social-app</id>
    <link href="https://github.com/bluesky-social/social-app" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Bluesky Social application for Web, iOS, and Android&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Bluesky Social App&lt;/h1&gt; &#xA;&lt;p&gt;Welcome friends! This is the codebase for the Bluesky Social app.&lt;/p&gt; &#xA;&lt;p&gt;Get the app itself:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Web: &lt;a href=&#34;https://bsky.app&#34;&gt;bsky.app&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;iOS: &lt;a href=&#34;https://apps.apple.com/us/app/bluesky-social/id6444370199&#34;&gt;App Store&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Android: &lt;a href=&#34;https://play.google.com/store/apps/details?id=xyz.blueskyweb.app&#34;&gt;Play Store&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development Resources&lt;/h2&gt; &#xA;&lt;p&gt;This is a &lt;a href=&#34;https://reactnative.dev/&#34;&gt;React Native&lt;/a&gt; application, written in the TypeScript programming language. It builds on the &lt;code&gt;atproto&lt;/code&gt; TypeScript packages (like &lt;a href=&#34;https://www.npmjs.com/package/@atproto/api&#34;&gt;&lt;code&gt;@atproto/api&lt;/code&gt;&lt;/a&gt;), code for which is also open source, but in &lt;a href=&#34;https://github.com/bluesky-social/atproto&#34;&gt;a different git repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There is a small amount of Go language source code (in &lt;code&gt;./bskyweb/&lt;/code&gt;), for a web service that returns the React Native Web application.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/bluesky-social/social-app/main/docs/build.md&#34;&gt;Build Instructions&lt;/a&gt; are a good place to get started with the app itself.&lt;/p&gt; &#xA;&lt;p&gt;The Authenticated Transfer Protocol (&#34;AT Protocol&#34; or &#34;atproto&#34;) is a decentralized social media protocol. You don&#39;t &lt;em&gt;need&lt;/em&gt; to understand AT Protocol to work with this application, but it can help. Learn more at:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://atproto.com/guides/overview&#34;&gt;Overview and Guides&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bluesky-social/atproto/discussions&#34;&gt;Github Discussions&lt;/a&gt; üëà Great place to ask questions&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://atproto.com/specs/atp&#34;&gt;Protocol Specifications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bsky.social/about/blog/3-6-2022-a-self-authenticating-social-protocol&#34;&gt;Blogpost on self-authenticating data structures&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The Bluesky Social application encompasses a set of schemas and APIs built in the overall AT Protocol framework. The namespace for these &#34;Lexicons&#34; is &lt;code&gt;app.bsky.*&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;While we do accept contributions, we prioritize high quality issues and pull requests. Adhering to the below guidelines will ensure a more timely review.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rules:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We may not respond to your issue or PR.&lt;/li&gt; &#xA; &lt;li&gt;We may close an issue or PR without much feedback.&lt;/li&gt; &#xA; &lt;li&gt;We may lock discussions or contributions if our attention is getting DDOSed.&lt;/li&gt; &#xA; &lt;li&gt;We&#39;re not going to provide support for build issues.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Guidelines:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check for existing issues before filing a new one please.&lt;/li&gt; &#xA; &lt;li&gt;Open an issue and give some time for discussion before submitting a PR.&lt;/li&gt; &#xA; &lt;li&gt;Stay away from PRs like... &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Changing &#34;Post&#34; to &#34;Skeet.&#34;&lt;/li&gt; &#xA;   &lt;li&gt;Refactoring the codebase, e.g., to replace MobX with Redux or something.&lt;/li&gt; &#xA;   &lt;li&gt;Adding entirely new features without prior discussion.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Remember, we serve a wide community of users. Our day-to-day involves us constantly asking &#34;which top priority is our top priority.&#34; If you submit well-written PRs that solve problems concisely, that&#39;s an awesome contribution. Otherwise, as much as we&#39;d love to accept your ideas and contributions, we really don&#39;t have the bandwidth. That&#39;s what forking is for!&lt;/p&gt; &#xA;&lt;h2&gt;Forking guidelines&lt;/h2&gt; &#xA;&lt;p&gt;You have our blessing ü™Ñ‚ú® to fork this application! However, it&#39;s very important to be clear to users when you&#39;re giving them a fork.&lt;/p&gt; &#xA;&lt;p&gt;Please be sure to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change all branding in the repository and UI to clearly differentiate from Bluesky.&lt;/li&gt; &#xA; &lt;li&gt;Change any support links (feedback, email, terms of service, etc) to your own systems.&lt;/li&gt; &#xA; &lt;li&gt;Replace any analytics or error-collection systems with your own so we don&#39;t get super confused.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Security disclosures&lt;/h2&gt; &#xA;&lt;p&gt;If you discover any security issues, please send an email to &lt;a href=&#34;mailto:security@bsky.app&#34;&gt;security@bsky.app&lt;/a&gt;. The email is automatically CCed to the entire team and we&#39;ll respond promptly.&lt;/p&gt; &#xA;&lt;h2&gt;Are you a developer interested in building on atproto?&lt;/h2&gt; &#xA;&lt;p&gt;Bluesky is an open social network built on the AT Protocol, a flexible technology that will never lock developers out of the ecosystems that they help build. With atproto, third-party integration can be as seamless as first-party through custom feeds, federated services, clients, and more.&lt;/p&gt; &#xA;&lt;h2&gt;License (MIT)&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/bluesky-social/social-app/main/LICENSE&#34;&gt;./LICENSE&lt;/a&gt; for the full license.&lt;/p&gt; &#xA;&lt;h2&gt;P.S.&lt;/h2&gt; &#xA;&lt;p&gt;We ‚ù§Ô∏è you and all of the ways you support us. Thank you for making Bluesky a great place!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>anthropics/courses</title>
    <updated>2024-11-03T01:38:22Z</updated>
    <id>tag:github.com,2024-11-03:/anthropics/courses</id>
    <link href="https://github.com/anthropics/courses" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Anthropic&#39;s educational courses&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Anthropic courses&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to Anthropic&#39;s educational courses. This repository currently contains five courses. We suggest completing the courses in the following order:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anthropics/courses/master/anthropic_api_fundamentals/README.md&#34;&gt;Anthropic API fundamentals&lt;/a&gt; - teaches the essentials of working with the Claude SDK: getting an API key, working with model parameters, writing multimodal prompts, streaming responses, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anthropics/courses/master/prompt_engineering_interactive_tutorial/README.md&#34;&gt;Prompt engineering interactive tutorial&lt;/a&gt; - a comprehensive step-by-step guide to key prompting techniques. [&lt;a href=&#34;https://catalog.us-east-1.prod.workshops.aws/workshops/0644c9e9-5b82-45f2-8835-3b5aa30b1848/en-US&#34;&gt;AWS Workshop version&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anthropics/courses/master/real_world_prompting/README.md&#34;&gt;Real world prompting&lt;/a&gt; - learn how to incorporate prompting techniques into complex, real world prompts. [&lt;a href=&#34;https://github.com/anthropics/courses/tree/vertex/real_world_prompting&#34;&gt;Google Vertex version&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anthropics/courses/master/prompt_evaluations/README.md&#34;&gt;Prompt evaluations&lt;/a&gt; - learn how to write production prompt evaluations to measure the quality of your prompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anthropics/courses/master/tool_use/README.md&#34;&gt;Tool use&lt;/a&gt; - teaches everything you need to know to implement tool use successfully in your workflows with Claude.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note that these courses often favor our lowest-cost model, Claude 3 Haiku, to keep API costs down for students following along with the materials. Feel free to use other Claude models if you prefer.&lt;/strong&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/lerobot</title>
    <updated>2024-11-03T01:38:22Z</updated>
    <id>tag:github.com,2024-11-03:/huggingface/lerobot</id>
    <link href="https://github.com/huggingface/lerobot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü§ó LeRobot: Making AI for Robotics more accessible with end-to-end learning&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;media/lerobot-logo-thumbnail.png&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;media/lerobot-logo-thumbnail.png&#34;&gt; &#xA;  &lt;img alt=&#34;LeRobot, Hugging Face Robotics Library&#34; src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/lerobot-logo-thumbnail.png&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA; &lt;/picture&gt; &lt;br&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml/badge.svg?branch=main&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/huggingface/lerobot&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/lerobot&#34; alt=&#34;Python versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lerobot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/status/lerobot&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lerobot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/lerobot&#34; alt=&#34;Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/tree/main/examples&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Examples-green.svg?sanitize=true&#34; alt=&#34;Examples&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/CODE_OF_CONDUCT.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg?sanitize=true&#34; alt=&#34;Contributor Covenant&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/s3KuuzsPFb&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/examples/10_use_so100.md&#34;&gt;New robot in town: SO-100&lt;/a&gt;&lt;/p&gt; &lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/so100/leader_follower.webp?raw=true&#34; alt=&#34;SO-100 leader and follower arms&#34; title=&#34;SO-100 leader and follower arms&#34; width=&#34;50%&#34;&gt; &#xA; &lt;p&gt;We just added a new tutorial on how to build a more affordable robot, at the price of $110 per arm!&lt;/p&gt; &#xA; &lt;p&gt;Teach it new skills by showing it a few moves with just a laptop.&lt;/p&gt; &#xA; &lt;p&gt;Then watch your homemade robot act autonomously ü§Ø&lt;/p&gt; &#xA; &lt;p&gt;Follow the link to the &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/examples/10_use_so100.md&#34;&gt;full tutorial for SO-100&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;p&gt;LeRobot: State-of-the-art AI for real-world robotics&lt;/p&gt; &lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;ü§ó LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.&lt;/p&gt; &#xA;&lt;p&gt;ü§ó LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.&lt;/p&gt; &#xA;&lt;p&gt;ü§ó LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.&lt;/p&gt; &#xA;&lt;p&gt;ü§ó LeRobot hosts pretrained models and datasets on this Hugging Face community page: &lt;a href=&#34;https://huggingface.co/lerobot&#34;&gt;huggingface.co/lerobot&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Examples of pretrained models on simulation environments&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;http://remicadene.com/assets/gif/aloha_act.gif&#34; width=&#34;100%&#34; alt=&#34;ACT policy on ALOHA env&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;http://remicadene.com/assets/gif/simxarm_tdmpc.gif&#34; width=&#34;100%&#34; alt=&#34;TDMPC policy on SimXArm env&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;http://remicadene.com/assets/gif/pusht_diffusion.gif&#34; width=&#34;100%&#34; alt=&#34;Diffusion policy on PushT env&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACT policy on ALOHA env&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TDMPC policy on SimXArm env&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Diffusion policy on PushT env&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Acknowledgment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to Tony Zaho, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from &lt;a href=&#34;https://tonyzhaozh.github.io/aloha&#34;&gt;ALOHA&lt;/a&gt; and &lt;a href=&#34;https://mobile-aloha.github.io&#34;&gt;Mobile ALOHA&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from &lt;a href=&#34;https://diffusion-policy.cs.columbia.edu&#34;&gt;Diffusion Policy&lt;/a&gt; and &lt;a href=&#34;https://umi-gripper.github.io&#34;&gt;UMI Gripper&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from &lt;a href=&#34;https://github.com/nicklashansen/tdmpc&#34;&gt;TDMPC&lt;/a&gt; and &lt;a href=&#34;https://www.yunhaifeng.com/FOWM&#34;&gt;FOWM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Antonio Loquercio and Ashish Kumar for their early support.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://sjlee.cc/&#34;&gt;Seungjae (Jay) Lee&lt;/a&gt;, &lt;a href=&#34;https://mahis.life/&#34;&gt;Mahi Shafiullah&lt;/a&gt; and colleagues for open sourcing &lt;a href=&#34;https://sjlee.cc/vq-bet/&#34;&gt;VQ-BeT&lt;/a&gt; policy and helping us adapt the codebase to our repository. The policy is adapted from &lt;a href=&#34;https://github.com/jayLEE0301/vq_bet_official&#34;&gt;VQ-BeT repo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download our source code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/huggingface/lerobot.git&#xA;cd lerobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a virtual environment with Python 3.10 and activate it, e.g. with &lt;a href=&#34;https://docs.anaconda.com/free/miniconda/index.html&#34;&gt;&lt;code&gt;miniconda&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -y -n lerobot python=3.10&#xA;conda activate lerobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install ü§ó LeRobot:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Depending on your platform, If you encounter any build errors during this step you may need to install &lt;code&gt;cmake&lt;/code&gt; and &lt;code&gt;build-essential&lt;/code&gt; for building some of our dependencies. On linux: &lt;code&gt;sudo apt-get install cmake build-essential&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For simulations, ü§ó LeRobot comes with gymnasium environments that can be installed as extras:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-aloha&#34;&gt;aloha&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-xarm&#34;&gt;xarm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-pusht&#34;&gt;pusht&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For instance, to install ü§ó LeRobot with aloha and pusht, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e &#34;.[aloha, pusht]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use &lt;a href=&#34;https://docs.wandb.ai/quickstart&#34;&gt;Weights and Biases&lt;/a&gt; for experiment tracking, log in with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wandb login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(note: you will also need to enable WandB in the configuration. See below.)&lt;/p&gt; &#xA;&lt;h2&gt;Walkthrough&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;‚îú‚îÄ‚îÄ examples             # contains demonstration examples, start here to learn about LeRobot&#xA;|   ‚îî‚îÄ‚îÄ advanced         # contains even more examples for those who have mastered the basics&#xA;‚îú‚îÄ‚îÄ lerobot&#xA;|   ‚îú‚îÄ‚îÄ configs          # contains hydra yaml files with all options that you can override in the command line&#xA;|   |   ‚îú‚îÄ‚îÄ default.yaml   # selected by default, it loads pusht environment and diffusion policy&#xA;|   |   ‚îú‚îÄ‚îÄ env            # various sim environments and their datasets: aloha.yaml, pusht.yaml, xarm.yaml&#xA;|   |   ‚îî‚îÄ‚îÄ policy         # various policies: act.yaml, diffusion.yaml, tdmpc.yaml&#xA;|   ‚îú‚îÄ‚îÄ common           # contains classes and utilities&#xA;|   |   ‚îú‚îÄ‚îÄ datasets       # various datasets of human demonstrations: aloha, pusht, xarm&#xA;|   |   ‚îú‚îÄ‚îÄ envs           # various sim environments: aloha, pusht, xarm&#xA;|   |   ‚îú‚îÄ‚îÄ policies       # various policies: act, diffusion, tdmpc&#xA;|   |   ‚îú‚îÄ‚îÄ robot_devices  # various real devices: dynamixel motors, opencv cameras, koch robots&#xA;|   |   ‚îî‚îÄ‚îÄ utils          # various utilities&#xA;|   ‚îî‚îÄ‚îÄ scripts          # contains functions to execute via command line&#xA;|       ‚îú‚îÄ‚îÄ eval.py                 # load policy and evaluate it on an environment&#xA;|       ‚îú‚îÄ‚îÄ train.py                # train a policy via imitation learning and/or reinforcement learning&#xA;|       ‚îú‚îÄ‚îÄ control_robot.py        # teleoperate a real robot, record data, run a policy&#xA;|       ‚îú‚îÄ‚îÄ push_dataset_to_hub.py  # convert your dataset into LeRobot dataset format and upload it to the Hugging Face hub&#xA;|       ‚îî‚îÄ‚îÄ visualize_dataset.py    # load a dataset and render its demonstrations&#xA;‚îú‚îÄ‚îÄ outputs               # contains results of scripts execution: logs, videos, model checkpoints&#xA;‚îî‚îÄ‚îÄ tests                 # contains pytest utilities for continuous integration&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualize datasets&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py&#34;&gt;example 1&lt;/a&gt; that illustrates how to use our dataset class which automatically download data from the Hugging Face hub.&lt;/p&gt; &#xA;&lt;p&gt;You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/visualize_dataset.py \&#xA;    --repo-id lerobot/pusht \&#xA;    --episode-index 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or from a dataset in a local folder with the root &lt;code&gt;DATA_DIR&lt;/code&gt; environment variable (in the following case the dataset will be searched for in &lt;code&gt;./my_local_data_dir/lerobot/pusht&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DATA_DIR=&#39;./my_local_data_dir&#39; python lerobot/scripts/visualize_dataset.py \&#xA;    --repo-id lerobot/pusht \&#xA;    --episode-index 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will open &lt;code&gt;rerun.io&lt;/code&gt; and display the camera streams, robot states and actions, like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&#34;&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our script can also visualize datasets stored on a distant server. See &lt;code&gt;python lerobot/scripts/visualize_dataset.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h3&gt;The &lt;code&gt;LeRobotDataset&lt;/code&gt; format&lt;/h3&gt; &#xA;&lt;p&gt;A dataset in &lt;code&gt;LeRobotDataset&lt;/code&gt; format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. &lt;code&gt;dataset = LeRobotDataset(&#34;lerobot/aloha_static_coffee&#34;)&lt;/code&gt; and can be indexed into like any Hugging Face and PyTorch dataset. For instance &lt;code&gt;dataset[0]&lt;/code&gt; will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.&lt;/p&gt; &#xA;&lt;p&gt;A specificity of &lt;code&gt;LeRobotDataset&lt;/code&gt; is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting &lt;code&gt;delta_timestamps&lt;/code&gt; to a list of relative times with respect to the indexed frame. For example, with &lt;code&gt;delta_timestamps = {&#34;observation.image&#34;: [-1, -0.5, -0.2, 0]}&lt;/code&gt; one can retrieve, for a given index, 4 frames: 3 &#34;previous&#34; frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py&#34;&gt;1_load_lerobot_dataset.py&lt;/a&gt; for more details on &lt;code&gt;delta_timestamps&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Under the hood, the &lt;code&gt;LeRobotDataset&lt;/code&gt; format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.&lt;/p&gt; &#xA;&lt;p&gt;Here are the important details and internal structure organization of a typical &lt;code&gt;LeRobotDataset&lt;/code&gt; instantiated with &lt;code&gt;dataset = LeRobotDataset(&#34;lerobot/aloha_static_coffee&#34;)&lt;/code&gt;. The exact features will change from dataset to dataset but not the main aspects:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dataset attributes:&#xA;  ‚îú hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:&#xA;  ‚îÇ  ‚îú observation.images.cam_high (VideoFrame):&#xA;  ‚îÇ  ‚îÇ   VideoFrame = {&#39;path&#39;: path to a mp4 video, &#39;timestamp&#39; (float32): timestamp in the video}&#xA;  ‚îÇ  ‚îú observation.state (list of float32): position of an arm joints (for instance)&#xA;  ‚îÇ  ... (more observations)&#xA;  ‚îÇ  ‚îú action (list of float32): goal position of an arm joints (for instance)&#xA;  ‚îÇ  ‚îú episode_index (int64): index of the episode for this sample&#xA;  ‚îÇ  ‚îú frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode&#xA;  ‚îÇ  ‚îú timestamp (float32): timestamp in the episode&#xA;  ‚îÇ  ‚îú next.done (bool): indicates the end of en episode ; True for the last frame in each episode&#xA;  ‚îÇ  ‚îî index (int64): general index in the whole dataset&#xA;  ‚îú episode_data_index: contains 2 tensors with the start and end indices of each episode&#xA;  ‚îÇ  ‚îú from (1D int64 tensor): first frame index for each episode ‚Äî shape (num episodes,) starts with 0&#xA;  ‚îÇ  ‚îî to: (1D int64 tensor): last frame index for each episode ‚Äî shape (num episodes,)&#xA;  ‚îú stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance&#xA;  ‚îÇ  ‚îú observation.images.cam_high: {&#39;max&#39;: tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}&#xA;  ‚îÇ  ...&#xA;  ‚îú info: a dictionary of metadata on the dataset&#xA;  ‚îÇ  ‚îú codebase_version (str): this is to keep track of the codebase version the dataset was created with&#xA;  ‚îÇ  ‚îú fps (float): frame per second the dataset is recorded/synchronized to&#xA;  ‚îÇ  ‚îú video (bool): indicates if frames are encoded in mp4 video files to save space or stored as png files&#xA;  ‚îÇ  ‚îî encoding (dict): if video, this documents the main options that were used with ffmpeg to encode the videos&#xA;  ‚îú videos_dir (Path): where the mp4 videos or png images are stored/accessed&#xA;  ‚îî camera_keys (list of string): the keys to access camera features in the item returned by the dataset (e.g. `[&#34;observation.images.cam_high&#34;, ...]`)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A &lt;code&gt;LeRobotDataset&lt;/code&gt; is serialised using several widespread file formats for each of its parts, namely:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;hf_dataset stored using Hugging Face datasets library serialization to parquet&lt;/li&gt; &#xA; &lt;li&gt;videos are stored in mp4 format to save space or png files&lt;/li&gt; &#xA; &lt;li&gt;episode_data_index saved using &lt;code&gt;safetensor&lt;/code&gt; tensor serialization format&lt;/li&gt; &#xA; &lt;li&gt;stats saved using &lt;code&gt;safetensor&lt;/code&gt; tensor serialization format&lt;/li&gt; &#xA; &lt;li&gt;info are saved using JSON&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can set the &lt;code&gt;DATA_DIR&lt;/code&gt; environment variable to your root dataset folder as illustrated in the above section on dataset visualization.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluate a pretrained policy&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/2_evaluate_pretrained_policy.py&#34;&gt;example 2&lt;/a&gt; that illustrates how to download a pretrained policy from Hugging Face hub, and run an evaluation on its corresponding environment.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a more capable script to parallelize the evaluation over multiple environments during the same rollout. Here is an example with a pretrained model hosted on &lt;a href=&#34;https://huggingface.co/lerobot/diffusion_pusht&#34;&gt;lerobot/diffusion_pusht&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/eval.py \&#xA;    -p lerobot/diffusion_pusht \&#xA;    eval.n_episodes=10 \&#xA;    eval.batch_size=10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: After training your own policy, you can re-evaluate the checkpoints with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/eval.py -p {OUTPUT_DIR}/checkpoints/last/pretrained_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;python lerobot/scripts/eval.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h3&gt;Train your own policy&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/3_train_policy.py&#34;&gt;example 3&lt;/a&gt; that illustrates how to train a model using our core library in python, and &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/4_train_policy_with_script.md&#34;&gt;example 4&lt;/a&gt; that shows how to use our training script from command line.&lt;/p&gt; &#xA;&lt;p&gt;In general, you can use our training script to easily train any policy. Here is an example of training the ACT policy on trajectories collected by humans on the Aloha simulation environment for the insertion task:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/train.py \&#xA;    policy=act \&#xA;    env=aloha \&#xA;    env.task=AlohaInsertion-v0 \&#xA;    dataset_repo_id=lerobot/aloha_sim_insertion_human \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The experiment directory is automatically generated and will show up in yellow in your terminal. It looks like &lt;code&gt;outputs/train/2024-05-05/20-21-12_aloha_act_default&lt;/code&gt;. You can manually specify an experiment directory by adding this argument to the &lt;code&gt;train.py&lt;/code&gt; python command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    hydra.run.dir=your/new/experiment/dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the experiment directory there will be a folder called &lt;code&gt;checkpoints&lt;/code&gt; which will have the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;checkpoints&#xA;‚îú‚îÄ‚îÄ 000250  # checkpoint_dir for training step 250&#xA;‚îÇ   ‚îú‚îÄ‚îÄ pretrained_model  # Hugging Face pretrained model dir&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.json  # Hugging Face pretrained model config&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.yaml  # consolidated Hydra config&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.safetensors  # model weights&#xA;‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md  # Hugging Face model card&#xA;‚îÇ   ‚îî‚îÄ‚îÄ training_state.pth  # optimizer/scheduler/rng state and training step&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To resume training from a checkpoint, you can add these to the &lt;code&gt;train.py&lt;/code&gt; python command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    hydra.run.dir=your/original/experiment/dir resume=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will load the pretrained model, optimizer and scheduler states for training. For more information please see our tutorial on training resumption &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/examples/5_resume_training.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To use wandb for logging training and evaluation curves, make sure you&#39;ve run &lt;code&gt;wandb login&lt;/code&gt; as a one-time setup step. Then, when running the training command above, enable WandB in the configuration by adding:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    wandb.enable=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A link to the wandb logs for the run will also show up in yellow in your terminal. Here is an example of what they look like in your browser. Please also check &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/examples/4_train_policy_with_script.md#typical-logs-and-metrics&#34;&gt;here&lt;/a&gt; for the explaination of some commonly used metrics in logs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/wandb.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: For efficiency, during training every checkpoint is evaluated on a low number of episodes. You may use &lt;code&gt;eval.n_episodes=500&lt;/code&gt; to evaluate on more episodes than the default. Or, after training, you may want to re-evaluate your best checkpoints on more episodes or change the evaluation settings. See &lt;code&gt;python lerobot/scripts/eval.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h4&gt;Reproduce state-of-the-art (SOTA)&lt;/h4&gt; &#xA;&lt;p&gt;We have organized our configuration files (found under &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/lerobot/configs&#34;&gt;&lt;code&gt;lerobot/configs&lt;/code&gt;&lt;/a&gt;) such that they reproduce SOTA results from a given model variant in their respective original works. Simply running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/train.py policy=diffusion env=pusht&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;reproduces SOTA results for Diffusion Policy on the PushT task.&lt;/p&gt; &#xA;&lt;p&gt;Pretrained policies, along with reproduction details, can be found under the &#34;Models&#34; section of &lt;a href=&#34;https://huggingface.co/lerobot&#34;&gt;https://huggingface.co/lerobot&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to contribute to ü§ó LeRobot, please check out our &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/CONTRIBUTING.md&#34;&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Add a new dataset&lt;/h3&gt; &#xA;&lt;p&gt;To add a dataset to the hub, you need to login using a write-access token, which can be generated from the &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;Hugging Face settings&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then point to your raw dataset folder (e.g. &lt;code&gt;data/aloha_static_pingpong_test_raw&lt;/code&gt;), and push your dataset to the hub with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/push_dataset_to_hub.py \&#xA;--raw-dir data/aloha_static_pingpong_test_raw \&#xA;--out-dir data \&#xA;--repo-id lerobot/aloha_static_pingpong_test \&#xA;--raw-format aloha_hdf5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;python lerobot/scripts/push_dataset_to_hub.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;p&gt;If your dataset format is not supported, implement your own in &lt;code&gt;lerobot/common/datasets/push_dataset_to_hub/${raw_format}_format.py&lt;/code&gt; by copying examples like &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/pusht_zarr_format.py&#34;&gt;pusht_zarr&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/umi_zarr_format.py&#34;&gt;umi_zarr&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/aloha_hdf5_format.py&#34;&gt;aloha_hdf5&lt;/a&gt;, or &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/xarm_pkl_format.py&#34;&gt;xarm_pkl&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Add a pretrained policy&lt;/h3&gt; &#xA;&lt;p&gt;Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like &lt;code&gt;${hf_user}/${repo_name}&lt;/code&gt; (e.g. &lt;a href=&#34;https://huggingface.co/lerobot/diffusion_pusht&#34;&gt;lerobot/diffusion_pusht&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;You first need to find the checkpoint folder located inside your experiment directory (e.g. &lt;code&gt;outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500&lt;/code&gt;). Within that there is a &lt;code&gt;pretrained_model&lt;/code&gt; directory which should contain:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;config.json&lt;/code&gt;: A serialized version of the policy configuration (following the policy&#39;s dataclass config).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model.safetensors&lt;/code&gt;: A set of &lt;code&gt;torch.nn.Module&lt;/code&gt; parameters, saved in &lt;a href=&#34;https://huggingface.co/docs/safetensors/index&#34;&gt;Hugging Face Safetensors&lt;/a&gt; format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;config.yaml&lt;/code&gt;: A consolidated Hydra training configuration containing the policy, environment, and dataset configs. The policy configuration should match &lt;code&gt;config.json&lt;/code&gt; exactly. The environment config is useful for anyone who wants to evaluate your policy. The dataset config just serves as a paper trail for reproducibility.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To upload these to the hub, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli upload ${hf_user}/${repo_name} path/to/pretrained_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/scripts/eval.py&#34;&gt;eval.py&lt;/a&gt; for an example of how other people may use your policy.&lt;/p&gt; &#xA;&lt;h3&gt;Improve your code with profiling&lt;/h3&gt; &#xA;&lt;p&gt;An example of a code snippet to profile the evaluation of a policy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.profiler import profile, record_function, ProfilerActivity&#xA;&#xA;def trace_handler(prof):&#xA;    prof.export_chrome_trace(f&#34;tmp/trace_schedule_{prof.step_num}.json&#34;)&#xA;&#xA;with profile(&#xA;    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],&#xA;    schedule=torch.profiler.schedule(&#xA;        wait=2,&#xA;        warmup=2,&#xA;        active=3,&#xA;    ),&#xA;    on_trace_ready=trace_handler&#xA;) as prof:&#xA;    with record_function(&#34;eval_policy&#34;):&#xA;        for i in range(num_episodes):&#xA;            prof.step()&#xA;            # insert code to profile, potentially whole body of eval_policy function&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you want, you can cite this work with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{cadene2024lerobot,&#xA;    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Wolf, Thomas},&#xA;    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},&#xA;    howpublished = &#34;\url{https://github.com/huggingface/lerobot}&#34;,&#xA;    year = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additionally, if you are using any of the particular policy architecture, pretrained models, or datasets, it is recommended to cite the original authors of the work as they appear below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffusion-policy.cs.columbia.edu&#34;&gt;Diffusion Policy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{chi2024diffusionpolicy,&#xA;&#x9;author = {Cheng Chi and Zhenjia Xu and Siyuan Feng and Eric Cousineau and Yilun Du and Benjamin Burchfiel and Russ Tedrake and Shuran Song},&#xA;&#x9;title ={Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},&#xA;&#x9;journal = {The International Journal of Robotics Research},&#xA;&#x9;year = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tonyzhaozh.github.io/aloha&#34;&gt;ACT or ALOHA&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{zhao2023learning,&#xA;  title={Learning fine-grained bimanual manipulation with low-cost hardware},&#xA;  author={Zhao, Tony Z and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},&#xA;  journal={arXiv preprint arXiv:2304.13705},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nicklashansen.com/td-mpc/&#34;&gt;TDMPC&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Hansen2022tdmpc,&#xA;&#x9;title={Temporal Difference Learning for Model Predictive Control},&#xA;&#x9;author={Nicklas Hansen and Xiaolong Wang and Hao Su},&#xA;&#x9;booktitle={ICML},&#xA;&#x9;year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sjlee.cc/vq-bet/&#34;&gt;VQ-BeT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{lee2024behavior,&#xA;  title={Behavior generation with latent actions},&#xA;  author={Lee, Seungjae and Wang, Yibin and Etukuru, Haritheja and Kim, H Jin and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},&#xA;  journal={arXiv preprint arXiv:2403.03181},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>