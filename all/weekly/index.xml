<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-09T01:38:48Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>patchy631/ai-engineering-hub</title>
    <updated>2025-03-09T01:38:48Z</updated>
    <id>tag:github.com,2025-03-09:/patchy631/ai-engineering-hub</id>
    <link href="https://github.com/patchy631/ai-engineering-hub" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Engineering Hub ðŸš€&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the &lt;strong&gt;AI Engineering Hub&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;ðŸŒŸ Why This Repo?&lt;/h2&gt; &#xA;&lt;p&gt;AI Engineering is advancing rapidly, and staying at the forefront requires both deep understanding and hands-on experience. Here, you will find:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In-depth tutorials on &lt;strong&gt;LLMs and RAGs&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Real-world &lt;strong&gt;AI agent&lt;/strong&gt; applications&lt;/li&gt; &#xA; &lt;li&gt;Examples to implement, adapt, and scale in your projects&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Whether youâ€™re a beginner, practitioner, or researcher, this repo provides resources for all skill levels to experiment and succeed in AI engineering.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ðŸ“¬ Stay Updated with Our Newsletter!&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Get a FREE Data Science eBook&lt;/strong&gt; ðŸ“– with 150+ essential lessons in Data Science when you subscribe to our newsletter! Stay in the loop with the latest tutorials, insights, and exclusive resources. &lt;a href=&#34;https://join.dailydoseofds.com&#34;&gt;Subscribe now!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://join.dailydoseofds.com&#34;&gt;&lt;img src=&#34;https://github.com/patchy631/ai-engineering/raw/main/resources/join_ddods.png&#34; alt=&#34;Daily Dose of Data Science Newsletter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ðŸ“¢ Contribute to the AI Engineering Hub!&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributors! Whether you want to add new tutorials, improve existing code, or report issues, your contributions make this community thrive. Hereâ€™s how to get involved:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fork&lt;/strong&gt; the repository.&lt;/li&gt; &#xA; &lt;li&gt;Create a new branch for your contribution.&lt;/li&gt; &#xA; &lt;li&gt;Submit a &lt;strong&gt;Pull Request&lt;/strong&gt; and describe the improvements.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ðŸ“œ License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;ðŸ’¬ Connect&lt;/h2&gt; &#xA;&lt;p&gt;For discussions, suggestions, and more, feel free to &lt;a href=&#34;https://github.com/patchy631/ai-engineering/issues&#34;&gt;create an issue&lt;/a&gt; or reach out directly!&lt;/p&gt; &#xA;&lt;p&gt;Happy Coding! ðŸŽ‰&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LadybirdBrowser/ladybird</title>
    <updated>2025-03-09T01:38:48Z</updated>
    <id>tag:github.com,2025-03-09:/LadybirdBrowser/ladybird</id>
    <link href="https://github.com/LadybirdBrowser/ladybird" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Truly independent web browser&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ladybird&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ladybird.org&#34;&gt;Ladybird&lt;/a&gt; is a truly independent web browser, using a novel engine based on web standards.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Ladybird is in a pre-alpha state, and only suitable for use by developers&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;We aim to build a complete, usable browser for the modern web.&lt;/p&gt; &#xA;&lt;p&gt;Ladybird uses a multi-process architecture with a main UI process, several WebContent renderer processes, an ImageDecoder process, and a RequestServer process.&lt;/p&gt; &#xA;&lt;p&gt;Image decoding and network connections are done out of process to be more robust against malicious content. Each tab has its own renderer process, which is sandboxed from the rest of the system.&lt;/p&gt; &#xA;&lt;p&gt;At the moment, many core library support components are inherited from SerenityOS:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LibWeb: Web rendering engine&lt;/li&gt; &#xA; &lt;li&gt;LibJS: JavaScript engine&lt;/li&gt; &#xA; &lt;li&gt;LibWasm: WebAssembly implementation&lt;/li&gt; &#xA; &lt;li&gt;LibCrypto/LibTLS: Cryptography primitives and Transport Layer Security&lt;/li&gt; &#xA; &lt;li&gt;LibHTTP: HTTP/1.1 client&lt;/li&gt; &#xA; &lt;li&gt;LibGfx: 2D Graphics Library, Image Decoding and Rendering&lt;/li&gt; &#xA; &lt;li&gt;LibUnicode: Unicode and locale support&lt;/li&gt; &#xA; &lt;li&gt;LibMedia: Audio and video playback&lt;/li&gt; &#xA; &lt;li&gt;LibCore: Event loop, OS abstraction layer&lt;/li&gt; &#xA; &lt;li&gt;LibIPC: Inter-process communication&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How do I build and run this?&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/BuildInstructionsLadybird.md&#34;&gt;build instructions&lt;/a&gt; for information on how to build Ladybird.&lt;/p&gt; &#xA;&lt;p&gt;Ladybird runs on Linux, macOS, Windows (with WSL2), and many other *Nixes.&lt;/p&gt; &#xA;&lt;h2&gt;How do I read the documentation?&lt;/h2&gt; &#xA;&lt;p&gt;Code-related documentation can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/&#34;&gt;documentation&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Get in touch and participate!&lt;/h2&gt; &#xA;&lt;p&gt;Join &lt;a href=&#34;https://discord.gg/nvfjVJ4Svh&#34;&gt;our Discord server&lt;/a&gt; to participate in development discussion.&lt;/p&gt; &#xA;&lt;p&gt;Please read &lt;a href=&#34;https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/GettingStartedContributing.md&#34;&gt;Getting started contributing&lt;/a&gt; if you plan to contribute to Ladybird for the first time.&lt;/p&gt; &#xA;&lt;p&gt;Before opening an issue, please see the &lt;a href=&#34;https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/CONTRIBUTING.md#issue-policy&#34;&gt;issue policy&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/ISSUES.md&#34;&gt;detailed issue-reporting guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The full contribution guidelines can be found in &lt;a href=&#34;https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/CONTRIBUTING.md&#34;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Ladybird is licensed under a 2-clause BSD license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>modelscope/DiffSynth-Studio</title>
    <updated>2025-03-09T01:38:48Z</updated>
    <id>tag:github.com,2025-03-09:/modelscope/DiffSynth-Studio</id>
    <link href="https://github.com/modelscope/DiffSynth-Studio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Enjoy the magic of Diffusion models!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DiffSynth Studio&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/DiffSynth/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/DiffSynth&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/modelscope/DiffSynth-Studio.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/modelscope/DiffSynth-Studio.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/modelscope/DiffSynth-Studio/pull/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/modelscope/DiffSynth-Studio.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/modelscope/DiffSynth-Studio/commit/&#34;&gt;&lt;img src=&#34;https://badgen.net/github/last-commit/modelscope/DiffSynth-Studio&#34; alt=&#34;GitHub latest commit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://trendshift.io/repositories/10946&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/10946&#34; alt=&#34;modelscope%2FDiffSynth-Studio | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Document: &lt;a href=&#34;https://diffsynth-studio.readthedocs.io/zh-cn/latest/index.html&#34;&gt;https://diffsynth-studio.readthedocs.io/zh-cn/latest/index.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;DiffSynth Studio is a Diffusion engine. We have restructured architectures including Text Encoder, UNet, VAE, among others, maintaining compatibility with models from the open-source community while enhancing computational performance. We provide many interesting features. Enjoy the magic of Diffusion models!&lt;/p&gt; &#xA;&lt;p&gt;Until now, DiffSynth Studio has supported the following models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Wan-Video/Wan2.1&#34;&gt;Wan-Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stepfun-ai/Step-Video-T2V&#34;&gt;StepVideo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/HunyuanVideo&#34;&gt;HunyuanVideo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/THUDM/CogVideoX-5b&#34;&gt;CogVideoX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-dev&#34;&gt;FLUX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;ExVideo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Kwai-Kolors/Kolors&#34;&gt;Kolors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-3-medium&#34;&gt;Stable Diffusion 3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt&#34;&gt;Stable Video Diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/HunyuanDiT&#34;&gt;Hunyuan-DiT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hzwer/ECCV2022-RIFE&#34;&gt;RIFE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xinntao/ESRGAN&#34;&gt;ESRGAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;Ip-Adapter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/guoyww/animatediff/&#34;&gt;AnimateDiff&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0&#34;&gt;Stable Diffusion XL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;Stable Diffusion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;February 25, 2025&lt;/strong&gt; We support Wan-Video, a collection of SOTA video synthesis models open-sourced by Alibaba. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/&#34;&gt;./examples/wanvideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;February 17, 2025&lt;/strong&gt; We support &lt;a href=&#34;https://modelscope.cn/models/stepfun-ai/stepvideo-t2v/summary&#34;&gt;StepVideo&lt;/a&gt;! State-of-the-art video synthesis model! See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/stepvideo/&#34;&gt;./examples/stepvideo&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 31, 2024&lt;/strong&gt; We propose EliGen, a novel framework for precise entity-level controlled text-to-image generation, complemented by an inpainting fusion pipeline to extend its capabilities to image inpainting tasks. EliGen seamlessly integrates with existing community models, such as IP-Adapter and In-Context LoRA, enhancing its versatility. For more details, see &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/EntityControl/&#34;&gt;./examples/EntityControl&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2501.01097&#34;&gt;EliGen: Entity-Level Controlled Image Generation with Regional Attention&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Model: &lt;a href=&#34;https://www.modelscope.cn/models/DiffSynth-Studio/Eligen&#34;&gt;ModelScope&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/modelscope/EliGen&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Online Demo: &lt;a href=&#34;https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen&#34;&gt;ModelScope EliGen Studio&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Training Dataset: &lt;a href=&#34;https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet&#34;&gt;EliGen Train Set&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 19, 2024&lt;/strong&gt; We implement advanced VRAM management for HunyuanVideo, making it possible to generate videos at a resolution of 129x720x1280 using 24GB of VRAM, or at 129x512x384 resolution with just 6GB of VRAM. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/HunyuanVideo/&#34;&gt;./examples/HunyuanVideo/&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 18, 2024&lt;/strong&gt; We propose ArtAug, an approach designed to improve text-to-image synthesis models through synthesis-understanding interactions. We have trained an ArtAug enhancement module for FLUX.1-dev in the format of LoRA. This model integrates the aesthetic understanding of Qwen2-VL-72B into FLUX.1-dev, leading to an improvement in the quality of generated images.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2412.12888&#34;&gt;https://arxiv.org/abs/2412.12888&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Examples: &lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug&#34;&gt;https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Model: &lt;a href=&#34;https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1&#34;&gt;ModelScope&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Demo: &lt;a href=&#34;https://modelscope.cn/aigc/imageGeneration?tab=advanced&amp;amp;versionId=7228&amp;amp;modelType=LoRA&amp;amp;sdVersion=FLUX_1&amp;amp;modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0&#34;&gt;ModelScope&lt;/a&gt;, HuggingFace (Coming soon)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 25, 2024&lt;/strong&gt; We provide extensive FLUX ControlNet support. This project supports many different ControlNet models that can be freely combined, even if their structures differ. Additionally, ControlNet models are compatible with high-resolution refinement and partition control techniques, enabling very powerful controllable image generation. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ControlNet/&#34;&gt;&lt;code&gt;./examples/ControlNet/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 8, 2024.&lt;/strong&gt; We release the extended LoRA based on CogVideoX-5B and ExVideo. You can download this model from &lt;a href=&#34;https://modelscope.cn/models/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1&#34;&gt;ModelScope&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 22, 2024.&lt;/strong&gt; CogVideoX-5B is supported in this project. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/video_synthesis/&#34;&gt;here&lt;/a&gt;. We provide several interesting features for this text-to-video model, including&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Text to video&lt;/li&gt; &#xA;   &lt;li&gt;Video editing&lt;/li&gt; &#xA;   &lt;li&gt;Self-upscaling&lt;/li&gt; &#xA;   &lt;li&gt;Video interpolation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 22, 2024.&lt;/strong&gt; We have implemented an interesting painter that supports all text-to-image models. Now you can create stunning images using the painter, with assistance from AI!&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use it in our &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/#usage-in-webui&#34;&gt;WebUI&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 21, 2024.&lt;/strong&gt; FLUX is supported in DiffSynth-Studio.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Enable CFG and highres-fix to improve visual quality. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/image_synthesis/README.md&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;LoRA, ControlNet, and additional models will be available soon.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 21, 2024.&lt;/strong&gt; ðŸ”¥ðŸ”¥ðŸ”¥ We propose ExVideo, a post-tuning technique aimed at enhancing the capability of video generation models. We have extended Stable Video Diffusion to achieve the generation of long videos up to 128 frames.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/ExVideoProjectPage/&#34;&gt;Project Page&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Source code is released in this repo. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ExVideo/&#34;&gt;&lt;code&gt;examples/ExVideo&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Models are released on &lt;a href=&#34;https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;HuggingFace&lt;/a&gt; and &lt;a href=&#34;https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;ModelScope&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Technical report is released on &lt;a href=&#34;https://arxiv.org/abs/2406.14130&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;You can try ExVideo in this &lt;a href=&#34;https://huggingface.co/spaces/modelscope/ExVideo-SVD-128f-v1&#34;&gt;Demo&lt;/a&gt;!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 13, 2024.&lt;/strong&gt; DiffSynth Studio is transferred to ModelScope. The developers have transitioned from &#34;I&#34; to &#34;we&#34;. Of course, I will still participate in development and maintenance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Jan 29, 2024.&lt;/strong&gt; We propose Diffutoon, a fantastic solution for toon shading.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/DiffutoonProjectPage/&#34;&gt;Project Page&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;The source codes are released in this project.&lt;/li&gt; &#xA;   &lt;li&gt;The technical report (IJCAI 2024) is released on &lt;a href=&#34;https://arxiv.org/abs/2401.16224&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dec 8, 2023.&lt;/strong&gt; We decide to develop a new Project, aiming to release the potential of diffusion models, especially in video synthesis. The development of this project is started.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Nov 15, 2023.&lt;/strong&gt; We propose FastBlend, a powerful video deflickering algorithm.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The sd-webui extension is released on &lt;a href=&#34;https://github.com/Artiprocher/sd-webui-fastblend&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Demo videos are shown on Bilibili, including three tasks. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1d94y1W7PE&#34;&gt;Video deflickering&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Lw411m71p&#34;&gt;Video interpolation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1RB4y1Z7LF&#34;&gt;Image-driven video rendering&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;The technical report is released on &lt;a href=&#34;https://arxiv.org/abs/2311.09265&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;An unofficial ComfyUI extension developed by other users is released on &lt;a href=&#34;https://github.com/AInseven/ComfyUI-fastblend&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Oct 1, 2023.&lt;/strong&gt; We release an early version of this project, namely FastSDXL. A try for building a diffusion engine.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The source codes are released on &lt;a href=&#34;https://github.com/Artiprocher/FastSDXL&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;FastSDXL includes a trainable OLSS scheduler for efficiency improvement. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;The original repo of OLSS is &lt;a href=&#34;https://github.com/alibaba/EasyNLP/tree/master/diffusion/olss_scheduler&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;The technical report (CIKM 2023) is released on &lt;a href=&#34;https://arxiv.org/abs/2305.14677&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;A demo video is shown on &lt;a href=&#34;https://www.bilibili.com/video/BV1w8411y7uj&#34;&gt;Bilibili&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;Since OLSS requires additional training, we don&#39;t implement it in this project.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Aug 29, 2023.&lt;/strong&gt; We propose DiffSynth, a video synthesis framework.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/DiffSynth.github.io/&#34;&gt;Project Page&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The source codes are released in &lt;a href=&#34;https://github.com/alibaba/EasyNLP/tree/master/diffusion/DiffSynth&#34;&gt;EasyNLP&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The technical report (ECML PKDD 2024) is released on &lt;a href=&#34;https://arxiv.org/abs/2308.03463&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install from source code (recommended):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/modelscope/DiffSynth-Studio.git&#xA;cd DiffSynth-Studio&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or install from pypi (There is a delay in the update. If you want to experience the latest features, please do not use this installation method.):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install diffsynth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you encounter issues during installation, it may be caused by the packages we depend on. Please refer to the documentation of the package that caused the problem.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;torch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/sentencepiece&#34;&gt;sentencepiece&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cmake.org&#34;&gt;cmake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.cupy.dev/en/stable/install.html&#34;&gt;cupy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage (in Python code)&lt;/h2&gt; &#xA;&lt;p&gt;The Python examples are in &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt;. We provide an overview here.&lt;/p&gt; &#xA;&lt;h3&gt;Download Models&lt;/h3&gt; &#xA;&lt;p&gt;Download the pre-set models. Model IDs can be found in &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/diffsynth/configs/model_config.py&#34;&gt;config file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from diffsynth import download_models&#xA;&#xA;download_models([&#34;FLUX.1-dev&#34;, &#34;Kolors&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download your own models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from diffsynth.models.downloader import download_from_huggingface, download_from_modelscope&#xA;&#xA;# From Modelscope (recommended)&#xA;download_from_modelscope(&#34;Kwai-Kolors/Kolors&#34;, &#34;vae/diffusion_pytorch_model.fp16.bin&#34;, &#34;models/kolors/Kolors/vae&#34;)&#xA;# From Huggingface&#xA;download_from_huggingface(&#34;Kwai-Kolors/Kolors&#34;, &#34;vae/diffusion_pytorch_model.fp16.safetensors&#34;, &#34;models/kolors/Kolors/vae&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Video Synthesis&lt;/h3&gt; &#xA;&lt;h4&gt;Text-to-video using CogVideoX-5B&lt;/h4&gt; &#xA;&lt;p&gt;CogVideoX-5B is released by ZhiPu. We provide an improved pipeline, supporting text-to-video, video editing, self-upscaling and video interpolation. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/video_synthesis/&#34;&gt;&lt;code&gt;examples/video_synthesis&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The video on the left is generated using the original text-to-video pipeline, while the video on the right is the result after editing and frame interpolation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/26b044c1-4a60-44a4-842f-627ff289d006&#34;&gt;https://github.com/user-attachments/assets/26b044c1-4a60-44a4-842f-627ff289d006&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Long Video Synthesis&lt;/h4&gt; &#xA;&lt;p&gt;We trained extended video synthesis models, which can generate 128 frames. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ExVideo/&#34;&gt;&lt;code&gt;examples/ExVideo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc&#34;&gt;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/321ee04b-8c17-479e-8a95-8cbcf21f8d7e&#34;&gt;https://github.com/user-attachments/assets/321ee04b-8c17-479e-8a95-8cbcf21f8d7e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Toon Shading&lt;/h4&gt; &#xA;&lt;p&gt;Render realistic videos in a flatten style and enable video editing features. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/Diffutoon/&#34;&gt;&lt;code&gt;examples/Diffutoon&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/20528af5-5100-474a-8cdc-440b9efdd86c&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/20528af5-5100-474a-8cdc-440b9efdd86c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Video Stylization&lt;/h4&gt; &#xA;&lt;p&gt;Video stylization without video models. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/diffsynth/&#34;&gt;&lt;code&gt;examples/diffsynth&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image Synthesis&lt;/h3&gt; &#xA;&lt;p&gt;Generate high-resolution images, by breaking the limitation of diffusion models! &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/image_synthesis/&#34;&gt;&lt;code&gt;examples/image_synthesis&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;LoRA fine-tuning is supported in &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/train/&#34;&gt;&lt;code&gt;examples/train&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;FLUX&lt;/th&gt; &#xA;   &lt;th&gt;Stable Diffusion 3&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/984561e9-553d-4952-9443-79ce144f379f&#34; alt=&#34;image_1024_cfg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/4df346db-6f91-420a-b4c1-26e205376098&#34; alt=&#34;image_1024&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kolors&lt;/th&gt; &#xA;   &lt;th&gt;Hunyuan-DiT&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/53ef6f41-da11-4701-8665-9f64392607bf&#34; alt=&#34;image_1024&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/60b022c8-df3f-4541-95ab-bf39f2fa8bb5&#34; alt=&#34;image_1024&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Stable Diffusion&lt;/th&gt; &#xA;   &lt;th&gt;Stable Diffusion XL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/6fc84611-8da6-4a1f-8fee-9a34eba3b4a5&#34; alt=&#34;1024&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/67687748-e738-438c-aee5-96096f09ac90&#34; alt=&#34;1024&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage (in WebUI)&lt;/h2&gt; &#xA;&lt;p&gt;Create stunning images using the painter, with assistance from AI!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/95265d21-cdd6-4125-a7cb-9fbcf6ceb7b0&#34;&gt;https://github.com/user-attachments/assets/95265d21-cdd6-4125-a7cb-9fbcf6ceb7b0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This video is not rendered in real-time.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before launching the WebUI, please download models to the folder &lt;code&gt;./models&lt;/code&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/#download-models&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Gradio&lt;/code&gt; version&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install gradio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;python apps/gradio/DiffSynth_Studio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/59613157-de51-4109-99b3-97cbffd88076&#34; alt=&#34;20240822102002&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Streamlit&lt;/code&gt; version&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install streamlit streamlit-drawable-canvas&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m streamlit run apps/streamlit/DiffSynth_Studio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/93085557-73f3-4eee-a205-9829591ef954&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/93085557-73f3-4eee-a205-9829591ef954&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>