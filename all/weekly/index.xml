<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-10T01:48:56Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>LC044/WeChatMsg</title>
    <updated>2023-12-10T01:48:56Z</updated>
    <id>tag:github.com,2023-12-10:/LC044/WeChatMsg</id>
    <link href="https://github.com/LC044/WeChatMsg" rel="alternate"></link>
    <summary type="html">&lt;p&gt;æå–å¾®ä¿¡èŠå¤©è®°å½•ï¼Œå°†å…¶å¯¼å‡ºæˆHTMLã€Wordã€CSVæ–‡æ¡£æ°¸ä¹…ä¿å­˜ï¼Œå¯¹èŠå¤©è®°å½•è¿›è¡Œåˆ†æç”Ÿæˆå¹´åº¦èŠå¤©æŠ¥å‘Š&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;æˆ‘çš„æ•°æ®æˆ‘åšä¸»&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/WeChat-ç•™ç—•-blue.svg&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/LC044/WeChatMsg/stargazers&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/LC044/WeChatMsg.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/LC044/WeChatMsg/issues&#34;&gt; &lt;img alt=&#34;Issues&#34; src=&#34;https://img.shields.io/github/issues/LC044/WeChatMsg?color=0088ff&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/readme.md&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/æ–‡æ¡£-æœ€æ–°-brightgreen.svg&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/LC044/WeChatMsg&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/logo.png&#34; height=&#34;240&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;div style=&#34;background-color: #eaf7ea; border-radius: 10px; padding: 20px; position: relative;&#34;&gt; &#xA;  &lt;div style=&#34;position: relative;&#34;&gt; &#xA;   &lt;div style=&#34;position: absolute;top: 0;bottom: 0;left: 0;width: 2px;background-color: #000000;&#34;&gt;&lt;/div&gt; &#xA;   &lt;h2&gt;å‰è¨€&lt;/h2&gt; &#xA;   &lt;div style=&#34;text-indent: 2em;&#34;&gt; &#xA;    &lt;p style=&#34;text-indent:2em;&#34;&gt;æˆ‘æ·±ä¿¡æœ‰æ„ä¹‰çš„ä¸æ˜¯å¾®ä¿¡ï¼Œè€Œæ˜¯éšè—åœ¨å¯¹è¯æ¡†èƒŒåçš„ä¸€ä¸ªä¸ª&lt;strong&gt;æ·±åˆ»æ•…äº‹&lt;/strong&gt;ã€‚æœªæ¥ï¼Œæ¯ä¸ªäººéƒ½èƒ½æ‹¥æœ‰AIçš„é™ªä¼´ï¼Œè€Œä½ çš„æ•°æ®èƒ½å¤Ÿèµ‹äºˆå®ƒæœ‰å…³ä¸ä½ è¿‡å»çš„çè´µè®°å¿†ã€‚æˆ‘å¸Œæœ›æ¯ä¸ªäººéƒ½æœ‰å°†è‡ªå·±çš„ç”Ÿæ´»ç—•è¿¹ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦ğŸ‘šğŸ¥—ğŸ ï¸ğŸš´ğŸ§‹â›¹ï¸ğŸ›ŒğŸ›€ç•™å­˜çš„æƒåˆ©ï¼Œè€Œä¸æ˜¯å°†ä¹‹é—å¿˜ğŸ’€ã€‚&lt;/p&gt; &#xA;    &lt;p style=&#34;text-indent:2em;&#34;&gt;AIçš„å‘å±•ä¸ä»…ä»…æ˜¯æŠ€æœ¯çš„æå‡ï¼Œæ›´æ˜¯æƒ…æ„ŸğŸ’çš„å»¶ç»­ã€‚æ¯ä¸€ä¸ªå¯¹è¯ã€æ¯ä¸€ä¸ªäº’åŠ¨éƒ½æ˜¯ç”Ÿæ´»ä¸­ç‹¬ä¸€æ— äºŒçš„ç‰‡æ®µï¼Œæ˜¯çœŸå®è€ŒåŠ¨äººçš„æƒ…æ„Ÿäº¤æµã€‚å› æ­¤ï¼Œæˆ‘å¸Œæœ›AIå·¥ä½œè€…ä»¬èƒ½å¤Ÿ&lt;strong&gt;å–„ç”¨è¿™äº›è‡ªå·±çš„æ•°æ®&lt;/strong&gt;ï¼Œç”¨äºåŸ¹è®­ç‹¬ç‰¹çš„ã€å±äºä¸ªä½“çš„äººå·¥æ™ºèƒ½ã€‚è®©&lt;strong&gt;ä¸ªäººAIæˆä¸ºç”Ÿæ´»ä¸­çš„æœ‹å‹&lt;/strong&gt;ï¼Œèƒ½å¤Ÿç†è§£ã€è®°å½•å¹¶åˆ†äº«æˆ‘ä»¬çš„æ¬¢ç¬‘ã€æ³ªæ°´å’Œæˆé•¿ã€‚&lt;/p&gt; &#xA;    &lt;p style=&#34;text-indent:2em;&#34;&gt;é‚£å¤©ï¼ŒAIä¸å†æ˜¯é«˜ä¸å¯æ”€çš„å­˜åœ¨ï¼Œè€Œæ˜¯èå…¥å¯»å¸¸ç™¾å§“å®¶çš„ä¸€éƒ¨åˆ†ã€‚å› ä¸º&lt;strong&gt;æ¯ä¸ªäººèƒ½æ‹¥æœ‰è‡ªå·±çš„AI&lt;/strong&gt;ï¼Œå°†ç§‘æŠ€çš„åŠ›é‡èå…¥ç”Ÿæ´»çš„æ–¹æ–¹é¢é¢ã€‚è¿™æ˜¯ä¸€åœºå…³äºçœŸæƒ…å®æ„Ÿçš„é©å‘½ï¼Œä¸€åœºè®©æŠ€æœ¯å˜å¾—æ›´åŠ äººæ€§åŒ–çš„æ¢ç´¢ï¼Œè®©æˆ‘ä»¬å…±åŒè§è¯æœªæ¥çš„ç¾å¥½ã€‚&lt;/p&gt; &#xA;    &lt;p align=&#34;center&#34;&gt;&lt;strong&gt;æ‰€ä»¥ã€Šç•™ç—•ã€‹&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;/div&gt; &#xA;  &lt;/div&gt; &#xA; &lt;/div&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;ğŸ‰åŠŸèƒ½&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ”’ï¸ğŸ”‘ğŸ”“ï¸æ‰‹æœºæœ¬åœ°å¾®ä¿¡æ•°æ®åº“ğŸ¶&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ”’ï¸ğŸ”‘ğŸ”“ï¸PCç«¯æœ¬åœ°å¾®ä¿¡æ•°æ®åº“ğŸ¶&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;è¿˜åŸå¾®ä¿¡èŠå¤©ç•Œé¢&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ğŸ—¨æ–‡æœ¬âœ…&lt;/li&gt; &#xA;   &lt;li&gt;ğŸå›¾ç‰‡âœ…&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ»â€â„ï¸è¡¨æƒ…åŒ…âœ…&lt;/li&gt; &#xA;   &lt;li&gt;æ‹ä¸€æ‹ç­‰ç³»ç»Ÿæ¶ˆæ¯âœ…&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å¯¼å‡ºèŠå¤©è®°å½•&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;sqliteæ•°æ®åº“âœ…&lt;/li&gt; &#xA;   &lt;li&gt;HTML(æ–‡æœ¬ã€å›¾ç‰‡ã€è§†é¢‘ã€è¡¨æƒ…åŒ…)âœ…&lt;/li&gt; &#xA;   &lt;li&gt;Wordæ–‡æ¡£âœ…&lt;/li&gt; &#xA;   &lt;li&gt;CSVæ–‡æ¡£âœ…&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;åˆ†æèŠå¤©æ•°æ®ï¼Œåšæˆå¯è§†åŒ–å¹´æŠ¥(æ€¥éœ€å‰ç«¯å¤§ä½¬æä¾›ä¼˜è´¨æ¨¡æ¿)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ”¥&lt;strong&gt;é¡¹ç›®æŒç»­æ›´æ–°ä¸­&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;å¯¼å‡ºå…¨éƒ¨è¡¨æƒ…åŒ…â&lt;/li&gt; &#xA;   &lt;li&gt;åˆå¹¶å¤šä¸ªå¤‡ä»½æ•°æ®â&lt;/li&gt; &#xA;   &lt;li&gt;è¯­éŸ³â&lt;/li&gt; &#xA;   &lt;li&gt;æ–‡ä»¶â&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å°ä¼™ä¼´ä»¬æƒ³è¦å…¶ä»–åŠŸèƒ½å¯ä»¥ç•™è¨€å“¦ğŸ“¬&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;æœ‰ä»»ä½•é—®é¢˜å¯ä»¥éšæ—¶è”ç³»æˆ‘(&lt;a href=&#34;mailto:863909694@qq.com&#34;&gt;863909694@qq.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ä¸ºäº†æ–¹ä¾¿å¤§å®¶äº¤æµï¼Œæˆ‘æ–°å»ºäº†ä¸€ä¸ªQQç¾¤ğŸ’¬ï¼š&lt;strong&gt;474379264&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;å¤§å®¶æœ‰ä»»ä½•æƒ³æ³•ğŸ’¡ã€å»ºè®®æˆ–bugå¯ä»¥ç¾¤é‡Œåé¦ˆç»™æˆ‘&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ¥¤æ•ˆæœ&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;img alt=&#34;èŠå¤©ç•Œé¢&#34; src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/chat_.png&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/image-20230520235220104.png&#34; alt=&#34;image-20230520235220104&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/image-20230520235338305.png&#34; alt=&#34;image-20230520235338305&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/image-20230520235351749.png&#34; alt=&#34;image-20230520235351749&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/image-20230520235400772.png&#34; alt=&#34;image-20230520235400772&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/image-20230520235409112.png&#34; alt=&#34;image-20230520235409112&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/image-20230520235422128.png&#34; alt=&#34;image-20230520235422128&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/image-20230520235431091.png&#34; alt=&#34;image-20230520235431091&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;âŒ›ä½¿ç”¨&lt;/h1&gt; &#xA;&lt;p&gt;å°ç™½å¯ä»¥å…ˆç‚¹ä¸ªstarâ­(ğŸ’˜é¡¹ç›®ä¸æ–­æ›´æ–°ä¸­),ç„¶åå»æ—è¾¹&lt;a href=&#34;https://github.com/LC044/WeChatMsg/releases&#34;&gt;Release&lt;/a&gt; ä¸‹è½½æ‰“åŒ…å¥½çš„exeå¯æ‰§è¡Œæ–‡ä»¶ï¼ŒåŒå‡»å³å¯è¿è¡Œ&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;âš ï¸æ³¨æ„ï¼šè‹¥å‡ºç°é—ªé€€æƒ…å†µè¯·å³å‡»é€‰æ‹©ç”¨ç®¡ç†å‘˜èº«ä»½è¿è¡Œexeç¨‹åºï¼Œè¯¥ç¨‹åºä¸å­˜åœ¨ä»»ä½•ç—…æ¯’ï¼Œè‹¥æ€æ¯’è½¯ä»¶æç¤ºæœ‰é£é™©é€‰æ‹©ç•¥è¿‡å³å¯&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ä¸æ‡‚ç¼–ç¨‹çš„è¯·ç§»æ­¥&lt;a href=&#34;https://github.com/LC044/WeChatMsg/releases&#34;&gt;Release&lt;/a&gt;ï¼Œä¸‹é¢çš„ä¸œè¥¿çœ‹äº†å¯èƒ½è¦é•¿è„‘å­å•¦ğŸ¶&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;PCç‰ˆå¾®ä¿¡&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;h3&gt;1. å®‰è£…&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Python&amp;gt;=3.10&#xA;git clone https://github.com/LC044/WeChatMsg&#xA;cd WeChatMsg&#xA;pip install -r requirements_pc.txt -i https://pypi.tuna.tsinghua.edu.cn/simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;2. ä½¿ç”¨&lt;/h3&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;ç™»å½•å¾®ä¿¡&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;æ‰‹æœºç«¯ä½¿ç”¨èŠå¤©è®°å½•è¿ç§»åŠŸèƒ½å°†èŠå¤©æ•°æ®è¿ç§»åˆ°ç”µè„‘ä¸Š&lt;/p&gt; &#xA; &lt;p&gt;æ“ä½œæ­¥éª¤ï¼š&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;å®‰å“ï¼š æ‰‹æœºå¾®ä¿¡-&amp;gt;æˆ‘-&amp;gt;è®¾ç½®-&amp;gt;èŠå¤©-&amp;gt;èŠå¤©è®°å½•è¿ç§»ä¸å¤‡ä»½-&amp;gt;è¿ç§»-&amp;gt; è¿ç§»åˆ°ç”µè„‘å¾®ä¿¡ï¼ˆè¿ç§»å®Œæˆåé‡å¯å¾®ä¿¡ï¼‰&lt;a href=&#34;https://github.com/LC044/WeChatMsg/issues/27&#34;&gt;å¦åˆ™&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;iOSï¼š æ‰‹æœºå¾®ä¿¡-&amp;gt;æˆ‘-&amp;gt;è®¾ç½®-&amp;gt;é€šç”¨-&amp;gt;èŠå¤©è®°å½•è¿ç§»ä¸å¤‡ä»½-&amp;gt;è¿ç§»-&amp;gt; è¿ç§»åˆ°ç”µè„‘å¾®ä¿¡ï¼ˆè¿ç§»å®Œæˆåé‡å¯å¾®ä¿¡ï¼‰&lt;a href=&#34;https://github.com/LC044/WeChatMsg/issues/27&#34;&gt;å¦åˆ™&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;è¿è¡Œç¨‹åº&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python main_pc.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;ç‚¹å‡»è·å–ä¿¡æ¯&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/pc_decrypt_info.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;è®¾ç½®å¾®ä¿¡å®‰è£…è·¯å¾„(å¦‚æœè‡ªåŠ¨è®¾ç½®å¥½äº†å°±&lt;strong&gt;ä¸ç”¨ç®¡&lt;/strong&gt;äº†)&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;å¯ä»¥åˆ°å¾®ä¿¡-&amp;gt;è®¾ç½®-&amp;gt;æ–‡ä»¶ç®¡ç†æŸ¥çœ‹&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/setting.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;ç‚¹å‡»&lt;strong&gt;è®¾ç½®å¾®ä¿¡è·¯å¾„&lt;/strong&gt;æŒ‰é’®ï¼Œé€‰æ‹©è¯¥æ–‡ä»¶å¤¹è·¯å¾„ä¸‹çš„å¸¦æœ‰wxid_xxxçš„è·¯å¾„(æ²¡æœ‰wxidçš„è¯å…ˆé€‰æ‹©å…¶ä¸­ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸å¯¹çš„è¯æ¢å…¶ä»–æ–‡ä»¶å¤¹)&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/path_select.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;ol start=&#34;5&#34;&gt; &#xA;  &lt;li&gt;è·å–åˆ°å¯†é’¥å’Œå¾®ä¿¡è·¯å¾„ä¹‹åç‚¹å‡»å¼€å§‹è§£å¯†&lt;/li&gt; &#xA;  &lt;li&gt;è§£å¯†åçš„æ•°æ®åº“æ–‡ä»¶ä¿å­˜åœ¨./app/DataBase/Msgè·¯å¾„ä¸‹&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;h3&gt;3. æŸ¥çœ‹&lt;/h3&gt; &#xA; &lt;p&gt;éšä¾¿ä¸‹è½½ä¸€ä¸ªSQLiteæ•°æ®åº“æŸ¥çœ‹è½¯ä»¶å°±èƒ½æ‰“å¼€æ•°æ®åº“ï¼Œä¾‹å¦‚&lt;a href=&#34;https://sqlitebrowser.org/dl/&#34;&gt;DB Browser for SQLite&lt;/a&gt; ï¼ˆä¸æ‡‚SQLçš„ç¨å¾®å­¦ä¸‹SQLå’±å†æ¥ï¼Œæˆ–è€…è‡ªåŠ¨è·³è¿‡è¯¥æ­¥éª¤ç›´æ¥å¾€ä¸‹çœ‹æœ€ç»ˆæ•ˆæœï¼‰&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BB%8B%E7%BB%8D.md&#34;&gt;æ•°æ®åº“åŠŸèƒ½ä»‹ç»&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/%E7%94%B5%E8%84%91%E7%AB%AF%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B.md&#34;&gt;æ›´å¤šåŠŸèƒ½ä»‹ç»&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;æ˜¾ç¤ºæ•ˆæœ&lt;/p&gt; &#xA; &lt;img alt=&#34;èŠå¤©ç•Œé¢&#34; src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/chat_.png&#34;&gt; &#xA; &lt;h3&gt;4. pcç«¯åŠŸèƒ½å±•ç¤º&lt;/h3&gt; &#xA; &lt;p&gt;æ­å–œä½ è§£å¯†æˆåŠŸï¼Œæ¥çœ‹çœ‹æ•ˆæœå§~&lt;/p&gt; &#xA; &lt;h4&gt;4.1 æœ€ä¸Šæ–¹å¯¼èˆªæ &lt;/h4&gt; &#xA; &lt;p&gt;å¯ä»¥ç‚¹å‡»è·å–æ•™ç¨‹ï¼Œç›¸å…³ä¿¡æ¯ï¼Œå¯¼å‡ºå…¨éƒ¨ä¿¡æ¯çš„csvæ–‡ä»¶ã€‚&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/main_window.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;h4&gt;4.2 èŠå¤©ç•Œé¢&lt;/h4&gt; &#xA; &lt;p&gt;ç‚¹å‡»&lt;strong&gt;å·¦ä¾§å¯¼èˆªæ â€”â€”&amp;gt;èŠå¤©&lt;/strong&gt; ï¼Œä¼šéšæœºè·³è½¬åˆ°æŸä¸€ä¸ªå¥½å‹çš„ç•Œé¢ï¼Œæ»šè½®æ»šåŠ¨ï¼Œå¯ä»¥å‘ä¸Šç¿»çœ‹æ›´æ—©çš„èŠå¤©è®°å½•ï¼ˆæ¸©é¦¨æç¤ºï¼šå¯èƒ½åœ¨ç¿»çš„æ—¶å€™ä¼šæœ‰å¡é¡¿çš„ç°è±¡ï¼Œæ˜¯å› ä¸ºæ•°æ®åŠ è½½éœ€è¦æ—¶é—´ï¼ŒåæœŸæˆ‘ä»¬ä¼šç»§ç»­ä¼˜åŒ–å—·~ ï¼‰ã€‚ç›®å‰èŠå¤©è®°å½•ä¸­æ–‡å­—ã€å›¾ç‰‡ã€è¡¨æƒ…åŒ…åŸºæœ¬å¯ä»¥æ­£å¸¸æ˜¾ç¤º~&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/chat_window1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;å½“ä½ æƒ³è¦æŸ¥æ‰¾æŸä¸€ä½å¥½å‹çš„ä¿¡æ¯æ—¶ï¼Œå¯ä»¥åœ¨å›¾ä¸­çº¢æ¡†è¾“å…¥ä¿¡æ¯ï¼Œç‚¹å‡»Enterå›è½¦é”®ï¼Œè¿›è¡Œæ£€ç´¢&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/chat_window2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;h4&gt;4.3 å¥½å‹ç•Œé¢&lt;/h4&gt; &#xA; &lt;p&gt;ç‚¹å‡»&lt;strong&gt;å·¦ä¾§å¯¼èˆªæ â€”â€”&amp;gt;å¥½å‹&lt;/strong&gt;ï¼Œä¼šè·³è½¬åˆ°å¥½å‹çš„ç•Œé¢ï¼ŒåŒæ ·å¯ä»¥é€‰æ‹©å¥½å‹ï¼Œå³ä¸Šæ–¹å¯¼èˆªæ ä¸­æœ‰ï¼ˆ1ï¼‰ç»Ÿè®¡ä¿¡æ¯ï¼ˆ2ï¼‰æƒ…æ„Ÿåˆ†æï¼ˆ3ï¼‰å¹´åº¦æŠ¥å‘Šï¼ˆ4ï¼‰é€€å‡ºï¼ˆ5ï¼‰å¯¼å‡ºèŠå¤©è®°å½•ï¼šå¯é€‰æ‹©å¯¼å‡ºä¸ºwordã€csvã€htmlæ ¼å¼ã€‚&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/contact_window.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;åŠŸèƒ½éƒ¨åˆ†æœªé›†æˆæˆ–å¼€å‘ï¼Œè¯·æ‚¨è€å¿ƒç­‰å¾…å‘€~&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;PCç«¯ä½¿ç”¨è¿‡ç¨‹ä¸­éƒ¨åˆ†é—®é¢˜è§£å†³ï¼ˆå¯å‚è€ƒï¼‰&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;h4&gt;ğŸ¤”å¦‚æœæ‚¨åœ¨pcç«¯ä½¿ç”¨çš„æ—¶å€™å‡ºç°é—®é¢˜ï¼Œå¯ä»¥å…ˆå‚è€ƒä»¥ä¸‹æ–¹é¢ï¼Œå¦‚æœä»æœªè§£å†³ï¼Œå¯ä»¥åœ¨ç¾¤é‡Œäº¤æµ~&lt;/h4&gt; &#xA; &lt;p&gt;å¦‚æœæ‚¨é‡åˆ°ä¸‹å›¾æ‰€ç¤ºçš„é—®é¢˜ï¼Œæ˜¯ç”±äºæ²¡æœ‰åœ¨ç”µè„‘ç«¯ç™»å½•å¾®ä¿¡&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/login_wx.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;å¦‚æœæ‚¨é‡åˆ°ä¸‹å›¾æ‰€ç¤ºçš„é—®é¢˜ï¼Œéœ€è¦å…ˆè¿è¡Œ&lt;code&gt;decrypt_window&lt;/code&gt;çš„å¯æ‰§è¡Œæ–‡ä»¶æˆ–è€…æºä»£ç æ–‡ä»¶&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python decrypt_window.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/decrypt_wx.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;å¦‚æœæ‚¨åœ¨è¿è¡Œå¯æ‰§è¡Œç¨‹åºçš„æ—¶å€™å‡ºç°é—ªé€€çš„ç°è±¡ï¼Œè¯·å³å‡»è½¯ä»¶ä½¿ç”¨ç®¡ç†å‘˜æƒé™è¿è¡Œã€‚&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/exe_file.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;å¦‚æœæ‚¨åœ¨è·å–ä¿¡æ¯çš„æ—¶å€™ï¼Œ&lt;code&gt;wxid&lt;/code&gt; æ˜¾ç¤ºnoneï¼Œä½†æ˜¯å¯†é’¥æ˜¯å­˜åœ¨çš„ï¼Œéœ€è¦åœ¨å¾®ä¿¡æ–‡ä»¶ä¿å­˜çš„è·¯å¾„ä¸­é€‰æ‹©è¯¥æ–‡ä»¶å¤¹è·¯å¾„ä¸‹çš„å¸¦æœ‰wxid_xxxçš„åå­—ï¼Œå¡«åˆ°wxidä½ç½®å¹¶ç»§ç»­ç‚¹å‡»&lt;code&gt;å¼€å§‹å¯åŠ¨&lt;/code&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/with_wxid_name.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;å¦‚æœå‡ºç°å¦‚å›¾æ‰€ç¤ºçš„æŠ¥é”™ä¿¡æ¯ï¼Œå°†&lt;code&gt;app/database/msg&lt;/code&gt;æ–‡ä»¶å¤¹åˆ é™¤ï¼Œé‡æ–°è¿è¡Œ&lt;code&gt;main_pc.py&lt;/code&gt;ã€‚&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/err_log.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;å¯¼å‡ºæ•°æ®æˆåŠŸä¹‹åï¼Œè¯¥æ–‡ä»¶ä½ç½®ä¸exeæ–‡ä»¶ä½ç½®ç›¸åŒï¼ˆæˆ–è€…åœ¨æºç .dataæ–‡ä»¶ä¸‹ï¼‰&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/message.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;h4&gt;ğŸ¤”æ³¨æ„&lt;/h4&gt; &#xA; &lt;p&gt;è§£å¯†ä¸€ä¸ªå¾®ä¿¡æ•°æ®åº“ä¹‹åï¼Œç™»å½•æ–°çš„å¾®ä¿¡å¹¶ä¸ä¼šå®æ—¶æ›´æ”¹&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;ä½¿ç”¨æ¨¡æ‹Ÿå™¨ï¼ˆæ”¯æŒå¯è§†åŒ–åˆ†æï¼‰&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;&lt;strong&gt;ä¸æ¨èä½¿ç”¨ï¼ŒPCç«¯å¾®ä¿¡å¯è§†åŒ–åŠŸèƒ½é©¬ä¸Šå®ç°&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;æ ¹æ®&lt;a href=&#34;https://blog.csdn.net/m0_59452630/article/details/124222235?spm=1001.2014.3001.5501&#34;&gt;æ•™ç¨‹&lt;/a&gt;è·å¾—ä¸¤ä¸ªæ–‡ä»¶ &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;auth_info_key_prefs.xmlâ€”â€”è§£ææ•°æ®åº“å¯†ç &lt;/li&gt; &#xA;    &lt;li&gt;EnMicroMsg.dbâ€”â€”èŠå¤©æ•°æ®åº“&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;ä¸Šé¢è¿™ä¸¤ä¸ªæ–‡ä»¶å°±å¯ä»¥&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;å®‰è£…ä¾èµ–åº“&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;pythonç‰ˆæœ¬&amp;gt;=3.10&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;è¯´æ˜:ç”¨åˆ°äº†python3.10çš„matchè¯­æ³•ï¼Œä¸æ–¹ä¾¿æ›´æ¢pythonç‰ˆæœ¬çš„å°ä¼™ä¼´å¯ä»¥æŠŠmatch(è¿è¡ŒæŠ¥é”™çš„åœ°æ–¹)æ›´æ”¹ä¸ºif else&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;å‘½ä»¤è¡Œè¿è¡Œä»¥ä¸‹ä»£ç ï¼ˆ&lt;strong&gt;å»ºè®®ä½¿ç”¨Pycharmæ‰“å¼€é¡¹ç›®ï¼ŒPycharmä¼šè‡ªåŠ¨é…ç½®å¥½æ‰€æœ‰ä¸œè¥¿ï¼Œç›´æ¥è¿è¡Œmain.pyå³å¯&lt;/strong&gt;ï¼‰&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;è¿è¡Œmain.py&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;å‡ºç°è§£å¯†ç•Œé¢&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/image-20230521001305274.png&#34; alt=&#34;image-20230521001305274&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;æŒ‰ç…§æç¤ºé€‰æ‹©ä¸Šé¢è·å¾—çš„ä¸¤ä¸ªæ–‡ä»¶ï¼Œç­‰å¾…è§£å¯†å®Œæˆï¼Œé‡æ–°è¿è¡Œç¨‹åº&lt;/p&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;è¿›å…¥ä¸»ç•Œé¢&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;è¿™æ—¶å€™ä¸æ˜¾ç¤ºå¤´åƒï¼Œå› ä¸ºå¤´åƒæ–‡ä»¶æ²¡æœ‰å¯¼å…¥è¿›æ¥&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/image-20230521001547481.png&#34; alt=&#34;image-20230521001547481&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;æ ¹æ®&lt;a href=&#34;https://blog.csdn.net/m0_59452630/article/details/124222235?spm=1001.2014.3001.5501&#34;&gt;æ•™ç¨‹&lt;/a&gt; å°†å¤´åƒæ–‡ä»¶å¤¹avatarå¤åˆ¶åˆ°å·¥ç¨‹ç›®å½•./app/data/ç›®å½•ä¸‹&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/image-20230521001726799.png&#34; alt=&#34;image-20230521001726799&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;å¦‚æœæƒ³è¦æ˜¾ç¤ºèŠå¤©å›¾åƒå°±æŠŠ&lt;a href=&#34;https://blog.csdn.net/m0_59452630/article/details/124222235?spm=1001.2014.3001.5501&#34;&gt;æ•™ç¨‹&lt;/a&gt; é‡Œçš„image2æ–‡ä»¶å¤¹å¤åˆ¶åˆ°./app/dataæ–‡ä»¶å¤¹é‡Œï¼Œæ•ˆæœè·Ÿä¸Šå›¾ä¸€æ ·&lt;/p&gt; &#xA; &lt;p&gt;å¤åˆ¶è¿›æ¥ä¹‹åå†è¿è¡Œç¨‹åºå°±æœ‰å›¾åƒäº†&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/image-20230520235113261.png&#34; alt=&#34;image-20230520235113261&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;é¡¹ç›®è¿˜æœ‰å¾ˆå¤šbugï¼Œå¸Œæœ›å¤§å®¶èƒ½å¤ŸåŠæ—¶åé¦ˆ&lt;/h2&gt; &#xA;&lt;p&gt;é¡¹ç›®åœ°å€ï¼š&lt;a href=&#34;https://github.com/LC044/WeChatMsg&#34;&gt;https://github.com/LC044/WeChatMsg&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ğŸ†è‡´è°¢&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PCå¾®ä¿¡è§£å¯†å·¥å…·:&lt;a href=&#34;https://github.com/xaoyaoo/PyWxDump&#34;&gt;https://github.com/xaoyaoo/PyWxDump&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PyQtç»„ä»¶åº“:&lt;a href=&#34;https://github.com/PyQt5/CustomWidgets&#34;&gt;https://github.com/PyQt5/CustomWidgets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;æˆ‘çš„å¾—åŠ›åŠ©æ‰‹:&lt;a href=&#34;https://chat.openai.com/&#34;&gt;ChatGPT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;å£°æ˜ï¼šè¯¥é¡¹ç›®æœ‰ä¸”ä»…æœ‰ä¸€ä¸ªç›®çš„ï¼šâ€œç•™ç—•â€â€”â€”æˆ‘çš„æ•°æ®æˆ‘åšä¸»ï¼Œå‰ææ˜¯â€œæˆ‘çš„æ•°æ®â€å…¶æ¬¡æ‰æ˜¯â€œæˆ‘åšä¸»â€ï¼Œç¦æ­¢ä»»ä½•äººä»¥ä»»ä½•å½¢å¼å°†å…¶ç”¨äºä»»ä½•éæ³•ç”¨é€”ï¼Œå¯¹äºä½¿ç”¨è¯¥ç¨‹åºæ‰€é€ æˆçš„ä»»ä½•åæœï¼Œæ‰€æœ‰åˆ›ä½œè€…ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ğŸ™„&lt;br&gt; è¯¥è½¯ä»¶ä¸ä¼šå¯¹æ‚¨ä½¿ç”¨çš„å¾®ä¿¡é€ æˆä»»ä½•å½±å“ï¼Œæ›´ä¸ä¼šå¯¹ä»–äººçš„å¾®ä¿¡é€ æˆä»»ä½•å½±å“ï¼Œä¸èƒ½æ‰¾å›åˆ é™¤çš„èŠå¤©è®°å½•ï¼Œä»»ä½•ä¼å›¾ç¯¡æ”¹å¾®ä¿¡èŠå¤©æ•°æ®çš„æƒ³æ³•éƒ½æ˜¯æ— ç¨½ä¹‹è°ˆã€‚æœ¬é¡¹ç›®æ‰€æœ‰åŠŸèƒ½å‡å»ºç«‹åœ¨â€å‰è¨€â€œçš„åŸºç¡€ä¹‹ä¸Šï¼ŒåŸºäºè¯¥é¡¹ç›®çš„æ‰€æœ‰å¼€å‘è€…å‡ä¸èƒ½æ¥å—ä»»ä½•æœ‰æ‚–äºâ€å‰è¨€â€œçš„åŠŸèƒ½éœ€æ±‚ï¼Œè¿è€…åæœè‡ªè´Ÿã€‚&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/?utm_source=bestxtools.com#LC044/WeChatMsg&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=LC044/WeChatMsg&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ğŸ„æ¸©é¦¨æç¤º&lt;/h1&gt; &#xA;&lt;p&gt;å¦‚æœæ‚¨åœ¨ä½¿ç”¨è¯¥è½¯ä»¶çš„è¿‡ç¨‹ä¸­&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å‘ç°æ–°çš„bug&lt;/li&gt; &#xA; &lt;li&gt;æœ‰æ–°çš„åŠŸèƒ½è¯‰æ±‚&lt;/li&gt; &#xA; &lt;li&gt;æ“ä½œæ¯”è¾ƒç¹ç&lt;/li&gt; &#xA; &lt;li&gt;è§‰å¾—UIä¸å¤Ÿç¾è§‚&lt;/li&gt; &#xA; &lt;li&gt;ç­‰å…¶ä»–ç»™æ‚¨é€ æˆå›°æ‰°çš„åœ°æ–¹&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;è¯·æèµ·&lt;a href=&#34;https://github.com/LC044/WeChatMsg/issues&#34;&gt;issue&lt;/a&gt;æˆ–è€…æ·»åŠ QQç¾¤(è¿›ç¾¤å‰å…ˆç‚¹ä¸ªâ­å“¦):&lt;a href=&#34;https://raw.githubusercontent.com/LC044/WeChatMsg/master/doc/images/qq.jpg&#34;&gt;&lt;strong&gt;474379264&lt;/strong&gt;&lt;/a&gt; ï¼Œæˆ‘å°†å°½å¿«ä¸ºæ‚¨è§£å†³é—®é¢˜&lt;/p&gt; &#xA;&lt;p&gt;å¦‚æœæ‚¨æ˜¯ä¸€åå¼€å‘è€…ï¼Œæœ‰æ–°çš„æƒ³æ³•æˆ–å»ºè®®ï¼Œæ¬¢è¿&lt;a href=&#34;https://github.com/LC044/WeChatMsg/forks&#34;&gt;fork&lt;/a&gt; è¯¥é¡¹ç›®å¹¶å‘èµ·&lt;a href=&#34;https://github.com/LC044/WeChatMsg/pulls&#34;&gt;PR&lt;/a&gt;ï¼Œæˆ‘å°†æŠŠæ‚¨çš„åå­—å†™å…¥è´¡çŒ®è€…åå•ä¸­&lt;/p&gt; &#xA;&lt;p&gt;å¦å¤–æœ¬äººç¡¬ä»¶é…ç½®ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;åœ¨æ ¡å¤§å­¦ç”Ÿä¸€æšï¼Œæ—¶é—´æœ‰é™ã€ç²¾åŠ›æœ‰é™ã€èƒ½åŠ›æœ‰é™&lt;/li&gt; &#xA; &lt;li&gt;24å¯¸1080på±å¹•ä¸€ä¸ª(ç¼©æ”¾ç‡100%ï¼Œä¸æ˜¯500%)&lt;/li&gt; &#xA; &lt;li&gt;CPU:AMD 6800H&lt;/li&gt; &#xA; &lt;li&gt;å†…å­˜:64G(é‚£äº›è¯´æ€§èƒ½å·®çˆ†å†…å­˜çš„æœ‰æ²¡æœ‰ä»è‡ªèº«æ‰¾åŸå› ï¼Œè¿™ä¹ˆå¤šå¹´äº†ï¼Œæœ‰æ²¡æœ‰åŠªåŠ›ï¼Œå†…å­˜å®¹é‡æ¶¨æ²¡æ¶¨)&lt;/li&gt; &#xA; &lt;li&gt;æ“ä½œç³»ç»Ÿ:Win11(å°±ä¿©å­—â€œå¥½çœ‹â€)&lt;/li&gt; &#xA; &lt;li&gt;æ‰€ä»¥ä»»ä½•è¶…å‡ºç¡¬ä»¶æ”¯æŒä¹‹å¤–çš„éœ€æ±‚åœ¨æˆ‘ç¡¬ä»¶æ²¡å˜ä¹‹å‰éƒ½ä¸ä¼šé€‚é…&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/seamless_communication</title>
    <updated>2023-12-10T01:48:56Z</updated>
    <id>tag:github.com,2023-12-10:/facebookresearch/seamless_communication</id>
    <link href="https://github.com/facebookresearch/seamless_communication" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Foundational Models for State-of-the-Art Speech and Text Translation&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/23-11_SEAMLESS_BlogHero_11.17.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Seamless Intro&lt;/h1&gt; &#xA;&lt;h2&gt;SeamlessM4T&lt;/h2&gt; &#xA;&lt;p&gt;SeamlessM4T is our foundational all-in-one &lt;strong&gt;M&lt;/strong&gt;assively &lt;strong&gt;M&lt;/strong&gt;ultilingual and &lt;strong&gt;M&lt;/strong&gt;ultimodal &lt;strong&gt;M&lt;/strong&gt;achine &lt;strong&gt;T&lt;/strong&gt;ranslation model delivering high-quality translation for speech and text in nearly 100 languages.&lt;/p&gt; &#xA;&lt;p&gt;SeamlessM4T models support the tasks of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speech-to-speech translation (S2ST)&lt;/li&gt; &#xA; &lt;li&gt;Speech-to-text translation (S2TT)&lt;/li&gt; &#xA; &lt;li&gt;Text-to-speech translation (T2ST)&lt;/li&gt; &#xA; &lt;li&gt;Text-to-text translation (T2TT)&lt;/li&gt; &#xA; &lt;li&gt;Automatic speech recognition (ASR)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;ğŸŒŸ&lt;/span&gt; We are releasing SeamlessM4T v2, an updated version with our novel &lt;em&gt;UnitY2&lt;/em&gt; architecture. This new model improves over SeamlessM4T v1 in quality as well as inference latency in speech generation tasks.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about the collection of SeamlessM4T models, the approach used in each, their language coverage and their performance, visit the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/m4t/README.md&#34;&gt;SeamlessM4T README&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-v2-large&#34;&gt;ğŸ¤— Model Card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;SeamlessExpressive&lt;/h2&gt; &#xA;&lt;p&gt;SeamlessExpressive is a speech-to-speech translation model that captures certain underexplored aspects of prosody such as speech rate and pauses, while preserving the style of one&#39;s voice and high content translation quality.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about SeamlessExpressive models, visit the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/expressive/README.md&#34;&gt;SeamlessExpressive README&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/facebook/seamless-expressive&#34;&gt;ğŸ¤— Model Card&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;SeamlessStreaming&lt;/h2&gt; &#xA;&lt;p&gt;SeamlessStreaming is a streaming translation model. The model supports speech as input modality and speech/text as output modalities.&lt;/p&gt; &#xA;&lt;p&gt;The SeamlessStreaming model supports the following tasks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speech-to-speech translation (S2ST)&lt;/li&gt; &#xA; &lt;li&gt;Speech-to-text translation (S2TT)&lt;/li&gt; &#xA; &lt;li&gt;Automatic speech recognition (ASR)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To learn more about SeamlessStreaming models, visit the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/streaming/README.md&#34;&gt;SeamlessStreaming README&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/facebook/seamless-streaming&#34;&gt;ğŸ¤— Model Card&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Seamless&lt;/h2&gt; &#xA;&lt;p&gt;The Seamless model is the unified model for expressive streaming speech-to-speech translations.&lt;/p&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;h3&gt;Blog&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ai.meta.com/research/seamless-communication/&#34;&gt;AI at Meta Blog&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Papers&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ai.facebook.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/&#34;&gt;Seamless&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ai.meta.com/research/publications/efficient-monotonic-multihead-attention/&#34;&gt;EMMA&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ai.meta.com/research/publications/sonar-expressive-zero-shot-expressive-speech-to-speech-translation/&#34;&gt;SONAR&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demos&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;SeamlessM4T v2&lt;/th&gt; &#xA;   &lt;th&gt;SeamlessExpressive&lt;/th&gt; &#xA;   &lt;th&gt;SeamlessStreaming&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Demo&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://seamless.metademolab.com/m4t?utm_source=github&amp;amp;utm_medium=web&amp;amp;utm_campaign=seamless&amp;amp;utm_content=readme&#34;&gt;SeamlessM4T v2 Demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://seamless.metademolab.com/expressive?utm_source=github&amp;amp;utm_medium=web&amp;amp;utm_campaign=seamless&amp;amp;utm_content=readme&#34;&gt;SeamlessExpressive Demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HuggingFace Space Demo&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/facebook/seamless-m4t-v2-large&#34;&gt;ğŸ¤— SeamlessM4T v2 Space&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/facebook/seamless-expressive&#34;&gt;ğŸ¤— SeamlessExpressive Space&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/facebook/seamless-streaming&#34;&gt;ğŸ¤— SeamlessStreaming Space&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;What&#39;s new&lt;/h2&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] One of the prerequisites is &lt;a href=&#34;https://github.com/facebookresearch/fairseq2&#34;&gt;fairseq2&lt;/a&gt; which has pre-built packages available only for Linux x86-64 and Apple-silicon Mac computers. In addition it has a dependency on &lt;a href=&#34;https://github.com/libsndfile/libsndfile&#34;&gt;libsndfile&lt;/a&gt; which might not be installed on your machine. If you experience any installation issues, please refer to its &lt;a href=&#34;https://github.com/facebookresearch/fairseq2&#34;&gt;README&lt;/a&gt; for further instructions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Transcribing inference audio for computing metric uses &lt;a href=&#34;https://github.com/openai/whisper#setup&#34;&gt;Whisper&lt;/a&gt;, which is automatically installed. Whisper in turn requires the command-line tool &lt;a href=&#34;https://ffmpeg.org/&#34;&gt;&lt;code&gt;ffmpeg&lt;/code&gt;&lt;/a&gt; to be installed on your system, which is available from most package managers.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Running inference&lt;/h2&gt; &#xA;&lt;h3&gt;SeamlessM4T Inference&lt;/h3&gt; &#xA;&lt;p&gt;Hereâ€™s an example of using the CLI from the root directory to run inference.&lt;/p&gt; &#xA;&lt;p&gt;S2ST task:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;m4t_predict &amp;lt;path_to_input_audio&amp;gt; --task s2st --tgt_lang &amp;lt;tgt_lang&amp;gt; --output_path &amp;lt;path_to_save_audio&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;T2TT task:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;m4t_predict &amp;lt;input_text&amp;gt; --task t2tt --tgt_lang &amp;lt;tgt_lang&amp;gt; --src_lang &amp;lt;src_lang&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/cli/m4t/predict&#34;&gt;inference README&lt;/a&gt; for detailed instruction on how to run inference and the list of supported languages on the source, target sides for speech, text modalities.&lt;/p&gt; &#xA;&lt;p&gt;For running S2TT/ASR natively (without Python) using GGML, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/#unitycpp&#34;&gt;the unity.cpp section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;SeamlessExpressive Inference&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Please check the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/#seamlessexpressive-models&#34;&gt;section&lt;/a&gt; on how to download the model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Below is the script for efficient batched inference.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export MODEL_DIR=&#34;/path/to/SeamlessExpressive/model&#34;&#xA;export TEST_SET_TSV=&#34;input.tsv&#34; # Your dataset in a TSV file, with headers &#34;id&#34;, &#34;audio&#34;&#xA;export TGT_LANG=&#34;spa&#34; # Target language to translate into, options including &#34;fra&#34;, &#34;deu&#34;, &#34;eng&#34; (&#34;cmn&#34; and &#34;ita&#34; are experimental)&#xA;export OUTPUT_DIR=&#34;tmp/&#34; # Output directory for generated text/unit/waveform&#xA;export TGT_TEXT_COL=&#34;tgt_text&#34; # The column in your ${TEST_SET_TSV} for reference target text to calcuate BLEU score. You can skip this argument.&#xA;export DFACTOR=&#34;1.0&#34; # Duration factor for model inference to tune predicted duration (preddur=DFACTOR*preddur) per each position which affects output speech rate. Greater value means slower speech rate (default to 1.0). See expressive evaluation README for details on duration factor we used.&#xA;python src/seamless_communication/cli/expressivity/evaluate/pretssel_inference.py \&#xA;  ${TEST_SET_TSV} --gated-model-dir ${MODEL_DIR} --task s2st --tgt_lang ${TGT_LANG}\&#xA;  --audio_root_dir &#34;&#34; --output_path ${OUTPUT_DIR} --ref_field ${TGT_TEXT_COL} \&#xA;  --model_name seamless_expressivity --vocoder_name vocoder_pretssel \&#xA;  --text_unk_blocking True --duration_factor ${DFACTOR}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SeamlessStreaming and Seamless Inference&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/cli/streaming&#34;&gt;Streaming Evaluation README&lt;/a&gt; has detailed instructions for running evaluations for the SeamlessStreaming and Seamless models. The CLI has an &lt;code&gt;--no-scoring&lt;/code&gt; option that can be used to skip the scoring part and just run inference.&lt;/p&gt; &#xA;&lt;h2&gt;Running SeamlessStreaming Demo&lt;/h2&gt; &#xA;&lt;p&gt;You can duplicate the &lt;a href=&#34;https://huggingface.co/spaces/facebook/seamless-streaming?duplicate=true&#34;&gt;SeamlessStreaming HF space&lt;/a&gt; to run the streaming demo.&lt;/p&gt; &#xA;&lt;p&gt;You can also run the demo locally, by cloning the space from &lt;a href=&#34;https://huggingface.co/spaces/facebook/seamless-streaming/tree/main&#34;&gt;here&lt;/a&gt;. See the &lt;a href=&#34;https://huggingface.co/spaces/facebook/seamless-streaming/blob/main/README.md&#34;&gt;README&lt;/a&gt; of the SeamlessStreaming HF repo for more details on installation.&lt;/p&gt; &#xA;&lt;h2&gt;Running SeamlessM4T &amp;amp; SeamlessExpressive &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt; demos locally&lt;/h2&gt; &#xA;&lt;p&gt;To launch the same demo Space we host on Hugging Face locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd demo&#xA;pip install -r requirements.txt&#xA;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Seamless M4T is also available in the ğŸ¤— Transformers library. For more details, refer to the &lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/model_doc/seamless_m4t_v2&#34;&gt;SeamlessM4T docs&lt;/a&gt; or this hands-on &lt;a href=&#34;https://colab.research.google.com/github/ylacombe/explanatory_notebooks/blob/main/seamless_m4t_hugging_face.ipynb&#34;&gt;Google Colab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Resources and usage&lt;/h1&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;h3&gt;SeamlessM4T models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th&gt;#params&lt;/th&gt; &#xA;   &lt;th&gt;checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;metrics&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeamlessM4T-Large v2&lt;/td&gt; &#xA;   &lt;td&gt;2.3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-v2-large&#34;&gt;ğŸ¤— Model card&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-v2-large/resolve/main/seamlessM4T_v2_large.pt&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_large_v2.zip&#34;&gt;metrics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeamlessM4T-Large (v1)&lt;/td&gt; &#xA;   &lt;td&gt;2.3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-large&#34;&gt;ğŸ¤— Model card&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-large/resolve/main/multitask_unity_large.pt&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_large.zip&#34;&gt;metrics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeamlessM4T-Medium (v1)&lt;/td&gt; &#xA;   &lt;td&gt;1.2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-medium&#34;&gt;ğŸ¤— Model card&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-medium/resolve/main/multitask_unity_medium.pt&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_medium.zip&#34;&gt;metrics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;SeamlessExpressive models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/facebook/seamless-expressive&#34;&gt;ğŸ¤— Model card&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To access and download SeamlessExpressive, please request the model artifacts through &lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/seamless-downloads/&#34;&gt;this request form&lt;/a&gt;. Upon approval, you will then receive an email with download links to each model artifact.&lt;/p&gt; &#xA;&lt;p&gt;Please note that SeamlessExpressive is made available under its own &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/SEAMLESS_LICENSE&#34;&gt;License&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/ACCEPTABLE_USE_POLICY&#34;&gt;Acceptable Use Policy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;SeamlessStreaming models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th&gt;#params&lt;/th&gt; &#xA;   &lt;th&gt;checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;metrics&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeamlessStreaming&lt;/td&gt; &#xA;   &lt;td&gt;2.5B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/seamless-streaming&#34;&gt;ğŸ¤— Model card&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/facebook/seamless-streaming/resolve/main/seamless_streaming_monotonic_decoder.pt&#34;&gt;monotonic decoder checkpoint&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/facebook/seamless-streaming/resolve/main/seamless_streaming_unity.pt&#34;&gt;streaming UnitY2 checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/seamless/metrics/streaming/seamless_streaming.zip&#34;&gt;metrics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Seamless models&lt;/h3&gt; &#xA;&lt;p&gt;Seamless model is simply the SeamlessStreaming model with the non-expressive &lt;code&gt;vocoder_v2&lt;/code&gt; swapped out with the expressive &lt;code&gt;vocoder_pretssel&lt;/code&gt;. Please check out above &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/#seamlessexpressive-models&#34;&gt;section&lt;/a&gt; on how to acquire &lt;code&gt;vocoder_pretssel&lt;/code&gt; checkpoint.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;SeamlessM4T Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;To reproduce our results, or to evaluate using the same metrics over your own test sets, please check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/cli/m4t/evaluate&#34;&gt;README here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;SeamlessExpressive Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Please check out this &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/expressive/README.md#automatic-evaluation&#34;&gt;README section&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;SeamlessStreaming and Seamless Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/cli/streaming&#34;&gt;Streaming Evaluation README&lt;/a&gt; has detailed instructions for running evaluations on the SeamlessStreaming and Seamless models.&lt;/p&gt; &#xA;&lt;h2&gt;Unity.cpp&lt;/h2&gt; &#xA;&lt;p&gt;To enable Seamless Communication Everywhere, we implemented unity.cpp so users could run SeamlessM4T models in GGML - a C tensor library allowing easier integration on verbose platforms.&lt;/p&gt; &#xA;&lt;p&gt;To transcribe/translte a given audio,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./ggml/bin/unity --model seamlessM4T_medium.ggml input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For details of build and more usage please check out &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/ggml&#34;&gt;unity.cpp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Expressive Datasets&lt;/h2&gt; &#xA;&lt;p&gt;We created two expressive speech-to-speech translation datasets, mExpresso and mDRAL, between English and five other languages -- French, German, Italian, Mandarin and Spanish. We currently open source the speech-to-text of mExpresso for out-of-English directions, and we will open source the remaining part of the datasets soon. For details, please check out &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/expressive/README.md#benchmark-datasets&#34;&gt;README&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;SeamlessAlignExpressive&lt;/h3&gt; &#xA;&lt;p&gt;Weâ€™re introducing the first expressive speech alignment procedure. Starting with raw data, the expressive alignment procedure automatically discovers pairs of audio segments sharing not only the same meaning, but the same overall expressivity. To showcase this procedure, we are making metadata available to create a benchmarking dataset called SeamlessAlignExpressive, that can be used to validate the quality of our alignment method. SeamlessAlignExpressive is the first large-scale (11k+ hours) collection of multilingual audio alignments for expressive translation. More details can be found on the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/expressive/seamless_align_expressive_README.md&#34;&gt;SeamlessAlignExpressive README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Converting raw audio to units&lt;/h2&gt; &#xA;&lt;p&gt;Please check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/cli/m4t/audio_to_units&#34;&gt;README here&lt;/a&gt;. Note that SeamlessM4T v1 model uses reduced units and other models use non-reduced units.&lt;/p&gt; &#xA;&lt;h1&gt;Libraries&lt;/h1&gt; &#xA;&lt;p&gt;Seamless Communication depends on 4 libraries developed by Meta.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/facebookresearch/fairseq2&#34;&gt;fairseq2&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;fairseq2 is our next-generation open-source library of sequence modeling components that provides researchers and developers with building blocks for machine translation, language modeling, and other sequence generation tasks. All SeamlessM4T models in this repository are powered by fairseq2.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/facebookresearch/SONAR&#34;&gt;SONAR and BLASER 2.0&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;SONAR, Sentence-level multimOdal and laNguage-Agnostic Representations is a new multilingual and -modal sentence embedding space which outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim and xsim++ multilingual similarity search tasks. SONAR provides text and speech encoders for many languages. SeamlessAlign was mined based on SONAR embeddings.&lt;/p&gt; &#xA;&lt;p&gt;BLASER 2.0 is our latest model-based evaluation metric for multimodal translation. It is an extension of BLASER, supporting both speech and text. It operates directly on the source signal, and as such, does not require any intermediate ASR system like ASR-BLEU. As in the first version, BLASER 2.0 leverages the similarity between input and output sentence embeddings. SONAR is the underlying embedding space for BLASER 2.0. Scripts to run evaluation with BLASER 2.0 can be found in the &lt;a href=&#34;https://github.com/facebookresearch/SONAR&#34;&gt;SONAR repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/facebookresearch/stopes&#34;&gt;stopes&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;As part of the seamless communication project, we&#39;ve extended the stopes library. Version 1 provided a text-to-text mining tool to build training dataset for translation models. Version 2 has been extended thanks to SONAR, to support tasks around training large speech translation models. In particular, we provide tools to read/write the fairseq audiozip datasets and a new mining pipeline that can do speech-to-speech, text-to-speech, speech-to-text and text-to-text mining, all based on the new SONAR embedding space.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/facebookresearch/SimulEval&#34;&gt;SimulEval&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;SimulEval is a library used for evaluating simulaneous translation models. SimulEval also provides a backend for generation using partial/incremental inputs with flexible/extensible states, which is used to implement streaming inference. Users define agents which implement SimulEval&#39;s interface, which can be connected together in a pipeline. You can find agents implemented for SeamlessStreaming &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/streaming/agents&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;[Legacy] SeamlessM4T v1 instructions&lt;/h2&gt; &#xA;&lt;h4&gt;Finetuning SeamlessM4T v1 models&lt;/h4&gt; &#xA;&lt;p&gt;Please check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/cli/m4t/finetune&#34;&gt;README here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;On-device models&lt;/h4&gt; &#xA;&lt;p&gt;Apart from Seamless-M4T large (2.3B) and medium (1.2B) models, we are also releasing a small model (281M) targeted for on-device inference. To learn more about the usage and model details check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/m4t/on_device_README.md&#34;&gt;README here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;SeamlessAlign mined dataset&lt;/h4&gt; &#xA;&lt;p&gt;We open-source the metadata to SeamlessAlign, the largest open dataset for multimodal translation, totaling 270k+ hours of aligned Speech and Text data. The dataset can be rebuilt by the community based on the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/m4t/seamless_align_README.md&#34;&gt;SeamlessAlign readme&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use Seamless in your work or any models/datasets/artifacts published in Seamless, please cite :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{seamless2023,&#xA;   title=&#34;Seamless: Multilingual Expressive and Streaming Speech Translation&#34;,&#xA;   author=&#34;{Seamless Communication}, Lo{\&#34;i}c Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma, Christopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan Mavlyutov, Alice Rakotoarison, Kaushik Ram Sadagopan, Abinesh Ramakrishnan, Tuan Tran, Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernandez, Cynthia Gao, Prangthip Hansanti, Elahe Kalbassi, Amanda Kallet, Artyom Kozhevnikov, Gabriel Mejia, Robin San Roman, Christophe Touret, Corinne Wong, Carleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen Chen, Marta R. Costa-juss{\`a}, Maha Elbayad, Hongyu Gong, Francisco Guzm{\&#39;a}n, Kevin Heffernan, Somya Jain, Justine Kao, Ann Lee, Xutai Ma, Alex Mourachko, Benjamin Peloquin, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Anna Sun, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang, Mary Williamson&#34;,&#xA;  journal={ArXiv},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;We have three license categories.&lt;/p&gt; &#xA;&lt;p&gt;The following non-generative components are MIT licensed as found in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/MIT_LICENSE&#34;&gt;MIT_LICENSE&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Code&lt;/li&gt; &#xA; &lt;li&gt;Text only part of the mExpresso dataset found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/expressive/README.md&#34;&gt;SeamlessExpressive README&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;UnitY2 forced alignment extractor found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/m4t/unity2_aligner_README.md&#34;&gt;UnitY2 Aligner README&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Speech toxicity tool with the etox dataset found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/src/seamless_communication/cli/toxicity&#34;&gt;Toxicity README&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following models are CC-BY-NC 4.0 licensed as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SeamlessM4T models (v1 and v2).&lt;/li&gt; &#xA; &lt;li&gt;SeamlessStreaming models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following models are Seamless licensed as found in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/SEAMLESS_LICENSE&#34;&gt;SEAMLESS_LICENSE&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Seamless models.&lt;/li&gt; &#xA; &lt;li&gt;SeamlessExpressive models.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>cubiq/ComfyUI_IPAdapter_plus</title>
    <updated>2023-12-10T01:48:56Z</updated>
    <id>tag:github.com,2023-12-10:/cubiq/ComfyUI_IPAdapter_plus</id>
    <link href="https://github.com/cubiq/ComfyUI_IPAdapter_plus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI IPAdapter plus&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt; reference implementation for &lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter/&#34;&gt;IPAdapter&lt;/a&gt; models.&lt;/p&gt; &#xA;&lt;p&gt;IPAdapter implementation that follows the ComfyUI way of doing things. The code is memory efficient, fast, and shouldn&#39;t break with Comfy updates.&lt;/p&gt; &#xA;&lt;h2&gt;Important updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/12/05&lt;/strong&gt;: Added &lt;code&gt;batch embeds&lt;/code&gt; node. This lets you encode images in batches and merge them together into an &lt;code&gt;IPAdapter Apply Encoded&lt;/code&gt; node. Useful mostly for animations because the clip vision encoder takes a lot of VRAM. My suggestion is to split the animation in batches of about 120 frames.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/29&lt;/strong&gt;: Added &lt;code&gt;unfold_batch&lt;/code&gt; option to send the reference images sequentially to a latent batch. Useful for animations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/26&lt;/strong&gt;: Added &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/#timestepping&#34;&gt;timestepping&lt;/a&gt;. You may need to delete the old nodes and recreate them. &lt;strong&gt;Important:&lt;/strong&gt; For this to work you need to update ComfyUI to the latest version.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/24&lt;/strong&gt;: Support for multiple attention masks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/23&lt;/strong&gt;: Small but important update: the new default location for the IPAdapter models is &lt;code&gt;ComfyUI/models/ipadapter&lt;/code&gt;. &lt;strong&gt;No panic&lt;/strong&gt;: the legacy &lt;code&gt;ComfyUI/custom_nodes/ComfyUI_IPAdapter_plus/models&lt;/code&gt; location still works and nothing will break.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/08&lt;/strong&gt;: Added &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/#attention-masking&#34;&gt;attention masking&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/07&lt;/strong&gt;: Added three ways to apply the weight. &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/#weight-types&#34;&gt;See below&lt;/a&gt; for more info. &lt;strong&gt;This might break things!&lt;/strong&gt; Please let me know if you are having issues. When loading an old workflow try to reload the page a couple of times or delete the &lt;code&gt;IPAdapter Apply&lt;/code&gt; node and insert a new one.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/02&lt;/strong&gt;: Added compatibility with the new models in safetensors format (available on &lt;a href=&#34;https://huggingface.co/h94/IP-Adapter&#34;&gt;huggingface&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/10/12&lt;/strong&gt;: Added image weighting in the &lt;code&gt;IPAdapterEncoder&lt;/code&gt; node. This update is somewhat breaking; if you use &lt;code&gt;IPAdapterEncoder&lt;/code&gt; and &lt;code&gt;PrepImageForClipVision&lt;/code&gt; nodes you need to remove them from your workflow, refresh and recreate them. In the examples you&#39;ll find a &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_weighted.json&#34;&gt;workflow&lt;/a&gt; for weighted images.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;(previous updates removed for better readability)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is it?&lt;/h2&gt; &#xA;&lt;p&gt;The IPAdapter are very powerful models for image-to-image conditioning. Given a reference image you can do variations augmented by text prompt, controlnets and masks. Think of it as a 1-image lora.&lt;/p&gt; &#xA;&lt;h2&gt;Example workflow&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/ipadapter_workflow.png&#34; alt=&#34;IPAdapter Example workflow&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Video Introduction&lt;/h2&gt; &#xA;&lt;a href=&#34;https://youtu.be/7m9ZZFU3HWo&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.youtube.com/vi/7m9ZZFU3HWo/hqdefault.jpg&#34; alt=&#34;Watch the video&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ğŸ¤“&lt;/span&gt; &lt;a href=&#34;https://youtu.be/7m9ZZFU3HWo&#34;&gt;Basic usage video&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ğŸš€&lt;/span&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=mJQ62ly7jrg&#34;&gt;Advanced features video&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ğŸ‘º&lt;/span&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=vqG1VXKteQg&#34;&gt;Attention Masking video&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ğŸ¥&lt;/span&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=ddYbhv3WgWw&#34;&gt;Animation Features video&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download or git clone this repository inside &lt;code&gt;ComfyUI/custom_nodes/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;The pre-trained models are available on &lt;a href=&#34;https://huggingface.co/h94/IP-Adapter&#34;&gt;huggingface&lt;/a&gt;, download and place them in the &lt;code&gt;ComfyUI/models/ipadapter&lt;/code&gt; directory (create it if not present). You can also use any custom location setting an &lt;code&gt;ipadapter&lt;/code&gt; entry in the &lt;code&gt;extra_model_paths.yaml&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;Note: the legacy &lt;code&gt;ComfyUI/custom_nodes/ComfyUI_IPAdapter_plus/models&lt;/code&gt; is still supported and it will be ignored only if the global directory is present.&lt;/p&gt; &#xA;&lt;p&gt;For SD1.5 you need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15.bin&#34;&gt;ip-adapter_sd15.bin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/blob/main/models/ip-adapter_sd15_light.safetensors&#34;&gt;ip-adapter_sd15_light.bin&lt;/a&gt;, use this when text prompt is more important than reference images&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-plus_sd15.bin&#34;&gt;ip-adapter-plus_sd15.bin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-plus-face_sd15.bin&#34;&gt;ip-adapter-plus-face_sd15.bin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-full-face_sd15.bin&#34;&gt;ip-adapter-full-face_sd15.bin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15_vit-G.bin&#34;&gt;ip-adapter_sd15_vit-G.bin&lt;/a&gt;, this model requires the vit-bigG image encoder (the SDXL one below)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For SDXL you need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl.bin&#34;&gt;ip-adapter_sdxl.bin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl_vit-h.bin&#34;&gt;ip-adapter_sdxl_vit-h.bin&lt;/a&gt; &lt;strong&gt;This model requires the use of the SD1.5 encoder despite being for SDXL checkpoints&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter-plus_sdxl_vit-h.bin&#34;&gt;ip-adapter-plus_sdxl_vit-h.bin&lt;/a&gt; Same as above, use the SD1.5 encoder&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter-plus-face_sdxl_vit-h.bin&#34;&gt;ip-adapter-plus-face_sdxl_vit-h.bin&lt;/a&gt; As always, use the SD1.5 encoder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please note that now the models are also available in safetensors format, you can find them on &lt;a href=&#34;https://huggingface.co/h94/IP-Adapter&#34;&gt;huggingface&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Additionally you need the image encoders to be placed in the &lt;code&gt;ComfyUI/models/clip_vision/&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/image_encoder/model.safetensors&#34;&gt;SD 1.5 model&lt;/a&gt; (use this also for all models ending with &lt;strong&gt;_vit-h&lt;/strong&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/image_encoder/model.safetensors&#34;&gt;SDXL model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can rename them to something easier to remember or put them into a sub-directory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the image encoders are actually &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K&#34;&gt;ViT-H&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k&#34;&gt;ViT-bigG&lt;/a&gt; (used only for one SDXL model). You probably already have them.&lt;/p&gt; &#xA;&lt;h2&gt;How to&lt;/h2&gt; &#xA;&lt;p&gt;There&#39;s a basic workflow included in this repo and a few examples in the &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/&#34;&gt;examples&lt;/a&gt; directory. Usually it&#39;s a good idea to lower the &lt;code&gt;weight&lt;/code&gt; to at least &lt;code&gt;0.8&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;noise&lt;/code&gt; parameter is an experimental exploitation of the IPAdapter models. You can set it as low as &lt;code&gt;0.01&lt;/code&gt; for an arguably better result.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;More info about the noise option&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/noise_example.jpg&#34; width=&#34;100%&#34; alt=&#34;canny controlnet&#34;&gt; &#xA; &lt;p&gt;Basically the IPAdapter sends two pictures for the conditioning, one is the reference the other --that you don&#39;t see-- is an empty image that could be considered like a negative conditioning.&lt;/p&gt; &#xA; &lt;p&gt;What I&#39;m doing is to send a very noisy image instead of an empty one. The &lt;code&gt;noise&lt;/code&gt; parameter determines the amount of noise that is added. A value of &lt;code&gt;0.01&lt;/code&gt; adds a lot of noise (more noise == less impact becaue the model doesn&#39;t get it); a value of &lt;code&gt;1.0&lt;/code&gt; removes most of noise so the generated image gets conditioned more.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Preparing the reference image&lt;/h3&gt; &#xA;&lt;p&gt;The reference image needs to be encoded by the CLIP vision model. The encoder resizes the image to 224Ã—224 &lt;strong&gt;and crops it to the center!&lt;/strong&gt;. It&#39;s not an IPAdapter thing, it&#39;s how the clip vision works. This means that if you use a portrait or landscape image and the main attention (eg: the face of a character) is not in the middle you&#39;ll likely get undesired results. Use square pictures as reference for more predictable results.&lt;/p&gt; &#xA;&lt;p&gt;I&#39;ve added a &lt;code&gt;PrepImageForClipVision&lt;/code&gt; node that does all the required operations for you. You just have to select the crop position (top/left/center/etc...) and a sharpening amount if you want.&lt;/p&gt; &#xA;&lt;p&gt;In the image below you can see the difference between prepped and not prepped images.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/prep_images.jpg&#34; width=&#34;100%&#34; alt=&#34;prepped images&#34;&gt; &#xA;&lt;h3&gt;KSampler configuration suggestions&lt;/h3&gt; &#xA;&lt;p&gt;The IPAdapter generally requires a few more &lt;code&gt;steps&lt;/code&gt; than usual, if the result is underwhelming try to add 10+ steps. The model tends to burn the images a little. If needed lower the CFG scale.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;noise&lt;/code&gt; option generally grants better results, experiment with it.&lt;/p&gt; &#xA;&lt;h3&gt;IPAdapter + ControlNet&lt;/h3&gt; &#xA;&lt;p&gt;The model is very effective when paired with a ControlNet. In the example below I experimented with Canny. &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_Canny.json&#34;&gt;The workflow&lt;/a&gt; is in the examples directory.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/canny_controlnet.jpg&#34; width=&#34;100%&#34; alt=&#34;canny controlnet&#34;&gt; &#xA;&lt;h3&gt;IPAdapter Face&lt;/h3&gt; &#xA;&lt;p&gt;IPAdapter offers an interesting model for a kind of &#34;face swap&#34; effect. &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_face.json&#34;&gt;The workflow is provided&lt;/a&gt;. Set a close up face as reference image and then input your text prompt as always. The generated character should have the face of the reference. It also works with img2img given a high denoise.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/face_swap.jpg&#34; width=&#34;50%&#34; alt=&#34;face swap&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; there&#39;s a new &lt;code&gt;full-face&lt;/code&gt; model available that&#39;s arguably better.&lt;/p&gt; &#xA;&lt;h3&gt;Masking (Inpainting)&lt;/h3&gt; &#xA;&lt;p&gt;The most effective way to apply the IPAdapter to a region is by an &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_inpaint.json&#34;&gt;inpainting workflow&lt;/a&gt;. Remeber to use a specific checkpoint for inpainting otherwise it won&#39;t work. Even if you are inpainting a face I find that the &lt;em&gt;IPAdapter-Plus&lt;/em&gt; (not the &lt;em&gt;face&lt;/em&gt; one), works best.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/inpainting.jpg&#34; width=&#34;100%&#34; alt=&#34;inpainting&#34;&gt; &#xA;&lt;h3&gt;Image Batches&lt;/h3&gt; &#xA;&lt;p&gt;It is possible to pass multiple images for the conditioning with the &lt;code&gt;Batch Images&lt;/code&gt; node. An &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_batch_images.json&#34;&gt;example workflow&lt;/a&gt; is provided; in the picture below you can see the result of one and two images conditioning.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/batch_images.jpg&#34; width=&#34;100%&#34; alt=&#34;batcg images&#34;&gt; &#xA;&lt;p&gt;It seems to be effective with 2-3 images, beyond that it tends to &lt;em&gt;blur&lt;/em&gt; the information too much.&lt;/p&gt; &#xA;&lt;h3&gt;Image Weighting&lt;/h3&gt; &#xA;&lt;p&gt;When sending multiple images you can increase/decrease the weight of each image by using the &lt;code&gt;IPAdapterEncoder&lt;/code&gt; node. The workflow (&lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_weighted.json&#34;&gt;included in the examples&lt;/a&gt;) looks like this:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/image_weighting.jpg&#34; width=&#34;100%&#34; alt=&#34;image weighting&#34;&gt; &#xA;&lt;p&gt;The node accepts 4 images, but remember that you can send batches of images to each slot.&lt;/p&gt; &#xA;&lt;h3&gt;Weight types&lt;/h3&gt; &#xA;&lt;p&gt;You can choose how the IPAdapter weight is applied to the image embeds. Options are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;original&lt;/strong&gt;: The weight is applied to the aggregated tensors. The weight works predictably for values greater and lower than 1.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;linear&lt;/strong&gt;: The weight is applied to the individual tensors before aggretating them. Compared to &lt;code&gt;original&lt;/code&gt; the influence is weaker when weight is &amp;lt;1 and stronger when &amp;gt;1. &lt;strong&gt;Note:&lt;/strong&gt; at weight &lt;code&gt;1&lt;/code&gt; the two methods are equivalent.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;channel penalty&lt;/strong&gt;: This method is a modified version of Lvmin Zhang&#39;s (Fooocus). Results are sometimes sharper. It works very well also when weight is &amp;gt;1. Still experimental, may change in the future.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The image below shows the difference (zoom in).&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/weight_types.jpg&#34; width=&#34;100%&#34; alt=&#34;weight types&#34;&gt; &#xA;&lt;p&gt;In the examples directory you can find &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_weight_types.json&#34;&gt;a workflow&lt;/a&gt; that lets you easily compare the three methods.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I&#39;m not still sure whether all methods will stay. &lt;code&gt;Linear&lt;/code&gt; seems the most sensible but I wanted to keep the &lt;code&gt;original&lt;/code&gt; for backward compatibility. &lt;code&gt;channel penalty&lt;/code&gt; has a weird non-commercial clause but it&#39;s still part of a GNU GPLv3 software (ie: there&#39;s a licensing clash) so I&#39;m trying to understand how to deal with that.&lt;/p&gt; &#xA;&lt;h3&gt;Attention masking&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s possible to add a mask to define the area where the IPAdapter will be applied to. Everything outside the mask will ignore the reference images and will only listen to the text prompt.&lt;/p&gt; &#xA;&lt;p&gt;It is suggested to use a mask of the same size of the final generated image.&lt;/p&gt; &#xA;&lt;p&gt;In the picture below I use two reference images masked one on the left and the other on the right. The image is generated only with IPAdapter and one ksampler (without in/outpainting or area conditioning).&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/masking.jpg&#34; width=&#34;512&#34; alt=&#34;masking&#34;&gt; &#xA;&lt;p&gt;It is also possible to send a batch of masks that will be applied to a batch of latents, one per frame. The size should be the same but if needed some normalization will be performed to avoid errors. This feature also supports (experimentally) AnimateDiff including context sliding.&lt;/p&gt; &#xA;&lt;p&gt;In the examples directory you&#39;ll find a couple of masking workflows: &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_mask.json&#34;&gt;simple&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_2_masks.json&#34;&gt;two masks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Timestepping&lt;/h3&gt; &#xA;&lt;p&gt;In the &lt;code&gt;Apply IPAdapter&lt;/code&gt; node you can set a start and an end point. The IPAdapter will be applied exclusively in that timeframe of the generation. This is a very powerful tool to modulate the intesity of IPAdapter models.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/timestepping.jpg&#34; width=&#34;100%&#34; alt=&#34;timestepping&#34;&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;Please check the &lt;a href=&#34;https://github.com/cubiq/ComfyUI_IPAdapter_plus/issues/108&#34;&gt;troubleshooting&lt;/a&gt; before posting a new issue.&lt;/p&gt; &#xA;&lt;h2&gt;Diffusers version&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested I&#39;ve also implemented the same features for &lt;a href=&#34;https://github.com/cubiq/Diffusers_IPAdapter&#34;&gt;Huggingface Diffusers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter/&#34;&gt;IPAdapter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/laksjdjf/IPAdapter-ComfyUI/&#34;&gt;laksjdjf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lllyasviel/Fooocus/raw/main/fooocus_extras/ip_adapter.py&#34;&gt;fooocus&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;IPAdapter in the wild&lt;/h2&gt; &#xA;&lt;p&gt;Let me know if you spot the IPAdapter in the wild or tag @latentvision in the video description!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For German speakers you can find interesting YouTube tutorials on &lt;a href=&#34;https://www.youtube.com/watch?v=rAWn_0YOBU0&#34;&gt;A Latent Place&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In Chinese &lt;a href=&#34;https://www.youtube.com/watch?v=xl8f3oxZgY8&#34;&gt;Introversify&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xzGdynQDzsM&#34;&gt;Scott Detweiler&lt;/a&gt; covered this extension.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>