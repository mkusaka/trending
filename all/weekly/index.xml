<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-16T01:49:42Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>imClumsyPanda/langchain-ChatGLM</title>
    <updated>2023-04-16T01:49:42Z</updated>
    <id>tag:github.com,2023-04-16:/imClumsyPanda/langchain-ChatGLM</id>
    <link href="https://github.com/imClumsyPanda/langchain-ChatGLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;langchain-ChatGLM, local knowledge based ChatGLM with langchain ｜ 基于本地知识的 ChatGLM 问答&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;基于本地知识的 ChatGLM 应用实现&lt;/h1&gt; &#xA;&lt;h2&gt;介绍&lt;/h2&gt; &#xA;&lt;p&gt;🌍 &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/README_en.md&#34;&gt;&lt;em&gt;READ THIS IN ENGLISH&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;🤖️ 一种利用 &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt; + &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;langchain&lt;/a&gt; 实现的基于本地知识的 ChatGLM 应用。&lt;/p&gt; &#xA;&lt;p&gt;💡 受 &lt;a href=&#34;https://github.com/GanymedeNil&#34;&gt;GanymedeNil&lt;/a&gt; 的项目 &lt;a href=&#34;https://github.com/GanymedeNil/document.ai&#34;&gt;document.ai&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/AlexZhangji&#34;&gt;AlexZhangji&lt;/a&gt; 创建的 &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B/pull/216&#34;&gt;ChatGLM-6B Pull Request&lt;/a&gt; 启发，建立了全部基于开源模型实现的本地知识问答应用。&lt;/p&gt; &#xA;&lt;p&gt;✅ 本项目中 Embedding 选用的是 &lt;a href=&#34;https://huggingface.co/GanymedeNil/text2vec-large-chinese/tree/main&#34;&gt;GanymedeNil/text2vec-large-chinese&lt;/a&gt;，LLM 选用的是 &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt;。依托上述模型，本项目可实现全部使用&lt;strong&gt;开源&lt;/strong&gt;模型&lt;strong&gt;离线私有部署&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;p&gt;⛓️ 本项目实现原理如下图所示，过程包括加载文件 -&amp;gt; 读取文本 -&amp;gt; 文本分割 -&amp;gt; 文本向量化 -&amp;gt; 问句向量化 -&amp;gt; 在文本向量中匹配出与问句向量最相似的&lt;code&gt;top k&lt;/code&gt;个 -&amp;gt; 匹配出的文本作为上下文和问题一起添加到&lt;code&gt;prompt&lt;/code&gt;中 -&amp;gt; 提交给&lt;code&gt;LLM&lt;/code&gt;生成回答。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/img/langchain+chatglm.png&#34; alt=&#34;实现原理图&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;🚩 本项目未涉及微调、训练过程，但可利用微调或训练对本项目效果进行优化。&lt;/p&gt; &#xA;&lt;h2&gt;更新信息&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/04/15]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;重构项目结构，在根目录下保留命令行 Demo &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/cli_demo.py&#34;&gt;cli_demo.py&lt;/a&gt; 和 Web UI Demo &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/webui.py&#34;&gt;webui.py&lt;/a&gt;；&lt;/li&gt; &#xA; &lt;li&gt;对 Web UI 进行改进，修改为运行 Web UI 后首先按照 &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/configs/model_config.py&#34;&gt;configs/model_config.py&lt;/a&gt; 默认选项加载模型，并增加报错提示信息等；&lt;/li&gt; &#xA; &lt;li&gt;对常见问题进行补充说明。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/04/12]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;替换 Web UI 中的样例文件，避免出现 Ubuntu 中出现因文件编码无法读取的问题；&lt;/li&gt; &#xA; &lt;li&gt;替换&lt;code&gt;knowledge_based_chatglm.py&lt;/code&gt;中的 prompt 模版，避免出现因 prompt 模版包含中英双语导致 chatglm 返回内容错乱的问题。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/04/11]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;加入 Web UI V0.1 版本（感谢 &lt;a href=&#34;https://github.com/liangtongt&#34;&gt;@liangtongt&lt;/a&gt;）；&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;README.md&lt;/code&gt;中增加常见问题（感谢 &lt;a href=&#34;https://github.com/calcitem&#34;&gt;@calcitem&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/bolongliu&#34;&gt;@bolongliu&lt;/a&gt;）；&lt;/li&gt; &#xA; &lt;li&gt;增加 LLM 和 Embedding 模型运行设备是否可用&lt;code&gt;cuda&lt;/code&gt;、&lt;code&gt;mps&lt;/code&gt;、&lt;code&gt;cpu&lt;/code&gt;的自动判断。&lt;/li&gt; &#xA; &lt;li&gt;在&lt;code&gt;knowledge_based_chatglm.py&lt;/code&gt;中增加对&lt;code&gt;filepath&lt;/code&gt;的判断，在之前支持单个文件导入的基础上，现支持单个文件夹路径作为输入，输入后将会遍历文件夹中各个文件，并在命令行中显示每个文件是否成功加载。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/04/09]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;使用&lt;code&gt;langchain&lt;/code&gt;中的&lt;code&gt;RetrievalQA&lt;/code&gt;替代之前选用的&lt;code&gt;ChatVectorDBChain&lt;/code&gt;，替换后可以有效减少提问 2-3 次后因显存不足而停止运行的问题；&lt;/li&gt; &#xA; &lt;li&gt;在&lt;code&gt;knowledge_based_chatglm.py&lt;/code&gt;中增加&lt;code&gt;EMBEDDING_MODEL&lt;/code&gt;、&lt;code&gt;VECTOR_SEARCH_TOP_K&lt;/code&gt;、&lt;code&gt;LLM_MODEL&lt;/code&gt;、&lt;code&gt;LLM_HISTORY_LEN&lt;/code&gt;、&lt;code&gt;REPLY_WITH_SOURCE&lt;/code&gt;参数值设置；&lt;/li&gt; &#xA; &lt;li&gt;增加 GPU 显存需求更小的&lt;code&gt;chatglm-6b-int4&lt;/code&gt;、&lt;code&gt;chatglm-6b-int4-qe&lt;/code&gt;作为 LLM 模型备选项；&lt;/li&gt; &#xA; &lt;li&gt;更正&lt;code&gt;README.md&lt;/code&gt;中的代码错误（感谢 &lt;a href=&#34;https://github.com/calcitem&#34;&gt;@calcitem&lt;/a&gt;）。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/04/07]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;解决加载 ChatGLM 模型时发生显存占用为双倍的问题 (感谢 &lt;a href=&#34;https://github.com/suc16&#34;&gt;@suc16&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/myml&#34;&gt;@myml&lt;/a&gt;) ；&lt;/li&gt; &#xA; &lt;li&gt;新增清理显存机制；&lt;/li&gt; &#xA; &lt;li&gt;新增&lt;code&gt;nghuyong/ernie-3.0-nano-zh&lt;/code&gt;和&lt;code&gt;nghuyong/ernie-3.0-base-zh&lt;/code&gt;作为 Embedding 模型备选项，相比&lt;code&gt;GanymedeNil/text2vec-large-chinese&lt;/code&gt;占用显存资源更少 (感谢 &lt;a href=&#34;https://github.com/lastrei&#34;&gt;@lastrei&lt;/a&gt;)。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;使用方式&lt;/h2&gt; &#xA;&lt;h3&gt;硬件需求&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ChatGLM-6B 模型硬件需求&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th&gt;&lt;strong&gt;量化等级&lt;/strong&gt;&lt;/th&gt; &#xA;     &lt;th&gt;&lt;strong&gt;最低 GPU 显存&lt;/strong&gt;（推理）&lt;/th&gt; &#xA;     &lt;th&gt;&lt;strong&gt;最低 GPU 显存&lt;/strong&gt;（高效参数微调）&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;FP16（无量化）&lt;/td&gt; &#xA;     &lt;td&gt;13 GB&lt;/td&gt; &#xA;     &lt;td&gt;14 GB&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;INT8&lt;/td&gt; &#xA;     &lt;td&gt;8 GB&lt;/td&gt; &#xA;     &lt;td&gt;9 GB&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td&gt;INT4&lt;/td&gt; &#xA;     &lt;td&gt;6 GB&lt;/td&gt; &#xA;     &lt;td&gt;7 GB&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Embedding 模型硬件需求&lt;/p&gt; &lt;p&gt;本项目中默认选用的 Embedding 模型 &lt;a href=&#34;https://huggingface.co/GanymedeNil/text2vec-large-chinese/tree/main&#34;&gt;GanymedeNil/text2vec-large-chinese&lt;/a&gt; 约占用显存 3GB，也可修改为在 CPU 中运行。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;软件需求&lt;/h3&gt; &#xA;&lt;p&gt;本项目已在 Python 3.8，CUDA 11.7 环境下完成测试。&lt;/p&gt; &#xA;&lt;h3&gt;1. 安装环境&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;环境检查&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 首先，确信你的机器安装了 Python 3.8 及以上版本&#xA;$ python --version&#xA;Python 3.8.13&#xA;&#xA;# 如果低于这个版本，可使用conda安装环境&#xA;$ conda create -p /your_path/env_name python=3.8&#xA;&#xA;# 激活环境&#xA;$ source activate /your_path/env_name&#xA;&#xA;# 关闭环境&#xA;$ source deactivate /your_path/env_name&#xA;&#xA;# 删除环境&#xA;$ conda env remove -p  /your_path/env_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;项目依赖&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 拉取仓库&#xA;$ git clone https://github.com/imClumsyPanda/langchain-ChatGLM.git&#xA;&#xA;# 安装依赖&#xA;$ pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注：使用 langchain.document_loaders.UnstructuredFileLoader 进行非结构化文件接入时，可能需要依据文档进行其他依赖包的安装，请参考 &lt;a href=&#34;https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/unstructured_file.html&#34;&gt;langchain 文档&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;2. 设置模型默认参数&lt;/h3&gt; &#xA;&lt;p&gt;在开始执行 Web UI 或命令行交互前，请先检查 &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/configs/model_config.py&#34;&gt;configs/model_config.py&lt;/a&gt; 中的各项模型参数设计是否符合需求。&lt;/p&gt; &#xA;&lt;h3&gt;3. 执行脚本体验 Web UI 或命令行交互&lt;/h3&gt; &#xA;&lt;p&gt;执行 &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/webui.py&#34;&gt;webui.py&lt;/a&gt; 脚本体验 &lt;strong&gt;Web 交互&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python webui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注：如未将模型下载至本地，请执行前检查&lt;code&gt;$HOME/.cache/huggingface/&lt;/code&gt;文件夹剩余空间，至少15G&lt;/p&gt; &#xA;&lt;p&gt;执行后效果如下图所示： &lt;img src=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/img/ui1.png&#34; alt=&#34;webui&#34;&gt; Web UI 可以实现如下功能：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;运行前自动读取&lt;code&gt;configs/model_config.py&lt;/code&gt;中&lt;code&gt;LLM&lt;/code&gt;及&lt;code&gt;Embedding&lt;/code&gt;模型枚举及默认模型设置运行模型，如需重新加载模型，可在界面重新选择后点击&lt;code&gt;重新加载模型&lt;/code&gt;进行模型加载；&lt;/li&gt; &#xA; &lt;li&gt;可手动调节保留对话历史长度，可根据显存大小自行调节&lt;/li&gt; &#xA; &lt;li&gt;添加上传文件功能，通过下拉框选择已上传的文件，点击&lt;code&gt;加载文件&lt;/code&gt;按钮，过程中可随时更换加载的文件&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;或执行 &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/cli_demo.py&#34;&gt;knowledge_based_chatglm.py&lt;/a&gt; 脚本体验&lt;strong&gt;命令行交互&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python knowledge_based_chatglm.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;常见问题&lt;/h3&gt; &#xA;&lt;p&gt;Q1: 本项目支持哪些文件格式？&lt;/p&gt; &#xA;&lt;p&gt;A1: 目前已测试支持 txt、docx、md、pdf 格式文件，更多文件格式请参考 &lt;a href=&#34;https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/unstructured_file.html&#34;&gt;langchain 文档&lt;/a&gt;。目前已知文档中若含有特殊字符，可能存在文件无法加载的问题。&lt;/p&gt; &#xA;&lt;p&gt;Q3: 使用过程中 Python 包&lt;code&gt;nltk&lt;/code&gt;发生了&lt;code&gt;Resource punkt not found.&lt;/code&gt;报错，该如何解决？&lt;/p&gt; &#xA;&lt;p&gt;A3: &lt;a href=&#34;https://github.com/nltk/nltk_data/raw/gh-pages/packages/tokenizers/punkt.zip&#34;&gt;https://github.com/nltk/nltk_data/raw/gh-pages/packages/tokenizers/punkt.zip&lt;/a&gt; 中的 &lt;code&gt;packages/tokenizers&lt;/code&gt; 解压，放到 &lt;code&gt;nltk_data/tokenizers&lt;/code&gt; 存储路径下。&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;nltk_data&lt;/code&gt; 存储路径可以通过 &lt;code&gt;nltk.data.path&lt;/code&gt; 查询。&lt;/p&gt; &#xA;&lt;p&gt;Q4: 使用过程中 Python 包&lt;code&gt;nltk&lt;/code&gt;发生了&lt;code&gt;Resource averaged_perceptron_tagger not found.&lt;/code&gt;报错，该如何解决？&lt;/p&gt; &#xA;&lt;p&gt;A4: 将 &lt;a href=&#34;https://github.com/nltk/nltk_data/raw/gh-pages/packages/taggers/averaged_perceptron_tagger.zip&#34;&gt;https://github.com/nltk/nltk_data/blob/gh-pages/packages/taggers/averaged_perceptron_tagger.zip&lt;/a&gt; 下载，解压放到 &lt;code&gt;nltk_data/taggers&lt;/code&gt; 存储路径下。&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;nltk_data&lt;/code&gt; 存储路径可以通过 &lt;code&gt;nltk.data.path&lt;/code&gt; 查询。&lt;/p&gt; &#xA;&lt;p&gt;Q5: 本项目可否在 colab 中运行？&lt;/p&gt; &#xA;&lt;p&gt;A5: 可以尝试使用 chatglm-6b-int4 模型在 colab 中运行，需要注意的是，如需在 colab 中运行 Web UI，需将&lt;code&gt;webui.py&lt;/code&gt;中&lt;code&gt;demo.queue(concurrency_count=3).launch( server_name=&#39;0.0.0.0&#39;, share=False, inbrowser=False)&lt;/code&gt;中参数&lt;code&gt;share&lt;/code&gt;设置为&lt;code&gt;True&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;Q6: 在 Anaconda 中使用 pip 安装包无效如何解决？&lt;/p&gt; &#xA;&lt;p&gt;A6: 此问题是系统环境问题，详细见 &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/docs/%E5%9C%A8Anaconda%E4%B8%AD%E4%BD%BF%E7%94%A8pip%E5%AE%89%E8%A3%85%E5%8C%85%E6%97%A0%E6%95%88%E9%97%AE%E9%A2%98.md&#34;&gt;在Anaconda中使用pip安装包无效问题&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Q7: 本项目中所需模型如何下载至本地？&lt;/p&gt; &#xA;&lt;p&gt;A7: 本项目中使用的模型均为&lt;code&gt;huggingface.com&lt;/code&gt;中可下载的开源模型，以默认选择的&lt;code&gt;chatglm-6b&lt;/code&gt;和&lt;code&gt;text2vec-large-chinese&lt;/code&gt;模型为例，下载模型可执行如下代码：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 安装 git lfs&#xA;$ git lfs install&#xA;&#xA;# 下载 LLM 模型&#xA;$ git clone https://huggingface.co/THUDM/chatglm-6b /your_path/chatglm-6b&#xA;&#xA;# 下载 Embedding 模型&#xA;$ git clone https://huggingface.co/GanymedeNil/text2vec-large-chinese /your_path/text2vec&#xA;&#xA;# 模型需要更新时，可打开模型所在文件夹后拉取最新模型文件/代码&#xA;$ git pull&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Q8: &lt;code&gt;huggingface.com&lt;/code&gt;中模型下载速度较慢怎么办？&lt;/p&gt; &#xA;&lt;p&gt;A8: 可使用本项目用到的模型权重文件百度网盘地址：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ernie-3.0-base-zh.zip 链接: &lt;a href=&#34;https://pan.baidu.com/s/1CIvKnD3qzE-orFouA8qvNQ?pwd=4wih&#34;&gt;https://pan.baidu.com/s/1CIvKnD3qzE-orFouA8qvNQ?pwd=4wih&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ernie-3.0-nano-zh.zip 链接: &lt;a href=&#34;https://pan.baidu.com/s/1Fh8fgzVdavf5P1omAJJ-Zw?pwd=q6s5&#34;&gt;https://pan.baidu.com/s/1Fh8fgzVdavf5P1omAJJ-Zw?pwd=q6s5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;text2vec-large-chinese.zip 链接: &lt;a href=&#34;https://pan.baidu.com/s/1sMyPzBIXdEzHygftEoyBuA?pwd=4xs7&#34;&gt;https://pan.baidu.com/s/1sMyPzBIXdEzHygftEoyBuA?pwd=4xs7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;chatglm-6b-int4-qe.zip 链接: &lt;a href=&#34;https://pan.baidu.com/s/1DDKMOMHtNZccOOBGWIOYww?pwd=22ji&#34;&gt;https://pan.baidu.com/s/1DDKMOMHtNZccOOBGWIOYww?pwd=22ji&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;chatglm-6b-int4.zip 链接: &lt;a href=&#34;https://pan.baidu.com/s/1pvZ6pMzovjhkA6uPcRLuJA?pwd=3gjd&#34;&gt;https://pan.baidu.com/s/1pvZ6pMzovjhkA6uPcRLuJA?pwd=3gjd&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;chatglm-6b.zip 链接: &lt;a href=&#34;https://pan.baidu.com/s/1B-MpsVVs1GHhteVBetaquw?pwd=djay&#34;&gt;https://pan.baidu.com/s/1B-MpsVVs1GHhteVBetaquw?pwd=djay&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Q9: 下载完模型后，如何修改代码以执行本地模型？&lt;/p&gt; &#xA;&lt;p&gt;A9: 模型下载完成后，请在 &lt;a href=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/configs/model_config.py&#34;&gt;configs/model_config.py&lt;/a&gt; 文件中，对&lt;code&gt;embedding_model_dict&lt;/code&gt;和&lt;code&gt;llm_model_dict&lt;/code&gt;参数进行修改，如把&lt;code&gt;llm_model_dict&lt;/code&gt;从&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;embedding_model_dict = {&#xA;    &#34;ernie-tiny&#34;: &#34;nghuyong/ernie-3.0-nano-zh&#34;,&#xA;    &#34;ernie-base&#34;: &#34;nghuyong/ernie-3.0-base-zh&#34;,&#xA;    &#34;text2vec&#34;: &#34;GanymedeNil/text2vec-large-chinese&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;修改为&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;embedding_model_dict = {&#xA;                        &#34;ernie-tiny&#34;: &#34;nghuyong/ernie-3.0-nano-zh&#34;,&#xA;                        &#34;ernie-base&#34;: &#34;nghuyong/ernie-3.0-base-zh&#34;,&#xA;                        &#34;text2vec&#34;: &#34;/Users/liuqian/Downloads/ChatGLM-6B/text2vec-large-chinese&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;DEMO&lt;/h2&gt; &#xA;&lt;p&gt;以问题&lt;code&gt;chatglm-6b 的局限性具体体现在哪里，如何实现改进&lt;/code&gt;为例&lt;/p&gt; &#xA;&lt;p&gt;未使用 langchain 接入本地文档时：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ChatGLM-6B 是一个基于 GPT-3 的聊天机器人,它的局限性主要体现在以下几个方面:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;语言模型的精度:GPT-3 是一种大型的语言模型,它的精度比较高,但是仍然有一些局限性。例如,它可能无法准确地理解某些方言、口音或者复杂的句子结构。&lt;/li&gt; &#xA;  &lt;li&gt;知识图谱的完整性:GPT-3 可以生成一些知识图谱,但是其完整性和准确性可能不够。例如,它可能无法准确地理解某些领域的专业知识和术语。&lt;/li&gt; &#xA;  &lt;li&gt;对话生成的连贯性:GPT-3 可以生成一些连贯的对话,但是其生成的对话可能不够自然。例如,它可能会生成一些虚假的、不连贯的对话或者误解用户的意图。&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;为了改进 ChatGLM-6B,可以考虑以下几个方面:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;提高语言模型的精度:可以通过增加训练数据、使用更好的语言模型架构、优化模型训练过程等方式来提高语言模型的精度。&lt;/li&gt; &#xA;  &lt;li&gt;增强知识图谱的完整性:可以通过增加知识图谱的维度、使用更好的知识图谱构建方法、优化知识图谱的匹配算法等方式来增强知识图谱的完整性。&lt;/li&gt; &#xA;  &lt;li&gt;提高对话生成的连贯性:可以通过增加对话生成模型的上下文信息、使用更好的上下文信息提取方法、优化对话生成模型的算法等方式来提高对话生成的连贯性。&lt;/li&gt; &#xA;  &lt;li&gt;引入更多的人类反馈:人类反馈可以帮助 ChatGLM-6B 更好地理解用户的意图和需求,从而提高对话生成的质量和准确性。可以通过引入更多的人类客服、使用人类反馈收集器等方式来引入更多的人类反馈。&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;使用 langchain 接入 ChatGLM-6B 项目的 README.md 文件后 ChatGLM 的回答：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ChatGLM-6B 是一个基于深度学习的自然语言处理模型,它在回答问题方面表现出色。但是,它也有一些局限性,具体如下:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;语言理解有限:ChatGLM-6B 只学习了中文自然语言,对于其他语言可能会存在理解上的局限性。&lt;/li&gt; &#xA;  &lt;li&gt;知识储备不足:ChatGLM-6B 的训练数据集只包含了中文语料,因此它可能无法回答一些非中文的问题或者某些特定领域的问题。&lt;/li&gt; &#xA;  &lt;li&gt;数据量有限:ChatGLM-6B 的训练数据集只有几百万条记录,因此它可能无法回答一些非常具体或者复杂的问题。&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;为了改进 ChatGLM-6B,可以考虑以下几个方面:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;学习更多的语言知识:可以学习其他语言的自然语言处理技术,扩大语言理解的范围。&lt;/li&gt; &#xA;  &lt;li&gt;扩大知识储备:可以收集更多的中文语料,或者使用其他语言的数据集来扩充知识储备。&lt;/li&gt; &#xA;  &lt;li&gt;增加数据量:可以使用更大的数据集来训练 ChatGLM-6B,提高模型的表现。&lt;/li&gt; &#xA;  &lt;li&gt;引入更多的评估指标:可以引入更多的评估指标来评估模型的表现,从而发现 ChatGLM-6B 存在的不足和局限性。&lt;/li&gt; &#xA;  &lt;li&gt;改进模型架构:可以改进 ChatGLM-6B 的模型架构,提高模型的性能和表现。例如,可以使用更大的神经网络或者改进的卷积神经网络结构。&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;路线图&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 实现 langchain + ChatGLM-6B 本地知识应用&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 基于 langchain 实现非结构化文件接入 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; .md&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; .pdf(需要按照常见问题 Q2 中描述进行&lt;code&gt;detectron2&lt;/code&gt;的安装)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; .docx&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; .txt&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 搜索引擎与本地网页&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 增加更多 LLM 模型支持 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; THUDM/chatglm-6b&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; THUDM/chatglm-6b-int4&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; THUDM/chatglm-6b-int4-qe&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 增加 Web UI DEMO &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 利用 gradio 实现 Web UI DEMO&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 添加输出内容及错误提示&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 引用标注&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 利用 fastapi 实现 API 部署方式，并实现调用 API 的 web ui DEMO&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;项目交流群&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/img/qr_code.jpg&#34; alt=&#34;二维码&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;🎉 langchain-ChatGLM 项目交流群，如果你也对本项目感兴趣，欢迎加入群聊参与讨论交流。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yoheinakajima/babyagi</title>
    <updated>2023-04-16T01:49:42Z</updated>
    <id>tag:github.com,2023-04-16:/yoheinakajima/babyagi</id>
    <link href="https://github.com/yoheinakajima/babyagi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Translations:&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-fr.md&#34;&gt;&lt;img title=&#34;Français&#34; alt=&#34;Français&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/fr.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-pt-br.md&#34;&gt;&lt;img title=&#34;Portuguese&#34; alt=&#34;Portuguese&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/br.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-ro.md&#34;&gt;&lt;img title=&#34;Romanian&#34; alt=&#34;Romanian&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/ro.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-ru.md&#34;&gt;&lt;img title=&#34;Russian&#34; alt=&#34;Russian&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/ru.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-si.md&#34;&gt;&lt;img title=&#34;Slovenian&#34; alt=&#34;Slovenian&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/si.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-es.md&#34;&gt;&lt;img title=&#34;Spanish&#34; alt=&#34;Spanish&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/es.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-tr.md&#34;&gt;&lt;img title=&#34;Turkish&#34; alt=&#34;Turkish&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/tr.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-ua.md&#34;&gt;&lt;img title=&#34;Ukrainian&#34; alt=&#34;Ukrainian&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/ua.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-cn.md&#34;&gt;&lt;img title=&#34;简体中文&#34; alt=&#34;Simplified Chinese&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/cn.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/docs/README-zh-tw.md&#34;&gt;&lt;img title=&#34;繁體中文 (Traditional Chinese)&#34; alt=&#34;繁體中文 (Traditional Chinese)&#34; src=&#34;https://cdn.staticaly.com/gh/hjnilsson/country-flags/master/svg/tw.svg?sanitize=true&#34; width=&#34;22&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Objective&lt;/h1&gt; &#xA;&lt;p&gt;This Python script is an example of an AI-powered task management system. The system uses OpenAI and Pinecone APIs to create, prioritize, and execute tasks. The main idea behind this system is that it creates tasks based on the result of previous tasks and a predefined objective. The script then uses OpenAI&#39;s natural language processing (NLP) capabilities to create new tasks based on the objective, and Pinecone to store and retrieve task results for context. This is a pared-down version of the original &lt;a href=&#34;https://twitter.com/yoheinakajima/status/1640934493489070080?s=20&#34;&gt;Task-Driven Autonomous Agent&lt;/a&gt; (Mar 28, 2023).&lt;/p&gt; &#xA;&lt;p&gt;This README will cover the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/#how-it-works&#34;&gt;How the script works&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/#how-to-use&#34;&gt;How to use the script&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/#supported-models&#34;&gt;Supported Models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yoheinakajima/babyagi/main/#continous-script-warning&#34;&gt;Warning about running the script continuously&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;How It Works&lt;a name=&#34;how-it-works&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;The script works by running an infinite loop that does the following steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pulls the first task from the task list.&lt;/li&gt; &#xA; &lt;li&gt;Sends the task to the execution agent, which uses OpenAI&#39;s API to complete the task based on the context.&lt;/li&gt; &#xA; &lt;li&gt;Enriches the result and stores it in Pinecone.&lt;/li&gt; &#xA; &lt;li&gt;Creates new tasks and reprioritizes the task list based on the objective and the result of the previous task. &lt;br&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The execution_agent() function is where the OpenAI API is used. It takes two parameters: the objective and the task. It then sends a prompt to OpenAI&#39;s API, which returns the result of the task. The prompt consists of a description of the AI system&#39;s task, the objective, and the task itself. The result is then returned as a string. &lt;br&gt; The task_creation_agent() function is where OpenAI&#39;s API is used to create new tasks based on the objective and the result of the previous task. The function takes four parameters: the objective, the result of the previous task, the task description, and the current task list. It then sends a prompt to OpenAI&#39;s API, which returns a list of new tasks as strings. The function then returns the new tasks as a list of dictionaries, where each dictionary contains the name of the task. &lt;br&gt; The prioritization_agent() function is where OpenAI&#39;s API is used to reprioritize the task list. The function takes one parameter, the ID of the current task. It sends a prompt to OpenAI&#39;s API, which returns the reprioritized task list as a numbered list.&lt;/p&gt; &#xA;&lt;p&gt;Finally, the script uses Pinecone to store and retrieve task results for context. The script creates a Pinecone index based on the table name specified in the YOUR_TABLE_NAME variable. Pinecone is then used to store the results of the task in the index, along with the task name and any additional metadata.&lt;/p&gt; &#xA;&lt;h1&gt;How to Use&lt;a name=&#34;how-to-use&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;To use the script, you will need to follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository via &lt;code&gt;git clone https://github.com/yoheinakajima/babyagi.git&lt;/code&gt; and &lt;code&gt;cd&lt;/code&gt; into the cloned repository.&lt;/li&gt; &#xA; &lt;li&gt;Install the required packages: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copy the .env.example file to .env: &lt;code&gt;cp .env.example .env&lt;/code&gt;. This is where you will set the following variables.&lt;/li&gt; &#xA; &lt;li&gt;Set your OpenAI and Pinecone API keys in the OPENAI_API_KEY, OPENAPI_API_MODEL, and PINECONE_API_KEY variables.&lt;/li&gt; &#xA; &lt;li&gt;Set the Pinecone environment in the PINECONE_ENVIRONMENT variable.&lt;/li&gt; &#xA; &lt;li&gt;Set the name of the table where the task results will be stored in the TABLE_NAME variable.&lt;/li&gt; &#xA; &lt;li&gt;(Optional) Set the objective of the task management system in the OBJECTIVE variable.&lt;/li&gt; &#xA; &lt;li&gt;(Optional) Set the first task of the system in the INITIAL_TASK variable.&lt;/li&gt; &#xA; &lt;li&gt;Run the script.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;All optional values above can also be specified on the command line.&lt;/p&gt; &#xA;&lt;h1&gt;Running inside a docker container&lt;/h1&gt; &#xA;&lt;p&gt;As a prerequisite, you will need docker and docker-compose installed. Docker desktop is the simplest option &lt;a href=&#34;https://www.docker.com/products/docker-desktop/&#34;&gt;https://www.docker.com/products/docker-desktop/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run the system inside a docker container, setup your .env file as per steps above and then run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Supported Models&lt;a name=&#34;supported-models&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This script works with all OpenAI models, as well as Llama through Llama.cpp. Default model is &lt;strong&gt;gpt-3.5-turbo&lt;/strong&gt;. To use a different model, specify it through OPENAI_API_MODEL or use the command line.&lt;/p&gt; &#xA;&lt;h2&gt;Llama&lt;/h2&gt; &#xA;&lt;p&gt;Download the latest version of &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;Llama.cpp&lt;/a&gt; and follow instructions to make it. You will also need the Llama model weights.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Under no circumstances share IPFS, magnet links, or any other links to model downloads anywhere in this repository, including in issues, discussions or pull requests. They will be immediately deleted.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After that link &lt;code&gt;llama/main&lt;/code&gt; to llama.cpp/main and &lt;code&gt;models&lt;/code&gt; to the folder where you have the Llama model weights. Then run the script with &lt;code&gt;OPENAI_API_MODEL=llama&lt;/code&gt; or &lt;code&gt;-l&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;h1&gt;Warning&lt;a name=&#34;continous-script-warning&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This script is designed to be run continuously as part of a task management system. Running this script continuously can result in high API usage, so please use it responsibly. Additionally, the script requires the OpenAI and Pinecone APIs to be set up correctly, so make sure you have set up the APIs before running the script.&lt;/p&gt; &#xA;&lt;h1&gt;Contribution&lt;/h1&gt; &#xA;&lt;p&gt;Needless to say, BabyAGI is still in its infancy and thus we are still determining its direction and the steps to get there. Currently, a key design goal for BabyAGI is to be &lt;em&gt;simple&lt;/em&gt; such that it&#39;s easy to understand and build upon. To maintain this simplicity, we kindly request that you adhere to the following guidelines when submitting PRs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Focus on small, modular modifications rather than extensive refactoring.&lt;/li&gt; &#xA; &lt;li&gt;When introducing new features, provide a detailed description of the specific use case you are addressing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A note from @yoheinakajima (Apr 5th, 2023):&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;I know there are a growing number of PRs, appreciate your patience - as I am both new to GitHub/OpenSource, and did not plan my time availability accordingly this week. Re:direction, I&#39;ve been torn on keeping it simple vs expanding - currently leaning towards keeping a core Baby AGI simple, and using this as a platform to support and promote different approaches to expanding this (eg. BabyAGIxLangchain as one direction). I believe there are various opinionated approaches that are worth exploring, and I see value in having a central place to compare and discuss. More updates coming shortly.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;I am new to GitHub and open source, so please be patient as I learn to manage this project properly. I run a VC firm by day, so I will generally be checking PRs and issues at night after I get my kids down - which may not be every night. Open to the idea of bringing in support, will be updating this section soon (expectations, visions, etc). Talking to lots of people and learning - hang tight for updates!&lt;/p&gt; &#xA;&lt;h1&gt;Backstory&lt;/h1&gt; &#xA;&lt;p&gt;BabyAGI is a pared-down version of the original &lt;a href=&#34;https://twitter.com/yoheinakajima/status/1640934493489070080?s=20&#34;&gt;Task-Driven Autonomous Agent&lt;/a&gt; (Mar 28, 2023) shared on Twitter. This version is down to 140 lines: 13 comments, 22 blanks, and 105 code. The name of the repo came up in the reaction to the original autonomous agent - the author does not mean to imply that this is AGI.&lt;/p&gt; &#xA;&lt;p&gt;Made with love by &lt;a href=&#34;https://twitter.com/yoheinakajima&#34;&gt;@yoheinakajima&lt;/a&gt;, who happens to be a VC (would love to see what you&#39;re building!)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databrickslabs/dolly</title>
    <updated>2023-04-16T01:49:42Z</updated>
    <id>tag:github.com,2023-04-16:/databrickslabs/dolly</id>
    <link href="https://github.com/databrickslabs/dolly" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Databricks’ Dolly, a large language model trained on the Databricks Machine Learning Platform&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dolly&lt;/h1&gt; &#xA;&lt;p&gt;Databricks’ &lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-12b&#34;&gt;Dolly&lt;/a&gt; is an instruction-following large language model trained on the Databricks machine learning platform that is licensed for commercial use. Based on &lt;code&gt;pythia-12b&lt;/code&gt;, Dolly is trained on ~15k instruction/response fine tuning records &lt;a href=&#34;https://github.com/databrickslabs/dolly/tree/master/data&#34;&gt;&lt;code&gt;databricks-dolly-15k&lt;/code&gt;&lt;/a&gt; generated by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA and summarization. &lt;code&gt;dolly-v2-12b&lt;/code&gt; is not a state-of-the-art model, but does exhibit surprisingly high quality instruction following behavior not characteristic of the foundation model on which it is based.&lt;/p&gt; &#xA;&lt;p&gt;Databricks is committed to ensuring that every organization and individual benefits from the transformative power of artificial intelligence. The Dolly model family represents our first steps along this journey, and we’re excited to share this technology with the world.&lt;/p&gt; &#xA;&lt;p&gt;The model is available on Hugging Face as &lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-12b&#34;&gt;databricks/dolly-v2-12b&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Model Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;dolly-v2-12b&lt;/code&gt; is a 12 billion parameter causal language model created by &lt;a href=&#34;https://databricks.com/&#34;&gt;Databricks&lt;/a&gt; that is derived from &lt;a href=&#34;https://www.eleuther.ai/&#34;&gt;EleutherAI’s&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-12b&#34;&gt;Pythia-12b&lt;/a&gt; and fine-tuned on a &lt;a href=&#34;https://github.com/databrickslabs/dolly/tree/master/data&#34;&gt;~15K record instruction corpus&lt;/a&gt; generated by Databricks employees and released under a permissive license (CC-BY-SA)&lt;/p&gt; &#xA;&lt;h2&gt;Known Limitations&lt;/h2&gt; &#xA;&lt;h3&gt;Performance Limitations&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;dolly-v2-12b&lt;/code&gt; is not a state-of-the-art generative language model&lt;/strong&gt; and, though quantitative benchmarking is ongoing, is not designed to perform competitively with more modern model architectures or models subject to larger pretraining corpuses.&lt;/p&gt; &#xA;&lt;p&gt;The Dolly model family is under active development, and so any list of shortcomings is unlikely to be exhaustive, but we include known limitations and misfires here as a means to document and share our preliminary findings with the community. In particular, &lt;code&gt;dolly-v2-12b&lt;/code&gt; struggles with: syntactically complex prompts, programming problems, mathematical operations, factual errors, dates and times, open-ended question answering, hallucination, enumerating lists of specific length, stylistic mimicry, having a sense of humor, etc. Moreover, we find that &lt;code&gt;dolly-v2-12b&lt;/code&gt; does not have some capabilities, such as well-formatted letter writing, present in the original model.&lt;/p&gt; &#xA;&lt;h3&gt;Dataset Limitations&lt;/h3&gt; &#xA;&lt;p&gt;Like all language models, &lt;code&gt;dolly-v2-12b&lt;/code&gt; reflects the content and limitations of its training corpuses.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;The Pile&lt;/strong&gt;: GPT-J’s pre-training corpus contains content mostly collected from the public internet, and like most web-scale datasets, it contains content many users would find objectionable. As such, the model is likely to reflect these shortcomings, potentially overtly in the case it is explicitly asked to produce objectionable content, and sometimes subtly, as in the case of biased or harmful implicit associations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;databricks-dolly-15k&lt;/code&gt;&lt;/strong&gt;: The training data on which &lt;code&gt;dolly-v2-12b&lt;/code&gt; is instruction tuned represents natural language instructions generated by Databricks employees during a period spanning March and April 2023 and includes passages from Wikipedia as references passages for instruction categories like closed QA and summarization. To our knowledge it does not contain obscenity, intellectual property or personally identifying information about non-public figures, but it may contain typos and factual errors. The dataset may also reflect biases found in Wikipedia. Finally, the dataset likely reflects the interests and semantic choices of Databricks employees, a demographic which is not representative of the global population at large.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Databricks is committed to ongoing research and development efforts to develop helpful, honest and harmless AI technologies that maximize the potential of all individuals and organizations.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started with Response Generation&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to simply test the model without training, the model is available on Hugging Face as &lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-12b&#34;&gt;databricks/dolly-v2-12b&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To use the model with the &lt;code&gt;transformers&lt;/code&gt; library on a machine with GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from transformers import pipeline&#xA;&#xA;instruct_pipeline = pipeline(model=&#34;databricks/dolly-v2-12b&#34;, trust_remote_code=True, device_map=&#34;auto&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then use the pipeline to answer instructions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;instruct_pipeline(&#34;Explain to me the difference between nuclear fission and fusion.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To reduce memory usage you can load the model with &lt;code&gt;bfloat16&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;from transformers import pipeline&#xA;&#xA;instruct_pipeline = pipeline(model=&#34;databricks/dolly-v2-12b&#34;, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=&#34;auto&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started with Training&lt;/h2&gt; &#xA;&lt;p&gt;The following instructions refer to Dolly v1 and still need to be updated for v2 training.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add the &lt;code&gt;dolly&lt;/code&gt; repo to Databricks (under Repos click Add Repo, enter &lt;code&gt;https://github.com/databrickslabs/dolly.git&lt;/code&gt;, then click Create Repo).&lt;/li&gt; &#xA; &lt;li&gt;Start a &lt;code&gt;12.2 LTS ML (includes Apache Spark 3.3.2, GPU, Scala 2.12)&lt;/code&gt; single-node cluster with node type having 8 A100 GPUs (e.g. &lt;code&gt;Standard_ND96asr_v4&lt;/code&gt; or &lt;code&gt;p4d.24xlarge&lt;/code&gt;). Note that these instance types may not be available in all regions, or may be difficult to provision. In Databricks, note that you must select the GPU runtime first, and unselect &#34;Use Photon&#34;, for these instance types to appear (where supported).&lt;/li&gt; &#xA; &lt;li&gt;Open the &lt;code&gt;train_dolly&lt;/code&gt; notebook in the Repo (which is the &lt;code&gt;train_dolly.py&lt;/code&gt; file in the Github &lt;code&gt;dolly&lt;/code&gt; repo), attach to your GPU cluster, and run all cells. When training finishes, the notebook will save the model under &lt;code&gt;/dbfs/dolly_training&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Training on Other Instances&lt;/h2&gt; &#xA;&lt;p&gt;A100 instance types are not available in all cloud regions, or can be hard to provision. Training is possible on other GPU instance types, with small modifications to reduce memory usage. Training will take longer on these instances. These modifications are not necessarily optimal, but are simple to make.&lt;/p&gt; &#xA;&lt;h3&gt;A10 GPUs&lt;/h3&gt; &#xA;&lt;p&gt;To run on A10 instances (ex: &lt;code&gt;g5.24xlarge&lt;/code&gt;, 4 x A10 24GB; &lt;code&gt;Standard_NV72ads_A10_v5&lt;/code&gt;, 2 x A10), make the following changes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modify the deepspeed config file &lt;code&gt;ds_z3_bf16_config.json&lt;/code&gt; to configure optimizer offload. Within the &lt;code&gt;&#34;zero_optimization&#34;&lt;/code&gt; section, add: &lt;pre&gt;&lt;code&gt;&#34;offload_optimizer&#34;: {&#xA;  &#34;device&#34;: &#34;cpu&#34;,&#xA;  &#34;pin_memory&#34;: true&#xA;},&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Set the &lt;code&gt;num_gpus&lt;/code&gt; widget in &lt;code&gt;train_dolly&lt;/code&gt; to the number of GPUs in your instance, such as 2 or 4, before running&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;With 4 A10s, an epoch completes in about 7 hours.&lt;/p&gt; &#xA;&lt;h3&gt;V100 GPUs&lt;/h3&gt; &#xA;&lt;p&gt;To run on V100 instances with 32GB of GPU memory (ex: &lt;code&gt;p3dn.24xlarge&lt;/code&gt; or &lt;code&gt;Standard_ND40rs_v2&lt;/code&gt;), make the following changes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modify the deepspeed config to enable optimizer offload, as above&lt;/li&gt; &#xA; &lt;li&gt;Modify &lt;code&gt;trainer.py&lt;/code&gt; to disable &lt;code&gt;bf16&lt;/code&gt; and enable &lt;code&gt;fp16&lt;/code&gt; in &lt;code&gt;TrainingArguments&lt;/code&gt;: &lt;pre&gt;&lt;code&gt;...&#xA;fp16=True,&#xA;bf16=False,&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Set the &lt;code&gt;num_gpus&lt;/code&gt; widget in &lt;code&gt;train_dolly&lt;/code&gt; to the number of GPUs in your instance, typically 8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;With 8 V100s, an epoch completes in about 3.5 hours. Note that the resulting model may be slightly different when trained with &lt;code&gt;fp16&lt;/code&gt; versus &lt;code&gt;bf16&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Running Unit Tests Locally&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pyenv local 3.8.13&#xA;python -m venv .venv&#xA;. .venv/bin/activate&#xA;pip install -r requirements_dev.txt&#xA;./run_pytest.sh&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>