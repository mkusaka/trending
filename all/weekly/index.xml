<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-22T01:46:06Z</updated>
  <subtitle>Weekly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>iv-org/invidious</title>
    <updated>2023-10-22T01:46:06Z</updated>
    <id>tag:github.com,2023-10-22:/iv-org/invidious</id>
    <link href="https://github.com/iv-org/invidious" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Invidious is an alternative front-end to YouTube&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/assets/invidious-colored-vector.svg?sanitize=true&#34; width=&#34;192&#34; height=&#34;192&#34; alt=&#34;Invidious logo&#34;&gt; &#xA; &lt;h1&gt;Invidious&lt;/h1&gt; &#xA; &lt;a href=&#34;https://www.gnu.org/licenses/agpl-3.0.en.html&#34;&gt; &lt;img alt=&#34;License: AGPLv3&#34; src=&#34;https://shields.io/badge/License-AGPL%20v3-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/iv-org/invidious/actions&#34;&gt; &lt;img alt=&#34;Build Status&#34; src=&#34;https://github.com/iv-org/invidious/workflows/Invidious%20CI/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/iv-org/invidious/commits/master&#34;&gt; &lt;img alt=&#34;GitHub commits&#34; src=&#34;https://img.shields.io/github/commit-activity/y/iv-org/invidious?color=red&amp;amp;label=commits&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/iv-org/invidious/issues&#34;&gt; &lt;img alt=&#34;GitHub issues&#34; src=&#34;https://img.shields.io/github/issues/iv-org/invidious?color=important&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/iv-org/invidious/pulls&#34;&gt; &lt;img alt=&#34;GitHub pull requests&#34; src=&#34;https://img.shields.io/github/issues-pr/iv-org/invidious?color=blueviolet&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://hosted.weblate.org/engage/invidious/&#34;&gt; &lt;img alt=&#34;Translation Status&#34; src=&#34;https://hosted.weblate.org/widgets/invidious/-/translations/svg-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/humanetech-community/awesome-humane-tech&#34;&gt; &lt;img alt=&#34;Awesome Humane Tech&#34; src=&#34;https://raw.githubusercontent.com/humanetech-community/awesome-humane-tech/main/humane-tech-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;h3&gt;An open source alternative front-end to YouTube&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://invidious.io/&#34;&gt;Website&lt;/a&gt; &amp;nbsp;‚Ä¢&amp;nbsp; &lt;a href=&#34;https://instances.invidious.io/&#34;&gt;Instances list&lt;/a&gt; &amp;nbsp;‚Ä¢&amp;nbsp; &lt;a href=&#34;https://docs.invidious.io/faq/&#34;&gt;FAQ&lt;/a&gt; &amp;nbsp;‚Ä¢&amp;nbsp; &lt;a href=&#34;https://docs.invidious.io/&#34;&gt;Documentation&lt;/a&gt; &amp;nbsp;‚Ä¢&amp;nbsp; &lt;a href=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/#contribute&#34;&gt;Contribute&lt;/a&gt; &amp;nbsp;‚Ä¢&amp;nbsp; &lt;a href=&#34;https://invidious.io/donate/&#34;&gt;Donate&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h5&gt;Chat with us:&lt;/h5&gt; &#xA; &lt;a href=&#34;https://matrix.to/#/#invidious:matrix.org&#34;&gt; &lt;img alt=&#34;Matrix&#34; src=&#34;https://img.shields.io/matrix/invidious:matrix.org?label=Matrix&amp;amp;color=darkgreen&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://web.libera.chat/?channel=#invidious&#34;&gt; &lt;img alt=&#34;Libera.chat (IRC)&#34; src=&#34;https://img.shields.io/badge/IRC%20%28Libera.chat%29-%23invidious-darkgreen&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a rel=&#34;me&#34; href=&#34;https://social.tchncs.de/@invidious&#34;&gt; &lt;img alt=&#34;Fediverse: @invidious@social.tchncs.de&#34; src=&#34;https://img.shields.io/badge/Fediverse-%40invidious%40social.tchncs.de-darkgreen&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://invidious.io/contact/&#34;&gt; &lt;img alt=&#34;E-mail&#34; src=&#34;https://img.shields.io/badge/E%2d%2dmail-darkgreen&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Player&lt;/th&gt; &#xA;   &lt;th&gt;Preferences&lt;/th&gt; &#xA;   &lt;th&gt;Subscriptions&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/screenshots/01_player.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/screenshots/02_preferences.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/screenshots/03_subscriptions.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/screenshots/04_description.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/screenshots/05_preferences.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/screenshots/06_subscriptions.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;User features&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lightweight&lt;/li&gt; &#xA; &lt;li&gt;No ads&lt;/li&gt; &#xA; &lt;li&gt;No tracking&lt;/li&gt; &#xA; &lt;li&gt;No JavaScript required&lt;/li&gt; &#xA; &lt;li&gt;Light/Dark themes&lt;/li&gt; &#xA; &lt;li&gt;Customizable homepage&lt;/li&gt; &#xA; &lt;li&gt;Subscriptions independent from Google&lt;/li&gt; &#xA; &lt;li&gt;Notifications for all subscribed channels&lt;/li&gt; &#xA; &lt;li&gt;Audio-only mode (with background play on mobile)&lt;/li&gt; &#xA; &lt;li&gt;Support for Reddit comments&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/locales/&#34;&gt;Available in many languages&lt;/a&gt;, thanks to &lt;a href=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/#contribute&#34;&gt;our translators&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data import/export&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Import subscriptions from YouTube, NewPipe and Freetube&lt;/li&gt; &#xA; &lt;li&gt;Import watch history from YouTube and NewPipe&lt;/li&gt; &#xA; &lt;li&gt;Export subscriptions to NewPipe and Freetube&lt;/li&gt; &#xA; &lt;li&gt;Import/Export Invidious user data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Technical features&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Embedded video support&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.invidious.io/api/&#34;&gt;Developer API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Does not use official YouTube APIs&lt;/li&gt; &#xA; &lt;li&gt;No Contributor License Agreement (CLA)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using invidious:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://instances.invidious.io&#34;&gt;Select a public instance from the list&lt;/a&gt; and start watching videos right now!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hosting invidious:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.invidious.io/installation/&#34;&gt;Follow the installation instructions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The full documentation can be accessed online at &lt;a href=&#34;https://docs.invidious.io/&#34;&gt;https://docs.invidious.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The documentation&#39;s source code is available in this repository: &lt;a href=&#34;https://github.com/iv-org/documentation&#34;&gt;https://github.com/iv-org/documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Extensions&lt;/h3&gt; &#xA;&lt;p&gt;We highly recommend the use of &lt;a href=&#34;https://github.com/SimonBrazell/privacy-redirect#get&#34;&gt;Privacy Redirect&lt;/a&gt;, a browser extension that automatically redirects Youtube URLs to any Invidious instance and replaces embedded youtube videos on other websites with invidious.&lt;/p&gt; &#xA;&lt;p&gt;The documentation contains a list of browser extensions that we recommended to use along with Invidious.&lt;/p&gt; &#xA;&lt;p&gt;You can read more here: &lt;a href=&#34;https://docs.invidious.io/applications/&#34;&gt;https://docs.invidious.io/applications/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;h3&gt;Code&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork it ( &lt;a href=&#34;https://github.com/iv-org/invidious/fork&#34;&gt;https://github.com/iv-org/invidious/fork&lt;/a&gt; ).&lt;/li&gt; &#xA; &lt;li&gt;Create your feature branch (&lt;code&gt;git checkout -b my-new-feature&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Stage your files (&lt;code&gt;git add .&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Commit your changes (&lt;code&gt;git commit -am &#39;Add some feature&#39;&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Push to the branch (&lt;code&gt;git push origin my-new-feature&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Create a new pull request ( &lt;a href=&#34;https://github.com/iv-org/invidious/compare&#34;&gt;https://github.com/iv-org/invidious/compare&lt;/a&gt; ).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Translations&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://weblate.org&#34;&gt;Weblate&lt;/a&gt; to manage Invidious translations.&lt;/p&gt; &#xA;&lt;p&gt;You can suggest new translations and/or correction here: &lt;a href=&#34;https://hosted.weblate.org/engage/invidious/&#34;&gt;https://hosted.weblate.org/engage/invidious/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Creating an account is not required, but recommended, especially if you want to contribute regularly. Weblate also allows you to log-in with major SSO providers like Github, Gitlab, BitBucket, Google, ...&lt;/p&gt; &#xA;&lt;h2&gt;Projects using Invidious&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FreeTubeApp/FreeTube&#34;&gt;FreeTube&lt;/a&gt;: A libre software YouTube app for privacy.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sr.ht/~cadence/tube/&#34;&gt;CloudTube&lt;/a&gt;: A JavaScript-rich alternate YouTube player.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gitlab.com/Cha_de_L/peertubeify&#34;&gt;PeerTubeify&lt;/a&gt;: On YouTube, displays a link to the same video on PeerTube, if it exists.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deep-gaurav/MusicPiped&#34;&gt;MusicPiped&lt;/a&gt;: A material design music player that streams music from YouTube.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stephane-r/holoplay-pwa&#34;&gt;HoloPlay&lt;/a&gt;: Progressive Web App connecting on Invidious API&#39;s with search, playlists and favorites.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WatchTubeTeam/WatchTube&#34;&gt;WatchTube&lt;/a&gt;: Powerful YouTube client for Apple Watch.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yattee/yattee&#34;&gt;Yattee&lt;/a&gt;: Alternative YouTube frontend for iPhone, iPad, Mac and Apple TV.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codeberg.org/777/TubiTui&#34;&gt;TubiTui&lt;/a&gt;: A lightweight, libre, TUI-based YouTube client.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pystardust/ytfzf&#34;&gt;Ytfzf&lt;/a&gt;: A posix script to find and watch youtube videos from the terminal. (Without API).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iBicha/playlet&#34;&gt;Playlet&lt;/a&gt;: Unofficial Youtube client for Roku TV.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lamarios/clipious&#34;&gt;Clipious&lt;/a&gt;: Unofficial Invidious client for Android.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Liability&lt;/h2&gt; &#xA;&lt;p&gt;We take no responsibility for the use of our tool, or external instances provided by third parties. We strongly recommend you abide by the valid official regulations in your country. Furthermore, we refuse liability for any inappropriate use of Invidious, such as illegal downloading. This tool is provided to you in the spirit of free, open software.&lt;/p&gt; &#xA;&lt;p&gt;You may view the LICENSE in which this software is provided to you &lt;a href=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/LICENSE&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ol start=&#34;16&#34;&gt; &#xA;  &lt;li&gt;Limitation of Liability.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>THUDM/CogVLM</title>
    <updated>2023-10-22T01:46:06Z</updated>
    <id>tag:github.com,2023-10-22:/THUDM/CogVLM</id>
    <link href="https://github.com/THUDM/CogVLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;a state-of-the-art-level open visual language model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CogVLM&lt;/h1&gt; &#xA;&lt;p&gt;üìñ &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/cogvlm-paper.pdf&#34;&gt;PaperÔºàËÆ∫ÊñáÔºâ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üåê &lt;a href=&#34;http://36.103.203.44:7861/&#34;&gt;web demoÔºàÊµãËØïÁΩëÂùÄÔºâ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;CogVLM is a powerful &lt;strong&gt;open-source visual language model&lt;/strong&gt; (&lt;strong&gt;VLM&lt;/strong&gt;). CogVLM-17B has 10 billion vision parameters and 7 billion language parameters.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and rank the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., &lt;strong&gt;surpassing or matching PaLI-X 55B&lt;/strong&gt;. CogVLM can also &lt;a href=&#34;http://36.103.203.44:7861&#34;&gt;chat with you&lt;/a&gt; about images.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Chinese brief introduction: CogVLM ÊòØ‰∏Ä‰∏™Âº∫Â§ßÁöÑÂºÄÊ∫êËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºåÂà©Áî®ËßÜËßâ‰∏ìÂÆ∂Ê®°ÂùóÊ∑±Â∫¶Êï¥ÂêàËØ≠Ë®ÄÁºñÁ†ÅÂíåËßÜËßâÁºñÁ†ÅÔºåÂú® 10 È°πÊùÉÂ®ÅË∑®Ê®°ÊÄÅÂü∫ÂáÜ‰∏äÂèñÂæó‰∫ÜSOTAÊÄßËÉΩ„ÄÇÁõÆÂâç‰ªÖÊîØÊåÅËã±ÊñáÔºåÂêéÁª≠‰ºöÊèê‰æõ‰∏≠Ëã±ÂèåËØ≠ÁâàÊú¨ÊîØÊåÅÔºåÊ¨¢ËøéÊåÅÁª≠ÂÖ≥Ê≥®ÔºÅ&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/metrics-min.png&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;!-- CogVLM is powerful for answering various types of visual questions, including **Detailed Description &amp; Visual Question Answering**,  **Complex Counting**, **Visual Math Problem Solving**, **OCR-Free Reasonging**, **OCR-Free Visual Question Answering**, **World Knowledge**, **Referring Expression Comprehension**, **Programming with Visual Input**, **Grounding with Caption**, **Grounding Visual Question Answering**, etc. --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;CogVLM can accurately describe images in details with &lt;strong&gt;very few hallucinations&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Click for comparison with LLAVA-1.5 and MiniGPT-4.&lt;/summary&gt; &#xA;   &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/llava-comparison-min.png&#34; alt=&#34;LLAVA Comparision&#34;&gt;&lt;/p&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CogVLM can understand and answer various types of questions, and has a &lt;strong&gt;visual grounding&lt;/strong&gt; version.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/pear_grounding.png&#34; width=&#34;90%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CogVLM sometimes captures more detailed content than GPT-4V(ision).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/compare-min.png&#34; width=&#34;90%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- ![compare](assets/compare.png) --&gt; &#xA;&lt;br&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to expand more examples.&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/chat.png&#34; alt=&#34;Chat Examples&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Method&lt;/h2&gt; &#xA;&lt;p&gt;CogVLM model comprises four fundamental components: a vision transformer (ViT) encoder, an MLP adapter, a pretrained large language model (GPT), and a &lt;strong&gt;visual expert module&lt;/strong&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/cogvlm-paper.pdf&#34;&gt;Paper&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/method-min.png&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;We support two GUIs for model inference, &lt;strong&gt;web demo&lt;/strong&gt; and &lt;strong&gt;CLI&lt;/strong&gt;. If you want to use it in your python code, it is easy to modify the CLI scripts for your case.&lt;/p&gt; &#xA;&lt;p&gt;First, we need to install the dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;python -m spacy download en_core_web_sm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Hardware requirement&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Model Inference: 1 * A100(80G) or 2 * RTX 3090(24G).&lt;/li&gt; &#xA; &lt;li&gt;Finetuning: 4 * A100(80G) &lt;em&gt;[Recommend]&lt;/em&gt; or 8* RTX 3090(24G).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- ### Online Web Demo&#xA;We provide a [web demo](http://36.103.203.44:7861/) based on [Gradio](https://gradio.app). --&gt; &#xA;&lt;h3&gt;Web Demo&lt;/h3&gt; &#xA;&lt;p&gt;We also offer a local web demo based on Gradio. First, install Gradio by running: &lt;code&gt;pip install gradio&lt;/code&gt;. Then download and enter this repository and run &lt;code&gt;web_demo.py&lt;/code&gt;. See the next section for detailed usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python web_demo.py --from_pretrained cogvlm-chat --version chat --english --bf16&#xA;python web_demo.py --from_pretrained cogvlm-grounding-generalist --version base --english --bf16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The GUI of the web demo looks like:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/web_demo-min.png&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;CLI&lt;/h3&gt; &#xA;&lt;p&gt;We open-source different checkpoints for different downstreaming tasks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;cogvlm-chat&lt;/code&gt; The model after SFT for alignment, which supports chat like GPT-4V.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cogvlm-base-224&lt;/code&gt; The original checkpoint after text-image pretraining.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cogvlm-base-490&lt;/code&gt; The finetuned version on &lt;code&gt;490px&lt;/code&gt; resolution from &lt;code&gt;cogvlm-base-224&lt;/code&gt;. The finetuning data includes the training sets of VQA datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cogvlm-grounding-generalist&lt;/code&gt;. This checkpoint supports different visual grounding tasks, e.g. REC, Grounding Captioning, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Run CLI demo via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python cli_demo.py --from_pretrained cogvlm-base-224 --version base --english --bf16 --no_prompt&#xA;python cli_demo.py --from_pretrained cogvlm-base-490 --version base --english --bf16 --no_prompt&#xA;python cli_demo.py --from_pretrained cogvlm-chat --version chat --english --bf16&#xA;python cli_demo.py --from_pretrained cogvlm-grounding-generalist --version base --english --bf16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The program will automatically download the sat model and interact in the command line. You can generate replies by entering instructions and pressing enter. Enter &lt;code&gt;clear&lt;/code&gt; to clear the conversation history and &lt;code&gt;stop&lt;/code&gt; to stop the program.&lt;/p&gt; &#xA;&lt;h4&gt;Multi-GPU inference&lt;/h4&gt; &#xA;&lt;p&gt;We also support model parallel inference, which splits model to multiple (2/4/8) GPUs. &lt;code&gt;--nproc-per-node=[n]&lt;/code&gt; in the following command controls the number of used GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --standalone --nnodes=1 --nproc-per-node=2 cli_demo.py --from_pretrained cogvlm-chat --version chat --english --bf16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have trouble in accessing huggingface.co, you can add &lt;code&gt;--local_tokenizer /path/to/vicuna-7b-v1.5&lt;/code&gt; to load the tokenizer.&lt;/li&gt; &#xA; &lt;li&gt;If you have trouble in automatically downloading model with üî®&lt;a href=&#34;https://github.com/THUDM/SwissArmyTransformer&#34;&gt;SAT&lt;/a&gt;, try downloading from ü§ñ&lt;a href=&#34;https://www.modelscope.cn/models/ZhipuAI/CogVLM/summary&#34;&gt;modelscope&lt;/a&gt; or ü§ó&lt;a href=&#34;https://huggingface.co/THUDM/CogVLM&#34;&gt;huggingface&lt;/a&gt; manually.&lt;/li&gt; &#xA; &lt;li&gt;Download model using üî®&lt;a href=&#34;https://github.com/THUDM/SwissArmyTransformer&#34;&gt;SAT&lt;/a&gt;, the model will be saved to the default location &lt;code&gt;~/.sat_models&lt;/code&gt;. Change the default location by setting the environment variable &lt;code&gt;SAT_HOME&lt;/code&gt;. For example, if you want to save the model to &lt;code&gt;/path/to/my/models&lt;/code&gt;, you can run &lt;code&gt;export SAT_HOME=/path/to/my/models&lt;/code&gt; before running the python command.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The program provides the following hyperparameters to control the generation process:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: cli_demo.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE] [--english]&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --max_length MAX_LENGTH&#xA;                        max length of the total sequence&#xA;  --top_p TOP_P         top p for nucleus sampling&#xA;  --top_k TOP_K         top k for top k sampling&#xA;  --temperature TEMPERATURE&#xA;                        temperature for sampling&#xA;  --english             only output English&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Finetuning&lt;/h3&gt; &#xA;&lt;p&gt;You may want to use CogVLM in your own task, which needs a &lt;strong&gt;different output style or domain knowledge&lt;/strong&gt;. We here provide a finetuning example for &lt;strong&gt;Captcha Recognition&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Start by downloading the &lt;a href=&#34;https://www.kaggle.com/datasets/aadhavvignesh/captcha-images&#34;&gt;Captcha Images dataset&lt;/a&gt;. Once downloaded, extract the contents of the ZIP file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To create a train/validation/test split in the ratio of 80/5/15, execute the following:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/split_dataset.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start the fine-tuning process with this command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/finetune_(224/490)_lora.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Merge the model to &lt;code&gt;model_parallel_size=1&lt;/code&gt;: (replace the 4 below with your training &lt;code&gt;MP_SIZE&lt;/code&gt;)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --standalone --nnodes=1 --nproc-per-node=4 merge_model.py --version base --bf16 --from_pretrained ./checkpoints/merged_lora_(224/490)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Evaluate the performance of your model.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/evaluate_(224/490).sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;It is recommended to use the &lt;code&gt;490px&lt;/code&gt; version. However, if you have limited GPU resources (such as only one node with 8* RTX 3090), you can try &lt;code&gt;224px&lt;/code&gt; version with model parallel.&lt;/p&gt; &#xA;&lt;p&gt;The anticipated result of this script is around &lt;code&gt;95%&lt;/code&gt; accuracy on test set.&lt;/p&gt; &#xA;&lt;p&gt;It is worth noting that the fine-tuning examples only tune limited parameters. (Expert only) If you want to get &lt;code&gt;&amp;gt;98%&lt;/code&gt; accuracy, you need to increase the trainable parameters in &lt;code&gt;finetune_demo.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code in this repository is open source under the &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/LICENSE&#34;&gt;Apache-2.0 license&lt;/a&gt;, while the use of the CogVLM model weights must comply with the &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/MODEL_LICENSE&#34;&gt;Model License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation &amp;amp; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, please consider citing the following papers&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Yes, you can help us!!!&#xA;The paper (ArXiv ID 5148899) has been &#34;on hold&#34; by arXiv for more than two weeks without clear reason. &#xA;If you happen to know the moderators (cs.CV), please help to accelarate the process. Thank you!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the instruction fine-tuning phase of the CogVLM, there are some English image-text data from the &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;MiniGPT-4&lt;/a&gt;, &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLAVA&lt;/a&gt;, &lt;a href=&#34;https://github.com/FuxiaoLiu/LRV-Instruction&#34;&gt;LRV-Instruction&lt;/a&gt;, &lt;a href=&#34;https://github.com/SALT-NLP/LLaVAR&#34;&gt;LLaVAR&lt;/a&gt; and &lt;a href=&#34;https://github.com/shikras/shikra&#34;&gt;Shikra&lt;/a&gt; projects, as well as many classic cross-modal work datasets. We sincerely thank them for their contributions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Audio-AGI/AudioSep</title>
    <updated>2023-10-22T01:46:06Z</updated>
    <id>tag:github.com,2023-10-22:/Audio-AGI/AudioSep</id>
    <link href="https://github.com/Audio-AGI/AudioSep" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of &#34;Separate Anything You Describe&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Separate Anything You Describe&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.05037&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Audio-AGI/AudioSep/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Audio-AGI/AudioSep?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://audio-agi.github.io/Separate-Anything-You-Describe&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&amp;amp;style=flat-square&#34; alt=&#34;githubio&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/Audio-AGI/AudioSep/blob/main/AudioSep_Colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Audio-AGI/AudioSep&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/cjwbw/audiosep&#34;&gt;&lt;img src=&#34;https://replicate.com/cjwbw/audiosep/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the official implementation of &lt;a href=&#34;https://audio-agi.github.io/Separate-Anything-You-Describe/AudioSep_arXiv.pdf&#34;&gt;&#34;Separate Anything You Describe&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We introduce AudioSep, a foundation model for open-domain sound separation with natural language queries. AudioSep demonstrates strong separation performance and impressive zero-shot generalization ability on numerous tasks such as audio event separation, musical instrument separation, and speech enhancement. Check the separated audio examples in the &lt;a href=&#34;https://audio-agi.github.io/Separate-Anything-You-Describe/&#34;&gt;Demo Page&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;middle&#34; width=&#34;800&#34; src=&#34;https://raw.githubusercontent.com/Audio-AGI/AudioSep/main/assets/results.png&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; AudioSep training &amp;amp; finetuning code release.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; AudioSep base model checkpoint release.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Evaluation benchmark release.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repository and setup the conda environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/Audio-AGI/AudioSep.git &amp;amp;&amp;amp; \&#xA;cd AudioSep &amp;amp;&amp;amp; \ &#xA;conda env create -f environment.yml &amp;amp;&amp;amp; \&#xA;conda activate AudioSep&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://huggingface.co/spaces/Audio-AGI/AudioSep/tree/main/checkpoint&#34;&gt;model weights&lt;/a&gt; at &lt;code&gt;checkpoint/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pipeline import build_audiosep, inference&#xA;import torch&#xA;&#xA;device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)&#xA;&#xA;model = build_audiosep(&#xA;      config_yaml=&#39;config/audiosep_base.yaml&#39;, &#xA;      checkpoint_path=&#39;checkpoint/audiosep_base_4M_steps.ckpt&#39;, &#xA;      device=device)&#xA;&#xA;audio_file = &#39;path_to_audio_file&#39;&#xA;text = &#39;textual_description&#39;&#xA;output_file=&#39;separated_audio.wav&#39;&#xA;&#xA;# AudioSep processes the audio at 32 kHz sampling rate  &#xA;inference(model, audio_file, text, output_file, device)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;To load directly from Hugging Face, you can do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from models.audiosep import AudioSep&#xA;from utils import get_ss_model&#xA;import torch&#xA;&#xA;device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)&#xA;&#xA;ss_model = get_ss_model(&#39;config/audiosep_base.yaml&#39;)&#xA;&#xA;model = AudioSep.from_pretrained(&#34;nielsr/audiosep-demo&#34;, ss_model=ss_model)&#xA;&#xA;audio_file = &#39;path_to_audio_file&#39;&#xA;text = &#39;textual_description&#39;&#xA;output_file=&#39;separated_audio.wav&#39;&#xA;&#xA;# AudioSep processes the audio at 32 kHz sampling rate  &#xA;inference(model, audio_file, text, output_file, device)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Use chunk-based inference to save memory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;inference(model, audio_file, text, output_file, device, use_chunk=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To utilize your audio-text paired dataset:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Format your dataset to match our JSON structure. Refer to the provided template at &lt;code&gt;datafiles/template.json&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Update the &lt;code&gt;config/audiosep_base.yaml&lt;/code&gt; file by listing your formatted JSON data files under &lt;code&gt;datafiles&lt;/code&gt;. For example:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;data:&#xA;    datafiles:&#xA;        - &#39;datafiles/your_datafile_1.json&#39;&#xA;        - &#39;datafiles/your_datafile_2.json&#39;&#xA;        ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train AudioSep from scatch:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python train.py --workspace workspace/AudioSep --config_yaml config/audiosep_base.yaml --resume_checkpoint_path checkpoint/ &#39;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finetune AudioSep from pretrained checkpoint:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python train.py --workspace workspace/AudioSep --config_yaml config/audiosep_base.yaml --resume_checkpoint_path path_to_checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Benchmark Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Download the &lt;a href=&#34;https://drive.google.com/drive/folders/1PbCsuvdrzwAZZ_fwIzF0PeVGZkTk0-kL?usp=sharing&#34;&gt;evaluation data&lt;/a&gt; under the &lt;code&gt;evaluation/data&lt;/code&gt; folder. The data should be organized as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;evaluation:&#xA;    data:&#xA;        - audioset/&#xA;        - audiocaps/&#xA;        - vggsound/&#xA;        - music/&#xA;        - clotho/&#xA;        - esc50/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run benchmark inference script, the results will be saved at &lt;code&gt;eval_logs/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python benchmark.py --checkpoint_path audiosep_base_4M_steps.ckpt&#xA;&#xA;&#34;&#34;&#34;&#xA;Evaluation Results:&#xA;&#xA;VGGSound Avg SDRi: 9.144, SISDR: 9.043&#xA;MUSIC Avg SDRi: 10.508, SISDR: 9.425&#xA;ESC-50 Avg SDRi: 10.040, SISDR: 8.810&#xA;AudioSet Avg SDRi: 7.739, SISDR: 6.903&#xA;AudioCaps Avg SDRi: 8.220, SISDR: 7.189&#xA;Clotho Avg SDRi: 6.850, SISDR: 5.242&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cite this work&lt;/h2&gt; &#xA;&lt;p&gt;If you found this tool useful, please consider citing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{liu2023separate,&#xA;  title={Separate Anything You Describe},&#xA;  author={Liu, Xubo and Kong, Qiuqiang and Zhao, Yan and Liu, Haohe and Yuan, Yi and Liu, Yuzhuo and Xia, Rui and Wang, Yuxuan and Plumbley, Mark D and Wang, Wenwu},&#xA;  journal={arXiv preprint arXiv:2308.05037},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{liu22w_interspeech,&#xA;  title={Separate What You Describe: Language-Queried Audio Source Separation},&#xA;  author={Liu, Xubo and Liu, Haohe and Kong, Qiuqiang and Mei, Xinhao and Zhao, Jinzheng and Huang, Qiushi and Plumbley, Mark D and Wang, Wenwu},&#xA;  year=2022,&#xA;  booktitle={Proc. Interspeech},&#xA;  pages={1801--1805},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>