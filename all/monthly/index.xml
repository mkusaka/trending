<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-01T01:55:27Z</updated>
  <subtitle>Monthly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>BuilderIO/qwik</title>
    <updated>2022-10-01T01:55:27Z</updated>
    <id>tag:github.com,2022-10-01:/BuilderIO/qwik</id>
    <link href="https://github.com/BuilderIO/qwik" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The HTML-first framework. Instant apps of any size with ~ 1kb JS&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Qwik Logo&#34; width=&#34;400&#34; src=&#34;https://raw.githubusercontent.com/BuilderIO/qwik/main/.github/assets/qwik-logo.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;a href=&#34;https://youtu.be/0dC11DMR3fU?t=154&#34;&gt; &lt;img width=&#34;1229&#34; alt=&#34;WWC22 - Qwik + Partytown: How to remove 99% of JavaScript from main thread&#34; src=&#34;https://raw.githubusercontent.com/BuilderIO/qwik/main/.github/assets/Qwik-video-thumbnail.png&#34;&gt; &lt;/a&gt; &#xA;&lt;h1&gt;The HTML-first framework&lt;/h1&gt; &#xA;&lt;p&gt;Qwik offers the fastest possible page load times - regardless of the complexity of your website. Qwik is so fast because it allows fully interactive sites to load with almost no JavaScript and &lt;a href=&#34;https://qwik.builder.io/docs/concepts/resumable/&#34;&gt;pickup from where the server left off&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;As users interact with the site, only the necessary parts of the site load on-demand. This &lt;a href=&#34;https://qwik.builder.io/docs/concepts/progressive/&#34;&gt;precision lazy-loading&lt;/a&gt; is what makes Qwik so quick.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm create qwik@latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understand the difference between &lt;a href=&#34;https://qwik.builder.io/docs/concepts/resumable/&#34;&gt;resumable and replayable&lt;/a&gt; applications.&lt;/li&gt; &#xA; &lt;li&gt;Learn about Qwik&#39;s high level &lt;a href=&#34;https://qwik.builder.io/docs/think-qwik/&#34;&gt;mental model&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://qwik.builder.io/&#34;&gt;Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://qwik.builder.io/examples/introduction/hello-world/&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://qwik.builder.io/tutorial/welcome/overview/&#34;&gt;Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://qwik.builder.io/media/#videos&#34;&gt;Videos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://qwik.builder.io/media/#podcasts&#34;&gt;Podcasts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://qwik.builder.io/media/#presentations&#34;&gt;Presentations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://qwik.builder.io/media/#blogs&#34;&gt;Blogs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ping us at &lt;a href=&#34;https://twitter.com/QwikDev&#34;&gt;@QwikDev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Join our &lt;a href=&#34;https://qwik.builder.io/chat&#34;&gt;Discord&lt;/a&gt; community&lt;/li&gt; &#xA; &lt;li&gt;Join all the &lt;a href=&#34;https://qwikcommunity.com&#34;&gt;other community groups&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://github.com/BuilderIO/qwik/raw/main/CONTRIBUTING.md&#34;&gt;Contributing.md&lt;/a&gt; for more information on how to build Qwik from the source and contribute!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://partytown.builder.io/&#34;&gt;Partytown&lt;/a&gt;: Relocate resource intensive third-party scripts off of the main thread and into a web worker üéâ.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BuilderIO/mitosis&#34;&gt;Mitosis&lt;/a&gt;: Write components once, run everywhere. Compiles to Vue, React, Solid, Angular, Svelte, and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BuilderIO/builder&#34;&gt;Builder&lt;/a&gt;: Drag and drop page builder and CMS for React, Vue, Angular, and more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Made with ‚ù§Ô∏è by &lt;a target=&#34;_blank&#34; href=&#34;https://www.builder.io/&#34;&gt;Builder.io&lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AUTOMATIC1111/stable-diffusion-webui</title>
    <updated>2022-10-01T01:55:27Z</updated>
    <id>tag:github.com,2022-10-01:/AUTOMATIC1111/stable-diffusion-webui</id>
    <link href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stable Diffusion web UI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stable Diffusion web UI&lt;/h1&gt; &#xA;&lt;p&gt;A browser interface based on Gradio library for Stable Diffusion.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/txt2img_Screenshot.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts&#34;&gt;custom scripts&lt;/a&gt; wiki page for extra scripts developed by users.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features&#34;&gt;Detailed feature showcase with images&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Original txt2img and img2img modes&lt;/li&gt; &#xA; &lt;li&gt;One click install and run script (but you still must install python and git)&lt;/li&gt; &#xA; &lt;li&gt;Outpainting&lt;/li&gt; &#xA; &lt;li&gt;Inpainting&lt;/li&gt; &#xA; &lt;li&gt;Prompt&lt;/li&gt; &#xA; &lt;li&gt;Stable Diffusion upscale&lt;/li&gt; &#xA; &lt;li&gt;Attention, specify parts of text that the model should pay more attention to &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;a man in a ((txuedo)) - will pay more attentinoto tuxedo&lt;/li&gt; &#xA;   &lt;li&gt;a man in a (txuedo:1.21) - alternative syntax&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Loopback, run img2img procvessing multiple times&lt;/li&gt; &#xA; &lt;li&gt;X/Y plot, a way to draw a 2 dimensional plot of images with different parameters&lt;/li&gt; &#xA; &lt;li&gt;Textual Inversion &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;have as many embeddings as you want and use any names you like for them&lt;/li&gt; &#xA;   &lt;li&gt;use multiple embeddings with different numbers of vectors per token&lt;/li&gt; &#xA;   &lt;li&gt;works with half precision floating point numbers&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Extras tab with: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GFPGAN, neural network that fixes faces&lt;/li&gt; &#xA;   &lt;li&gt;CodeFormer, face restoration tool as an alternative to GFPGAN&lt;/li&gt; &#xA;   &lt;li&gt;RealESRGAN, neural network upscaler&lt;/li&gt; &#xA;   &lt;li&gt;ESRGAN, neural network upscaler with a lot of third party models&lt;/li&gt; &#xA;   &lt;li&gt;SwinIR, neural network upscaler&lt;/li&gt; &#xA;   &lt;li&gt;LDSR, Latent diffusion super resolution upscaling&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Resizing aspect ratio options&lt;/li&gt; &#xA; &lt;li&gt;Sampling method selection&lt;/li&gt; &#xA; &lt;li&gt;Interrupt processing at any time&lt;/li&gt; &#xA; &lt;li&gt;4GB video card support (also reports of 2GB working)&lt;/li&gt; &#xA; &lt;li&gt;Correct seeds for batches&lt;/li&gt; &#xA; &lt;li&gt;Prompt length validation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;get length of prompt in tokensas you type&lt;/li&gt; &#xA;   &lt;li&gt;get a warning after geenration if some text was truncated&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Generation parameters &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;parameters you used to generate images are saved with that image&lt;/li&gt; &#xA;   &lt;li&gt;in PNG chunks for PNG, in EXIF for JPEG&lt;/li&gt; &#xA;   &lt;li&gt;can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI&lt;/li&gt; &#xA;   &lt;li&gt;can be disabled in settings&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Settings page&lt;/li&gt; &#xA; &lt;li&gt;Running arbitrary python code from UI (must run with commandline flag to enable)&lt;/li&gt; &#xA; &lt;li&gt;Mouseover hints for most UI elements&lt;/li&gt; &#xA; &lt;li&gt;Possible to change defaults/mix/max/step values for UI elements via text config&lt;/li&gt; &#xA; &lt;li&gt;Random artist button&lt;/li&gt; &#xA; &lt;li&gt;Tiling support, a checkbox to create images that can be tiled like textures&lt;/li&gt; &#xA; &lt;li&gt;Progress bar and live image generation preview&lt;/li&gt; &#xA; &lt;li&gt;Negative prompt, an extra text field that allows you to list what you don&#39;t want to see in generated image&lt;/li&gt; &#xA; &lt;li&gt;Styles, a way to save part of prompt and easily apply them via dropdown later&lt;/li&gt; &#xA; &lt;li&gt;Variations, a way to generate same image but with tiny differences&lt;/li&gt; &#xA; &lt;li&gt;Seed resizing, a way to generate same image but at slightly different resolution&lt;/li&gt; &#xA; &lt;li&gt;CLIP interrogator, a button that tries to guess prompt from an image&lt;/li&gt; &#xA; &lt;li&gt;Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway&lt;/li&gt; &#xA; &lt;li&gt;Batch Processing, process a group of files using img2img&lt;/li&gt; &#xA; &lt;li&gt;Img2img Alternative&lt;/li&gt; &#xA; &lt;li&gt;Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions&lt;/li&gt; &#xA; &lt;li&gt;Reloading checkpoints on the fly&lt;/li&gt; &#xA; &lt;li&gt;Checkpoint Merger, a tab that allows you to merge two checkpoints into one&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts&#34;&gt;Custom scripts&lt;/a&gt; with many extensions from community&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation and Running&lt;/h2&gt; &#xA;&lt;p&gt;Make sure the required &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies&#34;&gt;dependencies&lt;/a&gt; are met and follow the instructions available for both &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs&#34;&gt;NVidia&lt;/a&gt; (recommended) and &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs&#34;&gt;AMD&lt;/a&gt; GPUs.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, use Google Colab:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1kw3egmSn-KgWsikYvOMjJkVDsPLjEMzl&#34;&gt;Colab, maintained by Akaibu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Iy-xW9t1-OQWhb0hNxueGij8phCyluOh&#34;&gt;Colab, original by me, outdated&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Automatic Installation on Windows&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://www.python.org/downloads/windows/&#34;&gt;Python 3.10.6&lt;/a&gt;, checking &#34;Add Python to PATH&#34;&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://git-scm.com/download/win&#34;&gt;git&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download the stable-diffusion-webui repository, for example by running &lt;code&gt;git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Place &lt;code&gt;model.ckpt&lt;/code&gt; in the &lt;code&gt;models&lt;/code&gt; directory (see &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies&#34;&gt;dependencies&lt;/a&gt; for where to get it).&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;em&gt;(Optional)&lt;/em&gt;&lt;/em&gt; Place &lt;code&gt;GFPGANv1.4.pth&lt;/code&gt; in the base directory, alongside &lt;code&gt;webui.py&lt;/code&gt; (see &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies&#34;&gt;dependencies&lt;/a&gt; for where to get it).&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;webui-user.bat&lt;/code&gt; from Windows Explorer as normal, non-administrator, user.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Automatic Installation on Linux&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Debian-based:&#xA;sudo apt install wget git python3 python3-venv&#xA;# Red Hat-based:&#xA;sudo dnf install wget git python3&#xA;# Arch-based:&#xA;sudo pacman -S wget git python3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;To install in &lt;code&gt;/home/$(whoami)/stable-diffusion-webui/&lt;/code&gt;, run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash &amp;lt;(wget -qO- https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation on Apple Silicon&lt;/h3&gt; &#xA;&lt;p&gt;Find the instructions &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s how to add code to this repo: &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The documentation was moved from this README over to the project&#39;s &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki&#34;&gt;wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stable Diffusion - &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;https://github.com/CompVis/stable-diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;https://github.com/CompVis/taming-transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;k-diffusion - &lt;a href=&#34;https://github.com/crowsonkb/k-diffusion.git&#34;&gt;https://github.com/crowsonkb/k-diffusion.git&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GFPGAN - &lt;a href=&#34;https://github.com/TencentARC/GFPGAN.git&#34;&gt;https://github.com/TencentARC/GFPGAN.git&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CodeFormer - &lt;a href=&#34;https://github.com/sczhou/CodeFormer&#34;&gt;https://github.com/sczhou/CodeFormer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ESRGAN - &lt;a href=&#34;https://github.com/xinntao/ESRGAN&#34;&gt;https://github.com/xinntao/ESRGAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SwinIR - &lt;a href=&#34;https://github.com/JingyunLiang/SwinIR&#34;&gt;https://github.com/JingyunLiang/SwinIR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LDSR - &lt;a href=&#34;https://github.com/Hafiidz/latent-diffusion&#34;&gt;https://github.com/Hafiidz/latent-diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ideas for optimizations - &lt;a href=&#34;https://github.com/basujindal/stable-diffusion&#34;&gt;https://github.com/basujindal/stable-diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Doggettx - Cross Attention layer optimization - &lt;a href=&#34;https://github.com/Doggettx/stable-diffusion&#34;&gt;https://github.com/Doggettx/stable-diffusion&lt;/a&gt;, original idea for prompt editing.&lt;/li&gt; &#xA; &lt;li&gt;Idea for SD upscale - &lt;a href=&#34;https://github.com/jquesnelle/txt2imghd&#34;&gt;https://github.com/jquesnelle/txt2imghd&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Noise generation for outpainting mk2 - &lt;a href=&#34;https://github.com/parlance-zz/g-diffuser-bot&#34;&gt;https://github.com/parlance-zz/g-diffuser-bot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CLIP interrogator idea and borrowing some code - &lt;a href=&#34;https://github.com/pharmapsychotic/clip-interrogator&#34;&gt;https://github.com/pharmapsychotic/clip-interrogator&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.&lt;/li&gt; &#xA; &lt;li&gt;(You)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>CompVis/stable-diffusion</title>
    <updated>2022-10-01T01:55:27Z</updated>
    <id>tag:github.com,2022-10-01:/CompVis/stable-diffusion</id>
    <link href="https://github.com/CompVis/stable-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A latent text-to-image diffusion model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Stable Diffusion was made possible thanks to a collaboration with &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt; and &lt;a href=&#34;https://runwayml.com/&#34;&gt;Runway&lt;/a&gt; and builds upon our previous work:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/rromb&#34;&gt;Robin Rombach&lt;/a&gt;*, &lt;a href=&#34;https://github.com/ablattmann&#34;&gt;Andreas Blattmann&lt;/a&gt;*, &lt;a href=&#34;https://github.com/qp-qp&#34;&gt;Dominik Lorenz&lt;/a&gt;, &lt;a href=&#34;https://github.com/pesser&#34;&gt;Patrick Esser&lt;/a&gt;, &lt;a href=&#34;https://hci.iwr.uni-heidelberg.de/Staff/bommer&#34;&gt;Bj√∂rn Ommer&lt;/a&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html&#34;&gt;CVPR &#39;22 Oral&lt;/a&gt; | &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;Project page&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/txt2img/merged-0006.png&#34; alt=&#34;txt2img-stable2&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#stable-diffusion-v1&#34;&gt;Stable Diffusion&lt;/a&gt; is a latent text-to-image diffusion model. Thanks to a generous compute donation from &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt; and support from &lt;a href=&#34;https://laion.ai/&#34;&gt;LAION&lt;/a&gt;, we were able to train a Latent Diffusion Model on 512x512 images from a subset of the &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-5B&lt;/a&gt; database. Similar to Google&#39;s &lt;a href=&#34;https://arxiv.org/abs/2205.11487&#34;&gt;Imagen&lt;/a&gt;, this model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. See &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#stable-diffusion-v1&#34;&gt;this section&lt;/a&gt; below and the &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion&#34;&gt;model card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;A suitable &lt;a href=&#34;https://conda.io/&#34;&gt;conda&lt;/a&gt; environment named &lt;code&gt;ldm&lt;/code&gt; can be created and activated with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also update an existing &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;latent diffusion&lt;/a&gt; environment by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install pytorch torchvision -c pytorch&#xA;pip install transformers==4.19.2 diffusers invisible-watermark&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Stable Diffusion v1&lt;/h2&gt; &#xA;&lt;p&gt;Stable Diffusion v1 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 860M UNet and CLIP ViT-L/14 text encoder for the diffusion model. The model was pretrained on 256x256 images and then finetuned on 512x512 images.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Stable Diffusion v1 is a general text-to-image diffusion model and therefore mirrors biases and (mis-)conceptions that are present in its training data. Details on the training procedure and data, as well as the intended use of the model can be found in the corresponding &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/Stable_Diffusion_v1_Model_Card.md&#34;&gt;model card&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;The weights are available via &lt;a href=&#34;https://huggingface.co/CompVis&#34;&gt;the CompVis organization at Hugging Face&lt;/a&gt; under &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE&#34;&gt;a license which contains specific use-based restrictions to prevent misuse and harm as informed by the model card, but otherwise remains permissive&lt;/a&gt;. While commercial use is permitted under the terms of the license, &lt;strong&gt;we do not recommend using the provided weights for services or products without additional safety mechanisms and considerations&lt;/strong&gt;, since there are &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/Stable_Diffusion_v1_Model_Card.md#limitations-and-bias&#34;&gt;known limitations and biases&lt;/a&gt; of the weights, and research on safe and ethical deployment of general text-to-image models is an ongoing effort. &lt;strong&gt;The weights are research artifacts and should be treated as such.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE&#34;&gt;The CreativeML OpenRAIL M license&lt;/a&gt; is an &lt;a href=&#34;https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses&#34;&gt;Open RAIL M license&lt;/a&gt;, adapted from the work that &lt;a href=&#34;https://bigscience.huggingface.co/&#34;&gt;BigScience&lt;/a&gt; and &lt;a href=&#34;https://www.licenses.ai/&#34;&gt;the RAIL Initiative&lt;/a&gt; are jointly carrying in the area of responsible AI licensing. See also &lt;a href=&#34;https://bigscience.huggingface.co/blog/the-bigscience-rail-license&#34;&gt;the article about the BLOOM Open RAIL license&lt;/a&gt; on which our license is based.&lt;/p&gt; &#xA;&lt;h3&gt;Weights&lt;/h3&gt; &#xA;&lt;p&gt;We currently provide the following checkpoints:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sd-v1-1.ckpt&lt;/code&gt;: 237k steps at resolution &lt;code&gt;256x256&lt;/code&gt; on &lt;a href=&#34;https://huggingface.co/datasets/laion/laion2B-en&#34;&gt;laion2B-en&lt;/a&gt;. 194k steps at resolution &lt;code&gt;512x512&lt;/code&gt; on &lt;a href=&#34;https://huggingface.co/datasets/laion/laion-high-resolution&#34;&gt;laion-high-resolution&lt;/a&gt; (170M examples from LAION-5B with resolution &lt;code&gt;&amp;gt;= 1024x1024&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sd-v1-2.ckpt&lt;/code&gt;: Resumed from &lt;code&gt;sd-v1-1.ckpt&lt;/code&gt;. 515k steps at resolution &lt;code&gt;512x512&lt;/code&gt; on &lt;a href=&#34;https://laion.ai/blog/laion-aesthetics/&#34;&gt;laion-aesthetics v2 5+&lt;/a&gt; (a subset of laion2B-en with estimated aesthetics score &lt;code&gt;&amp;gt; 5.0&lt;/code&gt;, and additionally filtered to images with an original size &lt;code&gt;&amp;gt;= 512x512&lt;/code&gt;, and an estimated watermark probability &lt;code&gt;&amp;lt; 0.5&lt;/code&gt;. The watermark estimate is from the &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-5B&lt;/a&gt; metadata, the aesthetics score is estimated using the &lt;a href=&#34;https://github.com/christophschuhmann/improved-aesthetic-predictor&#34;&gt;LAION-Aesthetics Predictor V2&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sd-v1-3.ckpt&lt;/code&gt;: Resumed from &lt;code&gt;sd-v1-2.ckpt&lt;/code&gt;. 195k steps at resolution &lt;code&gt;512x512&lt;/code&gt; on &#34;laion-aesthetics v2 5+&#34; and 10% dropping of the text-conditioning to improve &lt;a href=&#34;https://arxiv.org/abs/2207.12598&#34;&gt;classifier-free guidance sampling&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sd-v1-4.ckpt&lt;/code&gt;: Resumed from &lt;code&gt;sd-v1-2.ckpt&lt;/code&gt;. 225k steps at resolution &lt;code&gt;512x512&lt;/code&gt; on &#34;laion-aesthetics v2 5+&#34; and 10% dropping of the text-conditioning to improve &lt;a href=&#34;https://arxiv.org/abs/2207.12598&#34;&gt;classifier-free guidance sampling&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling steps show the relative improvements of the checkpoints: &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/v1-variants-scores.jpg&#34; alt=&#34;sd evaluation results&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Text-to-Image with Stable Diffusion&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/txt2img/merged-0005.png&#34; alt=&#34;txt2img-stable2&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/txt2img/merged-0007.png&#34; alt=&#34;txt2img-stable2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Stable Diffusion is a latent diffusion model conditioned on the (non-pooled) text embeddings of a CLIP ViT-L/14 text encoder. We provide a &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#reference-sampling-script&#34;&gt;reference script for sampling&lt;/a&gt;, but there also exists a &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#diffusers-integration&#34;&gt;diffusers integration&lt;/a&gt;, which we expect to see more active community development.&lt;/p&gt; &#xA;&lt;h4&gt;Reference Sampling Script&lt;/h4&gt; &#xA;&lt;p&gt;We provide a reference sampling script, which incorporates&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a &lt;a href=&#34;https://github.com/CompVis/stable-diffusion/pull/36&#34;&gt;Safety Checker Module&lt;/a&gt;, to reduce the probability of explicit outputs,&lt;/li&gt; &#xA; &lt;li&gt;an &lt;a href=&#34;https://github.com/ShieldMnt/invisible-watermark&#34;&gt;invisible watermarking&lt;/a&gt; of the outputs, to help viewers &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/scripts/tests/test_watermark.py&#34;&gt;identify the images as machine-generated&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#weights&#34;&gt;obtaining the &lt;code&gt;stable-diffusion-v1-*-original&lt;/code&gt; weights&lt;/a&gt;, link them&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p models/ldm/stable-diffusion-v1/&#xA;ln -s &amp;lt;path/to/model.ckpt&amp;gt; models/ldm/stable-diffusion-v1/model.ckpt &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and sample with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a photograph of an astronaut riding a horse&#34; --plms &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, this uses a guidance scale of &lt;code&gt;--scale 7.5&lt;/code&gt;, &lt;a href=&#34;https://github.com/CompVis/latent-diffusion/pull/51&#34;&gt;Katherine Crowson&#39;s implementation&lt;/a&gt; of the &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;PLMS&lt;/a&gt; sampler, and renders images of size 512x512 (which it was trained on) in 50 steps. All supported arguments are listed below (type &lt;code&gt;python scripts/txt2img.py --help&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;usage: txt2img.py [-h] [--prompt [PROMPT]] [--outdir [OUTDIR]] [--skip_grid] [--skip_save] [--ddim_steps DDIM_STEPS] [--plms] [--laion400m] [--fixed_code] [--ddim_eta DDIM_ETA]&#xA;                  [--n_iter N_ITER] [--H H] [--W W] [--C C] [--f F] [--n_samples N_SAMPLES] [--n_rows N_ROWS] [--scale SCALE] [--from-file FROM_FILE] [--config CONFIG] [--ckpt CKPT]&#xA;                  [--seed SEED] [--precision {full,autocast}]&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --prompt [PROMPT]     the prompt to render&#xA;  --outdir [OUTDIR]     dir to write results to&#xA;  --skip_grid           do not save a grid, only individual samples. Helpful when evaluating lots of samples&#xA;  --skip_save           do not save individual samples. For speed measurements.&#xA;  --ddim_steps DDIM_STEPS&#xA;                        number of ddim sampling steps&#xA;  --plms                use plms sampling&#xA;  --laion400m           uses the LAION400M model&#xA;  --fixed_code          if enabled, uses the same starting code across samples&#xA;  --ddim_eta DDIM_ETA   ddim eta (eta=0.0 corresponds to deterministic sampling&#xA;  --n_iter N_ITER       sample this often&#xA;  --H H                 image height, in pixel space&#xA;  --W W                 image width, in pixel space&#xA;  --C C                 latent channels&#xA;  --f F                 downsampling factor&#xA;  --n_samples N_SAMPLES&#xA;                        how many samples to produce for each given prompt. A.k.a. batch size&#xA;  --n_rows N_ROWS       rows in the grid (default: n_samples)&#xA;  --scale SCALE         unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))&#xA;  --from-file FROM_FILE&#xA;                        if specified, load prompts from this file&#xA;  --config CONFIG       path to config which constructs model&#xA;  --ckpt CKPT           path to checkpoint of model&#xA;  --seed SEED           the seed (for reproducible sampling)&#xA;  --precision {full,autocast}&#xA;                        evaluate at this precision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: The inference config for all v1 versions is designed to be used with EMA-only checkpoints. For this reason &lt;code&gt;use_ema=False&lt;/code&gt; is set in the configuration, otherwise the code will try to switch from non-EMA to EMA weights. If you want to examine the effect of EMA vs no EMA, we provide &#34;full&#34; checkpoints which contain both types of weights. For these, &lt;code&gt;use_ema=False&lt;/code&gt; will load and use the non-EMA weights.&lt;/p&gt; &#xA;&lt;h4&gt;Diffusers Integration&lt;/h4&gt; &#xA;&lt;p&gt;A simple way to download and sample Stable Diffusion is by using the &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main#new--stable-diffusion-is-now-fully-compatible-with-diffusers&#34;&gt;diffusers library&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;# make sure you&#39;re logged in with `huggingface-cli login`&#xA;from torch import autocast&#xA;from diffusers import StableDiffusionPipeline&#xA;&#xA;pipe = StableDiffusionPipeline.from_pretrained(&#xA;&#x9;&#34;CompVis/stable-diffusion-v1-4&#34;, &#xA;&#x9;use_auth_token=True&#xA;).to(&#34;cuda&#34;)&#xA;&#xA;prompt = &#34;a photo of an astronaut riding a horse on mars&#34;&#xA;with autocast(&#34;cuda&#34;):&#xA;    image = pipe(prompt)[&#34;sample&#34;][0]  &#xA;    &#xA;image.save(&#34;astronaut_rides_horse.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Image Modification with Stable Diffusion&lt;/h3&gt; &#xA;&lt;p&gt;By using a diffusion-denoising mechanism as first proposed by &lt;a href=&#34;https://arxiv.org/abs/2108.01073&#34;&gt;SDEdit&lt;/a&gt;, the model can be used for different tasks such as text-guided image-to-image translation and upscaling. Similar to the txt2img sampling script, we provide a script to perform image modification with Stable Diffusion.&lt;/p&gt; &#xA;&lt;p&gt;The following describes an example where a rough sketch made in &lt;a href=&#34;https://www.pinta-project.com/&#34;&gt;Pinta&lt;/a&gt; is converted into a detailed artwork.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/img2img.py --prompt &#34;A fantasy landscape, trending on artstation&#34; --init-img &amp;lt;path-to-img.jpg&amp;gt; --strength 0.8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, strength is a value between 0.0 and 1.0, that controls the amount of noise that is added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input. See the following example.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg&#34; alt=&#34;sketch-in&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/mountains-3.png&#34; alt=&#34;out3&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/mountains-2.png&#34; alt=&#34;out2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This procedure can, for example, also be used to upscale samples from the base model.&lt;/p&gt; &#xA;&lt;h2&gt;Comments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Our codebase for the diffusion models builds heavily on &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;OpenAI&#39;s ADM codebase&lt;/a&gt; and &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;https://github.com/lucidrains/denoising-diffusion-pytorch&lt;/a&gt;. Thanks for open-sourcing!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The implementation of the transformer encoder is from &lt;a href=&#34;https://github.com/lucidrains/x-transformers&#34;&gt;x-transformers&lt;/a&gt; by &lt;a href=&#34;https://github.com/lucidrains?tab=repositories&#34;&gt;lucidrains&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;      title={High-Resolution Image Synthesis with Latent Diffusion Models}, &#xA;      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Bj√∂rn Ommer},&#xA;      year={2021},&#xA;      eprint={2112.10752},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>