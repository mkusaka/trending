<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-01T02:06:59Z</updated>
  <subtitle>Monthly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>oobabooga/text-generation-webui</title>
    <updated>2023-04-01T02:06:59Z</updated>
    <id>tag:github.com,2023-04-01:/oobabooga/text-generation-webui</id>
    <link href="https://github.com/oobabooga/text-generation-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, OPT, and GALACTICA.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Text generation web UI&lt;/h1&gt; &#xA;&lt;p&gt;A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, OPT, and GALACTICA.&lt;/p&gt; &#xA;&lt;p&gt;Its goal is to become the &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt; of text generation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/oobabooga/AI-Notebooks/blob/main/Colab-TextGen-GPU.ipynb&#34;&gt;[Try it on Google Colab]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/qa.png&#34; alt=&#34;Image1&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/cai3.png&#34; alt=&#34;Image2&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/gpt4chan.png&#34; alt=&#34;Image3&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/galactica.png&#34; alt=&#34;Image4&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Switch between different models using a dropdown menu.&lt;/li&gt; &#xA; &lt;li&gt;Notebook mode that resembles OpenAI&#39;s playground.&lt;/li&gt; &#xA; &lt;li&gt;Chat mode for conversation and role playing.&lt;/li&gt; &#xA; &lt;li&gt;Generate nice HTML output for GPT-4chan.&lt;/li&gt; &#xA; &lt;li&gt;Generate Markdown output for &lt;a href=&#34;https://github.com/paperswithcode/galai&#34;&gt;GALACTICA&lt;/a&gt;, including LaTeX support.&lt;/li&gt; &#xA; &lt;li&gt;Support for &lt;a href=&#34;https://huggingface.co/models?search=pygmalionai/pygmalion&#34;&gt;Pygmalion&lt;/a&gt; and custom characters in JSON or TavernAI Character Card formats (&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Pygmalion-chat-model-FAQ&#34;&gt;FAQ&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Advanced chat features (send images, get audio responses with TTS).&lt;/li&gt; &#xA; &lt;li&gt;Stream the text output in real time very efficiently.&lt;/li&gt; &#xA; &lt;li&gt;Load parameter presets from text files.&lt;/li&gt; &#xA; &lt;li&gt;Load large models in 8-bit mode.&lt;/li&gt; &#xA; &lt;li&gt;Split large models across your GPU(s), CPU, and disk.&lt;/li&gt; &#xA; &lt;li&gt;CPU mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/FlexGen&#34;&gt;FlexGen offload&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/DeepSpeed&#34;&gt;DeepSpeed ZeRO-3 offload&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Get responses via API, &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/raw/main/api-example-streaming.py&#34;&gt;with&lt;/a&gt; or &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/raw/main/api-example.py&#34;&gt;without&lt;/a&gt; streaming.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model&#34;&gt;LLaMA model, including 4-bit GPTQ support&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/llama.cpp-models&#34;&gt;llama.cpp support&lt;/a&gt;. &lt;strong&gt;*NEW!*&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/RWKV-model&#34;&gt;RWKV model&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Using-LoRAs&#34;&gt;Supports LoRAs&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Supports softprompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Extensions&#34;&gt;Supports extensions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Running-on-Colab&#34;&gt;Works on Google Colab&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;One-click installers&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga-windows.zip&#34;&gt;oobabooga-windows.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Just download the zip above, extract it, and double click on &#34;install&#34;. The web UI and all its dependencies will be installed in the same folder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To download a model, double click on &#34;download-model&#34;&lt;/li&gt; &#xA; &lt;li&gt;To start the web UI, double click on &#34;start-webui&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Source codes: &lt;a href=&#34;https://github.com/oobabooga/one-click-installers&#34;&gt;https://github.com/oobabooga/one-click-installers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/jllllll&#34;&gt;@jllllll&lt;/a&gt; and &lt;a href=&#34;https://github.com/ClayShoaf&#34;&gt;@ClayShoaf&lt;/a&gt;, the Windows 1-click installer now sets up 8-bit and 4-bit requirements out of the box. No additional installation steps are necessary.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;There is no need to run the installer as admin.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Manual installation using Conda&lt;/h3&gt; &#xA;&lt;p&gt;Recommended if you have some experience with the command-line.&lt;/p&gt; &#xA;&lt;p&gt;On Windows, I additionally recommend carrying out the installation on WSL instead of the base system: &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Windows-Subsystem-for-Linux-(Ubuntu)-Installation-Guide&#34;&gt;WSL installation guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;0. Install Conda&lt;/h4&gt; &#xA;&lt;p&gt;Conda can be downloaded here: &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;https://docs.conda.io/en/latest/miniconda.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;On Linux or WSL, it can be automatically installed with these two commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -sL &#34;https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh&#34; &amp;gt; &#34;Miniconda3.sh&#34;&#xA;bash Miniconda3.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Source: &lt;a href=&#34;https://educe-ubc.github.io/conda.html&#34;&gt;https://educe-ubc.github.io/conda.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1. Create a new conda environment&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n textgen python=3.10.9&#xA;conda activate textgen&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Install Pytorch&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;System&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;Command&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux/WSL&lt;/td&gt; &#xA;   &lt;td&gt;NVIDIA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;pip3 install torch torchvision torchaudio&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux&lt;/td&gt; &#xA;   &lt;td&gt;AMD&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.4.2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MacOS + MPS (untested)&lt;/td&gt; &#xA;   &lt;td&gt;Any&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;pip3 install torch torchvision torchaudio&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The up to date commands can be found here: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;MacOS users, refer to the comments here: &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/pull/393&#34;&gt;https://github.com/oobabooga/text-generation-webui/pull/393&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3. Install the web UI&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/oobabooga/text-generation-webui&#xA;cd text-generation-webui&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;For bitsandbytes and &lt;code&gt;--load-in-8bit&lt;/code&gt; to work on Linux/WSL, this dirty fix is currently necessary: &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/400#issuecomment-1474876859&#34;&gt;https://github.com/oobabooga/text-generation-webui/issues/400#issuecomment-1474876859&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Alternative: manual Windows installation&lt;/h3&gt; &#xA;&lt;p&gt;As an alternative to the recommended WSL method, you can install the web UI natively on Windows using this guide. It will be a lot harder and the performance may be slower: &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Windows-installation-guide&#34;&gt;Windows installation guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Alternative: Docker&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/174&#34;&gt;https://github.com/oobabooga/text-generation-webui/issues/174&lt;/a&gt;, &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/87&#34;&gt;https://github.com/oobabooga/text-generation-webui/issues/87&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Downloading models&lt;/h2&gt; &#xA;&lt;p&gt;Models should be placed inside the &lt;code&gt;models&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&#34;&gt;Hugging Face&lt;/a&gt; is the main place to download models. These are some noteworthy examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=eleutherai/pythia&#34;&gt;Pythia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=facebook/opt&#34;&gt;OPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=facebook/galactica&#34;&gt;GALACTICA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/tree/main&#34;&gt;GPT-J 6B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&amp;amp;search=eleutherai+%2F+gpt-neo&#34;&gt;GPT-Neo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=erebus&#34;&gt;*-Erebus&lt;/a&gt; (NSFW)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=pygmalion&#34;&gt;Pygmalion&lt;/a&gt; (NSFW)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can automatically download a model from HF using the script &lt;code&gt;download-model.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py organization/model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py facebook/opt-1.3b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to download a model manually, note that all you need are the json, txt, and pytorch*.bin (or model*.safetensors) files. The remaining files are not necessary.&lt;/p&gt; &#xA;&lt;h3&gt;GPT-4chan&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/ykilcher/gpt-4chan&#34;&gt;GPT-4chan&lt;/a&gt; has been shut down from Hugging Face, so you need to download it elsewhere. You have two options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Torrent: &lt;a href=&#34;https://archive.org/details/gpt4chan_model_float16&#34;&gt;16-bit&lt;/a&gt; / &lt;a href=&#34;https://archive.org/details/gpt4chan_model&#34;&gt;32-bit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Direct download: &lt;a href=&#34;https://theswissbay.ch/pdf/_notpdf_/gpt4chan_model_float16/&#34;&gt;16-bit&lt;/a&gt; / &lt;a href=&#34;https://theswissbay.ch/pdf/_notpdf_/gpt4chan_model/&#34;&gt;32-bit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The 32-bit version is only relevant if you intend to run the model in CPU mode. Otherwise, you should use the 16-bit version.&lt;/p&gt; &#xA;&lt;p&gt;After downloading the model, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Place the files under &lt;code&gt;models/gpt4chan_model_float16&lt;/code&gt; or &lt;code&gt;models/gpt4chan_model&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Place GPT-J 6B&#39;s config.json file in that same folder: &lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/raw/main/config.json&#34;&gt;config.json&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download GPT-J 6B&#39;s tokenizer files (they will be automatically detected when you attempt to load GPT-4chan):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py EleutherAI/gpt-j-6B --text-only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Starting the web UI&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate textgen&#xA;cd text-generation-webui&#xA;python server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then browse to&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;http://localhost:7860/?__theme=dark&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Optionally, you can use the following command-line flags:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Flag&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;show this help message and exit&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--notebook&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in notebook mode, where the output is written to the same text box as the input.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--chat&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in chat mode.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cai-chat&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in chat mode with a style similar to Character.AI&#39;s. If the file &lt;code&gt;img_bot.png&lt;/code&gt; or &lt;code&gt;img_bot.jpg&lt;/code&gt; exists in the same folder as server.py, this image will be used as the bot&#39;s profile picture. Similarly, &lt;code&gt;img_me.png&lt;/code&gt; or &lt;code&gt;img_me.jpg&lt;/code&gt; will be used as your profile picture.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--model MODEL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Name of the model to load by default.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--lora LORA&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Name of the LoRA to apply to the model by default.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--model-dir MODEL_DIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Path to directory with all the models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--lora-dir LORA_DIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Path to directory with all the loras&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--no-stream&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Don&#39;t stream the text output in real time.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--settings SETTINGS_FILE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the default interface settings from this json file. See &lt;code&gt;settings-template.json&lt;/code&gt; for an example. If you create a file called &lt;code&gt;settings.json&lt;/code&gt;, this file will be loaded by default without the need to use the &lt;code&gt;--settings&lt;/code&gt; flag.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--extensions EXTENSIONS [EXTENSIONS ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The list of extensions to load. If you want to load more than one extension, write the names separated by spaces.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--verbose&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Print the prompts to the terminal.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cpu&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Use the CPU to generate text.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--auto-devices&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Automatically split the model across the available GPU(s) and CPU.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--gpu-memory GPU_MEMORY [GPU_MEMORY ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maxmimum GPU memory in GiB to be allocated per GPU. Example: &lt;code&gt;--gpu-memory 10&lt;/code&gt; for a single GPU, &lt;code&gt;--gpu-memory 10 5&lt;/code&gt; for two GPUs. You can also set values in MiB like &lt;code&gt;--gpu-memory 3500MiB&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cpu-memory CPU_MEMORY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maximum CPU memory in GiB to allocate for offloaded weights. Must be an integer number. Defaults to 99.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--disk&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;If the model is too large for your GPU(s) and CPU combined, send the remaining layers to the disk.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--disk-cache-dir DISK_CACHE_DIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Directory to save the disk cache to. Defaults to &lt;code&gt;cache/&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--load-in-8bit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the model with 8-bit precision.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--bf16&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the model with bfloat16 precision. Requires NVIDIA Ampere GPU.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--no-cache&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Set &lt;code&gt;use_cache&lt;/code&gt; to False while generating text. This reduces the VRAM usage a bit with a performance cost.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--threads&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Number of threads to use in llama.cpp.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--wbits WBITS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPTQ: Load a pre-quantized model with specified precision in bits. 2, 3, 4 and 8 are supported.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--model_type MODEL_TYPE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPTQ: Model type of pre-quantized model. Currently LLaMA, OPT, and GPT-J are supported.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--groupsize GROUPSIZE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPTQ: Group size.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--pre_layer PRE_LAYER&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPTQ: The number of layers to preload.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--flexgen&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Enable the use of FlexGen offloading.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--percent PERCENT [PERCENT ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: allocation percentages. Must be 6 numbers separated by spaces (default: 0, 100, 100, 0, 100, 0).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--compress-weight&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: Whether to compress weight (default: False).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--pin-weight [PIN_WEIGHT]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: whether to pin weights (setting this to False reduces CPU memory by 20%).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--deepspeed&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Enable the use of DeepSpeed ZeRO-3 for inference via the Transformers integration.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--nvme-offload-dir NVME_OFFLOAD_DIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeed: Directory to use for ZeRO-3 NVME offloading.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--local_rank LOCAL_RANK&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeed: Optional argument for distributed setups.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--rwkv-strategy RWKV_STRATEGY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RWKV: The strategy to use while loading the model. Examples: &#34;cpu fp32&#34;, &#34;cuda fp16&#34;, &#34;cuda fp16i8&#34;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--rwkv-cuda-on&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RWKV: Compile the CUDA kernel for better performance.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--listen&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Make the web UI reachable from your local network.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--listen-port LISTEN_PORT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The listening port that the server will use.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--share&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Create a public URL. This is useful for running the web UI on Google Colab or similar.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--auto-launch&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Open the web UI in the default browser upon launch.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--gradio-auth-path GRADIO_AUTH_PATH&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Set the gradio authentication file path. The file should contain one or more user:password pairs in this format: &#34;u1:p1,u2:p2,u3:p3&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Out of memory errors? &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide&#34;&gt;Check the low VRAM guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Presets&lt;/h2&gt; &#xA;&lt;p&gt;Inference settings presets can be created under &lt;code&gt;presets/&lt;/code&gt; as text files. These files are detected automatically at startup.&lt;/p&gt; &#xA;&lt;p&gt;By default, 10 presets by NovelAI and KoboldAI are included. These were selected out of a sample of 43 presets after applying a K-Means clustering algorithm and selecting the elements closest to the average of each cluster.&lt;/p&gt; &#xA;&lt;h2&gt;System requirements&lt;/h2&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/System-requirements&#34;&gt;wiki&lt;/a&gt; for some examples of VRAM and RAM usage in both GPU and CPU mode.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests, suggestions, and issue reports are welcome.&lt;/p&gt; &#xA;&lt;p&gt;Before reporting a bug, make sure that you have:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Created a conda environment and installed the dependencies exactly as in the &lt;em&gt;Installation&lt;/em&gt; section above.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues&#34;&gt;Searched&lt;/a&gt; to see if an issue already exists for the issue you encountered.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gradio dropdown menu refresh button, code for reloading the interface: &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Verbose preset: Anonymous 4chan user.&lt;/li&gt; &#xA; &lt;li&gt;NovelAI and KoboldAI presets: &lt;a href=&#34;https://github.com/KoboldAI/KoboldAI-Client/wiki/Settings-Presets&#34;&gt;https://github.com/KoboldAI/KoboldAI-Client/wiki/Settings-Presets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Code for early stopping in chat mode, code for some of the sliders: &lt;a href=&#34;https://github.com/PygmalionAI/gradio-ui/&#34;&gt;https://github.com/PygmalionAI/gradio-ui/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>comfyanonymous/ComfyUI</title>
    <updated>2023-04-01T02:06:59Z</updated>
    <id>tag:github.com,2023-04-01:/comfyanonymous/ComfyUI</id>
    <link href="https://github.com/comfyanonymous/ComfyUI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A powerful and modular stable diffusion GUI with a graph/nodes interface.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI&lt;/h1&gt; &#xA;&lt;h2&gt;A powerful and modular stable diffusion GUI.&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/comfyui_screenshot.png&#34; alt=&#34;ComfyUI Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This ui will let you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. For some workflow examples and see what ComfyUI can do you can check out:&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/&#34;&gt;ComfyUI Examples&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.&lt;/li&gt; &#xA; &lt;li&gt;Fully supports SD1.x and SD2.x&lt;/li&gt; &#xA; &lt;li&gt;Asynchronous Queue system&lt;/li&gt; &#xA; &lt;li&gt;Many optimizations: Only re-executes the parts of the workflow that changes between executions.&lt;/li&gt; &#xA; &lt;li&gt;Command line option: &lt;code&gt;--lowvram&lt;/code&gt; to make it work on GPUs with less than 3GB vram (enabled automatically on GPUs with low vram)&lt;/li&gt; &#xA; &lt;li&gt;Works even if you don&#39;t have a GPU with: &lt;code&gt;--cpu&lt;/code&gt; (slow)&lt;/li&gt; &#xA; &lt;li&gt;Can load both ckpt and safetensors models/checkpoints. Standalone VAEs and CLIP models.&lt;/li&gt; &#xA; &lt;li&gt;Embeddings/Textual inversion&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/lora/&#34;&gt;Loras (regular, locon and loha)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Loading full workflows (with seeds) from generated PNG files.&lt;/li&gt; &#xA; &lt;li&gt;Saving/Loading workflows as Json files.&lt;/li&gt; &#xA; &lt;li&gt;Nodes interface can be used to create complex workflows like one for &lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/&#34;&gt;Hires fix&lt;/a&gt; or much more advanced ones.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/area_composition/&#34;&gt;Area Composition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/inpaint/&#34;&gt;Inpainting&lt;/a&gt; with both regular and inpainting models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/controlnet/&#34;&gt;ControlNet and T2I-Adapter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/&#34;&gt;Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Starts up very fast.&lt;/li&gt; &#xA; &lt;li&gt;Works fully offline: will never download anything.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example&#34;&gt;Config file&lt;/a&gt; to set the search paths for models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Workflow examples can be found on the &lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/&#34;&gt;Examples page&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Shortcuts&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ctrl + A&lt;/strong&gt; select all nodes&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ctrl + M&lt;/strong&gt; mute/unmute selected nodes&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Delete&lt;/strong&gt; or &lt;strong&gt;Backspace&lt;/strong&gt; delete selected nodes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installing&lt;/h1&gt; &#xA;&lt;h2&gt;Windows&lt;/h2&gt; &#xA;&lt;p&gt;There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI/releases&#34;&gt;releases page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_windows_portable_nvidia_cu118_or_cpu.7z&#34;&gt;Direct link to download&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Just download, extract and run. Make sure you put your Stable Diffusion checkpoints/models (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints&lt;/p&gt; &#xA;&lt;h4&gt;How do I share models between another UI and ComfyUI?&lt;/h4&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example&#34;&gt;Config file&lt;/a&gt; to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.&lt;/p&gt; &#xA;&lt;h2&gt;Colab Notebook&lt;/h2&gt; &#xA;&lt;p&gt;To run it on colab or paperspace you can use my &lt;a href=&#34;https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/notebooks/comfyui_colab.ipynb&#34;&gt;Colab Notebook&lt;/a&gt; here: &lt;a href=&#34;https://colab.research.google.com/github/comfyanonymous/ComfyUI/blob/master/notebooks/comfyui_colab.ipynb&#34;&gt;Link to open with google colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Manual Install (Windows, Linux)&lt;/h2&gt; &#xA;&lt;p&gt;Git clone this repo.&lt;/p&gt; &#xA;&lt;p&gt;Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints&lt;/p&gt; &#xA;&lt;p&gt;Put your VAE in: models/vae&lt;/p&gt; &#xA;&lt;p&gt;At the time of writing this pytorch has issues with python versions higher than 3.10 so make sure your python/pip versions are 3.10.&lt;/p&gt; &#xA;&lt;h3&gt;AMD (Linux only)&lt;/h3&gt; &#xA;&lt;p&gt;AMD users can install rocm and pytorch with pip if you don&#39;t have it already installed, this is the command to install the stable version:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/rocm5.4.2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;NVIDIA&lt;/h3&gt; &#xA;&lt;p&gt;Nvidia users should install torch and xformers using this command:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118 xformers&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Troubleshooting&lt;/h4&gt; &#xA;&lt;p&gt;If you get the &#34;Torch not compiled with CUDA enabled&#34; error, uninstall torch with:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip uninstall torch&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;And install it again with the command above.&lt;/p&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Install the dependencies by opening your terminal inside the ComfyUI folder and:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;After this you should have everything installed and can proceed to running ComfyUI.&lt;/p&gt; &#xA;&lt;h3&gt;I already have another UI for Stable Diffusion installed do I really have to install all of these dependencies?&lt;/h3&gt; &#xA;&lt;p&gt;You don&#39;t. If you have another UI installed and working with it&#39;s own python venv you can use that venv to run ComfyUI. You can open up your favorite terminal and activate it:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;source path_to_other_sd_gui/venv/bin/activate&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;or on Windows:&lt;/p&gt; &#xA;&lt;p&gt;With Powershell: &lt;code&gt;&#34;path_to_other_sd_gui\venv\Scripts\Activate.ps1&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;With cmd.exe: &lt;code&gt;&#34;path_to_other_sd_gui\venv\Scripts\activate.bat&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;And then you can use that terminal to run Comfyui without installing any dependencies. Note that the venv folder might be called something else depending on the SD UI.&lt;/p&gt; &#xA;&lt;h1&gt;Running&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;For AMD 6700, 6600 and maybe others&lt;/h3&gt; &#xA;&lt;p&gt;Try running it with this command if you have issues:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Notes&lt;/h1&gt; &#xA;&lt;p&gt;Only parts of the graph that have an output with all the correct inputs will be executed.&lt;/p&gt; &#xA;&lt;p&gt;Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.&lt;/p&gt; &#xA;&lt;p&gt;Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.&lt;/p&gt; &#xA;&lt;p&gt;You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \( or \).&lt;/p&gt; &#xA;&lt;p&gt;You can use {day|night}, for wildcard/dynamic prompts. With this syntax &#34;{wild|card|test}&#34; will be randomly replaced by either &#34;wild&#34;, &#34;card&#34; or &#34;test&#34; by the frontend every time you queue the prompt. To use {} characters in your actual prompt escape them like: \{ or \}.&lt;/p&gt; &#xA;&lt;p&gt;To use a textual inversion concepts/embeddings in a text prompt put them in the models/embeddings directory and use them in the CLIPTextEncode node like this (you can omit the .pt extension):&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;embedding:embedding_filename.pt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Fedora&lt;/h3&gt; &#xA;&lt;p&gt;To get python 3.10 on fedora: &lt;code&gt;dnf install python3.10&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you can:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python3.10 -m ensurepip&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will let you use: pip3.10 to install all the dependencies.&lt;/p&gt; &#xA;&lt;h2&gt;How to increase generation speed?&lt;/h2&gt; &#xA;&lt;p&gt;Make sure you use the CheckpointLoaderSimple node to load checkpoints. It will auto pick the right settings depending on your GPU.&lt;/p&gt; &#xA;&lt;p&gt;You can set this command line setting to disable the upcasting to fp32 in some cross attention operations which will increase your speed. Note that this doesn&#39;t do anything when xformers is enabled and will very likely give you black images on SD2.x models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--dont-upcast-attention&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Support and dev channel&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://app.element.io/#/room/%23comfyui_space%3Amatrix.org&#34;&gt;Matrix space: #comfyui_space:matrix.org&lt;/a&gt; (it&#39;s like discord but open source).&lt;/p&gt; &#xA;&lt;h1&gt;QA&lt;/h1&gt; &#xA;&lt;h3&gt;Why did you make this?&lt;/h3&gt; &#xA;&lt;p&gt;I wanted to learn how Stable Diffusion worked in detail. I also wanted something clean and powerful that would let me experiment with SD without restrictions.&lt;/p&gt; &#xA;&lt;h3&gt;Who is this for?&lt;/h3&gt; &#xA;&lt;p&gt;This is for anyone that wants to make complex workflows with SD or that wants to learn more how SD works. The interface follows closely how SD works and the code should be much more simple to understand than other SD UIs.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>gencay/vscode-chatgpt</title>
    <updated>2023-04-01T02:06:59Z</updated>
    <id>tag:github.com,2023-04-01:/gencay/vscode-chatgpt</id>
    <link href="https://github.com/gencay/vscode-chatgpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An unofficial Visual Studio Code - OpenAI ChatGPT integration&lt;/p&gt;&lt;hr&gt;&lt;h3 align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gencay/vscode-chatgpt/main/images/ai-logo.png&#34; height=&#34;64&#34;&gt;&lt;br&gt;An Unofficial VS Code - Chat GPT extension&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=gencay.vscode-chatgpt&#34; alt=&#34;Marketplace version&#34;&gt; &lt;img src=&#34;https://img.shields.io/visual-studio-marketplace/v/gencay.vscode-chatgpt?color=orange&amp;amp;label=VS%20Code&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=gencay.vscode-chatgpt&#34; alt=&#34;Marketplace download count&#34;&gt; &lt;img src=&#34;https://img.shields.io/visual-studio-marketplace/d/gencay.vscode-chatgpt?color=blueviolet&amp;amp;label=Downloads&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/gencay/vscode-chatgpt&#34; alt=&#34;Github star count&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/gencay/vscode-chatgpt?color=blue&amp;amp;label=Github%20Stars&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;The most loved* ChatGPT extension in VS Code open-sourced&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The project is built as a hobby project - With &lt;strong&gt;no affiliation&lt;/strong&gt; to any organization&lt;/p&gt; &#xA; &lt;p&gt;The functionality fully relies on OpenAI services without any affiliation to them&lt;/p&gt; &#xA; &lt;p&gt;*Downloaded by ~500,000 developers with more than 100 5 star ratings within 3 months of release&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Try out this extension on marketplace: &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=genieai.chatgpt-vscode&#34;&gt;ChatGPT - Genie AI&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The extension is stripped out of Browser capabilities and published as an integration point for community to continue utilizing GPT models in Sidebar conversations or responses in editor, favorites, personalization and more.&lt;/p&gt; &#xA;&lt;h2&gt;Forks and continuation of the original project&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Feel free to contribute here.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Continued by &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=genieai.chatgpt-vscode&#34;&gt;ChatGPT - Genie AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Forked by @Christopher-Hayes: &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=chris-hayes.chatgpt-reborn&#34;&gt;ChatGPT Reborn&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ûï Use GPT-4, GPT-3.5, GPT3 or Codex models using your OpenAI API Key&lt;/li&gt; &#xA; &lt;li&gt;üìÉ Get streaming answers to your prompts in sidebar conversation window&lt;/li&gt; &#xA; &lt;li&gt;üî• Stop the responses to save your tokens.&lt;/li&gt; &#xA; &lt;li&gt;üìù Create files or fix your code with one click or with keyboard shortcuts.&lt;/li&gt; &#xA; &lt;li&gt;‚û°Ô∏è Export all your conversation history at once in Markdown format.&lt;/li&gt; &#xA; &lt;li&gt;Automatic partial code response detection. Continues and combines automatically, when response is cut off.&lt;/li&gt; &#xA; &lt;li&gt;Ad-hoc prompt prefixes for you to customize what you are asking ChatGPT&lt;/li&gt; &#xA; &lt;li&gt;Edit and resend a previous prompt&lt;/li&gt; &#xA; &lt;li&gt;Copy, insert or create new file from the code, ChatGPT is suggesting right into your editor.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you are developing completely another extension and release it to the public, make sure you follow this.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This project is released under ISC License - See root License for details. Copyright notice and the respective permission notices must appear in all copies.&lt;/p&gt; &#xA;&lt;h2&gt;How to build and run&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone the repository to your local machine&lt;/li&gt; &#xA; &lt;li&gt;On the root directory, run &lt;code&gt;yarn&lt;/code&gt; command to install the dependencies listed in &lt;code&gt;package.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Within VS Code - run the project by simply hitting &lt;code&gt;F5&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to install locally&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;code&gt;vsce&lt;/code&gt; if you don&#39;t have it on your machine (The Visual Studio Code Extension Manager) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;npm install --global vsce&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;vsce package&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Follow the &lt;a href=&#34;https://code.visualstudio.com/docs/editor/extension-marketplace#_install-from-a-vsix&#34;&gt;instructions&lt;/a&gt; and install manually.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>