<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-01T01:51:12Z</updated>
  <subtitle>Monthly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>datawhalechina/happy-llm</title>
    <updated>2025-08-01T01:51:12Z</updated>
    <id>tag:github.com,2025-08-01:/datawhalechina/happy-llm</id>
    <link href="https://github.com/datawhalechina/happy-llm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;📚 从零开始的大语言模型原理与实践教程&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/head.jpg&#34; alt=&#34;alt text&#34; width=&#34;100%&#34;&gt; &#xA; &lt;h1&gt;Happy-LLM&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/github/stars/datawhalechina/happy-llm?style=flat&amp;amp;logo=github&#34; alt=&#34;GitHub stars&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/github/forks/datawhalechina/happy-llm?style=flat&amp;amp;logo=github&#34; alt=&#34;GitHub forks&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/language-Chinese-brightgreen?style=flat&#34; alt=&#34;Language&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/datawhalechina/happy-llm&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;amp;logo=github&#34; alt=&#34;GitHub Project&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://swanlab.cn/@kmno4/Happy-LLM/overview&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg?sanitize=true&#34; alt=&#34;SwanLab&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://trendshift.io/repositories/14175&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/14175&#34; alt=&#34;datawhalechina%2Fhappy-llm | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/README.md&#34;&gt;中文&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/README_en.md&#34;&gt;English&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://datawhalechina.github.io/happy-llm/&#34;&gt;📚 在线阅读地址&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;📚 从零开始的大语言模型原理与实践教程&lt;/h3&gt; &#xA; &lt;p&gt;&lt;em&gt;深入理解 LLM 核心原理，动手实现你的第一个大模型&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;🎯 项目介绍&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;  &lt;em&gt;很多小伙伴在看完 Datawhale开源项目： &lt;a href=&#34;https://github.com/datawhalechina/self-llm&#34;&gt;self-llm 开源大模型食用指南&lt;/a&gt; 后，感觉意犹未尽，想要深入了解大语言模型的原理和训练过程。于是我们（Datawhale）决定推出《Happy-LLM》项目，旨在帮助大家深入理解大语言模型的原理和训练过程。&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;  本项目是一个&lt;strong&gt;系统性的 LLM 学习教程&lt;/strong&gt;，将从 NLP 的基本研究方法出发，根据 LLM 的思路及原理逐层深入，依次为读者剖析 LLM 的架构基础和训练过程。同时，我们会结合目前 LLM 领域最主流的代码框架，演练如何亲手搭建、训练一个 LLM，期以实现授之以鱼，更授之以渔。希望大家能从这本书开始走入 LLM 的浩瀚世界，探索 LLM 的无尽可能。&lt;/p&gt; &#xA;&lt;h3&gt;✨ 你将收获什么？&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📚 &lt;strong&gt;Datawhale 开源免费&lt;/strong&gt; 完全免费的学习本项目所有内容&lt;/li&gt; &#xA; &lt;li&gt;🔍 &lt;strong&gt;深入理解&lt;/strong&gt; Transformer 架构和注意力机制&lt;/li&gt; &#xA; &lt;li&gt;📚 &lt;strong&gt;掌握&lt;/strong&gt; 预训练语言模型的基本原理&lt;/li&gt; &#xA; &lt;li&gt;🧠 &lt;strong&gt;了解&lt;/strong&gt; 现有大模型的基本结构&lt;/li&gt; &#xA; &lt;li&gt;🏗️ &lt;strong&gt;动手实现&lt;/strong&gt; 一个完整的 LLaMA2 模型&lt;/li&gt; &#xA; &lt;li&gt;⚙️ &lt;strong&gt;掌握训练&lt;/strong&gt; 从预训练到微调的全流程&lt;/li&gt; &#xA; &lt;li&gt;🚀 &lt;strong&gt;实战应用&lt;/strong&gt; RAG、Agent 等前沿技术&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📖 内容导航&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;章节&lt;/th&gt; &#xA;   &lt;th&gt;关键内容&lt;/th&gt; &#xA;   &lt;th&gt;状态&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/%E5%89%8D%E8%A8%80.md&#34;&gt;前言&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;本项目的缘起、背景及读者建议&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter1/%E7%AC%AC%E4%B8%80%E7%AB%A0%20NLP%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5.md&#34;&gt;第一章 NLP 基础概念&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;什么是 NLP、发展历程、任务分类、文本表示演进&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter2/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20Transformer%E6%9E%B6%E6%9E%84.md&#34;&gt;第二章 Transformer 架构&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;注意力机制、Encoder-Decoder、手把手搭建 Transformer&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter3/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.md&#34;&gt;第三章 预训练语言模型&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Encoder-only、Encoder-Decoder、Decoder-Only 模型对比&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter4/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.md&#34;&gt;第四章 大语言模型&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLM 定义、训练策略、涌现能力分析&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter5/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B.md&#34;&gt;第五章 动手搭建大模型&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;实现 LLaMA2、训练 Tokenizer、预训练小型 LLM&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter6/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E5%AE%9E%E8%B7%B5.md&#34;&gt;第六章 大模型训练实践&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;预训练、有监督微调、LoRA/QLoRA 高效微调&lt;/td&gt; &#xA;   &lt;td&gt;🚧&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter7/%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8.md&#34;&gt;第七章 大模型应用&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;模型评测、RAG 检索增强、Agent 智能体&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/&#34;&gt;Extra Chapter LLM Blog&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;优秀的大模型 学习笔记/Blog ，欢迎大家来 PR ！&lt;/td&gt; &#xA;   &lt;td&gt;🚧&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Extra Chapter LLM Blog&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/why-fine-tune-small-large-language-models/readme.md&#34;&gt;大模型都这么厉害了，微调0.6B的小模型有什么意义？&lt;/a&gt; @&lt;a href=&#34;https://github.com/KMnO4-zx&#34;&gt;不要葱姜蒜&lt;/a&gt; 2025-7-11&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/transformer-architecture/&#34;&gt;Transformer 整体模块设计解读&lt;/a&gt; @&lt;a href=&#34;https://github.com/ditingdapeng&#34;&gt;ditingdapeng&lt;/a&gt; 2025-7-14&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/text-data-processing/readme.md&#34;&gt;文本数据处理详解&lt;/a&gt; @&lt;a href=&#34;https://github.com/xinala-781&#34;&gt;蔡鋆捷&lt;/a&gt; 2025-7-14&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/vlm-concatenation-finetune/README.md&#34;&gt;Qwen3-&#34;VL&#34;——超小中文多模态模型的“拼接微调”之路&lt;/a&gt; @&lt;a href=&#34;https://github.com/ShaohonChen&#34;&gt;ShaohonChen&lt;/a&gt; 2025-7-30&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;  &lt;em&gt;如果大家在学习 Happy-LLM 项目或 LLM 相关知识中有自己独到的见解、认知、实践，欢迎大家 PR 在 &lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/&#34;&gt;Extra Chapter LLM Blog&lt;/a&gt; 中。请遵守 Extra Chapter LLM Blog 的 &lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/Readme.md&#34;&gt;PR 规范&lt;/a&gt;，我们会视 PR 内容的质量和价值来决定是否合并或补充到 Happy-LLM 正文中来。&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;模型下载&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;模型名称&lt;/th&gt; &#xA;   &lt;th&gt;下载地址&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Happy-LLM-Chapter5-Base-215M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.modelscope.cn/models/kmno4zx/happy-llm-215M-base&#34;&gt;🤖 ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Happy-LLM-Chapter5-SFT-215M&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.modelscope.cn/models/kmno4zx/happy-llm-215M-sft&#34;&gt;🤖 ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;ModelScope 创空间体验地址：&lt;a href=&#34;https://www.modelscope.cn/studios/kmno4zx/happy_llm_215M_sft&#34;&gt;🤖 创空间&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;PDF 版本下载&lt;/h3&gt; &#xA;&lt;p&gt;  &lt;em&gt;&lt;strong&gt;本 Happy-LLM PDF 教程完全开源免费。为防止各类营销号加水印后贩卖给大模型初学者，我们特地在 PDF 文件中预先添加了不影响阅读的 Datawhale 开源标志水印，敬请谅解～&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Happy-LLM PDF : &lt;a href=&#34;https://github.com/datawhalechina/happy-llm/releases/tag/v1.0.1&#34;&gt;https://github.com/datawhalechina/happy-llm/releases/tag/v1.0.1&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;💡 如何学习&lt;/h2&gt; &#xA;&lt;p&gt;  本项目适合大学生、研究人员、LLM 爱好者。在学习本项目之前，建议具备一定的编程经验，尤其是要对 Python 编程语言有一定的了解。最好具备深度学习的相关知识，并了解 NLP 领域的相关概念和术语，以便更轻松地学习本项目。&lt;/p&gt; &#xA;&lt;p&gt;  本项目分为两部分——基础知识与实战应用。第1章～第4章是基础知识部分，从浅入深介绍 LLM 的基本原理。其中，第1章简单介绍 NLP 的基本任务和发展，为非 NLP 领域研究者提供参考；第2章介绍 LLM 的基本架构——Transformer，包括原理介绍及代码实现，作为 LLM 最重要的理论基础；第3章整体介绍经典的 PLM，包括 Encoder-Only、Encoder-Decoder 和 Decoder-Only 三种架构，也同时介绍了当前一些主流 LLM 的架构和思想；第4章则正式进入 LLM 部分，详细介绍 LLM 的特点、能力和整体训练过程。第5章～第7章是实战应用部分，将逐步带领大家深入 LLM 的底层细节。其中，第5章将带领大家者基于 PyTorch 层亲手搭建一个 LLM，并实现预训练、有监督微调的全流程；第6章将引入目前业界主流的 LLM 训练框架 Transformers，带领学习者基于该框架快速、高效地实现 LLM 训练过程；第7章则将介绍 基于 LLM 的各种应用，补全学习者对 LLM 体系的认知，包括 LLM 的评测、检索增强生成（Retrieval-Augmented Generation，RAG）、智能体（Agent）的思想和简单实现。你可以根据个人兴趣和需求，选择性地阅读相关章节。&lt;/p&gt; &#xA;&lt;p&gt;  在阅读本书的过程中，建议你将理论和实际相结合。LLM 是一个快速发展、注重实践的领域，我们建议你多投入实战，复现本书提供的各种代码，同时积极参加 LLM 相关的项目与比赛，真正投入到 LLM 开发的浪潮中。我们鼓励你关注 Datawhale 及其他 LLM 相关开源社区，当遇到问题时，你可以随时在本项目的 issue 区提问。&lt;/p&gt; &#xA;&lt;p&gt;  最后，欢迎每一位读者在学习完本项目后加入到 LLM 开发者的行列。作为国内 AI 开源社区，我们希望充分聚集共创者，一起丰富这个开源 LLM 的世界，打造更多、更全面特色 LLM 的教程。星火点点，汇聚成海。我们希望成为 LLM 与普罗大众的阶梯，以自由、平等的开源精神，拥抱更恢弘而辽阔的 LLM 世界。&lt;/p&gt; &#xA;&lt;h2&gt;🤝 如何贡献&lt;/h2&gt; &#xA;&lt;p&gt;我们欢迎任何形式的贡献！&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🐛 &lt;strong&gt;报告 Bug&lt;/strong&gt; - 发现问题请提交 Issue&lt;/li&gt; &#xA; &lt;li&gt;💡 &lt;strong&gt;功能建议&lt;/strong&gt; - 有好想法就告诉我们&lt;/li&gt; &#xA; &lt;li&gt;📝 &lt;strong&gt;内容完善&lt;/strong&gt; - 帮助改进教程内容&lt;/li&gt; &#xA; &lt;li&gt;🔧 &lt;strong&gt;代码优化&lt;/strong&gt; - 提交 Pull Request&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🙏 致谢&lt;/h2&gt; &#xA;&lt;h3&gt;核心贡献者&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KMnO4-zx&#34;&gt;宋志学-项目负责人&lt;/a&gt; (Datawhale成员-中国矿业大学(北京))&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/logan-zou&#34;&gt;邹雨衡-项目负责人&lt;/a&gt; (Datawhale成员-对外经济贸易大学)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://xinzhongzhu.github.io/&#34;&gt;朱信忠-指导专家&lt;/a&gt;（Datawhale首席科学家-浙江师范大学杭州人工智能研究院教授）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Extra-Chapter 贡献者&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ditingdapeng&#34;&gt;ditingdapeng&lt;/a&gt;（内容贡献者-云原生基础架构工程师）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xinala-781&#34;&gt;蔡鋆捷&lt;/a&gt;（内容贡献者-福州大学）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ShaohonChen&#34;&gt;ShaohonChen&lt;/a&gt; （情感机器实验室研究员-西安电子科技大学在读硕士）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;特别感谢&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;感谢 &lt;a href=&#34;https://github.com/Sm1les&#34;&gt;@Sm1les&lt;/a&gt; 对本项目的帮助与支持&lt;/li&gt; &#xA; &lt;li&gt;感谢所有为本项目做出贡献的开发者们 ❤️&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;margin-top: 30px;&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/datawhalechina/happy-llm/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=datawhalechina/happy-llm&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/star-history-2025710.png&#34; alt=&#34;Datawhale&#34; width=&#34;90%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;⭐ 如果这个项目对你有帮助，请给我们一个 Star！&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;关于 Datawhale&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/datawhale.png&#34; alt=&#34;Datawhale&#34; width=&#34;30%&#34;&gt; &#xA; &lt;p&gt;扫描二维码关注 Datawhale 公众号，获取更多优质开源内容&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;📜 开源协议&lt;/h2&gt; &#xA;&lt;p&gt;本作品采用&lt;a href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议&lt;/a&gt;进行许可。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>musistudio/claude-code-router</title>
    <updated>2025-08-01T01:51:12Z</updated>
    <id>tag:github.com,2025-08-01:/musistudio/claude-code-router</id>
    <link href="https://github.com/musistudio/claude-code-router" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Use Claude Code as the foundation for coding infrastructure, allowing you to decide how to interact with the model while enjoying updates from Anthropic.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Code Router&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/musistudio/claude-code-router/main/README_zh.md&#34;&gt;中文版&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A powerful tool to route Claude Code requests to different models and customize any request.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/images/claude-code.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;✨ Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model Routing&lt;/strong&gt;: Route requests to different models based on your needs (e.g., background tasks, thinking, long context).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Provider Support&lt;/strong&gt;: Supports various model providers like OpenRouter, DeepSeek, Ollama, Gemini, Volcengine, and SiliconFlow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Request/Response Transformation&lt;/strong&gt;: Customize requests and responses for different providers using transformers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dynamic Model Switching&lt;/strong&gt;: Switch models on-the-fly within Claude Code using the &lt;code&gt;/model&lt;/code&gt; command.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GitHub Actions Integration&lt;/strong&gt;: Trigger Claude Code tasks in your GitHub workflows.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt;: Extend functionality with custom transformers.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🚀 Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;1. Installation&lt;/h3&gt; &#xA;&lt;p&gt;First, ensure you have &lt;a href=&#34;https://docs.anthropic.com/en/docs/claude-code/quickstart&#34;&gt;Claude Code&lt;/a&gt; installed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm install -g @anthropic-ai/claude-code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, install Claude Code Router:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm install -g @musistudio/claude-code-router&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Create and configure your &lt;code&gt;~/.claude-code-router/config.json&lt;/code&gt; file. For more details, you can refer to &lt;code&gt;config.example.json&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;config.json&lt;/code&gt; file has several key sections:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;PROXY_URL&lt;/code&gt;&lt;/strong&gt; (optional): You can set a proxy for API requests, for example: &lt;code&gt;&#34;PROXY_URL&#34;: &#34;http://127.0.0.1:7890&#34;&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;LOG&lt;/code&gt;&lt;/strong&gt; (optional): You can enable logging by setting it to &lt;code&gt;true&lt;/code&gt;. The log file will be located at &lt;code&gt;$HOME/.claude-code-router.log&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;APIKEY&lt;/code&gt;&lt;/strong&gt; (optional): You can set a secret key to authenticate requests. When set, clients must provide this key in the &lt;code&gt;Authorization&lt;/code&gt; header (e.g., &lt;code&gt;Bearer your-secret-key&lt;/code&gt;) or the &lt;code&gt;x-api-key&lt;/code&gt; header. Example: &lt;code&gt;&#34;APIKEY&#34;: &#34;your-secret-key&#34;&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;HOST&lt;/code&gt;&lt;/strong&gt; (optional): You can set the host address for the server. If &lt;code&gt;APIKEY&lt;/code&gt; is not set, the host will be forced to &lt;code&gt;127.0.0.1&lt;/code&gt; for security reasons to prevent unauthorized access. Example: &lt;code&gt;&#34;HOST&#34;: &#34;0.0.0.0&#34;&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;Providers&lt;/code&gt;&lt;/strong&gt;: Used to configure different model providers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;Router&lt;/code&gt;&lt;/strong&gt;: Used to set up routing rules. &lt;code&gt;default&lt;/code&gt; specifies the default model, which will be used for all requests if no other route is configured.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;API_TIMEOUT_MS&lt;/code&gt;&lt;/strong&gt;: Specifies the timeout for API calls in milliseconds.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is a comprehensive example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;APIKEY&#34;: &#34;your-secret-key&#34;,&#xA;  &#34;PROXY_URL&#34;: &#34;http://127.0.0.1:7890&#34;,&#xA;  &#34;LOG&#34;: true,&#xA;  &#34;API_TIMEOUT_MS&#34;: 600000,&#xA;  &#34;Providers&#34;: [&#xA;    {&#xA;      &#34;name&#34;: &#34;openrouter&#34;,&#xA;      &#34;api_base_url&#34;: &#34;https://openrouter.ai/api/v1/chat/completions&#34;,&#xA;      &#34;api_key&#34;: &#34;sk-xxx&#34;,&#xA;      &#34;models&#34;: [&#xA;        &#34;google/gemini-2.5-pro-preview&#34;,&#xA;        &#34;anthropic/claude-sonnet-4&#34;,&#xA;        &#34;anthropic/claude-3.5-sonnet&#34;,&#xA;        &#34;anthropic/claude-3.7-sonnet:thinking&#34;&#xA;      ],&#xA;      &#34;transformer&#34;: {&#xA;        &#34;use&#34;: [&#34;openrouter&#34;]&#xA;      }&#xA;    },&#xA;    {&#xA;      &#34;name&#34;: &#34;deepseek&#34;,&#xA;      &#34;api_base_url&#34;: &#34;https://api.deepseek.com/chat/completions&#34;,&#xA;      &#34;api_key&#34;: &#34;sk-xxx&#34;,&#xA;      &#34;models&#34;: [&#34;deepseek-chat&#34;, &#34;deepseek-reasoner&#34;],&#xA;      &#34;transformer&#34;: {&#xA;        &#34;use&#34;: [&#34;deepseek&#34;],&#xA;        &#34;deepseek-chat&#34;: {&#xA;          &#34;use&#34;: [&#34;tooluse&#34;]&#xA;        }&#xA;      }&#xA;    },&#xA;    {&#xA;      &#34;name&#34;: &#34;ollama&#34;,&#xA;      &#34;api_base_url&#34;: &#34;http://localhost:11434/v1/chat/completions&#34;,&#xA;      &#34;api_key&#34;: &#34;ollama&#34;,&#xA;      &#34;models&#34;: [&#34;qwen2.5-coder:latest&#34;]&#xA;    },&#xA;    {&#xA;      &#34;name&#34;: &#34;gemini&#34;,&#xA;      &#34;api_base_url&#34;: &#34;https://generativelanguage.googleapis.com/v1beta/models/&#34;,&#xA;      &#34;api_key&#34;: &#34;sk-xxx&#34;,&#xA;      &#34;models&#34;: [&#34;gemini-2.5-flash&#34;, &#34;gemini-2.5-pro&#34;],&#xA;      &#34;transformer&#34;: {&#xA;        &#34;use&#34;: [&#34;gemini&#34;]&#xA;      }&#xA;    },&#xA;    {&#xA;      &#34;name&#34;: &#34;volcengine&#34;,&#xA;      &#34;api_base_url&#34;: &#34;https://ark.cn-beijing.volces.com/api/v3/chat/completions&#34;,&#xA;      &#34;api_key&#34;: &#34;sk-xxx&#34;,&#xA;      &#34;models&#34;: [&#34;deepseek-v3-250324&#34;, &#34;deepseek-r1-250528&#34;],&#xA;      &#34;transformer&#34;: {&#xA;        &#34;use&#34;: [&#34;deepseek&#34;]&#xA;      }&#xA;    },&#xA;    {&#xA;      &#34;name&#34;: &#34;modelscope&#34;,&#xA;      &#34;api_base_url&#34;: &#34;https://api-inference.modelscope.cn/v1/chat/completions&#34;,&#xA;      &#34;api_key&#34;: &#34;&#34;,&#xA;      &#34;models&#34;: [&#34;Qwen/Qwen3-Coder-480B-A35B-Instruct&#34;, &#34;Qwen/Qwen3-235B-A22B-Thinking-2507&#34;],&#xA;      &#34;transformer&#34;: {&#xA;        &#34;use&#34;: [&#xA;          [&#xA;            &#34;maxtoken&#34;,&#xA;            {&#xA;              &#34;max_tokens&#34;: 65536&#xA;            }&#xA;          ],&#xA;          &#34;enhancetool&#34;&#xA;        ],&#xA;        &#34;Qwen/Qwen3-235B-A22B-Thinking-2507&#34;: {&#xA;          &#34;use&#34;: [&#34;reasoning&#34;]&#xA;        }&#xA;      }&#xA;    },&#xA;    {&#xA;      &#34;name&#34;: &#34;dashscope&#34;,&#xA;      &#34;api_base_url&#34;: &#34;https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions&#34;,&#xA;      &#34;api_key&#34;: &#34;&#34;,&#xA;      &#34;models&#34;: [&#34;qwen3-coder-plus&#34;],&#xA;      &#34;transformer&#34;: {&#xA;        &#34;use&#34;: [&#xA;          [&#xA;            &#34;maxtoken&#34;,&#xA;            {&#xA;              &#34;max_tokens&#34;: 65536&#xA;            }&#xA;          ],&#xA;          &#34;enhancetool&#34;&#xA;        ]&#xA;      }&#xA;    },&#xA;    {&#xA;      &#34;name&#34;: &#34;aihubmix&#34;,&#xA;      &#34;api_base_url&#34;: &#34;https://aihubmix.com/v1/chat/completions&#34;,&#xA;      &#34;api_key&#34;: &#34;sk-&#34;,&#xA;      &#34;models&#34;: [&#xA;        &#34;Z/glm-4.5&#34;,&#xA;        &#34;claude-opus-4-20250514&#34;,&#xA;        &#34;gemini-2.5-pro&#34;&#xA;      ]&#xA;    }&#xA;  ],&#xA;  &#34;Router&#34;: {&#xA;    &#34;default&#34;: &#34;deepseek,deepseek-chat&#34;,&#xA;    &#34;background&#34;: &#34;ollama,qwen2.5-coder:latest&#34;,&#xA;    &#34;think&#34;: &#34;deepseek,deepseek-reasoner&#34;,&#xA;    &#34;longContext&#34;: &#34;openrouter,google/gemini-2.5-pro-preview&#34;,&#xA;    &#34;longContextThreshold&#34;: 60000,&#xA;    &#34;webSearch&#34;: &#34;gemini,gemini-2.5-flash&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Running Claude Code with the Router&lt;/h3&gt; &#xA;&lt;p&gt;Start Claude Code using the router:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ccr code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: After modifying the configuration file, you need to restart the service for the changes to take effect:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ccr restart&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;4. UI Mode (Beta)&lt;/h3&gt; &#xA;&lt;p&gt;For a more intuitive experience, you can use the UI mode to manage your configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ccr ui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will open a web-based interface where you can easily view and edit your &lt;code&gt;config.json&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/images/ui.png&#34; alt=&#34;UI&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The UI mode is currently in beta. 100% vibe coding: including project initialization, I just created a folder and a project.md document, and all code was generated by ccr + qwen3-coder + gemini(webSearch). If you encounter any issues, please submit an issue on GitHub.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Providers&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;Providers&lt;/code&gt; array is where you define the different model providers you want to use. Each provider object requires:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;name&lt;/code&gt;: A unique name for the provider.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;api_base_url&lt;/code&gt;: The full API endpoint for chat completions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;api_key&lt;/code&gt;: Your API key for the provider.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;models&lt;/code&gt;: A list of model names available from this provider.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;transformer&lt;/code&gt; (optional): Specifies transformers to process requests and responses.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Transformers&lt;/h4&gt; &#xA;&lt;p&gt;Transformers allow you to modify the request and response payloads to ensure compatibility with different provider APIs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Global Transformer&lt;/strong&gt;: Apply a transformer to all models from a provider. In this example, the &lt;code&gt;openrouter&lt;/code&gt; transformer is applied to all models under the &lt;code&gt;openrouter&lt;/code&gt; provider.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;name&#34;: &#34;openrouter&#34;,&#xA;  &#34;api_base_url&#34;: &#34;https://openrouter.ai/api/v1/chat/completions&#34;,&#xA;  &#34;api_key&#34;: &#34;sk-xxx&#34;,&#xA;  &#34;models&#34;: [&#xA;    &#34;google/gemini-2.5-pro-preview&#34;,&#xA;    &#34;anthropic/claude-sonnet-4&#34;,&#xA;    &#34;anthropic/claude-3.5-sonnet&#34;&#xA;  ],&#xA;  &#34;transformer&#34;: { &#34;use&#34;: [&#34;openrouter&#34;] }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model-Specific Transformer&lt;/strong&gt;: Apply a transformer to a specific model. In this example, the &lt;code&gt;deepseek&lt;/code&gt; transformer is applied to all models, and an additional &lt;code&gt;tooluse&lt;/code&gt; transformer is applied only to the &lt;code&gt;deepseek-chat&lt;/code&gt; model.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;name&#34;: &#34;deepseek&#34;,&#xA;  &#34;api_base_url&#34;: &#34;https://api.deepseek.com/chat/completions&#34;,&#xA;  &#34;api_key&#34;: &#34;sk-xxx&#34;,&#xA;  &#34;models&#34;: [&#34;deepseek-chat&#34;, &#34;deepseek-reasoner&#34;],&#xA;  &#34;transformer&#34;: {&#xA;    &#34;use&#34;: [&#34;deepseek&#34;],&#xA;    &#34;deepseek-chat&#34;: { &#34;use&#34;: [&#34;tooluse&#34;] }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Passing Options to a Transformer&lt;/strong&gt;: Some transformers, like &lt;code&gt;maxtoken&lt;/code&gt;, accept options. To pass options, use a nested array where the first element is the transformer name and the second is an options object.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;name&#34;: &#34;siliconflow&#34;,&#xA;  &#34;api_base_url&#34;: &#34;https://api.siliconflow.cn/v1/chat/completions&#34;,&#xA;  &#34;api_key&#34;: &#34;sk-xxx&#34;,&#xA;  &#34;models&#34;: [&#34;moonshotai/Kimi-K2-Instruct&#34;],&#xA;  &#34;transformer&#34;: {&#xA;    &#34;use&#34;: [&#xA;      [&#xA;        &#34;maxtoken&#34;,&#xA;        {&#xA;          &#34;max_tokens&#34;: 16384&#xA;        }&#xA;      ]&#xA;    ]&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Available Built-in Transformers:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;deepseek&lt;/code&gt;: Adapts requests/responses for DeepSeek API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gemini&lt;/code&gt;: Adapts requests/responses for Gemini API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;openrouter&lt;/code&gt;: Adapts requests/responses for OpenRouter API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;groq&lt;/code&gt;: Adapts requests/responses for groq API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;maxtoken&lt;/code&gt;: Sets a specific &lt;code&gt;max_tokens&lt;/code&gt; value.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tooluse&lt;/code&gt;: Optimizes tool usage for certain models via &lt;code&gt;tool_choice&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gemini-cli&lt;/code&gt; (experimental): Unofficial support for Gemini via Gemini CLI &lt;a href=&#34;https://gist.github.com/musistudio/1c13a65f35916a7ab690649d3df8d1cd&#34;&gt;gemini-cli.js&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;reasoning&lt;/code&gt;: Used to process the &lt;code&gt;reasoning_content&lt;/code&gt; field.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sampling&lt;/code&gt;: Used to process sampling information fields such as &lt;code&gt;temperature&lt;/code&gt;, &lt;code&gt;top_p&lt;/code&gt;, &lt;code&gt;top_k&lt;/code&gt;, and &lt;code&gt;repetition_penalty&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enhancetool&lt;/code&gt;: Adds a layer of error tolerance to the tool call parameters returned by the LLM (this will cause the tool call information to no longer be streamed).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cleancache&lt;/code&gt;: Clears the &lt;code&gt;cache_control&lt;/code&gt; field from requests.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;vertex-gemini&lt;/code&gt;: Handles the Gemini API using Vertex authentication.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Custom Transformers:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also create your own transformers and load them via the &lt;code&gt;transformers&lt;/code&gt; field in &lt;code&gt;config.json&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;transformers&#34;: [&#xA;    {&#xA;      &#34;path&#34;: &#34;$HOME/.claude-code-router/plugins/gemini-cli.js&#34;,&#xA;      &#34;options&#34;: {&#xA;        &#34;project&#34;: &#34;xxx&#34;&#xA;      }&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Router&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;Router&lt;/code&gt; object defines which model to use for different scenarios:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;default&lt;/code&gt;: The default model for general tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;background&lt;/code&gt;: A model for background tasks. This can be a smaller, local model to save costs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;think&lt;/code&gt;: A model for reasoning-heavy tasks, like Plan Mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;longContext&lt;/code&gt;: A model for handling long contexts (e.g., &amp;gt; 60K tokens).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;longContextThreshold&lt;/code&gt; (optional): The token count threshold for triggering the long context model. Defaults to 60000 if not specified.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;webSearch&lt;/code&gt;: Used for handling web search tasks and this requires the model itself to support the feature. If you&#39;re using openrouter, you need to add the &lt;code&gt;:online&lt;/code&gt; suffix after the model name.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also switch models dynamically in Claude Code with the &lt;code&gt;/model&lt;/code&gt; command: &lt;code&gt;/model provider_name,model_name&lt;/code&gt; Example: &lt;code&gt;/model openrouter,anthropic/claude-3.5-sonnet&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Custom Router&lt;/h4&gt; &#xA;&lt;p&gt;For more advanced routing logic, you can specify a custom router script via the &lt;code&gt;CUSTOM_ROUTER_PATH&lt;/code&gt; in your &lt;code&gt;config.json&lt;/code&gt;. This allows you to implement complex routing rules beyond the default scenarios.&lt;/p&gt; &#xA;&lt;p&gt;In your &lt;code&gt;config.json&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;CUSTOM_ROUTER_PATH&#34;: &#34;$HOME/.claude-code-router/custom-router.js&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The custom router file must be a JavaScript module that exports an &lt;code&gt;async&lt;/code&gt; function. This function receives the request object and the config object as arguments and should return the provider and model name as a string (e.g., &lt;code&gt;&#34;provider_name,model_name&#34;&lt;/code&gt;), or &lt;code&gt;null&lt;/code&gt; to fall back to the default router.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of a &lt;code&gt;custom-router.js&lt;/code&gt; based on &lt;code&gt;custom-router.example.js&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// $HOME/.claude-code-router/custom-router.js&#xA;&#xA;/**&#xA; * A custom router function to determine which model to use based on the request.&#xA; *&#xA; * @param {object} req - The request object from Claude Code, containing the request body.&#xA; * @param {object} config - The application&#39;s config object.&#xA; * @returns {Promise&amp;lt;string|null&amp;gt;} - A promise that resolves to the &#34;provider,model_name&#34; string, or null to use the default router.&#xA; */&#xA;module.exports = async function router(req, config) {&#xA;  const userMessage = req.body.messages.find((m) =&amp;gt; m.role === &#34;user&#34;)?.content;&#xA;&#xA;  if (userMessage &amp;amp;&amp;amp; userMessage.includes(&#34;explain this code&#34;)) {&#xA;    // Use a powerful model for code explanation&#xA;    return &#34;openrouter,anthropic/claude-3.5-sonnet&#34;;&#xA;  }&#xA;&#xA;  // Fallback to the default router configuration&#xA;  return null;&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🤖 GitHub Actions&lt;/h2&gt; &#xA;&lt;p&gt;Integrate Claude Code Router into your CI/CD pipeline. After setting up &lt;a href=&#34;https://docs.anthropic.com/en/docs/claude-code/github-actions&#34;&gt;Claude Code Actions&lt;/a&gt;, modify your &lt;code&gt;.github/workflows/claude.yaml&lt;/code&gt; to use the router:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;name: Claude Code&#xA;&#xA;on:&#xA;  issue_comment:&#xA;    types: [created]&#xA;  # ... other triggers&#xA;&#xA;jobs:&#xA;  claude:&#xA;    if: |&#xA;      (github.event_name == &#39;issue_comment&#39; &amp;amp;&amp;amp; contains(github.event.comment.body, &#39;@claude&#39;)) ||&#xA;      # ... other conditions&#xA;    runs-on: ubuntu-latest&#xA;    permissions:&#xA;      contents: read&#xA;      pull-requests: read&#xA;      issues: read&#xA;      id-token: write&#xA;    steps:&#xA;      - name: Checkout repository&#xA;        uses: actions/checkout@v4&#xA;        with:&#xA;          fetch-depth: 1&#xA;&#xA;      - name: Prepare Environment&#xA;        run: |&#xA;          curl -fsSL https://bun.sh/install | bash&#xA;          mkdir -p $HOME/.claude-code-router&#xA;          cat &amp;lt;&amp;lt; &#39;EOF&#39; &amp;gt; $HOME/.claude-code-router/config.json&#xA;          {&#xA;            &#34;log&#34;: true,&#xA;            &#34;OPENAI_API_KEY&#34;: &#34;${{ secrets.OPENAI_API_KEY }}&#34;,&#xA;            &#34;OPENAI_BASE_URL&#34;: &#34;https://api.deepseek.com&#34;,&#xA;            &#34;OPENAI_MODEL&#34;: &#34;deepseek-chat&#34;&#xA;          }&#xA;          EOF&#xA;        shell: bash&#xA;&#xA;      - name: Start Claude Code Router&#xA;        run: |&#xA;          nohup ~/.bun/bin/bunx @musistudio/claude-code-router@1.0.8 start &amp;amp;&#xA;        shell: bash&#xA;&#xA;      - name: Run Claude Code&#xA;        id: claude&#xA;        uses: anthropics/claude-code-action@beta&#xA;        env:&#xA;          ANTHROPIC_BASE_URL: http://localhost:3456&#xA;        with:&#xA;          anthropic_api_key: &#34;any-string-is-ok&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This setup allows for interesting automations, like running tasks during off-peak hours to reduce API costs.&lt;/p&gt; &#xA;&lt;h2&gt;📝 Further Reading&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/en/project-motivation-and-how-it-works.md&#34;&gt;Project Motivation and How It Works&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/en/maybe-we-can-do-more-with-the-route.md&#34;&gt;Maybe We Can Do More with the Router&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;❤️ Support &amp;amp; Sponsoring&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project helpful, please consider sponsoring its development. Your support is greatly appreciated!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ko-fi.com/F1F31GN2GM&#34;&gt;&lt;img src=&#34;https://ko-fi.com/img/githubbutton_sm.svg?sanitize=true&#34; alt=&#34;ko-fi&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/images/alipay.jpg&#34; width=&#34;200&#34; alt=&#34;Alipay&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/images/wechat.jpg&#34; width=&#34;200&#34; alt=&#34;WeChat Pay&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Our Sponsors&lt;/h3&gt; &#xA;&lt;p&gt;A huge thank you to all our sponsors for their generous support!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aihubmix.com/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/images/sponsors/aihubmix.png&#34; width=&#34;100&#34; alt=&#34;aihubmix&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;@Simon Leischnig&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/duanshuaimin&#34;&gt;@duanshuaimin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vrgitadmin&#34;&gt;@vrgitadmin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;@*o&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ceilwoo&#34;&gt;@ceilwoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;@*说&lt;/li&gt; &#xA; &lt;li&gt;@*更&lt;/li&gt; &#xA; &lt;li&gt;@K*g&lt;/li&gt; &#xA; &lt;li&gt;@R*R&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bobleer&#34;&gt;@bobleer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;@*苗&lt;/li&gt; &#xA; &lt;li&gt;@*划&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Clarence-pan&#34;&gt;@Clarence-pan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/carter003&#34;&gt;@carter003&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;@S*r&lt;/li&gt; &#xA; &lt;li&gt;@*晖&lt;/li&gt; &#xA; &lt;li&gt;@*敏&lt;/li&gt; &#xA; &lt;li&gt;@Z*z&lt;/li&gt; &#xA; &lt;li&gt;@*然&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cluic&#34;&gt;@cluic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;@*苗&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PromptExpert&#34;&gt;@PromptExpert&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;@*应&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yusnake&#34;&gt;@yusnake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;@*飞&lt;/li&gt; &#xA; &lt;li&gt;@董*&lt;/li&gt; &#xA; &lt;li&gt;@*汀&lt;/li&gt; &#xA; &lt;li&gt;@*涯&lt;/li&gt; &#xA; &lt;li&gt;@*:-）&lt;/li&gt; &#xA; &lt;li&gt;@**磊&lt;/li&gt; &#xA; &lt;li&gt;@*琢&lt;/li&gt; &#xA; &lt;li&gt;@*成&lt;/li&gt; &#xA; &lt;li&gt;@Z*o&lt;/li&gt; &#xA; &lt;li&gt;@*琨&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/congzhangzh&#34;&gt;@congzhangzh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;@*_&lt;/li&gt; &#xA; &lt;li&gt;@Z*m&lt;/li&gt; &#xA; &lt;li&gt;@*鑫&lt;/li&gt; &#xA; &lt;li&gt;@c*y&lt;/li&gt; &#xA; &lt;li&gt;@*昕&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;(If your name is masked, please contact me via my homepage email to update it with your GitHub username.)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>getzep/graphiti</title>
    <updated>2025-08-01T01:51:12Z</updated>
    <id>tag:github.com,2025-08-01:/getzep/graphiti</id>
    <link href="https://github.com/getzep/graphiti" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build Real-Time Knowledge Graphs for AI Agents&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.getzep.com/&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73&#34; width=&#34;150&#34; alt=&#34;Zep Logo&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; Graphiti &lt;/h1&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; Build Real-Time Knowledge Graphs for AI Agents&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/getzep/Graphiti/actions/workflows/lint.yml&#34;&gt;&lt;img src=&#34;https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat&#34; alt=&#34;Lint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;Unit Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml&#34;&gt;&lt;img src=&#34;https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg?sanitize=true&#34; alt=&#34;MyPy Check&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/getzep/graphiti&#34; alt=&#34;GitHub Repo stars&#34;&gt; &lt;a href=&#34;https://discord.com/invite/W8Kw6bsgXQ&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2501.13956&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/getzep/graphiti/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/getzep/graphiti?style=flat&amp;amp;label=Release&amp;amp;color=limegreen&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/12986&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/12986&#34; alt=&#34;getzep%2Fgraphiti | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;span&gt;⭐&lt;/span&gt; &lt;em&gt;Help us reach more developers and grow the Graphiti community. Star this repo!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Check out the new &lt;a href=&#34;https://raw.githubusercontent.com/getzep/graphiti/main/mcp_server/README.md&#34;&gt;MCP server for Graphiti&lt;/a&gt;! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.&lt;/p&gt; &#xA;&lt;p&gt;Use Graphiti to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integrate and maintain dynamic user interactions and business data.&lt;/li&gt; &#xA; &lt;li&gt;Facilitate state-based reasoning and task automation for agents.&lt;/li&gt; &#xA; &lt;li&gt;Query complex, evolving data with semantic, keyword, and graph-based search methods.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/getzep/graphiti/main/images/graphiti-graph-intro.gif&#34; alt=&#34;Graphiti temporal walkthrough&#34; width=&#34;700px&#34;&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;A knowledge graph is a network of interconnected facts, such as &lt;em&gt;&#34;Kendra loves Adidas shoes.&#34;&lt;/em&gt; Each fact is a &#34;triplet&#34; represented by two entities, or nodes (&#34;Kendra&#34;, &#34;Adidas shoes&#34;), and their relationship, or edge (&#34;loves&#34;). Knowledge Graphs have been explored extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph while handling changing relationships and maintaining historical context.&lt;/p&gt; &#xA;&lt;h2&gt;Graphiti and Zep&#39;s Context Engineering Platform.&lt;/h2&gt; &#xA;&lt;p&gt;Graphiti powers the core of &lt;a href=&#34;https://www.getzep.com&#34;&gt;Zep&lt;/a&gt;, a turn-key context engineering platform for AI Agents. Zep offers agent memory, Graph RAG for dynamic data, and context retrieval and assembly.&lt;/p&gt; &#xA;&lt;p&gt;Using Graphiti, we&#39;ve demonstrated Zep is the &lt;a href=&#34;https://blog.getzep.com/state-of-the-art-agent-memory/&#34;&gt;State of the Art in Agent Memory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Read our paper: &lt;a href=&#34;https://arxiv.org/abs/2501.13956&#34;&gt;Zep: A Temporal Knowledge Graph Architecture for Agent Memory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2501.13956&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/getzep/graphiti/main/images/arxiv-screenshot.png&#34; alt=&#34;Zep: A Temporal Knowledge Graph Architecture for Agent Memory&#34; width=&#34;700px&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Why Graphiti?&lt;/h2&gt; &#xA;&lt;p&gt;Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Real-Time Incremental Updates:&lt;/strong&gt; Immediate integration of new data episodes without batch recomputation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bi-Temporal Data Model:&lt;/strong&gt; Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient Hybrid Retrieval:&lt;/strong&gt; Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Custom Entity Definitions:&lt;/strong&gt; Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scalability:&lt;/strong&gt; Efficiently manages large datasets with parallel processing, suitable for enterprise environments.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/getzep/graphiti/main/images/graphiti-intro-slides-stock-2.gif&#34; alt=&#34;Graphiti structured + unstructured demo&#34; width=&#34;700px&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Graphiti vs. GraphRAG&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Aspect&lt;/th&gt; &#xA;   &lt;th&gt;GraphRAG&lt;/th&gt; &#xA;   &lt;th&gt;Graphiti&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Primary Use&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Static document summarization&lt;/td&gt; &#xA;   &lt;td&gt;Dynamic data management&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Data Handling&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Batch-oriented processing&lt;/td&gt; &#xA;   &lt;td&gt;Continuous, incremental updates&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Knowledge Structure&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Entity clusters &amp;amp; community summaries&lt;/td&gt; &#xA;   &lt;td&gt;Episodic data, semantic entities, communities&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Retrieval Method&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sequential LLM summarization&lt;/td&gt; &#xA;   &lt;td&gt;Hybrid semantic, keyword, and graph-based search&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Adaptability&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Low&lt;/td&gt; &#xA;   &lt;td&gt;High&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Temporal Handling&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Basic timestamp tracking&lt;/td&gt; &#xA;   &lt;td&gt;Explicit bi-temporal tracking&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Contradiction Handling&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLM-driven summarization judgments&lt;/td&gt; &#xA;   &lt;td&gt;Temporal edge invalidation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Query Latency&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Seconds to tens of seconds&lt;/td&gt; &#xA;   &lt;td&gt;Typically sub-second latency&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Custom Entity Types&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes, customizable&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Moderate&lt;/td&gt; &#xA;   &lt;td&gt;High, optimized for large datasets&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10 or higher&lt;/li&gt; &#xA; &lt;li&gt;Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)&lt;/li&gt; &#xA; &lt;li&gt;OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini). Using other services may result in incorrect output schemas and ingestion failures. This is particularly problematic when using smaller models.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Optional:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] The simplest way to install Neo4j is via &lt;a href=&#34;https://neo4j.com/download/&#34;&gt;Neo4j Desktop&lt;/a&gt;. It provides a user-friendly interface to manage Neo4j instances and databases. Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install graphiti-core&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv add graphiti-core&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installing with FalkorDB Support&lt;/h3&gt; &#xA;&lt;p&gt;If you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install graphiti-core[falkordb]&#xA;&#xA;# or with uv&#xA;uv add graphiti-core[falkordb]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;You can also install optional LLM providers as extras:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install with Anthropic support&#xA;pip install graphiti-core[anthropic]&#xA;&#xA;# Install with Groq support&#xA;pip install graphiti-core[groq]&#xA;&#xA;# Install with Google Gemini support&#xA;pip install graphiti-core[google-genai]&#xA;&#xA;# Install with multiple providers&#xA;pip install graphiti-core[anthropic,groq,google-genai]&#xA;&#xA;# Install with FalkorDB and LLM providers&#xA;pip install graphiti-core[falkordb,anthropic,google-genai]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Default to Low Concurrency; LLM Provider 429 Rate Limit Errors&lt;/h2&gt; &#xA;&lt;p&gt;Graphiti&#39;s ingestion pipelines are designed for high concurrency. By default, concurrency is set low to avoid LLM Provider 429 Rate Limit Errors. If you find Graphiti slow, please increase concurrency as described below.&lt;/p&gt; &#xA;&lt;p&gt;Concurrency controlled by the &lt;code&gt;SEMAPHORE_LIMIT&lt;/code&gt; environment variable. By default, &lt;code&gt;SEMAPHORE_LIMIT&lt;/code&gt; is set to &lt;code&gt;10&lt;/code&gt; concurrent operations to help prevent &lt;code&gt;429&lt;/code&gt; rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.&lt;/p&gt; &#xA;&lt;p&gt;If your LLM provider allows higher throughput, you can increase &lt;code&gt;SEMAPHORE_LIMIT&lt;/code&gt; to boost episode ingestion performance.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; is set in your environment. Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI compatible APIs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For a complete working example, see the &lt;a href=&#34;https://raw.githubusercontent.com/getzep/graphiti/main/examples/quickstart/README.md&#34;&gt;Quickstart Example&lt;/a&gt; in the examples directory. The quickstart demonstrates:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Connecting to a Neo4j or FalkorDB database&lt;/li&gt; &#xA; &lt;li&gt;Initializing Graphiti indices and constraints&lt;/li&gt; &#xA; &lt;li&gt;Adding episodes to the graph (both text and structured JSON)&lt;/li&gt; &#xA; &lt;li&gt;Searching for relationships (edges) using hybrid search&lt;/li&gt; &#xA; &lt;li&gt;Reranking search results using graph distance&lt;/li&gt; &#xA; &lt;li&gt;Searching for nodes using predefined search recipes&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.&lt;/p&gt; &#xA;&lt;h2&gt;MCP Server&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;mcp_server&lt;/code&gt; directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti&#39;s knowledge graph capabilities through the MCP protocol.&lt;/p&gt; &#xA;&lt;p&gt;Key features of the MCP server include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Episode management (add, retrieve, delete)&lt;/li&gt; &#xA; &lt;li&gt;Entity management and relationship handling&lt;/li&gt; &#xA; &lt;li&gt;Semantic and hybrid search capabilities&lt;/li&gt; &#xA; &lt;li&gt;Group management for organizing related data&lt;/li&gt; &#xA; &lt;li&gt;Graph maintenance operations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.&lt;/p&gt; &#xA;&lt;p&gt;For detailed setup instructions and usage examples, see the &lt;a href=&#34;https://raw.githubusercontent.com/getzep/graphiti/main/mcp_server/README.md&#34;&gt;MCP server README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;REST Service&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;server&lt;/code&gt; directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.&lt;/p&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://raw.githubusercontent.com/getzep/graphiti/main/server/README.md&#34;&gt;server README&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Optional Environment Variables&lt;/h2&gt; &#xA;&lt;p&gt;In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables. If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables must be set.&lt;/p&gt; &#xA;&lt;h3&gt;Database Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Database names are configured directly in the driver constructors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neo4j&lt;/strong&gt;: Database name defaults to &lt;code&gt;neo4j&lt;/code&gt; (hardcoded in Neo4jDriver)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FalkorDB&lt;/strong&gt;: Database name defaults to &lt;code&gt;default_db&lt;/code&gt; (hardcoded in FalkorDriver)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;As of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the &lt;code&gt;graph_driver&lt;/code&gt; parameter.&lt;/p&gt; &#xA;&lt;h4&gt;Neo4j with Custom Database Name&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from graphiti_core import Graphiti&#xA;from graphiti_core.driver.neo4j_driver import Neo4jDriver&#xA;&#xA;# Create a Neo4j driver with custom database name&#xA;driver = Neo4jDriver(&#xA;    uri=&#34;bolt://localhost:7687&#34;,&#xA;    user=&#34;neo4j&#34;,&#xA;    password=&#34;password&#34;,&#xA;    database=&#34;my_custom_database&#34;  # Custom database name&#xA;)&#xA;&#xA;# Pass the driver to Graphiti&#xA;graphiti = Graphiti(graph_driver=driver)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;FalkorDB with Custom Database Name&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from graphiti_core import Graphiti&#xA;from graphiti_core.driver.falkordb_driver import FalkorDriver&#xA;&#xA;# Create a FalkorDB driver with custom database name&#xA;driver = FalkorDriver(&#xA;    host=&#34;localhost&#34;,&#xA;    port=6379,&#xA;    username=&#34;falkor_user&#34;,  # Optional&#xA;    password=&#34;falkor_password&#34;,  # Optional&#xA;    database=&#34;my_custom_graph&#34;  # Custom database name&#xA;)&#xA;&#xA;# Pass the driver to Graphiti&#xA;graphiti = Graphiti(graph_driver=driver)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Performance Configuration&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;USE_PARALLEL_RUNTIME&lt;/code&gt; is an optional boolean variable that can be set to true if you wish to enable Neo4j&#39;s parallel runtime feature for several of our search queries. Note that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances, as such this feature is off by default.&lt;/p&gt; &#xA;&lt;h2&gt;Using Graphiti with Azure OpenAI&lt;/h2&gt; &#xA;&lt;p&gt;Graphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import AsyncAzureOpenAI&#xA;from graphiti_core import Graphiti&#xA;from graphiti_core.llm_client import LLMConfig, OpenAIClient&#xA;from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig&#xA;from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient&#xA;&#xA;# Azure OpenAI configuration - use separate endpoints for different services&#xA;api_key = &#34;&amp;lt;your-api-key&amp;gt;&#34;&#xA;api_version = &#34;&amp;lt;your-api-version&amp;gt;&#34;&#xA;llm_endpoint = &#34;&amp;lt;your-llm-endpoint&amp;gt;&#34;  # e.g., &#34;https://your-llm-resource.openai.azure.com/&#34;&#xA;embedding_endpoint = &#34;&amp;lt;your-embedding-endpoint&amp;gt;&#34;  # e.g., &#34;https://your-embedding-resource.openai.azure.com/&#34;&#xA;&#xA;# Create separate Azure OpenAI clients for different services&#xA;llm_client_azure = AsyncAzureOpenAI(&#xA;    api_key=api_key,&#xA;    api_version=api_version,&#xA;    azure_endpoint=llm_endpoint&#xA;)&#xA;&#xA;embedding_client_azure = AsyncAzureOpenAI(&#xA;    api_key=api_key,&#xA;    api_version=api_version,&#xA;    azure_endpoint=embedding_endpoint&#xA;)&#xA;&#xA;# Create LLM Config with your Azure deployment names&#xA;azure_llm_config = LLMConfig(&#xA;    small_model=&#34;gpt-4.1-nano&#34;,&#xA;    model=&#34;gpt-4.1-mini&#34;,&#xA;)&#xA;&#xA;# Initialize Graphiti with Azure OpenAI clients&#xA;graphiti = Graphiti(&#xA;    &#34;bolt://localhost:7687&#34;,&#xA;    &#34;neo4j&#34;,&#xA;    &#34;password&#34;,&#xA;    llm_client=OpenAIClient(&#xA;        llm_config=azure_llm_config,&#xA;        client=llm_client_azure&#xA;    ),&#xA;    embedder=OpenAIEmbedder(&#xA;        config=OpenAIEmbedderConfig(&#xA;            embedding_model=&#34;text-embedding-3-small-deployment&#34;  # Your Azure embedding deployment name&#xA;        ),&#xA;        client=embedding_client_azure&#xA;    ),&#xA;    cross_encoder=OpenAIRerankerClient(&#xA;        llm_config=LLMConfig(&#xA;            model=azure_llm_config.small_model  # Use small model for reranking&#xA;        ),&#xA;        client=llm_client_azure&#xA;    )&#xA;)&#xA;&#xA;# Now you can use Graphiti with Azure OpenAI&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.&lt;/p&gt; &#xA;&lt;h2&gt;Using Graphiti with Google Gemini&lt;/h2&gt; &#xA;&lt;p&gt;Graphiti supports Google&#39;s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you&#39;ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.&lt;/p&gt; &#xA;&lt;p&gt;Install Graphiti:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv add &#34;graphiti-core[google-genai]&#34;&#xA;&#xA;# or&#xA;&#xA;pip install &#34;graphiti-core[google-genai]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from graphiti_core import Graphiti&#xA;from graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig&#xA;from graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig&#xA;from graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient&#xA;&#xA;# Google API key configuration&#xA;api_key = &#34;&amp;lt;your-google-api-key&amp;gt;&#34;&#xA;&#xA;# Initialize Graphiti with Gemini clients&#xA;graphiti = Graphiti(&#xA;    &#34;bolt://localhost:7687&#34;,&#xA;    &#34;neo4j&#34;,&#xA;    &#34;password&#34;,&#xA;    llm_client=GeminiClient(&#xA;        config=LLMConfig(&#xA;            api_key=api_key,&#xA;            model=&#34;gemini-2.0-flash&#34;&#xA;        )&#xA;    ),&#xA;    embedder=GeminiEmbedder(&#xA;        config=GeminiEmbedderConfig(&#xA;            api_key=api_key,&#xA;            embedding_model=&#34;embedding-001&#34;&#xA;        )&#xA;    ),&#xA;    cross_encoder=GeminiRerankerClient(&#xA;        config=LLMConfig(&#xA;            api_key=api_key,&#xA;            model=&#34;gemini-2.5-flash-lite-preview-06-17&#34;&#xA;        )&#xA;    )&#xA;)&#xA;&#xA;# Now you can use Graphiti with Google Gemini for all components&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The Gemini reranker uses the &lt;code&gt;gemini-2.5-flash-lite-preview-06-17&lt;/code&gt; model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini&#39;s log probabilities feature to rank passage relevance.&lt;/p&gt; &#xA;&lt;h2&gt;Using Graphiti with Ollama (Local LLM)&lt;/h2&gt; &#xA;&lt;p&gt;Graphiti supports Ollama for running local LLMs and embedding models via Ollama&#39;s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.&lt;/p&gt; &#xA;&lt;p&gt;Install the models: ollama pull deepseek-r1:7b # LLM ollama pull nomic-embed-text # embeddings&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from graphiti_core import Graphiti&#xA;from graphiti_core.llm_client.config import LLMConfig&#xA;from graphiti_core.llm_client.openai_client import OpenAIClient&#xA;from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig&#xA;from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient&#xA;&#xA;# Configure Ollama LLM client&#xA;llm_config = LLMConfig(&#xA;    api_key=&#34;abc&#34;,  # Ollama doesn&#39;t require a real API key&#xA;    model=&#34;deepseek-r1:7b&#34;,&#xA;    small_model=&#34;deepseek-r1:7b&#34;,&#xA;    base_url=&#34;http://localhost:11434/v1&#34;, # Ollama provides this port&#xA;)&#xA;&#xA;llm_client = OpenAIClient(config=llm_config)&#xA;&#xA;# Initialize Graphiti with Ollama clients&#xA;graphiti = Graphiti(&#xA;    &#34;bolt://localhost:7687&#34;,&#xA;    &#34;neo4j&#34;,&#xA;    &#34;password&#34;,&#xA;    llm_client=llm_client,&#xA;    embedder=OpenAIEmbedder(&#xA;        config=OpenAIEmbedderConfig(&#xA;            api_key=&#34;abc&#34;,&#xA;            embedding_model=&#34;nomic-embed-text&#34;,&#xA;            embedding_dim=768,&#xA;            base_url=&#34;http://localhost:11434/v1&#34;,&#xA;        )&#xA;    ),&#xA;    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),&#xA;)&#xA;&#xA;# Now you can use Graphiti with local Ollama models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ensure Ollama is running (&lt;code&gt;ollama serve&lt;/code&gt;) and that you have pulled the models you want to use.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.getzep.com/graphiti&#34;&gt;Guides and API documentation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.getzep.com/graphiti/graphiti/quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.getzep.com/graphiti/graphiti/lang-graph-agent&#34;&gt;Building an agent with LangChain&#39;s LangGraph and Graphiti&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Telemetry&lt;/h2&gt; &#xA;&lt;p&gt;Graphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here&#39;s exactly what we collect and why.&lt;/p&gt; &#xA;&lt;h3&gt;What We Collect&lt;/h3&gt; &#xA;&lt;p&gt;When you initialize a Graphiti instance, we collect:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Anonymous identifier&lt;/strong&gt;: A randomly generated UUID stored locally in &lt;code&gt;~/.cache/graphiti/telemetry_anon_id&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;System information&lt;/strong&gt;: Operating system, Python version, and system architecture&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Graphiti version&lt;/strong&gt;: The version you&#39;re using&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Configuration choices&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;LLM provider type (OpenAI, Azure, Anthropic, etc.)&lt;/li&gt; &#xA;   &lt;li&gt;Database backend (Neo4j, FalkorDB)&lt;/li&gt; &#xA;   &lt;li&gt;Embedder provider (OpenAI, Azure, Voyage, etc.)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;What We Don&#39;t Collect&lt;/h3&gt; &#xA;&lt;p&gt;We are committed to protecting your privacy. We &lt;strong&gt;never&lt;/strong&gt; collect:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Personal information or identifiers&lt;/li&gt; &#xA; &lt;li&gt;API keys or credentials&lt;/li&gt; &#xA; &lt;li&gt;Your actual data, queries, or graph content&lt;/li&gt; &#xA; &lt;li&gt;IP addresses or hostnames&lt;/li&gt; &#xA; &lt;li&gt;File paths or system-specific information&lt;/li&gt; &#xA; &lt;li&gt;Any content from your episodes, nodes, or edges&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Why We Collect This Data&lt;/h3&gt; &#xA;&lt;p&gt;This information helps us:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understand which configurations are most popular to prioritize support and testing&lt;/li&gt; &#xA; &lt;li&gt;Identify which LLM and database providers to focus development efforts on&lt;/li&gt; &#xA; &lt;li&gt;Track adoption patterns to guide our roadmap&lt;/li&gt; &#xA; &lt;li&gt;Ensure compatibility across different Python versions and operating systems&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By sharing this anonymous information, you help us make Graphiti better for everyone in the community.&lt;/p&gt; &#xA;&lt;h3&gt;View the Telemetry Code&lt;/h3&gt; &#xA;&lt;p&gt;The Telemetry code &lt;a href=&#34;https://raw.githubusercontent.com/getzep/graphiti/main/graphiti_core/telemetry/telemetry.py&#34;&gt;may be found here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;How to Disable Telemetry&lt;/h3&gt; &#xA;&lt;p&gt;Telemetry is &lt;strong&gt;opt-out&lt;/strong&gt; and can be disabled at any time. To disable telemetry collection:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Option 1: Environment Variable&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export GRAPHITI_TELEMETRY_ENABLED=false&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Option 2: Set in your shell profile&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For bash users (~/.bashrc or ~/.bash_profile)&#xA;echo &#39;export GRAPHITI_TELEMETRY_ENABLED=false&#39; &amp;gt;&amp;gt; ~/.bashrc&#xA;&#xA;# For zsh users (~/.zshrc)&#xA;echo &#39;export GRAPHITI_TELEMETRY_ENABLED=false&#39; &amp;gt;&amp;gt; ~/.zshrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Option 3: Set for a specific Python session&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;os.environ[&#39;GRAPHITI_TELEMETRY_ENABLED&#39;] = &#39;false&#39;&#xA;&#xA;# Then initialize Graphiti as usual&#xA;from graphiti_core import Graphiti&#xA;graphiti = Graphiti(...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Telemetry is automatically disabled during test runs (when &lt;code&gt;pytest&lt;/code&gt; is detected).&lt;/p&gt; &#xA;&lt;h3&gt;Technical Details&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Telemetry uses PostHog for anonymous analytics collection&lt;/li&gt; &#xA; &lt;li&gt;All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality&lt;/li&gt; &#xA; &lt;li&gt;The anonymous ID is stored locally and is not tied to any personal information&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Status and Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Graphiti is under active development. We aim to maintain API stability while working on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Supporting custom graph schemas: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Allow developers to provide their own defined node and edge classes when ingesting episodes&lt;/li&gt; &#xA;   &lt;li&gt;Enable more flexible knowledge representation tailored to specific use cases&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Enhancing retrieval capabilities with more robust and configurable options&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Graphiti MCP Server&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Expanding test coverage to ensure reliability and catch edge cases&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We encourage and appreciate all forms of contributions, whether it&#39;s code, documentation, addressing GitHub Issues, or answering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/getzep/graphiti/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Join the &lt;a href=&#34;https://discord.com/invite/W8Kw6bsgXQ&#34;&gt;Zep Discord server&lt;/a&gt; and make your way to the &lt;strong&gt;#Graphiti&lt;/strong&gt; channel!&lt;/p&gt;</summary>
  </entry>
</feed>