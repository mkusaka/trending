<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-01T01:46:45Z</updated>
  <subtitle>Monthly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>toeverything/AFFiNE</title>
    <updated>2024-09-01T01:46:45Z</updated>
    <id>tag:github.com,2024-09-01:/toeverything/AFFiNE</id>
    <link href="https://github.com/toeverything/AFFiNE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;There can be more than Notion and Miro. AFFiNE(pronounced […ô‚Äòfain]) is a next-gen knowledge base that brings planning, sorting and creating all together. Privacy first, open-source, customizable and ready to use.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1 style=&#34;border-bottom: none&#34;&gt; &lt;b&gt;&lt;a href=&#34;https://affine.pro&#34;&gt;AFFiNE.PRO&lt;/a&gt;&lt;/b&gt;&lt;br&gt; Write, Draw and Plan All at Once &lt;br&gt; &lt;/h1&gt; &#xA; &lt;a href=&#34;https://affine.pro/download&#34;&gt; &lt;img alt=&#34;affine logo&#34; src=&#34;https://cdn.affine.pro/Github_hero_image1.png&#34; style=&#34;width: 100%&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;p align=&#34;center&#34;&gt; A privacy-focused, local-first, open-source, and ready-to-use alternative for Notion &amp;amp; Miro. &lt;br&gt; One hyper-fused platform for wildly creative minds. &lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://www.producthunt.com/posts/affine-3?utm_source=badge-featured&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-affine-3&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=440671&amp;amp;theme=light&#34; alt=&#34;AFFiNE - One app for all - Where Notion meets Miro | Product Hunt&#34; style=&#34;width: 250px; height: 54px;&#34; width=&#34;250&#34; height=&#34;54&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://affine.pro&#34;&gt;Home Page&lt;/a&gt; | &#xA;  &lt;a href=&#34;https://discord.gg/whd5mjYqVw&#34;&gt;Discord&lt;/a&gt; | &#xA;  &lt;a href=&#34;https://app.affine.pro&#34;&gt;Live Demo&lt;/a&gt; | &#xA;  &lt;a href=&#34;https://affine.pro/blog/&#34;&gt;Blog&lt;/a&gt; | &#xA;  &lt;a href=&#34;https://docs.affine.pro/docs/&#34;&gt;Documentation&lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/toeverything/AFFiNE/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/toeverything/AFFiNE/total&#34; alt=&#34;Releases&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/toeverything/AFFiNE/canary/#contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/toeverything/AFFiNE&#34; alt=&#34;All Contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.typescriptlang.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/package-json/dependency-version/toeverything/affine/dev/typescript&#34; alt=&#34;TypeScript-version-icon&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.rust-lang.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Rust-1.79.0-dea584&#34; alt=&#34;Rust-version-icon&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;em&gt;Docs, canvas and tables are hyper-merged with AFFiNE - just like the word affine (…ôÀàf å…™n | a-fine).&lt;/em&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/toeverything/AFFiNE/assets/79301703/49a426bb-8d2b-4216-891a-fa5993642253&#34; style=&#34;width: 100%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Getting started &amp;amp; staying tuned with us.&lt;/h2&gt; &#xA;&lt;p&gt;Star us, and you will receive all release notifications from GitHub without any delay!&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/79301703/230891830-0110681e-8c7e-483b-b6d9-9e42b291b9ef.gif&#34; style=&#34;width: 100%&#34;&gt; &#xA;&lt;h2&gt;What is AFFiNE&lt;/h2&gt; &#xA;&lt;p&gt;AFFiNE is an open-source, all-in-one workspace and an operating system for all the building blocks that assemble your knowledge base and much more -- wiki, knowledge management, presentation and digital assets. It&#39;s a better alternative to Notion and Miro.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;A true canvas for blocks in any form. Docs and whiteboard are now fully merged.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Many editor apps claim to be a canvas for productivity, but AFFiNE is one of the very few which allows you to put any building block on an edgeless canvas -- rich text, sticky notes, any embedded web pages, multi-view databases, linked pages, shapes and even slides. We have it all.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multimodal AI partner ready to kick in any work&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Write up professional work report? Turn an outline into expressive and presentable slides? Summary an article into a well-structured mindmap? Sorting your job plan and backlog for tasks? Or... draw and code prototype apps and web pages directly all with one prompt? With you, AFFiNE AI pushes your creativity to the edge of your imagination.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Local-first &amp;amp; Real-time collaborative&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We love the idea of local-first that you always own your data on your disk, in spite of the cloud. Furthermore, AFFiNE supports real-time sync and collaborations on web and cross-platform clients.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-host &amp;amp; Shape your own AFFiNE&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You have the freedom to manage, self-host, fork and build your own AFFiNE. Plugin community and third-party blocks are coming soon. More tractions on &lt;a href=&#34;https://blocksuite.io&#34;&gt;Blocksuite&lt;/a&gt;. Check there to learn how to &lt;a href=&#34;https://docs.affine.pro/docs/self-host-affine&#34;&gt;self-host AFFiNE&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;‚ÄúWe shape our tools and thereafter our tools shape us‚Äù. A lot of pioneers have inspired us along the way, e.g.:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Quip &amp;amp; Notion with their great concept of ‚Äúeverything is a block‚Äù&lt;/li&gt; &#xA; &lt;li&gt;Trello with their Kanban&lt;/li&gt; &#xA; &lt;li&gt;Airtable &amp;amp; Miro with their no-code programmable datasheets&lt;/li&gt; &#xA; &lt;li&gt;Miro &amp;amp; Whimiscal with their edgeless visual whiteboard&lt;/li&gt; &#xA; &lt;li&gt;Remote &amp;amp; Capacities with their object-based tag system&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There is a large overlap of their atomic ‚Äúbuilding blocks‚Äù between these apps. They are not open source, nor do they have a plugin system like Vscode for contributors to customize. We want to have something that contains all the features we love and also goes one step even further.&lt;/p&gt; &#xA;&lt;p&gt;Thanks for checking us out, we appreciate your interest and sincerely hope that AFFiNE resonates with you! üéµ Checking &lt;a href=&#34;https://affine.pro/&#34;&gt;https://affine.pro/&lt;/a&gt; for more details ions.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Bug Reports&lt;/th&gt; &#xA;   &lt;th&gt;Feature Requests&lt;/th&gt; &#xA;   &lt;th&gt;Questions/Discussions&lt;/th&gt; &#xA;   &lt;th&gt;AFFiNE Community&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/toeverything/AFFiNE/issues/new?assignees=&amp;amp;labels=bug%2Cproduct-review&amp;amp;template=BUG-REPORT.yml&amp;amp;title=TITLE&#34;&gt;Create a bug report&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/toeverything/AFFiNE/issues/new?assignees=&amp;amp;labels=feat%2Cproduct-review&amp;amp;template=FEATURE-REQUEST.yml&amp;amp;title=TITLE&#34;&gt;Submit a feature request&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/toeverything/AFFiNE/discussions&#34;&gt;Check GitHub Discussion&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://community.affine.pro&#34;&gt;Vist the AFFiNE Community&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Something isn&#39;t working as expected&lt;/td&gt; &#xA;   &lt;td&gt;An idea for a new feature, or improvements&lt;/td&gt; &#xA;   &lt;td&gt;Discuss and ask questions&lt;/td&gt; &#xA;   &lt;td&gt;A place to ask, learn and engage with others&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Calling all developers, testers, tech writers and more! Contributions of all types are more than welcome, you can read more in &lt;a href=&#34;https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/types-of-contributions.md&#34;&gt;docs/types-of-contributions.md&lt;/a&gt;. If you are interested in contributing code, read our &lt;a href=&#34;https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/CONTRIBUTING.md&#34;&gt;docs/CONTRIBUTING.md&lt;/a&gt; and feel free to check out our GitHub issues to get stuck in to show us what you‚Äôre made of.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Before you start contributing, please make sure you have read and accepted our &lt;a href=&#34;https://github.com/toeverything/affine/edit/canary/.github/CLA.md&#34;&gt;Contributor License Agreement&lt;/a&gt;. To indicate your agreement, simply edit this file and submit a pull request.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For &lt;strong&gt;bug reports&lt;/strong&gt;, &lt;strong&gt;feature requests&lt;/strong&gt; and other &lt;strong&gt;suggestions&lt;/strong&gt; you can also &lt;a href=&#34;https://github.com/toeverything/AFFiNE/issues/new/choose&#34;&gt;create a new issue&lt;/a&gt; and choose the most appropriate template for your feedback.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;strong&gt;translation&lt;/strong&gt; and &lt;strong&gt;language support&lt;/strong&gt; you can visit our &lt;a href=&#34;https://community.affine.pro/c/i18n-general&#34;&gt;i18n General Space&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Looking for &lt;strong&gt;other ways to contribute&lt;/strong&gt; and wondering where to start? Check out the &lt;a href=&#34;https://community.affine.pro/c/start-here/affine-ambassador&#34;&gt;AFFiNE Ambassador program&lt;/a&gt;, we work closely with passionate community members and provide them with a wide range of support and resources.&lt;/p&gt; &#xA;&lt;p&gt;If you have questions, you are welcome to contact us. One of the best places to get more info and learn more is in the &lt;a href=&#34;https://community.affine.pro&#34;&gt;AFFiNE Community&lt;/a&gt; where you can engage with other like-minded individuals.&lt;/p&gt; &#xA;&lt;h2&gt;Ecosystem&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/toeverything/AFFiNE/canary/packages/frontend/component&#34;&gt;@affine/component&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AFFiNE Component Resources&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://img.shields.io/codecov/c/github/toeverything/affine?style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/toeverything/AFFiNE/canary/packages/common/theme&#34;&gt;@toeverything/theme&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AFFiNE theme&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.npmjs.com/package/@toeverything/theme&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/dm/@toeverything/theme?style=flat-square&amp;amp;color=eee&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Upstreams&lt;/h2&gt; &#xA;&lt;p&gt;We would also like to give thanks to open-source projects that make AFFiNE possible:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/toeverything/BlockSuite&#34;&gt;Blocksuite&lt;/a&gt; - üí† BlockSuite is the open-source collaborative editor project behind AFFiNE.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/toeverything/OctoBase&#34;&gt;OctoBase&lt;/a&gt; - üêô OctoBase is the open-source database behind AFFiNE, local-first, yet collaborative. A light-weight, scalable, data engine written in Rust.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yjs/yjs&#34;&gt;yjs&lt;/a&gt; - Fundamental support of CRDTs for our implementation on state management and data sync.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/electron/electron&#34;&gt;electron&lt;/a&gt; - Build cross-platform desktop apps with JavaScript, HTML, and CSS.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebook/react&#34;&gt;React&lt;/a&gt; - The library for web and native user interfaces.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/napi-rs/napi-rs&#34;&gt;napi-rs&lt;/a&gt; - A framework for building compiled Node.js add-ons in Rust via Node-API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pmndrs/jotai&#34;&gt;Jotai&lt;/a&gt; - Primitive and flexible state management for React.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Jack-Works/async-call-rpc&#34;&gt;async-call-rpc&lt;/a&gt; - A lightweight JSON RPC client &amp;amp; server.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vitejs/vite&#34;&gt;Vite&lt;/a&gt; - Next generation frontend tooling.&lt;/li&gt; &#xA; &lt;li&gt;Other upstream &lt;a href=&#34;https://github.com/toeverything/AFFiNE/network/dependencies&#34;&gt;dependencies&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks a lot to the community for providing such powerful and simple libraries, so that we can focus more on the implementation of the product logic, and we hope that in the future our projects will also provide a more easy-to-use knowledge base for everyone.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;We would like to express our gratitude to all the individuals who have already contributed to AFFiNE! If you have any AFFiNE-related project, documentation, tool or template, please feel free to contribute it by submitting a pull request to our curated list on GitHub: &lt;a href=&#34;https://github.com/toeverything/awesome-affine&#34;&gt;awesome-affine&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/toeverything/affine/graphs/contributors&#34;&gt; &lt;img alt=&#34;contributors&#34; src=&#34;https://opencollective.com/affine/contributors.svg?width=890&amp;amp;button=false&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Self-Host&lt;/h2&gt; &#xA;&lt;p&gt;Begin with Docker to deploy your own feature-rich, unrestricted version of AFFiNE. Our team is diligently updating to the latest version. For more information on how to self-host AFFiNE, please refer to our &lt;a href=&#34;https://docs.affine.pro/docs/self-host-affine&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Hiring&lt;/h2&gt; &#xA;&lt;p&gt;Some amazing companies, including AFFiNE, are looking for developers! Are you interested in joining AFFiNE or its partners? Check out our Discord channel for some of the latest jobs available.&lt;/p&gt; &#xA;&lt;h2&gt;Feature Request&lt;/h2&gt; &#xA;&lt;p&gt;For feature requests, please see &lt;a href=&#34;https://community.affine.pro/c/feature-requests/&#34;&gt;community.affine.pro&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;h3&gt;Codespaces&lt;/h3&gt; &#xA;&lt;p&gt;From the GitHub repo main page, click the green &#34;Code&#34; button and select &#34;Create codespace on master&#34;. This will open a new Codespace with the (supposedly auto-forked AFFiNE repo cloned, built, and ready to go.&lt;/p&gt; &#xA;&lt;h3&gt;Local&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/BUILDING.md&#34;&gt;BUILDING.md&lt;/a&gt; for instructions on how to build AFFiNE from source code.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions from everyone. See &lt;a href=&#34;https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/contributing/tutorial.md&#34;&gt;docs/contributing/tutorial.md&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Thanks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.chromatic.com/&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/321738/84662277-e3db4f80-af1b-11ea-88f5-91d67a5e59f6.png&#34; width=&#34;153&#34; height=&#34;30&#34; alt=&#34;Chromatic&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://www.chromatic.com/&#34;&gt;Chromatic&lt;/a&gt; for providing the visual testing platform that helps us review UI changes and catch visual regressions.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;h3&gt;Editions&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;AFFiNE Community Edition (CE) is the current available version, it&#39;s free for self-host under the MIT license.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;AFFiNE Enterprise Edition (EE) is yet to be published, it will have more advanced features and enterprise-oriented offerings, including but not exclusive to rebranding and SSO, advanced admin and audit, etc., you may refer to &lt;a href=&#34;https://affine.pro/pricing&#34;&gt;https://affine.pro/pricing&lt;/a&gt; for more information&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/toeverything/AFFiNE/canary/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LLaVA-VL/LLaVA-NeXT</title>
    <updated>2024-09-01T01:46:45Z</updated>
    <id>tag:github.com,2024-09-01:/LLaVA-VL/LLaVA-NeXT</id>
    <link href="https://github.com/LLaVA-VL/LLaVA-NeXT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://i.postimg.cc/pL17YtG4/WX20240508-220230-2x.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;LLaVA-NeXT: Open Large Multimodal Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2408.03326&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_onevision-paper-green&#34; alt=&#34;Static Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://llava-vl.github.io/blog/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-blog-green&#34; alt=&#34;llava_next-blog&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://llava-onevision.lmms-lab.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_onevision-demo-red&#34; alt=&#34;llava_onevision-demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/lmms-lab/LLaVA-NeXT-Interleave-Demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-interleave_demo-red&#34; alt=&#34;llava_next-interleave_demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/WildVision/vision-arena&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-video_demo-red&#34; alt=&#34;llava_next-video_demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-onevision-66a259c3526e15166d6bba37&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_onevision-checkpoints-blue&#34; alt=&#34;llava_onevision-checkpoints&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-interleave-66763c55c411b340b35873d1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-interleave_checkpoints-blue&#34; alt=&#34;llava_next-interleave_checkpoints&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-video_checkpoints-blue&#34; alt=&#34;llava_next-video_checkpoints&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/lmms-lab&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-image_checkpoints-blue&#34; alt=&#34;llava_next-image_checkpoints&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Release Notes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/08/06] üî• &lt;strong&gt;üöÄ &lt;a href=&#34;https://llava-vl.github.io/blog/2024-08-05-llava-onevision/&#34;&gt;LLaVA-OneVision (OV)&lt;/a&gt;!&lt;/strong&gt; The new LLaVA-OV models (0.5B/7B/72B) achieve new state-of-the-art performance across single-image, multi-image, and video benchmarks, sometimes rivaling top commercial models on 47 diverse benchmarks. üìÑ Explore More:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2408.03326&#34;&gt;[Paper]&lt;/a&gt;: In-depth insights, new emegerging scenarios, ie, strong video understadning through task transfer from images.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-NeXT/raw/main/docs/LLaVA_OneVision.md&#34;&gt;[LLaVA-OV Doc]&lt;/a&gt;: Model inference and evaluation guidance.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-NeXT/raw/main/scripts/train&#34;&gt;[Scripts]&lt;/a&gt;: Start training models on your single-image/multi-image/video data.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/07/16] üî• &lt;strong&gt;LLaVA-NeXT-Video&lt;/strong&gt; has been upgraded. The new 32B model achieves the best open-source performance on several video benchmarks, including &lt;a href=&#34;https://video-mme.github.io/home_page.html#leaderboard&#34;&gt;Video-MME&lt;/a&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Video_0716.md&#34;&gt;this page&lt;/a&gt; for details, refer to &lt;a href=&#34;https://huggingface.co/spaces/WildVision/vision-arena&#34;&gt;llava_next-video_demo&lt;/a&gt; for demo.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/06/23] üî• &lt;strong&gt;LLaVA-NeXT-Interleave&lt;/strong&gt; is released. We utilize image-text interleaved format to unify multi-image, video, and 3D tasks in one LLM and achieve &lt;strong&gt;SoTA&lt;/strong&gt; performance on a wide range of benchmarks. Check out &lt;a href=&#34;https://arxiv.org/pdf/2407.07895&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/&#34;&gt;blog&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-interleave-66763c55c411b340b35873d1&#34;&gt;checkpoints&lt;/a&gt; to see new capabilities and improved performance! We have released 0.5b, 7b, and 7b-dpo models.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An all-round LLM for multi-image, video, and 3D with strong performance [&lt;a href=&#34;https://huggingface.co/spaces/lmms-lab/LLaVA-NeXT-Interleave-Demo&#34;&gt;demo&lt;/a&gt;]&lt;/li&gt; &#xA;   &lt;li&gt;Construct interleave training data &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/M4-Instruct-Data&#34;&gt;&lt;strong&gt;M4-Instruct&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Construct multi-image benchmark &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/LLaVA-NeXT-Interleave-Bench&#34;&gt;&lt;strong&gt;LLaVA-Interleave Bench&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/05/25] üî• Wondering &#34;&lt;a href=&#34;https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/&#34;&gt;What Else Influences Visual Instruction Tuning Beyond Data?&lt;/a&gt;&#34; Our new &lt;a href=&#34;https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/&#34;&gt;blog&lt;/a&gt; summarizes empirical explorations to ablate the various design choices in improving LMMs except instruct data itself. Meanwhile, open-source the recapioned high-quality data using LLaVA-NeXT-34B on &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-118K&#34;&gt;[COCO]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-558K&#34;&gt;[LCS]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/lmms-lab/LLaVA-ReCap-CC3M&#34;&gt;[CC3M]&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Architectures (LMM &amp;amp; Vision Encoder)&lt;/li&gt; &#xA;   &lt;li&gt;Visual Representations (Resolution &amp;amp; # Tokens)&lt;/li&gt; &#xA;   &lt;li&gt;Training Strategies (High-quality data &amp;amp; Trainable modules)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/05/10] üî• &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; (Stronger) models are released, with support of stronger LMM inlcuding LLama-3 (8B) and Qwen-1.5 (72B/110B) Check out [&lt;a href=&#34;https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/&#34;&gt;blog&lt;/a&gt;] and [&lt;a href=&#34;https://huggingface.co/lmms-lab&#34;&gt;checkpoints&lt;/a&gt;] to see improved performance!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/05/10] üî• &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; (Video) is released. The image-only-trained LLaVA-NeXT model is surprisingly strong on video tasks with zero-shot modality transfer. DPO training with AI feedback on videos can yield significant improvement. [&lt;a href=&#34;https://llava-vl.github.io/blog/2024-04-30-llava-next-video/&#34;&gt;Blog&lt;/a&gt;], [&lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944&#34;&gt;checkpoints&lt;/a&gt;] and [&lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;sglang&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/01/30] üî• &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; is out! With additional scaling to LLaVA-1.5, LLaVA-NeXT-34B outperforms Gemini Pro on some benchmarks. It can now process 4x more pixels and perform more tasks/applications than before. Check out the &lt;a href=&#34;https://llava-vl.github.io/blog/2024-01-30-llava-next/&#34;&gt;blog post&lt;/a&gt;, and explore the &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;! Models are available in &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;. Training/eval data and scripts coming soon.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;More&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024/03/10] üî• Releasing &lt;strong&gt;LMMs-Eval&lt;/strong&gt;, a highly efficient evaluation pipeline we used when developing LLaVA-NeXT. It supports the evaluation of LMMs on dozens of public datasets and allows new dataset onboarding, making the dev of new LMMs much faster. [&lt;a href=&#34;https://lmms-lab.github.io/lmms-eval-blog/lmms-eval-0.1/&#34;&gt;Blog&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EvolvingLMMs-Lab/lmms-eval&#34;&gt;Codebase&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/11/10] &lt;a href=&#34;https://llava-vl.github.io/llava-plus/&#34;&gt;LLaVA-Plus&lt;/a&gt; is released: Learning to Use Tools for Creating Multimodal Agents, with LLaVA-Plus (LLaVA that Plug and Learn to Use Skills). [&lt;a href=&#34;https://llava-vl.github.io/llava-plus/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://llavaplus.ngrok.io/&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-Plus-Codebase&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2311.05437&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/11/02] &lt;a href=&#34;https://llava-vl.github.io/llava-interactive/&#34;&gt;LLaVA-Interactive&lt;/a&gt; is released: Experience the future of human-AI multimodal interaction with an all-in-one demo for Image Chat, Segmentation, Generation and Editing. [&lt;a href=&#34;https://llava-vl.github.io/llava-interactive/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://llavainteractive.ngrok.io/&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-Interactive-Demo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2311.00571&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/26] üî• LLaVA-1.5 with LoRA achieves comparable performance as full-model finetuning, with a reduced GPU RAM requirement (&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md#llava-v15&#34;&gt;ckpts&lt;/a&gt;, &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#train&#34;&gt;script&lt;/a&gt;). We also provide a &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Finetune_Custom_Data.md&#34;&gt;doc&lt;/a&gt; on how to finetune LLaVA-1.5 on your own dataset with LoRA.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/12] Check out the Korean LLaVA (Ko-LLaVA), created by ETRI, who has generously supported our research! [&lt;a href=&#34;https://huggingface.co/spaces/etri-vilab/Ko-LLaVA&#34;&gt;ü§ó Demo&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/05] üî• LLaVA-1.5 is out! Achieving SoTA on 11 benchmarks, with just simple modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods like Qwen-VL-Chat that use billion-scale data. Check out the &lt;a href=&#34;https://arxiv.org/abs/2310.03744&#34;&gt;technical report&lt;/a&gt;, and explore the &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;! Models are available in &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;. The training data and scripts of LLaVA-1.5 are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#train&#34;&gt;here&lt;/a&gt;, and evaluation scripts are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/09/26] LLaVA is improved with reinforcement learning from human feedback (RLHF) to improve fact grounding and reduce hallucination. Check out the new SFT and RLHF checkpoints at project &lt;a href=&#34;https://llava-rlhf.github.io/&#34;&gt;[LLavA-RLHF]&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/09/22] &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;LLaVA&lt;/a&gt; is accepted by NeurIPS 2023 as &lt;strong&gt;oral presentation&lt;/strong&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2306.00890&#34;&gt;LLaVA-Med&lt;/a&gt; is accepted by NeurIPS 2023 Datasets and Benchmarks Track as &lt;strong&gt;spotlight presentation&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/11/06] Support &lt;strong&gt;Intel&lt;/strong&gt; dGPU and CPU platforms. &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/tree/intel/docs/intel&#34;&gt;More details here.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/12] LLaVA is now supported in &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/pull/3436&#34;&gt;llama.cpp&lt;/a&gt; with 4-bit / 5-bit quantization support!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/11] The training data and scripts of LLaVA-1.5 are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#train&#34;&gt;here&lt;/a&gt;, and evaluation scripts are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/10] &lt;a href=&#34;https://blog.roboflow.com/first-impressions-with-llava-1-5/&#34;&gt;Roboflow Deep Dive&lt;/a&gt;: First Impressions with LLaVA-1.5.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/09/20] We summarize our empirical study of training 33B and 65B LLaVA models in a &lt;a href=&#34;https://arxiv.org/abs/2309.09958&#34;&gt;note&lt;/a&gt;. Further, if you are interested in the comprehensive review, evolution and trend of multimodal foundation models, please check out our recent survey paper &lt;a href=&#34;https://arxiv.org/abs/2309.10020&#34;&gt;``Multimodal Foundation Models: From Specialists to General-Purpose Assistants&#39;&#39;.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings/raw/main/images/mfm_evolution.jpeg?raw=true&#34; width=&#34;50%/&#34;&gt; &lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;[2023/07/19] üî• We release a major upgrade, including support for LLaMA-2, LoRA training, 4-/8-bit inference, higher resolution (336x336), and a lot more. We release &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_Bench.md&#34;&gt;LLaVA Bench&lt;/a&gt; for benchmarking open-ended visual chat with results from Bard and Bing-Chat. We also support and verify training with RTX 3090 and RTX A6000. Check out &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_from_LLaMA2.md&#34;&gt;LLaVA-from-LLaMA-2&lt;/a&gt;, and our &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;model zoo&lt;/a&gt;!&lt;/li&gt; &#xA;  &lt;li&gt;[2023/06/26] &lt;a href=&#34;https://vlp-tutorial.github.io/&#34;&gt;CVPR 2023 Tutorial&lt;/a&gt; on &lt;strong&gt;Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4&lt;/strong&gt;! Please check out [&lt;a href=&#34;https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf&#34;&gt;Slides&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2306.14895&#34;&gt;Notes&lt;/a&gt;] [&lt;a href=&#34;https://youtu.be/mkI7EPD1vp8&#34;&gt;YouTube&lt;/a&gt;] [&lt;a href=&#34;https://www.bilibili.com/video/BV1Ng4y1T7v3/&#34;&gt;Bilibli&lt;/a&gt;].&lt;/li&gt; &#xA;  &lt;li&gt;[2023/06/11] We released the preview for the most requested feature: DeepSpeed and LoRA support! Please see documentations &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LoRA.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/06/01] We released &lt;strong&gt;LLaVA-Med: Large Language and Vision Assistant for Biomedicine&lt;/strong&gt;, a step towards building biomedical domain large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2306.00890&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://github.com/microsoft/LLaVA-Med&#34;&gt;page&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/05/06] We are releasing &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview&#34;&gt;LLaVA-Lighting-MPT-7B-preview&lt;/a&gt;, based on MPT-7B-Chat! See &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/#LLaVA-MPT-7b&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/05/02] üî• We are releasing LLaVA-Lighting! Train a lite, multimodal GPT-4 with just $40 in 3 hours! See &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/#train-llava-lightning&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/04/27] Thanks to the community effort, LLaVA-13B with 4-bit quantization allows you to run on a GPU with as few as 12GB VRAM! Try it out &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/tree/main/extensions/llava&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/04/17] üî• We released &lt;strong&gt;LLaVA: Large Language and Vision Assistant&lt;/strong&gt;. We propose visual instruction tuning, towards building large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;!-- &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;&lt;img src=&#34;assets/demo.gif&#34; width=&#34;70%&#34;&gt;&lt;/a&gt; --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage and License Notices&lt;/strong&gt;: This project utilizes certain datasets and checkpoints that are subject to their respective original licenses. Users must comply with all terms and conditions of these original licenses, including but not limited to the &lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;OpenAI Terms of Use&lt;/a&gt; for the dataset and the specific licenses for base language models for checkpoints trained using the dataset (e.g. &lt;a href=&#34;https://ai.meta.com/llama/license/&#34;&gt;Llama-1/2 community license&lt;/a&gt; for LLaMA-2 and Vicuna-v1.5, &lt;a href=&#34;https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/blob/main/LICENSE&#34;&gt;Tongyi Qianwen RESEARCH LICENSE AGREEMENT&lt;/a&gt; and &lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;Llama-3 Research License&lt;/a&gt;). This project does not impose any additional constraints beyond those stipulated in the original licenses. Furthermore, users are reminded to ensure that their use of the dataset and checkpoints is in compliance with all applicable laws and regulations.&lt;/p&gt; &#xA;&lt;h2&gt;Models &amp;amp; Scripts&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;h4&gt;1. &lt;strong&gt;Clone this repository and navigate to the LLaVA folder:&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/LLaVA-VL/LLaVA-NeXT&#xA;cd LLaVA-NeXT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. &lt;strong&gt;Install the inference package:&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n llava python=3.10 -y&#xA;conda activate llava&#xA;pip install --upgrade pip  # Enable PEP 660 support.&#xA;pip install -e &#34;.[train]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Project Navigation&lt;/h3&gt; &#xA;&lt;p&gt;Please checkout the following page for more inference &amp;amp; evaluation details.&lt;/p&gt; &#xA;&lt;h4&gt;- &lt;strong&gt;LLaVA-OneVision: Easy Task Transfer&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/%5B./docs/LLaVA-NeXT.md%5D(https://github.com/LLaVA-VL/LLaVA-NeXT/raw/main/docs/LLaVA_OneVision.md)&#34;&gt;LLaVA-OneVision&lt;/a&gt;: for demo inference. The evaluation code is in &lt;a href=&#34;https://github.com/EvolvingLMMs-Lab/lmms-eval&#34;&gt;lmms-eval&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;- &lt;strong&gt;LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT.md&#34;&gt;LLaVA-NeXT-Image&lt;/a&gt;: for image demo inference and evaluation of stronger LMMs using &lt;a href=&#34;https://github.com/EvolvingLMMs-Lab/lmms-eval&#34;&gt;lmms-eval&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;- LLaVA-NeXT: A Strong Zero-shot Video Understanding Model&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Video.md&#34;&gt;LLaVA-NeXT-Video&lt;/a&gt;: for video inference and evaluation scripts. We recommend to use &lt;a href=&#34;https://lmms-lab.github.io/posts/lmms-eval-0.2/&#34;&gt;LMMs-video&lt;/a&gt; for evaluation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;- LLaVA-NeXT: Tackling Multi-image, Video, and 3D in Large Multimodal Models&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/main/docs/LLaVA-NeXT-Interleave.md&#34;&gt;LLaVA-NeXT-Interleave&lt;/a&gt;: for multi-image demo and evaluation scripts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;SGLang for SpeedUp Inference and Deployment&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;SGLang&lt;/a&gt; to speed up inference and deployment of LLaVA-NeXT. You could make LLaVA-NeXT as a backend API service with SGLang.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prepare Environment&lt;/strong&gt;: Following the instruction in the &lt;a href=&#34;https://github.com/sgl-project/sglang?tab=readme-ov-file#install&#34;&gt;sglang&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;LLaVA-NeXT/OneVision&lt;/h3&gt; &#xA;&lt;p&gt;Checkout the HTTP Post/Get and SRT usage at &lt;a href=&#34;https://github.com/sgl-project/sglang/tree/main/examples/runtime/llava_onevision&#34;&gt;sglang/examples/runtime/llava_onevision&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;LLaVA-NeXT (Video)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Launch and Run on (K) Nodes&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go to sglang project &lt;pre&gt;&lt;code&gt;cd PATH_TO/sglang&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;First node: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash examples/usage/llava_video/srt_example_llava_v.sh K 0 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO&#xA;(e.g. bash examples/usage/llava_video/srt_example_llava_v.sh K 0 examples/usage/llava_video/videos/Q98Z4OTh8RwmDonc.mp4 lmms-lab/LLaVA-NeXT-Video-7B-DPO 16)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Second node: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash examples/usage/llava_video/srt_example_llava_v.sh K 1 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;The K node: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash examples/usage/llava_video/srt_example_llava_v.sh K K-1 YOUR_VIDEO_PATH YOUR_MODEL_PATH FRAMES_PER_VIDEO&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find it useful for your research and applications, please cite related papers/blogs using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{li2024llava,&#xA;  title={LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models},&#xA;  author={Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},&#xA;  journal={arXiv preprint arXiv:2407.07895},&#xA;  year={2024}&#xA;}&#xA;&#xA;@misc{li2024llavanext-ablations,&#xA;&#x9;title={LLaVA-NeXT: What Else Influences Visual Instruction Tuning Beyond Data?},&#xA;&#x9;url={https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/},&#xA;&#x9;author={Li, Bo and Zhang, Hao and Zhang, Kaichen and Guo, Dong and Zhang, Yuanhan and Zhang, Renrui and Li, Feng and Liu, Ziwei and Li, Chunyuan},&#xA;&#x9;month={May},&#xA;&#x9;year={2024}&#xA;}&#xA;&#xA;@misc{li2024llavanext-strong,&#xA;    title={LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild},&#xA;    url={https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/},&#xA;    author={Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan},&#xA;    month={May},&#xA;    year={2024}&#xA;}&#xA;&#xA;@misc{zhang2024llavanext-video,&#xA;  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},&#xA;  url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},&#xA;  author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},&#xA;  month={April},&#xA;  year={2024}&#xA;}&#xA;&#xA;@misc{liu2024llavanext,&#xA;    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},&#xA;    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},&#xA;    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},&#xA;    month={January},&#xA;    year={2024}&#xA;}&#xA;&#xA;@misc{liu2023improvedllava,&#xA;      title={Improved Baselines with Visual Instruction Tuning}, &#xA;      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},&#xA;      publisher={arXiv:2310.03744},&#xA;      year={2023},&#xA;}&#xA;&#xA;@misc{liu2023llava,&#xA;      title={Visual Instruction Tuning}, &#xA;      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},&#xA;      publisher={NeurIPS},&#xA;      year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;: the codebase we built upon, and our base model Vicuna-13B that has the amazing language capabilities!&lt;/li&gt; &#xA; &lt;li&gt;The LLaVA-NeXT project is currently maintained by the team along with our contributors (listed alphabetically by the first names): &lt;a href=&#34;https://brianboli.com/&#34;&gt;Bo Li&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/dongguoset/&#34;&gt;Dong Guo&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=ybRe9GcAAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate&#34;&gt;Feng Li&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=B8hPxMQAAAAJ&amp;amp;hl=en&#34;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/kaichen-zhang-014b17219/?originalSubdomain=sg&#34;&gt;Kaichen Zhang&lt;/a&gt;, &lt;a href=&#34;https://zrrskywalker.github.io/&#34;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&#34;https://zhangyuanhan-ai.github.io/&#34;&gt;Yuanhan Zhang&lt;/a&gt;, led by &lt;a href=&#34;https://chunyuan.li/&#34;&gt;Chunyuan Li&lt;/a&gt; and with the guidance and help from &lt;a href=&#34;https://hliu.cc/&#34;&gt;Haotian Liu&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;Ôªølmms-eval&lt;/code&gt; framework and its core contributors, including Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, and Kairui Hu, for their support on the evaluation side.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;Instruction Tuning with GPT-4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/LLaVA-Med&#34;&gt;LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Luodian/Otter&#34;&gt;Otter: In-Context Multi-Modal Instruction Tuning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For future project ideas, please check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34;&gt;SEEM: Segment Everything Everywhere All at Once&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt; to detect, segment, and generate anything by marrying &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment-Anything&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ValdikSS/GoodbyeDPI</title>
    <updated>2024-09-01T01:46:45Z</updated>
    <id>tag:github.com,2024-09-01:/ValdikSS/GoodbyeDPI</id>
    <link href="https://github.com/ValdikSS/GoodbyeDPI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GoodbyeDPI ‚Äî Deep Packet Inspection circumvention utility (for Windows)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GoodbyeDPI ‚Äî Deep Packet Inspection circumvention utility&lt;/h1&gt; &#xA;&lt;p&gt;This software designed to bypass Deep Packet Inspection systems found in many Internet Service Providers which block access to certain websites.&lt;/p&gt; &#xA;&lt;p&gt;It handles DPI connected using optical splitter or port mirroring (&lt;strong&gt;Passive DPI&lt;/strong&gt;) which do not block any data but just replying faster than requested destination, and &lt;strong&gt;Active DPI&lt;/strong&gt; connected in sequence.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Windows 7, 8, 8.1, 10 or 11&lt;/strong&gt; with administrator privileges required.&lt;/p&gt; &#xA;&lt;h1&gt;Quick start&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;For Russia&lt;/strong&gt;: Download &lt;a href=&#34;https://github.com/ValdikSS/GoodbyeDPI/releases&#34;&gt;latest version from Releases page&lt;/a&gt;, unpack the file and run &lt;strong&gt;1_russia_blacklist_dnsredir.cmd&lt;/strong&gt; script.&lt;/li&gt; &#xA; &lt;li&gt;For other countries: Download &lt;a href=&#34;https://github.com/ValdikSS/GoodbyeDPI/releases&#34;&gt;latest version from Releases page&lt;/a&gt;, unpack the file and run &lt;strong&gt;2_any_country_dnsredir.cmd&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These scripts launch GoodbyeDPI in recommended mode with DNS resolver redirection to Yandex DNS on non-standard port (to prevent DNS poisoning).&lt;br&gt; If it works ‚Äî congratulations! You can use it as-is or configure further.&lt;/p&gt; &#xA;&lt;h1&gt;How to use&lt;/h1&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://github.com/ValdikSS/GoodbyeDPI/releases&#34;&gt;latest version from Releases page&lt;/a&gt; and run.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Usage: goodbyedpi.exe [OPTION...]&#xA; -p          block passive DPI&#xA; -q          block QUIC/HTTP3&#xA; -r          replace Host with hoSt&#xA; -s          remove space between host header and its value&#xA; -m          mix Host header case (test.com -&amp;gt; tEsT.cOm)&#xA; -f &amp;lt;value&amp;gt;  set HTTP fragmentation to value&#xA; -k &amp;lt;value&amp;gt;  enable HTTP persistent (keep-alive) fragmentation and set it to value&#xA; -n          do not wait for first segment ACK when -k is enabled&#xA; -e &amp;lt;value&amp;gt;  set HTTPS fragmentation to value&#xA; -a          additional space between Method and Request-URI (enables -s, may break sites)&#xA; -w          try to find and parse HTTP traffic on all processed ports (not only on port 80)&#xA; --port        &amp;lt;value&amp;gt;    additional TCP port to perform fragmentation on (and HTTP tricks with -w)&#xA; --ip-id       &amp;lt;value&amp;gt;    handle additional IP ID (decimal, drop redirects and TCP RSTs with this ID).&#xA;                          This option can be supplied multiple times.&#xA; --dns-addr    &amp;lt;value&amp;gt;    redirect UDP DNS requests to the supplied IP address (experimental)&#xA; --dns-port    &amp;lt;value&amp;gt;    redirect UDP DNS requests to the supplied port (53 by default)&#xA; --dnsv6-addr  &amp;lt;value&amp;gt;    redirect UDPv6 DNS requests to the supplied IPv6 address (experimental)&#xA; --dnsv6-port  &amp;lt;value&amp;gt;    redirect UDPv6 DNS requests to the supplied port (53 by default)&#xA; --dns-verb               print verbose DNS redirection messages&#xA; --blacklist   &amp;lt;txtfile&amp;gt;  perform circumvention tricks only to host names and subdomains from&#xA;                          supplied text file (HTTP Host/TLS SNI).&#xA;                          This option can be supplied multiple times.&#xA; --allow-no-sni           perform circumvention if TLS SNI can&#39;t be detected with --blacklist enabled.&#xA; --frag-by-sni            if SNI is detected in TLS packet, fragment the packet right before SNI value.&#xA; --set-ttl     &amp;lt;value&amp;gt;    activate Fake Request Mode and send it with supplied TTL value.&#xA;                          DANGEROUS! May break websites in unexpected ways. Use with care (or --blacklist).&#xA; --auto-ttl    [a1-a2-m]  activate Fake Request Mode, automatically detect TTL and decrease&#xA;                          it based on a distance. If the distance is shorter than a2, TTL is decreased&#xA;                          by a2. If it&#39;s longer, (a1; a2) scale is used with the distance as a weight.&#xA;                          If the resulting TTL is more than m(ax), set it to m.&#xA;                          Default (if set): --auto-ttl 1-4-10. Also sets --min-ttl 3.&#xA;                          DANGEROUS! May break websites in unexpected ways. Use with care (or --blacklist).&#xA; --min-ttl     &amp;lt;value&amp;gt;    minimum TTL distance (128/64 - TTL) for which to send Fake Request&#xA;                          in --set-ttl and --auto-ttl modes.&#xA; --wrong-chksum           activate Fake Request Mode and send it with incorrect TCP checksum.&#xA;                          May not work in a VM or with some routers, but is safer than set-ttl.&#xA; --wrong-seq              activate Fake Request Mode and send it with TCP SEQ/ACK in the past.&#xA; --native-frag            fragment (split) the packets by sending them in smaller packets, without&#xA;                          shrinking the Window Size. Works faster (does not slow down the connection)&#xA;                          and better.&#xA; --reverse-frag           fragment (split) the packets just as --native-frag, but send them in the&#xA;                          reversed order. Works with the websites which could not handle segmented&#xA;                          HTTPS TLS ClientHello (because they receive the TCP flow &#34;combined&#34;).&#xA; --max-payload [value]    packets with TCP payload data more than [value] won&#39;t be processed.&#xA;                          Use this option to reduce CPU usage by skipping huge amount of data&#xA;                          (like file transfers) in already established sessions.&#xA;                          May skip some huge HTTP requests from being processed.&#xA;                          Default (if set): --max-payload 1200.&#xA;&#xA;&#xA;LEGACY modesets:&#xA; -1          -p -r -s -f 2 -k 2 -n -e 2 (most compatible mode)&#xA; -2          -p -r -s -f 2 -k 2 -n -e 40 (better speed for HTTPS yet still compatible)&#xA; -3          -p -r -s -e 40 (better speed for HTTP and HTTPS)&#xA; -4          -p -r -s (best speed)&#xA;&#xA;Modern modesets (more stable, more compatible, faster):&#xA; -5          -f 2 -e 2 --auto-ttl --reverse-frag --max-payload&#xA; -6          -f 2 -e 2 --wrong-seq --reverse-frag --max-payload&#xA; -7          -f 2 -e 2 --wrong-chksum --reverse-frag --max-payload&#xA; -8          -f 2 -e 2 --wrong-seq --wrong-chksum --reverse-frag --max-payload&#xA; -9          -f 2 -e 2 --wrong-seq --wrong-chksum --reverse-frag --max-payload -q (this is the default)&#xA;&#xA; Note: combination of --wrong-seq and --wrong-chksum generates two different fake packets.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To check if your ISP&#39;s DPI could be circumvented, first make sure that your provider does not poison DNS answers by enabling &#34;Secure DNS (DNS over HTTPS)&#34; option in your browser.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chrome&lt;/strong&gt;: Settings ‚Üí &lt;a href=&#34;chrome://settings/security&#34;&gt;Privacy and security&lt;/a&gt; ‚Üí Use secure DNS ‚Üí With: NextDNS&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Firefox&lt;/strong&gt;: &lt;a href=&#34;about:preferences&#34;&gt;Settings&lt;/a&gt; ‚Üí Network Settings ‚Üí Enable DNS over HTTPS ‚Üí Use provider: NextDNS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then run the &lt;code&gt;goodbyedpi.exe&lt;/code&gt; executable without any options. If it works ‚Äî congratulations! You can use it as-is or configure further, for example by using &lt;code&gt;--blacklist&lt;/code&gt; option if the list of blocked websites is known and available for your country.&lt;/p&gt; &#xA;&lt;p&gt;If your provider intercepts DNS requests, you may want to use &lt;code&gt;--dns-addr&lt;/code&gt; option to a public DNS resolver running on non-standard port (such as Yandex DNS &lt;code&gt;77.88.8.8:1253&lt;/code&gt;) or configure DNS over HTTPS/TLS using third-party applications.&lt;/p&gt; &#xA;&lt;p&gt;Check the .cmd scripts and modify it according to your preference and network conditions.&lt;/p&gt; &#xA;&lt;h1&gt;How does it work&lt;/h1&gt; &#xA;&lt;h3&gt;Passive DPI&lt;/h3&gt; &#xA;&lt;p&gt;Most Passive DPI send HTTP 302 Redirect if you try to access blocked website over HTTP and TCP Reset in case of HTTPS, faster than destination website. Packets sent by DPI usually have IP Identification field equal to &lt;code&gt;0x0000&lt;/code&gt; or &lt;code&gt;0x0001&lt;/code&gt;, as seen with Russian providers. These packets, if they redirect you to another website (censorship page), are blocked by GoodbyeDPI.&lt;/p&gt; &#xA;&lt;h3&gt;Active DPI&lt;/h3&gt; &#xA;&lt;p&gt;Active DPI is more tricky to fool. Currently the software uses 7 methods to circumvent Active DPI:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TCP-level fragmentation for first data packet&lt;/li&gt; &#xA; &lt;li&gt;TCP-level fragmentation for persistent (keep-alive) HTTP sessions&lt;/li&gt; &#xA; &lt;li&gt;Replacing &lt;code&gt;Host&lt;/code&gt; header with &lt;code&gt;hoSt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Removing space between header name and value in &lt;code&gt;Host&lt;/code&gt; header&lt;/li&gt; &#xA; &lt;li&gt;Adding additional space between HTTP Method (GET, POST etc) and URI&lt;/li&gt; &#xA; &lt;li&gt;Mixing case of Host header value&lt;/li&gt; &#xA; &lt;li&gt;Sending fake HTTP/HTTPS packets with low Time-To-Live value, incorrect checksum or incorrect TCP Sequence/Acknowledgement numbers to fool DPI and prevent delivering them to the destination&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These methods should not break any website as they&#39;re fully compatible with TCP and HTTP standards, yet it&#39;s sufficient to prevent DPI data classification and to circumvent censorship. Additional space may break some websites, although it&#39;s acceptable by HTTP/1.1 specification (see 19.3 Tolerant Applications).&lt;/p&gt; &#xA;&lt;p&gt;The program loads WinDivert driver which uses Windows Filtering Platform to set filters and redirect packets to the userspace. It&#39;s running as long as console window is visible and terminates when you close the window.&lt;/p&gt; &#xA;&lt;h1&gt;How to build from source&lt;/h1&gt; &#xA;&lt;p&gt;This project can be build using &lt;strong&gt;GNU Make&lt;/strong&gt; and &lt;a href=&#34;https://mingw-w64.org&#34;&gt;&lt;strong&gt;mingw&lt;/strong&gt;&lt;/a&gt;. The only dependency is &lt;a href=&#34;https://github.com/basil00/Divert&#34;&gt;WinDivert&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To build x86 exe run:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;make CPREFIX=i686-w64-mingw32- WINDIVERTHEADERS=/path/to/windivert/include WINDIVERTLIBS=/path/to/windivert/x86&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;And for x86_64:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;make CPREFIX=x86_64-w64-mingw32- BIT64=1 WINDIVERTHEADERS=/path/to/windivert/include WINDIVERTLIBS=/path/to/windivert/amd64&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;How to install as Windows Service&lt;/h1&gt; &#xA;&lt;p&gt;Check examples in &lt;code&gt;service_install_russia_blacklist.cmd&lt;/code&gt;, &lt;code&gt;service_install_russia_blacklist_dnsredir.cmd&lt;/code&gt; and &lt;code&gt;service_remove.cmd&lt;/code&gt; scripts.&lt;/p&gt; &#xA;&lt;p&gt;Modify them according to your own needs.&lt;/p&gt; &#xA;&lt;h1&gt;Known issues&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Horribly outdated Windows 7 installations are not able to load WinDivert driver due to missing support for SHA256 digital signatures. Install KB3033929 &lt;a href=&#34;https://www.microsoft.com/en-us/download/details.aspx?id=46078&#34;&gt;x86&lt;/a&gt;/&lt;a href=&#34;https://www.microsoft.com/en-us/download/details.aspx?id=46148&#34;&gt;x64&lt;/a&gt;, or better, update the whole system using Windows Update.&lt;/li&gt; &#xA; &lt;li&gt;Intel/Qualcomm Killer network cards: &lt;code&gt;Advanced Stream Detect&lt;/code&gt; in Killer Control Center is incompabitle with GoodbyeDPI, &lt;a href=&#34;https://github.com/ValdikSS/GoodbyeDPI/issues/541#issuecomment-2296038239&#34;&gt;disable it&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;del&gt;Some SSL/TLS stacks unable to process fragmented ClientHello packets, and HTTPS websites won&#39;t open. Bug: &lt;a href=&#34;https://github.com/ValdikSS/GoodbyeDPI/issues/4&#34;&gt;#4&lt;/a&gt;, &lt;a href=&#34;https://github.com/ValdikSS/GoodbyeDPI/issues/64&#34;&gt;#64&lt;/a&gt;.&lt;/del&gt; Fragmentation issues are fixed in v0.1.7.&lt;/li&gt; &#xA; &lt;li&gt;&lt;del&gt;ESET Antivirus is incompatible with WinDivert driver &lt;a href=&#34;https://github.com/ValdikSS/GoodbyeDPI/issues/91&#34;&gt;#91&lt;/a&gt;. This is most probably antivirus bug, not WinDivert.&lt;/del&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Similar projects&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/bol-van/zapret&#34;&gt;zapret&lt;/a&gt;&lt;/strong&gt; by @bol-van (for MacOS, Linux and Windows)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/SadeghHayeri/GreenTunnel&#34;&gt;Green Tunnel&lt;/a&gt;&lt;/strong&gt; by @SadeghHayeri (for MacOS, Linux and Windows)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/nomoresat/DPITunnel-cli&#34;&gt;DPI Tunnel CLI&lt;/a&gt;&lt;/strong&gt; by @zhenyolka (for Linux and routers)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/nomoresat/DPITunnel-android&#34;&gt;DPI Tunnel for Android&lt;/a&gt;&lt;/strong&gt; by @zhenyolka (for Android)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/krlvm/PowerTunnel&#34;&gt;PowerTunnel&lt;/a&gt;&lt;/strong&gt; by @krlvm (for Windows, MacOS and Linux)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/krlvm/PowerTunnel-Android&#34;&gt;PowerTunnel for Android&lt;/a&gt;&lt;/strong&gt; by @krlvm (for Android)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/xvzc/SpoofDPI&#34;&gt;SpoofDPI&lt;/a&gt;&lt;/strong&gt; by @xvzc (for macOS and Linux)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/macronut/ghostcp&#34;&gt;GhosTCP&lt;/a&gt;&lt;/strong&gt; by @macronut (for Windows)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/hufrea/byedpi&#34;&gt;ByeDPI&lt;/a&gt;&lt;/strong&gt; for Linux/Windows + &lt;strong&gt;&lt;a href=&#34;https://github.com/dovecoteescapee/ByeDPIAndroid/&#34;&gt;ByeDPIAndroid&lt;/a&gt;&lt;/strong&gt; for Android (no root)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/Waujito/youtubeUnblock/&#34;&gt;youtubeUnblock&lt;/a&gt;&lt;/strong&gt; by @Waujito (for OpenWRT/Entware routers and Linux)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Kudos&lt;/h1&gt; &#xA;&lt;p&gt;Thanks @basil00 for &lt;a href=&#34;https://github.com/basil00/Divert&#34;&gt;WinDivert&lt;/a&gt;. That&#39;s the main part of this program.&lt;/p&gt; &#xA;&lt;p&gt;Thanks for every &lt;a href=&#34;https://github.com/ValdikSS/blockcheck&#34;&gt;BlockCheck&lt;/a&gt; contributor. It would be impossible to understand DPI behaviour without this utility.&lt;/p&gt;</summary>
  </entry>
</feed>