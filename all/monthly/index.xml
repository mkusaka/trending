<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-01T02:04:06Z</updated>
  <subtitle>Monthly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>geekan/MetaGPT</title>
    <updated>2023-09-01T02:04:06Z</updated>
    <id>tag:github.com,2023-09-01:/geekan/MetaGPT</id>
    <link href="https://github.com/geekan/MetaGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🌟 The Multi-Agent Framework: Given one line Requirement, return PRD, Design, Tasks, Repo&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MetaGPT: The Multi-Agent Framework&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/MetaGPT-logo.jpeg&#34; alt=&#34;MetaGPT logo: Enable GPT to work in software company, collaborating to tackle more complex tasks.&#34; width=&#34;150px&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;Assign different roles to GPTs to form a collaborative software entity for complex tasks.&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/README_CN.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/文档-中文版-blue.svg&#34; alt=&#34;CN doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/document-English-blue.svg?sanitize=true&#34; alt=&#34;EN doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/README_JA.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ドキュメント-日本語-blue.svg&#34; alt=&#34;JA doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/wCp6Q3fsAk&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/wCp6Q3fsAk?compact=true&amp;amp;style=flat&#34; alt=&#34;Discord Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/ROADMAP.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ROADMAP-路线图-blue&#34; alt=&#34;roadmap&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/MetaGPT-WeChat-Personal.jpeg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/WeChat-微信-blue&#34; alt=&#34;roadmap&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/DeepWisdom2019&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/MetaGPT?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/geekan/MetaGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode&#34; alt=&#34;Open in Dev Containers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codespaces.new/geekan/MetaGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github_Codespace-Open-blue?logo=github&#34; alt=&#34;Open in GitHub Codespaces&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;MetaGPT takes a &lt;strong&gt;one line requirement&lt;/strong&gt; as input and outputs &lt;strong&gt;user stories / competitive analysis / requirements / data structures / APIs / documents, etc.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Internally, MetaGPT includes &lt;strong&gt;product managers / architects / project managers / engineers.&lt;/strong&gt; It provides the entire process of a &lt;strong&gt;software company along with carefully orchestrated SOPs.&lt;/strong&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;code&gt;Code = SOP(Team)&lt;/code&gt; is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/software_company_cd.jpeg&#34; alt=&#34;A software company consists of LLM-based roles&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Software Company Multi-Role Schematic (Gradually Implementing)&lt;/p&gt; &#xA;&lt;h2&gt;Examples (fully generated by GPT-4)&lt;/h2&gt; &#xA;&lt;p&gt;For example, if you type &lt;code&gt;python startup.py &#34;Design a RecSys like Toutiao&#34;&lt;/code&gt;, you would get many outputs, one of them is data &amp;amp; api design&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/workspace/content_rec_sys/resources/data_api_design.png&#34; alt=&#34;Jinri Toutiao Recsys Data &amp;amp; API Design&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;It costs approximately &lt;strong&gt;$0.2&lt;/strong&gt; (in GPT-4 API fees) to generate one example with analysis and design, and around &lt;strong&gt;$2.0&lt;/strong&gt; for a full project.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Traditional Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Step 1: Ensure that NPM is installed on your system. Then install mermaid-js.&#xA;npm --version&#xA;sudo npm install -g @mermaid-js/mermaid-cli&#xA;&#xA;# Step 2: Ensure that Python 3.9+ is installed on your system. You can check this by using:&#xA;python --version&#xA;&#xA;# Step 3: Clone the repository to your local machine, and install it.&#xA;git clone https://github.com/geekan/metagpt&#xA;cd metagpt&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If already have Chrome, Chromium, or MS Edge installed, you can skip downloading Chromium by setting the environment variable &lt;code&gt;PUPPETEER_SKIP_CHROMIUM_DOWNLOAD&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Some people are &lt;a href=&#34;https://github.com/mermaidjs/mermaid.cli/issues/15&#34;&gt;having issues&lt;/a&gt; installing this tool globally. Installing it locally is an alternative solution,&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install @mermaid-js/mermaid-cli&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;don&#39;t forget to the configuration for mmdc in config.yml&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;PUPPETEER_CONFIG: &#34;./config/puppeteer-config.json&#34;&#xA;MMDC: &#34;./node_modules/.bin/mmdc&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;if &lt;code&gt;python setup.py install&lt;/code&gt; fails with error &lt;code&gt;[Errno 13] Permission denied: &#39;/usr/local/lib/python3.11/dist-packages/test-easy-install-13129.write-test&#39;&lt;/code&gt;, try instead running &lt;code&gt;python setup.py install --user&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation by Docker&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Step 1: Download metagpt official image and prepare config.yaml&#xA;docker pull metagpt/metagpt:v0.3.1&#xA;mkdir -p /opt/metagpt/{config,workspace}&#xA;docker run --rm metagpt/metagpt:v0.3.1 cat /app/metagpt/config/config.yaml &amp;gt; /opt/metagpt/config/key.yaml&#xA;vim /opt/metagpt/config/key.yaml # Change the config&#xA;&#xA;# Step 2: Run metagpt demo with container&#xA;docker run --rm \&#xA;    --privileged \&#xA;    -v /opt/metagpt/config/key.yaml:/app/metagpt/config/key.yaml \&#xA;    -v /opt/metagpt/workspace:/app/metagpt/workspace \&#xA;    metagpt/metagpt:v0.3.1 \&#xA;    python startup.py &#34;Write a cli snake game&#34;&#xA;&#xA;# You can also start a container and execute commands in it&#xA;docker run --name metagpt -d \&#xA;    --privileged \&#xA;    -v /opt/metagpt/config/key.yaml:/app/metagpt/config/key.yaml \&#xA;    -v /opt/metagpt/workspace:/app/metagpt/workspace \&#xA;    metagpt/metagpt:v0.3.1&#xA;&#xA;docker exec -it metagpt /bin/bash&#xA;$ python startup.py &#34;Write a cli snake game&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The command &lt;code&gt;docker run ...&lt;/code&gt; do the following things:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run in privileged mode to have permission to run the browser&lt;/li&gt; &#xA; &lt;li&gt;Map host directory &lt;code&gt;/opt/metagpt/config&lt;/code&gt; to container directory &lt;code&gt;/app/metagpt/config&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Map host directory &lt;code&gt;/opt/metagpt/workspace&lt;/code&gt; to container directory &lt;code&gt;/app/metagpt/workspace&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Execute the demo command &lt;code&gt;python startup.py &#34;Write a cli snake game&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Build image by yourself&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# You can also build metagpt image by yourself.&#xA;git clone https://github.com/geekan/MetaGPT.git&#xA;cd MetaGPT &amp;amp;&amp;amp; docker build -t metagpt:custom .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Configure your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in any of &lt;code&gt;config/key.yaml / config/config.yaml / env&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Priority order: &lt;code&gt;config/key.yaml &amp;gt; config/config.yaml &amp;gt; env&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Copy the configuration file and make the necessary modifications.&#xA;cp config/config.yaml config/key.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Variable Name&lt;/th&gt; &#xA;   &lt;th&gt;config/key.yaml&lt;/th&gt; &#xA;   &lt;th&gt;env&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPENAI_API_KEY # Replace with your own key&lt;/td&gt; &#xA;   &lt;td&gt;OPENAI_API_KEY: &#34;sk-...&#34;&lt;/td&gt; &#xA;   &lt;td&gt;export OPENAI_API_KEY=&#34;sk-...&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPENAI_API_BASE # Optional&lt;/td&gt; &#xA;   &lt;td&gt;OPENAI_API_BASE: &#34;https://&amp;lt;YOUR_SITE&amp;gt;/v1&#34;&lt;/td&gt; &#xA;   &lt;td&gt;export OPENAI_API_BASE=&#34;https://&amp;lt;YOUR_SITE&amp;gt;/v1&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Tutorial: Initiating a startup&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python startup.py &#34;Write a cli snake game&#34;&#xA;# Use code review will cost more money, but will opt for better code quality.&#xA;python startup.py &#34;Write a cli snake game&#34; --code_review True &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running the script, you can find your new project in the &lt;code&gt;workspace/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;Preference of Platform or Tool&lt;/h3&gt; &#xA;&lt;p&gt;You can tell which platform or tool you want to use when stating your requirements.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python startup.py &#34;Write a cli snake game based on pygame&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;NAME&#xA;    startup.py - We are a software startup comprised of AI. By investing in us, you are empowering a future filled with limitless possibilities.&#xA;&#xA;SYNOPSIS&#xA;    startup.py IDEA &amp;lt;flags&amp;gt;&#xA;&#xA;DESCRIPTION&#xA;    We are a software startup comprised of AI. By investing in us, you are empowering a future filled with limitless possibilities.&#xA;&#xA;POSITIONAL ARGUMENTS&#xA;    IDEA&#xA;        Type: str&#xA;        Your innovative idea, such as &#34;Creating a snake game.&#34;&#xA;&#xA;FLAGS&#xA;    --investment=INVESTMENT&#xA;        Type: float&#xA;        Default: 3.0&#xA;        As an investor, you have the opportunity to contribute a certain dollar amount to this AI company.&#xA;    --n_round=N_ROUND&#xA;        Type: int&#xA;        Default: 5&#xA;&#xA;NOTES&#xA;    You can also use flags syntax for POSITIONAL ARGUMENTS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Code walkthrough&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from metagpt.software_company import SoftwareCompany&#xA;from metagpt.roles import ProjectManager, ProductManager, Architect, Engineer&#xA;&#xA;async def startup(idea: str, investment: float = 3.0, n_round: int = 5):&#xA;    &#34;&#34;&#34;Run a startup. Be a boss.&#34;&#34;&#34;&#xA;    company = SoftwareCompany()&#xA;    company.hire([ProductManager(), Architect(), ProjectManager(), Engineer()])&#xA;    company.invest(investment)&#xA;    company.start_project(idea)&#xA;    await company.run(n_round=n_round)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can check &lt;code&gt;examples&lt;/code&gt; for more details on single role (with knowledge base) and LLM only examples.&lt;/p&gt; &#xA;&lt;h2&gt;QuickStart&lt;/h2&gt; &#xA;&lt;p&gt;It is difficult to install and configure the local environment for some users. The following tutorials will allow you to quickly experience the charm of MetaGPT.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://deepwisdom.feishu.cn/wiki/CyY9wdJc4iNqArku3Lncl4v8n2b&#34;&gt;MetaGPT quickstart&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;For now, cite the &lt;a href=&#34;https://arxiv.org/abs/2308.00352&#34;&gt;Arxiv paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{hong2023metagpt,&#xA;      title={MetaGPT: Meta Programming for Multi-Agent Collaborative Framework}, &#xA;      author={Sirui Hong and Xiawu Zheng and Jonathan Chen and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu},&#xA;      year={2023},&#xA;      eprint={2308.00352},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact Information&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Email:&lt;/strong&gt; &lt;a href=&#34;mailto:alexanderwu@fuzhi.ai&#34;&gt;alexanderwu@fuzhi.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GitHub Issues:&lt;/strong&gt; For more technical inquiries, you can also create a new issue in our &lt;a href=&#34;https://github.com/geekan/metagpt/issues&#34;&gt;GitHub repository&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We will respond to all questions within 2-3 business days.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d&#34;&gt;https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Join us&lt;/h2&gt; &#xA;&lt;p&gt;📢 Join Our Discord Channel! &lt;a href=&#34;https://discord.gg/4WdszVjv&#34;&gt;https://discord.gg/4WdszVjv&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Looking forward to seeing you there! 🎉&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ymcui/Chinese-LLaMA-Alpaca-2</title>
    <updated>2023-09-01T02:04:06Z</updated>
    <id>tag:github.com,2023-09-01:/ymcui/Chinese-LLaMA-Alpaca-2</id>
    <link href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;中文LLaMA-2 &amp; Alpaca-2大模型二期项目 + 16K超长上下文模型 (Chinese LLaMA-2 &amp; Alpaca-2 LLMs, including 16K long context models)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-2/main/README.md&#34;&gt;&lt;strong&gt;🇨🇳中文&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-2/main/README_EN.md&#34;&gt;&lt;strong&gt;🌐English&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki&#34;&gt;&lt;strong&gt;📖文档/Docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/issues&#34;&gt;&lt;strong&gt;❓提问/Issues&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/discussions&#34;&gt;&lt;strong&gt;💬讨论/Discussions&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;http://llm-arena.ymcui.com/&#34;&gt;&lt;strong&gt;⚔️竞技场/Arena&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-2/main/pics/banner.png&#34; width=&#34;800&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/license/ymcui/Chinese-LLaMA-Alpaca-2.svg?color=blue&amp;amp;style=flat-square&#34;&gt; &lt;img alt=&#34;GitHub release (latest by date)&#34; src=&#34;https://img.shields.io/github/v/release/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt; &lt;img alt=&#34;GitHub top language&#34; src=&#34;https://img.shields.io/github/languages/top/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt; &lt;a href=&#34;https://app.codacy.com/gh/ymcui/Chinese-LLaMA-Alpaca-2/dashboard?utm_source=gh&amp;amp;utm_medium=referral&amp;amp;utm_content=&amp;amp;utm_campaign=Badge_grade&#34;&gt;&lt;img src=&#34;https://app.codacy.com/project/badge/Grade/1710faac5e634acaabfc26b0a778cdde&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;本项目基于Meta发布的可商用大模型&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama-2&lt;/a&gt;开发，是&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;中文LLaMA&amp;amp;Alpaca大模型&lt;/a&gt;的第二期项目，开源了&lt;strong&gt;中文LLaMA-2基座模型和Alpaca-2指令精调大模型&lt;/strong&gt;。这些模型&lt;strong&gt;在原版Llama-2的基础上扩充并优化了中文词表&lt;/strong&gt;，使用了大规模中文数据进行增量预训练，进一步提升了中文基础语义和指令理解能力，相比一代相关模型获得了显著性能提升。相关模型&lt;strong&gt;支持FlashAttention-2训练&lt;/strong&gt;。标准版模型支持4K上下文长度，&lt;strong&gt;长上下文版模型支持16K上下文长度&lt;/strong&gt;，并可通过NTK方法最高扩展至24K+上下文长度。&lt;/p&gt; &#xA;&lt;h4&gt;本项目主要内容&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🚀 针对Llama-2模型扩充了&lt;strong&gt;新版中文词表&lt;/strong&gt;，开源了中文LLaMA-2和Alpaca-2大模型&lt;/li&gt; &#xA; &lt;li&gt;🚀 开源了预训练脚本、指令精调脚本，用户可根据需要进一步训练模型&lt;/li&gt; &#xA; &lt;li&gt;🚀 使用个人电脑的CPU/GPU快速在本地进行大模型量化和部署体验&lt;/li&gt; &#xA; &lt;li&gt;🚀 支持&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;🤗transformers&lt;/a&gt;, &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;, &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui&#34;&gt;text-generation-webui&lt;/a&gt;, &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;, &lt;a href=&#34;https://github.com/imartinez/privateGPT&#34;&gt;privateGPT&lt;/a&gt;, &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;等LLaMA生态&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;已开源的模型&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;基座模型：Chinese-LLaMA-2-7B, Chinese-LLaMA-2-13B&lt;/li&gt; &#xA; &lt;li&gt;聊天模型：Chinese-Alpaca-2-7B, Chinese-Alpaca-2-13B&lt;/li&gt; &#xA; &lt;li&gt;长上下文模型：Chinese-LLaMA-2-7B-16K, Chinese-LLaMA-2-13B-16K&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-2/main/pics/screencast.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;中文LLaMA&amp;amp;Alpaca大模型&lt;/a&gt; | &lt;a href=&#34;https://github.com/airaria/Visual-Chinese-LLaMA-Alpaca&#34;&gt;多模态中文LLaMA&amp;amp;Alpaca大模型&lt;/a&gt; | &lt;a href=&#34;https://github.com/iflytek/VLE&#34;&gt;多模态VLE&lt;/a&gt; | &lt;a href=&#34;https://github.com/iflytek/MiniRBT&#34;&gt;中文MiniRBT&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/LERT&#34;&gt;中文LERT&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/PERT&#34;&gt;中英文PERT&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/MacBERT&#34;&gt;中文MacBERT&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/Chinese-ELECTRA&#34;&gt;中文ELECTRA&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/Chinese-XLNet&#34;&gt;中文XLNet&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/Chinese-BERT-wwm&#34;&gt;中文BERT&lt;/a&gt; | &lt;a href=&#34;https://github.com/airaria/TextBrewer&#34;&gt;知识蒸馏工具TextBrewer&lt;/a&gt; | &lt;a href=&#34;https://github.com/airaria/TextPruner&#34;&gt;模型裁剪工具TextPruner&lt;/a&gt; | &lt;a href=&#34;https://github.com/airaria/GRAIN&#34;&gt;蒸馏裁剪一体化GRAIN&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;新闻&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/08/25] 发布长上下文模型Chinese-LLaMA-2-7B-16K和Chinese-LLaMA-2-13B-16K，支持16K上下文，并可通过NTK方法进一步扩展至24K+。详情查看&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v3.0&#34;&gt;📚 v3.0版本发布日志&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2023/08/14] 发布Chinese-LLaMA-2-13B和Chinese-Alpaca-2-13B，添加text-generation-webui/LangChain/privateGPT支持，添加CFG Sampling解码方法等。详情查看&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v2.0&#34;&gt;📚 v2.0版本发布日志&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2023/08/02] 添加FlashAttention-2训练支持，基于vLLM的推理加速支持，提供长回复系统提示语模板等。详情查看&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v1.1&#34;&gt;📚 v1.1版本发布日志&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2023/07/31] 正式发布Chinese-LLaMA-2-7B（基座模型），使用120G中文语料增量训练（与一代Plus系列相同）；进一步通过5M条指令数据精调（相比一代略微增加），得到Chinese-Alpaca-2-7B（指令/chat模型）。详情查看&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v1.0&#34;&gt;📚 v1.0版本发布日志&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2023/07/19] 🚀启动&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;中文LLaMA-2、Alpaca-2开源大模型项目&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;内容导引&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;章节&lt;/th&gt; &#xA;   &lt;th&gt;描述&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-2/main/#%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B&#34;&gt;💁🏻‍♂️模型简介&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;简要介绍本项目相关模型的技术特点&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-2/main/#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD&#34;&gt;⏬模型下载&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;中文LLaMA-2、Alpaca-2大模型下载地址&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-2/main/#%E6%8E%A8%E7%90%86%E4%B8%8E%E9%83%A8%E7%BD%B2&#34;&gt;💻推理与部署&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;介绍了如何对模型进行量化并使用个人电脑部署并体验大模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-2/main/#%E7%B3%BB%E7%BB%9F%E6%95%88%E6%9E%9C&#34;&gt;💯系统效果&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;介绍了模型在部分任务上的效果&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-2/main/#%E8%AE%AD%E7%BB%83%E4%B8%8E%E7%B2%BE%E8%B0%83&#34;&gt;📝训练与精调&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;介绍了如何训练和精调中文LLaMA-2、Alpaca-2大模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-2/main/#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98&#34;&gt;❓常见问题&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;一些常见问题的回复&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;模型简介&lt;/h2&gt; &#xA;&lt;p&gt;本项目推出了基于Llama-2的中文LLaMA-2以及Alpaca-2系列模型，相比&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;一期项目&lt;/a&gt;其主要特点如下：&lt;/p&gt; &#xA;&lt;h4&gt;📖 经过优化的中文词表&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;在&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;一期项目&lt;/a&gt;中，我们针对一代LLaMA模型的32K词表扩展了中文字词（LLaMA：49953，Alpaca：49954）&lt;/li&gt; &#xA; &lt;li&gt;在本项目中，我们&lt;strong&gt;重新设计了新词表&lt;/strong&gt;（大小：55296），进一步提升了中文字词的覆盖程度，同时统一了LLaMA/Alpaca的词表，避免了因混用词表带来的问题，以期进一步提升模型对中文文本的编解码效率&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;⚡ 基于FlashAttention-2的高效注意力&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention-2&lt;/a&gt;是高效注意力机制的一种实现，相比其一代技术具有&lt;strong&gt;更快的速度和更优化的显存占用&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;当上下文长度更长时，为了避免显存爆炸式的增长，使用此类高效注意力技术尤为重要&lt;/li&gt; &#xA; &lt;li&gt;本项目的所有模型均使用了FlashAttention-2技术进行训练&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;🚄 基于PI和NTK的超长上下文扩展技术&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;在&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;一期项目&lt;/a&gt;中，我们实现了&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca/pull/743&#34;&gt;基于NTK的上下文扩展技术&lt;/a&gt;，可在不继续训练模型的情况下支持更长的上下文&lt;/li&gt; &#xA; &lt;li&gt;基于&lt;a href=&#34;https://arxiv.org/abs/2306.15595&#34;&gt;位置插值PI&lt;/a&gt;和NTK等方法推出了长上下文版模型，支持16K上下文，并可通过NTK方法最高扩展至24K-32K&lt;/li&gt; &#xA; &lt;li&gt;进一步设计了&lt;strong&gt;方便的自适应经验公式&lt;/strong&gt;，无需针对不同的上下文长度设置NTK超参，降低了使用难度&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;🤖 简化的中英双语系统提示语&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;在&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;一期项目&lt;/a&gt;中，中文Alpaca系列模型使用了&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;的指令模板和系统提示语&lt;/li&gt; &#xA; &lt;li&gt;初步实验发现，Llama-2-Chat系列模型的默认系统提示语未能带来统计显著的性能提升，且其内容过于冗长&lt;/li&gt; &#xA; &lt;li&gt;本项目中的Alpaca-2系列模型简化了系统提示语，同时遵循Llama-2-Chat指令模板，以便更好地适配相关生态&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;模型下载&lt;/h2&gt; &#xA;&lt;h3&gt;模型选择指引&lt;/h3&gt; &#xA;&lt;p&gt;下面是中文LLaMA-2和Alpaca-2模型的基本对比以及建议使用场景。&lt;strong&gt;如需和模型聊天交互，请选择Alpaca而不是LLaMA。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;对比项&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;中文LLaMA-2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;中文Alpaca-2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;模型类型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;基座模型&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;指令/Chat模型（类ChatGPT）&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;已开源大小&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7B、13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7B、13B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;训练类型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Causal-LM (CLM)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令精调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;训练方式&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LoRA + 全量emb/lm-head&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LoRA + 全量emb/lm-head&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;基于什么模型训练&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;原版Llama-2&lt;/a&gt;（非chat版）&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;中文LLaMA-2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;训练语料&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;无标注通用语料（120G纯文本）&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;有标注指令数据（500万条）&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;词表大小&lt;sup&gt;[1]&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55,296&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55,296&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;上下文长度&lt;sup&gt;[2]&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;标准版：4K（12K-18K）&lt;br&gt;长上下文版：16K（24K-32K）&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;标准版：4K（12K-18K）&lt;br&gt;长上下文版：16K（24K-32K）&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;输入模板&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;不需要&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;需要套用特定模板&lt;sup&gt;[3]&lt;/sup&gt;，类似Llama-2-Chat&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;适用场景&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;文本续写：给定上文，让模型生成下文&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令理解：问答、写作、聊天、交互等&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;不适用场景&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令理解 、多轮聊天等&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;文本无限制自由生成&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] [1] &lt;em&gt;本项目一代模型和二代模型的词表不同，请勿混用。二代LLaMA和Alpaca的词表相同。&lt;/em&gt;&lt;br&gt; [2] &lt;em&gt;括号内表示基于NTK上下文扩展支持的最大长度。&lt;/em&gt;&lt;br&gt; [3] &lt;em&gt;Alpaca-2采用了Llama-2-chat系列模板（格式相同，提示语不同），而不是一代Alpaca的模板，请勿混用。&lt;/em&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;完整模型下载&lt;/h3&gt; &#xA;&lt;p&gt;以下是完整版模型，直接下载即可使用，无需其他合并步骤。推荐网络带宽充足的用户。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;模型名称&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;类型&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;大小&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;下载地址&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chinese-LLaMA-2-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.7 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1T3RqEUSmyg6ZuBwMhwSmoQ?pwd=e9qy&#34;&gt;[百度]&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/drive/folders/1YNa5qJ0x59OEOI7tNODxea-1YvMPoH05?usp=share_link&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ziqingyang/chinese-llama-2-13b&#34;&gt;[🤗HF]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chinese-LLaMA-2-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.9 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1E5NI3nlQpx1j8z3eIzbIlg?pwd=n8k3&#34;&gt;[百度]&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/drive/folders/18pp4I-mvQxRA7b8vF9gP-2cH_ocnXVKh?usp=share_link&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ziqingyang/chinese-llama-2-7b&#34;&gt;[🤗HF]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chinese-Alpaca-2-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.7 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1MT_Zlap1OtdYMgoBNTS3dg?pwd=9xja&#34;&gt;[百度]&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/drive/folders/1MTsKlzR61xmbTR4hBWzQas_MOpUZsogN?usp=share_link&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ziqingyang/chinese-alpaca-2-13b&#34;&gt;[🤗HF]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chinese-Alpaca-2-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.9 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1wxx-CdgbMupXVRBcaN4Slw?pwd=kpn9&#34;&gt;[百度]&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/drive/folders/1JsJDVs7tE2y31PBNleBlDPsB7S0ZrY8d?usp=share_link&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ziqingyang/chinese-alpaca-2-7b&#34;&gt;[🤗HF]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;以下是长上下文版模型，&lt;strong&gt;推荐以长文本为主的下游任务使用&lt;/strong&gt;，否则建议使用上述标准版。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;模型名称&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;类型&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;大小&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;下载地址&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chinese-LLaMA-2-13B-16K 🆕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.7 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1XWrh3Ru9x4UI4-XmocVT2w?pwd=f7ik&#34;&gt;[百度]&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/drive/folders/1nii6lF0DgB1u81CnsE4cCK2jD5oq_OW-?usp=share_link&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ziqingyang/chinese-llama-2-13b-16k&#34;&gt;[🤗HF]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chinese-LLaMA-2-7B-16K 🆕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.9 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1ZH7T7KU_up61ugarSIXw2g?pwd=pquq&#34;&gt;[百度]&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/drive/folders/1Zc6jI5bl3myQbQsY79dWJJ8mP_fyf3iF?usp=share_link&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ziqingyang/chinese-llama-2-7b-16k&#34;&gt;[🤗HF]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] 使用长上下文模型推理时，必须按照文档要求进行设置，具体请参考各推理部署工具的&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki&#34;&gt;Wiki&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;LoRA模型下载&lt;/h3&gt; &#xA;&lt;p&gt;以下是LoRA模型（含emb/lm-head），与上述完整模型一一对应。需要注意的是&lt;strong&gt;LoRA模型无法直接使用&lt;/strong&gt;，必须按照教程与重构模型进行合并。推荐网络带宽不足，手头有原版Llama-2且需要轻量下载的用户。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;模型名称&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;类型&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;合并所需基模型&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;大小&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;LoRA下载地址&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chinese-LLaMA-2-LoRA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-13b-hf&#34;&gt;Llama-2-13B-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.5 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1PFKTBn54GjAjzWeQISKruw?pwd=we6s&#34;&gt;[百度]&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/file/d/10Z_k9A9N9D_6RHrMTmbHQRCuI6s1iMb1/view?usp=share_link&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ziqingyang/chinese-llama-2-lora-13b&#34;&gt;[🤗HF]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chinese-LLaMA-2-LoRA-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-hf&#34;&gt;Llama-2-7B-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.1 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1bmgqdyRh9E3a2uqOGyNqiQ?pwd=7kvq&#34;&gt;[百度]&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/file/d/1njJGSU_PRbzjYRNw5RSbC5-4fBOXTVY3/view?usp=share_link&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ziqingyang/chinese-llama-2-lora-7b&#34;&gt;[🤗HF]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chinese-Alpaca-2-LoRA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-13b-hf&#34;&gt;Llama-2-13B-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.5 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1Y5giIXOUUzI4Na6JOcviVA?pwd=tc2j&#34;&gt;[百度]&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/file/d/1z2FIInsYJBTXipgztc-Mv7kkeqscx442/view?usp=share_link&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ziqingyang/chinese-alpaca-2-lora-13b&#34;&gt;[🤗HF]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chinese-Alpaca-2-LoRA-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-hf&#34;&gt;Llama-2-7B-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.1 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1g0olPxkB_rlZ9UUVfOnbcw?pwd=5e7w&#34;&gt;[百度]&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/file/d/1MzJL-ZIzdJW7MIcAiYIDIDJ5dlMi8Kkk/view?usp=share_link&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ziqingyang/chinese-alpaca-2-lora-7b&#34;&gt;[🤗HF]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;以下是长上下文版模型，&lt;strong&gt;推荐以长文本为主的下游任务使用&lt;/strong&gt;，否则建议使用上述标准版。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;模型名称&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;类型&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;合并所需基模型&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;大小&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;LoRA下载地址&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chinese-LLaMA-2-LoRA-13B-16K 🆕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-13b-hf&#34;&gt;Llama-2-13B-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.5 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1VrfOJmhDnXxrXcdnfX00fA?pwd=4t2j&#34;&gt;[百度]&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/file/d/1mSpigmHcN9YX1spa4QN3IPtx43Vfs55H/view?usp=share_link&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ziqingyang/chinese-llama-2-lora-13b-16k&#34;&gt;[🤗HF]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chinese-LLaMA-2-LoRA-7B-16K 🆕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-hf&#34;&gt;Llama-2-7B-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.1 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/14Jnm7QmcDx3XsK_NHZz6Uw?pwd=5b7i&#34;&gt;[百度]&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/file/d/1yUdyQuBMAmxmUEAvGiKbjKuxTYPPI-or/view?usp=sharing&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ziqingyang/chinese-llama-2-lora-7b-16k&#34;&gt;[🤗HF]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] LoRA模型无法单独使用，必须与原版Llama-2进行合并才能转为完整模型。请通过以下方法对模型进行合并。&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/online_conversion_zh&#34;&gt;&lt;strong&gt;在线转换&lt;/strong&gt;&lt;/a&gt;：Colab用户可利用本项目提供的notebook进行在线转换并量化模型&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/manual_conversion_zh&#34;&gt;&lt;strong&gt;手动转换&lt;/strong&gt;&lt;/a&gt;：离线方式转换，生成不同格式的模型，以便进行量化或进一步精调&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;推理与部署&lt;/h2&gt; &#xA;&lt;p&gt;本项目中的相关模型主要支持以下量化、推理和部署方式，具体内容请参考对应教程。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;工具&lt;/th&gt; &#xA;   &lt;th&gt;特点&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CPU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GPU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;量化&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GUI&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;API&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;vLLM&lt;sup&gt;§&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;16K&lt;sup&gt;‡&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;教程&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;&lt;strong&gt;llama.cpp&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;丰富的量化选项和高效本地推理&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/llamacpp_zh&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;&lt;strong&gt;🤗Transformers&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;原生transformers推理接口&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/inference_with_transformers_zh&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1yu0eZ3a66by8Zqm883LLtRQrguBAb9MR?usp=sharing&#34;&gt;&lt;strong&gt;Colab Demo&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;在Colab中启动交互界面&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1yu0eZ3a66by8Zqm883LLtRQrguBAb9MR?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference&#34;&gt;&lt;strong&gt;仿OpenAI API调用&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;仿OpenAI API接口的服务器Demo&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/api_calls_zh&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui&#34;&gt;&lt;strong&gt;text-generation-webui&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;前端Web UI界面的部署方式&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/text-generation-webui_zh&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;&lt;strong&gt;LangChain&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;适合二次开发的大模型应用开源框架&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;sup&gt;†&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/langchain_zh&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/imartinez/privateGPT&#34;&gt;&lt;strong&gt;privateGPT&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;基于LangChain的多文档本地问答框架&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/privategpt_zh&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] &lt;sup&gt;†&lt;/sup&gt; 工具支持该特性，但教程中未实现；详细说明请参考对应官方文档。&lt;br&gt; &lt;sup&gt;‡&lt;/sup&gt; 指是否支持16K长上下文模型（需要第三方库支持自定义RoPE）&lt;br&gt; &lt;sup&gt;§&lt;/sup&gt; vLLM后端不支持16K长上下文模型。&lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;系统效果&lt;/h2&gt; &#xA;&lt;p&gt;为了评测相关模型的效果，本项目分别进行了生成效果评测和客观效果评测（NLU类），从不同角度对大模型进行评估。需要注意的是，综合评估大模型能力仍然是亟待解决的重要课题，单个数据集的结果并不能综合评估模型性能。推荐用户在自己关注的任务上进行测试，选择适配相关任务的模型。&lt;/p&gt; &#xA;&lt;h3&gt;生成效果评测&lt;/h3&gt; &#xA;&lt;p&gt;为了更加直观地了解模型的生成效果，本项目仿照&lt;a href=&#34;https://chat.lmsys.org/?arena&#34;&gt;Fastchat Chatbot Arena&lt;/a&gt;推出了模型在线对战平台，可浏览和评测模型回复质量。对战平台提供了胜率、Elo评分等评测指标，并且可以查看两两模型的对战胜率等结果。题库来自于&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples/f16-p7b-p13b-33b&#34;&gt;一期项目人工制作的200题&lt;/a&gt;，以及在此基础上额外增加的题目。生成回复具有随机性，受解码超参、随机种子等因素影响，因此相关评测并非绝对严谨，结果仅供晾晒参考，欢迎自行体验。部分生成样例请查看&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-2/main/examples&#34;&gt;examples目录&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;⚔️ 模型竞技场：&lt;a href=&#34;http://llm-arena.ymcui.com/&#34;&gt;http://llm-arena.ymcui.com&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;系统&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;对战胜率（无平局） ↓&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Elo评分&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Chinese-Alpaca-2-13B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.37%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1610.34&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese-Alpaca-Pro-33B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.72%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1610.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Chinese-Alpaca-2-7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.35%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1561.48&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese-Alpaca-Pro-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.76%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1583.41&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese-Alpaca-Pro-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.42%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1497.83&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese-Alpaca-Plus-33B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.62%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1439.39&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese-Alpaca-Plus-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.45%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1351.60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese-Alpaca-Plus-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.68%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1345.23&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] 以上结果截至2023年8月24日。最新结果请进入&lt;a href=&#34;http://llm-arena.ymcui.com/&#34;&gt;&lt;strong&gt;⚔️竞技场&lt;/strong&gt;&lt;/a&gt;进行查看。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;客观效果评测：C-Eval&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cevalbenchmark.com&#34;&gt;C-Eval&lt;/a&gt;是一个全面的中文基础模型评估套件，其中验证集包含1.3K个选择题，测试集包含12.3K个选择题，涵盖52个学科，题目类型为选择题。实验结果以“zero-shot / 5-shot”进行呈现。C-Eval推理代码请参考本项目 &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/ceval_zh&#34;&gt;📖GitHub Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;LLaMA Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Valid&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Test&lt;/th&gt; &#xA;   &lt;th&gt;Alpaca Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Valid&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Test&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Chinese-LLaMA-2-13B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.6 / 42.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.0 / 41.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Chinese-Alpaca-2-13B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.3 / 45.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.6 / 44.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Chinese-LLaMA-2-7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.2 / 36.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.3 / 34.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Chinese-Alpaca-2-7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.3 / 42.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.3 / 39.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-Plus-33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.4 / 40.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.7 / 38.3&lt;/td&gt; &#xA;   &lt;td&gt;Chinese-Alpaca-Plus-33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.5 / 46.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.9 / 43.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-Plus-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.3 / 34.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.8 / 33.3&lt;/td&gt; &#xA;   &lt;td&gt;Chinese-Alpaca-Plus-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.3 / 42.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.5 / 39.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-Plus-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.3 / 28.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.9 / 28.4&lt;/td&gt; &#xA;   &lt;td&gt;Chinese-Alpaca-Plus-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.7 / 32.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.4 / 32.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;客观效果评测：CMMLU&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/haonan-li/CMMLU&#34;&gt;CMMLU&lt;/a&gt;是另一个综合性中文评测数据集，专门用于评估语言模型在中文语境下的知识和推理能力，涵盖了从基础学科到高级专业水平的67个主题，共计11.5K个测试样例，题目类型为选择题。CMMLU推理代码请参考本项目 &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/cmmlu_zh&#34;&gt;📖GitHub Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;LLaMA Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Test (0/few-shot)&lt;/th&gt; &#xA;   &lt;th&gt;Alpaca Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Test (0/few-shot)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Chinese-LLaMA-2-13B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.9 / 42.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Chinese-Alpaca-2-13B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.2 / 45.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Chinese-LLaMA-2-7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.9 / 34.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Chinese-Alpaca-2-7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.0 / 41.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-Plus-33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.2 / 38.8&lt;/td&gt; &#xA;   &lt;td&gt;Chinese-Alpaca-Plus-33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.6 / 45.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-Plus-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.6 / 34.0&lt;/td&gt; &#xA;   &lt;td&gt;Chinese-Alpaca-Plus-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.6 / 39.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-Plus-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.4 / 26.3&lt;/td&gt; &#xA;   &lt;td&gt;Chinese-Alpaca-Plus-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.8 / 32.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;长上下文版模型评测&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/THUDM/LongBench&#34;&gt;LongBench&lt;/a&gt;是一个大模型长文本理解能力的评测基准，由6大类、20个不同的任务组成，多数任务的平均长度在5K-15K之间，共包含约4.5K条测试数据。以下是本项目长上下文版模型在该数据集（中文任务）上的评测效果。LongBench推理代码请参考本项目 &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/longbench_zh&#34;&gt;📖GitHub Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;单文档QA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;多文档QA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;摘要&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Few-shot学习&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;代码补全&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;合成任务&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Avg&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Chinese-LLaMA-2-13B-16K&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-2-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Chinese-LLaMA-2-7B-16K&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-2-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;量化效果评测&lt;/h3&gt; &#xA;&lt;p&gt;以Chinese-LLaMA-2-7B为例，对比不同精度下的模型大小、PPL（困惑度）、C-Eval效果，方便用户了解量化精度损失。PPL以4K上下文大小计算，C-Eval汇报的是valid集合上zero-shot和5-shot结果。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;精度&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;模型大小&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PPL&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;C-Eval&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;FP16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.9 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.373&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.2 / 36.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;8-bit量化&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.8 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.476&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.8 / 35.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4-bit量化&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.7 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.132&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.5 / 32.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;特别地，以下是在llama.cpp下不同量化方法的评测数据，供用户参考，速度以ms/tok计，测试设备为M1 Max。具体细节见&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/llamacpp_zh#%E5%85%B3%E4%BA%8E%E9%87%8F%E5%8C%96%E6%96%B9%E6%B3%95%E9%80%89%E6%8B%A9%E5%8F%8A%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6&#34;&gt;📖GitHub Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;llama.cpp&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;F16&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q2_K&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q3_K&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q4_0&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q4_1&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q4_K&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q5_0&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q5_1&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q5_K&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q6_K&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q8_0&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PPL&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;11.107&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.576&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.476&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.576&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.240&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.156&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.213&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.168&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.133&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.129&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Size&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;12.91G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2.41G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3.18G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3.69G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.08G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3.92G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.47G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.86G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.59G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.30G&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.81G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CPU Speed&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;117&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;42&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;51&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;39&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;44&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;43&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;48&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;51&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;50&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;54&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;65&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPU Speed&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;53&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;19&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;21&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;17&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;18&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;x&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;x&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;25&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;26&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;训练与精调&lt;/h2&gt; &#xA;&lt;h4&gt;预训练&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;在原版Llama-2的基础上，利用大规模无标注数据进行增量训练，得到Chinese-LLaMA-2系列基座模型&lt;/li&gt; &#xA; &lt;li&gt;训练数据采用了一期项目中Plus版本模型一致的数据，其总量约120G纯文本文件&lt;/li&gt; &#xA; &lt;li&gt;训练代码参考了🤗transformers中的&lt;a href=&#34;https://github.com/huggingface/transformers/raw/main/examples/pytorch/language-modeling/run_clm.py&#34;&gt;run_clm.py&lt;/a&gt;，使用方法见&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/pt_scripts_zh&#34;&gt;📖预训练脚本Wiki&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;指令精调&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;在Chinese-LLaMA-2的基础上，利用有标注指令数据进行进一步精调，得到Chinese-Alpaca-2系列模型&lt;/li&gt; &#xA; &lt;li&gt;训练数据采用了一期项目中Pro版本模型使用的指令数据，其总量约500万条指令数据（相比一期略增加）&lt;/li&gt; &#xA; &lt;li&gt;训练代码参考了&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;项目中数据集处理的相关部分，使用方法见&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/sft_scripts_zh&#34;&gt;📖指令精调脚本Wiki&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;常见问题&lt;/h2&gt; &#xA;&lt;p&gt;请在提Issue前务必先查看FAQ中是否已存在解决方案。具体问题和解答请参考本项目 &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/faq_zh&#34;&gt;📖GitHub Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;问题1：本项目和一期项目的区别？&#xA;问题2：模型能否商用？&#xA;问题3：接受第三方Pull Request吗？&#xA;问题4：为什么不对模型做全量预训练而是用LoRA？&#xA;问题5：二代模型支不支持某些支持一代LLaMA的工具？&#xA;问题6：Chinese-Alpaca-2是Llama-2-Chat训练得到的吗？&#xA;问题7：为什么24G显存微调Chinese-Alpaca-2-7B会OOM？&#xA;问题8：可以使用16K长上下文版模型替代标准版模型吗？&#xA;问题9：如何解读第三方公开榜单的结果？&#xA;问题10：会出34B或者70B级别的模型吗？&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;引用&lt;/h2&gt; &#xA;&lt;p&gt;如果您使用了本项目的相关资源，请参考引用本项目的技术报告：&lt;a href=&#34;https://arxiv.org/abs/2304.08177&#34;&gt;https://arxiv.org/abs/2304.08177&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{Chinese-LLaMA-Alpaca,&#xA;    title={Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca},&#xA;    author={Cui, Yiming and Yang, Ziqing and Yao, Xin},&#xA;    journal={arXiv preprint arXiv:2304.08177},&#xA;    url={https://arxiv.org/abs/2304.08177},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;致谢&lt;/h2&gt; &#xA;&lt;p&gt;本项目主要基于以下开源项目二次开发，在此对相关项目和研究开发人员表示感谢。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama-2 &lt;em&gt;by Meta&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp &lt;em&gt;by @ggerganov&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention-2 by &lt;em&gt;Dao-AILab&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;同时感谢Chinese-LLaMA-Alpaca（一期项目）的contributor以及&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca#%E8%87%B4%E8%B0%A2&#34;&gt;关联项目和人员&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;免责声明&lt;/h2&gt; &#xA;&lt;p&gt;本项目基于由Meta发布的Llama-2模型进行开发，使用过程中请严格遵守Llama-2的开源许可协议。如果涉及使用第三方代码，请务必遵从相关的开源许可协议。模型生成的内容可能会因为计算方法、随机因素以及量化精度损失等影响其准确性，因此，本项目不对模型输出的准确性提供任何保证，也不会对任何因使用相关资源和输出结果产生的损失承担责任。如果将本项目的相关模型用于商业用途，开发者应遵守当地的法律法规，确保模型输出内容的合规性，本项目不对任何由此衍生的产品或服务承担责任。&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;局限性声明&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;虽然本项目中的模型具备一定的中文理解和生成能力，但也存在局限性，包括但不限于：&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;可能会产生不可预测的有害内容以及不符合人类偏好和价值观的内容&lt;/li&gt; &#xA;  &lt;li&gt;由于算力和数据问题，相关模型的训练并不充分，中文理解能力有待进一步提升&lt;/li&gt; &#xA;  &lt;li&gt;暂时没有在线可互动的demo（注：用户仍然可以自行在本地部署和体验）&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;问题反馈&lt;/h2&gt; &#xA;&lt;p&gt;如有疑问，请在GitHub Issue中提交。礼貌地提出问题，构建和谐的讨论社区。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;在提交问题之前，请先查看FAQ能否解决问题，同时建议查阅以往的issue是否能解决你的问题。&lt;/li&gt; &#xA; &lt;li&gt;提交问题请使用本项目设置的Issue模板，以帮助快速定位具体问题。&lt;/li&gt; &#xA; &lt;li&gt;重复以及与本项目无关的issue会被&lt;a href=&#34;https://github.com/marketplace/stale&#34;&gt;stable-bot&lt;/a&gt;处理，敬请谅解。&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>THUDM/CodeGeeX2</title>
    <updated>2023-09-01T02:04:06Z</updated>
    <id>tag:github.com,2023-09-01:/THUDM/CodeGeeX2</id>
    <link href="https://github.com/THUDM/CodeGeeX2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CodeGeeX2: A More Powerful Multilingual Code Generation Model&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CodeGeeX2/main/resources/codegeex_logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 🏠 &lt;a href=&#34;https://codegeex.cn&#34; target=&#34;_blank&#34;&gt;主页&lt;/a&gt;｜🛠 插件 &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=aminer.codegeex&#34; target=&#34;_blank&#34;&gt;VS Code&lt;/a&gt;, &lt;a href=&#34;https://plugins.jetbrains.com/plugin/20587-codegeex&#34; target=&#34;_blank&#34;&gt;Jetbrains&lt;/a&gt;｜🤗 &lt;a href=&#34;https://huggingface.co/THUDM/codegeex2-6b&#34; target=&#34;_blank&#34;&gt;模型下载&lt;/a&gt;｜📄 &lt;a href=&#34;https://arxiv.org/abs/2303.17568&#34; target=&#34;_blank&#34;&gt;论文&lt;/a&gt;｜👋 加入&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CodeGeeX2/main/resources/wechat.md&#34; target=&#34;_blank&#34;&gt;微信开发者交流群&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Read this in &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CodeGeeX2/main/README_EN.md&#34;&gt;English&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CodeGeeX2/main/README_JA.md&#34;&gt;日本語&lt;/a&gt;で読む&lt;br&gt; Lire en &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CodeGeeX2/main/README_FR.md&#34;&gt;Français&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;CodeGeeX2: 更强大的多语言代码生成模型&lt;/h1&gt; &#xA;&lt;p&gt;CodeGeeX2 是多语言代码生成模型 &lt;a href=&#34;https://github.com/THUDM/CodeGeeX&#34;&gt;CodeGeeX&lt;/a&gt; (&lt;a href=&#34;https://arxiv.org/abs/2303.17568&#34;&gt;KDD’23&lt;/a&gt;) 的第二代模型。不同于一代 CodeGeeX（完全在国产华为昇腾芯片平台训练） ，CodeGeeX2 是基于 &lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B&#34;&gt;ChatGLM2&lt;/a&gt; 架构加入代码预训练实现，得益于 ChatGLM2 的更优性能，CodeGeeX2 在多项指标上取得性能提升（+107% &amp;gt; CodeGeeX；仅60亿参数即超过150亿参数的 StarCoder-15B 近10%），更多特性包括：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;更强大的代码能力&lt;/strong&gt;：基于 ChatGLM2-6B 基座语言模型，CodeGeeX2-6B 进一步经过了 600B 代码数据预训练，相比一代模型，在代码能力上全面提升，&lt;a href=&#34;https://huggingface.co/datasets/THUDM/humaneval-x&#34;&gt;HumanEval-X&lt;/a&gt; 评测集的六种编程语言均大幅提升 (Python +57%, C++ +71%, Java +54%, JavaScript +83%, Go +56%, Rust +321%)，在Python上达到 35.9% 的 Pass@1 一次通过率，超越规模更大的 StarCoder-15B。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;更优秀的模型特性&lt;/strong&gt;：继承 ChatGLM2-6B 模型特性，CodeGeeX2-6B 更好支持中英文输入，支持最大 8192 序列长度，推理速度较一代 CodeGeeX-13B 大幅提升，量化后仅需6GB显存即可运行，支持轻量级本地化部署。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;更全面的AI编程助手&lt;/strong&gt;：CodeGeeX插件（&lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=aminer.codegeex&#34;&gt;VS Code&lt;/a&gt;, &lt;a href=&#34;https://plugins.jetbrains.com/plugin/20587-codegeex&#34;&gt;Jetbrains&lt;/a&gt;）后端升级，支持超过100种编程语言，新增上下文补全、跨文件补全等实用功能。结合 Ask CodeGeeX 交互式AI编程助手，支持中英文对话解决各种编程问题，包括且不限于代码解释、代码翻译、代码纠错、文档生成等，帮助程序员更高效开发。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;更开放的协议&lt;/strong&gt;：CodeGeeX2-6B 权重对学术研究完全开放，填写&lt;a href=&#34;https://open.bigmodel.cn/mla/form?mcode=CodeGeeX2-6B&#34;&gt;登记表&lt;/a&gt;申请商业使用。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;使用教程&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CodeGeeX2/main/#%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B&#34;&gt;快速开始&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CodeGeeX2/main/docs/zh/inference_zh.md&#34;&gt;推理教程（多卡推理，加速推理，多平台推理等）&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;AI编程助手&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CodeGeeX2/main/resources/codegeex_demo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;我们开发了支持 VS Code、 IntelliJ IDEA、PyCharm、GoLand、WebStorm、Android Studio 等IDE的 CodeGeeX 插件。在插件中，可以更直接地体验到 CodeGeeX2 模型在代码生成与补全、添加注释、代码翻译及技术问答方面的能力为开发效率带来的提升。欢迎在IDE中下载 CodeGeeX 插件获得更加全面的AI编程体验，详情见&lt;a href=&#34;https://codegeex.cn/&#34;&gt;CodeGeeX主页&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;快速开始&lt;/h2&gt; &#xA;&lt;h3&gt;使用&lt;code&gt;transformers&lt;/code&gt;快速调用&lt;a href=&#34;https://huggingface.co/THUDM/codegeex2-6b&#34;&gt;CodeGeeX2-6B&lt;/a&gt;：&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModel&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;THUDM/codegeex2-6b&#34;, trust_remote_code=True)&#xA;model = AutoModel.from_pretrained(&#34;THUDM/codegeex2-6b&#34;, trust_remote_code=True, device=&#39;cuda&#39;)&#xA;model = model.eval()&#xA;&#xA;# remember adding a language tag for better performance&#xA;prompt = &#34;# language: Python\n# write a bubble sort function\n&#34;&#xA;inputs = tokenizer.encode(prompt, return_tensors=&#34;pt&#34;).to(model.device)&#xA;outputs = model.generate(inputs, max_length=256, top_k=1)&#xA;response = tokenizer.decode(outputs[0])&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; print(response)&#xA;# language: Python&#xA;# write a bubble sort function&#xA;&#xA;&#xA;def bubble_sort(list):&#xA;    for i in range(len(list) - 1):&#xA;        for j in range(len(list) - 1):&#xA;            if list[j] &amp;gt; list[j + 1]:&#xA;                list[j], list[j + 1] = list[j + 1], list[j]&#xA;    return list&#xA;&#xA;&#xA;print(bubble_sort([5, 2, 1, 8, 4]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;启动 Gradio DEMO：&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python ./demo/run_demo.py&#xA;&#xA;usage: run_demo.py [-h] [--model-path MODEL_PATH] [--example-path EXAMPLE_PATH] [--quantize QUANTIZE]&#xA;                   [--chatglm-cpp] [--fastllm] [--n-gpus N_GPUS] [--gpu GPU] [--cpu] [--auth] [--username yourname]&#xA;                   [--password yourpassword]&#xA;                   [--port PORT] [--listen ADDRESS]&#xA;&#xA;# 若要启用身份验证，请先启用--auth，然后定义--username与--password，如：&#xA;python run_demo.py --auth --username user --password password  # 若要监听所有地址请指定 --listen 0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;支持使用 &lt;a href=&#34;https://github.com/li-plus/chatglm.cpp&#34;&gt;ChatGLM.cpp&lt;/a&gt; 量化推理加速：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python ./demo/run_demo.py --quantize 4 --chatglm-cpp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;启动FAST API:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python ./demo/fastapicpu.py&#xA;usage: fastapicpu.py [-h] [--model-path MODEL_PATH] [--listen ADDRESS] [--port PORT] [--workders NUM] [--cpu] [--half] [--quantize QUANTIZE] [--chatglm-cpp]&#xA;# --cpu启用cpu --half启用.half()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;支持使用 &lt;a href=&#34;https://github.com/li-plus/chatglm.cpp&#34;&gt;ChatGLM.cpp&lt;/a&gt; 量化推理加速，同样添加 &lt;code&gt;--quantize 4 --chatglm-cpp&lt;/code&gt; 参数即可。&lt;/p&gt; &#xA;&lt;h3&gt;API使用示例&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X POST &#34;http://127.0.0.1:7860&#34; \&#xA;    -H &#39;Content-Type: application/json&#39; \&#xA;    -d &#39;{&#34;lang&#34;: &#34;Python&#34;, &#34;prompt&#34;: &#34;# Write a quick sort function&#34;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;❗️请注意：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;CodeGeeX2-6B 是一个基座代码生成模型，不具备聊天能力。请前往插件中体验更全面的 Ask CodeGeeX 聊天功能。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;在使用 CodeGeeX2-6B 的补全功能时，输入prompt需要遵循特定的格式以获得最好的效果。比如需要在开头加入编程语言标签（&lt;code&gt;# language: Python&lt;/code&gt;，请查看&lt;a href=&#34;https://github.com/THUDM/CodeGeeX2/raw/main/evaluation/utils.py#L14&#34;&gt;完整语言列表&lt;/a&gt;），以注释的形式写prompt等。参考&lt;code&gt;run_demo.py&lt;/code&gt;中的处理。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果显卡不支持&lt;code&gt;bfloat16&lt;/code&gt;格式，将会输出错误的内容，需要将模型转换成&lt;code&gt;float16&lt;/code&gt;格式：&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModel.from_pretrained(&#34;THUDM/codegeex2-6b&#34;, trust_remote_code=True).half().cuda()&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果需要使用多显卡加载模型,可以将以下代码：&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tokenizer = AutoTokenizer.from_pretrained(&#34;THUDM/codegeex2-6b&#34;, trust_remote_code=True)&#xA;model = AutoModel.from_pretrained(&#34;THUDM/codegeex2-6b&#34;, trust_remote_code=True, device=&#39;cuda&#39;)&#xA;model = model.eval()&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;替换为&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_model():&#xA;    tokenizer = AutoTokenizer.from_pretrained(&#34;THUDM/codegeex2-6b&#34;, trust_remote_code=True)&#xA;    from gpus import load_model_on_gpus&#xA;    # gpus文件在demo文件夹中&#xA;    model = load_model_on_gpus(&#34;THUDM/codegeex2-6b&#34;, num_gpus=2)&#xA;    model = model.eval()&#xA;    return tokenizer, model&#xA;&#xA;tokenizer, model = get_model()&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;代码能力评测&lt;/h2&gt; &#xA;&lt;p&gt;CodeGeeX2 作为一个多语言代码生成基座模型，代码能力较上一代大幅提升，以下是在 HumanEval，HumanEval-X, DS1000 基准上的评测结果（评价指标 Pass@k 定义与&lt;a href=&#34;https://arxiv.org/abs/2303.17568&#34;&gt;论文&lt;/a&gt;中一致）：&lt;/p&gt; &#xA;&lt;h3&gt;HumanEval (Pass@1,10,100)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Pass@1&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Pass@10&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Pass@100&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CodeGen-16B-multi&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CodeGeeX-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Codex-12B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CodeT5Plus-16B-mono&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Code-Cushman-001&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaMA-65B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaMA2-70B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CodeGen2.5-7B-mono&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;StarCoder-15B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;CodeGeeX2-6B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;35.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;62.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;88.3&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Pass@1&lt;/strong&gt; 使用 &lt;code&gt;n=20, t=0.2, top_p=0.95&lt;/code&gt;；&lt;strong&gt;Pass@10,Pass@100&lt;/strong&gt; 使用 &lt;code&gt;n=200, t=0.8, top_p=0.95&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;HumanEval-X (Pass@1)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;C++&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Java&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;JavaScript&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Go&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Rust&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CodeGen-16B-multi&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CodeGeeX-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Replit-code-v1-3B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CodeGen2.5-7B-multi&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;20.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;StarCoder-15B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;31.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;33.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;CodeGeeX2-6B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;35.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;29.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;22.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;28.1&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Pass@1&lt;/strong&gt; 使用 &lt;code&gt;n=20, t=0.2, top_p=0.95&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;以上结果可使用脚本&lt;code&gt;scripts/run_humanevalx.sh&lt;/code&gt;复现。环境配置和说明参见&lt;a href=&#34;https://github.com/THUDM/CodeGeeX/raw/main/codegeex/benchmark/README_zh.md&#34;&gt;评测环境&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;DS1000 (Pass@1)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Matplotlib&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Numpy&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Pandas&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Pytorch&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;SciPy&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Scikit-learn&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;TensorFlow&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;# Samples&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;155&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;220&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;291&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;106&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;115&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1000&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CodeGen-16B-Mono&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;code-cushman-001&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Codex-001&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;CodeGeeX2-6B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;StarCoder-15B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Codex-002&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;43.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;26.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;41.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;31.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;44.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;39.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;39.2&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Pass@1&lt;/strong&gt; 使用 &lt;code&gt;n=40, t=0.2, top_p=0.5&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;以上结果可使用&lt;a href=&#34;https://github.com/HKUNLP/DS-1000.git&#34;&gt;DS1000评测代码&lt;/a&gt;复现。&lt;/p&gt; &#xA;&lt;h2&gt;量化推理性能&lt;/h2&gt; &#xA;&lt;p&gt;CodeGeeX2 与上一代相比，对部署更加友好。得益于使用 Multi-Query Attention 和 Flash Attention，推理速度更快，且量化后仅需6GB显存即可运行：&lt;/p&gt; &#xA;&lt;h3&gt;量化&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FP16/BF16&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;INT8&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;INT4&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CodeGeeX-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.9 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.7 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;CodeGeeX2-6B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.1 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.2 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.5 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;基于 PyTorch 2.0 测试，利用&lt;code&gt;torch.nn.functional.scaled_dot_product_attention&lt;/code&gt;实现高效的 Attention 计算。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;推理&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;推理速度 (字符/秒)&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CodeGeeX-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;CodeGeeX2-6B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;94&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;batch_size=1, max_length=2048&lt;/code&gt;，均使用加速框架，测试硬件为&lt;code&gt;GeForce RTX-3090&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;协议&lt;/h2&gt; &#xA;&lt;p&gt;本仓库的代码依照 &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache-2.0&lt;/a&gt; 协议开源，模型的权重的使用则需要遵循 &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CodeGeeX2/main/MODEL_LICENSE&#34;&gt;Model License&lt;/a&gt;。CodeGeeX2-6B 权重对学术研究完全开放，填写&lt;a href=&#34;https://open.bigmodel.cn/mla/form?mcode=CodeGeeX2-6B&#34;&gt;登记表&lt;/a&gt;申请商业使用。&lt;/p&gt; &#xA;&lt;h2&gt;引用&lt;/h2&gt; &#xA;&lt;p&gt;如果觉得我们的工作有帮助，欢迎引用以下论文：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{zheng2023codegeex,&#xA;      title={CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X},&#xA;      author={Qinkai Zheng and Xiao Xia and Xu Zou and Yuxiao Dong and Shan Wang and Yufei Xue and Zihan Wang and Lei Shen and Andi Wang and Yang Li and Teng Su and Zhilin Yang and Jie Tang},&#xA;      booktitle={KDD},&#xA;      year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>