<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-01T01:48:12Z</updated>
  <subtitle>Monthly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>TencentARC/GFPGAN</title>
    <updated>2022-09-01T01:48:12Z</updated>
    <id>tag:github.com,2022-09-01:/TencentARC/GFPGAN</id>
    <link href="https://github.com/TencentARC/GFPGAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GFPGAN aims at developing Practical Algorithms for Real-world Face Restoration.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/assets/gfpgan_logo.png&#34; height=&#34;130&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  &lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/README_CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/b&gt;&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;a href=&#34;https://twitter.com/_Xintao_&#34; style=&#34;text-decoration:none;&#34;&gt;&#xA;    &lt;img src=&#34;https://user-images.githubusercontent.com/17445847/187162058-c764ced6-952f-404b-ac85-ba95cce18e7b.png&#34; width=&#34;4%&#34; alt=&#34;&#34; /&gt;&#xA;&lt;/a&gt; --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/TencentARC/GFPGAN/total.svg?sanitize=true&#34; alt=&#34;download&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/gfpgan/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/gfpgan&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/TencentARC/GFPGAN&#34; alt=&#34;Open issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-closed/TencentARC/GFPGAN&#34; alt=&#34;Closed issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/raw/master/.github/workflows/pylint.yml&#34;&gt;&lt;img src=&#34;https://github.com/TencentARC/GFPGAN/actions/workflows/pylint.yml/badge.svg?sanitize=true&#34; alt=&#34;python lint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/raw/master/.github/workflows/publish-pip.yml&#34;&gt;&lt;img src=&#34;https://github.com/TencentARC/GFPGAN/actions/workflows/publish-pip.yml/badge.svg?sanitize=true&#34; alt=&#34;Publish-pip&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;span&gt;üí•&lt;/span&gt; &lt;strong&gt;Updated&lt;/strong&gt; online demo: &lt;a href=&#34;https://replicate.com/tencentarc/gfpgan&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo&amp;amp;message=Replicate&amp;amp;color=blue&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;. Here is the &lt;a href=&#34;https://replicate.com/xinntao/gfpgan&#34;&gt;backup&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üí•&lt;/span&gt; &lt;strong&gt;Updated&lt;/strong&gt; online demo: &lt;a href=&#34;https://huggingface.co/spaces/Xintao/GFPGAN&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo&amp;amp;message=Huggingface%20Gradio&amp;amp;color=orange&#34; alt=&#34;Huggingface Gradio&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo&#34;&gt;Colab Demo&lt;/a&gt; for GFPGAN &lt;a href=&#34;https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;google colab logo&#34;&gt;&lt;/a&gt;; (Another &lt;a href=&#34;https://colab.research.google.com/drive/1Oa1WwKB4M4l1GmR7CtswDVgOCOeSLChA?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for the original paper model)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;!-- 3. Online demo: [Replicate.ai](https://replicate.com/xinntao/gfpgan) (may need to sign in, return the whole image)&#xA;4. Online demo: [Baseten.co](https://app.baseten.co/applications/Q04Lz0d/operator_views/8qZG6Bg) (backed by GPU, returns the whole image)&#xA;5. We provide a *clean* version of GFPGAN, which can run without CUDA extensions. So that it can run in **Windows** or on **CPU mode**. --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;üöÄ&lt;/span&gt; &lt;strong&gt;Thanks for your interest in our work. You may also want to check our new updates on the &lt;em&gt;tiny models&lt;/em&gt; for &lt;em&gt;anime images and videos&lt;/em&gt; in &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/raw/master/docs/anime_video_model.md&#34;&gt;Real-ESRGAN&lt;/a&gt;&lt;/strong&gt; &lt;span&gt;üòä&lt;/span&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;GFPGAN aims at developing a &lt;strong&gt;Practical Algorithm for Real-world Face Restoration&lt;/strong&gt;.&lt;br&gt; It leverages rich and diverse priors encapsulated in a pretrained face GAN (&lt;em&gt;e.g.&lt;/em&gt;, StyleGAN2) for blind face restoration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚ùì&lt;/span&gt; Frequently Asked Questions can be found in &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/FAQ.md&#34;&gt;FAQ.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üö©&lt;/span&gt; &lt;strong&gt;Updates&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;üî•&lt;/span&gt;&lt;span&gt;üî•&lt;/span&gt;&lt;span&gt;‚úÖ&lt;/span&gt; Add &lt;strong&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth&#34;&gt;V1.3 model&lt;/a&gt;&lt;/strong&gt;, which produces &lt;strong&gt;more natural&lt;/strong&gt; restoration results, and better results on &lt;em&gt;very low-quality&lt;/em&gt; / &lt;em&gt;high-quality&lt;/em&gt; inputs. See more in &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/#european_castle-model-zoo&#34;&gt;Model zoo&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/Comparisons.md&#34;&gt;Comparisons.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;‚úÖ&lt;/span&gt; Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/GFPGAN&#34;&gt;Gradio Web Demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;‚úÖ&lt;/span&gt; Support enhancing non-face regions (background) with &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;Real-ESRGAN&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;‚úÖ&lt;/span&gt; We provide a &lt;em&gt;clean&lt;/em&gt; version of GFPGAN, which does not require CUDA extensions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;‚úÖ&lt;/span&gt; We provide an updated model without colorizing faces.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If GFPGAN is helpful in your photos/projects, please help to &lt;span&gt;‚≠ê&lt;/span&gt; this repo or recommend it to your friends. Thanks&lt;span&gt;üòä&lt;/span&gt; Other recommended projects:&lt;br&gt; &lt;span&gt;‚ñ∂&lt;/span&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;Real-ESRGAN&lt;/a&gt;: A practical algorithm for general image restoration&lt;br&gt; &lt;span&gt;‚ñ∂&lt;/span&gt; &lt;a href=&#34;https://github.com/xinntao/BasicSR&#34;&gt;BasicSR&lt;/a&gt;: An open-source image and video restoration toolbox&lt;br&gt; &lt;span&gt;‚ñ∂&lt;/span&gt; &lt;a href=&#34;https://github.com/xinntao/facexlib&#34;&gt;facexlib&lt;/a&gt;: A collection that provides useful face-relation functions&lt;br&gt; &lt;span&gt;‚ñ∂&lt;/span&gt; &lt;a href=&#34;https://github.com/xinntao/HandyView&#34;&gt;HandyView&lt;/a&gt;: A PyQt5-based image viewer that is handy for view and comparison&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;span&gt;üìñ&lt;/span&gt; GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2101.04061&#34;&gt;Paper&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://xinntao.github.io/projects/gfpgan&#34;&gt;Project Page&lt;/a&gt;] ‚ÄÉ [Demo] &lt;br&gt; &lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&#34;https://yu-li.github.io/&#34;&gt;Yu Li&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=KjQLROoAAAAJ&#34;&gt;Honglun Zhang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=4oXBp9UAAAAJ&amp;amp;hl=en&#34;&gt;Ying Shan&lt;/a&gt; &lt;br&gt; Applied Research Center (ARC), Tencent PCG&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://xinntao.github.io/projects/GFPGAN_src/gfpgan_teaser.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;span&gt;üîß&lt;/span&gt; Dependencies and Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.7 (Recommend to use &lt;a href=&#34;https://www.anaconda.com/download/#linux&#34;&gt;Anaconda&lt;/a&gt; or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch &amp;gt;= 1.7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Option: NVIDIA GPU + &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;CUDA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Option: Linux&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;We now provide a &lt;em&gt;clean&lt;/em&gt; version of GFPGAN, which does not require customized CUDA extensions. &lt;br&gt; If you want to use the original model in our paper, please see &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/PaperModel.md&#34;&gt;PaperModel.md&lt;/a&gt; for installation.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone repo&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/TencentARC/GFPGAN.git&#xA;cd GFPGAN&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependent packages&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install basicsr - https://github.com/xinntao/BasicSR&#xA;# We use BasicSR for both training and inference&#xA;pip install basicsr&#xA;&#xA;# Install facexlib - https://github.com/xinntao/facexlib&#xA;# We use face detection and face restoration helper in the facexlib package&#xA;pip install facexlib&#xA;&#xA;pip install -r requirements.txt&#xA;python setup.py develop&#xA;&#xA;# If you want to enhance the background (non-face) regions with Real-ESRGAN,&#xA;# you also need to install the realesrgan package&#xA;pip install realesrgan&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚ö°&lt;/span&gt; Quick Inference&lt;/h2&gt; &#xA;&lt;p&gt;We take the v1.3 version for an example. More models can be found &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/#european_castle-model-zoo&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Download pre-trained models: &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth&#34;&gt;GFPGANv1.3.pth&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inference!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;Usage: python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2 [options]...&#xA;&#xA;  -h                   show this help&#xA;  -i input             Input image or folder. Default: inputs/whole_imgs&#xA;  -o output            Output folder. Default: results&#xA;  -v version           GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3&#xA;  -s upscale           The final upsampling scale of the image. Default: 2&#xA;  -bg_upsampler        background upsampler. Default: realesrgan&#xA;  -bg_tile             Tile size for background sampler, 0 for no tile during testing. Default: 400&#xA;  -suffix              Suffix of the restored faces&#xA;  -only_center_face    Only restore the center face&#xA;  -aligned             Input are aligned faces&#xA;  -ext                 Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use the original model in our paper, please see &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/PaperModel.md&#34;&gt;PaperModel.md&lt;/a&gt; for installation and inference.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üè∞&lt;/span&gt; Model Zoo&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Version&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;V1.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth&#34;&gt;GFPGANv1.3.pth&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Based on V1.2; &lt;strong&gt;more natural&lt;/strong&gt; restoration results; better results on very low-quality / high-quality inputs.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;V1.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth&#34;&gt;GFPGANCleanv1-NoCE-C2.pth&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No colorization; no CUDA extensions are required. Trained with more data with pre-processing.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;V1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth&#34;&gt;GFPGANv1.pth&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The paper model, with colorization.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The comparisons are in &lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/GFPGAN/master/Comparisons.md&#34;&gt;Comparisons.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that V1.3 is not always better than V1.2. You may need to select different models based on your purpose and inputs.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Version&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Strengths&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Weaknesses&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;V1.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì natural outputs&lt;br&gt; ‚úìbetter results on very low-quality inputs &lt;br&gt; ‚úì work on relatively high-quality inputs &lt;br&gt;‚úì can have repeated (twice) restorations&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úó not very sharp &lt;br&gt; ‚úó have a slight change on identity&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;V1.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì sharper output &lt;br&gt; ‚úì with beauty makeup&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úó some outputs are unnatural&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can find &lt;strong&gt;more models (such as the discriminators)&lt;/strong&gt; here: [&lt;a href=&#34;https://drive.google.com/drive/folders/17rLiFzcUMoQuhLnptDsKolegHWwJOnHu?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;], OR [&lt;a href=&#34;https://share.weiyun.com/ShYoCCoc&#34;&gt;Tencent Cloud ËÖæËÆØÂæÆ‰∫ë&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üíª&lt;/span&gt; Training&lt;/h2&gt; &#xA;&lt;p&gt;We provide the training codes for GFPGAN (used in our paper). &lt;br&gt; You could improve it according to your own needs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;More high quality faces can improve the restoration quality.&lt;/li&gt; &#xA; &lt;li&gt;You may need to perform some pre-processing, such as beauty makeup.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Procedures&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;(You can try a simple version ( &lt;code&gt;options/train_gfpgan_v1_simple.yml&lt;/code&gt;) that does not require face component landmarks.)&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Dataset preparation: &lt;a href=&#34;https://github.com/NVlabs/ffhq-dataset&#34;&gt;FFHQ&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download pre-trained models and other data. Put them in the &lt;code&gt;experiments/pretrained_models&lt;/code&gt; folder.&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/StyleGAN2_512_Cmul1_FFHQ_B12G4_scratch_800k.pth&#34;&gt;Pre-trained StyleGAN2 model: StyleGAN2_512_Cmul1_FFHQ_B12G4_scratch_800k.pth&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/FFHQ_eye_mouth_landmarks_512.pth&#34;&gt;Component locations of FFHQ: FFHQ_eye_mouth_landmarks_512.pth&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/arcface_resnet18.pth&#34;&gt;A simple ArcFace model: arcface_resnet18.pth&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Modify the configuration file &lt;code&gt;options/train_gfpgan_v1.yml&lt;/code&gt; accordingly.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Training&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;python -m torch.distributed.launch --nproc_per_node=4 --master_port=22021 gfpgan/train.py -opt options/train_gfpgan_v1.yml --launcher pytorch&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìú&lt;/span&gt; License and Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;GFPGAN is released under Apache License Version 2.0.&lt;/p&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@InProceedings{wang2021gfpgan,&#xA;    author = {Xintao Wang and Yu Li and Honglun Zhang and Ying Shan},&#xA;    title = {Towards Real-World Blind Face Restoration with Generative Facial Prior},&#xA;    booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;    year = {2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìß&lt;/span&gt; Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any question, please email &lt;code&gt;xintao.wang@outlook.com&lt;/code&gt; or &lt;code&gt;xintaowang@tencent.com&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kamranahmedse/developer-roadmap</title>
    <updated>2022-09-01T01:48:12Z</updated>
    <id>tag:github.com,2022-09-01:/kamranahmedse/developer-roadmap</id>
    <link href="https://github.com/kamranahmedse/developer-roadmap" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Roadmap to becoming a developer in 2022&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/kamranahmedse/developer-roadmap/master/public/brand.png&#34; height=&#34;128&#34;&gt; &lt;/p&gt;&#xA;&lt;h2 align=&#34;center&#34;&gt;roadmap.sh&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Community driven roadmaps, articles and resources for developers&lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://roadmap.sh/roadmaps&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-Roadmaps%20-0a0a0a.svg?style=flat&amp;amp;colorA=0a0a0a&#34; alt=&#34;roadmaps&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://youtube.com/theroadmap?sub_confirmation=1&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-Videos-0a0a0a.svg?style=flat&amp;amp;colorA=0a0a0a&#34; alt=&#34;videos&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/kamranahmedse/developer-roadmap/tree/0471d44c8fae58b6a36a7c57bba12253916d0249/translations&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-Translations-0a0a0a.svg?style=flat&amp;amp;colorA=0a0a0a&#34; alt=&#34;videos&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/channel/UCA0H2KIWgWTwpTFjSxp0now?sub_confirmation=1&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%E2%9D%A4-YouTube%20Channel-0a0a0a.svg?style=flat&amp;amp;colorA=0a0a0a&#34; alt=&#34;roadmaps&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/waxVImv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Roadmaps are being made interactive and have been moved to the website.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://roadmap.sh&#34;&gt;View all Roadmaps&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/waxVImv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here is the list of available roadmaps with more being actively worked upon.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/frontend&#34;&gt;Frontend Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/backend&#34;&gt;Backend Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/devops&#34;&gt;DevOps Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/react&#34;&gt;React Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/vue&#34;&gt;Vue Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/angular&#34;&gt;Angular Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/android&#34;&gt;Android Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/javascript&#34;&gt;JavaScript Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/nodejs&#34;&gt;Node.js Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/python&#34;&gt;Python Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/golang&#34;&gt;Go Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/java&#34;&gt;Java Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/postgresql-dba&#34;&gt;DBA Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roadmap.sh/blockchain&#34;&gt;Blockchain Roadmap&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/waxVImv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you think that these can be improved in any way, please do suggest. Also, if you would like to contribute to existing roadmaps or add a new roadmap, please open an issue or reach out to &lt;a href=&#34;https://twitter.com/kamranahmedse&#34;&gt;@kamranahmedse&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repository, install the dependencies and start the application&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:kamranahmedse/developer-roadmap.git&#xA;npm install&#xA;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Have a look at &lt;a href=&#34;https://raw.githubusercontent.com/kamranahmedse/developer-roadmap/master/contributing&#34;&gt;contribution docs&lt;/a&gt; for how to update any of the roadmaps&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Suggest changes to existing roadmaps&lt;/li&gt; &#xA; &lt;li&gt;Improve the site&#39;s codebase&lt;/li&gt; &#xA; &lt;li&gt;Add new Roadmap&lt;/li&gt; &#xA; &lt;li&gt;Write tests&lt;/li&gt; &#xA; &lt;li&gt;Discuss ideas in issues&lt;/li&gt; &#xA; &lt;li&gt;Spread the word&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Have a look at the &lt;a href=&#34;https://raw.githubusercontent.com/kamranahmedse/developer-roadmap/master/license&#34;&gt;license file&lt;/a&gt; for details&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>xinntao/Real-ESRGAN</title>
    <updated>2022-09-01T01:48:12Z</updated>
    <id>tag:github.com,2022-09-01:/xinntao/Real-ESRGAN</id>
    <link href="https://github.com/xinntao/Real-ESRGAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Real-ESRGAN aims at developing Practical Algorithms for General Image/Video Restoration.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/assets/realesrgan_logo.png&#34; height=&#34;120&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  &lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/README_CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/b&gt;&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;üëÄ&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#-demos-videos&#34;&gt;&lt;strong&gt;Demos&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üö©&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#-updates&#34;&gt;&lt;strong&gt;Updates&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; ‚ö°&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#-quick-inference&#34;&gt;&lt;strong&gt;Usage&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üè∞&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/model_zoo.md&#34;&gt;&lt;strong&gt;Model Zoo&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üîß&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#-dependencies-and-installation&#34;&gt;Install&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üíª&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/Training.md&#34;&gt;Train&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; ‚ùì&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/FAQ.md&#34;&gt;FAQ&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üé®&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/CONTRIBUTING.md&#34;&gt;Contribution&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/xinntao/Real-ESRGAN/total.svg?sanitize=true&#34; alt=&#34;download&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/realesrgan/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/realesrgan&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/xinntao/Real-ESRGAN&#34; alt=&#34;Open issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-closed/xinntao/Real-ESRGAN&#34; alt=&#34;Closed issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/xinntao/Real-ESRGAN.svg?sanitize=true&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/raw/master/.github/workflows/pylint.yml&#34;&gt;&lt;img src=&#34;https://github.com/xinntao/Real-ESRGAN/actions/workflows/pylint.yml/badge.svg?sanitize=true&#34; alt=&#34;python lint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/raw/master/.github/workflows/publish-pip.yml&#34;&gt;&lt;img src=&#34;https://github.com/xinntao/Real-ESRGAN/actions/workflows/publish-pip.yml/badge.svg?sanitize=true&#34; alt=&#34;Publish-pip&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üî• &lt;strong&gt;AnimeVideo-v3 model (Âä®Êº´ËßÜÈ¢ëÂ∞èÊ®°Âûã)&lt;/strong&gt;. Please see [&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_video_model.md&#34;&gt;&lt;em&gt;anime video models&lt;/em&gt;&lt;/a&gt;] and [&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_comparisons.md&#34;&gt;&lt;em&gt;comparisons&lt;/em&gt;&lt;/a&gt;]&lt;br&gt; üî• &lt;strong&gt;RealESRGAN_x4plus_anime_6B&lt;/strong&gt; for anime images &lt;strong&gt;(Âä®Êº´ÊèíÂõæÊ®°Âûã)&lt;/strong&gt;. Please see [&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_model.md&#34;&gt;&lt;em&gt;anime_model&lt;/em&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You can try in our website: &lt;a href=&#34;https://arc.tencent.com/en/ai-demos/imgRestore&#34;&gt;ARC Demo&lt;/a&gt; (now only support RealESRGAN_x4plus_anime_6B)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1k2Zod6kSHEvraybHl50Lys0LerhyTMCo?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for Real-ESRGAN &lt;strong&gt;|&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1yNl9ORUxxlL4N0keJa2SEPB61imPQd1B?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for Real-ESRGAN (&lt;strong&gt;anime videos&lt;/strong&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Portable &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-windows.zip&#34;&gt;Windows&lt;/a&gt; / &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-ubuntu.zip&#34;&gt;Linux&lt;/a&gt; / &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-macos.zip&#34;&gt;MacOS&lt;/a&gt; &lt;strong&gt;executable files for Intel/AMD/Nvidia GPU&lt;/strong&gt;. You can find more information &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#portable-executable-files-ncnn&#34;&gt;here&lt;/a&gt;. The ncnn implementation is in &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan&#34;&gt;Real-ESRGAN-ncnn-vulkan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can watch enhanced animations in &lt;a href=&#34;https://v.qq.com/s/topic/v_child/render/fC4iyCAM.html&#34;&gt;Tencent Video&lt;/a&gt;. Ê¨¢ËøéËßÇÁúã&lt;a href=&#34;https://v.qq.com/s/topic/v_child/render/fC4iyCAM.html&#34;&gt;ËÖæËÆØËßÜÈ¢ëÂä®Êº´‰øÆÂ§ç&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Real-ESRGAN aims at developing &lt;strong&gt;Practical Algorithms for General Image/Video Restoration&lt;/strong&gt;.&lt;br&gt; We extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data.&lt;/p&gt; &#xA;&lt;p&gt;üåå Thanks for your valuable feedbacks/suggestions. All the feedbacks are updated in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/feedback.md&#34;&gt;feedback.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If Real-ESRGAN is helpful, please help to ‚≠ê this repo or recommend it to your friends üòä &lt;br&gt; Other recommended projects:&lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;GFPGAN&lt;/a&gt;: A practical algorithm for real-world face restoration &lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/BasicSR&#34;&gt;BasicSR&lt;/a&gt;: An open-source image and video restoration toolbox&lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/facexlib&#34;&gt;facexlib&lt;/a&gt;: A collection that provides useful face-relation functions.&lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/HandyView&#34;&gt;HandyView&lt;/a&gt;: A PyQt5-based image viewer that is handy for view and comparison &lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/HandyFigure&#34;&gt;HandyFigure&lt;/a&gt;: Open source of paper figures &lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;üìñ Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2107.10833&#34;&gt;Paper&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://www.youtube.com/watch?v=fxHWoDSSvSc&#34;&gt;YouTube Video&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://www.bilibili.com/video/BV1H34y1m7sS/&#34;&gt;BÁ´ôËÆ≤Ëß£&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://xinntao.github.io/projects/RealESRGAN_src/RealESRGAN_poster.pdf&#34;&gt;Poster&lt;/a&gt;] ‚ÄÉ [&lt;a href=&#34;https://docs.google.com/presentation/d/1QtW6Iy8rm8rGLsJ0Ldti6kP-7Qyzy6XL/edit?usp=sharing&amp;amp;ouid=109799856763657548160&amp;amp;rtpof=true&amp;amp;sd=true&#34;&gt;PPT slides&lt;/a&gt;]&lt;br&gt; &lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;, Liangbin Xie, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ&#34;&gt;Chao Dong&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=4oXBp9UAAAAJ&amp;amp;hl=en&#34;&gt;Ying Shan&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://arc.tencent.com/en/ai-demos/imgRestore&#34;&gt;Tencent ARC Lab&lt;/a&gt;; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/assets/teaser.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;!-- Updates ---------------------------&gt; &#xA;&lt;h2&gt;üö© Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ Update the &lt;strong&gt;RealESRGAN AnimeVideo-v3&lt;/strong&gt; model. Please see &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_video_model.md&#34;&gt;anime video models&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_comparisons.md&#34;&gt;comparisons&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Add small models for anime videos. More details are in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_video_model.md&#34;&gt;anime video models&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Add the ncnn implementation &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan&#34;&gt;Real-ESRGAN-ncnn-vulkan&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Add &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth&#34;&gt;&lt;em&gt;RealESRGAN_x4plus_anime_6B.pth&lt;/em&gt;&lt;/a&gt;, which is optimized for &lt;strong&gt;anime&lt;/strong&gt; images with much smaller model size. More details and comparisons with &lt;a href=&#34;https://github.com/nihui/waifu2x-ncnn-vulkan&#34;&gt;waifu2x&lt;/a&gt; are in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_model.md&#34;&gt;&lt;strong&gt;anime_model.md&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Support finetuning on your own data or paired data (&lt;em&gt;i.e.&lt;/em&gt;, finetuning ESRGAN). See &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/Training.md#Finetune-Real-ESRGAN-on-your-own-dataset&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Integrate &lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;GFPGAN&lt;/a&gt; to support &lt;strong&gt;face enhancement&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/Real-ESRGAN&#34;&gt;Gradio Web Demo&lt;/a&gt;. Thanks &lt;a href=&#34;https://github.com/AK391&#34;&gt;@AK391&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Support arbitrary scale with &lt;code&gt;--outscale&lt;/code&gt; (It actually further resizes outputs with &lt;code&gt;LANCZOS4&lt;/code&gt;). Add &lt;em&gt;RealESRGAN_x2plus.pth&lt;/em&gt; model.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/inference_realesrgan.py&#34;&gt;The inference code&lt;/a&gt; supports: 1) &lt;strong&gt;tile&lt;/strong&gt; options; 2) images with &lt;strong&gt;alpha channel&lt;/strong&gt;; 3) &lt;strong&gt;gray&lt;/strong&gt; images; 4) &lt;strong&gt;16-bit&lt;/strong&gt; images.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ The training codes have been released. A detailed guide can be found in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/Training.md&#34;&gt;Training.md&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;!-- Demo videos ---------------------------&gt; &#xA;&lt;h2&gt;üëÄ Demos Videos&lt;/h2&gt; &#xA;&lt;h4&gt;Bilibili&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1ja41117zb&#34;&gt;Â§ßÈóπÂ§©ÂÆ´ÁâáÊÆµ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1wY4y1L7hT/&#34;&gt;Anime dance cut Âä®Êº´È≠îÊÄßËàûËπà&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1i3411L7Gy/&#34;&gt;Êµ∑Ë¥ºÁéãÁâáÊÆµ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;YouTube&lt;/h4&gt; &#xA;&lt;h2&gt;üîß Dependencies and Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.7 (Recommend to use &lt;a href=&#34;https://www.anaconda.com/download/#linux&#34;&gt;Anaconda&lt;/a&gt; or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch &amp;gt;= 1.7&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone repo&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/xinntao/Real-ESRGAN.git&#xA;cd Real-ESRGAN&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependent packages&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install basicsr - https://github.com/xinntao/BasicSR&#xA;# We use BasicSR for both training and inference&#xA;pip install basicsr&#xA;# facexlib and gfpgan are for face enhancement&#xA;pip install facexlib&#xA;pip install gfpgan&#xA;pip install -r requirements.txt&#xA;python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;‚ö° Quick Inference&lt;/h2&gt; &#xA;&lt;p&gt;There are usually three ways to inference Real-ESRGAN.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#online-inference&#34;&gt;Online inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#portable-executable-files-ncnn&#34;&gt;Portable executable files (NCNN)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/#python-script&#34;&gt;Python script&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Online inference&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You can try in our website: &lt;a href=&#34;https://arc.tencent.com/en/ai-demos/imgRestore&#34;&gt;ARC Demo&lt;/a&gt; (now only support RealESRGAN_x4plus_anime_6B)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1k2Zod6kSHEvraybHl50Lys0LerhyTMCo?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for Real-ESRGAN &lt;strong&gt;|&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1yNl9ORUxxlL4N0keJa2SEPB61imPQd1B?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; for Real-ESRGAN (&lt;strong&gt;anime videos&lt;/strong&gt;).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Portable executable files (NCNN)&lt;/h3&gt; &#xA;&lt;p&gt;You can download &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-windows.zip&#34;&gt;Windows&lt;/a&gt; / &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-ubuntu.zip&#34;&gt;Linux&lt;/a&gt; / &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-macos.zip&#34;&gt;MacOS&lt;/a&gt; &lt;strong&gt;executable files for Intel/AMD/Nvidia GPU&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This executable file is &lt;strong&gt;portable&lt;/strong&gt; and includes all the binaries and models required. No CUDA or PyTorch environment is needed.&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can simply run the following command (the Windows example, more information is in the README.md of each executable files):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./realesrgan-ncnn-vulkan.exe -i input.jpg -o output.png -n model_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have provided five models:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;realesrgan-x4plus (default)&lt;/li&gt; &#xA; &lt;li&gt;realesrnet-x4plus&lt;/li&gt; &#xA; &lt;li&gt;realesrgan-x4plus-anime (optimized for anime images, small model size)&lt;/li&gt; &#xA; &lt;li&gt;realesr-animevideov3 (animation video)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;-n&lt;/code&gt; argument for other models, for example, &lt;code&gt;./realesrgan-ncnn-vulkan.exe -i input.jpg -o output.png -n realesrnet-x4plus&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Usage of portable executable files&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Please refer to &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan#computer-usages&#34;&gt;Real-ESRGAN-ncnn-vulkan&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;Note that it does not support all the functions (such as &lt;code&gt;outscale&lt;/code&gt;) as the python script &lt;code&gt;inference_realesrgan.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;Usage: realesrgan-ncnn-vulkan.exe -i infile -o outfile [options]...&#xA;&#xA;  -h                   show this help&#xA;  -i input-path        input image path (jpg/png/webp) or directory&#xA;  -o output-path       output image path (jpg/png/webp) or directory&#xA;  -s scale             upscale ratio (can be 2, 3, 4. default=4)&#xA;  -t tile-size         tile size (&amp;gt;=32/0=auto, default=0) can be 0,0,0 for multi-gpu&#xA;  -m model-path        folder path to the pre-trained models. default=models&#xA;  -n model-name        model name (default=realesr-animevideov3, can be realesr-animevideov3 | realesrgan-x4plus | realesrgan-x4plus-anime | realesrnet-x4plus)&#xA;  -g gpu-id            gpu device to use (default=auto) can be 0,1,2 for multi-gpu&#xA;  -j load:proc:save    thread count for load/proc/save (default=1:2:2) can be 1:2,2,2:2 for multi-gpu&#xA;  -x                   enable tta mode&#34;&#xA;  -f format            output image format (jpg/png/webp, default=ext/png)&#xA;  -v                   verbose output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that it may introduce block inconsistency (and also generate slightly different results from the PyTorch implementation), because this executable file first crops the input image into several tiles, and then processes them separately, finally stitches together.&lt;/p&gt; &#xA;&lt;h3&gt;Python script&lt;/h3&gt; &#xA;&lt;h4&gt;Usage of python script&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You can use X4 model for &lt;strong&gt;arbitrary output size&lt;/strong&gt; with the argument &lt;code&gt;outscale&lt;/code&gt;. The program will further perform cheap resize operation after the Real-ESRGAN output.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;Usage: python inference_realesrgan.py -n RealESRGAN_x4plus -i infile -o outfile [options]...&#xA;&#xA;A common command: python inference_realesrgan.py -n RealESRGAN_x4plus -i infile --outscale 3.5 --face_enhance&#xA;&#xA;  -h                   show this help&#xA;  -i --input           Input image or folder. Default: inputs&#xA;  -o --output          Output folder. Default: results&#xA;  -n --model_name      Model name. Default: RealESRGAN_x4plus&#xA;  -s, --outscale       The final upsampling scale of the image. Default: 4&#xA;  --suffix             Suffix of the restored image. Default: out&#xA;  -t, --tile           Tile size, 0 for no tile during testing. Default: 0&#xA;  --face_enhance       Whether to use GFPGAN to enhance face. Default: False&#xA;  --fp32               Use fp32 precision during inference. Default: fp16 (half precision).&#xA;  --ext                Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Inference general images&lt;/h4&gt; &#xA;&lt;p&gt;Download pre-trained models: &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth&#34;&gt;RealESRGAN_x4plus.pth&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inference!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference_realesrgan.py -n RealESRGAN_x4plus -i inputs --face_enhance&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Results are in the &lt;code&gt;results&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;h4&gt;Inference anime images&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xinntao/public-figures/master/Real-ESRGAN/cmp_realesrgan_anime_1.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Pre-trained models: &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth&#34;&gt;RealESRGAN_x4plus_anime_6B&lt;/a&gt;&lt;br&gt; More details and comparisons with &lt;a href=&#34;https://github.com/nihui/waifu2x-ncnn-vulkan&#34;&gt;waifu2x&lt;/a&gt; are in &lt;a href=&#34;https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/docs/anime_model.md&#34;&gt;&lt;strong&gt;anime_model.md&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download model&#xA;wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models&#xA;# inference&#xA;python inference_realesrgan.py -n RealESRGAN_x4plus_anime_6B -i inputs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Results are in the &lt;code&gt;results&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@InProceedings{wang2021realesrgan,&#xA;    author    = {Xintao Wang and Liangbin Xie and Chao Dong and Ying Shan},&#xA;    title     = {Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data},&#xA;    booktitle = {International Conference on Computer Vision Workshops (ICCVW)},&#xA;    date      = {2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìß Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any question, please email &lt;code&gt;xintao.wang@outlook.com&lt;/code&gt; or &lt;code&gt;xintaowang@tencent.com&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;!-- Projects that use Real-ESRGAN ---------------------------&gt; &#xA;&lt;h2&gt;üß© Projects that use Real-ESRGAN&lt;/h2&gt; &#xA;&lt;p&gt;If you develop/use Real-ESRGAN in your projects, welcome to let me know.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NCNN-Android: &lt;a href=&#34;https://github.com/tumuyan/RealSR-NCNN-Android&#34;&gt;RealSR-NCNN-Android&lt;/a&gt; by &lt;a href=&#34;https://github.com/tumuyan&#34;&gt;tumuyan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VapourSynth: &lt;a href=&#34;https://github.com/HolyWu/vs-realesrgan&#34;&gt;vs-realesrgan&lt;/a&gt; by &lt;a href=&#34;https://github.com/HolyWu&#34;&gt;HolyWu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;NCNN: &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan&#34;&gt;Real-ESRGAN-ncnn-vulkan&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;strong&gt;GUI&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AaronFeng753/Waifu2x-Extension-GUI&#34;&gt;Waifu2x-Extension-GUI&lt;/a&gt; by &lt;a href=&#34;https://github.com/AaronFeng753&#34;&gt;AaronFeng753&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Justin62628/Squirrel-RIFE&#34;&gt;Squirrel-RIFE&lt;/a&gt; by &lt;a href=&#34;https://github.com/Justin62628&#34;&gt;Justin62628&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/scifx/Real-GUI&#34;&gt;Real-GUI&lt;/a&gt; by &lt;a href=&#34;https://github.com/scifx&#34;&gt;scifx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/net2cn/Real-ESRGAN_GUI&#34;&gt;Real-ESRGAN_GUI&lt;/a&gt; by &lt;a href=&#34;https://github.com/net2cn&#34;&gt;net2cn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WGzeyu/Real-ESRGAN-EGUI&#34;&gt;Real-ESRGAN-EGUI&lt;/a&gt; by &lt;a href=&#34;https://github.com/WGzeyu&#34;&gt;WGzeyu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/shangar21/anime_upscaler&#34;&gt;anime_upscaler&lt;/a&gt; by &lt;a href=&#34;https://github.com/shangar21&#34;&gt;shangar21&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ó Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Thanks for all the contributors.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AK391&#34;&gt;AK391&lt;/a&gt;: Integrate RealESRGAN to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/Real-ESRGAN&#34;&gt;Gradio Web Demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Asiimoviet&#34;&gt;Asiimoviet&lt;/a&gt;: Translate the README.md to Chinese (‰∏≠Êñá).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/2ji3150&#34;&gt;2ji3150&lt;/a&gt;: Thanks for the &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/issues/131&#34;&gt;detailed and valuable feedbacks/suggestions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Jared-02&#34;&gt;Jared-02&lt;/a&gt;: Translate the Training.md to Chinese (‰∏≠Êñá).&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>