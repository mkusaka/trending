<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-01T01:43:18Z</updated>
  <subtitle>Monthly Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ItzCrazyKns/Perplexica</title>
    <updated>2024-06-01T01:43:18Z</updated>
    <id>tag:github.com,2024-06-01:/ItzCrazyKns/Perplexica</id>
    <link href="https://github.com/ItzCrazyKns/Perplexica" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Perplexica is an AI-powered search engine. It is an Open source alternative to Perplexity AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ðŸš€ Perplexica - An AI-powered search engine ðŸ”Ž &#xA; &lt;!-- omit in toc --&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/.assets/perplexica-screenshot.png&#34; alt=&#34;preview&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#preview&#34;&gt;Preview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#getting-started-with-docker-recommended&#34;&gt;Getting Started with Docker (Recommended)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#non-docker-installation&#34;&gt;Non-Docker Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#ollama-connection-errors&#34;&gt;Ollama connection errors&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#using-as-a-search-engine&#34;&gt;Using as a Search Engine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#one-click-deployment&#34;&gt;One-Click Deployment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#upcoming-features&#34;&gt;Upcoming Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#support-us&#34;&gt;Support Us&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#donations&#34;&gt;Donations&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#contribution&#34;&gt;Contribution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#help-and-support&#34;&gt;Help and Support&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Perplexica is an open-source AI-powered searching tool or an AI-powered search engine that goes deep into the internet to find answers. Inspired by Perplexity AI, it&#39;s an open-source option that not just searches the web but understands your questions. It uses advanced machine learning algorithms like similarity searching and embeddings to refine results and provides clear answers with sources cited.&lt;/p&gt; &#xA;&lt;p&gt;Using SearxNG to stay current and fully open source, Perplexica ensures you always get the most up-to-date information without compromising your privacy.&lt;/p&gt; &#xA;&lt;p&gt;Want to know more about its architecture and how it works? You can read it &lt;a href=&#34;https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/architecture/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Preview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/.assets/perplexica-preview.gif&#34; alt=&#34;video-preview&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local LLMs&lt;/strong&gt;: You can make use local LLMs such as Llama3 and Mixtral using Ollama.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Two Main Modes:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Copilot Mode:&lt;/strong&gt; (In development) Boosts search by generating different queries to find more relevant internet sources. Like normal search instead of just using the context by SearxNG, it visits the top matches and tries to find relevant sources to the user&#39;s query directly from the page.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Normal Mode:&lt;/strong&gt; Processes your query and performs a web search.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Focus Modes:&lt;/strong&gt; Special modes to better answer specific types of questions. Perplexica currently has 6 focus modes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;All Mode:&lt;/strong&gt; Searches the entire web to find the best results.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Writing Assistant Mode:&lt;/strong&gt; Helpful for writing tasks that does not require searching the web.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Academic Search Mode:&lt;/strong&gt; Finds articles and papers, ideal for academic research.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;YouTube Search Mode:&lt;/strong&gt; Finds YouTube videos based on the search query.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Wolfram Alpha Search Mode:&lt;/strong&gt; Answers queries that need calculations or data analysis using Wolfram Alpha.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Reddit Search Mode:&lt;/strong&gt; Searches Reddit for discussions and opinions related to the query.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Current Information:&lt;/strong&gt; Some search tools might give you outdated info because they use data from crawling bots and convert them into embeddings and store them in a index. Unlike them, Perplexica uses SearxNG, a metasearch engine to get the results and rerank and get the most relevant source out of it, ensuring you always get the latest information without the overhead of daily data updates.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It has many more features like image and video search. Some of the planned features are mentioned in &lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#upcoming-features&#34;&gt;upcoming features&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;There are mainly 2 ways of installing Perplexica - With Docker, Without Docker. Using Docker is highly recommended.&lt;/p&gt; &#xA;&lt;h3&gt;Getting Started with Docker (Recommended)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure Docker is installed and running on your system.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the Perplexica repository:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ItzCrazyKns/Perplexica.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;After cloning, navigate to the directory containing the project files.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Rename the &lt;code&gt;sample.config.toml&lt;/code&gt; file to &lt;code&gt;config.toml&lt;/code&gt;. For Docker setups, you need only fill in the following fields:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;OPENAI&lt;/code&gt;: Your OpenAI API key. &lt;strong&gt;You only need to fill this if you wish to use OpenAI&#39;s models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;OLLAMA&lt;/code&gt;: Your Ollama API URL. You should enter it as &lt;code&gt;http://host.docker.internal:PORT_NUMBER&lt;/code&gt;. If you installed Ollama on port 11434, use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;. For other ports, adjust accordingly. &lt;strong&gt;You need to fill this if you wish to use Ollama&#39;s models instead of OpenAI&#39;s&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;GROQ&lt;/code&gt;: Your Groq API key. &lt;strong&gt;You only need to fill this if you wish to use Groq&#39;s hosted models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can change these after starting Perplexica from the settings dialog.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;SIMILARITY_MEASURE&lt;/code&gt;: The similarity measure to use (This is filled by default; you can leave it as is if you are unsure about it.)&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure you are in the directory containing the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file and execute:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Wait a few minutes for the setup to complete. You can access Perplexica at &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your web browser.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: After the containers are built, you can start Perplexica directly from Docker without having to open a terminal.&lt;/p&gt; &#xA;&lt;h3&gt;Non-Docker Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository and rename the &lt;code&gt;sample.config.toml&lt;/code&gt; file to &lt;code&gt;config.toml&lt;/code&gt; in the root directory. Ensure you complete all required fields in this file.&lt;/li&gt; &#xA; &lt;li&gt;Rename the &lt;code&gt;.env.example&lt;/code&gt; file to &lt;code&gt;.env&lt;/code&gt; in the &lt;code&gt;ui&lt;/code&gt; folder and fill in all necessary fields.&lt;/li&gt; &#xA; &lt;li&gt;After populating the configuration and environment files, run &lt;code&gt;npm i&lt;/code&gt; in both the &lt;code&gt;ui&lt;/code&gt; folder and the root directory.&lt;/li&gt; &#xA; &lt;li&gt;Install the dependencies and then execute &lt;code&gt;npm run build&lt;/code&gt; in both the &lt;code&gt;ui&lt;/code&gt; folder and the root directory.&lt;/li&gt; &#xA; &lt;li&gt;Finally, start both the frontend and the backend by running &lt;code&gt;npm run start&lt;/code&gt; in both the &lt;code&gt;ui&lt;/code&gt; folder and the root directory.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Using Docker is recommended as it simplifies the setup process, especially for managing environment variables and dependencies.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/installation&#34;&gt;installation documentation&lt;/a&gt; for more information like exposing it your network, etc.&lt;/p&gt; &#xA;&lt;h3&gt;Ollama connection errors&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re facing an Ollama connection error, it is often related to the backend not being able to connect to Ollama&#39;s API. How can you fix it? You can fix it by updating your Ollama API URL in the settings menu to the following:&lt;/p&gt; &#xA;&lt;p&gt;On Windows: &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;br&gt; On Mac: &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;br&gt; On Linux: &lt;code&gt;http://private_ip_of_computer_hosting_ollama:11434&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;You need to edit the ports accordingly.&lt;/p&gt; &#xA;&lt;h2&gt;Using as a Search Engine&lt;/h2&gt; &#xA;&lt;p&gt;If you wish to use Perplexica as an alternative to traditional search engines like Google or Bing, or if you want to add a shortcut for quick access from your browser&#39;s search bar, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open your browser&#39;s settings.&lt;/li&gt; &#xA; &lt;li&gt;Navigate to the &#39;Search Engines&#39; section.&lt;/li&gt; &#xA; &lt;li&gt;Add a new site search with the following URL: &lt;code&gt;http://localhost:3000/?q=%s&lt;/code&gt;. Replace &lt;code&gt;localhost&lt;/code&gt; with your IP address or domain name, and &lt;code&gt;3000&lt;/code&gt; with the port number if Perplexica is not hosted locally.&lt;/li&gt; &#xA; &lt;li&gt;Click the add button. Now, you can use Perplexica directly from your browser&#39;s search bar.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;One-Click Deployment&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://repocloud.io/details/?app_id=267&#34;&gt;&lt;img src=&#34;https://d16t0pc4846x52.cloudfront.net/deploylobe.svg?sanitize=true&#34; alt=&#34;Deploy to RepoCloud&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Upcoming Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Finalizing Copilot Mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add settings page&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Adding support for local LLMs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Adding Discover and History Saving features&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Introducing various Focus Modes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Support Us&lt;/h2&gt; &#xA;&lt;p&gt;If you find Perplexica useful, consider giving us a star on GitHub. This helps more people discover Perplexica and supports the development of new features. Your support is greatly appreciated.&lt;/p&gt; &#xA;&lt;h3&gt;Donations&lt;/h3&gt; &#xA;&lt;p&gt;We also accept donations to help sustain our project. If you would like to contribute, you can use the following button to make a donation in cryptocurrency. Thank you for your support!&lt;/p&gt; &#xA;&lt;a href=&#34;https://nowpayments.io/donation?api_key=RFFKJH1-GRR4DQG-HFV1DZP-00G6MMK&amp;amp;source=lk_donation&amp;amp;medium=referral&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://nowpayments.io/images/embeds/donation-button-white.svg?sanitize=true&#34; alt=&#34;Crypto donation button by NOWPayments&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Perplexica is built on the idea that AI and large language models should be easy for everyone to use. If you find bugs or have ideas, please share them in via GitHub Issues. For more information on contributing to Perplexica you can read the &lt;a href=&#34;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; file to learn more about Perplexica and how you can contribute to it.&lt;/p&gt; &#xA;&lt;h2&gt;Help and Support&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or feedback, please feel free to reach out to us. You can create an issue on GitHub or join our Discord server. There, you can connect with other users, share your experiences and reviews, and receive more personalized help. &lt;a href=&#34;https://discord.gg/EFwsmQDgAu&#34;&gt;Click here&lt;/a&gt; to join the Discord server. To discuss matters outside of regular support, feel free to contact me on Discord at &lt;code&gt;itzcrazykns&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thank you for exploring Perplexica, the AI-powered search engine designed to enhance your search experience. We are constantly working to improve Perplexica and expand its capabilities. We value your feedback and contributions which help us make Perplexica even better. Don&#39;t forget to check back for updates and new features!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mendableai/firecrawl</title>
    <updated>2024-06-01T01:43:18Z</updated>
    <id>tag:github.com,2024-06-01:/mendableai/firecrawl</id>
    <link href="https://github.com/mendableai/firecrawl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ðŸ”¥ Turn entire websites into LLM-ready markdown or structured data. Scrape, crawl and extract with a single API.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ðŸ”¥ Firecrawl&lt;/h1&gt; &#xA;&lt;p&gt;Crawl and convert any website into LLM-ready markdown. Built by &lt;a href=&#34;https://mendable.ai?ref=gfirecrawl&#34;&gt;Mendable.ai&lt;/a&gt; and the firecrawl community.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;This repository is in its early development stages. We are still merging custom modules in the mono repo. It&#39;s not completely yet ready for full self-host deployment, but you can already run it locally.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is Firecrawl?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://firecrawl.dev?ref=github&#34;&gt;Firecrawl&lt;/a&gt; is an API service that takes a URL, crawls it, and converts it into clean markdown or structured data. We crawl all accessible subpages and give you clean data for each. No sitemap required.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Pst. hey, you, join our stargazers :)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/mendableai/firecrawl/assets/44934913/53c4483a-0f0e-40c6-bd84-153a07f94d29&#34; width=&#34;200&#34;&gt; &#xA;&lt;h2&gt;How to use it?&lt;/h2&gt; &#xA;&lt;p&gt;We provide an easy to use API with our hosted version. You can find the playground and documentation &lt;a href=&#34;https://firecrawl.dev/playground&#34;&gt;here&lt;/a&gt;. You can also self host the backend if you&#39;d like.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://firecrawl.dev/playground&#34;&gt;API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/mendableai/firecrawl/tree/main/apps/python-sdk&#34;&gt;Python SDK&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/mendableai/firecrawl/tree/main/apps/js-sdk&#34;&gt;Node SDK&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://python.langchain.com/docs/integrations/document_loaders/firecrawl/&#34;&gt;Langchain Integration ðŸ¦œðŸ”—&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo/#using-firecrawl-reader&#34;&gt;Llama Index Integration ðŸ¦™&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl&#34;&gt;Langchain JS Integration ðŸ¦œðŸ”—&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Want an SDK or Integration? Let us know by opening an issue.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To run locally, refer to guide &lt;a href=&#34;https://github.com/mendableai/firecrawl/raw/main/CONTRIBUTING.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;API Key&lt;/h3&gt; &#xA;&lt;p&gt;To use the API, you need to sign up on &lt;a href=&#34;https://firecrawl.dev&#34;&gt;Firecrawl&lt;/a&gt; and get an API key.&lt;/p&gt; &#xA;&lt;h3&gt;Crawling&lt;/h3&gt; &#xA;&lt;p&gt;Used to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST https://api.firecrawl.dev/v0/crawl \&#xA;    -H &#39;Content-Type: application/json&#39; \&#xA;    -H &#39;Authorization: Bearer YOUR_API_KEY&#39; \&#xA;    -d &#39;{&#xA;      &#34;url&#34;: &#34;https://mendable.ai&#34;&#xA;    }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Returns a jobId&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{ &#34;jobId&#34;: &#34;1234-5678-9101&#34; }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Check Crawl Job&lt;/h3&gt; &#xA;&lt;p&gt;Used to check the status of a crawl job and get its result.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X GET https://api.firecrawl.dev/v0/crawl/status/1234-5678-9101 \&#xA;  -H &#39;Content-Type: application/json&#39; \&#xA;  -H &#39;Authorization: Bearer YOUR_API_KEY&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;status&#34;: &#34;completed&#34;,&#xA;  &#34;current&#34;: 22,&#xA;  &#34;total&#34;: 22,&#xA;  &#34;data&#34;: [&#xA;    {&#xA;      &#34;content&#34;: &#34;Raw Content &#34;,&#xA;      &#34;markdown&#34;: &#34;# Markdown Content&#34;,&#xA;      &#34;provider&#34;: &#34;web-scraper&#34;,&#xA;      &#34;metadata&#34;: {&#xA;        &#34;title&#34;: &#34;Mendable | AI for CX and Sales&#34;,&#xA;        &#34;description&#34;: &#34;AI for CX and Sales&#34;,&#xA;        &#34;language&#34;: null,&#xA;        &#34;sourceURL&#34;: &#34;https://www.mendable.ai/&#34;&#xA;      }&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scraping&lt;/h3&gt; &#xA;&lt;p&gt;Used to scrape a URL and get its content.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST https://api.firecrawl.dev/v0/scrape \&#xA;    -H &#39;Content-Type: application/json&#39; \&#xA;    -H &#39;Authorization: Bearer YOUR_API_KEY&#39; \&#xA;    -d &#39;{&#xA;      &#34;url&#34;: &#34;https://mendable.ai&#34;&#xA;    }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;success&#34;: true,&#xA;  &#34;data&#34;: {&#xA;    &#34;content&#34;: &#34;Raw Content &#34;,&#xA;    &#34;markdown&#34;: &#34;# Markdown Content&#34;,&#xA;    &#34;provider&#34;: &#34;web-scraper&#34;,&#xA;    &#34;metadata&#34;: {&#xA;      &#34;title&#34;: &#34;Mendable | AI for CX and Sales&#34;,&#xA;      &#34;description&#34;: &#34;AI for CX and Sales&#34;,&#xA;      &#34;language&#34;: null,&#xA;      &#34;sourceURL&#34;: &#34;https://www.mendable.ai/&#34;&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Search (Beta)&lt;/h3&gt; &#xA;&lt;p&gt;Used to search the web, get the most relevant results, scrape each page and return the markdown.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST https://api.firecrawl.dev/v0/search \&#xA;    -H &#39;Content-Type: application/json&#39; \&#xA;    -H &#39;Authorization: Bearer YOUR_API_KEY&#39; \&#xA;    -d &#39;{&#xA;      &#34;query&#34;: &#34;firecrawl&#34;,&#xA;      &#34;pageOptions&#34;: {&#xA;        &#34;fetchPageContent&#34;: true // false for a fast serp api&#xA;      }&#xA;    }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;success&#34;: true,&#xA;  &#34;data&#34;: [&#xA;    {&#xA;      &#34;url&#34;: &#34;https://mendable.ai&#34;,&#xA;      &#34;markdown&#34;: &#34;# Markdown Content&#34;,&#xA;      &#34;provider&#34;: &#34;web-scraper&#34;,&#xA;      &#34;metadata&#34;: {&#xA;        &#34;title&#34;: &#34;Mendable | AI for CX and Sales&#34;,&#xA;        &#34;description&#34;: &#34;AI for CX and Sales&#34;,&#xA;        &#34;language&#34;: null,&#xA;        &#34;sourceURL&#34;: &#34;https://www.mendable.ai/&#34;&#xA;      }&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Intelligent Extraction (Beta)&lt;/h3&gt; &#xA;&lt;p&gt;Used to extract structured data from scraped pages.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST https://api.firecrawl.dev/v0/scrape \&#xA;    -H &#39;Content-Type: application/json&#39; \&#xA;    -H &#39;Authorization: Bearer YOUR_API_KEY&#39; \&#xA;    -d &#39;{&#xA;      &#34;url&#34;: &#34;https://www.mendable.ai/&#34;,&#xA;      &#34;extractorOptions&#34;: {&#xA;        &#34;mode&#34;: &#34;llm-extraction&#34;,&#xA;        &#34;extractionPrompt&#34;: &#34;Based on the information on the page, extract the information from the schema. &#34;,&#xA;        &#34;extractionSchema&#34;: {&#xA;          &#34;type&#34;: &#34;object&#34;,&#xA;          &#34;properties&#34;: {&#xA;            &#34;company_mission&#34;: {&#xA;                      &#34;type&#34;: &#34;string&#34;&#xA;            },&#xA;            &#34;supports_sso&#34;: {&#xA;                      &#34;type&#34;: &#34;boolean&#34;&#xA;            },&#xA;            &#34;is_open_source&#34;: {&#xA;                      &#34;type&#34;: &#34;boolean&#34;&#xA;            },&#xA;            &#34;is_in_yc&#34;: {&#xA;                      &#34;type&#34;: &#34;boolean&#34;&#xA;            }&#xA;          },&#xA;          &#34;required&#34;: [&#xA;            &#34;company_mission&#34;,&#xA;            &#34;supports_sso&#34;,&#xA;            &#34;is_open_source&#34;,&#xA;            &#34;is_in_yc&#34;&#xA;          ]&#xA;        }&#xA;      }&#xA;    }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;success&#34;: true,&#xA;    &#34;data&#34;: {&#xA;      &#34;content&#34;: &#34;Raw Content&#34;,&#xA;      &#34;metadata&#34;: {&#xA;        &#34;title&#34;: &#34;Mendable&#34;,&#xA;        &#34;description&#34;: &#34;Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide&#34;,&#xA;        &#34;robots&#34;: &#34;follow, index&#34;,&#xA;        &#34;ogTitle&#34;: &#34;Mendable&#34;,&#xA;        &#34;ogDescription&#34;: &#34;Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide&#34;,&#xA;        &#34;ogUrl&#34;: &#34;https://mendable.ai/&#34;,&#xA;        &#34;ogImage&#34;: &#34;https://mendable.ai/mendable_new_og1.png&#34;,&#xA;        &#34;ogLocaleAlternate&#34;: [],&#xA;        &#34;ogSiteName&#34;: &#34;Mendable&#34;,&#xA;        &#34;sourceURL&#34;: &#34;https://mendable.ai/&#34;&#xA;      },&#xA;      &#34;llm_extraction&#34;: {&#xA;        &#34;company_mission&#34;: &#34;Train a secure AI on your technical resources that answers customer and employee questions so your team doesn&#39;t have to&#34;,&#xA;        &#34;supports_sso&#34;: true,&#xA;        &#34;is_open_source&#34;: false,&#xA;        &#34;is_in_yc&#34;: true&#xA;      }&#xA;    }&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using Python SDK&lt;/h2&gt; &#xA;&lt;h3&gt;Installing Python SDK&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install firecrawl-py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Crawl a website&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from firecrawl import FirecrawlApp&#xA;&#xA;app = FirecrawlApp(api_key=&#34;YOUR_API_KEY&#34;)&#xA;&#xA;crawl_result = app.crawl_url(&#39;mendable.ai&#39;, {&#39;crawlerOptions&#39;: {&#39;excludes&#39;: [&#39;blog/*&#39;]}})&#xA;&#xA;# Get the markdown&#xA;for result in crawl_result:&#xA;    print(result[&#39;markdown&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scraping a URL&lt;/h3&gt; &#xA;&lt;p&gt;To scrape a single URL, use the &lt;code&gt;scrape_url&lt;/code&gt; method. It takes the URL as a parameter and returns the scraped data as a dictionary.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;url = &#39;https://example.com&#39;&#xA;scraped_data = app.scrape_url(url)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Extracting structured data from a URL&lt;/h3&gt; &#xA;&lt;p&gt;With LLM extraction, you can easily extract structured data from any URL. We support pydanti schemas to make it easier for you too. Here is how you to use it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class ArticleSchema(BaseModel):&#xA;    title: str&#xA;    points: int &#xA;    by: str&#xA;    commentsURL: str&#xA;&#xA;class TopArticlesSchema(BaseModel):&#xA;    top: List[ArticleSchema] = Field(..., max_items=5, description=&#34;Top 5 stories&#34;)&#xA;&#xA;data = app.scrape_url(&#39;https://news.ycombinator.com&#39;, {&#xA;    &#39;extractorOptions&#39;: {&#xA;        &#39;extractionSchema&#39;: TopArticlesSchema.model_json_schema(),&#xA;        &#39;mode&#39;: &#39;llm-extraction&#39;&#xA;    },&#xA;    &#39;pageOptions&#39;:{&#xA;        &#39;onlyMainContent&#39;: True&#xA;    }&#xA;})&#xA;print(data[&#34;llm_extraction&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Search for a query&lt;/h3&gt; &#xA;&lt;p&gt;Performs a web search, retrieve the top results, extract data from each page, and returns their markdown.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;query = &#39;What is Mendable?&#39;&#xA;search_result = app.search(query)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using the Node SDK&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;To install the Firecrawl Node SDK, you can use npm:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install @mendable/firecrawl-js&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get an API key from &lt;a href=&#34;https://firecrawl.dev&#34;&gt;firecrawl.dev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Set the API key as an environment variable named &lt;code&gt;FIRECRAWL_API_KEY&lt;/code&gt; or pass it as a parameter to the &lt;code&gt;FirecrawlApp&lt;/code&gt; class.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Scraping a URL&lt;/h3&gt; &#xA;&lt;p&gt;To scrape a single URL with error handling, use the &lt;code&gt;scrapeUrl&lt;/code&gt; method. It takes the URL as a parameter and returns the scraped data as a dictionary.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;try {&#xA;  const url = &#39;https://example.com&#39;;&#xA;  const scrapedData = await app.scrapeUrl(url);&#xA;  console.log(scrapedData);&#xA;&#xA;} catch (error) {&#xA;  console.error(&#xA;    &#39;Error occurred while scraping:&#39;,&#xA;    error.message&#xA;  );&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Crawling a Website&lt;/h3&gt; &#xA;&lt;p&gt;To crawl a website with error handling, use the &lt;code&gt;crawlUrl&lt;/code&gt; method. It takes the starting URL and optional parameters as arguments. The &lt;code&gt;params&lt;/code&gt; argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;const crawlUrl = &#39;https://example.com&#39;;&#xA;const params = {&#xA;  crawlerOptions: {&#xA;    excludes: [&#39;blog/&#39;],&#xA;    includes: [], // leave empty for all pages&#xA;    limit: 1000,&#xA;  },&#xA;  pageOptions: {&#xA;    onlyMainContent: true&#xA;  }&#xA;};&#xA;const waitUntilDone = true;&#xA;const timeout = 5;&#xA;const crawlResult = await app.crawlUrl(&#xA;  crawlUrl,&#xA;  params,&#xA;  waitUntilDone,&#xA;  timeout&#xA;);&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Checking Crawl Status&lt;/h3&gt; &#xA;&lt;p&gt;To check the status of a crawl job with error handling, use the &lt;code&gt;checkCrawlStatus&lt;/code&gt; method. It takes the job ID as a parameter and returns the current status of the crawl job.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;const status = await app.checkCrawlStatus(jobId);&#xA;console.log(status);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Extracting structured data from a URL&lt;/h3&gt; &#xA;&lt;p&gt;With LLM extraction, you can easily extract structured data from any URL. We support zod schema to make it easier for you too. Here is how you to use it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;import FirecrawlApp from &#34;@mendable/firecrawl-js&#34;;&#xA;import { z } from &#34;zod&#34;;&#xA;&#xA;const app = new FirecrawlApp({&#xA;  apiKey: &#34;fc-YOUR_API_KEY&#34;,&#xA;});&#xA;&#xA;// Define schema to extract contents into&#xA;const schema = z.object({&#xA;  top: z&#xA;    .array(&#xA;      z.object({&#xA;        title: z.string(),&#xA;        points: z.number(),&#xA;        by: z.string(),&#xA;        commentsURL: z.string(),&#xA;      })&#xA;    )&#xA;    .length(5)&#xA;    .describe(&#34;Top 5 stories on Hacker News&#34;),&#xA;});&#xA;&#xA;const scrapeResult = await app.scrapeUrl(&#34;https://news.ycombinator.com&#34;, {&#xA;  extractorOptions: { extractionSchema: schema },&#xA;});&#xA;&#xA;console.log(scrapeResult.data[&#34;llm_extraction&#34;]);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Search for a query&lt;/h3&gt; &#xA;&lt;p&gt;With the &lt;code&gt;search&lt;/code&gt; method, you can search for a query in a search engine and get the top results along with the page content for each result. The method takes the query as a parameter and returns the search results.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;const query = &#39;what is mendable?&#39;;&#xA;const searchResults = await app.search(query, {&#xA;  pageOptions: {&#xA;    fetchPageContent: true // Fetch the page content for each search result&#xA;  }&#xA;});&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We love contributions! Please read our &lt;a href=&#34;https://raw.githubusercontent.com/mendableai/firecrawl/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; before submitting a pull request.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;It is the sole responsibility of the end users to respect websites&#39; policies when scraping, searching and crawling with Firecrawl. Users are advised to adhere to the applicable privacy policies and terms of use of the websites prior to initiating any scraping activities. By default, Firecrawl respects the directives specified in the websites&#39; robots.txt files when crawling. By utilizing Firecrawl, you expressly agree to comply with these conditions.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>KindXiaoming/pykan</title>
    <updated>2024-06-01T01:43:18Z</updated>
    <id>tag:github.com,2024-06-01:/KindXiaoming/pykan</id>
    <link href="https://github.com/KindXiaoming/pykan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kolmogorov Arnold Networks&lt;/p&gt;&lt;hr&gt;&lt;img width=&#34;600&#34; alt=&#34;kan_plot&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/a2d2d225-b4d2-4c1e-823e-bc45c7ea96f9&#34;&gt; &#xA;&lt;h1&gt;Kolmogorov-Arnold Networks (KANs)&lt;/h1&gt; &#xA;&lt;p&gt;This is the github repo for the paper &lt;a href=&#34;https://arxiv.org/abs/2404.19756&#34;&gt;&#34;KAN: Kolmogorov-Arnold Networks&#34;&lt;/a&gt;. Find the documentation &lt;a href=&#34;https://kindxiaoming.github.io/pykan/&#34;&gt;here&lt;/a&gt;. Here&#39;s &lt;a href=&#34;https://github.com/KindXiaoming/pykan?tab=readme-ov-file#authors-note&#34;&gt;author&#39;s note&lt;/a&gt; responding to current hype of KANs.&lt;/p&gt; &#xA;&lt;p&gt;Kolmogorov-Arnold Networks (KANs) are promising alternatives of Multi-Layer Perceptrons (MLPs). KANs have strong mathematical foundations just like MLPs: MLPs are based on the universal approximation theorem, while KANs are based on Kolmogorov-Arnold representation theorem. KANs and MLPs are dual: KANs have activation functions on edges, while MLPs have activation functions on nodes. This simple change makes KANs better (sometimes much better!) than MLPs in terms of both model &lt;strong&gt;accuracy&lt;/strong&gt; and &lt;strong&gt;interpretability&lt;/strong&gt;. A quick intro of KANs &lt;a href=&#34;https://kindxiaoming.github.io/pykan/intro.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img width=&#34;1163&#34; alt=&#34;mlp_kan_compare&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/695adc2d-0d0b-4e4b-bcff-db2c8070f841&#34;&gt; &#xA;&lt;h2&gt;Accuracy&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;KANs have faster scaling than MLPs. KANs have better accuracy than MLPs with fewer parameters.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please set &lt;code&gt;torch.set_default_dtype(torch.float64)&lt;/code&gt; if you want high precision.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 1: fitting symbolic formulas&lt;/strong&gt; &lt;img width=&#34;1824&#34; alt=&#34;Screenshot 2024-04-30 at 10 55 30&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/e1fc3dcc-c1f6-49d5-b58e-79ff7b98a49b&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 2: fitting special functions&lt;/strong&gt; &lt;img width=&#34;1544&#34; alt=&#34;Screenshot 2024-04-30 at 11 07 20&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/b2124337-cabf-4e00-9690-938e84058a91&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 3: PDE solving&lt;/strong&gt; &lt;img width=&#34;1665&#34; alt=&#34;Screenshot 2024-04-30 at 10 57 25&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/5da94412-c409-45d1-9a60-9086e11d6ccc&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 4: avoid catastrophic forgetting&lt;/strong&gt; &lt;img width=&#34;1652&#34; alt=&#34;Screenshot 2024-04-30 at 11 04 36&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/57d81de6-7cff-4e55-b8f9-c4768ace2c77&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Interpretability&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;KANs can be intuitively visualized. KANs offer interpretability and interactivity that MLPs cannot provide. We can use KANs to potentially discover new scientific laws.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 1: Symbolic formulas&lt;/strong&gt; &lt;img width=&#34;1510&#34; alt=&#34;Screenshot 2024-04-30 at 11 04 56&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/3cfd1ca2-cd3e-4396-845e-ef8f3a7c55ef&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 2: Discovering mathematical laws of knots&lt;/strong&gt; &lt;img width=&#34;1443&#34; alt=&#34;Screenshot 2024-04-30 at 11 05 25&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/80451ac2-c5fd-45b9-89a7-1637ba8145af&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 3: Discovering physical laws of Anderson localization&lt;/strong&gt; &lt;img width=&#34;1295&#34; alt=&#34;Screenshot 2024-04-30 at 11 05 53&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/8ee507a0-d194-44a9-8837-15d7f5984301&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 4: Training of a three-layer KAN&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/e9f215c7-a393-46b9-8528-c906878f015e&#34; alt=&#34;kan_training_low_res&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Pykan can be installed via PyPI or directly from GitHub.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pre-requisites:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Python 3.9.7 or higher&#xA;pip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Installation via github&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m venv pykan-env&#xA;source pykan-env/bin/activate  # On Windows use `pykan-env\Scripts\activate`&#xA;pip install git+https://github.com/KindXiaoming/pykan.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Installation via PyPI:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m venv pykan-env&#xA;source pykan-env/bin/activate  # On Windows use `pykan-env\Scripts\activate`&#xA;pip install pykan&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Requirements&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# python==3.9.7&#xA;matplotlib==3.6.2&#xA;numpy==1.24.4&#xA;scikit_learn==1.1.3&#xA;setuptools==65.5.0&#xA;sympy==1.11.1&#xA;torch==2.2.2&#xA;tqdm==4.66.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After activating the virtual environment, you can install specific package requirements as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optional: Conda Environment Setup&lt;/strong&gt; For those who prefer using Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create --name pykan-env python=3.9.7&#xA;conda activate pykan-env&#xA;pip install git+https://github.com/KindXiaoming/pykan.git  # For GitHub installation&#xA;# or&#xA;pip install pykan  # For PyPI installation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Computation requirements&lt;/h2&gt; &#xA;&lt;p&gt;Examples in &lt;a href=&#34;https://raw.githubusercontent.com/KindXiaoming/pykan/master/tutorials&#34;&gt;tutorials&lt;/a&gt; are runnable on a single CPU typically less than 10 minutes. All examples in the paper are runnable on a single CPU in less than one day. Training KANs for PDE is the most expensive and may take hours to days on a single CPU. We use CPUs to train our models because we carried out parameter sweeps (both for MLPs and KANs) to obtain Pareto Frontiers. There are thousands of small models which is why we use CPUs rather than GPUs. Admittedly, our problem scales are smaller than typical machine learning tasks, but are typical for science-related tasks. In case the scale of your task is large, it is advisable to use GPUs.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The documentation can be found &lt;a href=&#34;https://kindxiaoming.github.io/pykan/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Get started with &lt;a href=&#34;https://raw.githubusercontent.com/KindXiaoming/pykan/master/hellokan.ipynb&#34;&gt;hellokan.ipynb&lt;/a&gt; notebook.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;More demos&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;More Notebook tutorials can be found in &lt;a href=&#34;https://raw.githubusercontent.com/KindXiaoming/pykan/master/tutorials&#34;&gt;tutorials&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Advice on hyperparameter tuning&lt;/h2&gt; &#xA;&lt;p&gt;Many intuition about MLPs and other networks may not directy transfer to KANs. So how can I tune the hyperparameters effectively? Here is my general advice based on my experience playing with the problems reported in the paper. Since these problems are relatively small-scale and science-oriented, it is likely that my advice is not suitable to your case. But I want to at least share my experience such that users can have better clues where to start and what to expect from tuning hyperparameters.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Start from a simple setup (small KAN shape, small grid size, small data, no reguralization &lt;code&gt;lamb=0&lt;/code&gt;). This is very different from MLP literature, where people by default use widths of order &lt;code&gt;O(10^2)&lt;/code&gt; or higher. For example, if you have a task with 5 inputs and 1 outputs, I would try something as simple as &lt;code&gt;KAN(width=[5,1,1], grid=3, k=3)&lt;/code&gt;. If it doesn&#39;t work, I would gradually first increase width. If that still doesn&#39;t work, I would consider increasing depth. You don&#39;t need to be this extreme, if you have better understanding about the complexity of your task.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once an acceptable performance is achieved, you could then try refining your KAN (more accurate or more interpretable).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you care about accuracy, try grid extention technique. An example is &lt;a href=&#34;https://kindxiaoming.github.io/pykan/Examples/Example_1_function_fitting.html&#34;&gt;here&lt;/a&gt;. But watch out for overfitting, see below.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you care about interpretability, try sparsifying the network with, e.g., &lt;code&gt;model.train(lamb=0.01)&lt;/code&gt;. It would also be advisable to try increasing lamb gradually. After training with sparsification, plot it, if you see some neurons that are obvious useless, you may call &lt;code&gt;pruned_model = model.prune()&lt;/code&gt; to get the pruned model. You can then further train (either to encourage accuracy or encouarge sparsity), or do symbolic regression.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;I also want to emphasize that accuracy and interpretability (and also parameter efficiency) are not necessarily contradictory, e.g., Figure 2.3 in &lt;a href=&#34;https://arxiv.org/pdf/2404.19756&#34;&gt;our paper&lt;/a&gt;. They can be positively correlated in some cases but in other cases may dispaly some tradeoff. So it would be good not to be greedy and aim for one goal at a time. However, if you have a strong reason why you believe pruning (interpretability) can also help accuracy, you may want to plan ahead, such that even if your end goal is accuracy, you want to push interpretability first.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once you get a quite good result, try increasing data size and have a final run, which should give you even better results!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Disclaimer: Try the simplest thing first is the mindset of physicists, which could be personal/biased but I find this mindset quite effective and make things well-controlled for me. Also, The reason why I tend to choose a small dataset at first is to get faster feedback in the debugging stage (my initial implementation is slow, after all!). The hidden assumption is that a small dataset behaves qualitatively similar to a large dataset, which is not necessarily true in general, but usually true in small-scale problems that I have tried. To know if your data is sufficient, see the next paragraph.&lt;/p&gt; &#xA;&lt;p&gt;Another thing that would be good to keep in mind is that please constantly checking if your model is in underfitting or overfitting regime. If there is a large gap between train/test losses, you probably want to increase data or reduce model (&lt;code&gt;grid&lt;/code&gt; is more important than &lt;code&gt;width&lt;/code&gt;, so first try decreasing &lt;code&gt;grid&lt;/code&gt;, then &lt;code&gt;width&lt;/code&gt;). This is also the reason why I&#39;d love to start from simple models to make sure that the model is first in underfitting regime and then gradually expands to the &#34;Goldilocks zone&#34;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@article{liu2024kan,&#xA;  title={KAN: Kolmogorov-Arnold Networks},&#xA;  author={Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Solja{\v{c}}i{\&#39;c}, Marin and Hou, Thomas Y and Tegmark, Max},&#xA;  journal={arXiv preprint arXiv:2404.19756},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please contact &lt;a href=&#34;mailto:zmliu@mit.edu&#34;&gt;zmliu@mit.edu&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Author&#39;s note&lt;/h2&gt; &#xA;&lt;p&gt;I would like to thank everyone who&#39;s interested in KANs. When I designed KANs and wrote codes, I have math &amp;amp; physics examples (which are quite small scale!) in mind, so did not consider much optimization in efficiency or reusability. It&#39;s so honored to receive this unwarranted attention, which is way beyond my expectation. So I accept any criticism from people complaning about the efficiency and resuability of the codes, my apology. My only hope is that you find &lt;code&gt;model.plot()&lt;/code&gt; fun to play with :).&lt;/p&gt; &#xA;&lt;p&gt;For users who are interested in scientific discoveries and scientific computing (the orginal users intended for), I&#39;m happy to hear your applications and collaborate. This repo will continue remaining mostly for this purpose, probably without signifiant updates for efficiency. In fact, there are already implmentations like &lt;a href=&#34;https://github.com/Blealtan/efficient-kan&#34;&gt;efficientkan&lt;/a&gt; or &lt;a href=&#34;https://github.com/GistNoesis/FourierKAN/&#34;&gt;fouierkan&lt;/a&gt; that look promising for improving efficiency.&lt;/p&gt; &#xA;&lt;p&gt;For users who are machine learning focus, I have to be honest that KANs are likely not a simple plug-in that can be used out-of-the box (yet). Hyperparameters need tuning, and more tricks special to your applications should be introduced. For example, &lt;a href=&#34;https://github.com/WillHua127/GraphKAN-Graph-Kolmogorov-Arnold-Networks&#34;&gt;GraphKAN&lt;/a&gt; suggests that KANs should better be used in latent space (need embedding and unembedding linear layers after inputs and before outputs). &lt;a href=&#34;https://github.com/riiswa/kanrl&#34;&gt;KANRL&lt;/a&gt; suggests that some trainable parameters should better be fixed in reinforcement learning to increase training stability. The extra tricks required by KAN (e.g., grid updates and grid extension) beyond MLPs make it sometimes confusing on how to use them so we should be extra careful, e.g., &lt;a href=&#34;https://www.linkedin.com/feed/update/urn:li:activity:7196684191479070721/&#34;&gt;Prof. George Karniadakis&#39; post on LinkedIn&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/feed/update/urn:li:activity:7197097659017379840/&#34;&gt;my response&lt;/a&gt; is an example.&lt;/p&gt; &#xA;&lt;p&gt;The most common question I&#39;ve been asked lately is whether KANs will be next-gen LLMs. I don&#39;t have good intuition about this. KANs are designed for applications where one cares about high accuracy and/or interpretability. We do care about LLM interpretability for sure, but interpretability can mean wildly different things for LLM and for science. Do we care about high accuracy for LLMs? I don&#39;t know, scaling laws seem to imply so, but probably not too high precision. Also, accuracy can also mean different things for LLM and for science. This subtlety makes it hard to directly transfer conclusions in our paper to LLMs, or machine learning tasks in general. However, I would be very happy if you have enjoyed the high-level idea (learnable activation functions on edges, or interacting with AI for scientific discoveries), which is not necessariy &lt;em&gt;the future&lt;/em&gt;, but can hopefully inspire and impact &lt;em&gt;many possible futures&lt;/em&gt;. As a physicist, the message I want to convey is less of &#34;KANs are great&#34;, but more of &#34;try thinking of current architectures critically and seeking fundamentally different alternatives that can do fun and/or useful stuff&#34;.&lt;/p&gt; &#xA;&lt;p&gt;I would like to welcome people to be critical of KANs, but also to be critical of critiques as well. Practice is the only criterion for testing understanding (å®žè·µæ˜¯æ£€éªŒçœŸç†çš„å”¯ä¸€æ ‡å‡†). We don&#39;t know many things beforehand until they are really tried and shown to be succeeding or failing. As much as I&#39;m willing to see success mode of KANs, I&#39;m equally curious about failure modes of KANs, to better understand the boundaries. KANs and MLPs cannot replace each other (as far as I can tell); they each have advantages in some settings and limitations in others. I would be intrigued by a theoretical framework that encompasses both and could even suggest new alternatives (physicists love unified theories, sorry :).&lt;/p&gt;</summary>
  </entry>
</feed>