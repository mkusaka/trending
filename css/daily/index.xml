<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub CSS Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-21T01:27:56Z</updated>
  <subtitle>Daily Trending of CSS in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Artur-Galstyan/statedict2pytree</title>
    <updated>2024-05-21T01:27:56Z</updated>
    <id>tag:github.com,2024-05-21:/Artur-Galstyan/statedict2pytree</id>
    <link href="https://github.com/Artur-Galstyan/statedict2pytree" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;statedict2pytree&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Artur-Galstyan/statedict2pytree/main/torch2jax.png&#34; alt=&#34;statedict2pytree&#34; title=&#34;A ResNet demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Important&lt;/h2&gt; &#xA;&lt;p&gt;This package is still in its infancy! The code works, but it&#39;s far from perfect. With more and more iterations, it will eventually become stable and well tested. PRs and other contributions are &lt;em&gt;highly&lt;/em&gt; welcome! :)&lt;/p&gt; &#xA;&lt;p&gt;The goal of this package is to simplify the conversion from PyTorch models into JAX PyTrees (which can be used e.g. in Equinox). The way this works is by putting both models side my side and aligning the weights in the right order. Then, all statedict2pytree is doing, is iterating over both lists and matching the weight matrices.&lt;/p&gt; &#xA;&lt;p&gt;Usually, if you &lt;em&gt;declared the fields in the same order as in the PyTorch model&lt;/em&gt;, you don&#39;t have to rearrange anything -- but the option is there if you need it.&lt;/p&gt; &#xA;&lt;p&gt;(Theoretically, you can rearrange the model in any way you like - e.g. last layer as the first layer - as long as the shapes match!)&lt;/p&gt; &#xA;&lt;h2&gt;Shape Matching? What&#39;s that?&lt;/h2&gt; &#xA;&lt;p&gt;Currently, there is no sophisticated shape matching in place. Two matrices are considered &#34;matching&#34; if the product of their shape match. For example:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;(8, 1, 1) and (8, ) match, because (8 _ 1 _ 1 = 8)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install statedict2pytree&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Basic Example&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import equinox as eqx&#xA;import jax&#xA;import torch&#xA;import statedict2pytree as s2p&#xA;&#xA;&#xA;def test_mlp():&#xA;    in_size = 784&#xA;    out_size = 10&#xA;    width_size = 64&#xA;    depth = 2&#xA;    key = jax.random.PRNGKey(22)&#xA;&#xA;    class EqxMLP(eqx.Module):&#xA;        mlp: eqx.nn.MLP&#xA;        batch_norm: eqx.nn.BatchNorm&#xA;&#xA;        def __init__(self, in_size, out_size, width_size, depth, key):&#xA;            self.mlp = eqx.nn.MLP(in_size, out_size, width_size, depth, key=key)&#xA;            self.batch_norm = eqx.nn.BatchNorm(out_size, axis_name=&#34;batch&#34;)&#xA;&#xA;        def __call__(self, x, state):&#xA;            return self.batch_norm(self.mlp(x), state)&#xA;&#xA;    jax_model = EqxMLP(in_size, out_size, width_size, depth, key)&#xA;&#xA;    class TorchMLP(torch.nn.Module):&#xA;        def __init__(self, in_size, out_size, width_size, depth):&#xA;            super(TorchMLP, self).__init__()&#xA;            self.layers = torch.nn.ModuleList()&#xA;            self.layers.append(torch.nn.Linear(in_size, width_size))&#xA;            for _ in range(depth - 1):&#xA;                self.layers.append(torch.nn.Linear(width_size, width_size))&#xA;            self.layers.append(torch.nn.Linear(width_size, out_size))&#xA;            self.batch_norm = torch.nn.BatchNorm1d(out_size)&#xA;&#xA;        def forward(self, x):&#xA;            for layer in self.layers[:-1]:&#xA;                x = torch.relu(layer(x))&#xA;            x = self.batch_norm(self.layers[-1](x))&#xA;            return x&#xA;&#xA;    torch_model = TorchMLP(in_size, out_size, width_size, depth)&#xA;    state_dict = torch_model.state_dict()&#xA;    s2p.start_conversion(jax_model, state_dict)&#xA;&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    test_mlp()&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There exists also a function called &lt;code&gt;s2p.convert&lt;/code&gt; which does the actual conversion:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;class Field(BaseModel):&#xA;    path: str&#xA;    shape: tuple[int, ...]&#xA;&#xA;&#xA;class TorchField(Field):&#xA;    pass&#xA;&#xA;&#xA;class JaxField(Field):&#xA;    type: str&#xA;&#xA;def convert(&#xA;    jax_fields: list[JaxField],&#xA;    torch_fields: list[TorchField],&#xA;    pytree: PyTree,&#xA;    state_dict: dict,&#xA;):&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If your models already have the right &#34;order&#34;, then you might as well use this function directly. Note that the lists &lt;code&gt;jax_fields&lt;/code&gt; and &lt;code&gt;torch_fields&lt;/code&gt; must have the same length and each matching entry must have the same shape!&lt;/p&gt; &#xA;&lt;p&gt;For the full, automatic experience, use &lt;code&gt;autoconvert&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import statedict2pytree as s2p&#xA;&#xA;my_model = Model(...)&#xA;state_dict = ...&#xA;&#xA;model, state = s2p.autoconvert(my_model, state_dict)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will however only work if your PyTree fields have been declared in the same order as they appear in the state dict!&lt;/p&gt;</summary>
  </entry>
</feed>