<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub CSS Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-24T01:32:12Z</updated>
  <subtitle>Daily Trending of CSS in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cocktailpeanut/dalai</title>
    <updated>2023-03-24T01:32:12Z</updated>
    <id>tag:github.com,2023-03-24:/cocktailpeanut/dalai</id>
    <link href="https://github.com/cocktailpeanut/dalai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The simplest way to run LLaMA on your local machine&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dalai&lt;/h1&gt; &#xA;&lt;p&gt;Run LLaMA and Alpaca on your computer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cocktailpeanut/dalai&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-github&#34;&gt;&lt;/i&gt; GitHub&lt;/a&gt; &lt;a href=&#34;https://twitter.com/cocktailpeanut&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-twitter&#34;&gt;&lt;/i&gt; Twitter&lt;/a&gt; &lt;a href=&#34;https://discord.gg/WWfgrzzkCT&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-discord&#34;&gt;&lt;/i&gt; Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;JUST RUN THIS&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/npx2.png&#34; class=&#34;round&#34;&gt; &#xA;&lt;h2&gt;TO GET&lt;/h2&gt; &#xA;&lt;p&gt;Both alpaca and llama working on your computer!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/alpaca.gif&#34; alt=&#34;alpaca.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Powered by &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;, &lt;a href=&#34;https://github.com/shawwn/llama-dl&#34;&gt;llama-dl CDN&lt;/a&gt;, and &lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;alpaca.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hackable web app included&lt;/li&gt; &#xA; &lt;li&gt;Ships with JavaScript API&lt;/li&gt; &#xA; &lt;li&gt;Ships with &lt;a href=&#34;https://socket.io/&#34;&gt;Socket.io&lt;/a&gt; API&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Intro&lt;/h1&gt; &#xA;&lt;h2&gt;1. Cross platform&lt;/h2&gt; &#xA;&lt;p&gt;Dalai runs on all of the following operating systems:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Linux&lt;/li&gt; &#xA; &lt;li&gt;Mac&lt;/li&gt; &#xA; &lt;li&gt;Windows&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;2. Memory Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Runs on most modern computers. Unless your computer is very very old, it should work.&lt;/p&gt; &#xA;&lt;p&gt;According to &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/issues/13&#34;&gt;a llama.cpp discussion thread&lt;/a&gt;, here are the memory requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;7B =&amp;gt; ~4 GB&lt;/li&gt; &#xA; &lt;li&gt;13B =&amp;gt; ~8 GB&lt;/li&gt; &#xA; &lt;li&gt;30B =&amp;gt; ~16 GB&lt;/li&gt; &#xA; &lt;li&gt;65B =&amp;gt; ~32 GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;3. Disk Space Requirements&lt;/h2&gt; &#xA;&lt;h3&gt;Alpaca&lt;/h3&gt; &#xA;&lt;p&gt;Currently 7B and 13B models are available via &lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;alpaca.cpp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;7B&lt;/h4&gt; &#xA;&lt;p&gt;Alpaca comes fully quantized (compressed), and the only space you need for the 7B model is 4.21GB:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/alpaca_7b.png&#34; alt=&#34;alpaca_7b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;13B&lt;/h4&gt; &#xA;&lt;p&gt;Alpaca comes fully quantized (compressed), and the only space you need for the 13B model is 8.14GB:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/alpaca_13b.png&#34; alt=&#34;alpaca_13b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;LLaMA&lt;/h3&gt; &#xA;&lt;p&gt;You need a lot of space for storing the models. &lt;strong&gt;The model name must be one of: 7B, 13B, 30B, and 65B.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You do NOT have to install all models, you can install one by one. Let&#39;s take a look at how much space each model takes up:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTE&lt;/p&gt; &#xA; &lt;p&gt;The following numbers assume that you DO NOT touch the original model files and keep BOTH the original model files AND the quantized versions.&lt;/p&gt; &#xA; &lt;p&gt;You can optimize this if you delete the original models (which are much larger) after installation and keep only the quantized versions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;7B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 31.17GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 4.21GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/7b.png&#34; alt=&#34;7b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;13B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 60.21GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 4.07GB * 2 = 8.14GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/13b.png&#34; alt=&#34;13b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;30B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 150.48GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 5.09GB * 4 = 20.36GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/30b.png&#34; alt=&#34;30b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;65B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 432.64GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 5.11GB * 8 = 40.88GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/65b.png&#34; alt=&#34;65b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Quickstart&lt;/h1&gt; &#xA;&lt;h2&gt;Docker compose&lt;/h2&gt; &#xA;&lt;p&gt;Requires that you have docker installed and running.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose build&#xA;docker compose run dalai npx dalai alpaca install 7B # or a different model&#xA;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will dave the models in the &lt;code&gt;./models&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;p&gt;View the site at &lt;a href=&#34;http://127.0.0.1:3000/&#34;&gt;http://127.0.0.1:3000/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Mac&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install node.js &amp;gt;= 18&lt;/h3&gt; &#xA;&lt;p&gt;If your mac doesn&#39;t have node.js installed yet, make sure to install node.js &amp;gt;= 18&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en/download/&#34; class=&#34;btn&#34;&gt;Install Node.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Step 2.1. Install models&lt;/h3&gt; &#xA;&lt;p&gt;Currently supported engines are &lt;code&gt;llama&lt;/code&gt; and &lt;code&gt;alpaca&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Add alpaca models&lt;/h4&gt; &#xA;&lt;p&gt;To download alpaca models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai alpaca install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Add llama models&lt;/h4&gt; &#xA;&lt;p&gt;To download llama models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to download multiple models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B 13B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now go to step 3.&lt;/p&gt; &#xA;&lt;h3&gt;Step 2.2. Troubleshoot&lt;/h3&gt; &#xA;&lt;p&gt;Normally you don&#39;t need this step, but if running the commands above don&#39;t do anything and immediately end, it means something went wrong because some of the required modules are not installed on your system.&lt;/p&gt; &#xA;&lt;p&gt;In that case, try the following steps:&lt;/p&gt; &#xA;&lt;h4&gt;1. Install homebrew&lt;/h4&gt; &#xA;&lt;p&gt;In case homebrew is not installed on your computer, install it by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;/bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Or you can find the same instruction on the homebrew hompage: &lt;a href=&#34;https://brew.sh/&#34;&gt;https://brew.sh/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2. Install dependencies&lt;/h4&gt; &#xA;&lt;p&gt;Once homebrew is installed, install these dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install cmake&#xA;brew install pkg-config&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Update NPM&lt;/h4&gt; &#xA;&lt;p&gt;Just to make sure we cover every vector, let&#39;s update NPM as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install -g npm@latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now go back to step 2.1 and try running the &lt;code&gt;npx dalai&lt;/code&gt; commands again.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, run the following command to launch the web UI server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Windows&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install Visual Studio&lt;/h3&gt; &#xA;&lt;p&gt;On windows, you need to install Visual Studio before installing Dalai.&lt;/p&gt; &#xA;&lt;p&gt;Press the button below to visit the Visual Studio downloads page and download:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34; class=&#34;btn&#34;&gt;Download Microsoft Visual Studio&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT!!!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;When installing Visual Studio, make sure to check the 3 options as highlighted below:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python development&lt;/li&gt; &#xA; &lt;li&gt;Node.js development&lt;/li&gt; &#xA; &lt;li&gt;Desktop development with C++&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/vs.png&#34; alt=&#34;vs.png&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Step 2.1. Install models&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;On Windows, make sure to run all commands in &lt;strong&gt;cmd&lt;/strong&gt;.&lt;/p&gt; &#xA; &lt;p&gt;DO NOT run in &lt;strong&gt;powershell&lt;/strong&gt;. Powershell has unnecessarily strict permissions and makes the script fail silently.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Currently supported engines are &lt;code&gt;llama&lt;/code&gt; and &lt;code&gt;alpaca&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Install alpaca&lt;/h4&gt; &#xA;&lt;p&gt;To download alpaca models. Open your &lt;code&gt;cmd&lt;/code&gt; application and enter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai alpaca install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Add llama models&lt;/h4&gt; &#xA;&lt;p&gt;To download llama models. Open your &lt;code&gt;cmd&lt;/code&gt; application and enter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to download multiple models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B 13B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Step 2.2. Troubleshoot (optional)&lt;/h3&gt; &#xA;&lt;p&gt;In case above steps fail, try installing Node.js and Python separately.&lt;/p&gt; &#xA;&lt;p&gt;Install Python:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/ftp/python/3.10.10/python-3.10.10-embed-amd64.zip&#34; class=&#34;btn&#34;&gt;Download Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install Node.js &amp;gt;= 18:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en/download/&#34; class=&#34;btn&#34;&gt;Download Node.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;After both have been installed, open powershell and type &lt;code&gt;python&lt;/code&gt; to see if the application exists. And also type &lt;code&gt;node&lt;/code&gt; to see if the application exists as well.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;ve checked that they both exist, try again.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, run the following command to launch the web UI server (Make sure to run in &lt;code&gt;cmd&lt;/code&gt; and not powershell!):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Linux&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;You need to make sure you have the correct version of Python and Node.js installed.&lt;/p&gt; &#xA;&lt;h4&gt;Step 1.1. Python &amp;lt;= 3.10&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pimylifeup.com/installing-python-on-linux/&#34; class=&#34;btn&#34;&gt;Download Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Make sure the version is 3.10 or lower (not 3.11) Python must be 3.10 or below (pytorch and other libraries are not supported yet on the latest)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Step 1.2. Node.js &amp;gt;= 18&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en/download/package-manager/&#34; class=&#34;btn&#34;&gt;Download node.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Make sure the version is 18 or higher&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Step 2.1. Install models&lt;/h3&gt; &#xA;&lt;p&gt;Currently supported engines are &lt;code&gt;llama&lt;/code&gt; and &lt;code&gt;alpaca&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Add alpaca models&lt;/h4&gt; &#xA;&lt;p&gt;To download alpaca models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai alpaca install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Add llama models&lt;/h4&gt; &#xA;&lt;p&gt;To download llama models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to download multiple models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B 13B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2.2. Troubleshoot&lt;/h3&gt; &#xA;&lt;p&gt;In case the model install silently fails or hangs forever, try the following command, and try running the npx command again:&lt;/p&gt; &#xA;&lt;p&gt;On ubuntu/debian/etc.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install build-essential python3-venv -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On fedora/etc.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dnf install make automake gcc gcc-c++ kernel-devel python3-virtualenv -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 3. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, run the following command to launch the web UI server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;API&lt;/h1&gt; &#xA;&lt;p&gt;Dalai is also an NPM package:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;programmatically install&lt;/li&gt; &#xA; &lt;li&gt;locally make requests to the model&lt;/li&gt; &#xA; &lt;li&gt;run a dalai server (powered by socket.io)&lt;/li&gt; &#xA; &lt;li&gt;programmatically make requests to a remote dalai server (via socket.io)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Dalai is an NPM package. You can install it using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install dalai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;1. constructor()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai(home)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;home&lt;/code&gt;: (optional) manually specify the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By default, Dalai automatically stores the entire &lt;code&gt;llama.cpp&lt;/code&gt; repository under &lt;code&gt;~/llama.cpp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;However, often you may already have a &lt;code&gt;llama.cpp&lt;/code&gt; repository somewhere else on your machine and want to just use that folder. In this case you can pass in the &lt;code&gt;home&lt;/code&gt; attribute.&lt;/p&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;h4&gt;Basic&lt;/h4&gt; &#xA;&lt;p&gt;Creates a workspace at &lt;code&gt;~/llama.cpp&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Custom path&lt;/h4&gt; &#xA;&lt;p&gt;Manually set the &lt;code&gt;llama.cpp&lt;/code&gt; path:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai(&#34;/Documents/llama.cpp&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;2. request()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.request(req, callback)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;req&lt;/code&gt;: a request object. made up of the following attributes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;prompt&lt;/code&gt;: &lt;strong&gt;(required)&lt;/strong&gt; The prompt string&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: &lt;strong&gt;(required)&lt;/strong&gt; The model type + model name to query. Takes the following form: &lt;code&gt;&amp;lt;model_type&amp;gt;.&amp;lt;model_name&amp;gt;&lt;/code&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Example: &lt;code&gt;alpaca.7B&lt;/code&gt;, &lt;code&gt;llama.13B&lt;/code&gt;, ...&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;url&lt;/code&gt;: only needed if connecting to a remote dalai server &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;if unspecified, it uses the node.js API to directly run dalai locally&lt;/li&gt; &#xA;     &lt;li&gt;if specified (for example &lt;code&gt;ws://localhost:3000&lt;/code&gt;) it looks for a socket.io endpoint at the URL and connects to it.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;threads&lt;/code&gt;: The number of threads to use (The default is 8 if unspecified)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;n_predict&lt;/code&gt;: The number of tokens to return (The default is 128 if unspecified)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;seed&lt;/code&gt;: The seed. The default is -1 (none)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;top_k&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;top_p&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;repeat_last_n&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;repeat_penalty&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;temp&lt;/code&gt;: temperature&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;batch_size&lt;/code&gt;: batch size&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;skip_end&lt;/code&gt;: by default, every session ends with &lt;code&gt;\n\n&amp;lt;end&amp;gt;&lt;/code&gt;, which can be used as a marker to know when the full response has returned. However sometimes you may not want this suffix. Set &lt;code&gt;skip_end: true&lt;/code&gt; and the response will no longer end with &lt;code&gt;\n\n&amp;lt;end&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;callback&lt;/code&gt;: the streaming callback function that gets called every time the client gets any token response back from the model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;h4&gt;1. Node.js&lt;/h4&gt; &#xA;&lt;p&gt;Using node.js, you just need to initialize a Dalai object with &lt;code&gt;new Dalai()&lt;/code&gt; and then use it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#39;dalai&#39;)&#xA;new Dalai().request({&#xA;  model: &#34;7B&#34;,&#xA;  prompt: &#34;The following is a conversation between a boy and a girl:&#34;,&#xA;}, (token) =&amp;gt; {&#xA;  process.stdout.write(token)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Non node.js (socket.io)&lt;/h4&gt; &#xA;&lt;p&gt;To make use of this in a browser or any other language, you can use thie socket.io API.&lt;/p&gt; &#xA;&lt;h5&gt;Step 1. start a server&lt;/h5&gt; &#xA;&lt;p&gt;First you need to run a Dalai socket server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// server.js&#xA;const Dalai = require(&#39;dalai&#39;)&#xA;new Dalai().serve(3000)     // port 3000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Step 2. connect to the server&lt;/h5&gt; &#xA;&lt;p&gt;Then once the server is running, simply make requests to it by passing the &lt;code&gt;ws://localhost:3000&lt;/code&gt; socket url when initializing the Dalai object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;)&#xA;new Dalai().request({&#xA;  url: &#34;ws://localhost:3000&#34;,&#xA;  model: &#34;7B&#34;,&#xA;  prompt: &#34;The following is a conversation between a boy and a girl:&#34;,&#xA;}, (token) =&amp;gt; {&#xA;  console.log(&#34;token&#34;, token)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;3. serve()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;p&gt;Starts a socket.io server at &lt;code&gt;port&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.serve(port)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;)&#xA;new Dalai().serve(3000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;4. http()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;p&gt;connect with an existing &lt;code&gt;http&lt;/code&gt; instance (The &lt;code&gt;http&lt;/code&gt; npm package)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.http(http)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;http&lt;/code&gt;: The &lt;a href=&#34;https://nodejs.org/api/http.html&#34;&gt;http&lt;/a&gt; object&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;This is useful when you&#39;re trying to plug dalai into an existing node.js web app&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const app = require(&#39;express&#39;)();&#xA;const http = require(&#39;http&#39;).Server(app);&#xA;dalai.http(http)&#xA;http.listen(3000, () =&amp;gt; {&#xA;  console.log(&#34;server started&#34;)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;5. install()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;await dalai.install(model_type, model_name1, model_name2, ...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model_type&lt;/code&gt;: the name of the model. currently supports: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;alpaca&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&#34;llama&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model1&lt;/code&gt;, &lt;code&gt;model2&lt;/code&gt;, ...: the model names to install (&#34;7B&#34;`, &#34;13B&#34;, &#34;30B&#34;, &#34;65B&#34;, etc)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;Install Llama &#34;7B&#34; and &#34;13B&#34; models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;await dalai.install(&#34;llama&#34;, &#34;7B&#34;, &#34;13B&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install alpaca 7B model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;await dalai.install(&#34;alpaca&#34;, &#34;7B&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;6. installed()&lt;/h2&gt; &#xA;&lt;p&gt;returns the array of installed models&lt;/p&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const models = await dalai.installed()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;const models = await dalai.installed()&#xA;console.log(models)     // prints [&#34;7B&#34;, &#34;13B&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!--&#xA;&#xA;---&#xA;&#xA;## 7. download()&#xA;&#xA;Download models.&#xA;&#xA;There are two download options:&#xA;&#xA;1. **LLaMA:** Download the original LLaMA model, convert it, and quantize (compress) it&#xA;2. **LLaMA.zip:** Download the compressed version (generated from step 1 and published on HuggingFace)&#xA;&#xA;### Syntax&#xA;&#xA;```javascript&#xA;await dalai.download(model1, model2, model3, ...)&#xA;```&#xA;&#xA;- `models`: the model names to install. Can be: &#34;7B&#34;`, &#34;13B&#34;, &#34;30B&#34;, &#34;65B&#34;, &#34;7B.zip&#34;, &#34;13B.zip&#34;, &#34;30B.zip&#34;, &#34;65B.zip&#34;&#xA;  - &#34;7B&#34;, &#34;13B&#34;, &#34;30B&#34;, &#34;65B&#34;: download the raw model, convert, and quantize&#xA;  - &#34;7B.zip&#34;, &#34;13B.zip&#34;, &#34;30B.zip&#34;, &#34;65B.zip&#34;: download the quantized model (no need to waste time downloading huge files)&#xA;&#xA;### Examples&#xA;&#xA;Install the &#34;7B&#34; and &#34;13B&#34; models:&#xA;&#xA;&#xA;```javascript&#xA;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;await dalai.install(&#34;7B&#34;, &#34;13B&#34;)&#xA;```&#xA;&#xA;--&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h2&gt;Using a different home folder&lt;/h2&gt; &#xA;&lt;p&gt;By default Dalai uses your home directory to store the entire repository (&lt;code&gt;~/dalai&lt;/code&gt;). However sometimes you may want to store the archive elsewhere.&lt;/p&gt; &#xA;&lt;p&gt;In this case you can call all CLI methods using the &lt;code&gt;--home&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;h3&gt;1. Installing models to a custom path&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B --home ~/test_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Serving from the custom path&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve --home ~/test_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Updating to the latest&lt;/h2&gt; &#xA;&lt;p&gt;To make sure you update to the latest, first find the latest version at &lt;a href=&#34;https://www.npmjs.com/package/dalai&#34;&gt;https://www.npmjs.com/package/dalai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s say the latest version is &lt;code&gt;0.3.0&lt;/code&gt;. To update the dalai version, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai@0.3.0 setup&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Staying up to date&lt;/h2&gt; &#xA;&lt;p&gt;Have questions or feedback? Follow the project through the following outlets:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cocktailpeanut/dalai&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-github&#34;&gt;&lt;/i&gt; GitHub&lt;/a&gt; &lt;a href=&#34;https://twitter.com/cocktailpeanut&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-twitter&#34;&gt;&lt;/i&gt; Twitter&lt;/a&gt; &lt;a href=&#34;https://discord.gg/WWfgrzzkCT&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-discord&#34;&gt;&lt;/i&gt; Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>tiredmala/Roundmoled</title>
    <updated>2023-03-24T01:32:12Z</updated>
    <id>tag:github.com,2023-03-24:/tiredmala/Roundmoled</id>
    <link href="https://github.com/tiredmala/Roundmoled" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Simple but fancy Discord theme inspired by the AMOLED mode from Mobile, mixed with new rounded borders to the main sections of the user interface for a cleaner look. Tested on Powercord and BetterDiscord&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;roundmoled.theme.css&lt;/h1&gt; &#xA;&lt;h3&gt;Rounded corners? YES! TRUE black mode? YES!&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;Installing for BetterDiscord&lt;/h3&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download &lt;strong&gt;roundmoled.theme.css&lt;/strong&gt; (inside the BetterDiscord folder)&lt;/li&gt; &#xA; &lt;li&gt;Paste it into your themes folder (usually found at &lt;code&gt;AppData\Roaming\BetterDiscord\themes&lt;/code&gt; or at &lt;code&gt;~/.config/BetterDiscord/themes/&lt;/code&gt; if you&#39;re on Linux)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;Installing for Powercord&lt;/h3&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bat&#34;&gt;cd powercord/src/Powercord/themes &amp;amp;&amp;amp; git clone https://github.com/Rayrsn/RoundNotmoled &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Theme is easy to edit from the main.css file, everything is very self-explanatory&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/86564702/126011785-2a1aecdb-411a-4336-9dae-5f7d76f7fe72.png&#34; alt=&#34;presentation&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/86564702/126011824-aeb5ea84-2cba-48b7-9777-ec121187484d.png&#34; alt=&#34;presentation2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/86564702/124397086-c4a49100-dcdb-11eb-9e65-9763f3a59f0b.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/86564702/126012314-bd6f09e3-ba56-4762-8d86-04ab38482770.png&#34; alt=&#34;3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/86564702/126011947-afb45408-2163-48b8-8325-bc532716e429.png&#34; alt=&#34;5&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/86564702/124397129-12b99480-dcdc-11eb-8c48-546d2fb1dd46.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/86564702/124397131-164d1b80-dcdc-11eb-8aaa-d0c2e46ece24.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/86564702/126012470-757bf19e-b49d-4f52-b245-83fd4cce4c65.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/86564702/124397065-8c9d4e00-dcdb-11eb-8289-a0234c56f751.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/86564702/124397078-a8a0ef80-dcdb-11eb-9226-fca4659e1d9f.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>eduardojesus12/codigo-otros-1</title>
    <updated>2023-03-24T01:32:12Z</updated>
    <id>tag:github.com,2023-03-24:/eduardojesus12/codigo-otros-1</id>
    <link href="https://github.com/eduardojesus12/codigo-otros-1" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Código de otros 1&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>