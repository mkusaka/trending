<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub CSS Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-17T01:32:11Z</updated>
  <subtitle>Daily Trending of CSS in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>septiandwica/kado</title>
    <updated>2023-03-17T01:32:11Z</updated>
    <id>tag:github.com,2023-03-17:/septiandwica/kado</id>
    <link href="https://github.com/septiandwica/kado" rel="alternate"></link>
    <summary type="html">&lt;p&gt;flower&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kado&lt;/h1&gt; &#xA;&lt;p&gt;-Flower code from: &lt;a href=&#34;https://codepen.io/mdusmanansari/pen/BamepLe&#34;&gt;https://codepen.io/mdusmanansari/pen/BamepLe&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Description&lt;/h1&gt; &#xA;&lt;p&gt;Flower code tiktok trend&lt;/p&gt; &#xA;&lt;p&gt;Responsive Web -- bisa langsung disesuikan di file css --&amp;gt; style.css&lt;/p&gt; &#xA;&lt;h1&gt;Author&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Flower Code : Md Usman Ansari (@MdUsmanAnsari)&lt;/li&gt; &#xA; &lt;li&gt;Index Code : Septian Dwi Cahyo (@Septiandwica)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks to codepan and mdusmanansari&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>EgoAlpha/prompt-in-context-learning</title>
    <updated>2023-03-17T01:32:11Z</updated>
    <id>tag:github.com,2023-03-17:/EgoAlpha/prompt-in-context-learning</id>
    <link href="https://github.com/EgoAlpha/prompt-in-context-learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Awesome resources for in-context learning and prompt engineering: Mastery of the LLMs such as ChatGPT, GPT-3, and FlanT5, with up-to-date and cutting-edge updates.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/figures/Prompt-EgoAlpha_white.svg?sanitize=true&#34; width=&#34;600px&#34;&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://readme-typing-svg.demolab.com?font=Fira+Code&amp;amp;weight=500&amp;amp;size=30&amp;amp;duration=2500&amp;amp;pause=500&amp;amp;color=8D589A&amp;amp;background=FCFCFF00&amp;amp;center=true&amp;amp;vCenter=true&amp;amp;width=500&amp;amp;lines=Hello!+Human%2C+Are+You+Ready%3F;Welcome+to+my+world!&#34; alt=&#34;Typing SVG&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;strong&gt;An Open-Source Engineering Guide for Prompt-in-context-learning from EgoAlpha Lab.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;img width=&#34;200%&#34; src=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/figures/hr.gif&#34;&gt; &#xA; &lt;!-- &lt;h3 align=&#34;center&#34;&gt;&#xA;&#xA;    &lt;p&gt;Resources for prompt learning and engineering; Mastery of LLMs like ChatGPT, GPT3, FlanT5, etc.&lt;/p&gt;&#xA;&#xA;&lt;/h3&gt; --&gt; &#xA; &lt;h4 align=&#34;center&#34;&gt; &lt;p&gt; &lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/chatgptprompt_zh.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA; &lt;p&gt;&lt;a href=&#34;#üìú-papers&#34;&gt;üìù Papers&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/Playground.md&#34;&gt;‚ö°Ô∏è Playground&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PromptEngineering.md&#34;&gt;üõ† Prompt Engineering&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/chatgptprompt.md&#34;&gt;üåç ChatGPT Prompt&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- ![Build](https://img.shields.io/appveyor/build/gruntjs/grunt) --&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/version-v1.0.0-blue&#34; alt=&#34;version&#34;&gt; &lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/p&gt; &#xA; &lt;!-- ![license](https://img.shields.io/bower/l/bootstrap?style=plastic) --&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;‚≠êÔ∏è Shining ‚≠êÔ∏è:&lt;/strong&gt; This is fresh, daily-updated resources for in-context learning and prompt engineering. As Artificial General Intelligence (AGI) is approaching, let‚Äôs take action and become a super learner so as to position ourselves at the forefront of this exciting era and strive for personal and professional greatness.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The resources include:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;üéâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/#%F0%9F%93%9C-papers&#34;&gt;Papers&lt;/a&gt;üéâ&lt;/em&gt;: The latest papers about in-context learning or prompt engineering.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;üéâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/Playground.md&#34;&gt;Playground&lt;/a&gt;üéâ&lt;/em&gt;: Large language models that enable prompt experimentation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;üéâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PromptEngineering.md&#34;&gt;Prompt Engineering&lt;/a&gt;üéâ&lt;/em&gt;: Prompt techniques for leveraging large language models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;üéâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/chatgptprompt.md&#34;&gt;ChatGPT Prompt&lt;/a&gt;üéâ&lt;/em&gt;: Prompt examples that can be applied in our work and daily lives.&lt;/p&gt; &#xA;&lt;p&gt;In the future, there will likely be two types of people on Earth (perhaps even on Mars, but that&#39;s a question for Musk):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Those who enhance their abilities through the use of AI;&lt;/li&gt; &#xA; &lt;li&gt;Those whose jobs are replaced by AI automation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;üíéEgoAlpha: Hello! humanüë§, are you ready?&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üì¢ News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.3.16]&lt;/strong&gt; üí• Baidu announcing the LLM named &lt;a href=&#34;https://yiyan.baidu.com/welcome&#34;&gt;&#34;ÊñáÂøÉ‰∏ÄË®Ä&#34;(ERNIE3.0 + PLATO)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.3.15]&lt;/strong&gt; Two Breaking News:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Announcing &lt;a href=&#34;https://openai.com/product/gpt-4&#34;&gt;GPT4&lt;/a&gt; by OpenAI from Microsoft. &lt;strong&gt;&lt;a href=&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;&gt;Paperüîó&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Announcing &lt;a href=&#34;https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html&#34;&gt;PaLM&lt;/a&gt; API by Google.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.3.13]&lt;/strong&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;LLaMA has been fine-tuned by Stanford&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.3.10]&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/spaces/togethercomputer/OpenChatKit&#34;&gt;Announcing OpenChatKit by Together&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- &#xA;- **[2023.3.9]**  GPT-4 is coming next week and it will be multimodal,announced by OpenAI.&#xA;&#xA;- **[2023.3.8]** [Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models](https://arxiv.org/abs/2303.04671)&#xA;&#xA;- **[2023.3.7]** [Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846)&#xA;&#xA;- **[2023.3.6]** [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861) --&gt; &#xA;&lt;img width=&#34;200%&#34; src=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/figures/hr.gif&#34;&gt; &#xA;&lt;h1&gt;üìú Papers&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/#Survey&#34;&gt;Survey&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/#prompt-engineering&#34;&gt;Prompt Engineering&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/#in-context-learning&#34;&gt;In-context learning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/#multimodal-prompt&#34;&gt;Multimodal Prompt&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/#prompt-application&#34;&gt;Prompt Application&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- - [Knowledge Augmented Prompts](#knowledge-augmented-prompts)&#xA;&#xA;- [Prompt for Knowledge Graph](#prompt-for-knowledge-graph) --&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Survey&lt;/h2&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.07842&#34;&gt;&lt;strong&gt;Augmented Language Models: a Survey&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.15&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-4-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.00234&#34;&gt;&lt;strong&gt;A Survey for In-context Learning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.31&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/dqxiu/icl_paperlist&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-334-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10403&#34;&gt;&lt;strong&gt;Towards Reasoning in Large Language Models: A Survey&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.20&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-5-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/jeffhj/lm-reasoning&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-195-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09597&#34;&gt;&lt;strong&gt;Reasoning with Language Model Prompting: A Survey&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.19&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-7-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/zjunlp/Prompt4ReasoningPapers&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-169-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.07682&#34;&gt;&lt;strong&gt;Emergent Abilities of Large Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.06.15&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-156-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.1145/3560815&#34;&gt;&lt;strong&gt;Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2021.07.28&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-462-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/survey.md&#34;&gt;Complete paper list üîó for &#34;Survey&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h2&gt;Prompt Engineering&lt;/h2&gt; &#xA;&lt;h3&gt;üìå Prompt Design&lt;/h3&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12314&#34;&gt;&lt;strong&gt;Progressive Prompts: Continual Learning for Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.01.29&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.08721&#34;&gt;&lt;strong&gt;Batch Prompting: Efficient Inference with Large Language Model APIs&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.01.19&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/hkunlp/batch-prompting&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-22-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.08536&#34;&gt;&lt;strong&gt;Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.10.16&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/wjn1996/kp-plm&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-10-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.11755&#34;&gt;&lt;strong&gt;Promptagator: Few-shot Dense Retrieval From 8 Examples&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.09.23&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-16-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.1109/TVCG.2022.3209479&#34;&gt;&lt;strong&gt;Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.08.16&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-10-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-22-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2207.05987&#34;&gt;&lt;strong&gt;DocPrompting: Generating Code by Retrieving the Docs&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.07.13&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-4-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-11-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/shuyanzhou/docprompting&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-132-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.1145/3491102.3501825&#34;&gt;&lt;strong&gt;Design Guidelines for Prompt Engineering Text-to-Image Generative Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2021.09.14&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-44-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-48-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.07732&#34;&gt;&lt;strong&gt;Program Synthesis with Large Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2021.08.16&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-175-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-147-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.1016/j.aiopen.2022.11.003&#34;&gt;&lt;strong&gt;PTR: Prompt Tuning with Rules for Text Classification&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2021.05.24&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-172-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-226-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/thunlp/PTR&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-127-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.1162/tacl_a_00468&#34;&gt;&lt;strong&gt;PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2021.02.24&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-28-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-78-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/eyalbd2/PADA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-44-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/PromptDesignList.md&#34;&gt;Complete paper list üîó for &#34;Prompt Design&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h3&gt;üìå Automatic Prompt&lt;/h3&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.11520&#34;&gt;&lt;strong&gt;Guiding Large Language Models via Directional Stimulus Prompting&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.22&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.05619&#34;&gt;&lt;strong&gt;Evaluating the Robustness of Discrete Prompts&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.11&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/livnlp/prompt-robustness&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-2-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.295&#34;&gt;&lt;strong&gt;Making Pre-trained Language Models Better Few-shot Learners&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2021.01.01&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-648-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-704-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/princeton-nlp/LM-BFF&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-631-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.346&#34;&gt;&lt;strong&gt;Eliciting Knowledge from Language Models Using Automatically Generated Prompts&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2020.10.29&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-136-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-475-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/ucinlp/autoprompt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-340-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.5282/UBM/EPUB.74034&#34;&gt;&lt;strong&gt;Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2020.10.26&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-85-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/timoschick/pet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-1.5k-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/AutomaticPromptList.md&#34;&gt;Complete paper list üîó for &#34;Automatic Prompt&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h3&gt;üìå Chain of Thought&lt;/h3&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.00923&#34;&gt;&lt;strong&gt;Multimodal Chain-of-Thought Reasoning in Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.02&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-6-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/amazon-science/mm-cot&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-3.0k-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.00618&#34;&gt;&lt;strong&gt;Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.01&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.13379&#34;&gt;&lt;strong&gt;Faithful Chain-of-Thought Reasoning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.01.31&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-3-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/veronica320/faithful-cot&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-36-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10071&#34;&gt;&lt;strong&gt;Large Language Models Are Reasoning Teachers&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.20&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-5-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/zinengtang/VidLanKD&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-55-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08686&#34;&gt;&lt;strong&gt;The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.16&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.13892&#34;&gt;&lt;strong&gt;Complementary Explanations for Effective In-Context Learning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.11.25&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-5-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.09150&#34;&gt;&lt;strong&gt;Prompting GPT-3 To Be Reliable&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.10.17&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-9-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/noviscl/gpt3-reliability&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-47-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.09261&#34;&gt;&lt;strong&gt;Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.10.17&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-29-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/suzgunmirac/big-bench-hard&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-64-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.03493&#34;&gt;&lt;strong&gt;Automatic Chain of Thought Prompting in Large Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.10.07&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-24-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/amazon-research/auto-cot&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-288-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.03350&#34;&gt;&lt;strong&gt;Measuring and Narrowing the Compositionality Gap in Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.10.07&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-28-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/ofirpress/self-ask&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-163-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/ChainofThoughtList.md&#34;&gt;Complete paper list üîó for &#34;Chain of Thought&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h3&gt;üìå Evaluation &amp;amp; Reliability&lt;/h3&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12170&#34;&gt;&lt;strong&gt;Language Model Crossover: Variation through Few-Shot Prompting&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.23&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-1-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.05619&#34;&gt;&lt;strong&gt;Evaluating the Robustness of Discrete Prompts&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.11&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/livnlp/prompt-robustness&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-2-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.03269&#34;&gt;&lt;strong&gt;PLACES: Prompting Language Models for Social Conversation Synthesis&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.07&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-1-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/alexa/places&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-4-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.00093&#34;&gt;&lt;strong&gt;Large Language Models Can Be Easily Distracted by Irrelevant Context&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.01.31&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-3-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/google-research-datasets/gsm-ic&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-5-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09196&#34;&gt;&lt;strong&gt;Emergent Analogical Reasoning in Large Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.19&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-5-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/taylorwwebb/emergent_analogies_llm&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-8-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09251&#34;&gt;&lt;strong&gt;Discovering Language Model Behaviors with Model-Written Evaluations&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.19&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-8-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/anthropics/evals&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-101-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08061&#34;&gt;&lt;strong&gt;On Second Thought, Let&#39;s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.15&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.14275&#34;&gt;&lt;strong&gt;Solving math word problems with process- and outcome-based feedback&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.11.25&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-3-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.12588&#34;&gt;&lt;strong&gt;Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.11.22&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-22-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/wenhuchen/program-of-thoughts&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-57-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.05110&#34;&gt;&lt;strong&gt;Large Language Models with Controllable Working Memory&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.11.09&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-3-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/golosio/annabell&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-238-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/EvaluationReliabilityList.md&#34;&gt;Complete paper list üîó for &#34;Evaluation &amp;amp; Reliability&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h2&gt;In-context Learning&lt;/h2&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.00293&#34;&gt;&lt;strong&gt;How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.01&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12170&#34;&gt;&lt;strong&gt;Language Model Crossover: Variation through Few-Shot Prompting&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.23&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-1-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.11521&#34;&gt;&lt;strong&gt;How Does In-Context Learning Help Prompt Tuning?&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.22&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.09185&#34;&gt;&lt;strong&gt;Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.17&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/salt-nlp/bound-cap-llm&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-14-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09741&#34;&gt;&lt;strong&gt;One Embedder, Any Task: Instruction-Finetuned Text Embeddings&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.19&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.13892&#34;&gt;&lt;strong&gt;Complementary Explanations for Effective In-Context Learning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.11.25&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-5-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.09150&#34;&gt;&lt;strong&gt;Prompting GPT-3 To Be Reliable&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.10.17&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-9-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/noviscl/gpt3-reliability&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-47-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.09261&#34;&gt;&lt;strong&gt;Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.10.17&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-29-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/suzgunmirac/big-bench-hard&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-64-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.00720&#34;&gt;&lt;strong&gt;Complexity-Based Prompting for Multi-Step Reasoning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.10.03&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-18-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/allenai/decomp&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-19-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2207.00747&#34;&gt;&lt;strong&gt;Rationale-Augmented Ensembles in Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.07.02&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-26-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/InContextLearningList.md&#34;&gt;Complete paper list üîó for &#34;In-context Learning&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h2&gt;Multimodal Prompt&lt;/h2&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.04671&#34;&gt;&lt;strong&gt;Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.08&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-111-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/microsoft/visual-chatgpt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-13.9k-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.02861&#34;&gt;&lt;strong&gt;Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.06&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.00923&#34;&gt;&lt;strong&gt;Multimodal Chain-of-Thought Reasoning in Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.02&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-6-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/amazon-science/mm-cot&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-3.0k-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.1145/3503161.3548021&#34;&gt;&lt;strong&gt;CoHOZ: Contrasive Multimodal prompt Tuning for Hierarchical Open-set Zero-shot Recognition&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.10.10&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.03094&#34;&gt;&lt;strong&gt;VIMA: General Robot Manipulation with Multimodal Prompts&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.10.06&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-15-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/vimalabs/VIMABench&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-75-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.1007/s11263-022-01653-1&#34;&gt;&lt;strong&gt;Learning to Prompt for Vision-Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.09.01&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-1-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-399-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/kaiyangzhou/coop&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-762-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2203.12119&#34;&gt;&lt;strong&gt;Visual Prompt Tuning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.03.23&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-104-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/KMnP/vpt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-350-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.13884&#34;&gt;&lt;strong&gt;Multimodal Few-Shot Learning with Frozen Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2021.06.25&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-173-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-401-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/ivonajdenkoska/multimodal-meta-learn&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-3-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.2139/ssrn.4347542&#34;&gt;&lt;strong&gt;Similarity-Aware Multimodal Prompt Learning for Fake News Detection&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/MultimodalPromptList.md&#34;&gt;Complete paper list üîó for &#34;Multimodal Prompt&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h2&gt;Prompt Application&lt;/h2&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.00733&#34;&gt;&lt;strong&gt;SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.01&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.00815&#34;&gt;&lt;strong&gt;Soft Prompt Guided Joint Learning for Cross-Domain Sentiment Analysis&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.01&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.14838&#34;&gt;&lt;strong&gt;EvoPrompting: Language Models for Code-Level Neural Architecture Search&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.28&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12173&#34;&gt;&lt;strong&gt;More than you&#39;ve asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.23&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/greshake/lm-safety&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-351-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.08961&#34;&gt;&lt;strong&gt;Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.17&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.08068&#34;&gt;&lt;strong&gt;LabelPrompt: Effective Prompt-based Learning for Relation Classification&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.16&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.08102&#34;&gt;&lt;strong&gt;Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.16&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.04156&#34;&gt;&lt;strong&gt;Prompting for Multimodal Hateful Meme Classification&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.08&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-1-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2203.01543&#34;&gt;&lt;strong&gt;QaNER: Prompting Question Answering Models for Few-shot Named Entity Recognition&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.03.03&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-4-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/dayyass/QaNER&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-56-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.00720&#34;&gt;&lt;strong&gt;LightNER: A Lightweight Tuning Paradigm for Low-resource NER via Pluggable Prompting&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2021.08.31&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-9-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-20-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/promptapplication.md&#34;&gt;Complete paper list üîó for &#34;Prompt Application&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;!-- ### üìå Hard Prompt/ Discrete Prompt&#xA;&#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt;&#xA;&#xA;&#xA;&#xA;[**Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery**](https://doi.org/10.48550/arXiv.2302.03668) Ôºà**2023.02.07**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-360-blue)](https://github.com/YuxinWenRick/hard-prompts-made-easy)&#xA;&#xA;[**SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning**](https://doi.org/10.48550/arXiv.2212.10929) Ôºà**2022.12.21**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-1-green)&#xA;&#xA;[**RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning**](https://doi.org/10.48550/arXiv.2205.12548) Ôºà**2022.05.25**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-25-green)  [![](https://img.shields.io/badge/Github%20Stars-140-blue)](https://github.com/mingkaid/rl-prompt)&#xA;&#xA;[**Personalized Prompt Learning for Explainable Recommendation**](https://arxiv.org/abs/2202.07371) Ôºà**2022.02.15**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-10-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-13-red)  [![](https://img.shields.io/badge/Github%20Stars-57-blue)](https://github.com/lileipisces/pepler)&#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;üëâ[Complete paper list üîó for &#34;Hard Prompt&#34;](./PaperList/HardPromptList.md)üëà&#xA;&#xA;### üìå Soft Prompt/ Continuous Prompt&#xA;&#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt;&#xA;&#xA;&#xA;&#xA;[**Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness**](https://doi.org/10.48550/arXiv.2302.13793) Ôºà**2023.02.23**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-1-green)&#xA;&#xA;[**How Does In-Context Learning Help Prompt Tuning?**](https://doi.org/10.48550/arXiv.2302.11521) Ôºà**2023.02.22**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-0-green)&#xA;&#xA;[**Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts**](https://doi.org/10.48550/arXiv.2302.08958) Ôºà**2023.02.17**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-5-blue)](https://github.com/zhjohnchan/ptunifier)&#xA;&#xA;[**SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains**](https://doi.org/10.48550/arXiv.2302.06868) Ôºà**2023.02.14**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-44-blue)](https://github.com/boschresearch/switchprompt)&#xA;&#xA;[**Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning**](https://doi.org/10.48550/arXiv.2301.10915) Ôºà**2023.01.26**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-0-green)&#xA;&#xA;[**Toward Human Readable Prompt Tuning: Kubrick&#39;s The Shining is a good movie, and a good prompt too?**](https://doi.org/10.48550/arXiv.2212.10539) Ôºà**2022.12.20**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-1-green)&#xA;&#xA;[**Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility of Generated Text in AI**](https://doi.org/10.48550/arXiv.2212.02924) Ôºà**2022.12.06**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-0-green)&#xA;&#xA;[**Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning**](https://doi.org/10.48550/arXiv.2210.12587) Ôºà**2022.10.23**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-0-green)&#xA;&#xA;[**XPrompt: Exploring the Extreme of Prompt Tuning**](https://doi.org/10.48550/arXiv.2210.04457) Ôºà**2022.10.10**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-2-green)&#xA;&#xA;[**Knowledge Prompts: Injecting World Knowledge into Language Models through Soft Prompts**](https://doi.org/10.48550/arXiv.2210.04726) Ôºà**2022.10.10**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-0-green)&#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;üëâ[Complete paper list üîó for &#34;Soft Prompt&#34;](./PaperList/SoftPromptList.md)üëà --&gt; &#xA;&lt;!-- ## Knowledge Augmented Prompts&#xA;&#xA;// __PAPER_LIST__:{field:&#39;Prompt Design&#39;,size:10,state:&#39;corrected&#39;,type:&#39;lite&#39;}&#xA;&#xA;üëâ[Complete paper list üîó for &#34;Knowledge Augmented Prompts&#34;](./PaperList/KnowledgeAugmentedPromptList.md)üëà&#xA;&#xA;## Prompt for Knowledge Graph&#xA;&#xA;// __PAPER_LIST__:{field:&#39;Prompt Design&#39;,size:10,state:&#39;corrected&#39;,type:&#39;lite&#39;}&#xA;&#xA;üëâ[Complete paper list üîó for &#34;Prompt for Knowledge Graph&#34;](./PaperList/PromptKnowledgeGraphList.md)üëà --&gt; &#xA;&lt;img width=&#34;200%&#34; src=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/figures/hr.gif&#34;&gt; &#xA;&lt;!-- # üéì Citation&#xA;&#xA;If you find our work helps, please star our project and cite our paper. Thanks a lot!&#xA;&#xA;```&#xA;&#xA;ÁªºËø∞ËÆ∫ÊñáÂèØ‰ª•ÊîæÂú®Ëøô‰∏™‰ΩçÁΩÆ&#xA;&#xA;``` --&gt; &#xA;&lt;!-- &lt;img width=&#34;200%&#34; src=&#34;./figures/hr.gif&#34; /&gt; --&gt; &#xA;&lt;h1&gt;‚úâÔ∏è Contact&lt;/h1&gt; &#xA;&lt;p&gt;This repo is maintained by &lt;a href=&#34;https://github.com/EgoAlpha&#34;&gt;EgoAlpha Lab&lt;/a&gt;. Questions and discussions are welcome via &lt;code&gt;helloegoalpha@gmail.com&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We are willing to engage in discussions with friends from the academic and industrial communities, and explore the latest developments in prompt engineering and in-context learning together.&lt;/p&gt; &#xA;&lt;img width=&#34;200%&#34; src=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/figures/hr.gif&#34;&gt; &#xA;&lt;h1&gt;üôè Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Thanks to the PhD students from &lt;a href=&#34;https://github.com/EgoAlpha&#34;&gt;EgoAlpha Lab&lt;/a&gt; and other workers who participated in this repo. We will improve the project in the follow-up period and maintain this community well. We also would like to express our sincere gratitude to the authors of the relevant resources. Your efforts have broadened our horizons and enabled us to perceive a more wonderful world.&lt;/p&gt; &#xA;&lt;!-- &lt;img width=&#34;200%&#34; src=&#34;./figures/hr.gif&#34; /&gt; --&gt; &#xA;&lt;!-- # üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Contributors&#xA;&#xA;## Main Contributors&#xA;* [Yu Liu]()&#xA;* [Yifei Cao](https://github.com/cyfedu1024)&#xA;* [Jizhe Yu]()&#xA;* [Yuan Yao]()&#xA;* [He Qi]() --&gt; &#xA;&lt;!-- ## Guest Contributors&#xA;* [No] --&gt; &#xA;&lt;!-- &lt;img width=&#34;200%&#34; src=&#34;./figures/hr.gif&#34; /&gt;&#xA;&#xA;# üìî License&#xA;&#xA;This project is open source and available under the MIT&#xA;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;./figures/rocket.png&#34;/&gt;&#xA;&lt;/div&gt; --&gt;</summary>
  </entry>
</feed>