<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub CSS Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-26T01:51:00Z</updated>
  <subtitle>Weekly Trending of CSS in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cocktailpeanut/dalai</title>
    <updated>2023-03-26T01:51:00Z</updated>
    <id>tag:github.com,2023-03-26:/cocktailpeanut/dalai</id>
    <link href="https://github.com/cocktailpeanut/dalai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The simplest way to run LLaMA on your local machine&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dalai&lt;/h1&gt; &#xA;&lt;p&gt;Run LLaMA and Alpaca on your computer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cocktailpeanut/dalai&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-github&#34;&gt;&lt;/i&gt; GitHub&lt;/a&gt; &lt;a href=&#34;https://twitter.com/cocktailpeanut&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-twitter&#34;&gt;&lt;/i&gt; Twitter&lt;/a&gt; &lt;a href=&#34;https://discord.gg/WWfgrzzkCT&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-discord&#34;&gt;&lt;/i&gt; Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;JUST RUN THIS&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/npx2.png&#34; class=&#34;round&#34;&gt; &#xA;&lt;h2&gt;TO GET&lt;/h2&gt; &#xA;&lt;p&gt;Both alpaca and llama working on your computer!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/alpaca.gif&#34; alt=&#34;alpaca.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Powered by &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;, &lt;a href=&#34;https://github.com/shawwn/llama-dl&#34;&gt;llama-dl CDN&lt;/a&gt;, and &lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;alpaca.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hackable web app included&lt;/li&gt; &#xA; &lt;li&gt;Ships with JavaScript API&lt;/li&gt; &#xA; &lt;li&gt;Ships with &lt;a href=&#34;https://socket.io/&#34;&gt;Socket.io&lt;/a&gt; API&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Intro&lt;/h1&gt; &#xA;&lt;h2&gt;1. Cross platform&lt;/h2&gt; &#xA;&lt;p&gt;Dalai runs on all of the following operating systems:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Linux&lt;/li&gt; &#xA; &lt;li&gt;Mac&lt;/li&gt; &#xA; &lt;li&gt;Windows&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;2. Memory Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Runs on most modern computers. Unless your computer is very very old, it should work.&lt;/p&gt; &#xA;&lt;p&gt;According to &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/issues/13&#34;&gt;a llama.cpp discussion thread&lt;/a&gt;, here are the memory requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;7B =&amp;gt; ~4 GB&lt;/li&gt; &#xA; &lt;li&gt;13B =&amp;gt; ~8 GB&lt;/li&gt; &#xA; &lt;li&gt;30B =&amp;gt; ~16 GB&lt;/li&gt; &#xA; &lt;li&gt;65B =&amp;gt; ~32 GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;3. Disk Space Requirements&lt;/h2&gt; &#xA;&lt;h3&gt;Alpaca&lt;/h3&gt; &#xA;&lt;p&gt;Currently 7B and 13B models are available via &lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;alpaca.cpp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;7B&lt;/h4&gt; &#xA;&lt;p&gt;Alpaca comes fully quantized (compressed), and the only space you need for the 7B model is 4.21GB:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/alpaca_7b.png&#34; alt=&#34;alpaca_7b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;13B&lt;/h4&gt; &#xA;&lt;p&gt;Alpaca comes fully quantized (compressed), and the only space you need for the 13B model is 8.14GB:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/alpaca_13b.png&#34; alt=&#34;alpaca_13b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;LLaMA&lt;/h3&gt; &#xA;&lt;p&gt;You need a lot of space for storing the models. &lt;strong&gt;The model name must be one of: 7B, 13B, 30B, and 65B.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You do NOT have to install all models, you can install one by one. Let&#39;s take a look at how much space each model takes up:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTE&lt;/p&gt; &#xA; &lt;p&gt;The following numbers assume that you DO NOT touch the original model files and keep BOTH the original model files AND the quantized versions.&lt;/p&gt; &#xA; &lt;p&gt;You can optimize this if you delete the original models (which are much larger) after installation and keep only the quantized versions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;7B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 31.17GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 4.21GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/7b.png&#34; alt=&#34;7b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;13B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 60.21GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 4.07GB * 2 = 8.14GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/13b.png&#34; alt=&#34;13b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;30B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 150.48GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 5.09GB * 4 = 20.36GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/30b.png&#34; alt=&#34;30b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;65B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 432.64GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 5.11GB * 8 = 40.88GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/65b.png&#34; alt=&#34;65b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Quickstart&lt;/h1&gt; &#xA;&lt;h2&gt;Docker compose&lt;/h2&gt; &#xA;&lt;p&gt;Requires that you have docker installed and running.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose build&#xA;docker compose run dalai npx dalai alpaca install 7B # or a different model&#xA;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will dave the models in the &lt;code&gt;./models&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;p&gt;View the site at &lt;a href=&#34;http://127.0.0.1:3000/&#34;&gt;http://127.0.0.1:3000/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Mac&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install node.js &amp;gt;= 18&lt;/h3&gt; &#xA;&lt;p&gt;If your mac doesn&#39;t have node.js installed yet, make sure to install node.js &amp;gt;= 18&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en/download/&#34; class=&#34;btn&#34;&gt;Install Node.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Step 2.1. Install models&lt;/h3&gt; &#xA;&lt;p&gt;Currently supported engines are &lt;code&gt;llama&lt;/code&gt; and &lt;code&gt;alpaca&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Add alpaca models&lt;/h4&gt; &#xA;&lt;p&gt;To download alpaca models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai alpaca install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Add llama models&lt;/h4&gt; &#xA;&lt;p&gt;To download llama models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to download multiple models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B 13B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now go to step 3.&lt;/p&gt; &#xA;&lt;h3&gt;Step 2.2. Troubleshoot&lt;/h3&gt; &#xA;&lt;p&gt;Normally you don&#39;t need this step, but if running the commands above don&#39;t do anything and immediately end, it means something went wrong because some of the required modules are not installed on your system.&lt;/p&gt; &#xA;&lt;p&gt;In that case, try the following steps:&lt;/p&gt; &#xA;&lt;h4&gt;1. Install homebrew&lt;/h4&gt; &#xA;&lt;p&gt;In case homebrew is not installed on your computer, install it by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;/bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Or you can find the same instruction on the homebrew hompage: &lt;a href=&#34;https://brew.sh/&#34;&gt;https://brew.sh/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2. Install dependencies&lt;/h4&gt; &#xA;&lt;p&gt;Once homebrew is installed, install these dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install cmake&#xA;brew install pkg-config&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Update NPM&lt;/h4&gt; &#xA;&lt;p&gt;Just to make sure we cover every vector, let&#39;s update NPM as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install -g npm@latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now go back to step 2.1 and try running the &lt;code&gt;npx dalai&lt;/code&gt; commands again.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, run the following command to launch the web UI server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Windows&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install Visual Studio&lt;/h3&gt; &#xA;&lt;p&gt;On windows, you need to install Visual Studio before installing Dalai.&lt;/p&gt; &#xA;&lt;p&gt;Press the button below to visit the Visual Studio downloads page and download:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34; class=&#34;btn&#34;&gt;Download Microsoft Visual Studio&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT!!!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;When installing Visual Studio, make sure to check the 3 options as highlighted below:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python development&lt;/li&gt; &#xA; &lt;li&gt;Node.js development&lt;/li&gt; &#xA; &lt;li&gt;Desktop development with C++&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/vs.png&#34; alt=&#34;vs.png&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Step 2.1. Install models&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;On Windows, make sure to run all commands in &lt;strong&gt;cmd&lt;/strong&gt;.&lt;/p&gt; &#xA; &lt;p&gt;DO NOT run in &lt;strong&gt;powershell&lt;/strong&gt;. Powershell has unnecessarily strict permissions and makes the script fail silently.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Currently supported engines are &lt;code&gt;llama&lt;/code&gt; and &lt;code&gt;alpaca&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Install alpaca&lt;/h4&gt; &#xA;&lt;p&gt;To download alpaca models. Open your &lt;code&gt;cmd&lt;/code&gt; application and enter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai alpaca install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Add llama models&lt;/h4&gt; &#xA;&lt;p&gt;To download llama models. Open your &lt;code&gt;cmd&lt;/code&gt; application and enter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to download multiple models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B 13B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Step 2.2. Troubleshoot (optional)&lt;/h3&gt; &#xA;&lt;p&gt;In case above steps fail, try installing Node.js and Python separately.&lt;/p&gt; &#xA;&lt;p&gt;Install Python:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/ftp/python/3.10.10/python-3.10.10-embed-amd64.zip&#34; class=&#34;btn&#34;&gt;Download Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install Node.js &amp;gt;= 18:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en/download/&#34; class=&#34;btn&#34;&gt;Download Node.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;After both have been installed, open powershell and type &lt;code&gt;python&lt;/code&gt; to see if the application exists. And also type &lt;code&gt;node&lt;/code&gt; to see if the application exists as well.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;ve checked that they both exist, try again.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, run the following command to launch the web UI server (Make sure to run in &lt;code&gt;cmd&lt;/code&gt; and not powershell!):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Linux&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;You need to make sure you have the correct version of Python and Node.js installed.&lt;/p&gt; &#xA;&lt;h4&gt;Step 1.1. Python &amp;lt;= 3.10&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pimylifeup.com/installing-python-on-linux/&#34; class=&#34;btn&#34;&gt;Download Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Make sure the version is 3.10 or lower (not 3.11) Python must be 3.10 or below (pytorch and other libraries are not supported yet on the latest)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Step 1.2. Node.js &amp;gt;= 18&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en/download/package-manager/&#34; class=&#34;btn&#34;&gt;Download node.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Make sure the version is 18 or higher&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Step 2.1. Install models&lt;/h3&gt; &#xA;&lt;p&gt;Currently supported engines are &lt;code&gt;llama&lt;/code&gt; and &lt;code&gt;alpaca&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Add alpaca models&lt;/h4&gt; &#xA;&lt;p&gt;To download alpaca models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai alpaca install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Add llama models&lt;/h4&gt; &#xA;&lt;p&gt;To download llama models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to download multiple models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B 13B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2.2. Troubleshoot&lt;/h3&gt; &#xA;&lt;p&gt;In case the model install silently fails or hangs forever, try the following command, and try running the npx command again:&lt;/p&gt; &#xA;&lt;p&gt;On ubuntu/debian/etc.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install build-essential python3-venv -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On fedora/etc.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dnf install make automake gcc gcc-c++ kernel-devel python3-virtualenv -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 3. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, run the following command to launch the web UI server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;API&lt;/h1&gt; &#xA;&lt;p&gt;Dalai is also an NPM package:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;programmatically install&lt;/li&gt; &#xA; &lt;li&gt;locally make requests to the model&lt;/li&gt; &#xA; &lt;li&gt;run a dalai server (powered by socket.io)&lt;/li&gt; &#xA; &lt;li&gt;programmatically make requests to a remote dalai server (via socket.io)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Dalai is an NPM package. You can install it using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install dalai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;1. constructor()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai(home)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;home&lt;/code&gt;: (optional) manually specify the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By default, Dalai automatically stores the entire &lt;code&gt;llama.cpp&lt;/code&gt; repository under &lt;code&gt;~/llama.cpp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;However, often you may already have a &lt;code&gt;llama.cpp&lt;/code&gt; repository somewhere else on your machine and want to just use that folder. In this case you can pass in the &lt;code&gt;home&lt;/code&gt; attribute.&lt;/p&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;h4&gt;Basic&lt;/h4&gt; &#xA;&lt;p&gt;Creates a workspace at &lt;code&gt;~/llama.cpp&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Custom path&lt;/h4&gt; &#xA;&lt;p&gt;Manually set the &lt;code&gt;llama.cpp&lt;/code&gt; path:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai(&#34;/Documents/llama.cpp&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;2. request()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.request(req, callback)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;req&lt;/code&gt;: a request object. made up of the following attributes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;prompt&lt;/code&gt;: &lt;strong&gt;(required)&lt;/strong&gt; The prompt string&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: &lt;strong&gt;(required)&lt;/strong&gt; The model type + model name to query. Takes the following form: &lt;code&gt;&amp;lt;model_type&amp;gt;.&amp;lt;model_name&amp;gt;&lt;/code&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Example: &lt;code&gt;alpaca.7B&lt;/code&gt;, &lt;code&gt;llama.13B&lt;/code&gt;, ...&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;url&lt;/code&gt;: only needed if connecting to a remote dalai server &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;if unspecified, it uses the node.js API to directly run dalai locally&lt;/li&gt; &#xA;     &lt;li&gt;if specified (for example &lt;code&gt;ws://localhost:3000&lt;/code&gt;) it looks for a socket.io endpoint at the URL and connects to it.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;threads&lt;/code&gt;: The number of threads to use (The default is 8 if unspecified)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;n_predict&lt;/code&gt;: The number of tokens to return (The default is 128 if unspecified)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;seed&lt;/code&gt;: The seed. The default is -1 (none)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;top_k&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;top_p&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;repeat_last_n&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;repeat_penalty&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;temp&lt;/code&gt;: temperature&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;batch_size&lt;/code&gt;: batch size&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;skip_end&lt;/code&gt;: by default, every session ends with &lt;code&gt;\n\n&amp;lt;end&amp;gt;&lt;/code&gt;, which can be used as a marker to know when the full response has returned. However sometimes you may not want this suffix. Set &lt;code&gt;skip_end: true&lt;/code&gt; and the response will no longer end with &lt;code&gt;\n\n&amp;lt;end&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;callback&lt;/code&gt;: the streaming callback function that gets called every time the client gets any token response back from the model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;h4&gt;1. Node.js&lt;/h4&gt; &#xA;&lt;p&gt;Using node.js, you just need to initialize a Dalai object with &lt;code&gt;new Dalai()&lt;/code&gt; and then use it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#39;dalai&#39;)&#xA;new Dalai().request({&#xA;  model: &#34;7B&#34;,&#xA;  prompt: &#34;The following is a conversation between a boy and a girl:&#34;,&#xA;}, (token) =&amp;gt; {&#xA;  process.stdout.write(token)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Non node.js (socket.io)&lt;/h4&gt; &#xA;&lt;p&gt;To make use of this in a browser or any other language, you can use thie socket.io API.&lt;/p&gt; &#xA;&lt;h5&gt;Step 1. start a server&lt;/h5&gt; &#xA;&lt;p&gt;First you need to run a Dalai socket server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// server.js&#xA;const Dalai = require(&#39;dalai&#39;)&#xA;new Dalai().serve(3000)     // port 3000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Step 2. connect to the server&lt;/h5&gt; &#xA;&lt;p&gt;Then once the server is running, simply make requests to it by passing the &lt;code&gt;ws://localhost:3000&lt;/code&gt; socket url when initializing the Dalai object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;)&#xA;new Dalai().request({&#xA;  url: &#34;ws://localhost:3000&#34;,&#xA;  model: &#34;7B&#34;,&#xA;  prompt: &#34;The following is a conversation between a boy and a girl:&#34;,&#xA;}, (token) =&amp;gt; {&#xA;  console.log(&#34;token&#34;, token)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;3. serve()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;p&gt;Starts a socket.io server at &lt;code&gt;port&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.serve(port)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;)&#xA;new Dalai().serve(3000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;4. http()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;p&gt;connect with an existing &lt;code&gt;http&lt;/code&gt; instance (The &lt;code&gt;http&lt;/code&gt; npm package)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.http(http)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;http&lt;/code&gt;: The &lt;a href=&#34;https://nodejs.org/api/http.html&#34;&gt;http&lt;/a&gt; object&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;This is useful when you&#39;re trying to plug dalai into an existing node.js web app&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const app = require(&#39;express&#39;)();&#xA;const http = require(&#39;http&#39;).Server(app);&#xA;dalai.http(http)&#xA;http.listen(3000, () =&amp;gt; {&#xA;  console.log(&#34;server started&#34;)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;5. install()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;await dalai.install(model_type, model_name1, model_name2, ...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model_type&lt;/code&gt;: the name of the model. currently supports: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;alpaca&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&#34;llama&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model1&lt;/code&gt;, &lt;code&gt;model2&lt;/code&gt;, ...: the model names to install (&#34;7B&#34;`, &#34;13B&#34;, &#34;30B&#34;, &#34;65B&#34;, etc)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;Install Llama &#34;7B&#34; and &#34;13B&#34; models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;await dalai.install(&#34;llama&#34;, &#34;7B&#34;, &#34;13B&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install alpaca 7B model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;await dalai.install(&#34;alpaca&#34;, &#34;7B&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;6. installed()&lt;/h2&gt; &#xA;&lt;p&gt;returns the array of installed models&lt;/p&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const models = await dalai.installed()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;const models = await dalai.installed()&#xA;console.log(models)     // prints [&#34;7B&#34;, &#34;13B&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!--&#xA;&#xA;---&#xA;&#xA;## 7. download()&#xA;&#xA;Download models.&#xA;&#xA;There are two download options:&#xA;&#xA;1. **LLaMA:** Download the original LLaMA model, convert it, and quantize (compress) it&#xA;2. **LLaMA.zip:** Download the compressed version (generated from step 1 and published on HuggingFace)&#xA;&#xA;### Syntax&#xA;&#xA;```javascript&#xA;await dalai.download(model1, model2, model3, ...)&#xA;```&#xA;&#xA;- `models`: the model names to install. Can be: &#34;7B&#34;`, &#34;13B&#34;, &#34;30B&#34;, &#34;65B&#34;, &#34;7B.zip&#34;, &#34;13B.zip&#34;, &#34;30B.zip&#34;, &#34;65B.zip&#34;&#xA;  - &#34;7B&#34;, &#34;13B&#34;, &#34;30B&#34;, &#34;65B&#34;: download the raw model, convert, and quantize&#xA;  - &#34;7B.zip&#34;, &#34;13B.zip&#34;, &#34;30B.zip&#34;, &#34;65B.zip&#34;: download the quantized model (no need to waste time downloading huge files)&#xA;&#xA;### Examples&#xA;&#xA;Install the &#34;7B&#34; and &#34;13B&#34; models:&#xA;&#xA;&#xA;```javascript&#xA;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;await dalai.install(&#34;7B&#34;, &#34;13B&#34;)&#xA;```&#xA;&#xA;--&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h2&gt;Using a different home folder&lt;/h2&gt; &#xA;&lt;p&gt;By default Dalai uses your home directory to store the entire repository (&lt;code&gt;~/dalai&lt;/code&gt;). However sometimes you may want to store the archive elsewhere.&lt;/p&gt; &#xA;&lt;p&gt;In this case you can call all CLI methods using the &lt;code&gt;--home&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;h3&gt;1. Installing models to a custom path&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B --home ~/test_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Serving from the custom path&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve --home ~/test_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Updating to the latest&lt;/h2&gt; &#xA;&lt;p&gt;To make sure you update to the latest, first find the latest version at &lt;a href=&#34;https://www.npmjs.com/package/dalai&#34;&gt;https://www.npmjs.com/package/dalai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s say the latest version is &lt;code&gt;0.3.0&lt;/code&gt;. To update the dalai version, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai@0.3.0 setup&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Staying up to date&lt;/h2&gt; &#xA;&lt;p&gt;Have questions or feedback? Follow the project through the following outlets:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cocktailpeanut/dalai&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-github&#34;&gt;&lt;/i&gt; GitHub&lt;/a&gt; &lt;a href=&#34;https://twitter.com/cocktailpeanut&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-twitter&#34;&gt;&lt;/i&gt; Twitter&lt;/a&gt; &lt;a href=&#34;https://discord.gg/WWfgrzzkCT&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-discord&#34;&gt;&lt;/i&gt; Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>thinkingjimmy/Learning-Prompt</title>
    <updated>2023-03-26T01:51:00Z</updated>
    <id>tag:github.com,2023-03-26:/thinkingjimmy/Learning-Prompt</id>
    <link href="https://github.com/thinkingjimmy/Learning-Prompt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;å…è´¹ Prompt Engineering æ•™ç¨‹&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/thinkingjimmy/Learning-Prompt/main/static/img/logo.svg?sanitize=true&#34; width=&#34;180px&#34;&gt; &#xA; &lt;h1&gt;ğŸ‘‹ Welcome to Learning Prompt&lt;/h1&gt; &#xA; &lt;p&gt; &lt;strong&gt;å…è´¹ Prompt Engineering æ•™ç¨‹&lt;/strong&gt; &lt;/p&gt; &#xA; &lt;h4&gt; &lt;a href=&#34;https://learningprompt.wiki/&#34;&gt;é˜…è¯»æ•™ç¨‹&lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href=&#34;https://mcousdyt7h.feishu.cn/share/base/form/shrcn8p8MEmbkTiCDyVVPmdUoSg&#34;&gt;å¡«å†™åé¦ˆè¡¨å•&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;ğŸ˜ å…³äºæœ¬æ•™ç¨‹&lt;/h2&gt; &#xA;&lt;p&gt;å¦‚æœä½ ä¸çŸ¥é“èƒ½æ‹¿ ChatGPT æˆ–è€…å…¶ä»– AI äº§å“æ¥å¹²ä»€ä¹ˆï¼›&lt;/p&gt; &#xA;&lt;p&gt;å¦‚æœä½ ä¸çŸ¥é“å¦‚ä½•æ›´å¥½åœ°ä½¿ç”¨ OpenAI æä¾›çš„ APIï¼›&lt;/p&gt; &#xA;&lt;p&gt;é‚£æœ¬æ•™ç¨‹åº”è¯¥èƒ½å¸®åˆ°ä½ ã€‚&lt;/p&gt; &#xA;&lt;p&gt;è¿™æ˜¯ä¸€ä»½æ•™ä½ å¦‚ä½•æ›´å¥½ä½¿å¥½åœ°ä½¿ç”¨ ChatGPT å’Œå…¶ä»– AI äº§å“çš„å…è´¹æ•™ç¨‹ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å®ƒä¸æ˜¯ä»€ä¹ˆï¼Ÿ&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æœ¬æ•™ç¨‹ä¸æ˜¯ prompt å¤§å…¨ï¼Œå¦‚æœä½ é¢„æœŸæ˜¯æ‰¾åˆ°èƒ½ç›´æ¥ç”¨çš„ promptï¼Œå»ºè®®ä½ è°·æ­Œæ‰¾ä¸€æ‰¾ã€‚æœ¬æ•™ç¨‹æ›´å¤šåœ°æ˜¯æ•™ä½ æ–¹æ³•ï¼Œä»¥åŠè§£é‡Šè¿™äº›æ–¹æ³•ä¸ºä½•æœ‰æ•ˆã€‚&lt;/li&gt; &#xA; &lt;li&gt;æœ¬æ•™ç¨‹ä¸æ˜¯æƒå¨æŒ‡å—ï¼Œåœ¨è¿™ä¸ªé¢†åŸŸæˆ‘ä¹Ÿåªæ˜¯å­¦ç”Ÿã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ¤” ä¸ºä»€ä¹ˆä¼šæœ‰è¿™ä¸ªæ•™ç¨‹ï¼Ÿ&lt;/h2&gt; &#xA;&lt;p&gt;æˆ‘æœ€è¿‘ä¸€ç›´åœ¨ç ”ç©¶ Prompt Engineering ç›¸å…³çš„çŸ¥è¯†ï¼Œç„¶åå‘ç°å¤§å¤šæ•°æ•™ç¨‹éƒ½æ˜¯è‹±æ–‡çš„ï¼Œæœ‰äº›æ•™ç¨‹ä½œè€…ç”šè‡³æ˜¯ NLP æˆ–è€… ML çš„ä¸“å®¶ï¼Œå†…å®¹éƒ½éå¸¸å¥½ï¼Œä½†å¯¹æˆ‘è¿™ç§åˆå­¦è€…æ¥è¯´ï¼Œé˜…è¯»èµ·æ¥è¿˜æ˜¯æœ‰ç‚¹åƒåŠ›ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æ‰€ä»¥æˆ‘å°±èŒç”Ÿå‡ºäº†è‡ªå·±å†™ä¸€ä»½æ•™ç¨‹çš„æƒ³æ³•ï¼Œæƒ³ç€é€šè¿‡è¾“å‡ºå€’é€¼è¾“å…¥çš„æ–¹å¼å»å­¦ä¹ ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¦å¤–ï¼Œå†è¯´è¯´æ„Ÿæ€§çš„è¯ï¼Œæ¯æ¬¡çœ‹åˆ°å„ç§ä¸“å®¶åæ§½ä¸­å›½ AI å‘å±•å¤ªåŠŸåˆ©ï¼Œéƒ½æ˜¯å¾€é’±çœ‹ï¼Œä¸å¤Ÿæœ‰è¿œè§ï¼Œæ‰€ä»¥é”™å¤±äº†ä¸å°‘æœºä¼šã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¦ç‡è¯´æ¥ï¼Œæˆ‘è§‰å¾—å°±åƒå½“å¹´åæ§½ä¸­å›½ä¸ºäº†å‘å±•ç»æµä¸é¡¾ç¯ä¿ä¸€æ ·ï¼Œæˆ‘ä»¬éš¾é“ä¸çŸ¥é“ç¯ä¿å¾ˆé‡è¦å—ï¼Ÿå¹¶ä¸æ˜¯ï¼Œæˆ‘ä»¬å®¶åº•å°±è¿™æ ·ï¼Œè‚¯å®šä¼šæœ‰æ‰€å–èˆçš„ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æˆ‘ä»¬å¹¶ä¸ç¼ºæå‡ºé—®é¢˜çš„äººï¼Œä¸å…¶åæ§½å½“å¹´ä¸åŠªåŠ›ï¼Œä¸å¦‚ç°åœ¨åŸ‹å¤´è‹¦å¹²ï¼Œè´¡çŒ®è‡ªå·±çš„åŠ›é‡ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;å¾€è€…ä¸å¯è°ï¼Œæ¥è€…çŠ¹å¯è¿½ã€‚&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;æ‰€ä»¥å°±æœ‰äº†è¿™ä»½æ•™ç¨‹ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“– å¦‚ä½•é˜…è¯»ï¼Ÿ&lt;/h2&gt; &#xA;&lt;p&gt;å¾ˆæ„Ÿè°¢ä½ æ‰“å¼€è¿™ä»½æ•™ç¨‹ï¼Œä½†æˆ‘å»ºè®®ä½ åœ¨é˜…è¯»è¿™ä»½æ•™ç¨‹çš„æ—¶å€™ï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;é™ä½é¢„æœŸï¼š&lt;/strong&gt; æˆ‘ä¸æ˜¯ä¸“å®¶ï¼Œæˆ‘ä¹Ÿåœ¨å­¦ä¹ ï¼Œæˆ‘åªæ˜¯æ¯”ä½ å¤šèµ°äº†å‡ æ­¥è€Œå·²ã€‚æ•™ç¨‹é‡Œçš„å†…å®¹éš¾å…ä¼šæœ‰é—æ¼æˆ–é”™è¯¯ã€‚å¦å¤–ï¼Œæˆ‘è¿™ä»½æ•™ç¨‹ï¼Œç›®æ ‡è¯»è€…æ˜¯åˆå­¦è€…ï¼Œæ‰€ä»¥åœ¨æ•™ç¨‹ä¸­ï¼Œä¸ºäº†è®©å¤§å®¶æ›´å®¹æ˜“ç†è§£ï¼Œéš¾å…ä¼šç”¨åˆ°ä¸å¤ªå‡†ç¡®çš„ç±»æ¯”ï¼Œæˆ–è€…ç”¨è¯ï¼Œè¯·å„ä½è§è°…ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ä¸è¦åå•¬ä½ çš„åé¦ˆï¼š&lt;/strong&gt; å¦‚æœä½ é‡åˆ°æ— æ³•çœ‹æ‡‚çš„åœ°æ–¹ï¼Œäº¦æˆ–è€…æˆ‘å†™é”™äº†çš„åœ°æ–¹ï¼Œä¸å¦¨ç»™æˆ‘æä¸ª Issue æˆ–è€…å¡«å†™è¿™ä¸ªè¡¨å•ã€‚æˆ‘ä»¬å…±åŒè¿›æ­¥ï¼Œå¹¶ä¸º PE çš„ç§‘æ™®å‡ºä¸€ä»½åŠ›ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;é€šè¿‡è¾“å‡ºå€’é€¼è¾“å…¥ï¼š&lt;/strong&gt; æœ€å¥½çš„å­¦ä¹ æ–¹æ³•å°±æ˜¯å®æ“ï¼Œæ•™ç¨‹é‡Œä¼šæä¾›ä¸å°‘æ¡ˆä¾‹ï¼Œä½ è¾¹é˜…è¯»ï¼Œè¾¹å°è¯•ã€‚ç”šè‡³å¦‚æœå¯ä»¥çš„è¯ï¼Œä¸å¦¨å°†ä½ å†™çš„ prompt é€šè¿‡ Issue æˆ–è¡¨å•çš„æ–¹å¼ï¼Œåˆ†äº«ç»™æˆ‘ï¼Œæˆ‘ä¼šå°†ä¸é”™çš„æ¡ˆä¾‹åŠ åˆ°æ–‡æ¡£å†…ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;ä½ å¯ä»¥æ ¹æ®ä½ çš„éœ€æ±‚æ¥é˜…è¯»æ­¤æ•™ç¨‹ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å¦‚æœä½ å¯¹ AI å’Œ Prompt Engineering ä¸æ˜¯å¾ˆäº†è§£ï¼Œå»ºè®®ä½ ä»åŸºç¡€ç¯‡å¼€å§‹è¯»èµ·ã€‚åŸºç¡€ç¯‡æ›´å¤šçš„ä¼šä»ç”¨æˆ·çš„è§’åº¦æ•™ä½ å¦‚ä½•ä½¿ç”¨ AI äº§å“ï¼ˆç›®å‰ä»…æ›´æ–°äº†é¢å‘æ–‡å­—ç±» AIï¼Œåç»­ä¼šæ›´æ–° Midjourney çš„å†…å®¹ï¼‰ï¼Œæˆ–è€…æ¢å¥è¯è¯´ï¼Œä¼šè®²æ›´å¤š prompt çš„å†…å®¹ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å¦‚æœä½ å·²ç»äº†è§£åŸºæœ¬çš„ç”¨æ³•ï¼Œå¹¶ä¸”æƒ³è¦å­¦ä¹ å¦‚ä½•æ›´å¥½åœ°å¼€å‘ AI äº§å“ï¼Œæƒ³äº†è§£æ›´å¤š prompt engineering çš„å†…å®¹ã€‚é‚£å¯ä»¥ç›´æ¥è·³å»é«˜çº§ç¯‡é˜…è¯»ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å¦‚æœä½ åªæ˜¯æƒ³å¿«é€Ÿäº†è§£ä½¿ç”¨ AI äº§å“çš„æŠ€å·§ï¼Œä½ å¯ä»¥ç›´æ¥è¯»æŠ€å·§ç¯‡ï¼Œé‚£é‡Œæ±‡æ€»äº†æ‰€æœ‰ä½¿ç”¨æŠ€å·§ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å¦‚æœä½ å·²ç»äº†è§£å¦‚ä½•ä½¿ç”¨äº†ï¼Œä½†æƒ³äº†è§£æ›´å¤šä½¿ç”¨åœºæ™¯ï¼Œå¯ä»¥çœ‹çœ‹èµ„æ–™ &amp;amp; äº§å“æ¨èç¯‡ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;å¦å¤–ï¼Œæˆ‘ä¸€ç›´è®¤ä¸ºæœ€å¥½çš„å­¦ä¹ æ–¹æ³•ï¼Œå°±æ˜¯å®è·µï¼Œæ‰€ä»¥æˆ‘å»ºè®®ä½ ä½¿ç”¨ ChatGPT è¿è¡Œä¸€ä¸‹æ•™ç¨‹é‡Œçš„æ¡ˆä¾‹ã€‚è¿™æ ·èƒ½å¤Ÿè®°å¾—æ›´ç‰¢é ä¸€äº›ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æœ€åï¼Œåœ¨è¯»æ–‡æ¡£çš„è¿‡ç¨‹ä¸­ï¼Œä½ ä¼šçœ‹åˆ°ä»¥ä¸‹å‡ ä¸ª emojiï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸš§ ï¼šæ ‡æœ‰è¿™ä¸ª emoji ä»£è¡¨å†…å®¹ï¼Œæˆ‘è¿˜éœ€è¦è¿›ä¸€æ­¥å»å®Œå–„è¡¥å……ï¼Œä½†å¹¶ä¸å½±å“ä½ çš„é˜…è¯»ã€‚æˆ‘ä¼šåœ¨åç»­è¿­ä»£è¡¥å……ã€‚&lt;/li&gt; &#xA; &lt;li&gt;ğŸ†˜ ï¼šè¿™ä¸ªä»£è¡¨æˆ‘éœ€è¦å„ä½çš„å¸®åŠ©ï¼Œæ¯”å¦‚æœ‰å¯èƒ½æ˜¯å¸Œæœ›å¤§å®¶ç»™æˆ‘ä¸€äº›æŸåœºæ™¯çš„ prompt æ¡ˆä¾‹ç­‰ã€‚å¦‚æœä½ æœ‰æƒ³æ³•ï¼Œä¸å¦¨é€šè¿‡ Issue æˆ–&lt;a href=&#34;https://mcousdyt7h.feishu.cn/share/base/form/shrcn8p8MEmbkTiCDyVVPmdUoSg&#34;&gt;è¡¨å•&lt;/a&gt;çš„æ–¹å¼ï¼Œå‘æˆ‘åé¦ˆã€‚&lt;/li&gt; &#xA; &lt;li&gt;ğŸ”´ï¼šä¸ºäº†è®©å¤§å®¶èƒ½æ›´å¹³æ»‘åœ°å­¦ä¹ é«˜çº§ç¯‡çš„å†…å®¹ï¼Œæˆ‘ä¼šåœ¨åŸºç¡€ç¯‡æä¸€äº›é«˜çº§ç¯‡çš„æ¦‚å¿µï¼Œå½“ä½ çœ‹åˆ°è¿™ä¸ªçº¢è‰²åœ†æ—¶ï¼Œå°±è¡¨ç¤ºè¿™æ˜¯é«˜çº§å†…å®¹ï¼Œä½ å¯ä»¥ä¸æ·±ç©¶ï¼Œå…ˆäº†è§£å³å¯ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ¤ å¦‚ä½•è´¡çŒ®ä¸€ä»½ä½ çš„åŠ›é‡ï¼Ÿ&lt;/h2&gt; &#xA;&lt;p&gt;å¦‚æœå¯ä»¥çš„è¯ï¼Œä¸å¦¨åœ¨ Github Page ä¸Šç»™æˆ‘ç‚¹èµï¼Œæˆ–è€…å°†æœ¬æ•™ç¨‹åˆ†äº«ç»™å…¶ä»–äººã€‚æ„Ÿè°¢~&lt;/p&gt; &#xA;&lt;p&gt;åœ¨é˜…è¯»è¿™ä¸ªæ–‡æ¡£çš„æ—¶å€™ï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;é‡åˆ°é”™åˆ«å­—ã€å¥å­ä¸é€šé¡ºï¼Œäº¦æˆ–è€…è¯»ä¸æ‡‚çš„åœ°æ–¹ï¼Œæ¬¢è¿é€šè¿‡ Issue æˆ–&lt;a href=&#34;https://mcousdyt7h.feishu.cn/share/base/form/shrcn8p8MEmbkTiCDyVVPmdUoSg&#34;&gt;è¡¨å•&lt;/a&gt;çš„æ–¹å¼ï¼Œåé¦ˆç»™æˆ‘ï¼Œæˆ‘ä¼šå¯¹å…¶è¿›è¡Œä¿®æ”¹ã€‚ä¸€ç»é‡‡çº³ä¼šå°†ä½ çš„åå­—ç½—åˆ—åœ¨æ„Ÿè°¢äººåå•å†…ã€‚&lt;/li&gt; &#xA; &lt;li&gt;AI çš„å‘å±•éå¸¸å¿«ï¼Œä»Šå¤©å†™çš„æ•™ç¨‹ï¼Œå¾ˆæœ‰å¯èƒ½æ˜å¤©å°±è¿‡æœŸäº†ã€‚å¦‚æœä½ åœ¨æ–‡æ¡£é‡Œçœ‹åˆ°æœ‰ä»€ä¹ˆè½åçš„å†…å®¹ï¼Œæ¯”å¦‚æŸäº›æŒ‡ä»¤å·²ç»ä¸éœ€è¦äº†ï¼Œæˆ–æŒ‡ä»¤ä¸ç”Ÿæ•ˆäº†ï¼Œä¹Ÿæ¬¢è¿é€šè¿‡ Issue æˆ–&lt;a href=&#34;https://mcousdyt7h.feishu.cn/share/base/form/shrcn8p8MEmbkTiCDyVVPmdUoSg&#34;&gt;è¡¨å•&lt;/a&gt;çš„æ–¹å¼ï¼Œå‘æˆ‘åé¦ˆã€‚&lt;/li&gt; &#xA; &lt;li&gt;çœ‹åˆ°æ ‡æœ‰ ğŸ†˜ çš„å†…å®¹ï¼Œç„¶åä½ åˆæœ‰ä¸é”™çš„æƒ³æ³•ï¼Œä¸å¦¨é€šè¿‡ Issue æˆ–&lt;a href=&#34;https://mcousdyt7h.feishu.cn/share/base/form/shrcn8p8MEmbkTiCDyVVPmdUoSg&#34;&gt;è¡¨å•&lt;/a&gt;çš„æ–¹å¼ï¼Œå‘æˆ‘åé¦ˆä½ çš„æƒ³æ³•ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;åœ¨æ„Ÿè°¢äººåå•é¡µé¢é‡Œï¼Œæˆ‘ä¼šç½—åˆ—å¸®åŠ©è¿‡æˆ‘çš„æœ‹å‹ä»¬ ğŸ˜&lt;/p&gt; &#xA;&lt;p&gt;æœ€åï¼Œä½ ä¹Ÿå¯ä»¥ç»™æˆ‘&lt;a href=&#34;https://www.buymeacoffee.com/thinkingjimmy&#34;&gt;ä¹°ä¸€æ¯å’–å•¡&lt;/a&gt;ã€‚æˆ–è€…è´­ä¹°æˆ‘çš„å…¶ä»–ä»˜è´¹å†…å®¹äº§å“ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://xiaobot.net/p/xiaobushous1?refer=599951e8-47eb-4898-aa3b-a7d0a1a06b0f&#34;&gt;æ’­å®¢å°æ•æ‰‹ï¼š&lt;/a&gt;å¦‚æœä½ å–œæ¬¢æ’­å®¢ï¼Œä¸å¦¨çœ‹çœ‹ï¼Œæœªæ¥æˆ‘ä¼šåœ¨ä¸Šé¢å°è¯•æ›´å¤š AI ç›¸å…³çš„æƒ³æ³•ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://xiaobot.net/p/suiyisouxun2023?refer=599951e8-47eb-4898-aa3b-a7d0a1a06b0f&#34;&gt;éšæ„æœå¯» | 2023&lt;/a&gt; ï¼šæˆ‘çš„ä»˜è´¹ newsletterï¼Œæ¯å‘¨æ›´æ–°ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;â¤ï¸ æ„Ÿè°¢&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬æ•™ç¨‹åˆ¶ä½œè¿‡ç¨‹ä¸­ï¼Œå†…å®¹å’Œæ¡ˆä¾‹å‚è€ƒäº†ä»¥ä¸‹æ•™ç¨‹æˆ–æ–‡æ¡£ï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dair-ai/Prompt-Engineering-Guide&#34;&gt;Prompt-Engineering-Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learnprompting.org/&#34;&gt;Learn Prompting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mattnigh/ChatGPT3-Free-Prompt-List&#34;&gt;ChatGPT3-Free-Prompt-List&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf&#34;&gt;Natural Language Processing with Deep Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.edx.org/course/introduction-to-chatgpt&#34;&gt;edx ChatGPT101&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/examples&#34;&gt;OpenAI Examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>canisminor1990/sd-web-ui-kitchen-theme</title>
    <updated>2023-03-26T01:51:00Z</updated>
    <id>tag:github.com,2023-03-26:/canisminor1990/sd-web-ui-kitchen-theme</id>
    <link href="https://github.com/canisminor1990/sd-web-ui-kitchen-theme" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kitchen theme for stable diffusion webui https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kitchen Theme for stable-diffusion-web-ui&lt;/h1&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/canisminor1990/sd-web-ui-kitchen-theme/raw/main/assets/screenshot.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;As an extension (recommended) Either clone the repo into your extensions folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone &#34;https://github.com/canisminor1990/sd-web-ui-kitchen-theme&#34; extensions/kitchen-theme&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(The second argument specifies the name of the folder, you can choose whatever you like).&lt;/p&gt;</summary>
  </entry>
</feed>