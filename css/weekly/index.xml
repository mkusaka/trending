<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub CSS Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-16T01:52:42Z</updated>
  <subtitle>Weekly Trending of CSS in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>codebucks27/Next.js-Developer-Portfolio-Starter-Code</title>
    <updated>2023-04-16T01:52:42Z</updated>
    <id>tag:github.com,2023-04-16:/codebucks27/Next.js-Developer-Portfolio-Starter-Code</id>
    <link href="https://github.com/codebucks27/Next.js-Developer-Portfolio-Starter-Code" rel="alternate"></link>
    <summary type="html">&lt;p&gt;‚≠êBuild a stunning portfolio website with Next.js, Tailwind CSS and Framer-motion. If you want to learn to create this you can follow the tutorial link given in the Read me file.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Next.Js Website Tutorial: Create a Stunning Portfolio Website with Nextjs, Tailwind CSS and Framer-motionüåü&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/codebucks27/Next.js-Developer-Portfolio-Starter-Code?style=social&amp;amp;logo=ApacheSpark&amp;amp;label=Stars&#34; alt=&#34;GitHub stars&#34;&gt;&amp;nbsp;&amp;nbsp; &lt;img src=&#34;https://img.shields.io/github/forks/codebucks27/Next.js-Developer-Portfolio-Starter-Code?style=social&amp;amp;logo=KashFlow&amp;amp;maxAge=3600&#34; alt=&#34;GitHub forks&#34;&gt;&amp;nbsp;&amp;nbsp; &lt;img src=&#34;https://img.shields.io/github/followers/codebucks27.svg?style=social&amp;amp;label=Follow&#34; alt=&#34;Github Followers&#34;&gt;&amp;nbsp;&amp;nbsp;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains starter code for Portfolio website created using NextJs. &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;For Demo and Final Code checkout following linküëá: &lt;br&gt; &lt;a href=&#34;https://devdreaming.com//videos/nextjs-tutorial-build-portfolio-tailwind-css-framer-motion&#34;&gt;Nextjs Portfolio Website&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to learn how to create it please follow below tutorialüëá: &lt;br&gt; &lt;a href=&#34;https://youtu.be/Yw7yWHigGKI&#34;&gt;https://youtu.be/Yw7yWHigGKI&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://youtu.be/Yw7yWHigGKI&#34;&gt;&lt;img src=&#34;https://img.shields.io/youtube/views/Yw7yWHigGKI?style=social&#34; alt=&#34;YouTube Video Views&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3 align=&#34;left&#34;&gt;‚ñ∂ Support me via:&lt;/h3&gt;&#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/CodeBucks&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://www.buymeacoffee.com/assets/img/guidelines/download-assets-sm-1.svg?sanitize=true&#34; height=&#34;50&#34; width=&#34;210&#34; alt=&#34;CodeBucks&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;br&gt; &#xA;&lt;h3&gt;Images of The Portfolio Website:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/codebucks27/Next.js-Developer-Portfolio-Starter-Code/raw/main/website%20images/home-light-desktop.png&#34; alt=&#34;Nextjs Portfolio Website&#34;&gt; &lt;img src=&#34;https://github.com/codebucks27/Next.js-Developer-Portfolio-Starter-Code/raw/main/website%20images/home-dark-desktop.png&#34; alt=&#34;Nextjs Portfolio Website Dark Mode&#34;&gt; &lt;img src=&#34;https://github.com/codebucks27/Next.js-Developer-Portfolio-Starter-Code/raw/main/website%20images/about-light-desktop.png&#34; alt=&#34;Next.js Portfolio Website&#34;&gt; &lt;img src=&#34;https://github.com/codebucks27/Next.js-Developer-Portfolio-Starter-Code/raw/main/website%20images/projects-dark-desktop.png&#34; alt=&#34;Next js Portfolio Website&#34;&gt; &lt;img src=&#34;https://github.com/codebucks27/Next.js-Developer-Portfolio-Starter-Code/raw/main/website%20images/articles-light-desktop.png&#34; alt=&#34;Portfolio Website In Next.js&#34;&gt; &lt;img src=&#34;https://github.com/codebucks27/Next.js-Developer-Portfolio-Starter-Code/raw/main/website%20images/about-light-mobile.png&#34; alt=&#34;Responsive Portfolio Website In Nextjs&#34;&gt; &lt;img src=&#34;https://github.com/codebucks27/Next.js-Developer-Portfolio-Starter-Code/raw/main/website%20images/projects-light-mobile.png&#34; alt=&#34;Responsive Portfolio Website In Next js&#34;&gt; &lt;img src=&#34;https://github.com/codebucks27/Next.js-Developer-Portfolio-Starter-Code/raw/main/website%20images/articles-light-mobile.png&#34; alt=&#34;Mobile Responsive Portfolio Website In Next.js&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Resources Used in This Project&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Profile image in the home page created by using &lt;a href=&#34;https://www.midjourney.com/&#34;&gt;https://www.midjourney.com/&lt;/a&gt; tool.&lt;/li&gt; &#xA; &lt;li&gt;Profile image in the about page by &lt;a href=&#34;https://unsplash.com/@albertdera?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt;Albert Dera&lt;/a&gt; on &lt;a href=&#34;https://unsplash.com/photos/ILip77SbmOE?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&#34;&gt;Unsplash&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Fonts from &lt;a href=&#34;https://fonts.google.com/&#34;&gt;https://fonts.google.com/&lt;/a&gt; &lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;Icons from &lt;a href=&#34;https://iconify.design/&#34;&gt;https://iconify.design/&lt;/a&gt; &lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;LightBulb Svg from &lt;a href=&#34;https://lukaszadam.com/illustrations&#34;&gt;https://lukaszadam.com/illustrations&lt;/a&gt; &lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;External Libraries used in this project:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.framer.com/motion/&#34;&gt;framer-motion&lt;/a&gt; &lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tailwindcss.com/&#34;&gt;Tailwind css&lt;/a&gt; &lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>EgoAlpha/prompt-in-context-learning</title>
    <updated>2023-04-16T01:52:42Z</updated>
    <id>tag:github.com,2023-04-16:/EgoAlpha/prompt-in-context-learning</id>
    <link href="https://github.com/EgoAlpha/prompt-in-context-learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Awesome resources for in-context learning and prompt engineering: Mastery of the LLMs such as ChatGPT, GPT-3, and FlanT5, with up-to-date and cutting-edge updates.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/figures/Prompt-EgoAlpha_white.svg?sanitize=true&#34; width=&#34;600px&#34;&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://readme-typing-svg.demolab.com?font=Fira+Code&amp;amp;weight=500&amp;amp;size=30&amp;amp;duration=2500&amp;amp;pause=500&amp;amp;color=8D589A&amp;amp;background=FCFCFF00&amp;amp;center=true&amp;amp;vCenter=true&amp;amp;width=500&amp;amp;lines=Hello!+Human%2C+Are+You+Ready%3F;Welcome+to+my+world!&#34; alt=&#34;Typing SVG&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;strong&gt;An Open-Source Engineering Guide for Prompt-in-context-learning from EgoAlpha Lab.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;img width=&#34;200%&#34; src=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/figures/hr.gif&#34;&gt; &#xA; &lt;!-- &lt;h3 align=&#34;center&#34;&gt;&#xA;&#xA;    &lt;p&gt;Resources for prompt learning and engineering; Mastery of LLMs like ChatGPT, GPT3, FlanT5, etc.&lt;/p&gt;&#xA;&#xA;&lt;/h3&gt; --&gt; &#xA; &lt;h4 align=&#34;center&#34;&gt; &lt;p&gt; &lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/chatgptprompt_zh.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA; &lt;p&gt;&lt;a href=&#34;#üìú-papers&#34;&gt;üìù Papers&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/Playground.md&#34;&gt;‚ö°Ô∏è Playground&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PromptEngineering.md&#34;&gt;üõ† Prompt Engineering&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/chatgptprompt.md&#34;&gt;üåç ChatGPT Prompt&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- ![Build](https://img.shields.io/appveyor/build/gruntjs/grunt) --&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/version-v1.0.0-blue&#34; alt=&#34;version&#34;&gt; &lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/p&gt; &#xA; &lt;!-- ![license](https://img.shields.io/bower/l/bootstrap?style=plastic) --&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;‚≠êÔ∏è Shining ‚≠êÔ∏è:&lt;/strong&gt; This is fresh, daily-updated resources for in-context learning and prompt engineering. As Artificial General Intelligence (AGI) is approaching, let‚Äôs take action and become a super learner so as to position ourselves at the forefront of this exciting era and strive for personal and professional greatness.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The resources include:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;üéâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/#%F0%9F%93%9C-papers&#34;&gt;Papers&lt;/a&gt;üéâ&lt;/em&gt;: The latest papers about in-context learning or prompt engineering.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;üéâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/Playground.md&#34;&gt;Playground&lt;/a&gt;üéâ&lt;/em&gt;: Large language models that enable prompt experimentation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;üéâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PromptEngineering.md&#34;&gt;Prompt Engineering&lt;/a&gt;üéâ&lt;/em&gt;: Prompt techniques for leveraging large language models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;üéâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/chatgptprompt.md&#34;&gt;ChatGPT Prompt&lt;/a&gt;üéâ&lt;/em&gt;: Prompt examples that can be applied in our work and daily lives.&lt;/p&gt; &#xA;&lt;p&gt;In the future, there will likely be two types of people on Earth (perhaps even on Mars, but that&#39;s a question for Musk):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Those who enhance their abilities through the use of AI;&lt;/li&gt; &#xA; &lt;li&gt;Those whose jobs are replaced by AI automation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;üíéEgoAlpha: Hello! humanüë§, are you ready?&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üì¢ News&lt;/h1&gt; &#xA;&lt;!-- üî•üî•üî• --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.4.15]&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2303.17760&#34;&gt;CAMEL: Communicative Agents for &#34;Mind&#34; Exploration of Large Scale Language Model Society&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://cambridgeltl.github.io/visual-med-alpaca/&#34;&gt;Visual Med-Alpaca: Bridging Modalities in Biomedical Language Models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.4.14]&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;üî•üî•üî•&lt;a href=&#34;https://aws.amazon.com/cn/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/&#34;&gt;Amazon announcing new tools for building with Generative AI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Paper: &lt;a href=&#34;https://arxiv.org/pdf/2304.05613.pdf&#34;&gt;ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.4.13]&lt;/strong&gt; Three Amazing Works:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;AutoGPT: An Autonomous GPT-4 Experiment üëâ&lt;a href=&#34;https://github.com/torantulino/auto-gpt&#34;&gt;Code&lt;/a&gt;üëà&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm&#34;&gt;Databricks releases Dolly 2.0, the first open, instruction-following LLM for commercial use&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat&#34;&gt;Microsoft released the DeepSpeed Chat: Own your ChatGPT&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.4.12]&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2304.04370.pdf&#34;&gt;OpenAGI: When LLM Meets Domain Experts&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.4.11]&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2304.03843.pdf&#34;&gt;Why think step-by-step? Reasoning emerges from the locality of experience&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.4.10]&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2304.03022.pdf&#34;&gt;TagGPT: Large Language Models are Zero-shot Multimodal Taggers&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.4.9]&lt;/strong&gt; A new AI model from Meta AI: Segment Anything Model (SAM) (&lt;a href=&#34;https://arxiv.org/pdf/2304.02643.pdf&#34;&gt;Paper&lt;/a&gt;/&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Code&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.4.8]&lt;/strong&gt; EleutherAI&amp;amp;Yale et al. proposed a large-scale language model analysis suite that spans training and extension: Pythia (&lt;a href=&#34;https://arxiv.org/pdf/2304.01373.pdf&#34;&gt;Paper&lt;/a&gt;/&lt;a href=&#34;https://github.com/EleutherAI/pythia&#34;&gt;Code&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/historynews.md&#34;&gt;üëâ Complete history news üëà&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img width=&#34;200%&#34; src=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/figures/hr.gif&#34;&gt; &#xA;&lt;h1&gt;üìú Papers&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can directly click on the title to jump to the corresponding PDF link location&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/#Survey&#34;&gt;Survey&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/#prompt-engineering&#34;&gt;Prompt Engineering&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/#in-context-learning&#34;&gt;In-context learning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/#multimodal-prompt&#34;&gt;Multimodal Prompt&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/#prompt-application&#34;&gt;Prompt Application&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/#foundation-models&#34;&gt;Foundation Models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Survey&lt;/h2&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.18223&#34;&gt;&lt;strong&gt;A Survey of Large Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.31&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-72-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.07842&#34;&gt;&lt;strong&gt;Augmented Language Models: a Survey&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.15&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-4-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.00234&#34;&gt;&lt;strong&gt;A Survey for In-context Learning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.31&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10403&#34;&gt;&lt;strong&gt;Towards Reasoning in Large Language Models: A Survey&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.20&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-5-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/jeffhj/lm-reasoning&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-201-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09597&#34;&gt;&lt;strong&gt;Reasoning with Language Model Prompting: A Survey&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.19&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-7-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/zjunlp/Prompt4ReasoningPapers&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-179-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2206.07682&#34;&gt;&lt;strong&gt;Emergent Abilities of Large Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.06.15&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-156-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.1145/3560815&#34;&gt;&lt;strong&gt;Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2021.07.28&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-462-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-1.4k-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/mingkaid/rl-prompt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-153-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/survey.md&#34;&gt;Complete paper list üîó for &#34;Survey&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h2&gt;Prompt Engineering&lt;/h2&gt; &#xA;&lt;h3&gt;üìå Prompt Design&lt;/h3&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.04704&#34;&gt;&lt;strong&gt;Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.04.10&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-3-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/amazon-science/prompt-pretraining&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-18-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.11382&#34;&gt;&lt;strong&gt;A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.21&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-3-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.08043&#34;&gt;&lt;strong&gt;GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.16&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12314&#34;&gt;&lt;strong&gt;Progressive Prompts: Continual Learning for Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.01.29&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.08721&#34;&gt;&lt;strong&gt;Batch Prompting: Efficient Inference with Large Language Model APIs&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.01.19&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/hkunlp/batch-prompting&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-24-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09741&#34;&gt;&lt;strong&gt;One Embedder, Any Task: Instruction-Finetuned Text Embeddings&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.19&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.04092&#34;&gt;&lt;strong&gt;Successive Prompting for Decomposing Complex Questions&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.08&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-9-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2209.11755&#34;&gt;&lt;strong&gt;Promptagator: Few-shot Dense Retrieval From 8 Examples&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.09.23&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-16-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.1109/TVCG.2022.3209479&#34;&gt;&lt;strong&gt;Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.08.16&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-10-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-23-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.08531&#34;&gt;&lt;strong&gt;Black-box Prompt Learning for Pre-trained Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.01.21&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-17-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-33-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/shizhediao/black-box-prompt-learning&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-22-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/PromptDesignList.md&#34;&gt;Complete paper list üîó for &#34;Prompt Design&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h3&gt;üìå Automatic Prompt&lt;/h3&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12822&#34;&gt;&lt;strong&gt;Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.24&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.11520&#34;&gt;&lt;strong&gt;Guiding Large Language Models via Directional Stimulus Prompting&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.22&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.05619&#34;&gt;&lt;strong&gt;Evaluating the Robustness of Discrete Prompts&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.11&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/livnlp/prompt-robustness&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-2-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.03668&#34;&gt;&lt;strong&gt;Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.07&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/YuxinWenRick/hard-prompts-made-easy&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-365-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.02441&#34;&gt;&lt;strong&gt;Ask Me Anything: A simple strategy for prompting language models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.10.05&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-14-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/hazyresearch/ama_prompting&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-392-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2203.14465&#34;&gt;&lt;strong&gt;STaR: Bootstrapping Reasoning With Reasoning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.03.28&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-56-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2021.acl-long.295&#34;&gt;&lt;strong&gt;Making Pre-trained Language Models Better Few-shot Learners&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2021.01.01&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-648-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-708-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/princeton-nlp/LM-BFF&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-636-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.18653/v1/2020.emnlp-main.346&#34;&gt;&lt;strong&gt;Eliciting Knowledge from Language Models Using Automatically Generated Prompts&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2020.10.29&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-137-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-477-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.5282/UBM/EPUB.74034&#34;&gt;&lt;strong&gt;Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2020.10.26&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-85-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/timoschick/pet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-1.5k-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/AutomaticPromptList.md&#34;&gt;Complete paper list üîó for &#34;Automatic Prompt&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h3&gt;üìå Chain of Thought&lt;/h3&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.01904&#34;&gt;&lt;strong&gt;REFINER: Reasoning Feedback on Intermediate Representations&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.04.04&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-7-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12822&#34;&gt;&lt;strong&gt;Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.24&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12246&#34;&gt;&lt;strong&gt;Active Prompting with Chain-of-Thought for Large Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.23&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-1-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/shizhediao/active-cot&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-61-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.00923&#34;&gt;&lt;strong&gt;Multimodal Chain-of-Thought Reasoning in Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.02&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-6-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/amazon-science/mm-cot&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-3.1k-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.00618&#34;&gt;&lt;strong&gt;Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.01&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.13379&#34;&gt;&lt;strong&gt;Faithful Chain-of-Thought Reasoning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.01.31&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-3-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/veronica320/faithful-cot&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-36-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10071&#34;&gt;&lt;strong&gt;Large Language Models Are Reasoning Teachers&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.20&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-5-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.14275&#34;&gt;&lt;strong&gt;Solving math word problems with process- and outcome-based feedback&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.11.25&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-3-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.13892&#34;&gt;&lt;strong&gt;Complementary Explanations for Effective In-Context Learning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.11.25&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-5-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2211.12588&#34;&gt;&lt;strong&gt;Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.11.22&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-22-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/wenhuchen/program-of-thoughts&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-57-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/ChainofThoughtList.md&#34;&gt;Complete paper list üîó for &#34;Chain of Thought&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h3&gt;üìå Knowledge Augmented Prompt&lt;/h3&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.01441&#34;&gt;&lt;strong&gt;Commonsense-Aware Prompting for Controllable Empathetic Dialogue Generation&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.02&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.12652&#34;&gt;&lt;strong&gt;REPLUG: Retrieval-Augmented Black-Box Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.01.30&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-3-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10560&#34;&gt;&lt;strong&gt;Self-Instruct: Aligning Language Model with Self Generated Instructions&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.20&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-9-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/yizhongw/self-instruct&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-672-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08686&#34;&gt;&lt;strong&gt;The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.16&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.14803&#34;&gt;&lt;strong&gt;Don‚Äôt Prompt, Search! Mining-based Zero-Shot Learning with Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.10.26&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-1-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.08536&#34;&gt;&lt;strong&gt;Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.10.16&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/wjn1996/kp-plm&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-10-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2210.03304&#34;&gt;&lt;strong&gt;Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD Coding&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.10.07&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/whaleloops/KEPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-25-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2207.05987&#34;&gt;&lt;strong&gt;DocPrompting: Generating Code by Retrieving the Docs&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.07.13&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-4-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-16-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/shuyanzhou/docprompting&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-133-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.1145/3534678.3539382&#34;&gt;&lt;strong&gt;Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.06.19&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-4-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-26-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/rucaibox/unicrs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-43-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.14704&#34;&gt;&lt;strong&gt;Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.05.29&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-7-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/zjunlp/promptkg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-305-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/KnowledgeAugmentedPromptList.md&#34;&gt;Complete paper list üîó for &#34;Knowledge Augmented Prompt&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h3&gt;üìå Evaluation &amp;amp; Reliability&lt;/h3&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.16634&#34;&gt;&lt;strong&gt;GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.29&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-20-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.00293&#34;&gt;&lt;strong&gt;How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.01&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.09185&#34;&gt;&lt;strong&gt;Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.17&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/salt-nlp/bound-cap-llm&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-17-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.05619&#34;&gt;&lt;strong&gt;Evaluating the Robustness of Discrete Prompts&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.11&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/livnlp/prompt-robustness&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-2-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.01582&#34;&gt;&lt;strong&gt;Controlling for Stereotypes in Multimodal Language Model Evaluation&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.03&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.00093&#34;&gt;&lt;strong&gt;Large Language Models Can Be Easily Distracted by Irrelevant Context&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.01.31&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-3-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/google-research-datasets/gsm-ic&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-6-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09196&#34;&gt;&lt;strong&gt;Emergent Analogical Reasoning in Large Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.19&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-5-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/taylorwwebb/emergent_analogies_llm&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-10-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.09251&#34;&gt;&lt;strong&gt;Discovering Language Model Behaviors with Model-Written Evaluations&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.19&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-8-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/anthropics/evals&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-109-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08073&#34;&gt;&lt;strong&gt;Constitutional AI: Harmlessness from AI Feedback&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.15&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-21-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/anthropics/constitutionalharmlessnesspaper&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-69-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.08061&#34;&gt;&lt;strong&gt;On Second Thought, Let&#39;s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.15&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/EvaluationReliabilityList.md&#34;&gt;Complete paper list üîó for &#34;Evaluation &amp;amp; Reliability&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h2&gt;In-context Learning&lt;/h2&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.17651&#34;&gt;&lt;strong&gt;Self-Refine: Iterative Refinement with Self-Feedback&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.30&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-11-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.03846&#34;&gt;&lt;strong&gt;Larger language models do in-context learning differently&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.07&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12170&#34;&gt;&lt;strong&gt;Language Model Crossover: Variation through Few-Shot Prompting&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.23&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-1-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.11521&#34;&gt;&lt;strong&gt;How Does In-Context Learning Help Prompt Tuning?&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.22&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.03269&#34;&gt;&lt;strong&gt;PLACES: Prompting Language Models for Social Conversation Synthesis&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.07&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-1-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/alexa/places&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-4-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2301.11916&#34;&gt;&lt;strong&gt;Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.01.27&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-3-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/wangxinyilinda/concept-based-demonstration-selection&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-12-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.07067&#34;&gt;&lt;strong&gt;Transformers as Algorithms: Generalization and Stability in In-context Learning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.01.17&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-16-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.12017&#34;&gt;&lt;strong&gt;OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.22&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-11-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10873&#34;&gt;&lt;strong&gt;Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.21&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2212.10670&#34;&gt;&lt;strong&gt;In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.12.20&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/InContextLearningList.md&#34;&gt;Complete paper list üîó for &#34;In-context Learning&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h2&gt;Multimodal Prompt&lt;/h2&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.03307&#34;&gt;&lt;strong&gt;Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.04.06&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-5-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.12445&#34;&gt;&lt;strong&gt;MEDIMP: Medical Images and Prompts for renal transplant representation learning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.22&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-1-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/leomlck/medimp&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-1-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.11313&#34;&gt;&lt;strong&gt;CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D Recognition&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.20&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-5-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/deeptibhegde/clip-goes-3d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-62-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.11381&#34;&gt;&lt;strong&gt;MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.20&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.10826&#34;&gt;&lt;strong&gt;Visual Prompt Multi-Modal Tracking&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.20&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-7-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/jiawen-zhu/vipt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-27-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.07522&#34;&gt;&lt;strong&gt;Audio Visual Language Maps for Robot Navigation&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.13&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.04671&#34;&gt;&lt;strong&gt;Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.08&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-193-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/microsoft/visual-chatgpt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-22.6k-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.04751&#34;&gt;&lt;strong&gt;Multimodal Parameter-Efficient Few-Shot Class Incremental Learning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.08&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.02861&#34;&gt;&lt;strong&gt;Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.06&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-10-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.03369&#34;&gt;&lt;strong&gt;Multimodal Prompting with Missing Modalities for Visual Recognition&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.06&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/yilunlee/missing_aware_prompts&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-21-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/multimodalprompt.md&#34;&gt;Complete paper list üîó for &#34;Multimodal Prompt&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h2&gt;Prompt Application&lt;/h2&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.00733&#34;&gt;&lt;strong&gt;SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.01&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.00815&#34;&gt;&lt;strong&gt;Soft Prompt Guided Joint Learning for Cross-Domain Sentiment Analysis&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.01&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.14838&#34;&gt;&lt;strong&gt;EvoPrompting: Language Models for Code-Level Neural Architecture Search&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.28&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.12173&#34;&gt;&lt;strong&gt;More than you&#39;ve asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.23&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/greshake/lm-safety&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-372-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.08961&#34;&gt;&lt;strong&gt;Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.17&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.08068&#34;&gt;&lt;strong&gt;LabelPrompt: Effective Prompt-based Learning for Relation Classification&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.16&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.08102&#34;&gt;&lt;strong&gt;Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.16&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2302.04156&#34;&gt;&lt;strong&gt;Prompting for Multimodal Hateful Meme Classification&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.02.08&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-1-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2205.12390&#34;&gt;&lt;strong&gt;Toxicity Detection with Generative Prompt-based Inference&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.05.24&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-2-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2203.01543&#34;&gt;&lt;strong&gt;QaNER: Prompting Question Answering Models for Few-shot Named Entity Recognition&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2022.03.03&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-4-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/dayyass/QaNER&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-56-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/promptapplication.md&#34;&gt;Complete paper list üîó for &#34;Prompt Application&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;h2&gt;Foundation Models&lt;/h2&gt; &#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.03022&#34;&gt;&lt;strong&gt;TagGPT: Large Language Models are Zero-shot Multimodal Taggers&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.04.06&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-3-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/tencentarc/taggpt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-6-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.01373&#34;&gt;&lt;strong&gt;Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.04.03&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-3-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/eleutherai/pythia&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-491-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.17564&#34;&gt;&lt;strong&gt;BloombergGPT: A Large Language Model for Finance&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.30&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-82-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.14177&#34;&gt;&lt;strong&gt;Scaling Expert Language Models with Unsupervised Domain Discovery&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.24&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Mendeley%20Readers-3-red&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/kernelmachine/cbtm&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-6-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.12712&#34;&gt;&lt;strong&gt;Sparks of Artificial General Intelligence: Early experiments with GPT-4&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.22&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-5-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.09752&#34;&gt;&lt;strong&gt;CoLT5: Faster Long-Range Transformers with Conditional Computation&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.17&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.07295&#34;&gt;&lt;strong&gt;Meet in the Middle: A New Pre-training Paradigm&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.13&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.06865&#34;&gt;&lt;strong&gt;High-throughput Generative Inference of Large Language Models with a Single GPU&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.13&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://github.com/fminference/flexgen&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Github%20Stars-7.0k-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.06296&#34;&gt;&lt;strong&gt;Stabilizing Transformer Training by Preventing Attention Entropy Collapse&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.11&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://doi.org/10.48550/arXiv.2303.05759&#34;&gt;&lt;strong&gt;An Overview on Language Models: Recent Developments and Outlook&lt;/strong&gt;&lt;/a&gt; Ôºà&lt;strong&gt;2023.03.10&lt;/strong&gt;Ôºâ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Citations-0-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üëâ&lt;a href=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/PaperList/foundationmodels.md&#34;&gt;Complete paper list üîó for &#34;Foundation Models&#34;&lt;/a&gt;üëà&lt;/p&gt; &#xA;&lt;!-- ### üìå Hard Prompt/ Discrete Prompt&#xA;&#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt;&#xA;&#xA;&#xA;&#xA;[**Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery**](https://doi.org/10.48550/arXiv.2302.03668) Ôºà**2023.02.07**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-2-green)  [![](https://img.shields.io/badge/Github%20Stars-365-blue)](https://github.com/YuxinWenRick/hard-prompts-made-easy)&#xA;&#xA;[**SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning**](https://doi.org/10.48550/arXiv.2212.10929) Ôºà**2022.12.21**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-1-green)&#xA;&#xA;[**ADEPT: A DEbiasing PrompT Framework**](https://doi.org/10.48550/arXiv.2211.05414) Ôºà**2022.11.10**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-0-green)  [![](https://img.shields.io/badge/Github%20Stars-3-blue)](https://github.com/EmpathYang/ADEPT)&#xA;&#xA;[**PromptAttack: Prompt-based Attack for Language Models via Gradient Search**](https://doi.org/10.48550/arXiv.2209.01882) Ôºà**2022.09.05**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-0-green)&#xA;&#xA;[**RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning**](https://doi.org/10.48550/arXiv.2205.12548) Ôºà**2022.05.25**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-25-green)  [![](https://img.shields.io/badge/Github%20Stars-153-blue)](https://github.com/mingkaid/rl-prompt)&#xA;&#xA;[**Personalized Prompt Learning for Explainable Recommendation**](https://arxiv.org/abs/2202.07371) Ôºà**2022.02.15**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-10-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-13-red)  [![](https://img.shields.io/badge/Github%20Stars-58-blue)](https://github.com/lileipisces/pepler)&#xA;&#xA;[**Instance-aware Prompt Learning for Language Understanding and Generation**](https://arxiv.org/abs/2201.07126) Ôºà**2022.01.18**Ôºâ&#xA;&#xA;![](https://img.shields.io/badge/Citations-10-green)  ![](https://img.shields.io/badge/Mendeley%20Readers-21-red)  [![](https://img.shields.io/badge/Github%20Stars-6-blue)](https://github.com/jinfeihu-stan/ipl)&#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;üëâ[Complete paper list üîó for &#34;Hard Prompt&#34;](./PaperList/HardPromptList.md)üëà&#xA;&#xA;### üìå Soft Prompt/ Continuous Prompt&#xA;&#xA;&lt;div style=&#34;line-height:0.2em;&#34;&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;/div&gt;&#xA;&#xA;üëâ[Complete paper list üîó for &#34;Soft Prompt&#34;](./PaperList/SoftPromptList.md)üëà --&gt; &#xA;&lt;!-- ## Prompt for Knowledge Graph&#xA;&#xA;// __PAPER_LIST__:{field:&#39;Prompt Design&#39;,size:10,state:&#39;corrected&#39;,type:&#39;lite&#39;}&#xA;&#xA;üëâ[Complete paper list üîó for &#34;Prompt for Knowledge Graph&#34;](./PaperList/PromptKnowledgeGraphList.md)üëà --&gt; &#xA;&lt;img width=&#34;200%&#34; src=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/figures/hr.gif&#34;&gt; &#xA;&lt;!-- # üéì Citation&#xA;&#xA;If you find our work helps, please star our project and cite our paper. Thanks a lot!&#xA;&#xA;```&#xA;&#xA;ÁªºËø∞ËÆ∫ÊñáÂèØ‰ª•ÊîæÂú®Ëøô‰∏™‰ΩçÁΩÆ&#xA;&#xA;``` --&gt; &#xA;&lt;!-- &lt;img width=&#34;200%&#34; src=&#34;./figures/hr.gif&#34; /&gt; --&gt; &#xA;&lt;h1&gt;‚úâÔ∏è Contact&lt;/h1&gt; &#xA;&lt;p&gt;This repo is maintained by &lt;a href=&#34;https://github.com/EgoAlpha&#34;&gt;EgoAlpha Lab&lt;/a&gt;. Questions and discussions are welcome via &lt;code&gt;helloegoalpha@gmail.com&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We are willing to engage in discussions with friends from the academic and industrial communities, and explore the latest developments in prompt engineering and in-context learning together.&lt;/p&gt; &#xA;&lt;img width=&#34;200%&#34; src=&#34;https://raw.githubusercontent.com/EgoAlpha/prompt-in-context-learning/main/figures/hr.gif&#34;&gt; &#xA;&lt;h1&gt;üôè Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Thanks to the PhD students from &lt;a href=&#34;https://github.com/EgoAlpha&#34;&gt;EgoAlpha Lab&lt;/a&gt; and other workers who participated in this repo. We will improve the project in the follow-up period and maintain this community well. We also would like to express our sincere gratitude to the authors of the relevant resources. Your efforts have broadened our horizons and enabled us to perceive a more wonderful world.&lt;/p&gt; &#xA;&lt;!-- &lt;img width=&#34;200%&#34; src=&#34;./figures/hr.gif&#34; /&gt; --&gt; &#xA;&lt;!-- # üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Contributors&#xA;&#xA;## Main Contributors&#xA;* [Yu Liu]()&#xA;* [Yifei Cao](https://github.com/cyfedu1024)&#xA;* [Jizhe Yu]()&#xA;* [Yuan Yao]()&#xA;* [He Qi]() --&gt; &#xA;&lt;!-- ## Guest Contributors&#xA;* [No] --&gt; &#xA;&lt;!-- &lt;img width=&#34;200%&#34; src=&#34;./figures/hr.gif&#34; /&gt;&#xA;&#xA;# üìî License&#xA;&#xA;This project is open source and available under the MIT&#xA;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;./figures/rocket.png&#34;/&gt;&#xA;&lt;/div&gt; --&gt;</summary>
  </entry>
  <entry>
    <title>l15y/wenda</title>
    <updated>2023-04-16T01:52:42Z</updated>
    <id>tag:github.com,2023-04-16:/l15y/wenda</id>
    <link href="https://github.com/l15y/wenda" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ÈóªËææÔºö‰∏Ä‰∏™LLMË∞ÉÁî®Âπ≥Âè∞„ÄÇÊó®Âú®ÈÄöËøá‰ΩøÁî®‰∏∫Â∞èÊ®°ÂûãÂ§ñÊåÇÁü•ËØÜÂ∫ìÊü•ÊâæÁöÑÊñπÂºèÔºåÂú®‰∏çËÉΩÊ∂åÁé∞ÁöÑÂâçÊèê‰∏ãÂÆûÁé∞Ëøë‰ºº‰∫éÂ§ßÊ®°ÂûãÁöÑÁîüÊàêËÉΩÂäõ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ÈóªËææÔºö‰∏Ä‰∏™Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãË∞ÉÁî®Âπ≥Âè∞&lt;/h1&gt; &#xA;&lt;h2&gt;ÁÆÄ‰ªã&lt;/h2&gt; &#xA;&lt;p&gt;‰∏Ä‰∏™LLMË∞ÉÁî®Âπ≥Âè∞„ÄÇÊó®Âú®ÈÄöËøá‰ΩøÁî®‰∏∫Â∞èÊ®°ÂûãÂ§ñÊåÇÁü•ËØÜÂ∫ìÊü•ÊâæÁöÑÊñπÂºèÔºåÂÆûÁé∞Ëøë‰ºº‰∫éÂ§ßÊ®°ÂûãÁöÑÁîüÊàêËÉΩÂäõ„ÄÇ&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ÁõÆÂâçÊîØÊåÅÊ®°ÂûãÔºö&lt;code&gt;chatGLM-6B&lt;/code&gt;„ÄÅ&lt;code&gt;chatRWKV&lt;/code&gt;„ÄÅ&lt;code&gt;chatYuan&lt;/code&gt;„ÄÅ&lt;code&gt;llamaÁ≥ªÂàó&lt;/code&gt;„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;Áü•ËØÜÂ∫ìÊâ©Â±ïÊ®°ÂûãËÉΩÂäõ&lt;/li&gt; &#xA; &lt;li&gt;ÊîØÊåÅÂèÇÊï∞Âú®Á∫øË∞ÉÊï¥&lt;/li&gt; &#xA; &lt;li&gt;ÊîØÊåÅ&lt;code&gt;chatGLM-6B&lt;/code&gt;„ÄÅ&lt;code&gt;chatRWKV&lt;/code&gt;„ÄÅ&lt;code&gt;llamaÁ≥ªÂàó&lt;/code&gt;ÊµÅÂºèËæìÂá∫ÂíåËæìÂá∫ËøáÁ®ã‰∏≠‰∏≠Êñ≠&lt;/li&gt; &#xA; &lt;li&gt;Ëá™Âä®‰øùÂ≠òÂØπËØùÂéÜÂè≤Ëá≥ÊµèËßàÂô®ÔºàÂ§öÁî®Êà∑ÂêåÊó∂‰ΩøÁî®‰∏ç‰ºöÂÜ≤Á™ÅÔºå&lt;code&gt;chatRWKV&lt;/code&gt;ÊöÇ‰∏çÊîØÊåÅÔºâ&lt;/li&gt; &#xA; &lt;li&gt;ÂØπËØùÂéÜÂè≤ÁÆ°ÁêÜÔºàÂà†Èô§ÂçïÊù°„ÄÅÊ∏ÖÁ©∫Ôºâ&lt;/li&gt; &#xA; &lt;li&gt;ÊîØÊåÅÂ±ÄÂüüÁΩë„ÄÅÂÜÖÁΩëÈÉ®ÁΩ≤ÂíåÂ§öÁî®Êà∑ÂêåÊó∂‰ΩøÁî®„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;Â§öÁî®Êà∑ÂêåÊó∂‰ΩøÁî®‰∏≠‰ºöËá™Âä®ÊéíÈòüÔºåÂπ∂ÊòæÁ§∫ÂΩìÂâçÁî®Êà∑„ÄÇ&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ê¨¢ËøéÂêåÂ≠¶‰ª¨Âà∂‰ΩúÊïôÂ≠¶ËßÜÈ¢ë„ÄÅÊáí‰∫∫ÂåÖÁ≠âÔºåÂÅöÂ•ΩËØ∑ÂíåÊàëËÅîÁ≥ªÔºåÊàë‰ºöÊääÁõ∏ÂÖ≥ÈìæÊé•Âä†Âà∞readmeÈáå&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;‰∫§ÊµÅQQÁæ§Ôºö162451840&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Êà™Âõæ&lt;/h2&gt; &#xA;&lt;h4&gt;ËÆæÁΩÆÂíåÈ¢ÑËÆæÂäüËÉΩ&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/l15y/wenda/main/imgs/setting.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/l15y/wenda/main/imgs/setting2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Êáí‰∫∫ÂåÖ&lt;/h2&gt; &#xA;&lt;p&gt;ÈìæÊé•Ôºö&lt;a href=&#34;https://pan.baidu.com/s/105nOsldGt5mEPoT2np1ZoA?pwd=lyqz&#34;&gt;https://pan.baidu.com/s/105nOsldGt5mEPoT2np1ZoA?pwd=lyqz&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ÊèêÂèñÁ†ÅÔºölyqz&lt;/p&gt; &#xA;&lt;p&gt;ÈªòËÆ§ÂèÇÊï∞Âú®GTX1660TiÔºà6GÊòæÂ≠òÔºâ‰∏äËøêË°åËâØÂ•Ω„ÄÇ&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ÊóßÁâàÂåÖÂê´Á®ãÂ∫è‰∏ª‰ΩìÂíåchatGLM-6B„ÄÅchatYuanÔºåÂàÜÂà´ÊòØÁã¨Á´ãÁöÑÂéãÁº©Êñá‰ª∂„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;chatRWKVÊ®°ÂûãÊõ¥Êñ∞È¢ëÁπÅÔºåËØ∑ÂéªÂÆòÊñπÈìæÊé•‰∏ãÊúÄÊñ∞ÁöÑ„ÄÇÊöÇ‰∏çÊîØÊåÅchatPDFÂäüËÉΩÔºåÂæàÂø´Â∞±Âä†‰∏ä„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;Êñ∞ÁâàÊöÇÊó∂Âè™ÊúâchatGLM-6BÔºå‰ΩÜÈáçÊñ∞Âà∂‰ΩúÔºå‰ΩìÁßØÊõ¥Êñ∞ÔºåÂåÖÂê´ÂêÑÁßç‰ºòÂåñÔºåÈõÜÊàêÁü•ËØÜÂ∫ìÂäüËÉΩÔºåÊé®Ëçê‰ΩøÁî®„ÄÇ&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Ëá™Ë°åÂÆâË£Ö&lt;/h2&gt; &#xA;&lt;h3&gt;1.ÂÆâË£ÖÂ∫ì&lt;/h3&gt; &#xA;&lt;p&gt;ÈÄöÁî®‰æùËµñÔºö&lt;code&gt;pip install -r requirements.txt&lt;/code&gt; Áü•ËØÜÂ∫ìbingÊ®°ÂºèÔºö&lt;code&gt;pip install -r requirements-bing.txt&lt;/code&gt; Áü•ËØÜÂ∫ìfessÊ®°ÂºèÔºö&lt;code&gt;pip install -r requirements-fess.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;2.‰∏ãËΩΩÊ®°Âûã&lt;/h3&gt; &#xA;&lt;p&gt;Ê†πÊçÆÈúÄË¶ÅÔºå‰∏ãËΩΩÂØπÂ∫îÊ®°Âûã„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Âª∫ËÆÆ‰ΩøÁî®chatRWKVÁöÑRWKV-4-Raven-7B-v7-ChnEng-20230404-ctx2048ÔºàÊà™Ê≠¢4Êúà6Êó•ÊïàÊûúËæÉÂ•ΩÔºâÔºåÊàñchatGLM-6B„ÄÇ&lt;/p&gt; &#xA;&lt;h3&gt;3.ÂèÇÊï∞ËÆæÁΩÆ&lt;/h3&gt; &#xA;&lt;p&gt;Ê†πÊçÆ&lt;code&gt;settings.bat&lt;/code&gt;‰∏≠ËØ¥ÊòéÔºåÂ°´ÂÜô‰Ω†ÁöÑÊ®°Âûã‰∏ãËΩΩ‰ΩçÁΩÆÁ≠â‰ø°ÊÅØ&lt;/p&gt; &#xA;&lt;h3&gt;4.ÁîüÊàêÁü•ËØÜÂ∫ì&lt;/h3&gt; &#xA;&lt;p&gt;Â∞ÜtxtÊ†ºÂºèÁöÑËØ≠ÊñôÊîæÂà∞txtÊñá‰ª∂Â§π‰∏≠ÔºåËøêË°å&lt;code&gt;run_data_processing.bat&lt;/code&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;Áü•ËØÜÂ∫ì&lt;/h2&gt; &#xA;&lt;p&gt;Áü•ËØÜÂ∫ìÊúÄÁªàÊïàÊûúÊòØÁîüÊàê‰∏Ä‰∫õÊèêÁ§∫‰ø°ÊÅØÔºå‰ºöÊèíÂÖ•Âà∞ÂØπËØùÈáåÈù¢„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;fessÊ®°Âºè„ÄÅbingÊ®°Âºè„ÄÅbingxsÊ®°Âºè„ÄÅ bingsiteÊ®°ÂºèÂùáË∞ÉÁî®ÊêúÁ¥¢ÂºïÊìéÊêúÁ¥¢Ëé∑ÂèñÁ≠îÊ°à„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;ÊêúÁ¥¢ÂêéÂú®ÂõûÁ≠î‰πãÂâçÊèíÂÖ•ÊèêÁ§∫‰ø°ÊÅØÔºåÁü•ËØÜÂ∫ìÁöÑÊï∞ÊçÆÂ∞±Ë¢´Ê®°ÂûãÁü•ÈÅì‰∫Ü„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;‰∏∫Èò≤Ê≠¢ÁàÜÊòæÂ≠òÔºåÊèíÂÖ•ÁöÑÊï∞ÊçÆ‰∏çËÉΩÂ§™ÈïøÔºåÊâÄ‰ª•ÊúâÂ≠óÊï∞ÈôêÂà∂„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Áü•ËØÜÂ∫ìÂú®Á∫øÊ®°ÂºèÔºö&lt;code&gt;pip install -r requirements-bing.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;ÊòØÊúâ‰ª•‰∏ãÂá†ÁßçÊñπÊ°àÔºö&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;bingÊ®°ÂºèÔºåcn.bingÊêúÁ¥¢Ôºå‰ªÖÂõΩÂÜÖÂèØÁî®&lt;/li&gt; &#xA; &lt;li&gt;bingxsÊ®°ÂºèÔºåcn.bingÂ≠¶ÊúØÊêúÁ¥¢Ôºå‰ªÖÂõΩÂÜÖÂèØÁî®&lt;/li&gt; &#xA; &lt;li&gt;bingsiteÊ®°ÂºèÔºåbingÁ´ôÂÜÖÊêúÁ¥¢ÔºåÈúÄËÆæÁΩÆÁΩëÂùÄ&lt;/li&gt; &#xA; &lt;li&gt;mixÊ®°ÂºèÔºåËûçÂêà&lt;/li&gt; &#xA; &lt;li&gt;fessÊ®°ÂºèÔºåÊú¨Âú∞ÈÉ®ÁΩ≤ÁöÑ&lt;a href=&#34;https://github.com/codelibs/fess&#34;&gt;fessÊêúÁ¥¢&lt;/a&gt;ÔºåÊïàÊûúÂ•Ω‰∫éÂ∑≤Âà†Èô§ÁöÑs„ÄÅxÊ®°ÂºèÔºåÂπ∂‰ΩøÁî®&lt;a href=&#34;https://github.com/letiantian/TextRank4ZH&#34;&gt;letiantian/TextRank4ZH&lt;/a&gt;ËøõË°å‰∫ÜÂÖ≥ÈîÆËØçÊèêÂèñ&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;winÁ≥ªÁªüfess‰ΩøÁî®&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Êáí‰∫∫ÂåÖ‰∏≠‰∏ãËΩΩfess-14.7.0-with-jdk.7z&lt;/li&gt; &#xA; &lt;li&gt;Ëß£ÂéãÂà∞Âπ≥Êó∂ÊîæËΩØ‰ª∂ÁöÑÁõò&lt;/li&gt; &#xA; &lt;li&gt;ÊâìÂºÄËß£ÂéãÂá∫Êù•ÁöÑfess-14.7.0-with-jdk\binÁõÆÂΩï&lt;/li&gt; &#xA; &lt;li&gt;ÂèåÂáªfess.in.bat&lt;/li&gt; &#xA; &lt;li&gt;ÂèåÂáªfess.bat. ÂºπÂá∫ÂëΩ‰ª§Ë°åËøêË°åÊ°Ü. Â∞ÜÂÖ∂ÊúÄÂ∞èÂåñ&lt;/li&gt; &#xA; &lt;li&gt;ÊâìÂºÄÊµèËßàÂô®. ÊâìÂºÄÁΩëÂùÄ&lt;a href=&#34;http://localhost:8080/&#34;&gt;http://localhost:8080/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ÁÇπÂáªÂè≥‰∏äËßílog in ËæìÂÖ•Ë¥¶Âè∑:admin ÂØÜÁ†ÅÔºöwenda ËøõË°åÁôªÂΩï&lt;/li&gt; &#xA; &lt;li&gt;ÁÇπÂáª‰æßËæπÊ†è‰∏≠ÁöÑCrawler. ÁÇπÂáªFile System&lt;/li&gt; &#xA; &lt;li&gt;ÁÇπÂáªÂè≥‰∏äËßíÁöÑCreate New&lt;/li&gt; &#xA; &lt;li&gt;NameËæìÂÖ•‰æø‰∫éËÆ∞ÂøÜÁöÑËµÑÊñôÂ∫ìÁöÑÂêçÂ≠ó&lt;/li&gt; &#xA; &lt;li&gt;PathsËæìÂÖ•ËµÑÊñôÂ∫ìÁöÑÂú∞ÂùÄÔºàÊ†ºÂºèÁ§∫‰æãÔºöfile:///E:/pdfÔºâ&lt;/li&gt; &#xA; &lt;li&gt;ÂÖ∂‰ΩôÈÄâÈ°π‰øùÊåÅÈªòËÆ§. ‰∏ãÊªöËá≥ÊúÄ‰∏ãÊñπÁÇπÂáªCreate&lt;/li&gt; &#xA; &lt;li&gt;Ëá™Âä®ËøîÂõûFile SystemÈ°µÈù¢. ÁÇπÂáªÂàöÊâçÂàõÂª∫ÁöÑÈÄâÈ°πÔºàËá™Â∑±ËæìÂÖ•ÁöÑNameÔºâ&lt;/li&gt; &#xA; &lt;li&gt;ÁÇπÂáªCreate new job. ÁÇπÂáªCreate&lt;/li&gt; &#xA; &lt;li&gt;ËøõÂÖ•‰æßËæπÊ†èÁöÑSystemÂÜÖÁöÑScheduler. ÂèØ‰ª•ÁúãÂà∞ÂæàÂ§ö‰ªªÂä°&lt;/li&gt; &#xA; &lt;li&gt;ÁõÆÂΩïÁöÑÂâçÈù¢ÂèØ‰ª•ÁúãÂà∞ÂàöÂàöÂàõÂª∫ÁöÑjobÔºàÁ§∫‰æãÔºöFile Crawler - pdf searchÔºâ. ÁÇπÂáªËøõÂÖ•&lt;/li&gt; &#xA; &lt;li&gt;ÁÇπÂáªStart now. Âà∑Êñ∞ÁïåÈù¢Âç≥ÂèØÁúãÂà∞ËØ•‰ªªÂä°Ê≠£Âú®ËøêË°å. running&lt;/li&gt; &#xA; &lt;li&gt;Ê≠§Êó∂fessÂ∞±Âú®Áà¨ÂèñÊñá‰ª∂ÁöÑÂêçÂ≠óÂíåÂÜÖÂÆπ. ÂèØ‰ª•Âú®ËµÑÊ∫êÁÆ°ÁêÜÂô®ÁúãÂà∞cpuÊúâË¥üËΩΩ&lt;/li&gt; &#xA; &lt;li&gt;ÊåÇÊú∫„ÄÇÁ≠âÂæÖÁà¨ÂèñÂÆåÊàêÂç≥ÂèØÂ∞ùËØïÊêúÁ¥¢ÂÖ≥ÈîÆËØç&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Ë∞ÉËØïÂ∑•ÂÖ∑&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/l15y/wenda/main/imgs/zsk-test.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;chatGLM-6BÊ®°Âûã&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/l15y/wenda/main/imgs/zsk-glm.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;chatRWKVÊ®°Âûã&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/l15y/wenda/main/imgs/zsk-rwkv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1.Á¥¢ÂºïËØ≠Êñô&lt;/h3&gt; &#xA;&lt;p&gt;ÊääËá™Â∑±ÁöÑtxtÊ†ºÂºèÁöÑÊñáÊ°£ÊîæÂú®Âêç‰∏∫txtÁöÑÊñá‰ª∂Â§πÈáåÔºåËøêË°å:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;run_data_processing.bat&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåÁ¥¢ÂºïËØ≠ÊñôËá≥ÈíàÂØπs„ÄÅxÊ®°ÂºèÔºåÂú®Á∫øÁü•ËØÜÂ∫ìÔºàbingÊ®°ÂºèÁ≠âÔºâ‰∏çÈúÄË¶ÅÁ¥¢ÂºïÔºåËøêË°åÁ¥¢Âºï‰ºöÁõ¥Êé•Êä•Èîô„ÄÇ&lt;/p&gt; &#xA;&lt;h3&gt;2.‰ΩøÁî®&lt;/h3&gt; &#xA;&lt;p&gt;Ê≠£Â∏∏‰ΩøÁî®‰∏≠ÔºåÂãæÈÄâÂè≥‰∏äËßíÁü•ËØÜÂ∫ì&lt;/p&gt; &#xA;&lt;h2&gt;chatGLM-6B&lt;/h2&gt; &#xA;&lt;p&gt;ËøêË°åÔºö&lt;code&gt;run_GLM6B.bat&lt;/code&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Ê®°Âûã‰ΩçÁΩÆÁ≠âÂèÇÊï∞Ôºö‰øÆÊîπ&lt;code&gt;settings.bat&lt;/code&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;ÈªòËÆ§ÂèÇÊï∞Âú®GTX1660TiÔºà6GÊòæÂ≠òÔºâ‰∏äËøêË°åËâØÂ•Ω„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;chatRWKV&lt;/h2&gt; &#xA;&lt;p&gt;ËøêË°åÔºö&lt;code&gt;run_rwkv.bat&lt;/code&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Ê®°Âûã‰ΩçÁΩÆÁ≠âÂèÇÊï∞Ôºö‰øÆÊîπ&lt;code&gt;settings.bat&lt;/code&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;ÈªòËÆ§ÂèÇÊï∞Âú®GTX1660TiÔºà6GÊòæÂ≠òÔºâ‰∏äÊ≠£Â∏∏ËøêË°åÔºå‰ΩÜÈÄüÂ∫¶ËæÉÊÖ¢„ÄÇ&lt;/p&gt; &#xA;&lt;h3&gt;ÁîüÊàêÂ∞èËØ¥&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/l15y/wenda/main/imgs/novel.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;ÊñáÂ≠óÂÜíÈô©Ê∏∏Êàè&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/l15y/wenda/main/imgs/wzmx.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;llama&lt;/h2&gt; &#xA;&lt;p&gt;ËøêË°åÔºö&lt;code&gt;run_llama.bat&lt;/code&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Ê≥®ÊÑèÂ∫ìÊúÄÂ•Ω‰ΩøÁî®Êàë‰øÆÊîπÁöÑÔºö&lt;a href=&#34;https://github.com/l15y/llama-cpp-python&#34;&gt;llama-cpp-python&lt;/a&gt;ÔºåÊâçÂèØ‰ª•Ê≠£Â∏∏‰ΩøÁî®‰∏≠ÊñáÔºàÊà™Ê≠¢4Êúà15Êó•Ôºâ„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;ÁºñËØëÂ•ΩÁöÑÔºö&lt;a href=&#34;https://github.com/l15y/llama-cpp-python/releases&#34;&gt;https://github.com/l15y/llama-cpp-python/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ê®°Âûã‰ΩçÁΩÆÁ≠âÂèÇÊï∞Ôºö‰øÆÊîπ&lt;code&gt;settings.bat&lt;/code&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;‰∫åÊ¨°ÂºÄÂèë&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ÂÖºÂÆπchatboxÁöÑapiÔºö&lt;a href=&#34;http://127.0.0.1:17860/chat/completions&#34;&gt;http://127.0.0.1:17860/chat/completions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ÊµèËßàÂô®ÂâçÁ´ØÈóªËææAutoÂºÄÂèëÂáΩÊï∞Ôºö &lt;img src=&#34;https://raw.githubusercontent.com/l15y/wenda/main/imgs/autogpt.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;p&gt;ÂÆûÁé∞‰ª•‰∏ãÁü•ËØÜÂ∫ìÊ®°ÁªÑÔºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Áü•ËØÜÂõæË∞±&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ÂÆûÁé∞‰ª•‰∏ãÊ®°ÂûãÊ®°ÁªÑÔºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>