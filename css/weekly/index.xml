<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub CSS Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-26T01:51:00Z</updated>
  <subtitle>Weekly Trending of CSS in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cocktailpeanut/dalai</title>
    <updated>2023-03-26T01:51:00Z</updated>
    <id>tag:github.com,2023-03-26:/cocktailpeanut/dalai</id>
    <link href="https://github.com/cocktailpeanut/dalai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The simplest way to run LLaMA on your local machine&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dalai&lt;/h1&gt; &#xA;&lt;p&gt;Run LLaMA and Alpaca on your computer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cocktailpeanut/dalai&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-github&#34;&gt;&lt;/i&gt; GitHub&lt;/a&gt; &lt;a href=&#34;https://twitter.com/cocktailpeanut&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-twitter&#34;&gt;&lt;/i&gt; Twitter&lt;/a&gt; &lt;a href=&#34;https://discord.gg/WWfgrzzkCT&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-discord&#34;&gt;&lt;/i&gt; Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;JUST RUN THIS&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/npx2.png&#34; class=&#34;round&#34;&gt; &#xA;&lt;h2&gt;TO GET&lt;/h2&gt; &#xA;&lt;p&gt;Both alpaca and llama working on your computer!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/alpaca.gif&#34; alt=&#34;alpaca.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Powered by &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;, &lt;a href=&#34;https://github.com/shawwn/llama-dl&#34;&gt;llama-dl CDN&lt;/a&gt;, and &lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;alpaca.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hackable web app included&lt;/li&gt; &#xA; &lt;li&gt;Ships with JavaScript API&lt;/li&gt; &#xA; &lt;li&gt;Ships with &lt;a href=&#34;https://socket.io/&#34;&gt;Socket.io&lt;/a&gt; API&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Intro&lt;/h1&gt; &#xA;&lt;h2&gt;1. Cross platform&lt;/h2&gt; &#xA;&lt;p&gt;Dalai runs on all of the following operating systems:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Linux&lt;/li&gt; &#xA; &lt;li&gt;Mac&lt;/li&gt; &#xA; &lt;li&gt;Windows&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;2. Memory Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Runs on most modern computers. Unless your computer is very very old, it should work.&lt;/p&gt; &#xA;&lt;p&gt;According to &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/issues/13&#34;&gt;a llama.cpp discussion thread&lt;/a&gt;, here are the memory requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;7B =&amp;gt; ~4 GB&lt;/li&gt; &#xA; &lt;li&gt;13B =&amp;gt; ~8 GB&lt;/li&gt; &#xA; &lt;li&gt;30B =&amp;gt; ~16 GB&lt;/li&gt; &#xA; &lt;li&gt;65B =&amp;gt; ~32 GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;3. Disk Space Requirements&lt;/h2&gt; &#xA;&lt;h3&gt;Alpaca&lt;/h3&gt; &#xA;&lt;p&gt;Currently 7B and 13B models are available via &lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;alpaca.cpp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;7B&lt;/h4&gt; &#xA;&lt;p&gt;Alpaca comes fully quantized (compressed), and the only space you need for the 7B model is 4.21GB:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/alpaca_7b.png&#34; alt=&#34;alpaca_7b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;13B&lt;/h4&gt; &#xA;&lt;p&gt;Alpaca comes fully quantized (compressed), and the only space you need for the 13B model is 8.14GB:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/alpaca_13b.png&#34; alt=&#34;alpaca_13b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;LLaMA&lt;/h3&gt; &#xA;&lt;p&gt;You need a lot of space for storing the models. &lt;strong&gt;The model name must be one of: 7B, 13B, 30B, and 65B.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You do NOT have to install all models, you can install one by one. Let&#39;s take a look at how much space each model takes up:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTE&lt;/p&gt; &#xA; &lt;p&gt;The following numbers assume that you DO NOT touch the original model files and keep BOTH the original model files AND the quantized versions.&lt;/p&gt; &#xA; &lt;p&gt;You can optimize this if you delete the original models (which are much larger) after installation and keep only the quantized versions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;7B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 31.17GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 4.21GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/7b.png&#34; alt=&#34;7b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;13B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 60.21GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 4.07GB * 2 = 8.14GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/13b.png&#34; alt=&#34;13b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;30B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 150.48GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 5.09GB * 4 = 20.36GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/30b.png&#34; alt=&#34;30b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;65B&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 432.64GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 5.11GB * 8 = 40.88GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/65b.png&#34; alt=&#34;65b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Quickstart&lt;/h1&gt; &#xA;&lt;h2&gt;Docker compose&lt;/h2&gt; &#xA;&lt;p&gt;Requires that you have docker installed and running.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose build&#xA;docker compose run dalai npx dalai alpaca install 7B # or a different model&#xA;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will dave the models in the &lt;code&gt;./models&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;p&gt;View the site at &lt;a href=&#34;http://127.0.0.1:3000/&#34;&gt;http://127.0.0.1:3000/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Mac&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install node.js &amp;gt;= 18&lt;/h3&gt; &#xA;&lt;p&gt;If your mac doesn&#39;t have node.js installed yet, make sure to install node.js &amp;gt;= 18&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en/download/&#34; class=&#34;btn&#34;&gt;Install Node.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Step 2.1. Install models&lt;/h3&gt; &#xA;&lt;p&gt;Currently supported engines are &lt;code&gt;llama&lt;/code&gt; and &lt;code&gt;alpaca&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Add alpaca models&lt;/h4&gt; &#xA;&lt;p&gt;To download alpaca models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai alpaca install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Add llama models&lt;/h4&gt; &#xA;&lt;p&gt;To download llama models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to download multiple models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B 13B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now go to step 3.&lt;/p&gt; &#xA;&lt;h3&gt;Step 2.2. Troubleshoot&lt;/h3&gt; &#xA;&lt;p&gt;Normally you don&#39;t need this step, but if running the commands above don&#39;t do anything and immediately end, it means something went wrong because some of the required modules are not installed on your system.&lt;/p&gt; &#xA;&lt;p&gt;In that case, try the following steps:&lt;/p&gt; &#xA;&lt;h4&gt;1. Install homebrew&lt;/h4&gt; &#xA;&lt;p&gt;In case homebrew is not installed on your computer, install it by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;/bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Or you can find the same instruction on the homebrew hompage: &lt;a href=&#34;https://brew.sh/&#34;&gt;https://brew.sh/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2. Install dependencies&lt;/h4&gt; &#xA;&lt;p&gt;Once homebrew is installed, install these dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install cmake&#xA;brew install pkg-config&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Update NPM&lt;/h4&gt; &#xA;&lt;p&gt;Just to make sure we cover every vector, let&#39;s update NPM as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install -g npm@latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now go back to step 2.1 and try running the &lt;code&gt;npx dalai&lt;/code&gt; commands again.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, run the following command to launch the web UI server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Windows&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install Visual Studio&lt;/h3&gt; &#xA;&lt;p&gt;On windows, you need to install Visual Studio before installing Dalai.&lt;/p&gt; &#xA;&lt;p&gt;Press the button below to visit the Visual Studio downloads page and download:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34; class=&#34;btn&#34;&gt;Download Microsoft Visual Studio&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT!!!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;When installing Visual Studio, make sure to check the 3 options as highlighted below:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python development&lt;/li&gt; &#xA; &lt;li&gt;Node.js development&lt;/li&gt; &#xA; &lt;li&gt;Desktop development with C++&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/vs.png&#34; alt=&#34;vs.png&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Step 2.1. Install models&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;On Windows, make sure to run all commands in &lt;strong&gt;cmd&lt;/strong&gt;.&lt;/p&gt; &#xA; &lt;p&gt;DO NOT run in &lt;strong&gt;powershell&lt;/strong&gt;. Powershell has unnecessarily strict permissions and makes the script fail silently.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Currently supported engines are &lt;code&gt;llama&lt;/code&gt; and &lt;code&gt;alpaca&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Install alpaca&lt;/h4&gt; &#xA;&lt;p&gt;To download alpaca models. Open your &lt;code&gt;cmd&lt;/code&gt; application and enter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai alpaca install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Add llama models&lt;/h4&gt; &#xA;&lt;p&gt;To download llama models. Open your &lt;code&gt;cmd&lt;/code&gt; application and enter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to download multiple models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B 13B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Step 2.2. Troubleshoot (optional)&lt;/h3&gt; &#xA;&lt;p&gt;In case above steps fail, try installing Node.js and Python separately.&lt;/p&gt; &#xA;&lt;p&gt;Install Python:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/ftp/python/3.10.10/python-3.10.10-embed-amd64.zip&#34; class=&#34;btn&#34;&gt;Download Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install Node.js &amp;gt;= 18:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en/download/&#34; class=&#34;btn&#34;&gt;Download Node.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;After both have been installed, open powershell and type &lt;code&gt;python&lt;/code&gt; to see if the application exists. And also type &lt;code&gt;node&lt;/code&gt; to see if the application exists as well.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;ve checked that they both exist, try again.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, run the following command to launch the web UI server (Make sure to run in &lt;code&gt;cmd&lt;/code&gt; and not powershell!):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Linux&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;You need to make sure you have the correct version of Python and Node.js installed.&lt;/p&gt; &#xA;&lt;h4&gt;Step 1.1. Python &amp;lt;= 3.10&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pimylifeup.com/installing-python-on-linux/&#34; class=&#34;btn&#34;&gt;Download Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Make sure the version is 3.10 or lower (not 3.11) Python must be 3.10 or below (pytorch and other libraries are not supported yet on the latest)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Step 1.2. Node.js &amp;gt;= 18&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en/download/package-manager/&#34; class=&#34;btn&#34;&gt;Download node.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Make sure the version is 18 or higher&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Step 2.1. Install models&lt;/h3&gt; &#xA;&lt;p&gt;Currently supported engines are &lt;code&gt;llama&lt;/code&gt; and &lt;code&gt;alpaca&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Add alpaca models&lt;/h4&gt; &#xA;&lt;p&gt;To download alpaca models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai alpaca install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Add llama models&lt;/h4&gt; &#xA;&lt;p&gt;To download llama models, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to download multiple models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B 13B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2.2. Troubleshoot&lt;/h3&gt; &#xA;&lt;p&gt;In case the model install silently fails or hangs forever, try the following command, and try running the npx command again:&lt;/p&gt; &#xA;&lt;p&gt;On ubuntu/debian/etc.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install build-essential python3-venv -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On fedora/etc.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dnf install make automake gcc gcc-c++ kernel-devel python3-virtualenv -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 3. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, run the following command to launch the web UI server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;API&lt;/h1&gt; &#xA;&lt;p&gt;Dalai is also an NPM package:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;programmatically install&lt;/li&gt; &#xA; &lt;li&gt;locally make requests to the model&lt;/li&gt; &#xA; &lt;li&gt;run a dalai server (powered by socket.io)&lt;/li&gt; &#xA; &lt;li&gt;programmatically make requests to a remote dalai server (via socket.io)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Dalai is an NPM package. You can install it using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install dalai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;1. constructor()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai(home)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;home&lt;/code&gt;: (optional) manually specify the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By default, Dalai automatically stores the entire &lt;code&gt;llama.cpp&lt;/code&gt; repository under &lt;code&gt;~/llama.cpp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;However, often you may already have a &lt;code&gt;llama.cpp&lt;/code&gt; repository somewhere else on your machine and want to just use that folder. In this case you can pass in the &lt;code&gt;home&lt;/code&gt; attribute.&lt;/p&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;h4&gt;Basic&lt;/h4&gt; &#xA;&lt;p&gt;Creates a workspace at &lt;code&gt;~/llama.cpp&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Custom path&lt;/h4&gt; &#xA;&lt;p&gt;Manually set the &lt;code&gt;llama.cpp&lt;/code&gt; path:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai(&#34;/Documents/llama.cpp&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;2. request()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.request(req, callback)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;req&lt;/code&gt;: a request object. made up of the following attributes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;prompt&lt;/code&gt;: &lt;strong&gt;(required)&lt;/strong&gt; The prompt string&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: &lt;strong&gt;(required)&lt;/strong&gt; The model type + model name to query. Takes the following form: &lt;code&gt;&amp;lt;model_type&amp;gt;.&amp;lt;model_name&amp;gt;&lt;/code&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Example: &lt;code&gt;alpaca.7B&lt;/code&gt;, &lt;code&gt;llama.13B&lt;/code&gt;, ...&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;url&lt;/code&gt;: only needed if connecting to a remote dalai server &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;if unspecified, it uses the node.js API to directly run dalai locally&lt;/li&gt; &#xA;     &lt;li&gt;if specified (for example &lt;code&gt;ws://localhost:3000&lt;/code&gt;) it looks for a socket.io endpoint at the URL and connects to it.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;threads&lt;/code&gt;: The number of threads to use (The default is 8 if unspecified)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;n_predict&lt;/code&gt;: The number of tokens to return (The default is 128 if unspecified)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;seed&lt;/code&gt;: The seed. The default is -1 (none)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;top_k&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;top_p&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;repeat_last_n&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;repeat_penalty&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;temp&lt;/code&gt;: temperature&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;batch_size&lt;/code&gt;: batch size&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;skip_end&lt;/code&gt;: by default, every session ends with &lt;code&gt;\n\n&amp;lt;end&amp;gt;&lt;/code&gt;, which can be used as a marker to know when the full response has returned. However sometimes you may not want this suffix. Set &lt;code&gt;skip_end: true&lt;/code&gt; and the response will no longer end with &lt;code&gt;\n\n&amp;lt;end&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;callback&lt;/code&gt;: the streaming callback function that gets called every time the client gets any token response back from the model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;h4&gt;1. Node.js&lt;/h4&gt; &#xA;&lt;p&gt;Using node.js, you just need to initialize a Dalai object with &lt;code&gt;new Dalai()&lt;/code&gt; and then use it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#39;dalai&#39;)&#xA;new Dalai().request({&#xA;  model: &#34;7B&#34;,&#xA;  prompt: &#34;The following is a conversation between a boy and a girl:&#34;,&#xA;}, (token) =&amp;gt; {&#xA;  process.stdout.write(token)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Non node.js (socket.io)&lt;/h4&gt; &#xA;&lt;p&gt;To make use of this in a browser or any other language, you can use thie socket.io API.&lt;/p&gt; &#xA;&lt;h5&gt;Step 1. start a server&lt;/h5&gt; &#xA;&lt;p&gt;First you need to run a Dalai socket server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// server.js&#xA;const Dalai = require(&#39;dalai&#39;)&#xA;new Dalai().serve(3000)     // port 3000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Step 2. connect to the server&lt;/h5&gt; &#xA;&lt;p&gt;Then once the server is running, simply make requests to it by passing the &lt;code&gt;ws://localhost:3000&lt;/code&gt; socket url when initializing the Dalai object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;)&#xA;new Dalai().request({&#xA;  url: &#34;ws://localhost:3000&#34;,&#xA;  model: &#34;7B&#34;,&#xA;  prompt: &#34;The following is a conversation between a boy and a girl:&#34;,&#xA;}, (token) =&amp;gt; {&#xA;  console.log(&#34;token&#34;, token)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;3. serve()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;p&gt;Starts a socket.io server at &lt;code&gt;port&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.serve(port)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;)&#xA;new Dalai().serve(3000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;4. http()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;p&gt;connect with an existing &lt;code&gt;http&lt;/code&gt; instance (The &lt;code&gt;http&lt;/code&gt; npm package)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.http(http)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;http&lt;/code&gt;: The &lt;a href=&#34;https://nodejs.org/api/http.html&#34;&gt;http&lt;/a&gt; object&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;This is useful when you&#39;re trying to plug dalai into an existing node.js web app&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const app = require(&#39;express&#39;)();&#xA;const http = require(&#39;http&#39;).Server(app);&#xA;dalai.http(http)&#xA;http.listen(3000, () =&amp;gt; {&#xA;  console.log(&#34;server started&#34;)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;5. install()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;await dalai.install(model_type, model_name1, model_name2, ...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model_type&lt;/code&gt;: the name of the model. currently supports: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;alpaca&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&#34;llama&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model1&lt;/code&gt;, &lt;code&gt;model2&lt;/code&gt;, ...: the model names to install (&#34;7B&#34;`, &#34;13B&#34;, &#34;30B&#34;, &#34;65B&#34;, etc)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;Install Llama &#34;7B&#34; and &#34;13B&#34; models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;await dalai.install(&#34;llama&#34;, &#34;7B&#34;, &#34;13B&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install alpaca 7B model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;await dalai.install(&#34;alpaca&#34;, &#34;7B&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;6. installed()&lt;/h2&gt; &#xA;&lt;p&gt;returns the array of installed models&lt;/p&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const models = await dalai.installed()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;const models = await dalai.installed()&#xA;console.log(models)     // prints [&#34;7B&#34;, &#34;13B&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!--&#xA;&#xA;---&#xA;&#xA;## 7. download()&#xA;&#xA;Download models.&#xA;&#xA;There are two download options:&#xA;&#xA;1. **LLaMA:** Download the original LLaMA model, convert it, and quantize (compress) it&#xA;2. **LLaMA.zip:** Download the compressed version (generated from step 1 and published on HuggingFace)&#xA;&#xA;### Syntax&#xA;&#xA;```javascript&#xA;await dalai.download(model1, model2, model3, ...)&#xA;```&#xA;&#xA;- `models`: the model names to install. Can be: &#34;7B&#34;`, &#34;13B&#34;, &#34;30B&#34;, &#34;65B&#34;, &#34;7B.zip&#34;, &#34;13B.zip&#34;, &#34;30B.zip&#34;, &#34;65B.zip&#34;&#xA;  - &#34;7B&#34;, &#34;13B&#34;, &#34;30B&#34;, &#34;65B&#34;: download the raw model, convert, and quantize&#xA;  - &#34;7B.zip&#34;, &#34;13B.zip&#34;, &#34;30B.zip&#34;, &#34;65B.zip&#34;: download the quantized model (no need to waste time downloading huge files)&#xA;&#xA;### Examples&#xA;&#xA;Install the &#34;7B&#34; and &#34;13B&#34; models:&#xA;&#xA;&#xA;```javascript&#xA;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;await dalai.install(&#34;7B&#34;, &#34;13B&#34;)&#xA;```&#xA;&#xA;--&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h2&gt;Using a different home folder&lt;/h2&gt; &#xA;&lt;p&gt;By default Dalai uses your home directory to store the entire repository (&lt;code&gt;~/dalai&lt;/code&gt;). However sometimes you may want to store the archive elsewhere.&lt;/p&gt; &#xA;&lt;p&gt;In this case you can call all CLI methods using the &lt;code&gt;--home&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;h3&gt;1. Installing models to a custom path&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama install 7B --home ~/test_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Serving from the custom path&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai serve --home ~/test_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Updating to the latest&lt;/h2&gt; &#xA;&lt;p&gt;To make sure you update to the latest, first find the latest version at &lt;a href=&#34;https://www.npmjs.com/package/dalai&#34;&gt;https://www.npmjs.com/package/dalai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s say the latest version is &lt;code&gt;0.3.0&lt;/code&gt;. To update the dalai version, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai@0.3.0 setup&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Staying up to date&lt;/h2&gt; &#xA;&lt;p&gt;Have questions or feedback? Follow the project through the following outlets:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cocktailpeanut/dalai&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-github&#34;&gt;&lt;/i&gt; GitHub&lt;/a&gt; &lt;a href=&#34;https://twitter.com/cocktailpeanut&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-twitter&#34;&gt;&lt;/i&gt; Twitter&lt;/a&gt; &lt;a href=&#34;https://discord.gg/WWfgrzzkCT&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-discord&#34;&gt;&lt;/i&gt; Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>thinkingjimmy/Learning-Prompt</title>
    <updated>2023-03-26T01:51:00Z</updated>
    <id>tag:github.com,2023-03-26:/thinkingjimmy/Learning-Prompt</id>
    <link href="https://github.com/thinkingjimmy/Learning-Prompt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;免费 Prompt Engineering 教程&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/thinkingjimmy/Learning-Prompt/main/static/img/logo.svg?sanitize=true&#34; width=&#34;180px&#34;&gt; &#xA; &lt;h1&gt;👋 Welcome to Learning Prompt&lt;/h1&gt; &#xA; &lt;p&gt; &lt;strong&gt;免费 Prompt Engineering 教程&lt;/strong&gt; &lt;/p&gt; &#xA; &lt;h4&gt; &lt;a href=&#34;https://learningprompt.wiki/&#34;&gt;阅读教程&lt;/a&gt; &lt;span&gt; | &lt;/span&gt; &lt;a href=&#34;https://mcousdyt7h.feishu.cn/share/base/form/shrcn8p8MEmbkTiCDyVVPmdUoSg&#34;&gt;填写反馈表单&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;😎 关于本教程&lt;/h2&gt; &#xA;&lt;p&gt;如果你不知道能拿 ChatGPT 或者其他 AI 产品来干什么；&lt;/p&gt; &#xA;&lt;p&gt;如果你不知道如何更好地使用 OpenAI 提供的 API；&lt;/p&gt; &#xA;&lt;p&gt;那本教程应该能帮到你。&lt;/p&gt; &#xA;&lt;p&gt;这是一份教你如何更好使好地使用 ChatGPT 和其他 AI 产品的免费教程。&lt;/p&gt; &#xA;&lt;p&gt;它不是什么？&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;本教程不是 prompt 大全，如果你预期是找到能直接用的 prompt，建议你谷歌找一找。本教程更多地是教你方法，以及解释这些方法为何有效。&lt;/li&gt; &#xA; &lt;li&gt;本教程不是权威指南，在这个领域我也只是学生。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🤔 为什么会有这个教程？&lt;/h2&gt; &#xA;&lt;p&gt;我最近一直在研究 Prompt Engineering 相关的知识，然后发现大多数教程都是英文的，有些教程作者甚至是 NLP 或者 ML 的专家，内容都非常好，但对我这种初学者来说，阅读起来还是有点吃力。&lt;/p&gt; &#xA;&lt;p&gt;所以我就萌生出了自己写一份教程的想法，想着通过输出倒逼输入的方式去学习。&lt;/p&gt; &#xA;&lt;p&gt;另外，再说说感性的话，每次看到各种专家吐槽中国 AI 发展太功利，都是往钱看，不够有远见，所以错失了不少机会。&lt;/p&gt; &#xA;&lt;p&gt;坦率说来，我觉得就像当年吐槽中国为了发展经济不顾环保一样，我们难道不知道环保很重要吗？并不是，我们家底就这样，肯定会有所取舍的。&lt;/p&gt; &#xA;&lt;p&gt;我们并不缺提出问题的人，与其吐槽当年不努力，不如现在埋头苦干，贡献自己的力量。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;往者不可谏，来者犹可追。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;所以就有了这份教程。&lt;/p&gt; &#xA;&lt;h2&gt;📖 如何阅读？&lt;/h2&gt; &#xA;&lt;p&gt;很感谢你打开这份教程，但我建议你在阅读这份教程的时候：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;降低预期：&lt;/strong&gt; 我不是专家，我也在学习，我只是比你多走了几步而已。教程里的内容难免会有遗漏或错误。另外，我这份教程，目标读者是初学者，所以在教程中，为了让大家更容易理解，难免会用到不太准确的类比，或者用词，请各位见谅。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;不要吝啬你的反馈：&lt;/strong&gt; 如果你遇到无法看懂的地方，亦或者我写错了的地方，不妨给我提个 Issue 或者填写这个表单。我们共同进步，并为 PE 的科普出一份力。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;通过输出倒逼输入：&lt;/strong&gt; 最好的学习方法就是实操，教程里会提供不少案例，你边阅读，边尝试。甚至如果可以的话，不妨将你写的 prompt 通过 Issue 或表单的方式，分享给我，我会将不错的案例加到文档内。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;你可以根据你的需求来阅读此教程：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;如果你对 AI 和 Prompt Engineering 不是很了解，建议你从基础篇开始读起。基础篇更多的会从用户的角度教你如何使用 AI 产品（目前仅更新了面向文字类 AI，后续会更新 Midjourney 的内容），或者换句话说，会讲更多 prompt 的内容。&lt;/li&gt; &#xA; &lt;li&gt;如果你已经了解基本的用法，并且想要学习如何更好地开发 AI 产品，想了解更多 prompt engineering 的内容。那可以直接跳去高级篇阅读。&lt;/li&gt; &#xA; &lt;li&gt;如果你只是想快速了解使用 AI 产品的技巧，你可以直接读技巧篇，那里汇总了所有使用技巧。&lt;/li&gt; &#xA; &lt;li&gt;如果你已经了解如何使用了，但想了解更多使用场景，可以看看资料 &amp;amp; 产品推荐篇。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;另外，我一直认为最好的学习方法，就是实践，所以我建议你使用 ChatGPT 运行一下教程里的案例。这样能够记得更牢靠一些。&lt;/p&gt; &#xA;&lt;p&gt;最后，在读文档的过程中，你会看到以下几个 emoji：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🚧 ：标有这个 emoji 代表内容，我还需要进一步去完善补充，但并不影响你的阅读。我会在后续迭代补充。&lt;/li&gt; &#xA; &lt;li&gt;🆘 ：这个代表我需要各位的帮助，比如有可能是希望大家给我一些某场景的 prompt 案例等。如果你有想法，不妨通过 Issue 或&lt;a href=&#34;https://mcousdyt7h.feishu.cn/share/base/form/shrcn8p8MEmbkTiCDyVVPmdUoSg&#34;&gt;表单&lt;/a&gt;的方式，向我反馈。&lt;/li&gt; &#xA; &lt;li&gt;🔴：为了让大家能更平滑地学习高级篇的内容，我会在基础篇提一些高级篇的概念，当你看到这个红色圆时，就表示这是高级内容，你可以不深究，先了解即可。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🤝 如何贡献一份你的力量？&lt;/h2&gt; &#xA;&lt;p&gt;如果可以的话，不妨在 Github Page 上给我点赞，或者将本教程分享给其他人。感谢~&lt;/p&gt; &#xA;&lt;p&gt;在阅读这个文档的时候：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;遇到错别字、句子不通顺，亦或者读不懂的地方，欢迎通过 Issue 或&lt;a href=&#34;https://mcousdyt7h.feishu.cn/share/base/form/shrcn8p8MEmbkTiCDyVVPmdUoSg&#34;&gt;表单&lt;/a&gt;的方式，反馈给我，我会对其进行修改。一经采纳会将你的名字罗列在感谢人名单内。&lt;/li&gt; &#xA; &lt;li&gt;AI 的发展非常快，今天写的教程，很有可能明天就过期了。如果你在文档里看到有什么落后的内容，比如某些指令已经不需要了，或指令不生效了，也欢迎通过 Issue 或&lt;a href=&#34;https://mcousdyt7h.feishu.cn/share/base/form/shrcn8p8MEmbkTiCDyVVPmdUoSg&#34;&gt;表单&lt;/a&gt;的方式，向我反馈。&lt;/li&gt; &#xA; &lt;li&gt;看到标有 🆘 的内容，然后你又有不错的想法，不妨通过 Issue 或&lt;a href=&#34;https://mcousdyt7h.feishu.cn/share/base/form/shrcn8p8MEmbkTiCDyVVPmdUoSg&#34;&gt;表单&lt;/a&gt;的方式，向我反馈你的想法。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;在感谢人名单页面里，我会罗列帮助过我的朋友们 😁&lt;/p&gt; &#xA;&lt;p&gt;最后，你也可以给我&lt;a href=&#34;https://www.buymeacoffee.com/thinkingjimmy&#34;&gt;买一杯咖啡&lt;/a&gt;。或者购买我的其他付费内容产品：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://xiaobot.net/p/xiaobushous1?refer=599951e8-47eb-4898-aa3b-a7d0a1a06b0f&#34;&gt;播客小捕手：&lt;/a&gt;如果你喜欢播客，不妨看看，未来我会在上面尝试更多 AI 相关的想法。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://xiaobot.net/p/suiyisouxun2023?refer=599951e8-47eb-4898-aa3b-a7d0a1a06b0f&#34;&gt;随意搜寻 | 2023&lt;/a&gt; ：我的付费 newsletter，每周更新。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;❤️ 感谢&lt;/h2&gt; &#xA;&lt;p&gt;本教程制作过程中，内容和案例参考了以下教程或文档：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dair-ai/Prompt-Engineering-Guide&#34;&gt;Prompt-Engineering-Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learnprompting.org/&#34;&gt;Learn Prompting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mattnigh/ChatGPT3-Free-Prompt-List&#34;&gt;ChatGPT3-Free-Prompt-List&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf&#34;&gt;Natural Language Processing with Deep Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.edx.org/course/introduction-to-chatgpt&#34;&gt;edx ChatGPT101&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/examples&#34;&gt;OpenAI Examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>canisminor1990/sd-web-ui-kitchen-theme</title>
    <updated>2023-03-26T01:51:00Z</updated>
    <id>tag:github.com,2023-03-26:/canisminor1990/sd-web-ui-kitchen-theme</id>
    <link href="https://github.com/canisminor1990/sd-web-ui-kitchen-theme" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kitchen theme for stable diffusion webui https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kitchen Theme for stable-diffusion-web-ui&lt;/h1&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/canisminor1990/sd-web-ui-kitchen-theme/raw/main/assets/screenshot.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;As an extension (recommended) Either clone the repo into your extensions folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone &#34;https://github.com/canisminor1990/sd-web-ui-kitchen-theme&#34; extensions/kitchen-theme&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(The second argument specifies the name of the folder, you can choose whatever you like).&lt;/p&gt;</summary>
  </entry>
</feed>