<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub CSS Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-25T01:54:38Z</updated>
  <subtitle>Weekly Trending of CSS in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>LemmyNet/lemmy-ui</title>
    <updated>2023-06-25T01:54:38Z</updated>
    <id>tag:github.com,2023-06-25:/LemmyNet/lemmy-ui</id>
    <link href="https://github.com/LemmyNet/lemmy-ui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official web app for lemmy.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Lemmy-UI&lt;/h1&gt; &#xA;&lt;p&gt;The official web app for &lt;a href=&#34;https://github.com/LemmyNet/lemmy&#34;&gt;Lemmy&lt;/a&gt;, written in inferno.&lt;/p&gt; &#xA;&lt;p&gt;Based off of MrFoxPro&#39;s &lt;a href=&#34;https://github.com/MrFoxPro/inferno-isomorphic-template&#34;&gt;inferno-isomorphic-template&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The following environment variables can be used to configure lemmy-ui:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;code&gt;ENV_VAR&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;type&lt;/th&gt; &#xA;   &lt;th&gt;default&lt;/th&gt; &#xA;   &lt;th&gt;description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;LEMMY_UI_HOST&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0.0.0.0:1234&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The IP / port that the lemmy-ui isomorphic node server is hosted at.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;LEMMY_UI_LEMMY_INTERNAL_HOST&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0.0.0.0:8536&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The internal IP / port that lemmy is hosted at. Often &lt;code&gt;lemmy:8536&lt;/code&gt; if using docker.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;LEMMY_UI_LEMMY_EXTERNAL_HOST&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0.0.0.0:8536&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The external IP / port that lemmy is hosted at. Often &lt;code&gt;DOMAIN.TLD&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;LEMMY_UI_HTTPS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Whether to use https.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;LEMMY_UI_EXTRA_THEMES_FOLDER&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;./extra_themes&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A location for additional lemmy css themes.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;LEMMY_UI_DEBUG&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Loads the &lt;a href=&#34;https://github.com/liriliri/eruda&#34;&gt;Eruda&lt;/a&gt; debugging utility.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;LEMMY_UI_DISABLE_CSP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Disables CSP security headers&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;LEMMY_UI_CUSTOM_HTML_HEADER&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;string&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Injects a custom script into &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>patrickloeber/chatbot-deployment</title>
    <updated>2023-06-25T01:54:38Z</updated>
    <id>tag:github.com,2023-06-25:/patrickloeber/chatbot-deployment</id>
    <link href="https://github.com/patrickloeber/chatbot-deployment" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Deployment of PyTorch chatbot with Flask&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chatbot Deployment with Flask and JavaScript&lt;/h1&gt; &#xA;&lt;p&gt;In this tutorial we deploy the chatbot I created in &lt;a href=&#34;https://github.com/python-engineer/pytorch-chatbot&#34;&gt;this&lt;/a&gt; tutorial with Flask and JavaScript.&lt;/p&gt; &#xA;&lt;p&gt;This gives 2 deployment options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deploy within Flask app with jinja2 template&lt;/li&gt; &#xA; &lt;li&gt;Serve only the Flask prediction API. The used html and javascript files can be included in any Frontend application (with only a slight modification) and can run completely separate from the Flask App then.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Initial Setup:&lt;/h2&gt; &#xA;&lt;p&gt;This repo currently contains the starter files.&lt;/p&gt; &#xA;&lt;p&gt;Clone repo and create a virtual environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/python-engineer/chatbot-deployment.git&#xA;$ cd chatbot-deployment&#xA;$ python3 -m venv venv&#xA;$ . venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependencies&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ (venv) pip install Flask torch torchvision nltk&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install nltk package&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ (venv) python&#xA;&amp;gt;&amp;gt;&amp;gt; import nltk&#xA;&amp;gt;&amp;gt;&amp;gt; nltk.download(&#39;punkt&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Modify &lt;code&gt;intents.json&lt;/code&gt; with different intents and responses for your Chatbot&lt;/p&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ (venv) python train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will dump data.pth file. And then run the following command to test it in the console.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ (venv) python chat.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now for deployment follow my tutorial to implement &lt;code&gt;app.py&lt;/code&gt; and &lt;code&gt;app.js&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Watch the Tutorial&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/a37BL0stIuM&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/a37BL0stIuM/hqdefault.jpg&#34; alt=&#34;Alt text&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://youtu.be/a37BL0stIuM&#34;&gt;https://youtu.be/a37BL0stIuM&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Note&lt;/h2&gt; &#xA;&lt;p&gt;In the video we implement the first approach using jinja2 templates within our Flask app. Only slight modifications are needed to run the frontend separately. I put the final frontend code for a standalone frontend application in the &lt;a href=&#34;https://raw.githubusercontent.com/patrickloeber/chatbot-deployment/main/standalone-frontend&#34;&gt;standalone-frontend&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Credits:&lt;/h2&gt; &#xA;&lt;p&gt;This repo was used for the frontend code: &lt;a href=&#34;https://github.com/hitchcliff/front-end-chatjs&#34;&gt;https://github.com/hitchcliff/front-end-chatjs&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ParisNeo/lollms-webui</title>
    <updated>2023-06-25T01:54:38Z</updated>
    <id>tag:github.com,2023-06-25:/ParisNeo/lollms-webui</id>
    <link href="https://github.com/ParisNeo/lollms-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;gpt4all chatbot ui&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LoLLMS Web UI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/license/ParisNeo/lollms-webui&#34; alt=&#34;GitHub license&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/ParisNeo/lollms-webui&#34; alt=&#34;GitHub issues&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/ParisNeo/lollms-webui&#34; alt=&#34;GitHub stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/ParisNeo/lollms-webui&#34; alt=&#34;GitHub forks&#34;&gt; &lt;a href=&#34;https://discord.gg/4rR282WJb6&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1092918764925882418?color=7289da&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=ffffff&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/SpaceNerduino&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/SpaceNerduino?style=social&#34; alt=&#34;Follow me on Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/user/Parisneo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Follow%20Me%20on-YouTube-red?style=flat&amp;amp;logo=youtube&#34; alt=&#34;Follow Me on YouTube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ParisNeo/lollms-webui/actions/workflows/pages/pages-build-deployment&#34;&gt;&lt;img src=&#34;https://github.com/ParisNeo/lollms-webui/actions/workflows/pages/pages-build-deployment/badge.svg?sanitize=true&#34; alt=&#34;pages-build-deployment&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome to LoLLMS WebUI (Lord of Large Language Models: One tool to rule them all), the hub for LLM (Large Language Model) models. This project aims to provide a user-friendly interface to access and utilize various LLM models for a wide range of tasks. Whether you need help with writing, coding, organizing data, generating images, or seeking answers to your questions, LoLLMS WebUI has got you covered.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/ds_U0TDzbzI&#34;&gt;Click here for my youtube video on how to use the tool&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Choose your preferred binding, model, and personality for your tasks&lt;/li&gt; &#xA; &lt;li&gt;Enhance your emails, essays, code debugging, thought organization, and more&lt;/li&gt; &#xA; &lt;li&gt;Explore a wide range of functionalities, such as searching, data organization, and image generation&lt;/li&gt; &#xA; &lt;li&gt;Easy-to-use UI with light and dark mode options&lt;/li&gt; &#xA; &lt;li&gt;Integration with GitHub repository for easy access&lt;/li&gt; &#xA; &lt;li&gt;Support for different personalities with predefined welcome messages&lt;/li&gt; &#xA; &lt;li&gt;Thumb up/down rating for generated answers&lt;/li&gt; &#xA; &lt;li&gt;Copy, edit, and remove messages&lt;/li&gt; &#xA; &lt;li&gt;Local database storage for your discussions&lt;/li&gt; &#xA; &lt;li&gt;Search, export, and delete multiple discussions&lt;/li&gt; &#xA; &lt;li&gt;Support for Docker, conda, and manual virtual environment setups&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;Before installing LoLLMS WebUI, make sure you have the following dependencies installed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/downloads/release/python-3100/&#34;&gt;Python 3.10 or higher&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pip - installation depends on OS, but make sure you have it installed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;Git (for cloning the repository)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://visualstudio.microsoft.com/vs/community/&#34;&gt;Visual Studio Community&lt;/a&gt; with c++ build tools (for CUDA [nvidia GPU&#39;s]) - optional for windows&lt;/li&gt; &#xA; &lt;li&gt;Build essentials (for CUDA [nvidia GPU&#39;s]) - optional for linux&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;Nvidia CUDA toolkit 11.7 or higher&lt;/a&gt; (for CUDA [nvidia GPU&#39;s]) - optional&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda3&lt;/a&gt; - optional (more stable than python)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Ensure that the Python installation is in your system&#39;s PATH, and you can call it from the terminal. To verify your Python version, run the following command:&lt;/p&gt; &#xA;&lt;p&gt;Windows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python --version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Linux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 --version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you receive an error or the version is lower than 3.10, please install a newer version and try again.&lt;/p&gt; &#xA;&lt;h3&gt;Installation steps&lt;/h3&gt; &#xA;&lt;p&gt;For detailed installation steps please refer to these documents:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ParisNeo/lollms-webui/main/docs/usage/AdvancedInstallInstructions.md#windows-10-and-11&#34;&gt;Windows 10/11&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ParisNeo/lollms-webui/main/docs/usage/AdvancedInstallInstructions.md#linux&#34;&gt;Linux (tested on ubuntu)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Easy install&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the appropriate application launcher based on your platform: For Windows: &lt;code&gt;webui.bat&lt;/code&gt; For Linux: &lt;code&gt;webui.sh&lt;/code&gt; For Linux: &lt;code&gt;c_webui.sh&lt;/code&gt; - using miniconda3&lt;/li&gt; &#xA; &lt;li&gt;Place the downloaded launcher in a folder of your choice, for example: Windows: &lt;code&gt;C:\ai\LoLLMS-webui&lt;/code&gt; Linux: &lt;code&gt;/home/user/ai/LoLLMS-webui&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run the launcher script. Note that you might encounter warnings from antivirus or Windows Defender due to the tool&#39;s newness and limited usage. These warnings are false positives caused by reputation conditions in some antivirus software. You can safely proceed with running the script. Once the installation is complete, the LoLLMS WebUI will launch automatically.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Using Conda&lt;/h4&gt; &#xA;&lt;p&gt;If you use conda, you can create a virtual environment and install the required packages using the provided &lt;code&gt;requirements.txt&lt;/code&gt; file. Here&#39;s an example of how to set it up: First clone the project or download the zip file and unzip it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ParisNeo/lollms-webui.git&#xA;cd lollms-webui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now create a new conda environment, activate it and install requirements&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --prefix ./env python=3.10&#xA;conda activate ./env&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using Docker&lt;/h4&gt; &#xA;&lt;p&gt;Alternatively, you can use Docker to set up the LoLLMS WebUI. Please refer to the Docker documentation for installation instructions specific to your operating system.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;You can launch the app from the webui.sh or webui.bat launcher. It will automatically perform updates if any are present. If you don&#39;t prefer this method, you can also activate the virtual environment and launch the application using python app.py from the root of the project. Once the app is running, you can go to the application front link displayed in the console (by default localhost:9600 but can change if you change configuration)&lt;/p&gt; &#xA;&lt;h3&gt;Selecting a Model and Binding&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open the LoLLMS WebUI and navigate to the Settings page.&lt;/li&gt; &#xA; &lt;li&gt;In the Models Zoo tab, select a binding from the list (e.g., llama-cpp-official).&lt;/li&gt; &#xA; &lt;li&gt;Wait for the installation process to finish. You can monitor the progress in the console.&lt;/li&gt; &#xA; &lt;li&gt;Once the installation is complete, click the Install button next to the desired model.&lt;/li&gt; &#xA; &lt;li&gt;After the model installation finishes, select the model and press Apply changes.&lt;/li&gt; &#xA; &lt;li&gt;Remember to press the Save button to save the configuration.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Starting a Discussion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go to the Discussions view.&lt;/li&gt; &#xA; &lt;li&gt;Click the + button to create a new discussion.&lt;/li&gt; &#xA; &lt;li&gt;You will see a predefined welcome message based on the selected personality (by default, LoLLMS).&lt;/li&gt; &#xA; &lt;li&gt;Ask a question or provide an initial prompt to start the discussion.&lt;/li&gt; &#xA; &lt;li&gt;You can stop the generation process at any time by pressing the Stop Generating button.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Managing Discussions&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To edit a discussion title, simply type a new title or modify the existing one.&lt;/li&gt; &#xA; &lt;li&gt;To delete a discussion, click the Delete button.&lt;/li&gt; &#xA; &lt;li&gt;To search for specific discussions, use the search button and enter relevant keywords.&lt;/li&gt; &#xA; &lt;li&gt;To perform batch operations (exporting or deleting multiple discussions), enable Check Mode, select the discussions, and choose the desired action.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Contributions to LoLLMS WebUI are welcome! If you encounter any issues, have ideas for improvements, or want to contribute code, please open an issue or submit a pull request on the GitHub repository.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 License. You are free to use this software commercially, build upon it, and integrate it into your own projects. See the &lt;a href=&#34;https://github.com/ParisNeo/lollms-webui/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Please note that LoLLMS WebUI is not affiliated with the LoLLMS application developed by Nomic AI. The latter is a separate professional application available at LoLLMS.io, which has its own unique features and community.&lt;/p&gt; &#xA;&lt;p&gt;We express our gratitude to all the contributors who have made this project possible and welcome additional contributions to further enhance the tool for the benefit of all users.&lt;/p&gt; &#xA;&lt;h1&gt;Contact&lt;/h1&gt; &#xA;&lt;p&gt;For any questions or inquiries, feel free to reach out via our discord server: &lt;a href=&#34;https://discord.gg/4rR282WJb6&#34;&gt;https://discord.gg/4rR282WJb6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thank you for your interest and support!&lt;/p&gt; &#xA;&lt;p&gt;If you find this tool useful, don&#39;t forget to give it a star on GitHub, share your experience, and help us spread the word. Your feedback and bug reports are valuable to us as we continue developing and improving LoLLMS WebUI.&lt;/p&gt; &#xA;&lt;p&gt;If you enjoyed this tutorial, consider subscribing to our YouTube channel for more updates, tutorials, and exciting content.&lt;/p&gt; &#xA;&lt;p&gt;Happy exploring with LoLLMS WebUI!&lt;/p&gt;</summary>
  </entry>
</feed>