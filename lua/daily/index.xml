<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Lua Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-09T01:45:56Z</updated>
  <subtitle>Daily Trending of Lua in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>junyanz/CycleGAN</title>
    <updated>2022-07-09T01:45:56Z</updated>
    <id>tag:github.com,2022-07-09:/junyanz/CycleGAN</id>
    <link href="https://github.com/junyanz/CycleGAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Software that can generate photos from paintings, turn horses into zebras, perform style transfer, and more.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/junyanz/CycleGAN/master/imgs/horse2zebra.gif&#34; align=&#34;right&#34; width=&#34;384&#34;&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;CycleGAN&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&#34;&gt;PyTorch&lt;/a&gt; | &lt;a href=&#34;https://junyanz.github.io/CycleGAN/&#34;&gt;project page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/pdf/1703.10593.pdf&#34;&gt;paper&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Torch implementation for learning an image-to-image translation (i.e. &lt;a href=&#34;https://github.com/phillipi/pix2pix&#34;&gt;pix2pix&lt;/a&gt;) &lt;strong&gt;without&lt;/strong&gt; input-output pairs, for example:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;New&lt;/strong&gt;: Please check out &lt;a href=&#34;https://github.com/taesungp/contrastive-unpaired-translation&#34;&gt;contrastive-unpaired-translation&lt;/a&gt; (CUT), our new unpaired image-to-image translation model that enables fast and memory-efficient training.&lt;/p&gt; &#xA;&lt;img src=&#34;https://junyanz.github.io/CycleGAN/images/teaser_high_res.jpg&#34; width=&#34;1000px&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://junyanz.github.io/CycleGAN/&#34;&gt;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://people.eecs.berkeley.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;*, &lt;a href=&#34;https://taesung.me/&#34;&gt;Taesung Park&lt;/a&gt;*, &lt;a href=&#34;http://web.mit.edu/phillipi/&#34;&gt;Phillip Isola&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~efros/&#34;&gt;Alexei A. Efros&lt;/a&gt;&lt;br&gt; Berkeley AI Research Lab, UC Berkeley&lt;br&gt; In ICCV 2017. (* equal contributions)&lt;/p&gt; &#xA;&lt;p&gt;This package includes CycleGAN, &lt;a href=&#34;https://github.com/phillipi/pix2pix&#34;&gt;pix2pix&lt;/a&gt;, as well as other methods like &lt;a href=&#34;https://arxiv.org/abs/1605.09782&#34;&gt;BiGAN&lt;/a&gt;/&lt;a href=&#34;https://ishmaelbelghazi.github.io/ALI/&#34;&gt;ALI&lt;/a&gt; and Apple&#39;s paper &lt;a href=&#34;https://arxiv.org/pdf/1612.07828.pdf&#34;&gt;S+U learning&lt;/a&gt;.&lt;br&gt; The code was written by &lt;a href=&#34;https://github.com/junyanz&#34;&gt;Jun-Yan Zhu&lt;/a&gt; and &lt;a href=&#34;https://github.com/taesung&#34;&gt;Taesung Park&lt;/a&gt;.&lt;br&gt; &lt;strong&gt;Update&lt;/strong&gt;: Please check out &lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&#34;&gt;PyTorch&lt;/a&gt; implementation for CycleGAN and pix2pix. The PyTorch version is under active development and can produce results comparable or better than this Torch version.&lt;/p&gt; &#xA;&lt;h2&gt;Other implementations:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/leehomyc/cyclegan-1&#34;&gt; [Tensorflow]&lt;/a&gt; (by Harry Yang), &lt;a href=&#34;https://github.com/architrathore/CycleGAN/&#34;&gt;[Tensorflow]&lt;/a&gt; (by Archit Rathore), &lt;a href=&#34;https://github.com/vanhuyz/CycleGAN-TensorFlow&#34;&gt;[Tensorflow]&lt;/a&gt; (by Van Huy), &lt;a href=&#34;https://github.com/XHUJOY/CycleGAN-tensorflow&#34;&gt;[Tensorflow]&lt;/a&gt; (by Xiaowei Hu), &lt;a href=&#34;https://github.com/LynnHo/CycleGAN-Tensorflow-Simple&#34;&gt; [Tensorflow-simple]&lt;/a&gt; (by Zhenliang He), &lt;a href=&#34;https://github.com/luoxier/CycleGAN_Tensorlayer&#34;&gt; [TensorLayer]&lt;/a&gt; (by luoxier), &lt;a href=&#34;https://github.com/Aixile/chainer-cyclegan&#34;&gt;[Chainer]&lt;/a&gt; (by Yanghua Jin), &lt;a href=&#34;https://github.com/yunjey/mnist-svhn-transfer&#34;&gt;[Minimal PyTorch]&lt;/a&gt; (by yunjey), &lt;a href=&#34;https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN&#34;&gt;[Mxnet]&lt;/a&gt; (by Ldpe2G), &lt;a href=&#34;https://github.com/tjwei/GANotebooks&#34;&gt;[lasagne/Keras]&lt;/a&gt; (by tjwei), &lt;a href=&#34;https://github.com/simontomaskarlsson/CycleGAN-Keras&#34;&gt;[Keras]&lt;/a&gt; (by Simon Karlsson)&lt;/p&gt;  &#xA;&lt;h2&gt;Applications&lt;/h2&gt; &#xA;&lt;h3&gt;Monet Paintings to Photos&lt;/h3&gt; &#xA;&lt;img src=&#34;https://junyanz.github.io/CycleGAN/images/painting2photo.jpg&#34; width=&#34;1000px&#34;&gt; &#xA;&lt;h3&gt;Collection Style Transfer&lt;/h3&gt; &#xA;&lt;img src=&#34;https://junyanz.github.io/CycleGAN/images/photo2painting.jpg&#34; width=&#34;1000px&#34;&gt; &#xA;&lt;h3&gt;Object Transfiguration&lt;/h3&gt; &#xA;&lt;img src=&#34;https://junyanz.github.io/CycleGAN/images/objects.jpg&#34; width=&#34;1000px&#34;&gt; &#xA;&lt;h3&gt;Season Transfer&lt;/h3&gt; &#xA;&lt;img src=&#34;https://junyanz.github.io/CycleGAN/images/season.jpg&#34; width=&#34;1000px&#34;&gt; &#xA;&lt;h3&gt;Photo Enhancement: Narrow depth of field&lt;/h3&gt; &#xA;&lt;img src=&#34;https://junyanz.github.io/CycleGAN/images/photo_enhancement.jpg&#34; width=&#34;1000px&#34;&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux or OSX&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU + CUDA CuDNN (CPU mode and CUDA without CuDNN may work with minimal modification, but untested)&lt;/li&gt; &#xA; &lt;li&gt;For MAC users, you need the Linux/GNU commands &lt;code&gt;gfind&lt;/code&gt; and &lt;code&gt;gwc&lt;/code&gt;, which can be installed with &lt;code&gt;brew install findutils coreutils&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install torch and dependencies from &lt;a href=&#34;https://github.com/torch/distro&#34;&gt;https://github.com/torch/distro&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install torch packages &lt;code&gt;nngraph&lt;/code&gt;, &lt;code&gt;class&lt;/code&gt;, &lt;code&gt;display&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;luarocks install nngraph&#xA;luarocks install class&#xA;luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repo:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/junyanz/CycleGAN&#xA;cd CycleGAN&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Apply a Pre-trained Model&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the test photos (taken by &lt;a href=&#34;https://www.flickr.com/photos/aaefros&#34;&gt;Alexei Efros&lt;/a&gt;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash ./datasets/download_dataset.sh ae_photos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the pre-trained model &lt;code&gt;style_cezanne&lt;/code&gt; (For CPU model, use &lt;code&gt;style_cezanne_cpu&lt;/code&gt;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash ./pretrained_models/download_model.sh style_cezanne&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Now, let&#39;s generate Paul CÃ©zanne style images:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;DATA_ROOT=./datasets/ae_photos name=style_cezanne_pretrained model=one_direction_test phase=test loadSize=256 fineSize=256 resize_or_crop=&#34;scale_width&#34; th test.lua&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The test results will be saved to &lt;code&gt;./results/style_cezanne_pretrained/latest_test/index.html&lt;/code&gt;.&lt;br&gt; Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/junyanz/CycleGAN/master/#model-zoo&#34;&gt;Model Zoo&lt;/a&gt; for more pre-trained models. &lt;code&gt;./examples/test_vangogh_style_on_ae_photos.sh&lt;/code&gt; is an example script that downloads the pretrained Van Gogh style network and runs it on Efros&#39;s photos.&lt;/p&gt; &#xA;&lt;h3&gt;Train&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download a dataset (e.g. zebra and horse images from ImageNet):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./datasets/download_dataset.sh horse2zebra&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Train a model:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DATA_ROOT=./datasets/horse2zebra name=horse2zebra_model th train.lua&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(CPU only) The same training command without using a GPU or CUDNN. Setting the environment variables &lt;code&gt;gpu=0 cudnn=0&lt;/code&gt; forces CPU only&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DATA_ROOT=./datasets/horse2zebra name=horse2zebra_model gpu=0 cudnn=0 th train.lua&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(Optionally) start the display server to view results as the model trains. (See &lt;a href=&#34;https://raw.githubusercontent.com/junyanz/CycleGAN/master/#display-ui&#34;&gt;Display UI&lt;/a&gt; for more details):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;th -ldisplay.start 8000 0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Test&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Finally, test the model:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DATA_ROOT=./datasets/horse2zebra name=horse2zebra_model phase=test th test.lua&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The test results will be saved to an HTML file here: &lt;code&gt;./results/horse2zebra_model/latest_test/index.html&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;Download the pre-trained models with the following script. The model will be saved to &lt;code&gt;./checkpoints/model_name/latest_net_G.t7&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./pretrained_models/download_model.sh model_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;orange2apple&lt;/code&gt; (orange -&amp;gt; apple) and &lt;code&gt;apple2orange&lt;/code&gt;: trained on ImageNet categories &lt;code&gt;apple&lt;/code&gt; and &lt;code&gt;orange&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;horse2zebra&lt;/code&gt; (horse -&amp;gt; zebra) and &lt;code&gt;zebra2horse&lt;/code&gt; (zebra -&amp;gt; horse): trained on ImageNet categories &lt;code&gt;horse&lt;/code&gt; and &lt;code&gt;zebra&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;style_monet&lt;/code&gt; (landscape photo -&amp;gt; Monet painting style), &lt;code&gt;style_vangogh&lt;/code&gt; (landscape photo -&amp;gt; Van Gogh painting style), &lt;code&gt;style_ukiyoe&lt;/code&gt; (landscape photo -&amp;gt; Ukiyo-e painting style), &lt;code&gt;style_cezanne&lt;/code&gt; (landscape photo -&amp;gt; Cezanne painting style): trained on paintings and Flickr landscape photos.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;monet2photo&lt;/code&gt; (Monet paintings -&amp;gt; real landscape): trained on paintings and Flickr landscape photographs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cityscapes_photo2label&lt;/code&gt; (street scene -&amp;gt; label) and &lt;code&gt;cityscapes_label2photo&lt;/code&gt; (label -&amp;gt; street scene): trained on the Cityscapes dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;map2sat&lt;/code&gt; (map -&amp;gt; aerial photo) and &lt;code&gt;sat2map&lt;/code&gt; (aerial photo -&amp;gt; map): trained on Google maps.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;iphone2dslr_flower&lt;/code&gt; (iPhone photos of flowers -&amp;gt; DSLR photos of flowers): trained on Flickr photos.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;CPU models can be downloaded using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash pretrained_models/download_model.sh &amp;lt;name&amp;gt;_cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;, where &lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt; can be &lt;code&gt;horse2zebra&lt;/code&gt;, &lt;code&gt;style_monet&lt;/code&gt;, etc. You just need to append &lt;code&gt;_cpu&lt;/code&gt; to the target model.&lt;/p&gt; &#xA;&lt;h2&gt;Training and Test Details&lt;/h2&gt; &#xA;&lt;p&gt;To train a model,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DATA_ROOT=/path/to/data/ name=expt_name th train.lua&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Models are saved to &lt;code&gt;./checkpoints/expt_name&lt;/code&gt; (can be changed by passing &lt;code&gt;checkpoint_dir=your_dir&lt;/code&gt; in train.lua).&lt;br&gt; See &lt;code&gt;opt_train&lt;/code&gt; in &lt;code&gt;options.lua&lt;/code&gt; for additional training options.&lt;/p&gt; &#xA;&lt;p&gt;To test the model,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DATA_ROOT=/path/to/data/ name=expt_name phase=test th test.lua&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will run the model named &lt;code&gt;expt_name&lt;/code&gt; in both directions on all images in &lt;code&gt;/path/to/data/testA&lt;/code&gt; and &lt;code&gt;/path/to/data/testB&lt;/code&gt;.&lt;br&gt; A webpage with result images will be saved to &lt;code&gt;./results/expt_name&lt;/code&gt; (can be changed by passing &lt;code&gt;results_dir=your_dir&lt;/code&gt; in test.lua).&lt;br&gt; See &lt;code&gt;opt_test&lt;/code&gt; in &lt;code&gt;options.lua&lt;/code&gt; for additional test options. Please use &lt;code&gt;model=one_direction_test&lt;/code&gt; if you only would like to generate outputs of the trained network in only one direction, and specify &lt;code&gt;which_direction=AtoB&lt;/code&gt; or &lt;code&gt;which_direction=BtoA&lt;/code&gt; to set the direction.&lt;/p&gt; &#xA;&lt;p&gt;There are other options that can be used. For example, you can specify &lt;code&gt;resize_or_crop=crop&lt;/code&gt; option to avoid resizing the image to squares. This is indeed how we trained GTA2Cityscapes model in the projet &lt;a href=&#34;https://junyanz.github.io/CycleGAN/&#34;&gt;webpage&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/1711.03213.pdf&#34;&gt;Cycada&lt;/a&gt; model. We prepared the images at 1024px resolution, and used &lt;code&gt;resize_or_crop=crop fineSize=360&lt;/code&gt; to work with the cropped images of size 360x360. We also used &lt;code&gt;lambda_identity=1.0&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Download the datasets using the following script. Many of the datasets were collected by other researchers. Please cite their papers if you use the data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./datasets/download_dataset.sh dataset_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;facades&lt;/code&gt;: 400 images from the &lt;a href=&#34;http://cmp.felk.cvut.cz/~tylecr1/facade/&#34;&gt;CMP Facades dataset&lt;/a&gt;. [&lt;a href=&#34;https://raw.githubusercontent.com/junyanz/CycleGAN/master/datasets/bibtex/facades.tex&#34;&gt;Citation&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cityscapes&lt;/code&gt;: 2975 images from the &lt;a href=&#34;https://www.cityscapes-dataset.com/&#34;&gt;Cityscapes training set&lt;/a&gt;. [&lt;a href=&#34;https://raw.githubusercontent.com/junyanz/CycleGAN/master/datasets/bibtex/cityscapes.tex&#34;&gt;Citation&lt;/a&gt;]. Note: Due to license issue, we do not host the dataset on our repo. Please download the dataset directly from the Cityscapes webpage. Please refer to &lt;code&gt;./datasets/prepare_cityscapes_dataset.py&lt;/code&gt; for more detail.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;maps&lt;/code&gt;: 1096 training images scraped from Google Maps.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;horse2zebra&lt;/code&gt;: 939 horse images and 1177 zebra images downloaded from &lt;a href=&#34;http://www.image-net.org/&#34;&gt;ImageNet&lt;/a&gt; using the keywords &lt;code&gt;wild horse&lt;/code&gt; and &lt;code&gt;zebra&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;apple2orange&lt;/code&gt;: 996 apple images and 1020 orange images downloaded from &lt;a href=&#34;http://www.image-net.org/&#34;&gt;ImageNet&lt;/a&gt; using the keywords &lt;code&gt;apple&lt;/code&gt; and &lt;code&gt;navel orange&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;summer2winter_yosemite&lt;/code&gt;: 1273 summer Yosemite images and 854 winter Yosemite images were downloaded using Flickr API. See more details in our paper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;monet2photo&lt;/code&gt;, &lt;code&gt;vangogh2photo&lt;/code&gt;, &lt;code&gt;ukiyoe2photo&lt;/code&gt;, &lt;code&gt;cezanne2photo&lt;/code&gt;: The art images were downloaded from &lt;a href=&#34;https://www.wikiart.org/&#34;&gt;Wikiart&lt;/a&gt;. The real photos are downloaded from Flickr using the combination of the tags &lt;em&gt;landscape&lt;/em&gt; and &lt;em&gt;landscapephotography&lt;/em&gt;. The training set size of each class is Monet:1074, Cezanne:584, Van Gogh:401, Ukiyo-e:1433, Photographs:6853.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;iphone2dslr_flower&lt;/code&gt;: both classes of images were downloaded from Flickr. The training set size of each class is iPhone:1813, DSLR:3316. See more details in our paper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Display UI&lt;/h2&gt; &#xA;&lt;p&gt;Optionally, for displaying images during training and test, use the &lt;a href=&#34;https://github.com/szym/display&#34;&gt;display package&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install it with: &lt;code&gt;luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Then start the server with: &lt;code&gt;th -ldisplay.start&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open this URL in your browser: &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By default, the server listens on localhost. Pass &lt;code&gt;0.0.0.0&lt;/code&gt; to allow external connections on any interface:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;th -ldisplay.start 8000 0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then open &lt;code&gt;http://(hostname):(port)/&lt;/code&gt; in your browser to load the remote desktop.&lt;/p&gt; &#xA;&lt;h2&gt;Setup Training and Test data&lt;/h2&gt; &#xA;&lt;p&gt;To train CycleGAN model on your own datasets, you need to create a data folder with two subdirectories &lt;code&gt;trainA&lt;/code&gt; and &lt;code&gt;trainB&lt;/code&gt; that contain images from domain A and B. You can test your model on your training set by setting &lt;code&gt;phase=&#39;train&#39;&lt;/code&gt; in &lt;code&gt;test.lua&lt;/code&gt;. You can also create subdirectories &lt;code&gt;testA&lt;/code&gt; and &lt;code&gt;testB&lt;/code&gt; if you have test data.&lt;/p&gt; &#xA;&lt;p&gt;You should &lt;strong&gt;not&lt;/strong&gt; expect our method to work on just any random combination of input and output datasets (e.g. &lt;code&gt;cats&amp;lt;-&amp;gt;keyboards&lt;/code&gt;). From our experiments, we find it works better if two datasets share similar visual content. For example, &lt;code&gt;landscape painting&amp;lt;-&amp;gt;landscape photographs&lt;/code&gt; works much better than &lt;code&gt;portrait painting &amp;lt;-&amp;gt; landscape photographs&lt;/code&gt;. &lt;code&gt;zebras&amp;lt;-&amp;gt;horses&lt;/code&gt; achieves compelling results while &lt;code&gt;cats&amp;lt;-&amp;gt;dogs&lt;/code&gt; completely fails. See the following section for more discussion.&lt;/p&gt; &#xA;&lt;h2&gt;Failure cases&lt;/h2&gt; &#xA;&lt;img align=&#34;left&#34; style=&#34;padding:10px&#34; src=&#34;https://junyanz.github.io/CycleGAN/images/failure_putin.jpg&#34; width=&#34;320&#34;&gt; &#xA;&lt;p&gt;Our model does not work well when the test image is rather different from the images on which the model is trained, as is the case in the figure to the left (we trained on horses and zebras without riders, but test here one a horse with a rider). See additional typical failure cases &lt;a href=&#34;https://junyanz.github.io/CycleGAN/images/failures.jpg&#34;&gt;here&lt;/a&gt;. On translation tasks that involve color and texture changes, like many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of &lt;code&gt;dog&amp;lt;-&amp;gt;cat&lt;/code&gt; transfiguration, the learned translation degenerates into making minimal changes to the input. We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method. In some cases, this gap may be very hard -- or even impossible,-- to close: for example, our method sometimes permutes the labels for tree and building in the output of the cityscapes photos-&amp;gt;labels task.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code for your research, please cite our &lt;a href=&#34;https://junyanz.github.io/CycleGAN/&#34;&gt;paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{CycleGAN2017,&#xA;  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networkss},&#xA;  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},&#xA;  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},&#xA;  year={2017}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Related Projects:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/taesungp/contrastive-unpaired-translation&#34;&gt;contrastive-unpaired-translation&lt;/a&gt; (CUT)&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://github.com/phillipi/pix2pix&#34;&gt;pix2pix-Torch&lt;/a&gt; | &lt;a href=&#34;https://github.com/NVIDIA/pix2pixHD&#34;&gt;pix2pixHD&lt;/a&gt; | &lt;a href=&#34;https://github.com/junyanz/BicycleGAN&#34;&gt;BicycleGAN&lt;/a&gt; | &lt;a href=&#34;https://tcwang0509.github.io/vid2vid/&#34;&gt;vid2vid&lt;/a&gt; | &lt;a href=&#34;https://github.com/NVlabs/SPADE&#34;&gt;SPADE/GauGAN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://github.com/junyanz/iGAN&#34;&gt;iGAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/CSAILVision/GANDissect&#34;&gt;GAN Dissection&lt;/a&gt; | &lt;a href=&#34;http://ganpaint.io/&#34;&gt;GAN Paint&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Cat Paper Collection&lt;/h2&gt; &#xA;&lt;p&gt;If you love cats, and love reading cool graphics, vision, and ML papers, please check out the Cat Paper &lt;a href=&#34;https://github.com/junyanz/CatPapers&#34;&gt;Collection&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Code borrows from &lt;a href=&#34;https://github.com/phillipi/pix2pix&#34;&gt;pix2pix&lt;/a&gt; and &lt;a href=&#34;https://github.com/soumith/dcgan.torch&#34;&gt;DCGAN&lt;/a&gt;. The data loader is modified from &lt;a href=&#34;https://github.com/soumith/dcgan.torch&#34;&gt;DCGAN&lt;/a&gt; and &lt;a href=&#34;https://github.com/pathak22/context-encoder&#34;&gt;Context-Encoder&lt;/a&gt;. The generative network is adopted from &lt;a href=&#34;https://github.com/jcjohnson/neural-style&#34;&gt;neural-style&lt;/a&gt; with &lt;a href=&#34;https://github.com/DmitryUlyanov/texture_nets/raw/master/InstanceNormalization.lua&#34;&gt;Instance Normalization&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>axkirillov/easypick.nvim</title>
    <updated>2022-07-09T01:45:56Z</updated>
    <id>tag:github.com,2022-07-09:/axkirillov/easypick.nvim</id>
    <link href="https://github.com/axkirillov/easypick.nvim" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A neovim plugin that lets you easily create Telescope pickers from arbitraty console commands&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;easypick.nvim&lt;/h1&gt; &#xA;&lt;p&gt;Easypick is a neovim plugin that lets you easily create Telescope pickers (see &lt;a href=&#34;https://github.com/nvim-telescope/telescope.nvim&#34;&gt;telescope.nvim&lt;/a&gt;) from arbitraty console commands.&lt;/p&gt; &#xA;&lt;h1&gt;installation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;use {&#39;axkirillov/easypick.nvim&#39;, requires = &#39;nvim-telescope/telescope.nvim&#39;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;configuration&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;local easypick = require(&#34;easypick&#34;)&#xA;&#xA;-- only required for the example to work&#xA;local base_branch = &#34;develop&#34;&#xA;&#xA;easypick.setup({&#xA;&#x9;pickers = {&#xA;&#x9;&#x9;-- add your custom pickers here&#xA;&#x9;&#x9;-- below you can find some examples of what those can look like&#xA;&#xA;&#x9;&#x9;-- list files inside current folder with default previewer&#xA;&#x9;&#x9;{&#xA;&#x9;&#x9;&#x9;-- name for your custom picker, that can be invoked using :Easypick &amp;lt;name&amp;gt; (supports tab completion)&#xA;&#x9;&#x9;&#x9;name = &#34;ls&#34;,&#xA;&#x9;&#x9;&#x9;-- the command to execute, output has to be a list of plain text entries&#xA;&#x9;&#x9;&#x9;command = &#34;ls&#34;,&#xA;&#x9;&#x9;&#x9;-- specify your custom previwer, or use one of the easypick.previewers&#xA;&#x9;&#x9;&#x9;previewer = easypick.previewers.default()&#xA;&#x9;&#x9;},&#xA;&#xA;&#x9;&#x9;-- diff current branch with base_branch and show files that changed with respective diffs in preview &#xA;&#x9;&#x9;{&#xA;&#x9;&#x9;&#x9;name = &#34;changed_files&#34;,&#xA;&#x9;&#x9;&#x9;command = &#34;git diff --name-only $(git merge-base HEAD &#34; .. base_branch .. &#34; )&#34;,&#xA;&#x9;&#x9;&#x9;previewer = easypick.previewers.branch_diff(base_branch)&#xA;&#x9;&#x9;},&#xA;&#x9;&#x9;&#xA;&#x9;&#x9;-- list files that have conflicts with diffs in preview&#xA;&#x9;&#x9;{&#xA;&#x9;&#x9;&#x9;name = &#34;conflicts&#34;,&#xA;&#x9;&#x9;&#x9;command = &#34;git diff --name-only --diff-filter=U --relative&#34;,&#xA;&#x9;&#x9;&#x9;previewer = easypick.previewers.file_diff()&#xA;&#x9;&#x9;},&#xA;&#x9;}&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;usage&lt;/h1&gt; &#xA;&lt;p&gt;After the setup is called the Easypick command becomes available with all your pickers added to tab completion.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/32141102/176906224-3b8c138e-7707-42d8-b4d1-bbe47a0afa24.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>potamides/pantran.nvim</title>
    <updated>2022-07-09T01:45:56Z</updated>
    <id>tag:github.com,2022-07-09:/potamides/pantran.nvim</id>
    <link href="https://github.com/potamides/pantran.nvim" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Use your favorite machine translation engines without having to leave your favorite editor.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Pantran.nvim&lt;/h1&gt; &#xA;&lt;!-- panvimdoc-ignore-end --&gt; &#xA;&lt;p&gt;With pantran.nvim, you can use your favorite machine translation engines without having to leave your favorite editor. It makes use of Neovim&#39;s &lt;a href=&#34;https://neovim.io/doc/user/api.html#api-floatwin&#34;&gt;api-floatwin&lt;/a&gt; to implement an asynchronous, interactive machine translation interface, similar to how most of the various machine translation web font-ends work. In addition to that, other (non-interactive) modes are also supported and, if you try hard enough, pantran.nvim can also be used as an API.&lt;/p&gt; &#xA;&lt;!-- panvimdoc-ignore-start --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/53401822/177125473-572c8ebf-fd5c-448f-827c-f6992e24bb02.mp4&#34;&gt;Pantran.nvim demonstration&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: This is beta quality software. It should run stable for the most part, but don&#39;t be too surprised if you find a few bugs here and there. Use at your own risk!&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;You need at least &lt;a href=&#34;https://neovim.io/&#34;&gt;Neovim v0.6.0&lt;/a&gt; and &lt;a href=&#34;https://curl.se/&#34;&gt;curl v7.76.0&lt;/a&gt; to be able to use this plugin. You can install it using your favorite plugin manager.&lt;/p&gt; &#xA;&lt;p&gt;With &lt;a href=&#34;https://github.com/junegunn/vim-plug&#34;&gt;vim-plug&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-viml&#34;&gt;Plug &#34;potamides/pantran.nvim&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With &lt;a href=&#34;https://github.com/Shougo/dein.vim&#34;&gt;dein&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-viml&#34;&gt;call dein#add(&#34;potamides/pantran.nvim&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With &lt;a href=&#34;https://github.com/wbthomason/packer.nvim&#34;&gt;packer.nvim&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;use {&#xA;  &#34;potamides/pantran.nvim&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- panvimdoc-ignore-end --&gt; &#xA;&lt;h1&gt;Quickstart&lt;/h1&gt; &#xA;&lt;p&gt;Run the command &lt;code&gt;:Pantran&lt;/code&gt; to open an interactive translation window and start typing to get an understanding of how things work. Type &lt;code&gt;g?&lt;/code&gt; in normal mode or &lt;code&gt;i_CTRL-/&lt;/code&gt; in insert mode to open a help buffer with available keybindings. &lt;code&gt;:Pantran&lt;/code&gt; also supports command ranges to initialize the translation winodw. Further, some optional flags for configuration of the translation process are available&#xA; &lt;!-- panvimdoc-ignore-start --&gt;, consult the &lt;a href=&#34;https://raw.githubusercontent.com/potamides/pantran.nvim/main/doc/README.md&#34;&gt;documentation&lt;/a&gt; for more details&#xA; &lt;!-- panvimdoc-ignore-end --&gt;. If you plan to translate frequently, the command can also be mapped to the following recommended keybindings:&lt;/p&gt; &#xA;&lt;!-- panvimdoc-ignore-start --&gt; &#xA;&lt;details open&gt; &#xA; &lt;!-- panvimdoc-ignore-end --&gt; &#xA; &lt;summary&gt;Neovim 0.7+&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local opts = {noremap = true, silent = true}&#xA;vim.keymap.set(&#34;n&#34;, &#34;&amp;lt;leader&amp;gt;tr&#34;, pantran.motion_translate, opts)&#xA;vim.keymap.set(&#34;n&#34;, &#34;&amp;lt;leader&amp;gt;trr&#34;, function() return pantran.motion_translate() .. &#34;_&#34; end, opts)&#xA;vim.keymap.set(&#34;x&#34;, &#34;&amp;lt;leader&amp;gt;tr&#34;, pantran.motion_translate, opts)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Neovim 0.6&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local opts = {noremap = true, silent = true}&#xA;vim.api.nvim_set_keymap(&#34;n&#34;, &#34;&amp;lt;leader&amp;gt;tr&#34;, [[luaeval(&#34;require(&#39;pantran&#39;).motion_translate()&#34;)]], opts)&#xA;vim.api.nvim_set_keymap(&#34;n&#34;, &#34;&amp;lt;leader&amp;gt;trr&#34;, [[luaeval(&#34;require(&#39;pantran&#39;).motion_translate() .. &#39;_&#39;&#34;)]], opts)&#xA;vim.api.nvim_set_keymap(&#34;x&#34;, &#34;&amp;lt;leader&amp;gt;tr&#34;, [[luaeval(&#34;require(&#39;pantran&#39;).motion_translate()&#34;)]], opts)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;The mappings work similarly to the command in that they also allow you to use ranges. E.g., you can use &lt;code&gt;3&amp;lt;leader&amp;gt;trr&lt;/code&gt; to populate the translation window with the next three lines of text. One advantage over the command, however, is the additional support for &lt;a href=&#34;https://neovim.io/doc/user/motion.html#text-objects&#34;&gt;text objects&lt;/a&gt;. You can use &lt;code&gt;&amp;lt;leader&amp;gt;tris&lt;/code&gt; or &lt;code&gt;&amp;lt;leader&amp;gt;trip&lt;/code&gt; to translate the surrounding sentence or paragraph, for example. Other translation modes like replacing or appending text immediately without opening an interactive window are also implemented.&#xA; &lt;!-- panvimdoc-ignore-start --&gt; Again, consult the &lt;a href=&#34;https://raw.githubusercontent.com/potamides/pantran.nvim/main/doc/README.md&#34;&gt;documentation&lt;/a&gt; for more details.&#xA; &lt;!-- panvimdoc-ignore-end --&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Scope&lt;/h1&gt; &#xA;&lt;p&gt;The plugin already supports a few different translation engines. If you have any further suggestions feel free to open an &lt;a href=&#34;https://github.com/potamides/pantran.nvim/issues&#34;&gt;issue&lt;/a&gt; or &lt;a href=&#34;https://github.com/potamides/pantran.nvim/pulls&#34;&gt;pull request&lt;/a&gt;! The currently supported engines are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://apertium.org&#34;&gt;Apertium&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://translate.argosopentech.com&#34;&gt;Argos Translate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepl.com/translator&#34;&gt;DeepL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://translate.google.com&#34;&gt;Google Translate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://translate.yandex.com&#34;&gt;Yandex Translate&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Some of these engines are free and open-source and can be used right off the bat. However, some are commercial and might require additional setup steps. For stability reasons, the philosophy of this plugin is to prioritize official API endpoints for which commercial engines usually require some means of authentication, e.g., through an API key. If no such key is configured but free alternative endpoints exist, these are used as fallback options. Note, however, that these are often &lt;a href=&#34;https://github.com/soimort/translate-shell/issues/370&#34;&gt;severely rate-limited&lt;/a&gt; and in some instances produce &lt;a href=&#34;https://github.com/Animenosekai/translate/issues/22&#34;&gt;inferior translations&lt;/a&gt;. So if you want to use a commercial engine, configuring authentication is usually recommended&#xA; &lt;!-- panvimdoc-ignore-start --&gt; (see the &lt;a href=&#34;https://raw.githubusercontent.com/potamides/pantran.nvim/main/doc/README.md&#34;&gt;docs&lt;/a&gt; for more information)&#xA; &lt;!-- panvimdoc-ignore-end --&gt;.&lt;/p&gt; &#xA;&lt;!-- panvimdoc-ignore-start --&gt; &#xA;&lt;h1&gt;Configuration&lt;/h1&gt; &#xA;&lt;p&gt;Pantran.nvim supports a wide range of configuration options. Some essential ones are listed below, for a full list consult the additional &lt;a href=&#34;https://raw.githubusercontent.com/potamides/pantran.nvim/main/doc/README.md&#34;&gt;documentation&lt;/a&gt;. The invocation of &lt;code&gt;require(&#34;pantran&#34;).setup()&lt;/code&gt; is optional.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;pantran&#34;).setup{&#xA;  engines = {&#xA;    -- Configuration for individual engines goes here. To list available&#xA;    -- engine names run `:lua =vim.tbl_keys(require(&#34;pantran.engines&#34;))`.&#xA;    default_engine = &#34;argos&#34;&#xA;    yandex = {&#xA;      -- Default languages can be defined on a per engine basis. In this case&#xA;      -- `:lua require(&#34;pantran.async&#34;).run(function()&#xA;      -- vim.pretty_print(require(&#34;pantran.engines&#34;).yandex:languages()) end)`&#xA;      -- can be used to list available language identifiers.&#xA;      default_source = &#34;auto&#34;,&#xA;      default_target = &#34;en&#34;&#xA;    },&#xA;  },&#xA;  controls = {&#xA;    mappings = {&#xA;      edit = {&#xA;        n = {&#xA;          -- Use this table to add additional mappings for the normal mode in&#xA;          -- the translation window. Either strings or function references are&#xA;          -- supported.&#xA;          [&#34;j&#34;] = &#34;gj&#34;,&#xA;          [&#34;k&#34;] = &#34;gk&#34;&#xA;        }&#xA;        i = {&#xA;          -- Similar table but for insert mode. Using &#39;false&#39; disables&#xA;          -- existing keybindings.&#xA;          [&#34;&amp;lt;C-y&amp;gt;&#34;] = false,&#xA;          [&#34;&amp;lt;C-a&amp;gt;&#34;] = package.loaded.pantran.ui.actions.yank_close_translation&#xA;        }&#xA;      },&#xA;      select = {&#xA;        n = {&#xA;          -- Keybindings here are used in the selection window.&#xA;        }&#xA;      }&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- panvimdoc-ignore-end --&gt; &#xA;&lt;!-- vim: set textwidth=78: --&gt;</summary>
  </entry>
</feed>