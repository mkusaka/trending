<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Lua Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-30T01:32:42Z</updated>
  <subtitle>Daily Trending of Lua in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gsuuon/llm.nvim</title>
    <updated>2023-09-30T01:32:42Z</updated>
    <id>tag:github.com,2023-09-30:/gsuuon/llm.nvim</id>
    <link href="https://github.com/gsuuon/llm.nvim" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Neovim plugin for interacting with LLM&#39;s and building editor integrated prompts.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ðŸ§  llm.nvim&lt;/h1&gt; &#xA;&lt;p&gt;Use LLM&#39;s in Neovim. Build editor integrated prompts and customize your LLM workflow. The plugin comes with some starter prompts, but you can also create your own prompt library to suit your needs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/6422188/233238173-a3dcea16-9948-4e7c-a419-eeec04cb7e99.mp4&#34;&gt;https://user-images.githubusercontent.com/6422188/233238173-a3dcea16-9948-4e7c-a419-eeec04cb7e99.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ðŸŽª OpenAI GPT (and compatible API&#39;s), Google PaLM, Huggingface, LlamaCpp, Kobold&lt;/li&gt; &#xA; &lt;li&gt;ðŸ›¸ Add LLM capabilities from other Neovim plugins&lt;/li&gt; &#xA; &lt;li&gt;ðŸŽ¨ Build your own editor integrated completions&lt;/li&gt; &#xA; &lt;li&gt;ðŸ”Ž Basic local (to your git repo) vector store&lt;/li&gt; &#xA; &lt;li&gt;ðŸŒ  Streaming responses where available&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ðŸ¦¾ Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nvim 0.8.0 or higher&lt;/li&gt; &#xA; &lt;li&gt;For the OpenAI provider (default), set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable to your &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;api key&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For the PaLM provider, set the &lt;code&gt;PALM_API_KEY&lt;/code&gt; environment variable to your &lt;a href=&#34;https://makersuite.google.com/app/apikey&#34;&gt;api key&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;curl&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Optional&lt;/h4&gt; &#xA;&lt;p&gt;For local vector store:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10+&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install numpy openai tiktoken&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;With &lt;a href=&#34;https://github.com/wbthomason/packer.nvim&#34;&gt;packer.nvim&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;packer&#39;).startup(function(use)&#xA;  use &#39;gsuuon/llm.nvim&#39;&#xA;end)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;With &lt;a href=&#34;https://github.com/folke/lazy.nvim&#34;&gt;lazy.nvim&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;lazy&#39;).setup({&#xA;  &#39;gsuuon/llm.nvim&#39;&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ðŸ’­ Usage&lt;/h2&gt; &#xA;&lt;p&gt;llm.nvim comes with some &lt;a href=&#34;https://raw.githubusercontent.com/gsuuon/llm.nvim/main/lua/llm/prompts/starters.lua&#34;&gt;starter prompts&lt;/a&gt; and makes it easy to build your own prompt library. For an example of a more complex agent-like multi-step prompt (e.g. curl, ask gpt for intermediate data, then include data in a final prompt) look at the &lt;code&gt;openapi&lt;/code&gt; starter prompt.&lt;/p&gt; &#xA;&lt;p&gt;It can also be used from another plugin to easily add LLM capabilities, for an example look at &lt;a href=&#34;https://github.com/gsuuon/note.nvim/raw/main/lua/note/llm/prompts.lua&#34;&gt;note.nvim&lt;/a&gt; which adds some &lt;a href=&#34;https://github.com/gsuuon/note.nvim/raw/main/ftplugin/note.lua&#34;&gt;buffer-local&lt;/a&gt; prompts to note files.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;:Llm [prompt-name]&lt;/code&gt; â€” Start a completion of either the visual selection or the current buffer. Uses the default prompt if no prompt name is provided.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Select response &lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/gsuuon/llm.nvim/assets/6422188/fd5aca13-979f-4bcf-8570-f935fdebbf03&#34;&gt;https://github.com/gsuuon/llm.nvim/assets/6422188/fd5aca13-979f-4bcf-8570-f935fdebbf03&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;:LlmSelect&lt;/code&gt; â€” Select the response under the cursor.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Delete response &lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/6422188/233774216-4e100122-3a93-4dfb-a7c7-df50f1221bdd.mp4&#34;&gt;https://user-images.githubusercontent.com/6422188/233774216-4e100122-3a93-4dfb-a7c7-df50f1221bdd.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;:LlmDelete&lt;/code&gt; â€” Delete the response under the cursor. If &lt;code&gt;prompt.mode == &#39;replace&#39;&lt;/code&gt; then replace with the original text.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; ðŸš§ WIP - Local vector store &lt;/summary&gt; &#xA; &lt;h3&gt;Requirements&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Python 3.10+&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;pip install numpy openai tiktoken&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Usage&lt;/h3&gt; &#xA; &lt;p&gt;Check the module functions exposed in &lt;a href=&#34;https://raw.githubusercontent.com/gsuuon/llm.nvim/main/lua/llm/store/init.lua&#34;&gt;store&lt;/a&gt;. This uses the OpenAI embeddings api to generate vectors and queries them by cosine similarity.&lt;/p&gt; &#xA; &lt;p&gt;To add items call into the &lt;code&gt;llm.store&lt;/code&gt; lua module functions, e.g.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;:lua require(&#39;llm.store&#39;).add_lua_functions()&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;:lua require(&#39;llm.store&#39;).add_files(&#39;.&#39;)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Look at &lt;code&gt;store.add_lua_functions&lt;/code&gt; for an example of how to use treesitter to parse files to nodes and add them to the local store.&lt;/p&gt; &#xA; &lt;p&gt;To get query results call &lt;code&gt;store.prompt.query_store&lt;/code&gt; with your input text, desired count and similarity cutoff threshold (0.75 seems to be decent). It returns a list of {id: string, content: string}:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;builder = function(input, context)&#xA;  ---@type {id: string, content: string}[]&#xA;  local store_results = require(&#39;llm.store&#39;).prompt.query_store(input, 2, 0.75)&#xA;&#xA;  -- add store_results to your messages&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;:LlmStore [command]&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;:LlmStore init&lt;/code&gt; â€” initialize a store.json file at the closest git root directory&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;:LlmStore query &amp;lt;query text&amp;gt;&lt;/code&gt; â€” query a store.json&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Multiple simultaneous prompts &lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/6422188/233773433-d3b38147-540c-44ba-96ac-af2af8640e7c.mp4&#34;&gt;https://user-images.githubusercontent.com/6422188/233773433-d3b38147-540c-44ba-96ac-af2af8640e7c.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;:LlmMulti&lt;/code&gt; â€” Start multiple prompt completions at the same time with the same input. Must specify prompt names. Escape spaces in names e.g. &lt;code&gt;to\ spanish&lt;/code&gt;, or use tab completion. Always completes on next line and always &lt;code&gt;mode = &#39;append&#39;&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Cancel a long-running prompt &lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/6422188/233773436-3e9d2a15-bc87-47c2-bc5b-d62d62480297.mp4&#34;&gt;https://user-images.githubusercontent.com/6422188/233773436-3e9d2a15-bc87-47c2-bc5b-d62d62480297.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;:LlmCancel&lt;/code&gt; â€” Cancel the active response under the cursor.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Show response &lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/6422188/233773449-3b85355b-bad1-4e40-a699-6a8f5cf4bcd5.mp4&#34;&gt;https://user-images.githubusercontent.com/6422188/233773449-3b85355b-bad1-4e40-a699-6a8f5cf4bcd5.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;:LlmShow&lt;/code&gt; â€” Flash the response under the cursor if there is one.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ðŸ§µConfiguration&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;llm&#39;).setup({&#xA;  default_prompt? = .. , -- Prompt â€” modify the default prompt (`:Llm` with no argument)&#xA;  hl_group? = &#39;&#39;, -- string â€” Set the default highlight group of in-progress responses&#xA;  prompts? = {} -- table&amp;lt;string, Prompt&amp;gt;` â€” add prompt alternatives&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Prompts&lt;/h3&gt; &#xA;&lt;p&gt;Prompts go in the &lt;code&gt;prompts&lt;/code&gt; field of the setup table and can be used via &lt;code&gt;:Llm [prompt name]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A prompt entry defines how to handle a completion request - it takes in the editor input (either an entire file or a visual selection) and some context, and produces the api request data merging with any defaults. It also defines how to handle the API response - for example it can replace the selection (or file) with the response or insert it at the cursor positon.&lt;/p&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://raw.githubusercontent.com/gsuuon/llm.nvim/main/lua/llm/prompts/starters.lua&#34;&gt;starter prompts&lt;/a&gt; to see how to create prompts. Type definitions are in &lt;a href=&#34;https://raw.githubusercontent.com/gsuuon/llm.nvim/main/lua/llm/provider.lua&#34;&gt;provider.lua&lt;/a&gt;. If you want to use the starter prompts alongside your own, you can use &lt;code&gt;prompts = vim.tbl_extend(&#39;force&#39;, require(&#39;llm.prompts.starters&#39;), { ... })&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Library autoload&lt;/h3&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;require(&#39;util&#39;).module.autoload&lt;/code&gt; instead of a naked &lt;code&gt;require&lt;/code&gt; to always re-require a module on use. This makes the feedback loop for developing prompts faster:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;require(&#39;llm&#39;).setup({&#xA;-  prompts = require(&#39;prompt_library&#39;)&#xA;+  prompts = require(&#39;llm.util&#39;).module.autoload(&#39;prompt_library&#39;)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;I recommend setting this only during active prompt development, and switching to a normal &lt;code&gt;require&lt;/code&gt; otherwise.&lt;/p&gt; &#xA;&lt;h3&gt;Providers&lt;/h3&gt; &#xA;&lt;h4&gt;OpenAI ChatGPT (default)&lt;/h4&gt; &#xA;&lt;p&gt;Set the environment variable &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; to your &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;api key&lt;/a&gt; before starting nvim. OpenAI prompts can take an additional option field with a table containing &lt;code&gt;{ url?, endpoint?, authorization? }&lt;/code&gt; fields to talk to compatible API&#39;s. Check the &lt;code&gt;compat&lt;/code&gt; starter prompt for an example.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Configuration &lt;/summary&gt; &#xA; &lt;p&gt;Add default request parameters for &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat/create&#34;&gt;/chat/completions&lt;/a&gt; with &lt;code&gt;initialize()&lt;/code&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;require(&#39;llm.providers.openai&#39;).initialize({&#xA;  max_tokens = 120,&#xA;  temperature = 0.7,&#xA;  model = &#39;gpt-3.5-turbo-0301&#39;&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Google PaLM&lt;/h4&gt; &#xA;&lt;p&gt;Set the &lt;code&gt;PALM_API_KEY&lt;/code&gt; environment variable to your &lt;a href=&#34;https://makersuite.google.com/app/apikey&#34;&gt;api key&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Check the palm prompt in &lt;a href=&#34;https://raw.githubusercontent.com/gsuuon/llm.nvim/main/lua/llm/prompts/starters.lua&#34;&gt;starter prompts&lt;/a&gt; for a reference. Palm provider defaults to the chat model (&lt;code&gt;chat-bison-001&lt;/code&gt;). The builder&#39;s return params can include &lt;code&gt;model = &#39;text-bison-001&#39;&lt;/code&gt; to use the text model instead.&lt;/p&gt; &#xA;&lt;p&gt;Params should be either a &lt;a href=&#34;https://developers.generativeai.google/api/rest/generativelanguage/models/generateMessage#request-body&#34;&gt;generateMessage&lt;/a&gt; body by default, or a &lt;a href=&#34;https://developers.generativeai.google/api/rest/generativelanguage/models/generateText#request-body&#34;&gt;generateText&lt;/a&gt; body if using &lt;code&gt;model = &#39;text-bison-001&#39;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;[&#39;palm text completion&#39;] = {&#xA;  provider = palm,&#xA;  builder = function(input, context)&#xA;    return {&#xA;      model = &#39;text-bison-001&#39;,&#xA;      prompt = {&#xA;        text = input&#xA;      },&#xA;      temperature = 0.2&#xA;    }&#xA;  end&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Huggingface API&lt;/h4&gt; &#xA;&lt;p&gt;Set the &lt;code&gt;HUGGINGFACE_API_KEY&lt;/code&gt; environment variable to your &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;api key&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Set the model field on the params returned by the builder (or the static params in &lt;code&gt;prompt.params&lt;/code&gt;). Set &lt;code&gt;params.stream = false&lt;/code&gt; for models which don&#39;t support it (e.g. &lt;code&gt;gpt2&lt;/code&gt;). Check &lt;a href=&#34;https://huggingface.co/docs/api-inference/detailed_parameters&#34;&gt;huggingface api docs&lt;/a&gt; for per-task request body types.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;[&#39;huggingface bigcode&#39;] = {&#xA;  provider = huggingface,&#xA;  params = {&#xA;    model = &#39;bigcode/starcoder&#39;&#xA;  },&#xA;  builder = function(input)&#xA;    return { inputs = input }&#xA;  end&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;LlamaCpp&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This provider uses the llama.cpp server example - start the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/tree/master/examples/server&#34;&gt;server&lt;/a&gt; before running prompts with this provider.&lt;/p&gt; &#xA;&lt;h4&gt;Codellama&lt;/h4&gt; &#xA;&lt;p&gt;This is a llama.cpp based provider specialized for codellama infill / Fill in the Middle. Only 7B and 13B models support FIM, and the base models (not Instruct) seem to work better. Start the llama.cpp server example with one of the two supported models before using this provider.&lt;/p&gt; &#xA;&lt;h4&gt;Kobold&lt;/h4&gt; &#xA;&lt;p&gt;For older models that don&#39;t work with llama.cpp, koboldcpp might still support them. Check their &lt;a href=&#34;https://github.com/LostRuins/koboldcpp/&#34;&gt;repo&lt;/a&gt; for setup info.&lt;/p&gt; &#xA;&lt;h4&gt;Adding your own&lt;/h4&gt; &#xA;&lt;p&gt;Providers implement a simple interface so it&#39;s easy to add your own. Just set your provider as the &lt;code&gt;provider&lt;/code&gt; field in a prompt. Your provider needs to kick off the request and call the handlers as data streams in, finishes, or errors. Check &lt;a href=&#34;https://raw.githubusercontent.com/gsuuon/llm.nvim/main/lua/llm/providers/huggingface.lua&#34;&gt;the hf provider&lt;/a&gt; for a simpler example supporting server-sent events streaming. If you don&#39;t need streaming, just make a request and call &lt;code&gt;handler.on_finish&lt;/code&gt; with the result.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;---@class Provider&#xA;---@field request_completion fun(handler: StreamHandlers, params?: table, options?: table): function Request a completion stream from provider, returning a cancel callback&#xA;&#xA;---@class StreamHandlers&#xA;---@field on_partial (fun(partial_text: string): nil) Partial response of just the diff&#xA;---@field on_finish (fun(complete_text: string, finish_reason: string): nil) Complete response with finish reason&#xA;---@field on_error (fun(data: any, label?: string): nil) Error data and optional label&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Prompts&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;llm&#39;).setup({&#xA;  prompts = {&#xA;    [&#39;prompt name&#39;] = ...&#xA;  }&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Ask for additional user instruction&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/gsuuon/llm.nvim/assets/6422188/0e4b2b68-5873-42af-905c-3bd5a0bdfe46&#34;&gt;https://github.com/gsuuon/llm.nvim/assets/6422188/0e4b2b68-5873-42af-905c-3bd5a0bdfe46&lt;/a&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;  ask = {&#xA;    provider = openai,&#xA;    params = {&#xA;      temperature = 0.3,&#xA;      max_tokens = 1500&#xA;    },&#xA;    builder = function(input)&#xA;      local messages = {&#xA;        {&#xA;          role = &#39;user&#39;,&#xA;          content = input&#xA;        }&#xA;      }&#xA;&#xA;      return util.builder.user_prompt(function(user_input)&#xA;        if #user_input &amp;gt; 0 then&#xA;          table.insert(messages, {&#xA;            role = &#39;user&#39;,&#xA;            content = user_input&#xA;          })&#xA;        end&#xA;&#xA;        return {&#xA;          messages = messages&#xA;        }&#xA;      end, input)&#xA;    end,&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Create a commit message based on `git diff --staged`&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/6422188/233807212-d1830514-fe3b-4d38-877e-f3ecbdb222aa.mp4&#34;&gt;https://user-images.githubusercontent.com/6422188/233807212-d1830514-fe3b-4d38-877e-f3ecbdb222aa.mp4&lt;/a&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;  [&#39;commit message&#39;] = {&#xA;    provider = openai,&#xA;    mode = llm.mode.INSERT,&#xA;    builder = function()&#xA;      local git_diff = vim.fn.system {&#39;git&#39;, &#39;diff&#39;, &#39;--staged&#39;}&#xA;      return {&#xA;        messages = {&#xA;          {&#xA;            role = &#39;system&#39;,&#xA;            content = &#39;Write a short commit message according to the Conventional Commits specification for the following git diff: ```\n&#39; .. git_diff .. &#39;\n```&#39;&#xA;          }&#xA;        }&#xA;      }&#xA;    end,&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Modify input to append messages&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/6422188/233748890-5dac719a-eb9a-4f76-ab9d-8eba3694a350.mp4&#34;&gt;https://user-images.githubusercontent.com/6422188/233748890-5dac719a-eb9a-4f76-ab9d-8eba3694a350.mp4&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h4&gt;&lt;code&gt;lua/prompt_library.lua&lt;/code&gt;&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;--- Looks for `&amp;lt;llm:` at the end and splits into before and after&#xA;--- returns all text if no directive&#xA;local function match_llm_directive(text)&#xA;  local before, _, after = text:match(&#34;(.-)(&amp;lt;llm:)%s?(.*)$&#34;)&#xA;  if not before and not after then&#xA;    before, after = text, &#34;&#34;&#xA;  elseif not before then&#xA;    before = &#34;&#34;&#xA;  elseif not after then&#xA;    after = &#34;&#34;&#xA;  end&#xA;  return before, after&#xA;end&#xA;&#xA;local instruct_code = &#39;You are a highly competent programmer. Include only valid code in your response.&#39;&#xA;&#xA;return {&#xA;  [&#39;to code&#39;] = {&#xA;    provider = openai,&#xA;    builder = function(input)&#xA;      local text, directive = match_llm_directive(input)&#xA;&#xA;      local msgs ={&#xA;        {&#xA;          role = &#39;system&#39;,&#xA;          content = instruct_code,&#xA;        },&#xA;        {&#xA;          role = &#39;user&#39;,&#xA;          content = text,&#xA;        }&#xA;      }&#xA;&#xA;      if directive then&#xA;        table.insert(msgs, { role = &#39;user&#39;, content = directive })&#xA;      end&#xA;&#xA;      return {&#xA;        messages = msgs&#xA;      }&#xA;    end,&#xA;    mode = segment.mode.REPLACE&#xA;  },&#xA;  code = {&#xA;    provider = openai,&#xA;    builder = function(input)&#xA;      return {&#xA;        messages = {&#xA;          {&#xA;            role = &#39;system&#39;,&#xA;            content = instruct_code,&#xA;          },&#xA;          {&#xA;            role = &#39;user&#39;,&#xA;            content = input,&#xA;          }&#xA;        }&#xA;      }&#xA;    end,&#xA;  },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Replace text with Spanish&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local openai = require(&#39;llm.providers.openai&#39;)&#xA;local segment = require(&#39;llm.segment&#39;)&#xA;&#xA;require(&#39;llm&#39;).setup({&#xA;  prompts = {&#xA;    [&#39;to spanish&#39;] =&#xA;      {&#xA;        provider = openai,&#xA;        hl_group = &#39;SpecialComment&#39;,&#xA;        builder = function(input)&#xA;          return {&#xA;            messages = {&#xA;              {&#xA;                role = &#39;system&#39;,&#xA;                content = &#39;Translate to Spanish&#39;,&#xA;              },&#xA;              {&#xA;                role = &#39;user&#39;,&#xA;                content = input,&#xA;              }&#xA;            }&#xA;          }&#xA;        end,&#xA;        mode = segment.mode.REPLACE&#xA;      }&#xA;  }&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Notifies each stream part and the complete response&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local openai = require(&#39;llm.providers.openai&#39;)&#xA;&#xA;require(&#39;llm&#39;).setup({&#xA;  prompts = {&#xA;    [&#39;show parts&#39;] = {&#xA;      provider = openai,&#xA;      builder = openai.default_builder,&#xA;      mode = {&#xA;        on_finish = function (final)&#xA;          vim.notify(&#39;final: &#39; .. final)&#xA;        end,&#xA;        on_partial = function (partial)&#xA;          vim.notify(partial)&#xA;        end,&#xA;        on_error = function (msg)&#xA;          vim.notify(&#39;error: &#39; .. msg)&#xA;        end&#xA;      }&#xA;    },&#xA;  }&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;You can move prompts into their own file and use &lt;code&gt;util.module.autoload&lt;/code&gt; to quickly iterate on prompt development.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Setup&lt;/summary&gt; &#xA; &lt;h4&gt;&lt;code&gt;config = function()&lt;/code&gt;&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local openai = require(&#39;llm.providers.openai&#39;)&#xA;&#xA;-- configure default model params here for the provider&#xA;openai.initialize({&#xA;  model = &#39;gpt-3.5-turbo-0301&#39;,&#xA;  max_tokens = 400,&#xA;  temperature = 0.2,&#xA;})&#xA;&#xA;local util = require(&#39;llm.util&#39;)&#xA;&#xA;require(&#39;llm&#39;).setup({&#xA;  hl_group = &#39;Substitute&#39;,&#xA;  prompts = util.module.autoload(&#39;prompt_library&#39;),&#xA;  default_prompt = {&#xA;    provider = openai,&#xA;    builder = function(input)&#xA;      return {&#xA;        temperature = 0.3,&#xA;        max_tokens = 120,&#xA;        messages = {&#xA;          {&#xA;            role = &#39;system&#39;,&#xA;            content = &#39;You are helpful assistant.&#39;,&#xA;          },&#xA;          {&#xA;            role = &#39;user&#39;,&#xA;            content = input,&#xA;          }&#xA;        }&#xA;      }&#xA;    end&#xA;  }&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Prompt library&lt;/summary&gt; &#xA; &lt;h4&gt;&lt;code&gt;lua/prompt_library.lua&lt;/code&gt;&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local openai = require(&#39;llm.providers.openai&#39;)&#xA;local segment = require(&#39;llm.segment&#39;)&#xA;&#xA;return {&#xA;  code = {&#xA;    provider = openai,&#xA;    builder = function(input)&#xA;      return {&#xA;        messages = {&#xA;          {&#xA;            role = &#39;system&#39;,&#xA;            content = &#39;You are a 10x super elite programmer. Continue only with code. Do not write tests, examples, or output of code unless explicitly asked for.&#39;,&#xA;          },&#xA;          {&#xA;            role = &#39;user&#39;,&#xA;            content = input,&#xA;          }&#xA;        }&#xA;      }&#xA;    end,&#xA;  },&#xA;  [&#39;to spanish&#39;] = {&#xA;    provider = openai,&#xA;    hl_group = &#39;SpecialComment&#39;,&#xA;    builder = function(input)&#xA;      return {&#xA;        messages = {&#xA;          {&#xA;            role = &#39;system&#39;,&#xA;            content = &#39;Translate to Spanish&#39;,&#xA;          },&#xA;          {&#xA;            role = &#39;user&#39;,&#xA;            content = input,&#xA;          }&#xA;        }&#xA;      }&#xA;    end,&#xA;    mode = segment.mode.REPLACE&#xA;  },&#xA;  [&#39;to javascript&#39;] = {&#xA;    provider = openai,&#xA;    builder = function(input, ctx)&#xA;      return {&#xA;        messages = {&#xA;          {&#xA;            role = &#39;system&#39;,&#xA;            content = &#39;Convert the code to javascript&#39;&#xA;          },&#xA;          {&#xA;            role = &#39;user&#39;,&#xA;            content = input&#xA;          }&#xA;        }&#xA;      }&#xA;    end,&#xA;  },&#xA;  [&#39;to rap&#39;] = {&#xA;    provider = openai,&#xA;    hl_group = &#39;Title&#39;,&#xA;    builder = function(input)&#xA;      return {&#xA;        messages = {&#xA;          {&#xA;            role = &#39;system&#39;,&#xA;            content = &#34;Explain the code in 90&#39;s era rap lyrics&#34;&#xA;          },&#xA;          {&#xA;            role = &#39;user&#39;,&#xA;            content = input&#xA;          }&#xA;        }&#xA;      }&#xA;    end,&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
  <entry>
    <title>topepo/shinylive-in-book-test</title>
    <updated>2023-09-30T01:32:42Z</updated>
    <id>tag:github.com,2023-09-30:/topepo/shinylive-in-book-test</id>
    <link href="https://github.com/topepo/shinylive-in-book-test" rel="alternate"></link>
    <summary type="html">&lt;p&gt;testing shiny in quarto docs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Using shinylive in Quarto books&lt;/h1&gt; &#xA;&lt;p&gt;This repo is a test case for using shinylive (with R) to embed shiny in a &lt;a href=&#34;https://quarto.org/docs/books/&#34;&gt;Quarto book&lt;/a&gt; to provide simple visualizations.&lt;/p&gt; &#xA;&lt;p&gt;This is a work in progress and the APIs and methods will probably change (by becoming more simple and user-friendly) as time goes on.&lt;/p&gt; &#xA;&lt;p&gt;The source files are here. The rendered version is at the GitHub Pages site &lt;a href=&#34;https://topepo.github.io/shinylive-in-book-test/&#34;&gt;&lt;code&gt;https://topepo.github.io/shinylive-in-book-test/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>