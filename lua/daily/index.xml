<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Lua Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-26T01:32:51Z</updated>
  <subtitle>Daily Trending of Lua in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>milanglacier/minuet-ai.nvim</title>
    <updated>2025-03-26T01:32:51Z</updated>
    <id>tag:github.com,2025-03-26:/milanglacier/minuet-ai.nvim</id>
    <link href="https://github.com/milanglacier/minuet-ai.nvim" rel="alternate"></link>
    <summary type="html">&lt;p&gt;💃 Dance with Intelligence in Your Code. Minuet offers code completion as-you-type from popular LLMs including OpenAI, Gemini, Claude, Ollama, Llama.cpp, Codestral, and more.&lt;/p&gt;&lt;hr&gt;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#minuet-ai&#34;&gt;Minuet AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#virtual-text-setup&#34;&gt;Virtual Text Setup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#nvim-cmp-setup&#34;&gt;Nvim-cmp setup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#blink-cmp-setup&#34;&gt;Blink-cmp Setup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#built-in-completion-minicompletion-and-lsp-setup&#34;&gt;Built-in Completion, Mini.Completion, and LSP Setup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#llm-provider-examples&#34;&gt;LLM Provider Examples&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#openrouter-llama-33-70b&#34;&gt;Openrouter llama-3.3-70b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#deepseek&#34;&gt;Deepseek&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#ollama-qwen-25-coder7b&#34;&gt;Ollama Qwen-2.5-coder:7b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#llamacpp-qwen-25-coder15b&#34;&gt;Llama.cpp Qwen-2.5-coder:1.5b&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#selecting-a-provider-or-model&#34;&gt;Selecting a Provider or Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#configuration&#34;&gt;Configuration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#api-keys&#34;&gt;API Keys&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#prompt&#34;&gt;Prompt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#providers&#34;&gt;Providers&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#openai&#34;&gt;OpenAI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#claude&#34;&gt;Claude&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#codestral&#34;&gt;Codestral&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#gemini&#34;&gt;Gemini&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#experimental-configuration&#34;&gt;Experimental Configuration&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#openai-compatible&#34;&gt;OpenAI-compatible&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#openai-fim-compatible&#34;&gt;OpenAI-FIM-compatible&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#commands&#34;&gt;Commands&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#minuet-change_provider-minuet-change_model&#34;&gt;&lt;code&gt;Minuet change_provider&lt;/code&gt;, &lt;code&gt;Minuet change_model&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#minuet-change_preset&#34;&gt;&lt;code&gt;Minuet change_preset&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#minuet-blink-minuet-cmp&#34;&gt;&lt;code&gt;Minuet blink&lt;/code&gt;, &lt;code&gt;Minuet cmp&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#minuet-virtualtext&#34;&gt;&lt;code&gt;Minuet virtualtext&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#minuet-lsp&#34;&gt;&lt;code&gt;Minuet lsp&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#api&#34;&gt;API&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#virtual-text&#34;&gt;Virtual Text&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#lualine&#34;&gt;Lualine&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#minuet-event&#34;&gt;Minuet Event&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#faq&#34;&gt;FAQ&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#customize-cmp-ui-for-source-icon-and-kind-icon&#34;&gt;Customize &lt;code&gt;cmp&lt;/code&gt; ui for source icon and kind icon&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#customize-blink-ui-for-source-icon-and-kind-icon&#34;&gt;Customize &lt;code&gt;blink&lt;/code&gt; ui for source icon and kind icon&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#significant-input-delay-when-moving-to-a-new-line-with-nvim-cmp&#34;&gt;Significant Input Delay When Moving to a New Line with &lt;code&gt;nvim-cmp&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#integration-with-lazyvim&#34;&gt;Integration with &lt;code&gt;lazyvim&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#enhancement&#34;&gt;Enhancement&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#rag-experimental&#34;&gt;RAG (Experimental)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#troubleshooting&#34;&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Minuet AI&lt;/h1&gt; &#xA;&lt;p&gt;Minuet AI: Dance with Intelligence in Your Code 💃.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Minuet-ai&lt;/code&gt; brings the grace and harmony of a minuet to your coding process. Just as dancers move during a minuet.&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AI-powered code completion with dual modes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Specialized prompts and various enhancements for chat-based LLMs on code completion tasks.&lt;/li&gt; &#xA;   &lt;li&gt;Fill-in-the-middle (FIM) completion for compatible models (DeepSeek, Codestral, Qwen, and others).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support for multiple AI providers (OpenAI, Claude, Gemini, Codestral, Ollama, Llama-cpp, and OpenAI-compatible services).&lt;/li&gt; &#xA; &lt;li&gt;Customizable configuration options.&lt;/li&gt; &#xA; &lt;li&gt;Streaming support to enable completion delivery even with slower LLMs.&lt;/li&gt; &#xA; &lt;li&gt;No proprietary binary running in the background. Just curl and your preferred LLM provider.&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;code&gt;virtual-text&lt;/code&gt;, &lt;code&gt;nvim-cmp&lt;/code&gt;, &lt;code&gt;blink-cmp&lt;/code&gt;, &lt;code&gt;built-in&lt;/code&gt;, &lt;code&gt;mini.completion&lt;/code&gt; frontend.&lt;/li&gt; &#xA; &lt;li&gt;Act as an &lt;strong&gt;in-process LSP&lt;/strong&gt; server to provide completions (opt-in feature).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;With nvim-cmp / blink-cmp frontend&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/assets/example-cmp.png&#34; alt=&#34;example-cmp&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;With builtin completion frontend&lt;/strong&gt; (requires nvim 0.11+):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/assets/example-builtin-completion.jpg&#34; alt=&#34;example-builtin-completion&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;With virtual text frontend&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/assets/example-virtual-text.png&#34; alt=&#34;example-virtual-text&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Requirements&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Neovim 0.10+.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nvim-lua/plenary.nvim&#34;&gt;plenary.nvim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;optional: &lt;a href=&#34;https://github.com/hrsh7th/nvim-cmp&#34;&gt;nvim-cmp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;optional: &lt;a href=&#34;https://github.com/Saghen/blink.cmp&#34;&gt;blink.cmp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;An API key for at least one of the supported AI providers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lazy.nvim&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;specs = {&#xA;    {&#xA;        &#39;milanglacier/minuet-ai.nvim&#39;,&#xA;        config = function()&#xA;            require(&#39;minuet&#39;).setup {&#xA;                -- Your configuration options here&#xA;            }&#xA;        end,&#xA;    },&#xA;    { &#39;nvim-lua/plenary.nvim&#39; },&#xA;    -- optional, if you are using virtual-text frontend, nvim-cmp is not&#xA;    -- required.&#xA;    { &#39;hrsh7th/nvim-cmp&#39; },&#xA;    -- optional, if you are using virtual-text frontend, blink is not required.&#xA;    { &#39;Saghen/blink.cmp&#39; },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rocks.nvim&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Minuet&lt;/code&gt; is available on luarocks.org. Simply run &lt;code&gt;Rocks install minuet-ai.nvim&lt;/code&gt; to install it like any other luarocks package.&lt;/p&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;h2&gt;Virtual Text Setup&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;minuet&#39;).setup {&#xA;    virtualtext = {&#xA;        auto_trigger_ft = {},&#xA;        keymap = {&#xA;            -- accept whole completion&#xA;            accept = &#39;&amp;lt;A-A&amp;gt;&#39;,&#xA;            -- accept one line&#xA;            accept_line = &#39;&amp;lt;A-a&amp;gt;&#39;,&#xA;            -- accept n lines (prompts for number)&#xA;            -- e.g. &#34;A-z 2 CR&#34; will accept 2 lines&#xA;            accept_n_lines = &#39;&amp;lt;A-z&amp;gt;&#39;,&#xA;            -- Cycle to prev completion item, or manually invoke completion&#xA;            prev = &#39;&amp;lt;A-[&amp;gt;&#39;,&#xA;            -- Cycle to next completion item, or manually invoke completion&#xA;            next = &#39;&amp;lt;A-]&amp;gt;&#39;,&#xA;            dismiss = &#39;&amp;lt;A-e&amp;gt;&#39;,&#xA;        },&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Nvim-cmp setup&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;cmp&#39;).setup {&#xA;    sources = {&#xA;        {&#xA;             -- Include minuet as a source to enable autocompletion&#xA;            { name = &#39;minuet&#39; },&#xA;            -- and your other sources&#xA;        }&#xA;    },&#xA;    performance = {&#xA;        -- It is recommended to increase the timeout duration due to&#xA;        -- the typically slower response speed of LLMs compared to&#xA;        -- other completion sources. This is not needed when you only&#xA;        -- need manual completion.&#xA;        fetching_timeout = 2000,&#xA;    },&#xA;}&#xA;&#xA;&#xA;-- If you wish to invoke completion manually,&#xA;-- The following configuration binds `A-y` key&#xA;-- to invoke the configuration manually.&#xA;require(&#39;cmp&#39;).setup {&#xA;    mapping = {&#xA;        [&#34;&amp;lt;A-y&amp;gt;&#34;] = require(&#39;minuet&#39;).make_cmp_map()&#xA;        -- and your other keymappings&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Blink-cmp Setup&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;blink-cmp&#39;).setup {&#xA;    keymap = {&#xA;        -- Manually invoke minuet completion.&#xA;        [&#39;&amp;lt;A-y&amp;gt;&#39;] = require(&#39;minuet&#39;).make_blink_map(),&#xA;    },&#xA;    sources = {&#xA;         -- Enable minuet for autocomplete&#xA;        default = { &#39;lsp&#39;, &#39;path&#39;, &#39;buffer&#39;, &#39;snippets&#39;, &#39;minuet&#39; },&#xA;        -- For manual completion only, remove &#39;minuet&#39; from default&#xA;        providers = {&#xA;            minuet = {&#xA;                name = &#39;minuet&#39;,&#xA;                module = &#39;minuet.blink&#39;,&#xA;                score_offset = 8, -- Gives minuet higher priority among suggestions&#xA;            },&#xA;        },&#xA;    },&#xA;    -- Recommended to avoid unnecessary request&#xA;    completion = { trigger = { prefetch_on_insert = false } },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Built-in Completion, Mini.Completion, and LSP Setup&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Neovim version 0.11 or higher is necessary for built-in completion.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;minuet&#39;).setup {&#xA;    lsp = {&#xA;        enabled_ft = { &#39;toml&#39;, &#39;lua&#39;, &#39;cpp&#39; },&#xA;        -- Enables automatic completion triggering using `vim.lsp.completion.enable`&#xA;        enabled_auto_trigger_ft = { &#39;cpp&#39;, &#39;lua&#39; },&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The &lt;code&gt;enabled_auto_trigger_ft&lt;/code&gt; setting is relevant only for built-in completion. &lt;code&gt;Mini.Completion&lt;/code&gt; users can ignore this option, as Mini.Completion uses &lt;strong&gt;all&lt;/strong&gt; available LSPs for &lt;strong&gt;auto-triggered&lt;/strong&gt; completion.&lt;/p&gt; &#xA; &lt;p&gt;For manually triggered completion, ensure &lt;code&gt;vim.bo.omnifunc&lt;/code&gt; is set to &lt;code&gt;v:lua.vim.lsp.omnifunc&lt;/code&gt; and use &lt;code&gt;&amp;lt;C-x&amp;gt;&amp;lt;C-o&amp;gt;&lt;/code&gt; in Insert mode.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Recommendation:&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;For users of &lt;code&gt;blink-cmp&lt;/code&gt; and &lt;code&gt;nvim-cmp&lt;/code&gt;, it is recommended to use the native source rather than through LSP for two main reasons:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;code&gt;blink-cmp&lt;/code&gt; and &lt;code&gt;nvim-cmp&lt;/code&gt; offer better sorting and async management when Minuet is utilized as a separate source rather than alongside a regular LSP such as &lt;code&gt;clangd&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;With &lt;code&gt;blink-cmp&lt;/code&gt; and &lt;code&gt;nvim-cmp&lt;/code&gt; native sources, it&#39;s possible to configure Minuet for manual completion only, disabling automatic completion. However, when Minuet operates as an LSP server, it is impossible to determine whether completion is triggered automatically or manually.&lt;/p&gt; &lt;p&gt;The LSP protocol specification defines three &lt;code&gt;triggerKind&lt;/code&gt; values: &lt;code&gt;Invoked&lt;/code&gt;, &lt;code&gt;TriggerCharacter&lt;/code&gt;, and &lt;code&gt;TriggerForIncompleteCompletions&lt;/code&gt;. However, none of these specifically differentiates between manual and automatic completion requests.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: An upstream issue (&lt;a href=&#34;https://github.com/neovim/neovim/issues/32972&#34;&gt;tracked here&lt;/a&gt;) may cause unexpected indentation behavior when accepting multi-line completions.&lt;/p&gt; &#xA; &lt;p&gt;Therefore, consider the following options:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Ensure &lt;code&gt;config.add_single_line_entry = true&lt;/code&gt; and only accept single-line completions.&lt;/li&gt; &#xA;  &lt;li&gt;Avoid using Minuet and built-in completion with languages where indentation affects semantics, such as Python.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;&lt;strong&gt;Additional Note:&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;Users might call &lt;code&gt;vim.lsp.completion.enable {autotrigger = true}&lt;/code&gt; during the &lt;code&gt;LspAttach&lt;/code&gt; event when the client supports completion. However, this is not the desired behavior for Minuet. As an LLM completion source, Minuet can face significant rate limits during automatic triggering.&lt;/p&gt; &#xA; &lt;p&gt;Therefore, it&#39;s recommended to enable Minuet for automatic triggering using the &lt;code&gt;config.lsp.enabled_auto_trigger_ft&lt;/code&gt; setting.&lt;/p&gt; &#xA; &lt;p&gt;For users who uses &lt;code&gt;LspAttach&lt;/code&gt; event, it is recommeded to verify that the server is not the Minuet server before enabling autotrigger. An example configuration is shown below:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;vim.api.nvim_create_autocmd(&#39;LspAttach&#39;, {&#xA;    callback = function(args)&#xA;        local client_id = args.data.client_id&#xA;        local bufnr = args.buf&#xA;        local client = vim.lsp.get_client_by_id(client_id)&#xA;        if not client then&#xA;            return&#xA;        end&#xA;&#xA;        if client.server_capabilities.completionProvider and client.name ~= &#39;minuet&#39; then&#xA;            vim.lsp.completion.enable(true, client_id, bufnr, { autotrigger = true })&#xA;        end&#xA;    end,&#xA;    desc = &#39;Enable built-in auto completion&#39;,&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;LLM Provider Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Openrouter llama-3.3-70b&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;minuet&#39;).setup {&#xA;    provider = &#39;openai_compatible&#39;,&#xA;    request_timeout = 2.5,&#xA;    throttle = 1500, -- Increase to reduce costs and avoid rate limits&#xA;    debounce = 600, -- Increase to reduce costs and avoid rate limits&#xA;    provider_options = {&#xA;        openai_compatible = {&#xA;            api_key = &#39;OPENROUTER_API_KEY&#39;,&#xA;            end_point = &#39;https://openrouter.ai/api/v1/chat/completions&#39;,&#xA;            model = &#39;meta-llama/llama-3.3-70b-instruct&#39;,&#xA;            name = &#39;Openrouter&#39;,&#xA;            optional = {&#xA;                max_tokens = 128,&#xA;                top_p = 0.9,&#xA;                provider = {&#xA;                     -- Prioritize throughput for faster completion&#xA;                    sort = &#39;throughput&#39;,&#xA;                },&#xA;            },&#xA;        },&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Deepseek&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;-- you can use deepseek with both openai_fim_compatible or openai_compatible provider&#xA;require(&#39;minuet&#39;).setup {&#xA;    provider = &#39;openai_fim_compatible&#39;,&#xA;    provider_options = {&#xA;        openai_fim_compatible = {&#xA;            api_key = &#39;DEEPSEEK_API_KEY&#39;,&#xA;            name = &#39;deepseek&#39;,&#xA;            optional = {&#xA;                max_tokens = 256,&#xA;                top_p = 0.9,&#xA;            },&#xA;        },&#xA;    },&#xA;}&#xA;&#xA;&#xA;-- or&#xA;require(&#39;minuet&#39;).setup {&#xA;    provider = &#39;openai_compatible&#39;,&#xA;    provider_options = {&#xA;        openai_compatible = {&#xA;            end_point = &#39;https://api.deepseek.com/v1/chat/completions&#39;,&#xA;            api_key = &#39;DEEPSEEK_API_KEY&#39;,&#xA;            name = &#39;deepseek&#39;,&#xA;            optional = {&#xA;                max_tokens = 256,&#xA;                top_p = 0.9,&#xA;            },&#xA;        },&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Ollama Qwen-2.5-coder:7b&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;minuet&#39;).setup {&#xA;    provider = &#39;openai_fim_compatible&#39;,&#xA;    n_completions = 1, -- recommend for local model for resource saving&#xA;    -- I recommend beginning with a small context window size and incrementally&#xA;    -- expanding it, depending on your local computing power. A context window&#xA;    -- of 512, serves as an good starting point to estimate your computing&#xA;    -- power. Once you have a reliable estimate of your local computing power,&#xA;    -- you should adjust the context window to a larger value.&#xA;    context_window = 512,&#xA;    provider_options = {&#xA;        openai_fim_compatible = {&#xA;            api_key = &#39;TERM&#39;,&#xA;            name = &#39;Ollama&#39;,&#xA;            end_point = &#39;http://localhost:11434/v1/completions&#39;,&#xA;            model = &#39;qwen2.5-coder:7b&#39;,&#xA;            optional = {&#xA;                max_tokens = 56,&#xA;                top_p = 0.9,&#xA;            },&#xA;        },&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Llama.cpp Qwen-2.5-coder:1.5b&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;First, launch the &lt;code&gt;llama-server&lt;/code&gt; with your chosen model.&lt;/p&gt; &#xA; &lt;p&gt;Here&#39;s an example of a bash script to start the server if your system has less than 8GB of VRAM:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-server \&#xA;    -hf ggml-org/Qwen2.5-Coder-1.5B-Q8_0-GGUF \&#xA;    --port 8012 -ngl 99 -fa -ub 1024 -b 1024 \&#xA;    --ctx-size 0 --cache-reuse 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;minuet&#39;).setup {&#xA;    provider = &#39;openai_fim_compatible&#39;,&#xA;    n_completions = 1, -- recommend for local model for resource saving&#xA;    -- I recommend beginning with a small context window size and incrementally&#xA;    -- expanding it, depending on your local computing power. A context window&#xA;    -- of 512, serves as an good starting point to estimate your computing&#xA;    -- power. Once you have a reliable estimate of your local computing power,&#xA;    -- you should adjust the context window to a larger value.&#xA;    context_window = 512,&#xA;    provider_options = {&#xA;        openai_fim_compatible = {&#xA;            api_key = &#39;TERM&#39;,&#xA;            name = &#39;Llama.cpp&#39;,&#xA;            end_point = &#39;http://localhost:8012/v1/completions&#39;,&#xA;            -- The model is set by the llama-cpp server and cannot be altered&#xA;            -- post-launch.&#xA;            model = &#39;PLACEHOLDER&#39;,&#xA;            optional = {&#xA;                max_tokens = 56,&#xA;                top_p = 0.9,&#xA;            },&#xA;            -- Llama.cpp does not support the `suffix` option in FIM completion.&#xA;            -- Therefore, we must disable it and manually populate the special&#xA;            -- tokens required for FIM completion.&#xA;            template = {&#xA;                prompt = function(context_before_cursor, context_after_cursor)&#xA;                    return &#39;&amp;lt;|fim_prefix|&amp;gt;&#39;&#xA;                        .. context_before_cursor&#xA;                        .. &#39;&amp;lt;|fim_suffix|&amp;gt;&#39;&#xA;                        .. context_after_cursor&#xA;                        .. &#39;&amp;lt;|fim_middle|&amp;gt;&#39;&#xA;                end,&#xA;                suffix = false,&#xA;            },&#xA;        },&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For additional example bash scripts to run llama.cpp based on your local computing power, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/recipes.md&#34;&gt;recipes.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Selecting a Provider or Model&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;code&gt;gemini-flash&lt;/code&gt; and &lt;code&gt;codestral&lt;/code&gt; models offer high-quality output with free and fast processing. For optimal quality (albeit slower generation speed), consider using the &lt;code&gt;deepseek-chat&lt;/code&gt; model, which is compatible with both &lt;code&gt;openai-fim-compatible&lt;/code&gt; and &lt;code&gt;openai-compatible&lt;/code&gt; providers. For local LLM inference, you can deploy either &lt;code&gt;qwen-2.5-coder&lt;/code&gt; or &lt;code&gt;deepseek-coder-v2&lt;/code&gt; through Ollama using the &lt;code&gt;openai-fim-compatible&lt;/code&gt; provider.&lt;/p&gt; &#xA;&lt;p&gt;As of January 28, 2025: Due to high server demand, Deepseek users may experience significant response delays or timeout. We recommend trying alternative providers instead.&lt;/p&gt; &#xA;&lt;h1&gt;Configuration&lt;/h1&gt; &#xA;&lt;p&gt;Minuet AI comes with the following defaults:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;default_config = {&#xA;    -- Enable or disable auto-completion. Note that you still need to add&#xA;    -- Minuet to your cmp/blink sources. This option controls whether cmp/blink&#xA;    -- will attempt to invoke minuet when minuet is included in cmp/blink&#xA;    -- sources. This setting has no effect on manual completion; Minuet will&#xA;    -- always be enabled when invoked manually. You can use the command&#xA;    -- `Minuet cmp/blink toggle` to toggle this option.&#xA;    cmp = {&#xA;        enable_auto_complete = true,&#xA;    },&#xA;    blink = {&#xA;        enable_auto_complete = true,&#xA;    },&#xA;    -- LSP is recommended only for built-in completion. If you are using&#xA;    -- `cmp` or `blink`, utilizing LSP for code completion from Minuet is *not*&#xA;    -- recommended.&#xA;    lsp = {&#xA;        enabled_ft = {},&#xA;        -- Filetypes excluded from LSP activation. Useful when `enabled_ft` = { &#39;*&#39; }&#xA;        disabled_ft = {},&#xA;        -- Enables automatic completion triggering using `vim.lsp.completion.enable`&#xA;        enabled_auto_trigger_ft = {},&#xA;        -- Filetypes excluded from autotriggering. Useful when `enabled_auto_trigger_ft` = { &#39;*&#39; }&#xA;        disabled_auto_trigger_ft = {},&#xA;        -- if true, when the user is using blink or nvim-cmp, warn the user&#xA;        -- that they should use the native source instead.&#xA;        warn_on_blink_or_cmp = true,&#xA;    },&#xA;    virtualtext = {&#xA;        -- Specify the filetypes to enable automatic virtual text completion,&#xA;        -- e.g., { &#39;python&#39;, &#39;lua&#39; }. Note that you can still invoke manual&#xA;        -- completion even if the filetype is not on your auto_trigger_ft list.&#xA;        auto_trigger_ft = {},&#xA;        -- specify file types where automatic virtual text completion should be&#xA;        -- disabled. This option is useful when auto-completion is enabled for&#xA;        -- all file types i.e., when auto_trigger_ft = { &#39;*&#39; }&#xA;        auto_trigger_ignore_ft = {},&#xA;        keymap = {&#xA;            accept = nil,&#xA;            accept_line = nil,&#xA;            accept_n_lines = nil,&#xA;            -- Cycle to next completion item, or manually invoke completion&#xA;            next = nil,&#xA;            -- Cycle to prev completion item, or manually invoke completion&#xA;            prev = nil,&#xA;            dismiss = nil,&#xA;        },&#xA;        -- Whether show virtual text suggestion when the completion menu&#xA;        -- (nvim-cmp or blink-cmp) is visible.&#xA;        show_on_completion_menu = false,&#xA;    },&#xA;    provider = &#39;codestral&#39;,&#xA;    -- the maximum total characters of the context before and after the cursor&#xA;    -- 16000 characters typically equate to approximately 4,000 tokens for&#xA;    -- LLMs.&#xA;    context_window = 16000,&#xA;    -- when the total characters exceed the context window, the ratio of&#xA;    -- context before cursor and after cursor, the larger the ratio the more&#xA;    -- context before cursor will be used. This option should be between 0 and&#xA;    -- 1, context_ratio = 0.75 means the ratio will be 3:1.&#xA;    context_ratio = 0.75,&#xA;    throttle = 1000, -- only send the request every x milliseconds, use 0 to disable throttle.&#xA;    -- debounce the request in x milliseconds, set to 0 to disable debounce&#xA;    debounce = 400,&#xA;    -- Control notification display for request status&#xA;    -- Notification options:&#xA;    -- false: Disable all notifications (use boolean false, not string &#34;false&#34;)&#xA;    -- &#34;debug&#34;: Display all notifications (comprehensive debugging)&#xA;    -- &#34;verbose&#34;: Display most notifications&#xA;    -- &#34;warn&#34;: Display warnings and errors only&#xA;    -- &#34;error&#34;: Display errors only&#xA;    notify = &#39;warn&#39;,&#xA;    -- The request timeout, measured in seconds. When streaming is enabled&#xA;    -- (stream = true), setting a shorter request_timeout allows for faster&#xA;    -- retrieval of completion items, albeit potentially incomplete.&#xA;    -- Conversely, with streaming disabled (stream = false), a timeout&#xA;    -- occurring before the LLM returns results will yield no completion items.&#xA;    request_timeout = 3,&#xA;    -- If completion item has multiple lines, create another completion item&#xA;    -- only containing its first line. This option only has impact for cmp and&#xA;    -- blink. For virtualtext, no single line entry will be added.&#xA;    add_single_line_entry = true,&#xA;    -- The number of completion items encoded as part of the prompt for the&#xA;    -- chat LLM. For FIM model, this is the number of requests to send. It&#39;s&#xA;    -- important to note that when &#39;add_single_line_entry&#39; is set to true, the&#xA;    -- actual number of returned items may exceed this value. Additionally, the&#xA;    -- LLM cannot guarantee the exact number of completion items specified, as&#xA;    -- this parameter serves only as a prompt guideline.&#xA;    n_completions = 3,&#xA;    -- Defines the length of non-whitespace context after the cursor used to&#xA;    -- filter completion text. Set to 0 to disable filtering.&#xA;    --&#xA;    -- Example: With after_cursor_filter_length = 3 and context:&#xA;    --&#xA;    -- &#34;def fib(n):\n|\n\nfib(5)&#34; (where | represents cursor position),&#xA;    --&#xA;    -- if the completion text contains &#34;fib&#34;, then &#34;fib&#34; and subsequent text&#xA;    -- will be removed. This setting filters repeated text generated by the&#xA;    -- LLM. A large value (e.g., 15) is recommended to avoid false positives.&#xA;    after_cursor_filter_length = 15,&#xA;    -- proxy port to use&#xA;    proxy = nil,&#xA;    provider_options = {&#xA;        -- see the documentation in each provider in the following part.&#xA;    },&#xA;    -- see the documentation in the `Prompt` section&#xA;    default_template = {&#xA;        template = &#39;...&#39;,&#xA;        prompt = &#39;...&#39;,&#xA;        guidelines = &#39;...&#39;,&#xA;        n_completion_template = &#39;...&#39;,&#xA;    },&#xA;    default_fim_template = {&#xA;        prompt = &#39;...&#39;,&#xA;        suffix = &#39;...&#39;,&#xA;    },&#xA;    default_few_shots = { &#39;...&#39; },&#xA;    default_chat_input = { &#39;...&#39; },&#xA;    -- Config options for `Minuet change_preset` command&#xA;    presets = {}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;API Keys&lt;/h1&gt; &#xA;&lt;p&gt;Minuet AI requires API keys to function. Set the following environment variables:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt; for OpenAI&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;GEMINI_API_KEY&lt;/code&gt; for Gemini&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; for Claude&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CODESTRAL_API_KEY&lt;/code&gt; for Codestral&lt;/li&gt; &#xA; &lt;li&gt;Custom environment variable for OpenAI-compatible services (as specified in your configuration)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Provide the name of the environment variable to Minuet, not the actual value. For instance, pass &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; to Minuet, not the value itself (e.g., &lt;code&gt;sk-xxxx&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;If using Ollama, you need to assign an arbitrary, non-null environment variable as a placeholder for it to function.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, you can provide a function that returns the API key. This function should return the result instantly as it will be called for each completion request.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;mineut&#39;).setup {&#xA;    provider_options = {&#xA;        openai_compatible = {&#xA;            -- good&#xA;            api_key = &#39;FIREWORKS_API_KEY&#39;, -- will read the environment variable FIREWORKS_API_KEY&#xA;            -- good&#xA;            api_key = function() return &#39;sk-xxxx&#39; end,&#xA;            -- bad&#xA;            api_key = &#39;sk-xxxx&#39;,&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Prompt&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/prompt.md&#34;&gt;prompt&lt;/a&gt; for the default prompt used by &lt;code&gt;minuet&lt;/code&gt; and instructions on customization.&lt;/p&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;minuet&lt;/code&gt; employs two distinct prompt systems:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A system designed for chat-based LLMs (OpenAI, OpenAI-Compatible, Claude, and Gemini)&lt;/li&gt; &#xA; &lt;li&gt;A separate system designed for Codestral and OpenAI-FIM-compatible models&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Providers&lt;/h1&gt; &#xA;&lt;h2&gt;OpenAI&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;the following is the default configuration for OpenAI:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;provider_options = {&#xA;    openai = {&#xA;        model = &#39;gpt-4o-mini&#39;,&#xA;        system = &#34;see [Prompt] section for the default value&#34;,&#xA;        few_shots = &#34;see [Prompt] section for the default value&#34;,&#xA;        chat_input = &#34;See [Prompt Section for default value]&#34;,&#xA;        stream = true,&#xA;        api_key = &#39;OPENAI_API_KEY&#39;,&#xA;        optional = {&#xA;            -- pass any additional parameters you want to send to OpenAI request,&#xA;            -- e.g.&#xA;            -- stop = { &#39;end&#39; },&#xA;            -- max_tokens = 256,&#xA;            -- top_p = 0.9,&#xA;        },&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The following configuration is not the default, but recommended to prevent request timeout from outputing too many tokens.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;provider_options = {&#xA;    openai = {&#xA;        optional = {&#xA;            max_tokens = 256,&#xA;        },&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Claude&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;the following is the default configuration for Claude:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;provider_options = {&#xA;    claude = {&#xA;        max_tokens = 512,&#xA;        model = &#39;claude-3-5-haiku-20241022&#39;,&#xA;        system = &#34;see [Prompt] section for the default value&#34;,&#xA;        few_shots = &#34;see [Prompt] section for the default value&#34;,&#xA;        chat_input = &#34;See [Prompt Section for default value]&#34;,&#xA;        stream = true,&#xA;        api_key = &#39;ANTHROPIC_API_KEY&#39;,&#xA;        optional = {&#xA;            -- pass any additional parameters you want to send to claude request,&#xA;            -- e.g.&#xA;            -- stop_sequences = nil,&#xA;        },&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Codestral&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;Codestral is a text completion model, not a chat model, so the system prompt and few shot examples does not apply. Note that you should use the &lt;code&gt;CODESTRAL_API_KEY&lt;/code&gt;, not the &lt;code&gt;MISTRAL_API_KEY&lt;/code&gt;, as they are using different endpoint. To use the Mistral endpoint, simply modify the &lt;code&gt;end_point&lt;/code&gt; and &lt;code&gt;api_key&lt;/code&gt; parameters in the configuration.&lt;/p&gt; &#xA; &lt;p&gt;the following is the default configuration for Codestral:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;provider_options = {&#xA;    codestral = {&#xA;        model = &#39;codestral-latest&#39;,&#xA;        end_point = &#39;https://codestral.mistral.ai/v1/fim/completions&#39;,&#xA;        api_key = &#39;CODESTRAL_API_KEY&#39;,&#xA;        stream = true,&#xA;        template = {&#xA;            prompt = &#34;See [Prompt Section for default value]&#34;,&#xA;            suffix = &#34;See [Prompt Section for default value]&#34;,&#xA;        },&#xA;        optional = {&#xA;            stop = nil, -- the identifier to stop the completion generation&#xA;            max_tokens = nil,&#xA;        },&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The following configuration is not the default, but recommended to prevent request timeout from outputing too many tokens.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;provider_options = {&#xA;    codestral = {&#xA;        optional = {&#xA;            max_tokens = 256,&#xA;            stop = { &#39;\n\n&#39; },&#xA;        },&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Gemini&lt;/h2&gt; &#xA;&lt;p&gt;You should register the account and use the service from Google AI Studio instead of Google Cloud. You can get an API key via their &lt;a href=&#34;https://makersuite.google.com/app/apikey&#34;&gt;Google API page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;The following config is the default.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;provider_options = {&#xA;    gemini = {&#xA;        model = &#39;gemini-2.0-flash&#39;,&#xA;        system = &#34;see [Prompt] section for the default value&#34;,&#xA;        few_shots = &#34;see [Prompt] section for the default value&#34;,&#xA;        chat_input = &#34;See [Prompt Section for default value]&#34;,&#xA;        stream = true,&#xA;        api_key = &#39;GEMINI_API_KEY&#39;,&#xA;        optional = {},&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The following configuration is not the default, but recommended to prevent request timeout from outputing too many tokens. You can also adjust the safety settings following the example:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;provider_options = {&#xA;    gemini = {&#xA;        optional = {&#xA;            generationConfig = {&#xA;                maxOutputTokens = 256,&#xA;            },&#xA;            safetySettings = {&#xA;                {&#xA;                    -- HARM_CATEGORY_HATE_SPEECH,&#xA;                    -- HARM_CATEGORY_HARASSMENT&#xA;                    -- HARM_CATEGORY_SEXUALLY_EXPLICIT&#xA;                    category = &#39;HARM_CATEGORY_DANGEROUS_CONTENT&#39;,&#xA;                    -- BLOCK_NONE&#xA;                    threshold = &#39;BLOCK_ONLY_HIGH&#39;,&#xA;                },&#xA;            },&#xA;        },&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Experimental Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Gemini appears to perform better with an alternative input structure, unlike other chat-based LLMs. This observation is currently experimental and requires further validation. For details on the experimental prompt setup currently in use by the maintainer, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/prompt.md#an-experimental-configuration-setup-for-gemini&#34;&gt;prompt documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;OpenAI-compatible&lt;/h2&gt; &#xA;&lt;p&gt;Use any providers compatible with OpenAI&#39;s chat completion API.&lt;/p&gt; &#xA;&lt;p&gt;For example, you can set the &lt;code&gt;end_point&lt;/code&gt; to &lt;code&gt;http://localhost:11434/v1/chat/completions&lt;/code&gt; to use &lt;code&gt;ollama&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;Note that not all openAI compatible services has streaming support, you should change &lt;code&gt;stream=false&lt;/code&gt; to disable streaming in case your services do not support it.&lt;/p&gt; &#xA; &lt;p&gt;The following config is the default.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;provider_options = {&#xA;    openai_compatible = {&#xA;        model = &#39;llama-3.3-70b-versatile&#39;,&#xA;        system = &#34;see [Prompt] section for the default value&#34;,&#xA;        few_shots = &#34;see [Prompt] section for the default value&#34;,&#xA;        chat_input = &#34;See [Prompt Section for default value]&#34;,&#xA;        end_point = &#39;https://api.groq.com/openai/v1/chat/completions&#39;,&#xA;        api_key = &#39;GROQ_API_KEY&#39;,&#xA;        name = &#39;Groq&#39;,&#xA;        stream = true,&#xA;        optional = {&#xA;            stop = nil,&#xA;            max_tokens = nil,&#xA;        },&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;OpenAI-FIM-compatible&lt;/h2&gt; &#xA;&lt;p&gt;Use any provider compatible with OpenAI&#39;s completion API. This request uses the text &lt;code&gt;/completions&lt;/code&gt; endpoint, &lt;strong&gt;not&lt;/strong&gt; &lt;code&gt;/chat/completions&lt;/code&gt; endpoint, so system prompts and few-shot examples are not applicable.&lt;/p&gt; &#xA;&lt;p&gt;For example, you can set the &lt;code&gt;end_point&lt;/code&gt; to &lt;code&gt;http://localhost:11434/v1/completions&lt;/code&gt; to use &lt;code&gt;ollama&lt;/code&gt;, or set it to &lt;code&gt;http://localhost:8012/v1/completions&lt;/code&gt; to use &lt;code&gt;llama.cpp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Cmdline completion is available for models supported by these providers: &lt;code&gt;deepseek&lt;/code&gt;, &lt;code&gt;ollama&lt;/code&gt;, and &lt;code&gt;siliconflow&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;Refer to the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/completions&#34;&gt;Completions Legacy&lt;/a&gt; section of the OpenAI documentation for details.&lt;/p&gt; &#xA; &lt;p&gt;Please note that not all OpenAI-compatible services support streaming. If your service does not support streaming, you should set &lt;code&gt;stream=false&lt;/code&gt; to disable it.&lt;/p&gt; &#xA; &lt;p&gt;Additionally, for Ollama users, it is essential to verify whether the model&#39;s template supports FIM completion. For example, qwen2.5-coder offers FIM support, as suggested in its &lt;a href=&#34;https://ollama.com/library/qwen2.5-coder/blobs/e94a8ecb9327&#34;&gt;template&lt;/a&gt;. However it may come as a surprise to some users that, &lt;code&gt;deepseek-coder&lt;/code&gt; does not support the FIM template, and you should use &lt;code&gt;deepseek-coder-v2&lt;/code&gt; instead.&lt;/p&gt; &#xA; &lt;p&gt;For example bash scripts to run llama.cpp based on your local computing power, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/recipes.md&#34;&gt;recipes.md&lt;/a&gt;. Note that the model for &lt;code&gt;llama.cpp&lt;/code&gt; must be determined when you launch the &lt;code&gt;llama.cpp&lt;/code&gt; server and cannot be changed thereafter.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;provider_options = {&#xA;    openai_fim_compatible = {&#xA;        model = &#39;deepseek-chat&#39;,&#xA;        end_point = &#39;https://api.deepseek.com/beta/completions&#39;,&#xA;        api_key = &#39;DEEPSEEK_API_KEY&#39;,&#xA;        name = &#39;Deepseek&#39;,&#xA;        stream = true,&#xA;        template = {&#xA;            prompt = &#34;See [Prompt Section for default value]&#34;,&#xA;            suffix = &#34;See [Prompt Section for default value]&#34;,&#xA;        },&#xA;        optional = {&#xA;            stop = nil,&#xA;            max_tokens = nil,&#xA;        },&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The following configuration is not the default, but recommended to prevent request timeout from outputing too many tokens.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;provider_options = {&#xA;    openai_fim_compatible = {&#xA;        optional = {&#xA;            max_tokens = 256,&#xA;            stop = { &#39;\n\n&#39; },&#xA;        },&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Commands&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;code&gt;Minuet change_provider&lt;/code&gt;, &lt;code&gt;Minuet change_model&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;change_provider&lt;/code&gt; command allows you to change the provider after &lt;code&gt;Minuet&lt;/code&gt; has been setup.&lt;/p&gt; &#xA;&lt;p&gt;Example usage: &lt;code&gt;Minuet change_provider claude&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;change_model&lt;/code&gt; command allows you to change both the provider and model in one command. When called without arguments, it will open an interactive selection menu using &lt;code&gt;vim.ui.select&lt;/code&gt; to choose from available models. When called with an argument, the format is &lt;code&gt;provider:model&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Example usage:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Minuet change_model&lt;/code&gt; - Opens interactive model selection&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Minuet change_model gemini:gemini-1.5-pro-latest&lt;/code&gt; - Directly sets the model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note: For &lt;code&gt;openai_compatible&lt;/code&gt; and &lt;code&gt;openai_fim_compatible&lt;/code&gt; providers, the model completions in cmdline are determined by the &lt;code&gt;name&lt;/code&gt; field in your configuration. For example, if you configured:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;provider_options.openai_compatible.name = &#39;Fireworks&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When entering &lt;code&gt;Minuet change_model openai_compatible:&lt;/code&gt; in the cmdline, you&#39;ll see model completions specific to the Fireworks provider.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;Minuet change_preset&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;change_preset&lt;/code&gt; command allows you to switch between config presets that were defined during initial setup. Presets provide a convenient way to toggle between different config sets. This is particularly useful when you need to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Switch between different cloud providers (such as Fireworks or Groq) for the &lt;code&gt;openai_compatible&lt;/code&gt; provider&lt;/li&gt; &#xA; &lt;li&gt;Apply different throttle and debounce settings for different providers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When called, the command merges the selected preset with the current config table to create an updated configuration.&lt;/p&gt; &#xA;&lt;p&gt;Usage syntax: &lt;code&gt;Minuet change_preset preset_1&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Presets can be configured during the initial setup process.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;minuet&#39;).setup {&#xA;    presets = {&#xA;        preset_1 = {&#xA;            -- Configuration for cloud-based requests with large context window&#xA;            context_window = 20000,&#xA;            request_timeout = 4,&#xA;            throttle = 3000,&#xA;            debounce = 1000,&#xA;            provider = &#39;openai_compatible&#39;,&#xA;            provider_options = {&#xA;                openai_compatible = {&#xA;                    model = &#39;llama-3.3-70b-versatile&#39;,&#xA;                    api_key = &#39;GROQ_API_KEY&#39;,&#xA;                    name = &#39;Groq&#39;&#xA;                }&#xA;            }&#xA;        },&#xA;        preset_2 = {&#xA;            -- Configuration for local model with smaller context window&#xA;            provider = &#39;openai_fim_compatible&#39;,&#xA;            context_window = 2000,&#xA;            throttle = 400,&#xA;            debounce = 100,&#xA;            provider_options = {&#xA;                openai_fim_compatible = {&#xA;                    api_key = &#39;TERM&#39;,&#xA;                    name = &#39;Ollama&#39;,&#xA;                    end_point = &#39;http://localhost:11434/v1/completions&#39;,&#xA;                    model = &#39;qwen2.5-coder:7b&#39;,&#xA;                    optional = {&#xA;                        max_tokens = 256,&#xA;                        top_p = 0.9&#xA;                    }&#xA;                }&#xA;            }&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;code&gt;Minuet blink&lt;/code&gt;, &lt;code&gt;Minuet cmp&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Enable or disable autocompletion for &lt;code&gt;nvim-cmp&lt;/code&gt; or &lt;code&gt;blink.cmp&lt;/code&gt;. While Minuet must be added to your cmp/blink sources, this command only controls whether Minuet is triggered during autocompletion. The command does not affect manual completion behavior - Minuet remains active and available when manually invoked.&lt;/p&gt; &#xA;&lt;p&gt;Example usage: &lt;code&gt;Minuet blink toggle&lt;/code&gt;, &lt;code&gt;Minuet blink enable&lt;/code&gt;, &lt;code&gt;Minuet blink disable&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;Minuet virtualtext&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Enable or disable the automatic display of &lt;code&gt;virtual-text&lt;/code&gt; completion in the &lt;strong&gt;current buffer&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Example usage: &lt;code&gt;Minuet virtualtext toggle&lt;/code&gt;, &lt;code&gt;Minuet virtualtext enable&lt;/code&gt;, &lt;code&gt;Minuet virtualtext disable&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;Minuet lsp&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The Minuet LSP command provides commands for managing the in-process LSP server:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;:Minuet lsp attach&lt;/code&gt;: Attach the Minuet LSP server to the &lt;strong&gt;current buffer&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;:Minuet lsp detach&lt;/code&gt;: Detach the Minuet LSP server from the &lt;strong&gt;current buffer&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;:Minuet lsp enable_auto_trigger&lt;/code&gt;: Enable automatic completion triggering using &lt;code&gt;vim.lsp.completion.enable&lt;/code&gt; for &lt;strong&gt;current buffer&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;:Minuet lsp disable_auto_trigger&lt;/code&gt;: Disable automatic completion triggering for &lt;strong&gt;current buffer&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;API&lt;/h1&gt; &#xA;&lt;h2&gt;Virtual Text&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;minuet-ai.nvim&lt;/code&gt; offers the following functions to customize your key mappings:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;{&#xA;    -- accept whole completion&#xA;    require(&#39;minuet.virtualtext&#39;).action.accept,&#xA;    -- accept by line&#xA;    require(&#39;minuet.virtualtext&#39;).action.accept_line,&#xA;    -- accept n lines (prompts for number)&#xA;    require(&#39;minuet.virtualtext&#39;).action.accept_n_lines,&#xA;    require(&#39;minuet.virtualtext&#39;).action.next,&#xA;    require(&#39;minuet.virtualtext&#39;).action.prev,&#xA;    require(&#39;minuet.virtualtext&#39;).action.dismiss,&#xA;    -- whether the virtual text is visible in current buffer&#xA;    require(&#39;minuet.virtualtext&#39;).action.is_visible,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Lualine&lt;/h2&gt; &#xA;&lt;p&gt;Minuet provides a Lualine component that displays the current status of Minuet requests. This component shows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The name of the active provider&lt;/li&gt; &#xA; &lt;li&gt;The current request count (e.g., &#34;1/3&#34;)&lt;/li&gt; &#xA; &lt;li&gt;An animated spinner while processing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To use the Minuet Lualine component, add it to your Lualine configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;lualine&#39;).setup {&#xA;  sections = {&#xA;    lualine_x = {&#xA;      require(&#39;minuet.lualine&#39;),&#xA;      &#39;encoding&#39;,&#xA;      &#39;fileformat&#39;,&#xA;      &#39;filetype&#39;,&#xA;    },&#xA;  },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Minuet Event&lt;/h2&gt; &#xA;&lt;p&gt;Minuet emits three distinct events during its request workflow:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;MinuetRequestStartedPre&lt;/strong&gt;: Triggered before a completion request is initiated. This allows for pre-request operations, such as logging or updating the user interface.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MinuetRequestStarted&lt;/strong&gt;: Triggered immediately after the completion request is dispatched, signaling that the request is in progress.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MinuetRequestFinished&lt;/strong&gt;: Triggered upon completion of the request.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each event includes a &lt;code&gt;data&lt;/code&gt; field containing the following properties:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;provider&lt;/code&gt;: A string indicating the provider type (e.g., &#39;openai_compatible&#39;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;name&lt;/code&gt;: A string specifying the provider&#39;s name (e.g., &#39;OpenAI&#39;, &#39;Groq&#39;, &#39;Ollama&#39;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;n_requests&lt;/code&gt;: The number of requests encompassed in this completion cycle.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;request_idx&lt;/code&gt; (optional): The index of the current request, applicable when providers make multiple requests.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;timestamp&lt;/code&gt;: A Unix timestamp representing the start of the request cycle (corresponding to the &lt;code&gt;MinuetRequestStartedPre&lt;/code&gt; event).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h2&gt;Customize &lt;code&gt;cmp&lt;/code&gt; ui for source icon and kind icon&lt;/h2&gt; &#xA;&lt;p&gt;You can configure the icons of completion items returned by &lt;code&gt;minuet&lt;/code&gt; by using the following snippet (referenced from &lt;a href=&#34;https://github.com/hrsh7th/nvim-cmp/wiki/Menu-Appearance#basic-customisations&#34;&gt;cmp&#39;s wiki&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local kind_icons = {&#xA;    Number = &#39;󰎠&#39;,&#xA;    Array = &#39;&#39;,&#xA;    Variable = &#39;&#39;,&#xA;    -- and other icons&#xA;    -- LLM Provider icons&#xA;    claude = &#39;󰋦&#39;,&#xA;    openai = &#39;󱢆&#39;,&#xA;    codestral = &#39;󱎥&#39;,&#xA;    gemini = &#39;&#39;,&#xA;    Groq = &#39;&#39;,&#xA;    Openrouter = &#39;󱂇&#39;,&#xA;    Ollama = &#39;󰳆&#39;,&#xA;    [&#39;Llama.cpp&#39;] = &#39;󰳆&#39;,&#xA;    Deepseek = &#39;&#39;&#xA;    -- FALLBACK&#xA;    fallback = &#39;&#39;,&#xA;}&#xA;&#xA;local source_icons = {&#xA;    minuet = &#39;󱗻&#39;,&#xA;    nvim_lsp = &#39;&#39;,&#xA;    lsp = &#39;&#39;,&#xA;    buffer = &#39;&#39;,&#xA;    luasnip = &#39;&#39;,&#xA;    snippets = &#39;&#39;,&#xA;    path = &#39;&#39;,&#xA;    git = &#39;&#39;,&#xA;    tags = &#39;&#39;,&#xA;    -- FALLBACK&#xA;    fallback = &#39;󰜚&#39;,&#xA;}&#xA;&#xA;local cmp = require &#39;cmp&#39;&#xA;cmp.setup {&#xA;    formatting = {&#xA;        format = function(entry, vim_item)&#xA;            -- Kind icons&#xA;            -- This concatenates the icons with the name of the item kind&#xA;            vim_item.kind = string.format(&#39;%s %s&#39;, kind_icons[vim_item.kind] or kind_icons.fallback, vim_item.kind)&#xA;            -- Source&#xA;            vim_item.menu = source_icons[entry.source.name] or source_icons.fallback&#xA;            return vim_item&#xA;        end,&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Customize &lt;code&gt;blink&lt;/code&gt; ui for source icon and kind icon&lt;/h2&gt; &#xA;&lt;p&gt;You can configure the icons of completion items returned by &lt;code&gt;minuet&lt;/code&gt; by the following snippet:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;To customize the kind icons:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local kind_icons = {&#xA;    -- LLM Provider icons&#xA;    claude = &#39;󰋦&#39;,&#xA;    openai = &#39;󱢆&#39;,&#xA;    codestral = &#39;󱎥&#39;,&#xA;    gemini = &#39;&#39;,&#xA;    Groq = &#39;&#39;,&#xA;    Openrouter = &#39;󱂇&#39;,&#xA;    Ollama = &#39;󰳆&#39;,&#xA;    [&#39;Llama.cpp&#39;] = &#39;󰳆&#39;,&#xA;    Deepseek = &#39;&#39;&#xA;}&#xA;&#xA;require(&#39;blink-cmp&#39;).setup {&#xA;    appearance = {&#xA;        use_nvim_cmp_as_default = true,&#xA;        nerd_font_variant = &#39;normal&#39;,&#xA;        kind_icons = kind_icons&#xA;    },&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To customize the source icons:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local source_icons = {&#xA;    minuet = &#39;󱗻&#39;,&#xA;    orgmode = &#39;&#39;,&#xA;    otter = &#39;󰼁&#39;,&#xA;    nvim_lsp = &#39;&#39;,&#xA;    lsp = &#39;&#39;,&#xA;    buffer = &#39;&#39;,&#xA;    luasnip = &#39;&#39;,&#xA;    snippets = &#39;&#39;,&#xA;    path = &#39;&#39;,&#xA;    git = &#39;&#39;,&#xA;    tags = &#39;&#39;,&#xA;    cmdline = &#39;󰘳&#39;,&#xA;    latex_symbols = &#39;&#39;,&#xA;    cmp_nvim_r = &#39;󰟔&#39;,&#xA;    codeium = &#39;󰩂&#39;,&#xA;    -- FALLBACK&#xA;    fallback = &#39;󰜚&#39;,&#xA;}&#xA;&#xA;require(&#39;blink-cmp&#39;).setup {&#xA;    appearance = {&#xA;        use_nvim_cmp_as_default = true,&#xA;        nerd_font_variant = &#39;normal&#39;,&#xA;        kind_icons = kind_icons&#xA;    },&#xA;    completion = {&#xA;        menu = {&#xA;            draw = {&#xA;                columns = {&#xA;                    { &#39;label&#39;, &#39;label_description&#39;, gap = 1 },&#xA;                    { &#39;kind_icon&#39;, &#39;kind&#39; },&#xA;                    { &#39;source_icon&#39; },&#xA;                },&#xA;                components = {&#xA;                    source_icon = {&#xA;                        -- don&#39;t truncate source_icon&#xA;                        ellipsis = false,&#xA;                        text = function(ctx)&#xA;                            return source_icons[ctx.source_name:lower()] or source_icons.fallback&#xA;                        end,&#xA;                        highlight = &#39;BlinkCmpSource&#39;,&#xA;                    },&#xA;                },&#xA;            },&#xA;        },&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Significant Input Delay When Moving to a New Line with &lt;code&gt;nvim-cmp&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;When using Minuet with auto-complete enabled, you may occasionally experience a noticeable delay when pressing &lt;code&gt;&amp;lt;CR&amp;gt;&lt;/code&gt; to move to the next line. This occurs because Minuet triggers autocompletion at the start of a new line, while cmp blocks the &lt;code&gt;&amp;lt;CR&amp;gt;&lt;/code&gt; key, awaiting Minuet&#39;s response.&lt;/p&gt; &#xA;&lt;p&gt;To address this issue, consider the following solutions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Unbind the &lt;code&gt;&amp;lt;CR&amp;gt;&lt;/code&gt; key from your cmp keymap.&lt;/li&gt; &#xA; &lt;li&gt;Utilize cmp&#39;s internal API to avoid blocking calls, though be aware that this API may change without prior notice.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Here&#39;s an example of the second approach using Lua:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local cmp = require &#39;cmp&#39;&#xA;opts.mapping = {&#xA;    [&#39;&amp;lt;CR&amp;gt;&#39;] = cmp.mapping(function(fallback)&#xA;        -- use the internal non-blocking call to check if cmp is visible&#xA;        if cmp.core.view:visible() then&#xA;            cmp.confirm { select = true }&#xA;        else&#xA;            fallback()&#xA;        end&#xA;    end),&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Integration with &lt;code&gt;lazyvim&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;&lt;strong&gt;With nvim-cmp&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;{&#xA;    &#39;milanglacier/minuet-ai.nvim&#39;,&#xA;    config = function()&#xA;        require(&#39;minuet&#39;).setup {&#xA;            -- Your configuration options here&#xA;        }&#xA;    end&#xA;},&#xA;{&#xA;    &#39;nvim-cmp&#39;,&#xA;    optional = true,&#xA;    opts = function(_, opts)&#xA;        -- if you wish to use autocomplete&#xA;        table.insert(opts.sources, 1, {&#xA;            name = &#39;minuet&#39;,&#xA;            group_index = 1,&#xA;            priority = 100,&#xA;        })&#xA;&#xA;        opts.performance = {&#xA;            -- It is recommended to increase the timeout duration due to&#xA;            -- the typically slower response speed of LLMs compared to&#xA;            -- other completion sources. This is not needed when you only&#xA;            -- need manual completion.&#xA;            fetching_timeout = 2000,&#xA;        }&#xA;&#xA;        opts.mapping = vim.tbl_deep_extend(&#39;force&#39;, opts.mapping or {}, {&#xA;            -- if you wish to use manual complete&#xA;            [&#39;&amp;lt;A-y&amp;gt;&#39;] = require(&#39;minuet&#39;).make_cmp_map(),&#xA;        })&#xA;    end,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;With blink-cmp&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;-- set the following line in your config/options.lua&#xA;vim.g.lazyvim_blink_main = true&#xA;&#xA;{&#xA;    &#39;milanglacier/minuet-ai.nvim&#39;,&#xA;    config = function()&#xA;        require(&#39;minuet&#39;).setup {&#xA;            -- Your configuration options here&#xA;        }&#xA;    end,&#xA;},&#xA;{&#xA;    &#39;saghen/blink.cmp&#39;,&#xA;    optional = true,&#xA;    opts = {&#xA;        keymap = {&#xA;            [&#39;&amp;lt;A-y&amp;gt;&#39;] = {&#xA;                function(cmp)&#xA;                    cmp.show { providers = { &#39;minuet&#39; } }&#xA;                end,&#xA;            },&#xA;        },&#xA;        sources = {&#xA;            -- if you want to use auto-complete&#xA;            default =  { &#39;minuet&#39; },&#xA;            providers = {&#xA;                minuet = {&#xA;                    name = &#39;minuet&#39;,&#xA;                    module = &#39;minuet.blink&#39;,&#xA;                    score_offset = 100,&#xA;                },&#xA;            },&#xA;        },&#xA;    },&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Enhancement&lt;/h1&gt; &#xA;&lt;h2&gt;RAG (Experimental)&lt;/h2&gt; &#xA;&lt;p&gt;You can enhance the content sent to the LLM for code completion by leveraging RAG support through the &lt;a href=&#34;https://github.com/Davidyz/VectorCode&#34;&gt;VectorCode&lt;/a&gt; package.&lt;/p&gt; &#xA;&lt;p&gt;VectorCode contains two main components. The first is a standalone CLI program written in Python, available for installation via PyPI. This program is responsible for creating the vector database and processing RAG queries. The second component is a Neovim plugin that provides utility functions to send queries and manage buffer-related RAG information within Neovim.&lt;/p&gt; &#xA;&lt;p&gt;We offer two example recipes demonstrating VectorCode integration: one for chat-based LLMs (Gemini) and another for the FIM model (Qwen-2.5-Coder), available in &lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/recipes.md&#34;&gt;recipes.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For detailed instructions on setting up and using VectorCode, please refer to the &lt;a href=&#34;https://github.com/Davidyz/VectorCode/raw/main/docs/neovim.md&#34;&gt;official VectorCode documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Troubleshooting&lt;/h1&gt; &#xA;&lt;p&gt;If your setup failed, there are two most likely reasons:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You may set the API key incorrectly. Checkout the &lt;a href=&#34;https://raw.githubusercontent.com/milanglacier/minuet-ai.nvim/main/#api-keys&#34;&gt;API Key&lt;/a&gt; section to see how to correctly specify the API key.&lt;/li&gt; &#xA; &lt;li&gt;You are using a model or a context window that is too large, causing completion items to timeout before returning any tokens. This is particularly common with local LLM. It is recommended to start with the following settings to have a better understanding of your provider&#39;s inference speed. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Begin by testing with manual completions.&lt;/li&gt; &#xA;   &lt;li&gt;Use a smaller context window (e.g., &lt;code&gt;config.context_window = 768&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Use a smaller model&lt;/li&gt; &#xA;   &lt;li&gt;Set a longer request timeout (e.g., &lt;code&gt;config.request_timeout = 5&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To diagnose issues, set &lt;code&gt;config.notify = debug&lt;/code&gt; and examine the output.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Contributions are welcome! Please feel free to submit a Pull Request.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tzachar/cmp-ai&#34;&gt;cmp-ai&lt;/a&gt;: Reference for the integration with &lt;code&gt;nvim-cmp&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.continue.dev&#34;&gt;continue.dev&lt;/a&gt;: not a neovim plugin, but I find a lot LLM models from here.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zbirenbaum/copilot.lua&#34;&gt;copilot.lua&lt;/a&gt;: Reference for the virtual text frontend.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggml-org/llama.vim&#34;&gt;llama.vim&lt;/a&gt;: Reference for CLI parameters used to launch the llama-cpp server.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/saecki/crates.nvim&#34;&gt;crates.nvim&lt;/a&gt;: Reference for in-process LSP implemtation to provide completion.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>