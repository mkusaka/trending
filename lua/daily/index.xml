<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Lua Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-20T01:31:16Z</updated>
  <subtitle>Daily Trending of Lua in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>huggingface/llm.nvim</title>
    <updated>2024-06-20T01:31:16Z</updated>
    <id>tag:github.com,2024-06-20:/huggingface/llm.nvim</id>
    <link href="https://github.com/huggingface/llm.nvim" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM powered development for Neovim&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLM powered development for Neovim&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;llm.nvim&lt;/strong&gt; is a plugin for all things LLM. It uses &lt;a href=&#34;https://github.com/huggingface/llm-ls&#34;&gt;&lt;strong&gt;llm-ls&lt;/strong&gt;&lt;/a&gt; as a backend.&lt;/p&gt; &#xA;&lt;p&gt;This project is influenced by &lt;a href=&#34;https://github.com/github/copilot.vim&#34;&gt;copilot.vim&lt;/a&gt; and &lt;a href=&#34;https://github.com/codota/tabnine-nvim&#34;&gt;tabnine-nvim&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Formerly &lt;strong&gt;hfcc.nvim&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/llm.nvim/main/assets/llm_nvim_demo.gif&#34; alt=&#34;demonstration use of llm.nvim&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] When using the Inference API, you will probably encounter some limitations. Subscribe to the &lt;em&gt;PRO&lt;/em&gt; plan to avoid getting rate limited in the free tier.&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/pricing#pro&#34;&gt;https://huggingface.co/pricing#pro&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;h3&gt;Code completion&lt;/h3&gt; &#xA;&lt;p&gt;This plugin supports &#34;ghost-text&#34; code completion, Ã  la Copilot.&lt;/p&gt; &#xA;&lt;h3&gt;Choose your model&lt;/h3&gt; &#xA;&lt;p&gt;Requests for code generation are made via an HTTP request.&lt;/p&gt; &#xA;&lt;p&gt;You can use the Hugging Face &lt;a href=&#34;https://huggingface.co/inference-api&#34;&gt;Inference API&lt;/a&gt; or your own HTTP endpoint, provided it adheres to the APIs listed in &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/llm.nvim/main/#backend&#34;&gt;backend&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Always fit within the context window&lt;/h3&gt; &#xA;&lt;p&gt;The prompt sent to the model will always be sized to fit within the context window, with the number of tokens determined using &lt;a href=&#34;https://github.com/huggingface/tokenizers&#34;&gt;tokenizers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;h3&gt;Backend&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;llm.nvim&lt;/strong&gt; can interface with multiple backends hosting models.&lt;/p&gt; &#xA;&lt;p&gt;You can override the url of the backend with the &lt;code&gt;LLM_NVIM_URL&lt;/code&gt; environment variable. If url is &lt;code&gt;nil&lt;/code&gt;, it will default to the Inference API&#39;s &lt;a href=&#34;https://github.com/huggingface/llm-ls/raw/8926969265990202e3b399955364cc090df389f4/crates/custom-types/src/llm_ls.rs#L8&#34;&gt;default url&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;When &lt;code&gt;api_token&lt;/code&gt; is set, it will be passed as a header: &lt;code&gt;Authorization: Bearer &amp;lt;api_token&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;llm-ls&lt;/strong&gt; will try to add the correct path to the url to get completions if it does not already end with said path. You can disable this behavior by setting &lt;code&gt;disable_url_path_completion&lt;/code&gt; to true.&lt;/p&gt; &#xA;&lt;h4&gt;Inference API&lt;/h4&gt; &#xA;&lt;h5&gt;&lt;strong&gt;backend = &#34;huggingface&#34;&lt;/strong&gt;&lt;/h5&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task&#34;&gt;API&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create and get your API token from here &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;https://huggingface.co/settings/tokens&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Define how the plugin will read your token. For this you have multiple options, in order of precedence:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Pass &lt;code&gt;api_token = &amp;lt;your token&amp;gt;&lt;/code&gt; in plugin opts - this is not recommended if you use a versioning tool for your configuration files&lt;/li&gt; &#xA;   &lt;li&gt;Set the &lt;code&gt;LLM_NVIM_HF_API_TOKEN&lt;/code&gt; environment variable&lt;/li&gt; &#xA;   &lt;li&gt;You can define your &lt;code&gt;HF_HOME&lt;/code&gt; environment variable and create a file containing your token at &lt;code&gt;$HF_HOME/token&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Install the &lt;a href=&#34;https://huggingface.co/docs/huggingface_hub/quick-start&#34;&gt;huggingface-cli&lt;/a&gt; and run &lt;code&gt;huggingface-cli login&lt;/code&gt; - this will prompt you to enter your token and set it at the right path&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Choose your model on the &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face Hub&lt;/a&gt;, and, in order of precedence, you can either:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Set the &lt;code&gt;LLM_NVIM_MODEL&lt;/code&gt; environment variable&lt;/li&gt; &#xA;   &lt;li&gt;Pass &lt;code&gt;model = &amp;lt;model identifier&amp;gt;&lt;/code&gt; in plugin opts&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note: the &lt;code&gt;model&lt;/code&gt;&#39;s value will be appended to the url like so : &lt;code&gt;{url}/model/{model}&lt;/code&gt; as this is how we route requests to the right model.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://ollama.com/&#34;&gt;Ollama&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h5&gt;&lt;strong&gt;backend = &#34;ollama&#34;&lt;/strong&gt;&lt;/h5&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ollama/ollama/raw/main/docs/api.md&#34;&gt;API&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Refer to Ollama&#39;s documentation on how to run ollama. Here is an example configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;{&#xA;  model = &#34;codellama:7b&#34;,&#xA;  url = &#34;http://localhost:11434&#34;, -- llm-ls uses &#34;/api/generate&#34;&#xA;  -- cf https://github.com/ollama/ollama/blob/main/docs/api.md#parameters&#xA;  request_body = {&#xA;    -- Modelfile options for the model you use&#xA;    options = {&#xA;      temperature = 0.2,&#xA;      top_p = 0.95,&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: &lt;code&gt;model&lt;/code&gt;&#39;s value will be added to the request body.&lt;/p&gt; &#xA;&lt;h4&gt;Open AI&lt;/h4&gt; &#xA;&lt;h5&gt;&lt;strong&gt;backend = &#34;openai&#34;&lt;/strong&gt;&lt;/h5&gt; &#xA;&lt;p&gt;Refer to Ollama&#39;s documentation on how to run ollama. Here is an example configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;{&#xA;  model = &#34;codellama&#34;,&#xA;  url = &#34;http://localhost:8000&#34;, -- llm-ls uses &#34;/v1/completions&#34;&#xA;  -- cf https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#openai-compatible-web-server&#xA;  request_body = {&#xA;    temperature = 0.2,&#xA;    top_p = 0.95,&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: &lt;code&gt;model&lt;/code&gt;&#39;s value will be added to the request body.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt;TGI&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h5&gt;&lt;strong&gt;backend = &#34;tgi&#34;&lt;/strong&gt;&lt;/h5&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.github.io/text-generation-inference/#/Text%20Generation%20Inference/generate&#34;&gt;API&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Refer to TGI&#39;s documentation on how to run TGI. Here is an example configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;{&#xA;  model = &#34;bigcode/starcoder&#34;,&#xA;  url = &#34;http://localhost:8080&#34;, -- llm-ls uses &#34;/generate&#34;&#xA;  -- cf https://huggingface.github.io/text-generation-inference/#/Text%20Generation%20Inference/generate&#xA;  request_body = {&#xA;    parameters = {&#xA;      temperature = 0.2,&#xA;      top_p = 0.95,&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://huggingface.co/bigcode/starcoder&#34;&gt;Starcoder&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;{&#xA;  tokens_to_clear = { &#34;&amp;lt;|endoftext|&amp;gt;&#34; },&#xA;  fim = {&#xA;    enabled = true,&#xA;    prefix = &#34;&amp;lt;fim_prefix&amp;gt;&#34;,&#xA;    middle = &#34;&amp;lt;fim_middle&amp;gt;&#34;,&#xA;    suffix = &#34;&amp;lt;fim_suffix&amp;gt;&#34;,&#xA;  },&#xA;  model = &#34;bigcode/starcoder&#34;,&#xA;  context_window = 8192,&#xA;  tokenizer = {&#xA;    repository = &#34;bigcode/starcoder&#34;,&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] These are the default config values&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://huggingface.co/codellama/CodeLlama-13b-hf&#34;&gt;CodeLlama&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;{&#xA;  tokens_to_clear = { &#34;&amp;lt;EOT&amp;gt;&#34; },&#xA;  fim = {&#xA;    enabled = true,&#xA;    prefix = &#34;&amp;lt;PRE&amp;gt; &#34;,&#xA;    middle = &#34; &amp;lt;MID&amp;gt;&#34;,&#xA;    suffix = &#34; &amp;lt;SUF&amp;gt;&#34;,&#xA;  },&#xA;  model = &#34;codellama/CodeLlama-13b-hf&#34;,&#xA;  context_window = 4096,&#xA;  tokenizer = {&#xA;    repository = &#34;codellama/CodeLlama-13b-hf&#34;,&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Spaces are important here&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/huggingface/llm-ls&#34;&gt;&lt;strong&gt;llm-ls&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;By default, &lt;strong&gt;llm-ls&lt;/strong&gt; is installed by &lt;strong&gt;llm.nvim&lt;/strong&gt; the first time it is loaded. The binary is downloaded from the &lt;a href=&#34;https://github.com/huggingface/llm-ls/releases&#34;&gt;release page&lt;/a&gt; and stored in:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;vim.api.nvim_call_function(&#34;stdpath&#34;, { &#34;data&#34; }) .. &#34;/llm_nvim/bin&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When developing locally, when using mason or if you built your own binary because your platform is not supported, you can set the &lt;code&gt;lsp.bin_path&lt;/code&gt; setting to the path of the binary. You can also start &lt;strong&gt;llm-ls&lt;/strong&gt; via tcp using the &lt;code&gt;--port [PORT]&lt;/code&gt; option, which is useful when using a debugger.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;lsp.version&lt;/code&gt; is used only when &lt;strong&gt;llm.nvim&lt;/strong&gt; downloads &lt;strong&gt;llm-ls&lt;/strong&gt; from the release page.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;lsp.cmd_env&lt;/code&gt; can be used to set environment variables for the &lt;strong&gt;llm-ls&lt;/strong&gt; process.&lt;/p&gt; &#xA;&lt;h4&gt;Mason&lt;/h4&gt; &#xA;&lt;p&gt;You can install &lt;strong&gt;llm-ls&lt;/strong&gt; via &lt;a href=&#34;https://github.com/williamboman/mason.nvim&#34;&gt;mason.nvim&lt;/a&gt;. To do so, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-vim&#34;&gt;:MasonInstall llm-ls&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then reference &lt;strong&gt;llm-ls&lt;/strong&gt;&#39;s path in your configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;{&#xA;  -- ...&#xA;  lsp = {&#xA;    bin_path = vim.api.nvim_call_function(&#34;stdpath&#34;, { &#34;data&#34; }) .. &#34;/mason/bin/llm-ls&#34;,&#xA;  },&#xA;  -- ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tokenizer&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;llm-ls&lt;/strong&gt; uses &lt;a href=&#34;https://github.com/huggingface/tokenizers&#34;&gt;&lt;strong&gt;tokenizers&lt;/strong&gt;&lt;/a&gt; to make sure the prompt fits the &lt;code&gt;context_window&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To configure it, you have a few options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No tokenization, &lt;strong&gt;llm-ls&lt;/strong&gt; will count the number of characters instead:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;{&#xA;  tokenizer = nil,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;from a local file on your disk:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;{&#xA;  tokenizer = {&#xA;    path = &#34;/path/to/my/tokenizer.json&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;from a Hugging Face repository, &lt;strong&gt;llm-ls&lt;/strong&gt; will attempt to download &lt;code&gt;tokenizer.json&lt;/code&gt; at the root of the repository:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;{&#xA;  tokenizer = {&#xA;    repository = &#34;myusername/myrepo&#34;&#xA;    api_token = nil -- optional, in case the API token used for the backend is not the same&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;from an HTTP endpoint, &lt;strong&gt;llm-ls&lt;/strong&gt; will attempt to download a file via an HTTP GET request:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;{&#xA;  tokenizer = {&#xA;    url = &#34;https://my-endpoint.example.com/mytokenizer.json&#34;,&#xA;    to = &#34;/download/path/of/mytokenizer.json&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Suggestion behavior&lt;/h3&gt; &#xA;&lt;p&gt;You can tune the way the suggestions behave:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;enable_suggestions_on_startup&lt;/code&gt; lets you choose to enable or disable &#34;suggest-as-you-type&#34; suggestions on neovim startup. You can then toggle auto suggest with &lt;code&gt;LLMToggleAutoSuggest&lt;/code&gt; (see &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/llm.nvim/main/#commands&#34;&gt;Commands&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enable_suggestions_on_files&lt;/code&gt; lets you enable suggestions only on specific files that match the pattern matching syntax you will provide. It can either be a string or a list of strings, for example: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;to match on all types of buffers: &lt;code&gt;enable_suggestions_on_files: &#34;*&#34;&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;to match on all files in &lt;code&gt;my_project/&lt;/code&gt;: &lt;code&gt;enable_suggestions_on_files: &#34;/path/to/my_project/*&#34;&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;to match on all python and rust files: &lt;code&gt;enable_suggestions_on_files: { &#34;*.py&#34;, &#34;*.rs&#34; }&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Commands&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;llm.nvim&lt;/strong&gt; provides the following commands:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;LLMToggleAutoSuggest&lt;/code&gt; enables/disables automatic &#34;suggest-as-you-type&#34; suggestions&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;LLMSuggestion&lt;/code&gt; is used to manually request a suggestion&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Package manager&lt;/h3&gt; &#xA;&lt;h4&gt;Using &lt;a href=&#34;https://github.com/wbthomason/packer.nvim&#34;&gt;packer&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;packer&#34;).startup(function(use)&#xA;  use {&#xA;    &#39;huggingface/llm.nvim&#39;,&#xA;    config = function()&#xA;      require(&#39;llm&#39;).setup({&#xA;        -- cf Setup&#xA;      })&#xA;    end&#xA;  }&#xA;end)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using &lt;a href=&#34;https://github.com/folke/lazy.nvim&#34;&gt;lazy.nvim&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#34;lazy&#34;).setup({&#xA;  {&#xA;    &#39;huggingface/llm.nvim&#39;,&#xA;    opts = {&#xA;      -- cf Setup&#xA;    }&#xA;  },&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using &lt;a href=&#34;https://github.com/junegunn/vim-plug&#34;&gt;vim-plug&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-vim&#34;&gt;Plug &#39;huggingface/llm.nvim&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;require(&#39;llm&#39;).setup({&#xA;  -- cf Setup&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lua&#34;&gt;local llm = require(&#39;llm&#39;)&#xA;&#xA;llm.setup({&#xA;  api_token = nil, -- cf Install paragraph&#xA;  model = &#34;bigcode/starcoder2-15b&#34;, -- the model ID, behavior depends on backend&#xA;  backend = &#34;huggingface&#34;, -- backend ID, &#34;huggingface&#34; | &#34;ollama&#34; | &#34;openai&#34; | &#34;tgi&#34;&#xA;  url = nil, -- the http url of the backend&#xA;  tokens_to_clear = { &#34;&amp;lt;|endoftext|&amp;gt;&#34; }, -- tokens to remove from the model&#39;s output&#xA;  -- parameters that are added to the request body, values are arbitrary, you can set any field:value pair here it will be passed as is to the backend&#xA;  request_body = {&#xA;    parameters = {&#xA;      max_new_tokens = 60,&#xA;      temperature = 0.2,&#xA;      top_p = 0.95,&#xA;    },&#xA;  },&#xA;  -- set this if the model supports fill in the middle&#xA;  fim = {&#xA;    enabled = true,&#xA;    prefix = &#34;&amp;lt;fim_prefix&amp;gt;&#34;,&#xA;    middle = &#34;&amp;lt;fim_middle&amp;gt;&#34;,&#xA;    suffix = &#34;&amp;lt;fim_suffix&amp;gt;&#34;,&#xA;  },&#xA;  debounce_ms = 150,&#xA;  accept_keymap = &#34;&amp;lt;Tab&amp;gt;&#34;,&#xA;  dismiss_keymap = &#34;&amp;lt;S-Tab&amp;gt;&#34;,&#xA;  tls_skip_verify_insecure = false,&#xA;  -- llm-ls configuration, cf llm-ls section&#xA;  lsp = {&#xA;    bin_path = nil,&#xA;    host = nil,&#xA;    port = nil,&#xA;    cmd_env = nil, -- or { LLM_LOG_LEVEL = &#34;DEBUG&#34; } to set the log level of llm-ls&#xA;    version = &#34;0.5.3&#34;,&#xA;  },&#xA;  tokenizer = nil, -- cf Tokenizer paragraph&#xA;  context_window = 1024, -- max number of tokens for the context window&#xA;  enable_suggestions_on_startup = true,&#xA;  enable_suggestions_on_files = &#34;*&#34;, -- pattern matching syntax to enable suggestions on specific files, either a string or a list of strings&#xA;  disable_url_path_completion = false, -- cf Backend&#xA;})&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>