<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Lua Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-23T01:36:58Z</updated>
  <subtitle>Daily Trending of Lua in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AdiAddons/AdiBags</title>
    <updated>2022-11-23T01:36:58Z</updated>
    <id>tag:github.com,2022-11-23:/AdiAddons/AdiBags</id>
    <link href="https://github.com/AdiAddons/AdiBags" rel="alternate"></link>
    <summary type="html">&lt;p&gt;WoW Addon â€” Adirelle&#39;s bag addon.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AdiBags&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/AdiAddons/AdiBags?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/AdiAddons/AdiBags.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;AdiBags is a World of Warcraft addon that displays the contents of your bags in single view, distributed into several sections using smart filters. It is heavily inspired by Nargiddley&#39;s Baggins.&lt;/p&gt; &#xA;&lt;p&gt;Configuration is available through Blizzard addon panel, the &lt;code&gt;/adibags&lt;/code&gt; chat command, and by configuring &#34;right-click to open options&#34; and right clicking on any blank space in the bag window.&lt;/p&gt; &#xA;&lt;h2&gt;Main Features&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Filter Partitions&lt;/h3&gt; &#xA;&lt;p&gt;AdiBags will automatically filter and group items into partitions so that all like-items are always grouped together. AdiBags tries to make sure there are intelligent defaults that require little-to-no out of the box configuration for item grouping. The partiions them selves are layed out automatically as well, with no human interaction. The result is a beautiful item and bag experience, right from the start!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;span&gt;&lt;img width=&#34;570&#34; height=&#34;625&#34; src=&#34;https://i.imgur.com/PyCfmcR.gif&#34;&gt;&lt;/span&gt; &lt;br&gt; &lt;i&gt;Automatic filters based on Auction House categories, gear sets, and more!&lt;/i&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Character Currencies&lt;/h3&gt; &#xA;&lt;p&gt;AdiBags supports in-frame display of currencies. The specific currencies to display can be configured in the options panel, and currency display supports a dynamic layout that grows to the number of columns configured.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;span&gt;&lt;img width=&#34;417&#34; height=&#34;160&#34; src=&#34;https://i.imgur.com/dXICYfR.gif&#34;&gt;&lt;/span&gt; &lt;br&gt; &lt;i&gt;Currencies viewable directly in your bags.&lt;/i&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Full-text Item Searching&lt;/h3&gt; &#xA;&lt;p&gt;AdiBags has built-in support for item searching based on item names. Results are highlighted directly in the bag frame so they can be utilized right away.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;span&gt;&lt;img width=&#34;417&#34; height=&#34;160&#34; src=&#34;https://i.imgur.com/9Vc98w8.gif&#34;&gt;&lt;/span&gt; &lt;br&gt; &lt;i&gt;Items fade away when they don&#39;t match a search term.&lt;/i&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Modules&lt;/h2&gt; &#xA;&lt;p&gt;Users can write their own modules that integrate with AdiBags to extend the AddOn&#39;s functionality. There are two modules for AdiBags written by the authors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.curseforge.com/wow/addons/adibags_outfitter&#34;&gt;AdiBags_Outfitter&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add filters for the excellent &lt;a href=&#34;https://www.curseforge.com/wow/addons/outfitter&#34;&gt;Outfitter&lt;/a&gt; addon item sets&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.curseforge.com/wow/addons/adibags-pt3filter&#34;&gt;AdiBags_PT3Filter&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add filters based on &lt;a href=&#34;https://www.curseforge.com/wow/addons/libperiodictable-3-1&#34;&gt;LibPeriodicTable-3.1&lt;/a&gt; categories&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Feel free to submit your own modules in this readme!&lt;/p&gt; &#xA;&lt;h2&gt;Tips &amp;amp; Tricks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Custom item sections can be created using the &#34;manual filter&#34; option in the configuration panel. Items can then be dragged and dropped on section titles to reassign that item permanently to that section.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Known Issues, Bugs, and Feature Requests&lt;/h2&gt; &#xA;&lt;p&gt;We welcome all bug reports and feature requests, though we can&#39;t promise every single feature will be implemented. Please report any issues or feature requests via GitHub issues.&lt;/p&gt; &#xA;&lt;p&gt;We highly suggest installing both &lt;a href=&#34;https://www.curseforge.com/wow/addons/bug-grabber&#34;&gt;BugGrabber&lt;/a&gt; and &lt;a href=&#34;https://www.curseforge.com/wow/addons/bugsack&#34;&gt;BugSack&lt;/a&gt; in order to capture bugs in AdiBags. These bug messages should then be used in any bug reports filed through GitHub issues.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap and Future Thoughts&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Guild Bank&lt;/h3&gt; &#xA;&lt;p&gt;AdiBags is an abstraction on top of Blizzard bags, and does not work to make Blizzard bags consistent or sorted. Due to this abstraction, two users of AdiBags accessing the same underlying store, such as a guild bank, may see two different views. For this reason, we&#39;ve shied away from implementing a Guild Bank for now. However, there is renewed interest in solving this problem overall, so stay tuned for changes in this space.&lt;/p&gt; &#xA;&lt;h3&gt;Alt Bags and Bank&lt;/h3&gt; &#xA;&lt;p&gt;For the moment, alt bags and banks are not supported. Once we invest a bit more time into our abstraction engine, we may revisit this as an added feature.&lt;/p&gt; &#xA;&lt;h3&gt;Full Bag Skinning&lt;/h3&gt; &#xA;&lt;p&gt;Limited skinning is available using LibSharedMedia-1.0, however it is not as comprehensive as Masque support. Masque support is on our roadmap, and will eventually be implemented.&lt;/p&gt; &#xA;&lt;h3&gt;In-Depth Filter Editing&lt;/h3&gt; &#xA;&lt;p&gt;There are no plans to add a fully featured, scriptable, or logic/waterfall based filtering engine at this time.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;AdiBags is licensed under the GNU General Public License version 3.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>phillipi/pix2pix</title>
    <updated>2022-11-23T01:36:58Z</updated>
    <id>tag:github.com,2022-11-23:/phillipi/pix2pix</id>
    <link href="https://github.com/phillipi/pix2pix" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Image-to-image translation with conditional adversarial nets&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pix2pix&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://phillipi.github.io/pix2pix/&#34;&gt;Project&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/1611.07004&#34;&gt;Arxiv&lt;/a&gt; | &lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&#34;&gt;PyTorch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Torch implementation for learning a mapping from input images to output images, for example:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/phillipi/pix2pix/master/imgs/examples.jpg&#34; width=&#34;900px&#34;&gt; &#xA;&lt;p&gt;Image-to-Image Translation with Conditional Adversarial Networks&lt;br&gt; &lt;a href=&#34;http://web.mit.edu/phillipi/&#34;&gt;Phillip Isola&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~tinghuiz/&#34;&gt;Tinghui Zhou&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~efros/&#34;&gt;Alexei A. Efros&lt;/a&gt;&lt;br&gt; CVPR, 2017.&lt;/p&gt; &#xA;&lt;p&gt;On some tasks, decent results can be obtained fairly quickly and on small datasets. For example, to learn to generate facades (example shown above), we trained on just 400 images for about 2 hours (on a single Pascal Titan X GPU). However, for harder problems it may be important to train on far larger datasets, and for many hours or even days.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Please check out our &lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&#34;&gt;PyTorch&lt;/a&gt; implementation for pix2pix and CycleGAN. The PyTorch version is under active development and can produce results comparable to or better than this Torch version.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux or OSX&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU + CUDA CuDNN (CPU mode and CUDA without CuDNN may work with minimal modification, but untested)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install torch and dependencies from &lt;a href=&#34;https://github.com/torch/distro&#34;&gt;https://github.com/torch/distro&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install torch packages &lt;code&gt;nngraph&lt;/code&gt; and &lt;code&gt;display&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;luarocks install nngraph&#xA;luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repo:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:phillipi/pix2pix.git&#xA;cd pix2pix&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the dataset (e.g., &lt;a href=&#34;http://cmp.felk.cvut.cz/~tylecr1/facade/&#34;&gt;CMP Facades&lt;/a&gt;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./datasets/download_dataset.sh facades&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Train the model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA th train.lua&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(CPU only) The same training command without using a GPU or CUDNN. Setting the environment variables &lt;code&gt;gpu=0 cudnn=0&lt;/code&gt; forces CPU only&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA gpu=0 cudnn=0 batchSize=10 save_epoch_freq=5 th train.lua&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(Optionally) start the display server to view results as the model trains. ( See &lt;a href=&#34;https://raw.githubusercontent.com/phillipi/pix2pix/master/#display-ui&#34;&gt;Display UI&lt;/a&gt; for more details):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;th -ldisplay.start 8000 0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Finally, test the model:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA phase=val th test.lua&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The test results will be saved to an html file here: &lt;code&gt;./results/facades_generation/latest_net_G_val/index.html&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Train&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DATA_ROOT=/path/to/data/ name=expt_name which_direction=AtoB th train.lua&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Switch &lt;code&gt;AtoB&lt;/code&gt; to &lt;code&gt;BtoA&lt;/code&gt; to train translation in opposite direction.&lt;/p&gt; &#xA;&lt;p&gt;Models are saved to &lt;code&gt;./checkpoints/expt_name&lt;/code&gt; (can be changed by passing &lt;code&gt;checkpoint_dir=your_dir&lt;/code&gt; in train.lua).&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;opt&lt;/code&gt; in train.lua for additional training options.&lt;/p&gt; &#xA;&lt;h2&gt;Test&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DATA_ROOT=/path/to/data/ name=expt_name which_direction=AtoB phase=val th test.lua&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will run the model named &lt;code&gt;expt_name&lt;/code&gt; in direction &lt;code&gt;AtoB&lt;/code&gt; on all images in &lt;code&gt;/path/to/data/val&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Result images, and a webpage to view them, are saved to &lt;code&gt;./results/expt_name&lt;/code&gt; (can be changed by passing &lt;code&gt;results_dir=your_dir&lt;/code&gt; in test.lua).&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;opt&lt;/code&gt; in test.lua for additional testing options.&lt;/p&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Download the datasets using the following script. Some of the datasets are collected by other researchers. Please cite their papers if you use the data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./datasets/download_dataset.sh dataset_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;facades&lt;/code&gt;: 400 images from &lt;a href=&#34;http://cmp.felk.cvut.cz/~tylecr1/facade/&#34;&gt;CMP Facades dataset&lt;/a&gt;. [&lt;a href=&#34;https://raw.githubusercontent.com/phillipi/pix2pix/master/datasets/bibtex/facades.tex&#34;&gt;Citation&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cityscapes&lt;/code&gt;: 2975 images from the &lt;a href=&#34;https://www.cityscapes-dataset.com/&#34;&gt;Cityscapes training set&lt;/a&gt;. [&lt;a href=&#34;https://raw.githubusercontent.com/phillipi/pix2pix/master/datasets/bibtex/cityscapes.tex&#34;&gt;Citation&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;maps&lt;/code&gt;: 1096 training images scraped from Google Maps&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;edges2shoes&lt;/code&gt;: 50k training images from &lt;a href=&#34;http://vision.cs.utexas.edu/projects/finegrained/utzap50k/&#34;&gt;UT Zappos50K dataset&lt;/a&gt;. Edges are computed by &lt;a href=&#34;https://github.com/s9xie/hed&#34;&gt;HED&lt;/a&gt; edge detector + post-processing. [&lt;a href=&#34;https://raw.githubusercontent.com/phillipi/pix2pix/master/datasets/bibtex/shoes.tex&#34;&gt;Citation&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;edges2handbags&lt;/code&gt;: 137K Amazon Handbag images from &lt;a href=&#34;https://github.com/junyanz/iGAN&#34;&gt;iGAN project&lt;/a&gt;. Edges are computed by &lt;a href=&#34;https://github.com/s9xie/hed&#34;&gt;HED&lt;/a&gt; edge detector + post-processing. [&lt;a href=&#34;https://raw.githubusercontent.com/phillipi/pix2pix/master/datasets/bibtex/handbags.tex&#34;&gt;Citation&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;night2day&lt;/code&gt;: around 20K natural scene images from &lt;a href=&#34;http://transattr.cs.brown.edu/&#34;&gt;Transient Attributes dataset&lt;/a&gt; [&lt;a href=&#34;https://raw.githubusercontent.com/phillipi/pix2pix/master/datasets/bibtex/transattr.tex&#34;&gt;Citation&lt;/a&gt;]. To train a &lt;code&gt;day2night&lt;/code&gt; pix2pix model, you need to add &lt;code&gt;which_direction=BtoA&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;Download the pre-trained models with the following script. You need to rename the model (e.g., &lt;code&gt;facades_label2image&lt;/code&gt; to &lt;code&gt;/checkpoints/facades/latest_net_G.t7&lt;/code&gt;) after the download has finished.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./models/download_model.sh model_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;facades_label2image&lt;/code&gt; (label -&amp;gt; facade): trained on the CMP Facades dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cityscapes_label2image&lt;/code&gt; (label -&amp;gt; street scene): trained on the Cityscapes dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cityscapes_image2label&lt;/code&gt; (street scene -&amp;gt; label): trained on the Cityscapes dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;edges2shoes&lt;/code&gt; (edge -&amp;gt; photo): trained on UT Zappos50K dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;edges2handbags&lt;/code&gt; (edge -&amp;gt; photo): trained on Amazon handbags images.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;day2night&lt;/code&gt; (daytime scene -&amp;gt; nighttime scene): trained on around 100 &lt;a href=&#34;http://transattr.cs.brown.edu/&#34;&gt;webcams&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup Training and Test data&lt;/h2&gt; &#xA;&lt;h3&gt;Generating Pairs&lt;/h3&gt; &#xA;&lt;p&gt;We provide a python script to generate training data in the form of pairs of images {A,B}, where A and B are two different depictions of the same underlying scene. For example, these might be pairs {label map, photo} or {bw image, color image}. Then we can learn to translate A to B or B to A:&lt;/p&gt; &#xA;&lt;p&gt;Create folder &lt;code&gt;/path/to/data&lt;/code&gt; with subfolders &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;. &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; should each have their own subfolders &lt;code&gt;train&lt;/code&gt;, &lt;code&gt;val&lt;/code&gt;, &lt;code&gt;test&lt;/code&gt;, etc. In &lt;code&gt;/path/to/data/A/train&lt;/code&gt;, put training images in style A. In &lt;code&gt;/path/to/data/B/train&lt;/code&gt;, put the corresponding images in style B. Repeat same for other data splits (&lt;code&gt;val&lt;/code&gt;, &lt;code&gt;test&lt;/code&gt;, etc).&lt;/p&gt; &#xA;&lt;p&gt;Corresponding images in a pair {A,B} must be the same size and have the same filename, e.g., &lt;code&gt;/path/to/data/A/train/1.jpg&lt;/code&gt; is considered to correspond to &lt;code&gt;/path/to/data/B/train/1.jpg&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once the data is formatted this way, call:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/combine_A_and_B.py --fold_A /path/to/data/A --fold_B /path/to/data/B --fold_AB /path/to/data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will combine each pair of images (A,B) into a single image file, ready for training.&lt;/p&gt; &#xA;&lt;h3&gt;Notes on Colorization&lt;/h3&gt; &#xA;&lt;p&gt;No need to run &lt;code&gt;combine_A_and_B.py&lt;/code&gt; for colorization. Instead, you need to prepare some natural images and set &lt;code&gt;preprocess=colorization&lt;/code&gt; in the script. The program will automatically convert each RGB image into Lab color space, and create &lt;code&gt;L -&amp;gt; ab&lt;/code&gt; image pair during the training. Also set &lt;code&gt;input_nc=1&lt;/code&gt; and &lt;code&gt;output_nc=2&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Extracting Edges&lt;/h3&gt; &#xA;&lt;p&gt;We provide python and Matlab scripts to extract coarse edges from photos. Run &lt;code&gt;scripts/edges/batch_hed.py&lt;/code&gt; to compute &lt;a href=&#34;https://github.com/s9xie/hed&#34;&gt;HED&lt;/a&gt; edges. Run &lt;code&gt;scripts/edges/PostprocessHED.m&lt;/code&gt; to simplify edges with additional post-processing steps. Check the code documentation for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluating Labels2Photos on Cityscapes&lt;/h3&gt; &#xA;&lt;p&gt;We provide scripts for running the evaluation of the Labels2Photos task on the Cityscapes &lt;strong&gt;validation&lt;/strong&gt; set. We assume that you have installed &lt;code&gt;caffe&lt;/code&gt; (and &lt;code&gt;pycaffe&lt;/code&gt;) in your system. If not, see the &lt;a href=&#34;http://caffe.berkeleyvision.org/installation.html&#34;&gt;official website&lt;/a&gt; for installation instructions. Once &lt;code&gt;caffe&lt;/code&gt; is successfully installed, download the pre-trained FCN-8s semantic segmentation model (512MB) by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./scripts/eval_cityscapes/download_fcn8s.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then make sure &lt;code&gt;./scripts/eval_cityscapes/&lt;/code&gt; is in your system&#39;s python path. If not, run the following command to add it&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PYTHONPATH=${PYTHONPATH}:./scripts/eval_cityscapes/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can run the following command to evaluate your predictions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./scripts/eval_cityscapes/evaluate.py --cityscapes_dir /path/to/original/cityscapes/dataset/ --result_dir /path/to/your/predictions/ --output_dir /path/to/output/directory/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Images stored under &lt;code&gt;--result_dir&lt;/code&gt; should contain your model predictions on the Cityscapes &lt;strong&gt;validation&lt;/strong&gt; split, and have the original Cityscapes naming convention (e.g., &lt;code&gt;frankfurt_000001_038418_leftImg8bit.png&lt;/code&gt;). The script will output a text file under &lt;code&gt;--output_dir&lt;/code&gt; containing the metric.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Further notes&lt;/strong&gt;: Our pre-trained FCN model is &lt;strong&gt;not&lt;/strong&gt; supposed to work on Cityscapes in the original resolution (1024x2048) as it was trained on 256x256 images that are then upsampled to 1024x2048 during training. The purpose of the resizing during training was to 1) keep the label maps in the original high resolution untouched and 2) avoid the need to change the standard FCN training code and the architecture for Cityscapes. During test time, you need to synthesize 256x256 results. Our test code will automatically upsample your results to 1024x2048 before feeding them to the pre-trained FCN model. The output is at 1024x2048 resolution and will be compared to 1024x2048 ground truth labels. You do not need to resize the ground truth labels. The best way to verify whether everything is correct is to reproduce the numbers for real images in the paper first. To achieve it, you need to resize the original/real Cityscapes images (&lt;strong&gt;not&lt;/strong&gt; labels) to 256x256 and feed them to the evaluation code.&lt;/p&gt; &#xA;&lt;h2&gt;Display UI&lt;/h2&gt; &#xA;&lt;p&gt;Optionally, for displaying images during training and test, use the &lt;a href=&#34;https://github.com/szym/display&#34;&gt;display package&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install it with: &lt;code&gt;luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Then start the server with: &lt;code&gt;th -ldisplay.start&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open this URL in your browser: &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By default, the server listens on localhost. Pass &lt;code&gt;0.0.0.0&lt;/code&gt; to allow external connections on any interface:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;th -ldisplay.start 8000 0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then open &lt;code&gt;http://(hostname):(port)/&lt;/code&gt; in your browser to load the remote desktop.&lt;/p&gt; &#xA;&lt;p&gt;L1 error is plotted to the display by default. Set the environment variable &lt;code&gt;display_plot&lt;/code&gt; to a comma-separated list of values &lt;code&gt;errL1&lt;/code&gt;, &lt;code&gt;errG&lt;/code&gt; and &lt;code&gt;errD&lt;/code&gt; to visualize the L1, generator, and discriminator error respectively. For example, to plot only the generator and discriminator errors to the display instead of the default L1 error, set &lt;code&gt;display_plot=&#34;errG,errD&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code for your research, please cite our paper &lt;a href=&#34;https://arxiv.org/pdf/1611.07004v1.pdf&#34;&gt;Image-to-Image Translation Using Conditional Adversarial Networks&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{pix2pix2017,&#xA;  title={Image-to-Image Translation with Conditional Adversarial Networks},&#xA;  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},&#xA;  journal={CVPR},&#xA;  year={2017}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cat Paper Collection&lt;/h2&gt; &#xA;&lt;p&gt;If you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper Collection:&lt;br&gt; &lt;a href=&#34;https://github.com/junyanz/CatPapers&#34;&gt;[Github]&lt;/a&gt; &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/cat/cat_papers.html&#34;&gt;[Webpage]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Code borrows heavily from &lt;a href=&#34;https://github.com/soumith/dcgan.torch&#34;&gt;DCGAN&lt;/a&gt;. The data loader is modified from &lt;a href=&#34;https://github.com/soumith/dcgan.torch&#34;&gt;DCGAN&lt;/a&gt; and &lt;a href=&#34;https://github.com/pathak22/context-encoder&#34;&gt;Context-Encoder&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>