<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-01T02:00:10Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>CompVis/stable-diffusion</title>
    <updated>2022-10-01T02:00:10Z</updated>
    <id>tag:github.com,2022-10-01:/CompVis/stable-diffusion</id>
    <link href="https://github.com/CompVis/stable-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A latent text-to-image diffusion model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Stable Diffusion was made possible thanks to a collaboration with &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt; and &lt;a href=&#34;https://runwayml.com/&#34;&gt;Runway&lt;/a&gt; and builds upon our previous work:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/rromb&#34;&gt;Robin Rombach&lt;/a&gt;*, &lt;a href=&#34;https://github.com/ablattmann&#34;&gt;Andreas Blattmann&lt;/a&gt;*, &lt;a href=&#34;https://github.com/qp-qp&#34;&gt;Dominik Lorenz&lt;/a&gt;, &lt;a href=&#34;https://github.com/pesser&#34;&gt;Patrick Esser&lt;/a&gt;, &lt;a href=&#34;https://hci.iwr.uni-heidelberg.de/Staff/bommer&#34;&gt;Bj√∂rn Ommer&lt;/a&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html&#34;&gt;CVPR &#39;22 Oral&lt;/a&gt; | &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;Project page&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/txt2img/merged-0006.png&#34; alt=&#34;txt2img-stable2&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#stable-diffusion-v1&#34;&gt;Stable Diffusion&lt;/a&gt; is a latent text-to-image diffusion model. Thanks to a generous compute donation from &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt; and support from &lt;a href=&#34;https://laion.ai/&#34;&gt;LAION&lt;/a&gt;, we were able to train a Latent Diffusion Model on 512x512 images from a subset of the &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-5B&lt;/a&gt; database. Similar to Google&#39;s &lt;a href=&#34;https://arxiv.org/abs/2205.11487&#34;&gt;Imagen&lt;/a&gt;, this model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. See &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#stable-diffusion-v1&#34;&gt;this section&lt;/a&gt; below and the &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion&#34;&gt;model card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;A suitable &lt;a href=&#34;https://conda.io/&#34;&gt;conda&lt;/a&gt; environment named &lt;code&gt;ldm&lt;/code&gt; can be created and activated with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also update an existing &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;latent diffusion&lt;/a&gt; environment by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install pytorch torchvision -c pytorch&#xA;pip install transformers==4.19.2 diffusers invisible-watermark&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Stable Diffusion v1&lt;/h2&gt; &#xA;&lt;p&gt;Stable Diffusion v1 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 860M UNet and CLIP ViT-L/14 text encoder for the diffusion model. The model was pretrained on 256x256 images and then finetuned on 512x512 images.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Stable Diffusion v1 is a general text-to-image diffusion model and therefore mirrors biases and (mis-)conceptions that are present in its training data. Details on the training procedure and data, as well as the intended use of the model can be found in the corresponding &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/Stable_Diffusion_v1_Model_Card.md&#34;&gt;model card&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;The weights are available via &lt;a href=&#34;https://huggingface.co/CompVis&#34;&gt;the CompVis organization at Hugging Face&lt;/a&gt; under &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE&#34;&gt;a license which contains specific use-based restrictions to prevent misuse and harm as informed by the model card, but otherwise remains permissive&lt;/a&gt;. While commercial use is permitted under the terms of the license, &lt;strong&gt;we do not recommend using the provided weights for services or products without additional safety mechanisms and considerations&lt;/strong&gt;, since there are &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/Stable_Diffusion_v1_Model_Card.md#limitations-and-bias&#34;&gt;known limitations and biases&lt;/a&gt; of the weights, and research on safe and ethical deployment of general text-to-image models is an ongoing effort. &lt;strong&gt;The weights are research artifacts and should be treated as such.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE&#34;&gt;The CreativeML OpenRAIL M license&lt;/a&gt; is an &lt;a href=&#34;https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses&#34;&gt;Open RAIL M license&lt;/a&gt;, adapted from the work that &lt;a href=&#34;https://bigscience.huggingface.co/&#34;&gt;BigScience&lt;/a&gt; and &lt;a href=&#34;https://www.licenses.ai/&#34;&gt;the RAIL Initiative&lt;/a&gt; are jointly carrying in the area of responsible AI licensing. See also &lt;a href=&#34;https://bigscience.huggingface.co/blog/the-bigscience-rail-license&#34;&gt;the article about the BLOOM Open RAIL license&lt;/a&gt; on which our license is based.&lt;/p&gt; &#xA;&lt;h3&gt;Weights&lt;/h3&gt; &#xA;&lt;p&gt;We currently provide the following checkpoints:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sd-v1-1.ckpt&lt;/code&gt;: 237k steps at resolution &lt;code&gt;256x256&lt;/code&gt; on &lt;a href=&#34;https://huggingface.co/datasets/laion/laion2B-en&#34;&gt;laion2B-en&lt;/a&gt;. 194k steps at resolution &lt;code&gt;512x512&lt;/code&gt; on &lt;a href=&#34;https://huggingface.co/datasets/laion/laion-high-resolution&#34;&gt;laion-high-resolution&lt;/a&gt; (170M examples from LAION-5B with resolution &lt;code&gt;&amp;gt;= 1024x1024&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sd-v1-2.ckpt&lt;/code&gt;: Resumed from &lt;code&gt;sd-v1-1.ckpt&lt;/code&gt;. 515k steps at resolution &lt;code&gt;512x512&lt;/code&gt; on &lt;a href=&#34;https://laion.ai/blog/laion-aesthetics/&#34;&gt;laion-aesthetics v2 5+&lt;/a&gt; (a subset of laion2B-en with estimated aesthetics score &lt;code&gt;&amp;gt; 5.0&lt;/code&gt;, and additionally filtered to images with an original size &lt;code&gt;&amp;gt;= 512x512&lt;/code&gt;, and an estimated watermark probability &lt;code&gt;&amp;lt; 0.5&lt;/code&gt;. The watermark estimate is from the &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-5B&lt;/a&gt; metadata, the aesthetics score is estimated using the &lt;a href=&#34;https://github.com/christophschuhmann/improved-aesthetic-predictor&#34;&gt;LAION-Aesthetics Predictor V2&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sd-v1-3.ckpt&lt;/code&gt;: Resumed from &lt;code&gt;sd-v1-2.ckpt&lt;/code&gt;. 195k steps at resolution &lt;code&gt;512x512&lt;/code&gt; on &#34;laion-aesthetics v2 5+&#34; and 10% dropping of the text-conditioning to improve &lt;a href=&#34;https://arxiv.org/abs/2207.12598&#34;&gt;classifier-free guidance sampling&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sd-v1-4.ckpt&lt;/code&gt;: Resumed from &lt;code&gt;sd-v1-2.ckpt&lt;/code&gt;. 225k steps at resolution &lt;code&gt;512x512&lt;/code&gt; on &#34;laion-aesthetics v2 5+&#34; and 10% dropping of the text-conditioning to improve &lt;a href=&#34;https://arxiv.org/abs/2207.12598&#34;&gt;classifier-free guidance sampling&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling steps show the relative improvements of the checkpoints: &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/v1-variants-scores.jpg&#34; alt=&#34;sd evaluation results&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Text-to-Image with Stable Diffusion&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/txt2img/merged-0005.png&#34; alt=&#34;txt2img-stable2&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/txt2img/merged-0007.png&#34; alt=&#34;txt2img-stable2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Stable Diffusion is a latent diffusion model conditioned on the (non-pooled) text embeddings of a CLIP ViT-L/14 text encoder. We provide a &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#reference-sampling-script&#34;&gt;reference script for sampling&lt;/a&gt;, but there also exists a &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#diffusers-integration&#34;&gt;diffusers integration&lt;/a&gt;, which we expect to see more active community development.&lt;/p&gt; &#xA;&lt;h4&gt;Reference Sampling Script&lt;/h4&gt; &#xA;&lt;p&gt;We provide a reference sampling script, which incorporates&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a &lt;a href=&#34;https://github.com/CompVis/stable-diffusion/pull/36&#34;&gt;Safety Checker Module&lt;/a&gt;, to reduce the probability of explicit outputs,&lt;/li&gt; &#xA; &lt;li&gt;an &lt;a href=&#34;https://github.com/ShieldMnt/invisible-watermark&#34;&gt;invisible watermarking&lt;/a&gt; of the outputs, to help viewers &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/scripts/tests/test_watermark.py&#34;&gt;identify the images as machine-generated&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/#weights&#34;&gt;obtaining the &lt;code&gt;stable-diffusion-v1-*-original&lt;/code&gt; weights&lt;/a&gt;, link them&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p models/ldm/stable-diffusion-v1/&#xA;ln -s &amp;lt;path/to/model.ckpt&amp;gt; models/ldm/stable-diffusion-v1/model.ckpt &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and sample with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a photograph of an astronaut riding a horse&#34; --plms &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, this uses a guidance scale of &lt;code&gt;--scale 7.5&lt;/code&gt;, &lt;a href=&#34;https://github.com/CompVis/latent-diffusion/pull/51&#34;&gt;Katherine Crowson&#39;s implementation&lt;/a&gt; of the &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;PLMS&lt;/a&gt; sampler, and renders images of size 512x512 (which it was trained on) in 50 steps. All supported arguments are listed below (type &lt;code&gt;python scripts/txt2img.py --help&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;usage: txt2img.py [-h] [--prompt [PROMPT]] [--outdir [OUTDIR]] [--skip_grid] [--skip_save] [--ddim_steps DDIM_STEPS] [--plms] [--laion400m] [--fixed_code] [--ddim_eta DDIM_ETA]&#xA;                  [--n_iter N_ITER] [--H H] [--W W] [--C C] [--f F] [--n_samples N_SAMPLES] [--n_rows N_ROWS] [--scale SCALE] [--from-file FROM_FILE] [--config CONFIG] [--ckpt CKPT]&#xA;                  [--seed SEED] [--precision {full,autocast}]&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --prompt [PROMPT]     the prompt to render&#xA;  --outdir [OUTDIR]     dir to write results to&#xA;  --skip_grid           do not save a grid, only individual samples. Helpful when evaluating lots of samples&#xA;  --skip_save           do not save individual samples. For speed measurements.&#xA;  --ddim_steps DDIM_STEPS&#xA;                        number of ddim sampling steps&#xA;  --plms                use plms sampling&#xA;  --laion400m           uses the LAION400M model&#xA;  --fixed_code          if enabled, uses the same starting code across samples&#xA;  --ddim_eta DDIM_ETA   ddim eta (eta=0.0 corresponds to deterministic sampling&#xA;  --n_iter N_ITER       sample this often&#xA;  --H H                 image height, in pixel space&#xA;  --W W                 image width, in pixel space&#xA;  --C C                 latent channels&#xA;  --f F                 downsampling factor&#xA;  --n_samples N_SAMPLES&#xA;                        how many samples to produce for each given prompt. A.k.a. batch size&#xA;  --n_rows N_ROWS       rows in the grid (default: n_samples)&#xA;  --scale SCALE         unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))&#xA;  --from-file FROM_FILE&#xA;                        if specified, load prompts from this file&#xA;  --config CONFIG       path to config which constructs model&#xA;  --ckpt CKPT           path to checkpoint of model&#xA;  --seed SEED           the seed (for reproducible sampling)&#xA;  --precision {full,autocast}&#xA;                        evaluate at this precision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: The inference config for all v1 versions is designed to be used with EMA-only checkpoints. For this reason &lt;code&gt;use_ema=False&lt;/code&gt; is set in the configuration, otherwise the code will try to switch from non-EMA to EMA weights. If you want to examine the effect of EMA vs no EMA, we provide &#34;full&#34; checkpoints which contain both types of weights. For these, &lt;code&gt;use_ema=False&lt;/code&gt; will load and use the non-EMA weights.&lt;/p&gt; &#xA;&lt;h4&gt;Diffusers Integration&lt;/h4&gt; &#xA;&lt;p&gt;A simple way to download and sample Stable Diffusion is by using the &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main#new--stable-diffusion-is-now-fully-compatible-with-diffusers&#34;&gt;diffusers library&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;# make sure you&#39;re logged in with `huggingface-cli login`&#xA;from torch import autocast&#xA;from diffusers import StableDiffusionPipeline&#xA;&#xA;pipe = StableDiffusionPipeline.from_pretrained(&#xA;&#x9;&#34;CompVis/stable-diffusion-v1-4&#34;, &#xA;&#x9;use_auth_token=True&#xA;).to(&#34;cuda&#34;)&#xA;&#xA;prompt = &#34;a photo of an astronaut riding a horse on mars&#34;&#xA;with autocast(&#34;cuda&#34;):&#xA;    image = pipe(prompt)[&#34;sample&#34;][0]  &#xA;    &#xA;image.save(&#34;astronaut_rides_horse.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Image Modification with Stable Diffusion&lt;/h3&gt; &#xA;&lt;p&gt;By using a diffusion-denoising mechanism as first proposed by &lt;a href=&#34;https://arxiv.org/abs/2108.01073&#34;&gt;SDEdit&lt;/a&gt;, the model can be used for different tasks such as text-guided image-to-image translation and upscaling. Similar to the txt2img sampling script, we provide a script to perform image modification with Stable Diffusion.&lt;/p&gt; &#xA;&lt;p&gt;The following describes an example where a rough sketch made in &lt;a href=&#34;https://www.pinta-project.com/&#34;&gt;Pinta&lt;/a&gt; is converted into a detailed artwork.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/img2img.py --prompt &#34;A fantasy landscape, trending on artstation&#34; --init-img &amp;lt;path-to-img.jpg&amp;gt; --strength 0.8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, strength is a value between 0.0 and 1.0, that controls the amount of noise that is added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input. See the following example.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg&#34; alt=&#34;sketch-in&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Outputs&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/mountains-3.png&#34; alt=&#34;out3&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/mountains-2.png&#34; alt=&#34;out2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This procedure can, for example, also be used to upscale samples from the base model.&lt;/p&gt; &#xA;&lt;h2&gt;Comments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Our codebase for the diffusion models builds heavily on &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;OpenAI&#39;s ADM codebase&lt;/a&gt; and &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;https://github.com/lucidrains/denoising-diffusion-pytorch&lt;/a&gt;. Thanks for open-sourcing!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The implementation of the transformer encoder is from &lt;a href=&#34;https://github.com/lucidrains/x-transformers&#34;&gt;x-transformers&lt;/a&gt; by &lt;a href=&#34;https://github.com/lucidrains?tab=repositories&#34;&gt;lucidrains&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;      title={High-Resolution Image Synthesis with Latent Diffusion Models}, &#xA;      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Bj√∂rn Ommer},&#xA;      year={2021},&#xA;      eprint={2112.10752},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>mrdbourke/pytorch-deep-learning</title>
    <updated>2022-10-01T02:00:10Z</updated>
    <id>tag:github.com,2022-10-01:/mrdbourke/pytorch-deep-learning</id>
    <link href="https://github.com/mrdbourke/pytorch-deep-learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Materials for the Learn PyTorch for Deep Learning: Zero to Mastery course.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Learn PyTorch for Deep Learning (work in progress)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Update August 31 2022:&lt;/strong&gt; The &lt;a href=&#34;https://dbourke.link/ZTMPyTorch&#34;&gt;course is out on the Zero to Mastery Academy&lt;/a&gt; with videos for sections 00-07, 08 &amp;amp; 09 will come soon.&lt;/p&gt; &#xA;&lt;p&gt;Welcome to the &lt;a href=&#34;https://dbourke.link/ZTMPyTorch&#34;&gt;Zero to Mastery Learn PyTorch for Deep Learning course&lt;/a&gt;, the second best place to learn PyTorch on the internet (the first being the &lt;a href=&#34;https://pytorch.org/docs/stable/index.html&#34;&gt;PyTorch documentation&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://learnpytorch.io&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/misc-pytorch-course-launch-cover-white-text-black-background.jpg&#34; width=&#34;750&#34; alt=&#34;pytorch deep learning by zero to mastery cover photo with different sections of the course&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Contents of this page&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#course-materialsoutline&#34;&gt;Course materials/outline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#about-this-course&#34;&gt;About this course&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#status&#34;&gt;Status&lt;/a&gt; (the progress of the course creation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#log&#34;&gt;Log&lt;/a&gt; (a log of the course material creation process)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Course materials/outline&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìñ &lt;strong&gt;Online book version:&lt;/strong&gt; All of course materials are available in a readable online book at &lt;a href=&#34;https://learnpytorch.io&#34;&gt;learnpytorch.io&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üé• &lt;strong&gt;First five sections on YouTube:&lt;/strong&gt; Learn Pytorch in a day by watching the &lt;a href=&#34;https://youtu.be/Z_ikDlimN6A&#34;&gt;first 25-hours of material&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üî¨ &lt;strong&gt;Course focus:&lt;/strong&gt; code, code, code, experiment, experiment, experiment.&lt;/li&gt; &#xA; &lt;li&gt;üèÉ‚Äç‚ôÇÔ∏è &lt;strong&gt;Teaching style:&lt;/strong&gt; &lt;a href=&#34;https://sive.rs/kimo&#34;&gt;https://sive.rs/kimo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ü§î &lt;strong&gt;Ask a question:&lt;/strong&gt; See the &lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/discussions&#34;&gt;GitHub Discussions page&lt;/a&gt; for existing questions/ask your own.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Section&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;What does it cover?&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Exercises &amp;amp; Extra-curriculum&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Slides&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/00_pytorch_fundamentals/&#34;&gt;00 - PyTorch Fundamentals&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Many fundamental PyTorch operations used for deep learning and neural networks.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/00_pytorch_fundamentals/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/00_pytorch_and_deep_learning_fundamentals.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/01_pytorch_workflow/&#34;&gt;01 - PyTorch Workflow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Provides an outline for approaching deep learning problems and building neural networks with PyTorch.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/01_pytorch_workflow/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/01_pytorch_workflow.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/02_pytorch_classification/&#34;&gt;02 - PyTorch Neural Network Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Uses the PyTorch workflow from 01 to go through a neural network classification problem.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/02_pytorch_classification/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/02_pytorch_classification.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/03_pytorch_computer_vision/&#34;&gt;03 - PyTorch Computer Vision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Let&#39;s see how PyTorch can be used for computer vision problems using the same workflow from 01 &amp;amp; 02.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/03_pytorch_computer_vision/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/03_pytorch_computer_vision.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/04_pytorch_custom_datasets/&#34;&gt;04 - PyTorch Custom Datasets&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;How do you load a custom dataset into PyTorch? Also we&#39;ll be laying the foundations in this notebook for our modular code (covered in 05).&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/04_pytorch_custom_datasets/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/04_pytorch_custom_datasets.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/05_pytorch_going_modular/&#34;&gt;05 - PyTorch Going Modular&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch is designed to be modular, let&#39;s turn what we&#39;ve created into a series of Python scripts (this is how you&#39;ll often find PyTorch code in the wild).&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/05_pytorch_going_modular/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/05_pytorch_going_modular.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/06_pytorch_transfer_learning/&#34;&gt;06 - PyTorch Transfer Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Let&#39;s take a well performing pre-trained model and adjust it to one of our own problems.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/06_pytorch_transfer_learning/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/06_pytorch_transfer_learning.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/07_pytorch_experiment_tracking/&#34;&gt;07 - Milestone Project 1: PyTorch Experiment Tracking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;We&#39;ve built a bunch of models... wouldn&#39;t it be good to track how they&#39;re all going?&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/07_pytorch_experiment_tracking/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/07_pytorch_experiment_tracking.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/08_pytorch_paper_replicating/&#34;&gt;08 - Milestone Project 2: PyTorch Paper Replicating&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch is the most popular deep learning framework for machine learning research, let&#39;s see why by replicating a machine learning paper.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/08_pytorch_paper_replicating/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/08_pytorch_paper_replicating.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/09_pytorch_model_deployment/&#34;&gt;09 - Milestone Project 3: Model Deployment&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;So we&#39;ve built a working PyTorch model... how do we get it in the hands of others? Hint: deploy it to the internet.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/09_pytorch_model_deployment/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/09_pytorch_model_deployment.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/pytorch_extra_resources/&#34;&gt;PyTorch Extra Resources&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This course covers a large amount of PyTorch and deep learning but the field of machine learning is vast, inside here you&#39;ll find recommended books and resources for: PyTorch and deep learning, ML engineering, NLP (natural language processing), time series data, where to find datasets and more.&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;See the project page for work-in-progress board - &lt;a href=&#34;https://github.com/users/mrdbourke/projects/1&#34;&gt;https://github.com/users/mrdbourke/projects/1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Working on:&lt;/strong&gt; finished videos for 09, editing videos for 08 and 09 ready for upload&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Total video count:&lt;/strong&gt; 321&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done skeleton code for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done annotations (text) for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done images for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done keynotes for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done exercises and solutions for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#log&#34;&gt;log&lt;/a&gt; for almost daily updates.&lt;/p&gt; &#xA;&lt;h2&gt;About this course&lt;/h2&gt; &#xA;&lt;h3&gt;Who is this course for?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;You:&lt;/strong&gt; Are a beginner in the field of machine learning or deep learning and would like to learn PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This course:&lt;/strong&gt; Teaches you PyTorch and many machine learning concepts in a hands-on, code-first way.&lt;/p&gt; &#xA;&lt;p&gt;If you already have 1-year+ experience in machine learning, this course may help but it is specifically designed to be beginner-friendly.&lt;/p&gt; &#xA;&lt;h3&gt;What are the prerequisites?&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;3-6 months coding Python.&lt;/li&gt; &#xA; &lt;li&gt;At least one beginner machine learning course (however this might be able to be skipped, resources are linked for many different topics).&lt;/li&gt; &#xA; &lt;li&gt;Experience using Jupyter Notebooks or Google Colab (though you can pick this up as we go along).&lt;/li&gt; &#xA; &lt;li&gt;A willingness to learn (most important).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For 1 &amp;amp; 2, I&#39;d recommend the &lt;a href=&#34;https://dbourke.link/ZTMMLcourse&#34;&gt;Zero to Mastery Data Science and Machine Learning Bootcamp&lt;/a&gt;, it&#39;ll teach you the fundamentals of machine learning and Python (I&#39;m biased though, I also teach that course).&lt;/p&gt; &#xA;&lt;h3&gt;How is the course taught?&lt;/h3&gt; &#xA;&lt;p&gt;All of the course materials are available for free in an online book at &lt;a href=&#34;https://learnpytorch.io&#34;&gt;learnpytorch.io&lt;/a&gt;. If you like to read, I&#39;d recommend going through the resources there.&lt;/p&gt; &#xA;&lt;p&gt;If you prefer to learn via video, the course is also taught in apprenticeship-style format, meaning I write PyTorch code, you write PyTorch code.&lt;/p&gt; &#xA;&lt;p&gt;There&#39;s a reason the course motto&#39;s include &lt;em&gt;if in doubt, run the code&lt;/em&gt; and &lt;em&gt;experiment, experiment, experiment!&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;My whole goal is to help you to do one thing: learn machine learning by writing PyTorch code.&lt;/p&gt; &#xA;&lt;p&gt;The code is all written via &lt;a href=&#34;https://colab.research.google.com&#34;&gt;Google Colab Notebooks&lt;/a&gt; (you could also use Jupyter Notebooks), an incredible free resource to experiment with machine learning.&lt;/p&gt; &#xA;&lt;h3&gt;What will I get if I finish the course?&lt;/h3&gt; &#xA;&lt;p&gt;There&#39;s certificates and all that jazz if you go through the videos.&lt;/p&gt; &#xA;&lt;p&gt;But certificates are meh.&lt;/p&gt; &#xA;&lt;p&gt;You can consider this course a machine learning momentum builder.&lt;/p&gt; &#xA;&lt;p&gt;By the end, you&#39;ll have written hundreds of lines of PyTorch code.&lt;/p&gt; &#xA;&lt;p&gt;And will have been exposed to many of the most important concepts in machine learning.&lt;/p&gt; &#xA;&lt;p&gt;So when you go to build your own machine learning projects or inspect a public machine learning project made with PyTorch, it&#39;ll feel familiar and if it doesn&#39;t, at least you&#39;ll know where to look.&lt;/p&gt; &#xA;&lt;h3&gt;What will I build in the course?&lt;/h3&gt; &#xA;&lt;p&gt;We start with the barebone fundamentals of PyTorch and machine learning, so even if you&#39;re new to machine learning you&#39;ll be caught up to speed.&lt;/p&gt; &#xA;&lt;p&gt;Then we‚Äôll explore more advanced areas including PyTorch neural network classification, PyTorch workflows, computer vision, custom datasets, experiment tracking, model deployment, and my personal favourite: transfer learning, a powerful technique for taking what one machine learning model has learned on another problem and applying it to your own!&lt;/p&gt; &#xA;&lt;p&gt;Along the way, you‚Äôll build three milestone projects surrounding an overarching project called FoodVision, a neural network computer vision model to classify images of food.&lt;/p&gt; &#xA;&lt;p&gt;These milestone projects will help you practice using PyTorch to cover important machine learning concepts and create a portfolio you can show employers and say &#34;here&#39;s what I&#39;ve done&#34;.&lt;/p&gt; &#xA;&lt;h3&gt;How do I get started?&lt;/h3&gt; &#xA;&lt;p&gt;You can read the materials on any device but this course is best viewed and coded along within a desktop browser.&lt;/p&gt; &#xA;&lt;p&gt;The course uses a free tool called Google Colab. If you&#39;ve got no experience with it, I&#39;d go through the free &lt;a href=&#34;https://colab.research.google.com/notebooks/basic_features_overview.ipynb&#34;&gt;Introduction to Google Colab tutorial&lt;/a&gt; and then come back here.&lt;/p&gt; &#xA;&lt;p&gt;To start:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Click on one of the notebook or section links above like &#34;&lt;a href=&#34;https://www.learnpytorch.io/00_pytorch_fundamentals/&#34;&gt;00. PyTorch Fundamentals&lt;/a&gt;&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Click the &#34;Open in Colab&#34; button up the top.&lt;/li&gt; &#xA; &lt;li&gt;Press SHIFT+Enter a few times and see what happens.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;My question isn&#39;t answered&lt;/h3&gt; &#xA;&lt;p&gt;Please leave a &lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/discussions&#34;&gt;discussion&lt;/a&gt; or send me an email directly: daniel (at) mrdbourke (dot) com.&lt;/p&gt; &#xA;&lt;h2&gt;Log&lt;/h2&gt; &#xA;&lt;p&gt;Almost daily updates of what&#39;s happening.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;30 Aug 2022 - recorded 15 videos for 09, total videos: 321, finished section 09 videos!!!! ... even bigger than 08!!&lt;/li&gt; &#xA; &lt;li&gt;29 Aug 2022 - recorded 16 videos for 09, total videos: 306&lt;/li&gt; &#xA; &lt;li&gt;28 Aug 2022 - recorded 11 videos for 09, total videos: 290&lt;/li&gt; &#xA; &lt;li&gt;27 Aug 2022 - recorded 16 videos for 09, total videos: 279&lt;/li&gt; &#xA; &lt;li&gt;26 Aug 2022 - add finishing touchs to notebook 09, add slides for 09, create solutions and exercises for 09&lt;/li&gt; &#xA; &lt;li&gt;25 Aug 2022 - add annotations and cleanup 09, remove TK&#39;s, cleanup images, make slides for 09&lt;/li&gt; &#xA; &lt;li&gt;24 Aug 2022 - add annotations to 09, main takeaways, exercises and extra-curriculum done&lt;/li&gt; &#xA; &lt;li&gt;23 Aug 2022 - add annotations to 09, add plenty of images/slides&lt;/li&gt; &#xA; &lt;li&gt;22 Aug 2022 - add annotations to 09, start working on slides/images&lt;/li&gt; &#xA; &lt;li&gt;20 Aug 2022 - add annotations to 09&lt;/li&gt; &#xA; &lt;li&gt;19 Aug 2022 - add annotations to 09, check out the awesome demos!&lt;/li&gt; &#xA; &lt;li&gt;18 Aug 2022 - add annotations to 09&lt;/li&gt; &#xA; &lt;li&gt;17 Aug 2022 - add annotations to 09&lt;/li&gt; &#xA; &lt;li&gt;16 Aug 2022 - add annotations to 09&lt;/li&gt; &#xA; &lt;li&gt;15 Aug 2022 - add annotations to 09&lt;/li&gt; &#xA; &lt;li&gt;13 Aug 2022 - add annotations to 09&lt;/li&gt; &#xA; &lt;li&gt;12 Aug 2022 - add demo files for notebook 09 to &lt;code&gt;demos/&lt;/code&gt;, start annotating notebook 09 with explainer text&lt;/li&gt; &#xA; &lt;li&gt;11 Aug 2022 - finish skeleton code for notebook 09, course finishes deploying 2x models, one for FoodVision Mini &amp;amp; one for (secret)&lt;/li&gt; &#xA; &lt;li&gt;10 Aug 2022 - add section for PyTorch Extra Resources (places to learn more about PyTorch/deep learning): &lt;a href=&#34;https://www.learnpytorch.io/pytorch_extra_resources/&#34;&gt;https://www.learnpytorch.io/pytorch_extra_resources/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;09 Aug 2022 - add more skeleton code to notebook 09&lt;/li&gt; &#xA; &lt;li&gt;08 Aug 2022 - create draft notebook for 09, end goal to deploy FoodVision Mini model and make it publically accessible&lt;/li&gt; &#xA; &lt;li&gt;05 Aug 2022 - recorded 11 videos for 08, total videos: 263, section 08 videos finished!... the biggest section so far&lt;/li&gt; &#xA; &lt;li&gt;04 Aug 2022 - recorded 13 videos for 08, total videos: 252&lt;/li&gt; &#xA; &lt;li&gt;03 Aug 2022 - recorded 3 videos for 08, total videos: 239&lt;/li&gt; &#xA; &lt;li&gt;02 Aug 2022 - recorded 12 videos for 08, total videos: 236&lt;/li&gt; &#xA; &lt;li&gt;30 July 2022 - recorded 11 videos for 08, total videos: 224&lt;/li&gt; &#xA; &lt;li&gt;29 July 2022 - add exercises + solutions for 08, see live walkthrough on YouTube: &lt;a href=&#34;https://youtu.be/tjpW_BY8y3g&#34;&gt;https://youtu.be/tjpW_BY8y3g&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;28 July 2022 - add slides for 08&lt;/li&gt; &#xA; &lt;li&gt;27 July 2022 - cleanup much of 08, start on slides for 08, exercises and extra-curriculum next&lt;/li&gt; &#xA; &lt;li&gt;26 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;25 July 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;24 July 2022 - launched first half of course (notebooks 00-04) in a single video (25+ hours!!!) on YouTube: &lt;a href=&#34;https://youtu.be/Z_ikDlimN6A&#34;&gt;https://youtu.be/Z_ikDlimN6A&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;21 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;20 July 2022 - add annotations and images for 08, getting so close! this is an epic section&lt;/li&gt; &#xA; &lt;li&gt;19 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;15 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;14 July 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;12 July 2022 - add annotations for 08, woo woo this is bigggg section!&lt;/li&gt; &#xA; &lt;li&gt;11 July 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;9 July 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;8 July 2022 - add a bunch of annotations to 08&lt;/li&gt; &#xA; &lt;li&gt;6 July 2022 - course launched on ZTM Academy with videos for sections 00-07! üöÄ - &lt;a href=&#34;https://dbourke.link/ZTMPyTorch&#34;&gt;https://dbourke.link/ZTMPyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;1 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;30 June 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;28 June 2022 - recorded 11 videos for section 07, total video count 213, all videos for section 07 complete!&lt;/li&gt; &#xA; &lt;li&gt;27 June 2022 - recorded 11 videos for section 07, total video count 202&lt;/li&gt; &#xA; &lt;li&gt;25 June 2022 - recreated 7 videos for section 06 to include updated APIs, total video count 191&lt;/li&gt; &#xA; &lt;li&gt;24 June 2022 - recreated 12 videos for section 06 to include updated APIs&lt;/li&gt; &#xA; &lt;li&gt;23 June 2022 - finish annotations for 07, add exercise template and solutions for 07 + video walkthrough on YouTube: &lt;a href=&#34;https://youtu.be/cO_r2FYcAjU&#34;&gt;https://youtu.be/cO_r2FYcAjU&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;21 June 2022 - make 08 runnable end-to-end, add images and annotations for 07&lt;/li&gt; &#xA; &lt;li&gt;17 June 2022 - fix up 06, 07 v2 for upcoming torchvision version upgrade, add plenty of annotations to 08&lt;/li&gt; &#xA; &lt;li&gt;13 June 2022 - add notebook 08 first version, starting to replicate the Vision Transformer paper&lt;/li&gt; &#xA; &lt;li&gt;10 June 2022 - add annotations for 07 v2&lt;/li&gt; &#xA; &lt;li&gt;09 June 2022 - create 07 v2 for &lt;code&gt;torchvision&lt;/code&gt; v0.13 (this will replace 07 v1 when &lt;code&gt;torchvision=0.13&lt;/code&gt; is released)&lt;/li&gt; &#xA; &lt;li&gt;08 June 2022 - adapt 06 v2 for &lt;code&gt;torchvision&lt;/code&gt; v0.13 (this will replace 06 v1 when &lt;code&gt;torchvision=0.13&lt;/code&gt; is released)&lt;/li&gt; &#xA; &lt;li&gt;07 June 2022 - create notebook 06 v2 for upcoming &lt;code&gt;torchvision&lt;/code&gt; v0.13 update (new transfer learning methods)&lt;/li&gt; &#xA; &lt;li&gt;04 June 2022 - add annotations for 07&lt;/li&gt; &#xA; &lt;li&gt;03 June 2022 - huuuuuuge amount of annotations added to 07&lt;/li&gt; &#xA; &lt;li&gt;31 May 2022 - add a bunch of annotations for 07, make code runnable end-to-end&lt;/li&gt; &#xA; &lt;li&gt;30 May 2022 - record 4 videos for 06, finished section 06, onto section 07, total videos 186&lt;/li&gt; &#xA; &lt;li&gt;28 May 2022 - record 10 videos for 06, total videos 182&lt;/li&gt; &#xA; &lt;li&gt;24 May 2022 - add solutions and exercises for 06&lt;/li&gt; &#xA; &lt;li&gt;23 May 2022 - finished annotations and images for 06, time to do exercises and solutions&lt;/li&gt; &#xA; &lt;li&gt;22 May 2202 - add plenty of images to 06&lt;/li&gt; &#xA; &lt;li&gt;18 May 2022 - add plenty of annotations to 06&lt;/li&gt; &#xA; &lt;li&gt;17 May 2022 - added a bunch of annotations for section 06&lt;/li&gt; &#xA; &lt;li&gt;16 May 2022 - recorded 10 videos for section 05, finish videos for section 05 ‚úÖ&lt;/li&gt; &#xA; &lt;li&gt;12 May 2022 - added exercises and solutions for 05&lt;/li&gt; &#xA; &lt;li&gt;11 May 2022 - clean up part 1 and part 2 notebooks for 05, make slides for 05, start on exercises and solutions for 05&lt;/li&gt; &#xA; &lt;li&gt;10 May 2022 - huuuuge updates to the 05 section, see the website, it looks pretty: &lt;a href=&#34;https://www.learnpytorch.io/05_pytorch_going_modular/&#34;&gt;https://www.learnpytorch.io/05_pytorch_going_modular/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;09 May 2022 - add a bunch of materials for 05, cleanup docs&lt;/li&gt; &#xA; &lt;li&gt;08 May 2022 - add a bunch of materials for 05&lt;/li&gt; &#xA; &lt;li&gt;06 May 2022 - continue making materials for 05&lt;/li&gt; &#xA; &lt;li&gt;05 May 2022 - update section 05 with headings/outline&lt;/li&gt; &#xA; &lt;li&gt;28 Apr 2022 - recorded 13 videos for 04, finished videos for 04, now to make materials for 05&lt;/li&gt; &#xA; &lt;li&gt;27 Apr 2022 - recorded 3 videos for 04&lt;/li&gt; &#xA; &lt;li&gt;26 Apr 2022 - recorded 10 videos for 04&lt;/li&gt; &#xA; &lt;li&gt;25 Apr 2022 - recorded 11 videos for 04&lt;/li&gt; &#xA; &lt;li&gt;24 Apr 2022 - prepared slides for 04&lt;/li&gt; &#xA; &lt;li&gt;23 Apr 2022 - recorded 6 videos for 03, finished videos for 03, now to 04&lt;/li&gt; &#xA; &lt;li&gt;22 Apr 2022 - recorded 5 videos for 03&lt;/li&gt; &#xA; &lt;li&gt;21 Apr 2022 - recorded 9 videos for 03&lt;/li&gt; &#xA; &lt;li&gt;20 Apr 2022 - recorded 3 videos for 03&lt;/li&gt; &#xA; &lt;li&gt;19 Apr 2022 - recorded 11 videos for 03&lt;/li&gt; &#xA; &lt;li&gt;18 Apr 2022 - finish exercises/solutions for 04, added live-coding walkthrough of 04 exercises/solutions on YouTube: &lt;a href=&#34;https://youtu.be/vsFMF9wqWx0&#34;&gt;https://youtu.be/vsFMF9wqWx0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;16 Apr 2022 - finish exercises/solutions for 03, added live-coding walkthrough of 03 exercises/solutions on YouTube: &lt;a href=&#34;https://youtu.be/_PibmqpEyhA&#34;&gt;https://youtu.be/_PibmqpEyhA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;14 Apr 2022 - add final images/annotations for 04, begin on exercises/solutions for 03 &amp;amp; 04&lt;/li&gt; &#xA; &lt;li&gt;13 Apr 2022 - add more images/annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;3 Apr 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;2 Apr 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;1 Apr 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;31 Mar 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;29 Mar 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;27 Mar 2022 - starting to add annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;26 Mar 2022 - making dataset for 04&lt;/li&gt; &#xA; &lt;li&gt;25 Mar 2022 - make slides for 03&lt;/li&gt; &#xA; &lt;li&gt;24 Mar 2022 - fix error for 03 not working in docs (finally)&lt;/li&gt; &#xA; &lt;li&gt;23 Mar 2022 - add more images for 03&lt;/li&gt; &#xA; &lt;li&gt;22 Mar 2022 - add images for 03&lt;/li&gt; &#xA; &lt;li&gt;20 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;18 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;17 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;16 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;15 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;14 Mar 2022 - start adding annotations for notebook 03, see the work in progress here: &lt;a href=&#34;https://www.learnpytorch.io/03_pytorch_computer_vision/&#34;&gt;https://www.learnpytorch.io/03_pytorch_computer_vision/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;12 Mar 2022 - recorded 12 videos for 02, finished section 02, now onto making materials for 03, 04, 05&lt;/li&gt; &#xA; &lt;li&gt;11 Mar 2022 - recorded 9 videos for 02&lt;/li&gt; &#xA; &lt;li&gt;10 Mar 2022 - recorded 10 videos for 02&lt;/li&gt; &#xA; &lt;li&gt;9 Mar 2022 - cleaning up slides/code for 02, getting ready for recording&lt;/li&gt; &#xA; &lt;li&gt;8 Mar 2022 - recorded 9 videos for section 01, finished section 01, now onto 02&lt;/li&gt; &#xA; &lt;li&gt;7 Mar 2022 - recorded 4 videos for section 01&lt;/li&gt; &#xA; &lt;li&gt;6 Mar 2022 - recorded 4 videos for section 01&lt;/li&gt; &#xA; &lt;li&gt;4 Mar 2022 - recorded 10 videos for section 01&lt;/li&gt; &#xA; &lt;li&gt;20 Feb 2022 - recorded 8 videos for section 00, finished section, now onto 01&lt;/li&gt; &#xA; &lt;li&gt;18 Feb 2022 - recorded 13 videos for section 00&lt;/li&gt; &#xA; &lt;li&gt;17 Feb 2022 - recorded 11 videos for section 00&lt;/li&gt; &#xA; &lt;li&gt;16 Feb 2022 - added setup guide&lt;/li&gt; &#xA; &lt;li&gt;12 Feb 2022 - tidy up README with table of course materials, finish images and slides for 01&lt;/li&gt; &#xA; &lt;li&gt;10 Feb 2022 - finished slides and images for 00, notebook is ready for publishing: &lt;a href=&#34;https://www.learnpytorch.io/00_pytorch_fundamentals/&#34;&gt;https://www.learnpytorch.io/00_pytorch_fundamentals/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;01-07 Feb 2022 - add annotations for 02, finished, still need images, going to work on exercises/solutions today&lt;/li&gt; &#xA; &lt;li&gt;31 Jan 2022 - start adding annotations for 02&lt;/li&gt; &#xA; &lt;li&gt;28 Jan 2022 - add exercies and solutions for 01&lt;/li&gt; &#xA; &lt;li&gt;26 Jan 2022 - lots more annotations to 01, should be finished tomorrow, will do exercises + solutions then too&lt;/li&gt; &#xA; &lt;li&gt;24 Jan 2022 - add a bunch of annotations to 01&lt;/li&gt; &#xA; &lt;li&gt;21 Jan 2022 - start adding annotations for 01&lt;/li&gt; &#xA; &lt;li&gt;20 Jan 2022 - finish annotations for 00 (still need to add images), add exercises and solutions for 00&lt;/li&gt; &#xA; &lt;li&gt;19 Jan 2022 - add more annotations for 00&lt;/li&gt; &#xA; &lt;li&gt;18 Jan 2022 - add more annotations for 00&lt;/li&gt; &#xA; &lt;li&gt;17 Jan 2022 - back from holidays, adding more annotations to 00&lt;/li&gt; &#xA; &lt;li&gt;10 Dec 2021 - start adding annoations for 00&lt;/li&gt; &#xA; &lt;li&gt;9 Dec 2021 - Created a website for the course (&lt;a href=&#34;https://learnpytorch.io&#34;&gt;learnpytorch.io&lt;/a&gt;) you&#39;ll see updates posted there as development continues&lt;/li&gt; &#xA; &lt;li&gt;8 Dec 2021 - Clean up notebook 07, starting to go back through code and add annotations&lt;/li&gt; &#xA; &lt;li&gt;26 Nov 2021 - Finish skeleton code for 07, added four different experiments, need to clean up and make more straightforward&lt;/li&gt; &#xA; &lt;li&gt;25 Nov 2021 - clean code for 06, add skeleton code for 07 (experiment tracking)&lt;/li&gt; &#xA; &lt;li&gt;24 Nov 2021 - Update 04, 05, 06 notebooks for easier digestion and learning, each section should cover a max of 3 big ideas, 05 is now dedicated to turning notebook code into modular code&lt;/li&gt; &#xA; &lt;li&gt;22 Nov 2021 - Update 04 train and test functions to make more straightforward&lt;/li&gt; &#xA; &lt;li&gt;19 Nov 2021 - Added 05 (transfer learning) notebook, update custom data loading code in 04&lt;/li&gt; &#xA; &lt;li&gt;18 Nov 2021 - Updated vision code for 03 and added custom dataset loading code in 04&lt;/li&gt; &#xA; &lt;li&gt;12 Nov 2021 - Added a bunch of skeleton code to notebook 04 for custom dataset loading, next is modelling with custom data&lt;/li&gt; &#xA; &lt;li&gt;10 Nov 2021 - researching best practice for custom datasets for 04&lt;/li&gt; &#xA; &lt;li&gt;9 Nov 2021 - Update 03 skeleton code to finish off building CNN model, onto 04 for loading custom datasets&lt;/li&gt; &#xA; &lt;li&gt;4 Nov 2021 - Add GPU code to 03 + train/test loops + &lt;code&gt;helper_functions.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;3 Nov 2021 - Add basic start for 03, going to finish by end of week&lt;/li&gt; &#xA; &lt;li&gt;29 Oct 2021 - Tidied up skeleton code for 02, still a few more things to clean/tidy, created 03&lt;/li&gt; &#xA; &lt;li&gt;28 Oct 2021 - Finished skeleton code for 02, going to clean/tidy tomorrow, 03 next week&lt;/li&gt; &#xA; &lt;li&gt;27 Oct 2021 - add a bunch of code for 02, going to finish tomorrow/by end of week&lt;/li&gt; &#xA; &lt;li&gt;26 Oct 2021 - update 00, 01, 02 with outline/code, skeleton code for 00 &amp;amp; 01 done, 02 next&lt;/li&gt; &#xA; &lt;li&gt;23, 24 Oct 2021 - update 00 and 01 notebooks with more outline/code&lt;/li&gt; &#xA; &lt;li&gt;20 Oct 2021 - add v0 outlines for 01 and 02, add rough outline of course to README, this course will focus on less but better&lt;/li&gt; &#xA; &lt;li&gt;19 Oct 2021 - Start repo üî•, add fundamentals notebook draft v0&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>rinongal/textual_inversion</title>
    <updated>2022-10-01T02:00:10Z</updated>
    <id>tag:github.com,2022-10-01:/rinongal/textual_inversion</id>
    <link href="https://github.com/rinongal/textual_inversion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.01618&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2208.01618-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;Project Website&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion&lt;/strong&gt;&lt;br&gt; Rinon Gal&lt;sup&gt;1,2&lt;/sup&gt;, Yuval Alaluf&lt;sup&gt;1&lt;/sup&gt;, Yuval Atzmon&lt;sup&gt;2&lt;/sup&gt;, Or Patashnik&lt;sup&gt;1&lt;/sup&gt;, Amit H. Bermano&lt;sup&gt;1&lt;/sup&gt;, Gal Chechik&lt;sup&gt;2&lt;/sup&gt;, Daniel Cohen-Or&lt;sup&gt;1&lt;/sup&gt; &lt;br&gt; &lt;sup&gt;1&lt;/sup&gt;Tel Aviv University, &lt;sup&gt;2&lt;/sup&gt;NVIDIA&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: &lt;br&gt; Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn &lt;i&gt;our&lt;/i&gt; cat into a painting, or imagine a new product based on &lt;i&gt;our&lt;/i&gt; favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new &#34;words&#34; in the embedding space of a frozen text-to-image model. These &#34;words&#34; can be composed into natural language sentences, guiding &lt;i&gt;personalized&lt;/i&gt; creation in an intuitive way. Notably, we find evidence that a &lt;i&gt;single&lt;/i&gt; word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains the official code, data and sample inversions for our Textual Inversion paper.&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;29/08/2022&lt;/strong&gt; Merge embeddings now supports SD embeddings. Added SD pivotal tuning code (WIP), fixed training duration, checkpoint save iterations. &lt;strong&gt;21/08/2022&lt;/strong&gt; Code released!&lt;/p&gt; &#xA;&lt;h2&gt;TODO:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release code!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Optimize gradient storing / checkpointing. Memory requirements, training times reduced by ~55%&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release data sets&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release pre-trained embeddings&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add Stable Diffusion support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Our code builds on, and shares requirements with &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Latent Diffusion Models (LDM)&lt;/a&gt;. To set up their environment, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will also need the official LDM text-to-image checkpoint, available through the &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;LDM project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Currently, the model can be downloaded by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p models/ldm/text2img-large/&#xA;wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Inversion&lt;/h3&gt; &#xA;&lt;p&gt;To invert an image set, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --base configs/latent-diffusion/txt2img-1p4B-finetune.yaml &#xA;               -t &#xA;               --actual_resume /path/to/pretrained/model.ckpt &#xA;               -n &amp;lt;run_name&amp;gt; &#xA;               --gpus 0, &#xA;               --data_root /path/to/directory/with/images&#xA;               --init_word &amp;lt;initialization_word&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where the initialization word should be a single-token rough description of the object (e.g., &#39;toy&#39;, &#39;painting&#39;, &#39;sculpture&#39;). If the input is comprised of more than a single token, you will be prompted to replace it.&lt;/p&gt; &#xA;&lt;p&gt;Please note that &lt;code&gt;init_word&lt;/code&gt; is &lt;em&gt;not&lt;/em&gt; the placeholder string that will later represent the concept. It is only used as a beggining point for the optimization scheme.&lt;/p&gt; &#xA;&lt;p&gt;In the paper, we use 5k training iterations. However, some concepts (particularly styles) can converge much faster.&lt;/p&gt; &#xA;&lt;p&gt;To run on multiple GPUs, provide a comma-delimited list of GPU indices to the --gpus argument (e.g., &lt;code&gt;--gpus 0,3,7,8&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Embeddings and output images will be saved in the log directory.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;configs/latent-diffusion/txt2img-1p4B-finetune.yaml&lt;/code&gt; for more options, such as: changing the placeholder string which denotes the concept (defaults to &#34;*&#34;), changing the maximal number of training iterations, changing how often checkpoints are saved and more.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt; All training set images should be upright. If you are using phone captured images, check the inputs_gs*.jpg files in the output image directory and make sure they are oriented correctly. Many phones capture images with a 90 degree rotation and denote this in the image metadata. Windows parses these correctly, but PIL does not. Hence you will need to correct them manually (e.g. by pasting them into paint and re-saving) or wait until we add metadata parsing.&lt;/p&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;p&gt;To generate new images of the learned concept, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --ddim_eta 0.0 &#xA;                          --n_samples 8 &#xA;                          --n_iter 2 &#xA;                          --scale 10.0 &#xA;                          --ddim_steps 50 &#xA;                          --embedding_path /path/to/logs/trained_model/checkpoints/embeddings_gs-5049.pt &#xA;                          --ckpt_path /path/to/pretrained/model.ckpt &#xA;                          --prompt &#34;a photo of *&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where * is the placeholder string used during inversion.&lt;/p&gt; &#xA;&lt;h3&gt;Merging Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;LDM embedding checkpoints can be merged into a single file by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python merge_embeddings.py &#xA;--manager_ckpts /path/to/first/embedding.pt /path/to/second/embedding.pt [...]&#xA;--output_path /path/to/output/embedding.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For SD embeddings, simply add the flag: &lt;code&gt;-sd&lt;/code&gt; or &lt;code&gt;--stable_diffusion&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If the checkpoints contain conflicting placeholder strings, you will be prompted to select new placeholders. The merged checkpoint can later be used to prompt multiple concepts at once (&#34;A photo of * in the style of @&#34;).&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Models / Data&lt;/h3&gt; &#xA;&lt;p&gt;Datasets which appear in the paper are being uploaded &lt;a href=&#34;https://drive.google.com/drive/folders/1d2UXkX0GWM-4qUwThjNhFIPP7S6WUbQJ&#34;&gt;here&lt;/a&gt;. Some sets are unavailable due to image ownership. We will upload more as we recieve permissions to do so.&lt;/p&gt; &#xA;&lt;p&gt;Pretained models coming soon.&lt;/p&gt; &#xA;&lt;h2&gt;Stable Diffusion&lt;/h2&gt; &#xA;&lt;p&gt;Stable Diffusion support is a work in progress and will be completed soon‚Ñ¢.&lt;/p&gt; &#xA;&lt;h2&gt;Tips and Tricks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adding &#34;a photo of&#34; to the prompt usually results in better target consistency.&lt;/li&gt; &#xA; &lt;li&gt;Results can be seed sensititve. If you&#39;re unsatisfied with the model, try re-inverting with a new seed (by adding &lt;code&gt;--seed &amp;lt;#&amp;gt;&lt;/code&gt; to the prompt).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you make use of our work, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{gal2022textual,&#xA;      doi = {10.48550/ARXIV.2208.01618},&#xA;      url = {https://arxiv.org/abs/2208.01618},&#xA;      author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},&#xA;      title = {An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion},&#xA;      publisher = {arXiv},&#xA;      year = {2022},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;Here are some sample results. Please visit our &lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;project page&lt;/a&gt; or read our paper for more!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rinongal/textual_inversion/main/img/teaser.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rinongal/textual_inversion/main/img/samples.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rinongal/textual_inversion/main/img/style.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>