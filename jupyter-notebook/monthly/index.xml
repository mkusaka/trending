<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-01T01:52:00Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>alembics/disco-diffusion</title>
    <updated>2022-09-01T01:52:00Z</updated>
    <id>tag:github.com,2022-09-01:/alembics/disco-diffusion</id>
    <link href="https://github.com/alembics/disco-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Disco Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A frankensteinian amalgamation of notebooks, models and techniques for the generation of AI Art and Animations.&lt;/p&gt; &#xA;&lt;p&gt;[to be updated with further info soon]&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project uses a special conversion tool to convert the python files into notebooks for easier development.&lt;/p&gt; &#xA;&lt;p&gt;What this means is you do not have to touch the notebook directly to make changes to it&lt;/p&gt; &#xA;&lt;p&gt;the tool being used is called &lt;a href=&#34;https://github.com/MSFTserver/colab-convert&#34;&gt;Colab-Convert&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;install using &lt;code&gt;pip install colab-convert&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;convert .py to .ipynb &lt;code&gt;colab-convert /path/to/file.py /path/to/file.ipynb&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;convert .ipynb to .py &lt;code&gt;colab-convert /path/to/file.ipynb /path/to/file.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;h4&gt;v1 Oct 29th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial QoL improvements added, including user friendly UI, settings+prompt saving and improved google drive folder organization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v1.1 Nov 13th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Now includes sizing options, intermediate saves and fixed image prompts and perlin inits. unexposed batch option since it doesn&#39;t work&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2 Update: Nov 22nd 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial addition of Katherine Crowson&#39;s Secondary Model Method (&lt;a href=&#34;https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR&#34;&gt;https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix for incorrectly named settings files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v3 Update: Dec 24th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implemented Dango&#39;s advanced cutout method&lt;/li&gt; &#xA; &lt;li&gt;Added SLIP models, thanks to NeuralDivergent&lt;/li&gt; &#xA; &lt;li&gt;Fixed issue with NaNs resulting in black images, with massive help and testing from @Softology&lt;/li&gt; &#xA; &lt;li&gt;Perlin now changes properly within batches (not sure where this perlin_regen code came from originally, but thank you)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v4 Update: Jan 2022 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implemented Diffusion Zooming&lt;/li&gt; &#xA; &lt;li&gt;Added Chigozie keyframing&lt;/li&gt; &#xA; &lt;li&gt;Made a bunch of edits to processes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v4.1 Update: Jan 14th 2022 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added video input mode&lt;/li&gt; &#xA; &lt;li&gt;Added license that somehow went missing&lt;/li&gt; &#xA; &lt;li&gt;Added improved prompt keyframing, fixed image_prompts and multiple prompts&lt;/li&gt; &#xA; &lt;li&gt;Improved UI&lt;/li&gt; &#xA; &lt;li&gt;Significant under the hood cleanup and improvement&lt;/li&gt; &#xA; &lt;li&gt;Refined defaults for each mode&lt;/li&gt; &#xA; &lt;li&gt;Removed SLIP models for the time being due to import conflicts&lt;/li&gt; &#xA; &lt;li&gt;Added latent-diffusion SuperRes for sharpening&lt;/li&gt; &#xA; &lt;li&gt;Added resume run mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5 Update: Feb 20th 2022 - gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added 3D animation mode. Uses weighted combination of AdaBins and MiDaS depth estimation models. Uses pytorch3d for 3D transforms on Colab and/or Linux.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.1 Update: Mar 30th 2022 - zippy / Chris Allen and gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integrated Turbo+Smooth features from Disco Diffusion Turbo -- just the implementation, without its defaults.&lt;/li&gt; &#xA; &lt;li&gt;Implemented resume of turbo animations in such a way that it&#39;s now possible to resume from different batch folders and batch numbers.&lt;/li&gt; &#xA; &lt;li&gt;3D rotation parameter units are now degrees (rather than radians)&lt;/li&gt; &#xA; &lt;li&gt;Corrected name collision in sampling_mode (now diffusion_sampling_mode for plms/ddim, and sampling_mode for 3D transform sampling)&lt;/li&gt; &#xA; &lt;li&gt;Added video_init_seed_continuity option to make init video animations more continuous&lt;/li&gt; &#xA; &lt;li&gt;Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Remove Super Resolution&lt;/li&gt; &#xA; &lt;li&gt;Remove Slip Models&lt;/li&gt; &#xA; &lt;li&gt;Update for crossplatform support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.1 Update: Apr 4th 2022 - MSFTserver aka HostsServer&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Remove Super Resolution&lt;/li&gt; &#xA; &lt;li&gt;Remove Slip Models&lt;/li&gt; &#xA; &lt;li&gt;Update for crossplatform support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.2 Update: Apr 10th 2022 - nin_artificial / Tom Mason&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;VR Mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.3 Update: Jun 10th 2022 - nshepperd, huemin, cut_pow&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Horizontal and Vertical symmetry&lt;/li&gt; &#xA; &lt;li&gt;Addition of ViT-L/14@336px model (requires high VRAM)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.4 Update: Jun 14th 2022 - devdef / Alex Spirin, integrated into DD main by gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Warp mode - for smooth/continuous video input results leveraging optical flow estimation and frame blending&lt;/li&gt; &#xA; &lt;li&gt;Custom models support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.5 Update: Jul 11th 2022 - Palmweaver / Chris Scalf, KaliYuga_ai, further integration by gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenCLIP models integration&lt;/li&gt; &#xA; &lt;li&gt;Pixel Art Diffusion, Watercolor Diffusion, and Pulp SciFi Diffusion models&lt;/li&gt; &#xA; &lt;li&gt;cut_ic_pow scheduling&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.6 Update: Jul 13th 2022 - Felipe3DArtist, integration by gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integrated portrait_generator_v001 - 512x512 diffusion model trained on faces - from Felipe3DArtist&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notebook Provenance&lt;/h2&gt; &#xA;&lt;p&gt;Original notebook by Katherine Crowson (&lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;https://github.com/crowsonkb&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/RiversHaveWings&#34;&gt;https://twitter.com/RiversHaveWings&lt;/a&gt;). It uses either OpenAI&#39;s 256x256 unconditional ImageNet or Katherine Crowson&#39;s fine-tuned 512x512 diffusion model (&lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;https://github.com/openai/guided-diffusion&lt;/a&gt;), together with CLIP (&lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;https://github.com/openai/CLIP&lt;/a&gt;) to connect text prompts with images.&lt;/p&gt; &#xA;&lt;p&gt;Modified by Daniel Russell (&lt;a href=&#34;https://github.com/russelldc&#34;&gt;https://github.com/russelldc&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/danielrussruss&#34;&gt;https://twitter.com/danielrussruss&lt;/a&gt;) to include (hopefully) optimal params for quick generations in 15-100 timesteps rather than 1000, as well as more robust augmentations.&lt;/p&gt; &#xA;&lt;p&gt;Further improvements from Dango233 and nshepperd helped improve the quality of diffusion in general, and especially so for shorter runs like this notebook aims to achieve.&lt;/p&gt; &#xA;&lt;p&gt;Vark added code to load in multiple Clip models at once, which all prompts are evaluated against, which may greatly improve accuracy.&lt;/p&gt; &#xA;&lt;p&gt;The latest zoom, pan, rotation, and keyframes features were taken from Chigozie Nri&#39;s VQGAN Zoom Notebook (&lt;a href=&#34;https://github.com/chigozienri&#34;&gt;https://github.com/chigozienri&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/chigozienri&#34;&gt;https://twitter.com/chigozienri&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Advanced DangoCutn Cutout method is also from Dango223.&lt;/p&gt; &#xA;&lt;p&gt;--&lt;/p&gt; &#xA;&lt;p&gt;Somnai (&lt;a href=&#34;https://twitter.com/Somnai_dreams&#34;&gt;https://twitter.com/Somnai_dreams&lt;/a&gt;) added 2D Diffusion animation techniques, QoL improvements and various implementations of tech and techniques, mostly listed in the changelog below.&lt;/p&gt; &#xA;&lt;p&gt;3D animation implementation added by Adam Letts (&lt;a href=&#34;https://twitter.com/gandamu_ml&#34;&gt;https://twitter.com/gandamu_ml&lt;/a&gt;) in collaboration with Somnai.&lt;/p&gt; &#xA;&lt;p&gt;Turbo feature by Chris Allen (&lt;a href=&#34;https://twitter.com/zippy731&#34;&gt;https://twitter.com/zippy731&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Improvements to ability to run on local systems, Windows support, and dependency installation by HostsServer (&lt;a href=&#34;https://twitter.com/HostsServer&#34;&gt;https://twitter.com/HostsServer&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;VR Mode by Tom Mason (&lt;a href=&#34;https://twitter.com/nin_artificial&#34;&gt;https://twitter.com/nin_artificial&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Horizontal and Vertical symmetry functionality by nshepperd. Symmetry transformation_steps by huemin (&lt;a href=&#34;https://twitter.com/huemin_art&#34;&gt;https://twitter.com/huemin_art&lt;/a&gt;). Symmetry integration into Disco Diffusion by Dmitrii Tochilkin (&lt;a href=&#34;https://twitter.com/cut_pow&#34;&gt;https://twitter.com/cut_pow&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Warp and custom model support by Alex Spirin (&lt;a href=&#34;https://twitter.com/devdef&#34;&gt;https://twitter.com/devdef&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Pixel Art Diffusion, Watercolor Diffusion, and Pulp SciFi Diffusion models from KaliYuga (&lt;a href=&#34;https://twitter.com/KaliYuga_ai&#34;&gt;https://twitter.com/KaliYuga_ai&lt;/a&gt;). Follow KaliYuga&#39;s Twitter for the latest models and for notebooks with specialized settings.&lt;/p&gt; &#xA;&lt;p&gt;Integration of OpenCLIP models and initiation of integration of KaliYuga models by Palmweaver / Chris Scalf (&lt;a href=&#34;https://twitter.com/ChrisScalf11&#34;&gt;https://twitter.com/ChrisScalf11&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Integrated portrait_generator_v001 from Felipe3DArtist (&lt;a href=&#34;https://twitter.com/Felipe3DArtist&#34;&gt;https://twitter.com/Felipe3DArtist&lt;/a&gt;)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/notebooks</title>
    <updated>2022-09-01T01:52:00Z</updated>
    <id>tag:github.com,2022-09-01:/huggingface/notebooks</id>
    <link href="https://github.com/huggingface/notebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notebooks using the Hugging Face libraries 🤗&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;notebooks&lt;/h1&gt; &#xA;&lt;p&gt;Notebooks using the Hugging Face libraries 🤗&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jakevdp/PythonDataScienceHandbook</title>
    <updated>2022-09-01T01:52:00Z</updated>
    <id>tag:github.com,2022-09-01:/jakevdp/PythonDataScienceHandbook</id>
    <link href="https://github.com/jakevdp/PythonDataScienceHandbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python Data Science Handbook: full text in Jupyter Notebooks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python Data Science Handbook&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the entire &lt;a href=&#34;http://shop.oreilly.com/product/0636920034919.do&#34;&gt;Python Data Science Handbook&lt;/a&gt;, in the form of (free!) Jupyter notebooks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks/figures/PDSH-cover.png&#34; alt=&#34;cover image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to Use this Book&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Read the book in its entirety online at &lt;a href=&#34;https://jakevdp.github.io/PythonDataScienceHandbook/&#34;&gt;https://jakevdp.github.io/PythonDataScienceHandbook/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the code using the Jupyter notebooks available in this repository&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks&#34;&gt;notebooks&lt;/a&gt; directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Launch executable versions of these notebooks using &lt;a href=&#34;http://colab.research.google.com&#34;&gt;Google Colab&lt;/a&gt;: &lt;a href=&#34;https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Launch a live notebook server with these notebooks using &lt;a href=&#34;https://beta.mybinder.org/&#34;&gt;binder&lt;/a&gt;: &lt;a href=&#34;https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Buy the printed book through &lt;a href=&#34;http://shop.oreilly.com/product/0636920034919.do&#34;&gt;O&#39;Reilly Media&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;The book was written and tested with Python 3.5, though other Python versions (including Python 2.7) should work in nearly all cases.&lt;/p&gt; &#xA;&lt;p&gt;The book introduces the core libraries essential for working with data in Python: particularly &lt;a href=&#34;http://ipython.org&#34;&gt;IPython&lt;/a&gt;, &lt;a href=&#34;http://numpy.org&#34;&gt;NumPy&lt;/a&gt;, &lt;a href=&#34;http://pandas.pydata.org&#34;&gt;Pandas&lt;/a&gt;, &lt;a href=&#34;http://matplotlib.org&#34;&gt;Matplotlib&lt;/a&gt;, &lt;a href=&#34;http://scikit-learn.org&#34;&gt;Scikit-Learn&lt;/a&gt;, and related packages. Familiarity with Python as a language is assumed; if you need a quick introduction to the language itself, see the free companion project, &lt;a href=&#34;https://github.com/jakevdp/WhirlwindTourOfPython&#34;&gt;A Whirlwind Tour of Python&lt;/a&gt;: it&#39;s a fast-paced introduction to the Python language aimed at researchers and scientists.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&#34;&gt;Index.ipynb&lt;/a&gt; for an index of the notebooks available to accompany the text.&lt;/p&gt; &#xA;&lt;h2&gt;Software&lt;/h2&gt; &#xA;&lt;p&gt;The code in the book was tested with Python 3.5, though most (but not all) will also work correctly with Python 2.7 and other older Python versions.&lt;/p&gt; &#xA;&lt;p&gt;The packages I used to run the code in the book are listed in &lt;a href=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; (Note that some of these exact version numbers may not be available on your platform: you may have to tweak them for your own use). To install the requirements using &lt;a href=&#34;http://conda.pydata.org&#34;&gt;conda&lt;/a&gt;, run the following at the command-line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda install --file requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To create a stand-alone environment named &lt;code&gt;PDSH&lt;/code&gt; with Python 3.5 and all the required package versions, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda create -n PDSH python=3.5 --file requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can read more about using conda environments in the &lt;a href=&#34;http://conda.pydata.org/docs/using/envs.html&#34;&gt;Managing Environments&lt;/a&gt; section of the conda documentation.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;h3&gt;Code&lt;/h3&gt; &#xA;&lt;p&gt;The code in this repository, including all code samples in the notebooks listed above, is released under the &lt;a href=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/LICENSE-CODE&#34;&gt;MIT license&lt;/a&gt;. Read more at the &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Text&lt;/h3&gt; &#xA;&lt;p&gt;The text content of the book is released under the &lt;a href=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/LICENSE-TEXT&#34;&gt;CC-BY-NC-ND license&lt;/a&gt;. Read more at &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode&#34;&gt;Creative Commons&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>