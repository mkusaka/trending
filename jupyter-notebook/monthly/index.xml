<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-01T01:56:25Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>XavierXiao/Dreambooth-Stable-Diffusion</title>
    <updated>2022-11-01T01:56:25Z</updated>
    <id>tag:github.com,2022-11-01:/XavierXiao/Dreambooth-Stable-Diffusion</id>
    <link href="https://github.com/XavierXiao/Dreambooth-Stable-Diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Dreambooth (https://arxiv.org/abs/2208.12242) with Stable Diffusion&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dreambooth on Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;This is an implementtaion of Google&#39;s &lt;a href=&#34;https://arxiv.org/abs/2208.12242&#34;&gt;Dreambooth&lt;/a&gt; with &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;. The original Dreambooth is based on &lt;a href=&#34;https://imagen.research.google/&#34;&gt;Imagen&lt;/a&gt; text-to-image model. However, neither the model nor the pre-trained weights of Imagen is available. To enable people to fine-tune a text-to-image model with a few examples, I implemented the idea of Dreambooth on Stable diffusion.&lt;/p&gt; &#xA;&lt;p&gt;This code repository is based on that of &lt;a href=&#34;https://github.com/rinongal/textual_inversion&#34;&gt;Textual Inversion&lt;/a&gt;. Note that Textual Inversion only optimizes word ebedding, while dreambooth fine-tunes the whole diffusion model.&lt;/p&gt; &#xA;&lt;p&gt;The implementation makes minimum changes over the official codebase of Textual Inversion. In fact, due to lazyness, some components in Textual Inversion, such as the embedding manager, are not deleted, although they will never be used here.&lt;/p&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;9/20/2022&lt;/strong&gt;: I just found a way to reduce the GPU memory a bit. Remember that this code is based on Textual Inversion, and TI&#39;s code base has &lt;a href=&#34;https://github.com/rinongal/textual_inversion/raw/main/ldm/modules/diffusionmodules/util.py#L112&#34;&gt;this line&lt;/a&gt;, which disable gradient checkpointing in a hard-code way. This is because in TI, the Unet is not optimized. However, in Dreambooth we optimize the Unet, so we can turn on the gradient checkpoint pointing trick, as in the original SD repo &lt;a href=&#34;https://github.com/CompVis/stable-diffusion/raw/main/ldm/modules/diffusionmodules/util.py#L112&#34;&gt;here&lt;/a&gt;. The gradient checkpoint is default to be True in &lt;a href=&#34;https://github.com/XavierXiao/Dreambooth-Stable-Diffusion/raw/main/configs/stable-diffusion/v1-finetune_unfrozen.yaml#L47&#34;&gt;config&lt;/a&gt;. I have updated the codes.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Preparation&lt;/h3&gt; &#xA;&lt;p&gt;First set-up the &lt;code&gt;ldm&lt;/code&gt; enviroment following the instruction from textual inversion repo, or the original Stable Diffusion repo.&lt;/p&gt; &#xA;&lt;p&gt;To fine-tune a stable diffusion model, you need to obtain the pre-trained stable diffusion models following their &lt;a href=&#34;https://github.com/CompVis/stable-diffusion#stable-diffusion-v1&#34;&gt;instructions&lt;/a&gt;. Weights can be downloaded on &lt;a href=&#34;https://huggingface.co/CompVis&#34;&gt;HuggingFace&lt;/a&gt;. You can decide which version of checkpoint to use, but I use &lt;code&gt;sd-v1-4-full-ema.ckpt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also need to create a set of images for regularization, as the fine-tuning algorithm of Dreambooth requires that. Details of the algorithm can be found in the paper. Note that in the original paper, the regularization images seem to be generated on-the-fly. However, here I generated a set of regularization images before the training. The text prompt for generating regularization images can be &lt;code&gt;photo of a &amp;lt;class&amp;gt;&lt;/code&gt;, where &lt;code&gt;&amp;lt;class&amp;gt;&lt;/code&gt; is a word that describes the class of your object, such as &lt;code&gt;dog&lt;/code&gt;. The command is&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/stable_txt2img.py --ddim_eta 0.0 --n_samples 8 --n_iter 1 --scale 10.0 --ddim_steps 50  --ckpt /path/to/original/stable-diffusion/sd-v1-4-full-ema.ckpt --prompt &#34;a photo of a &amp;lt;class&amp;gt;&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;I generate 8 images for regularization, but more regularization images may lead to stronger regularization and better editability. After that, save the generated images (separately, one image per &lt;code&gt;.png&lt;/code&gt; file) at &lt;code&gt;/root/to/regularization/images&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Updates on 9/9&lt;/strong&gt; We should definitely use more images for regularization. Please try 100 or 200, to better align with the original paper. To acomodate this, I shorten the &#34;repeat&#34; of reg dataset in the &lt;a href=&#34;https://github.com/XavierXiao/Dreambooth-Stable-Diffusion/raw/main/configs/stable-diffusion/v1-finetune_unfrozen.yaml#L96&#34;&gt;config file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For some cases, if the generated regularization images are highly unrealistic (happens when you want to generate &#34;man&#34; or &#34;woman&#34;), you can find a diverse set of images (of man/woman) online, and use them as regularization images.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Training can be done by running the following command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --base configs/stable-diffusion/v1-finetune_unfrozen.yaml &#xA;                -t &#xA;                --actual_resume /path/to/original/stable-diffusion/sd-v1-4-full-ema.ckpt  &#xA;                -n &amp;lt;job name&amp;gt; &#xA;                --gpus 0, &#xA;                --data_root /root/to/training/images &#xA;                --reg_data_root /root/to/regularization/images &#xA;                --class_word &amp;lt;xxx&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Detailed configuration can be found in &lt;code&gt;configs/stable-diffusion/v1-finetune_unfrozen.yaml&lt;/code&gt;. In particular, the default learning rate is &lt;code&gt;1.0e-6&lt;/code&gt; as I found the &lt;code&gt;1.0e-5&lt;/code&gt; in the Dreambooth paper leads to poor editability. The parameter &lt;code&gt;reg_weight&lt;/code&gt; corresponds to the weight of regularization in the Dreambooth paper, and the default is set to &lt;code&gt;1.0&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Dreambooth requires a placeholder word &lt;code&gt;[V]&lt;/code&gt;, called identifier, as in the paper. This identifier needs to be a relatively rare tokens in the vocabulary. The original paper approaches this by using a rare word in T5-XXL tokenizer. For simplicity, here I just use a random word &lt;code&gt;sks&lt;/code&gt; and hard coded it.. If you want to change that, simply make a change in &lt;a href=&#34;https://github.com/XavierXiao/Dreambooth-Stable-Diffusion/raw/main/ldm/data/personalized.py#L10&#34;&gt;this file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Training will be run for 800 steps, and two checkpoints will be saved at &lt;code&gt;./logs/&amp;lt;job_name&amp;gt;/checkpoints&lt;/code&gt;, one at 500 steps and one at final step. Typically the one at 500 steps works well enough. I train the model use two A6000 GPUs and it takes ~15 mins.&lt;/p&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;p&gt;After training, personalized samples can be obtained by running the command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/stable_txt2img.py --ddim_eta 0.0 &#xA;                                 --n_samples 8 &#xA;                                 --n_iter 1 &#xA;                                 --scale 10.0 &#xA;                                 --ddim_steps 100  &#xA;                                 --ckpt /path/to/saved/checkpoint/from/training&#xA;                                 --prompt &#34;photo of a sks &amp;lt;class&amp;gt;&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In particular, &lt;code&gt;sks&lt;/code&gt; is the identifier, which should be replaced by your choice if you happen to change the identifier, and &lt;code&gt;&amp;lt;class&amp;gt;&lt;/code&gt; is the class word &lt;code&gt;--class_word&lt;/code&gt; for training.&lt;/p&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;Here I show some qualitative results. The training images are obtained from the &lt;a href=&#34;https://github.com/rinongal/textual_inversion/issues/8&#34;&gt;issue&lt;/a&gt; in the Textual Inversion repository, and they are 3 images of a large trash container. Regularization images are generated by prompt &lt;code&gt;photo of a container&lt;/code&gt;. Regularization images are shown here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/a-container-0038.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;After training, generated images with prompt &lt;code&gt;photo of a sks container&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/photo-of-a-sks-container-0018.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Generated images with prompt &lt;code&gt;photo of a sks container on the beach&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/photo-of-a-sks-container-on-the-beach-0017.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Generated images with prompt &lt;code&gt;photo of a sks container on the moon&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/photo-of-a-sks-container-on-the-moon-0016.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Some not-so-perfect but still interesting results:&lt;/p&gt; &#xA;&lt;p&gt;Generated images with prompt &lt;code&gt;photo of a red sks container&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/a-red-sks-container-0021.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Generated images with prompt &lt;code&gt;a dog on top of sks container&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/XavierXiao/Dreambooth-Stable-Diffusion/main/assets/a-dog-on-top-of-sks-container-0023.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>JoePenna/Dreambooth-Stable-Diffusion</title>
    <updated>2022-11-01T01:56:25Z</updated>
    <id>tag:github.com,2022-11-01:/JoePenna/Dreambooth-Stable-Diffusion</id>
    <link href="https://github.com/JoePenna/Dreambooth-Stable-Diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Dreambooth (https://arxiv.org/abs/2208.12242) by way of Textual Inversion (https://arxiv.org/abs/2208.01618) for Stable Diffusion (https://arxiv.org/abs/2112.10752). Tweaks focused on training faces, objects, and styles.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Index&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#notes-by-joe-penna&#34;&gt;Notes by Joe Penna&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#setup&#34;&gt;Setup&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#easy-runpod-instructions&#34;&gt;Easy RunPod Instructions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#vast-ai-setup&#34;&gt;Vast.AI Setup&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#text-vs-dreamb&#34;&gt;Textual Inversion vs. Dreambooth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#using-the-generated-model&#34;&gt;Using the Generated Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#debugging-your-results&#34;&gt;Debugging Your Results&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#they-dont-look-like-you&#34;&gt;They don&#39;t look like you at all!&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#they-sorta-look-like-you-but-exactly-like-your-training-images&#34;&gt;They sorta look like you, but exactly like your training images&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#they-look-like-you-but-not-when-you-try-different-styles&#34;&gt;They look like you, but not when you try different styles&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#hugging-face-diffusers&#34;&gt;Hugging Face Diffusers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;The Repo Formerly Known As &#34;Dreambooth&#34;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/100188076/192390551-cb89364f-af57-4aed-8f3d-f9eb9b61cf95.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;notes-by-joe-penna&#34;&gt;&lt;/a&gt; Notes by Joe Penna&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;strong&gt;INTRODUCTIONS!&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Hi! My name is Joe Penna.&lt;/p&gt; &#xA;&lt;p&gt;You might have seen a few YouTube videos of mine under &lt;em&gt;MysteryGuitarMan&lt;/em&gt;. I&#39;m now a feature film director. You might have seen &lt;a href=&#34;https://www.youtube.com/watch?v=N5aD9ppoQIo&amp;amp;t=6s&#34;&gt;ARCTIC&lt;/a&gt; or &lt;a href=&#34;https://www.youtube.com/watch?v=A_apvQkWsVY&#34;&gt;STOWAWAY&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For my movies, I need to be able to train specific actors, props, locations, etc. So, I did a bunch of changes to @XavierXiao&#39;s repo in order to train people&#39;s faces.&lt;/p&gt; &#xA;&lt;p&gt;I can&#39;t release all the tests for the movie I&#39;m working on, but when I test with my own face, I release those on my Twitter page - &lt;a href=&#34;https://twitter.com/MysteryGuitarM&#34;&gt;@MysteryGuitarM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Lots of these tests were done with a buddy of mine -- Niko from CorridorDigital. It might be how you found this repo!&lt;/p&gt; &#xA;&lt;p&gt;I&#39;m not really a coder. I&#39;m just stubborn, and I&#39;m not afraid of googling. So, eventually, some really smart folks joined in and have been contributing. In this repo, specifically: &lt;a href=&#34;https://github.com/djbielejeski&#34;&gt;@djbielejeski&lt;/a&gt; @gammagec @MrSaad –– but so many others in our Discord!&lt;/p&gt; &#xA;&lt;p&gt;This is no longer my repo. This is the people-who-wanna-see-Dreambooth-on-SD-working-well&#39;s repo!&lt;/p&gt; &#xA;&lt;p&gt;Now, if you wanna try to do this... please read the warnings below first:&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;WARNING!&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;This is bleeding edge stuff&lt;/strong&gt;... there is currently no easy way to run this. This repo is based on a repo based on another repo.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;At the moment, it takes a LOT of effort to create something that&#39;s basically duct tape and bubble gum -- but eventually works SUPER well.&lt;/li&gt; &#xA;   &lt;li&gt;Step in, please! Don&#39;t let that scare ya -- but please know that you&#39;re wading through the jungle at night, with no torch...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Unfreezing the model takes a lot of juice.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;del&gt;You&#39;re gonna need an A6000 / A40 / A100 (or similar top-of-the-line thousands-of-dollars GPU).&lt;/del&gt;&lt;/li&gt; &#xA;   &lt;li&gt;You can now run this on a GPU with 24GB of VRAM (e.g. 3090). Training will be slower, and you&#39;ll need to be sure this is the &lt;em&gt;only&lt;/em&gt; program running.&lt;/li&gt; &#xA;   &lt;li&gt;If, like myself, you don&#39;t happen to own one of those, I&#39;m including a Jupyter notebook here to help you run it on a rented cloud computing platform.&lt;/li&gt; &#xA;   &lt;li&gt;It&#39;s currently tailored to &lt;a href=&#34;https://runpod.io?ref=n8yfwyum&#34;&gt;runpod.io&lt;/a&gt;, but can work on &lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#vast-ai-setup&#34;&gt;vast.ai&lt;/a&gt; / etc.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;This implementation does not fully implement Google&#39;s ideas on how to preserve the latent space.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Most images that are similar to what you&#39;re training will be shifted towards that.&lt;/li&gt; &#xA;   &lt;li&gt;e.g. If you&#39;re training a person, all people will look like you. If you&#39;re training an object, anything in that class will look like your object.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;There doesn&#39;t seem to be an easy way to train two subjects consecutively. You will end up with an &lt;code&gt;11-12GB&lt;/code&gt; file before pruning.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The provided notebook has a pruner that crunches it down to &lt;code&gt;~2gb&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Best practice is to change the &lt;strong&gt;token&lt;/strong&gt; to a celebrity name (&lt;em&gt;note: token, not class&lt;/em&gt; -- so your prompt would be something like: &lt;code&gt;Chris Evans person&lt;/code&gt;). Here&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#using-the-generated-model&#34;&gt;my wife trained with the exact same settings, except for the token&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;setup&#34;&gt;&lt;/a&gt; Setup&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;easy-runpod-instructions&#34;&gt;&lt;/a&gt; Easy RunPod Instructions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sign up for RunPod. Feel free to use my &lt;a href=&#34;https://runpod.io?ref=n8yfwyum&#34;&gt;referral link here&lt;/a&gt;, so that I don&#39;t have to pay for it (but you do).&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;strong&gt;Deploy&lt;/strong&gt; on either &lt;code&gt;SECURE CLOUD&lt;/code&gt; or &lt;code&gt;COMMUNITY CLOUD&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Follow these video instructions here:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=7m__xadX0z0#t=5m33.1s&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/7m__xadX0z0/0.jpg&#34; alt=&#34;VIDEO INSTRUCTIONS&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;vast-ai-setup&#34;&gt;&lt;/a&gt; Vast.AI Instructions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sign up for &lt;a href=&#34;https://vast.ai/&#34;&gt;Vast.AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add some funds (I typically add them in $10 increments)&lt;/li&gt; &#xA; &lt;li&gt;Navigate to the &lt;a href=&#34;https://vast.ai/console/create/&#34;&gt;Client - Create page&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Select pytorch/pytorch as your docker image, and the buttons &#34;Use Jupyter Lab Interface&#34; and &#34;Jupyter direct HTTPS&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/vast-ai-step1-select-docker-image.png&#34; alt=&#34;img.png&#34;&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;You will want to increase your disk space, and filter on GPU RAM (12gb checkpoint files + 4gb model file + regularization images + other stuff adds up fast) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;I typically allocate 150GB&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/vast-ai-step2-instance-filters.png&#34; alt=&#34;img.png&#34;&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Also good to check the Upload/Download speed for enough bandwidth so you don&#39;t spend all your money waiting for things to download.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Select the instance you want, and click &lt;code&gt;Rent&lt;/code&gt;, then head over to your &lt;a href=&#34;https://vast.ai/console/instances/&#34;&gt;Instances&lt;/a&gt; page and click &lt;code&gt;Open&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/vast-ai-step3-instances.png&#34; alt=&#34;img.png&#34;&gt;&lt;/li&gt; &#xA;   &lt;li&gt;You will get an unsafe certificate warning. Click past the warning or install the &lt;a href=&#34;https://vast.ai/static/jvastai_root.cer&#34;&gt;Vast cert&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Notebook -&amp;gt; Python 3&lt;/code&gt; (You can do this next step a number of ways, but I typically do this) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/vast-ai-step4-get-repo.png&#34; alt=&#34;img.png&#34;&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Clone Joe&#39;s repo with this command &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;!git clone https://github.com/JoePenna/Dreambooth-Stable-Diffusion.git&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Click &lt;code&gt;run&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/vast-ai-step5-clone-repo.png&#34; alt=&#34;img.png&#34;&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Navigate into the new &lt;code&gt;Dreambooth-Stable-Diffusion&lt;/code&gt; directory on the left and open the &lt;code&gt;dreambooth_runpod_joepenna.ipynb&lt;/code&gt; file &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/vast-ai-step6-open-notebook.png&#34; alt=&#34;img.png&#34;&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Follow the instructions in the workbook and start training&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;text-vs-dreamb&#34;&gt;&lt;/a&gt; Textual Inversion vs. Dreambooth&lt;/h1&gt; &#xA;&lt;p&gt;The majority of the code in this repo was written by Rinon Gal et. al, the authors of the Textual Inversion research paper. Though a few ideas about regularization images and prior loss preservation (ideas from &#34;Dreambooth&#34;) were added in, out of respect to both the MIT team and the Google researchers, I&#39;m renaming this fork to: &lt;em&gt;&#34;The Repo Formerly Known As &#34;Dreambooth&#34;&#34;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For an alternate implementation , please see &lt;a href=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/#hugging-face-diffusers&#34;&gt;&#34;Alternate Option&#34;&lt;/a&gt; below.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;using-the-generated-model&#34;&gt;&lt;/a&gt; Using the generated model&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;code&gt;ground truth&lt;/code&gt; (real picture, caution: very beautiful woman) &lt;br&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/100188076/192403948-8d1d0e50-3e9f-495f-b8ba-1bcb6b536fc8.png&#34; width=&#34;200&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Same prompt for all of these images below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;code&gt;sks person&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;woman person&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;Natalie Portman person&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;Kate Mara person&lt;/code&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/100188076/192403506-ab96c652-f7d0-47b0-98fa-267defa1e511.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/100188076/192403491-cb258777-5091-4492-a6cc-82305fa729f4.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/100188076/192403437-f9a93720-d41c-4334-8901-fa2d2a10fe36.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/100188076/192403461-1f6972d9-64d0-46b0-b2ed-737e47aae31e.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;debugging-your-results&#34;&gt;&lt;/a&gt; Debugging your results&lt;/h1&gt; &#xA;&lt;h3&gt;❗❗ THE NUMBER ONE MISTAKE PEOPLE MAKE ❗❗&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompting with just your token. ie &#34;joepenna&#34; instead of &#34;joepenna person&#34;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you trained with &lt;code&gt;joepenna&lt;/code&gt; under the class &lt;code&gt;person&lt;/code&gt;, the model should only know your face as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;joepenna person&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example Prompts:&lt;/p&gt; &#xA;&lt;p&gt;🚫 Incorrect (missing &lt;code&gt;person&lt;/code&gt; following &lt;code&gt;joepenna&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;portrait photograph of joepenna 35mm film vintage glass&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;✅ This is right (&lt;code&gt;person&lt;/code&gt; is included after &lt;code&gt;joepenna&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;portrait photograph of joepenna person 35mm film vintage glass&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You might sometimes get someone who kinda looks like you with joepenna (especially if you trained for too many steps), but that&#39;s only because this current iteration of Dreambooth overtrains that token so much that it bleeds into that token.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;☢ Be careful with the types of images you train&lt;/h4&gt; &#xA;&lt;p&gt;While training, Stable doesn&#39;t know that you&#39;re a person. It&#39;s just going to mimic what it sees.&lt;/p&gt; &#xA;&lt;p&gt;So, if these are your training images look like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/caution-training.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You&#39;re only going to get generations of you outside next to a spiky tree, wearing a white-and-gray shirt, in the style of... well, selfie photograph.&lt;/p&gt; &#xA;&lt;p&gt;Instead, this training set is much better:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JoePenna/Dreambooth-Stable-Diffusion/main/readme-images/better-training-images.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The only thing that is consistent between images is the subject. So, Stable will look through the images and learn only your face, which will make &#34;editing&#34; it into other styles possible.&lt;/p&gt; &#xA;&lt;h2&gt;Oh no! You&#39;re not getting good generations!&lt;/h2&gt; &#xA;&lt;h4&gt;&lt;a name=&#34;they-dont-look-like-you&#34;&gt;&lt;/a&gt; OPTION 1: They&#39;re not looking like you at all! (Train longer, or get better training images)&lt;/h4&gt; &#xA;&lt;p&gt;Are you sure you&#39;re prompting it right?&lt;/p&gt; &#xA;&lt;p&gt;It should be &lt;code&gt;&amp;lt;token&amp;gt; &amp;lt;class&amp;gt;&lt;/code&gt;, not just &lt;code&gt;&amp;lt;token&amp;gt;&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;JoePenna person, portrait photograph, 85mm medium format photo&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If it still doesn&#39;t look like you, you didn&#39;t train long enough.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;a name=&#34;they-sorta-look-like-you-but-exactly-like-your-training-images&#34;&gt;&lt;/a&gt; OPTION 2: They&#39;re looking like you, but are all looking like your training images. (Train for less steps, get better training images, fix with prompting)&lt;/h4&gt; &#xA;&lt;p&gt;Okay, a few reasons why: you might have trained too long... or your images were too similar... or you didn&#39;t train with enough images.&lt;/p&gt; &#xA;&lt;p&gt;No problem. We can fix that with the prompt. Stable Diffusion puts a LOT of merit to whatever you type first. So save it for later:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;an exquisite portrait photograph, 85mm medium format photo of JoePenna person with a classic haircut&lt;/code&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;a name=&#34;they-look-like-you-but-not-when-you-try-different-styles&#34;&gt;&lt;/a&gt; OPTION 3: They&#39;re looking like you, but not when you try different styles. (Train longer, get better training images)&lt;/h4&gt; &#xA;&lt;p&gt;You didn&#39;t train long enough...&lt;/p&gt; &#xA;&lt;p&gt;No problem. We can fix that with the prompt:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;JoePenna person in a portrait photograph, JoePenna person in a 85mm medium format photo of JoePenna person&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;More tips and help here: &lt;a href=&#34;https://discord.com/invite/qbMuXBXyHA&#34;&gt;Stable Diffusion Dreambooth Discord&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;hugging-face-diffusers&#34;&gt;&lt;/a&gt; Hugging Face Diffusers - Alternate Option&lt;/h1&gt; &#xA;&lt;p&gt;Dreambooth is now supported in HuggingFace Diffusers for training with Stable Diffusion.&lt;/p&gt; &#xA;&lt;p&gt;Try it out here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rasbt/python-machine-learning-book-3rd-edition</title>
    <updated>2022-11-01T01:56:25Z</updated>
    <id>tag:github.com,2022-11-01:/rasbt/python-machine-learning-book-3rd-edition</id>
    <link href="https://github.com/rasbt/python-machine-learning-book-3rd-edition" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The &#34;Python Machine Learning (3rd edition)&#34; book code repository&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Python Machine Learning (3rd Ed.) Code Repository&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/#&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.7-blue.svg?sanitize=true&#34; alt=&#34;Python 3.6&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-MIT-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Code repositories for the 1st and 2nd edition are available at&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rasbt/python-machine-learning-book&#34;&gt;https://github.com/rasbt/python-machine-learning-book&lt;/a&gt; and&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rasbt/python-machine-learning-book-2nd-edition&#34;&gt;https://github.com/rasbt/python-machine-learning-book-2nd-edition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Python Machine Learning, 3rd Ed.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;to be published December 12th, 2019&lt;/p&gt; &#xA;&lt;p&gt;Paperback: 770 pages&lt;br&gt; Publisher: Packt Publishing&lt;br&gt; Language: English&lt;/p&gt; &#xA;&lt;p&gt;ISBN-10: 1789955750&lt;br&gt; ISBN-13: 978-1789955750&lt;br&gt; Kindle ASIN: B07VBLX2W7&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1789955750/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/.other/cover_1.jpg&#34; width=&#34;248&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1789955750/&#34;&gt;Amazon Page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.packtpub.com/data/python-machine-learning-third-edition&#34;&gt;Packt Page&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents and Code Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Helpful installation and setup instructions can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch01/README.md&#34;&gt;README.md file of Chapter 1&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note that these are just the code examples accompanying the book, which we uploaded for your convenience; be aware that these notebooks may not be useful without the formulae and descriptive text.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Machine Learning - Giving Computers the Ability to Learn from Data [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch01&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Training Machine Learning Algorithms for Classification [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch02&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;A Tour of Machine Learning Classifiers Using Scikit-Learn [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch03&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Building Good Training Sets – Data Pre-Processing [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch04&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Compressing Data via Dimensionality Reduction [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch05&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Learning Best Practices for Model Evaluation and Hyperparameter Optimization [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch06&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Combining Different Models for Ensemble Learning [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch07&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Applying Machine Learning to Sentiment Analysis [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch08&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Embedding a Machine Learning Model into a Web Application [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch09&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Predicting Continuous Target Variables with Regression Analysis [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch10&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Working with Unlabeled Data – Clustering Analysis [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch11&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Implementing a Multi-layer Artificial Neural Network from Scratch [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch12&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Parallelizing Neural Network Training with TensorFlow [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch13&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Going Deeper: The Mechanics of TensorFlow [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch14&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Classifying Images with Deep Convolutional Neural Networks [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch15&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Modeling Sequential Data Using Recurrent Neural Networks [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch16&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Generative Adversarial Networks for Synthesizing New Data [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch17&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Reinforcement Learning for Decision Making in Complex Environments [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch18&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Raschka, Sebastian, and Vahid Mirjalili. &lt;em&gt;Python Machine Learning, 3rd Ed&lt;/em&gt;. Packt Publishing, 2019.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@book{RaschkaMirjalili2019,  &#xA;address = {Birmingham, UK},  &#xA;author = {Raschka, Sebastian and Mirjalili, Vahid},  &#xA;edition = {3},  &#xA;isbn = {978-1789955750},   &#xA;publisher = {Packt Publishing},  &#xA;title = {{Python Machine Learning, 3rd Ed.}},  &#xA;year = {2019}  &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>