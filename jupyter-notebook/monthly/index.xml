<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-01T02:14:19Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>karpathy/micrograd</title>
    <updated>2023-03-01T02:14:19Z</updated>
    <id>tag:github.com,2023-03-01:/karpathy/micrograd</id>
    <link href="https://github.com/karpathy/micrograd" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;micrograd&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/karpathy/micrograd/master/puppy.jpg&#34; alt=&#34;awww&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A tiny Autograd engine (with a bite! :)). Implements backpropagation (reverse-mode autodiff) over a dynamically built DAG and a small neural networks library on top of it with a PyTorch-like API. Both are tiny, with about 100 and 50 lines of code respectively. The DAG only operates over scalar values, so e.g. we chop up each neuron into all of its individual tiny adds and multiplies. However, this is enough to build up entire deep neural nets doing binary classification, as the demo notebook shows. Potentially useful for educational purposes.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install micrograd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example usage&lt;/h3&gt; &#xA;&lt;p&gt;Below is a slightly contrived example showing a number of possible supported operations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from micrograd.engine import Value&#xA;&#xA;a = Value(-4.0)&#xA;b = Value(2.0)&#xA;c = a + b&#xA;d = a * b + b**3&#xA;c += c + 1&#xA;c += 1 + c + (-a)&#xA;d += d * 2 + (b + a).relu()&#xA;d += 3 * d + (b - a).relu()&#xA;e = c - d&#xA;f = e**2&#xA;g = f / 2.0&#xA;g += 10.0 / f&#xA;print(f&#39;{g.data:.4f}&#39;) # prints 24.7041, the outcome of this forward pass&#xA;g.backward()&#xA;print(f&#39;{a.grad:.4f}&#39;) # prints 138.8338, i.e. the numerical value of dg/da&#xA;print(f&#39;{b.grad:.4f}&#39;) # prints 645.5773, i.e. the numerical value of dg/db&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training a neural net&lt;/h3&gt; &#xA;&lt;p&gt;The notebook &lt;code&gt;demo.ipynb&lt;/code&gt; provides a full demo of training an 2-layer neural network (MLP) binary classifier. This is achieved by initializing a neural net from &lt;code&gt;micrograd.nn&lt;/code&gt; module, implementing a simple svm &#34;max-margin&#34; binary classification loss and using SGD for optimization. As shown in the notebook, using a 2-layer neural net with two 16-node hidden layers we achieve the following decision boundary on the moon dataset:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/karpathy/micrograd/master/moon_mlp.png&#34; alt=&#34;2d neuron&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Tracing / visualization&lt;/h3&gt; &#xA;&lt;p&gt;For added convenience, the notebook &lt;code&gt;trace_graph.ipynb&lt;/code&gt; produces graphviz visualizations. E.g. this one below is of a simple 2D neuron, arrived at by calling &lt;code&gt;draw_dot&lt;/code&gt; on the code below, and it shows both the data (left number in each node) and the gradient (right number in each node).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from micrograd import nn&#xA;n = nn.Neuron(2)&#xA;x = [Value(1.0), Value(-2.0)]&#xA;y = n(x)&#xA;dot = draw_dot(y)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/karpathy/micrograd/master/gout.svg?sanitize=true&#34; alt=&#34;2d neuron&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Running tests&lt;/h3&gt; &#xA;&lt;p&gt;To run the unit tests you will have to install &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;, which the tests use as a reference for verifying the correctness of the calculated gradients. Then simply:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pytest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>llSourcell/ChatGPT_Trading_Bot</title>
    <updated>2023-03-01T02:14:19Z</updated>
    <id>tag:github.com,2023-03-01:/llSourcell/ChatGPT_Trading_Bot</id>
    <link href="https://github.com/llSourcell/ChatGPT_Trading_Bot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the code for the &#34;ChatGPT Trading Bot&#34; Video by Siraj Raval on Youtube&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGPT_Trading_Bot&lt;/h1&gt; &#xA;&lt;p&gt;This is the code for the &#34;ChatGPT Trading Bot&#34; Video by Siraj Raval on Youtube&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;This is the code for this video on Youtube by Siraj Raval on building a ChatGPT trading bot. First, a disclaimer - Do NOT invest any money in any type of trading bot or algorithmic engine that you are not willing to lose. I gave this trading bot $2000 because I was willing to lose $2000 to make a great video for my AI Wizards out there. The entire codebase is contained in a single iPython notebook file, first published by the FinRL team as an example. Inside the notebook, 5 steps are performed.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pull 30 days of trading data for (Insert your stock or crypto) with Yahoo Finance Downloader API&lt;/li&gt; &#xA; &lt;li&gt;Create a simulated trading environment using real trading data with FinRL&lt;/li&gt; &#xA; &lt;li&gt;Train an neural network to predict that Stock Price using reinforcement learning inside this simulation with FinRL&lt;/li&gt; &#xA; &lt;li&gt;Once trained, backtest the predictions on the past 30 days data to compute potential returns with FinRL&lt;/li&gt; &#xA; &lt;li&gt;If the expectd returns are above a certain threshold, buy, else hold. If they&#39;re below a certain threshold, sell. (using Alpaca API)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;In order to have this Colab run automatically once a day, we can deploy it to a hosting platform like Vercel with a seperate file that repeatedly executes it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/4KJx9y0/Screen-Shot-2023-01-13-at-10-04-39-AM.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python 3.7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://alpaca.markets/&#34;&gt;Alpaca SDK&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinRL&#34;&gt;FinRL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vercel.com&#34;&gt;Vercel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/llSourcell/firebase_react_startup_template&#34;&gt;Firebase Template&lt;/a&gt; &lt;em&gt;optional&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup Instructions&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the iPython notebook in this repository and upload it to &lt;a href=&#34;https://raw.githubusercontent.com/llSourcell/ChatGPT_Trading_Bot/main/colab.research.google.com&#34;&gt;Colab&lt;/a&gt; to try it out.&lt;/li&gt; &#xA; &lt;li&gt;Setup a simple &lt;a href=&#34;https://flask.palletsprojects.com/en/1.1.x/quickstart/&#34;&gt;flask&lt;/a&gt; app.&lt;/li&gt; &#xA; &lt;li&gt;To set up a cron job for a Flask app deployed on Vercel that executes a Google Colab notebook at a given link every hour, you can use the built-in Vercel cron feature. Here are the steps to follow:&lt;/li&gt; &#xA; &lt;li&gt;In your Flask app, import the necessary modules to run the Colab notebook, such as gdown or pyngrok&lt;/li&gt; &#xA; &lt;li&gt;Create a new endpoint in your Flask app that triggers the execution of the Colab notebook, using the link to the notebook file.&lt;/li&gt; &#xA; &lt;li&gt;Go to the Vercel project settings for your app and navigate to the &#34;Cron&#34; tab.&lt;/li&gt; &#xA; &lt;li&gt;Create a new cron job that runs every hour by adding the endpoint you created in step 2 to the &#34;Cron Job&#34; field and select the frequency you want to run the job.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Here is a sample code snippet for step 2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from flask import Flask, jsonify&#xA;import gdown&#xA;app = Flask(__name__)&#xA;&#xA;@app.route(&#39;/run-colab&#39;)&#xA;def run_colab():&#xA;    gdown.download(&#39;https://drive.google.com/file/d/&amp;lt;colab_notebook_id&amp;gt;&#39;, &#39;colab.ipynb&#39;, quiet=False)&#xA;    return jsonify(message=&#39;colab notebook ran successfully&#39;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits &amp;amp; More Resources&lt;/h2&gt; &#xA;&lt;p&gt;Credits for the notebook go to the AI4FinanceFoundation, and for the API go to Alpaca.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>karpathy/nn-zero-to-hero</title>
    <updated>2023-03-01T02:14:19Z</updated>
    <id>tag:github.com,2023-03-01:/karpathy/nn-zero-to-hero</id>
    <link href="https://github.com/karpathy/nn-zero-to-hero" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Neural Networks: Zero to Hero&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Neural Networks: Zero to Hero&lt;/h2&gt; &#xA;&lt;p&gt;A course on neural networks that starts all the way at the basics. The course is a series of YouTube videos where we code and train neural networks together. The Jupyter notebooks we build in the videos are then captured here inside the &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/&#34;&gt;lectures&lt;/a&gt; directory. Every lecture also has a set of exercises included in the video description. (This may grow into something more respectable).&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lecture 1: The spelled-out intro to neural networks and backpropagation: building micrograd&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Backpropagation and training of neural networks. Assumes basic knowledge of Python and a vague recollection of calculus from high school.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VMj-3S1tku0&#34;&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/micrograd&#34;&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/micrograd&#34;&gt;micrograd Github repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lecture 2: The spelled-out intro to language modeling: building makemore&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We implement a bigram character-level language model, which we will further complexify in followup videos into a modern Transformer language model, like GPT. In this video, the focus is on (1) introducing torch.Tensor and its subtleties and use in efficiently evaluating neural networks and (2) the overall framework of language modeling that includes model training, sampling, and the evaluation of a loss (e.g. the negative log likelihood for classification).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=PaCmpygFfXo&#34;&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/makemore/makemore_part1_bigrams.ipynb&#34;&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/makemore&#34;&gt;makemore Github repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lecture 3: Building makemore Part 2: MLP&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We implement a multilayer perceptron (MLP) character-level language model. In this video we also introduce many basics of machine learning (e.g. model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/TCH_1BHY58I&#34;&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/makemore/makemore_part2_mlp.ipynb&#34;&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/makemore&#34;&gt;makemore Github repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lecture 4: Building makemore Part 3: Activations &amp;amp; Gradients, BatchNorm&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We dive into some of the internals of MLPs with multiple layers and scrutinize the statistics of the forward pass activations, backward pass gradients, and some of the pitfalls when they are improperly scaled. We also look at the typical diagnostic tools and visualizations you&#39;d want to use to understand the health of your deep network. We learn why training deep neural nets can be fragile and introduce the first modern innovation that made doing so much easier: Batch Normalization. Residual connections and the Adam optimizer remain notable todos for later video.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/P6sfmUTpUmc&#34;&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/makemore/makemore_part3_bn.ipynb&#34;&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/makemore&#34;&gt;makemore Github repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lecture 5: Building makemore Part 4: Becoming a Backprop Ninja&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We take the 2-layer MLP (with BatchNorm) from the previous video and backpropagate through it manually without using PyTorch autograd&#39;s loss.backward(). That is, we backprop through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table. Along the way, we get an intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in micrograd. This helps build competence and intuition around how neural nets are optimized and sets you up to more confidently innovate on and debug modern neural networks.&lt;/p&gt; &#xA;&lt;p&gt;I recommend you work through the exercise yourself but work with it in tandem and whenever you are stuck unpause the video and see me give away the answer. This video is not super intended to be simply watched. The exercise is &lt;a href=&#34;https://colab.research.google.com/drive/1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-?usp=sharing&#34;&gt;here as a Google Colab&lt;/a&gt;. Good luck :)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/q8SA3rM6ckI&#34;&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/makemore/makemore_part4_backprop.ipynb&#34;&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/makemore&#34;&gt;makemore Github repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lecture 6: Building makemore Part 5: Building WaveNet&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We take the 2-layer MLP from previous video and make it deeper with a tree-like structure, arriving at a convolutional neural network architecture similar to the WaveNet (2016) from DeepMind. In the WaveNet paper, the same hierarchical architecture is implemented more efficiently using causal dilated convolutions (not yet covered). Along the way we get a better sense of torch.nn and what it is and how it works under the hood, and what a typical deep learning development process looks like (a lot of reading of documentation, keeping track of multidimensional tensor shapes, moving between jupyter notebooks and repository code, ...).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/t3YJ5hKiMQ0&#34;&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/makemore/makemore_part5_cnn1.ipynb&#34;&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lecture 7: Let&#39;s build GPT: from scratch, in code, spelled out.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We build a Generatively Pretrained Transformer (GPT), following the paper &#34;Attention is All You Need&#34; and OpenAI&#39;s GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kCc8FmEb1nY&#34;&gt;YouTube video lecture&lt;/a&gt;. For all other links see the video description.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Ongoing...&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
</feed>