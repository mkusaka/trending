<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-01T01:46:44Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>KindXiaoming/pykan</title>
    <updated>2024-06-01T01:46:44Z</updated>
    <id>tag:github.com,2024-06-01:/KindXiaoming/pykan</id>
    <link href="https://github.com/KindXiaoming/pykan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kolmogorov Arnold Networks&lt;/p&gt;&lt;hr&gt;&lt;img width=&#34;600&#34; alt=&#34;kan_plot&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/a2d2d225-b4d2-4c1e-823e-bc45c7ea96f9&#34;&gt; &#xA;&lt;h1&gt;Kolmogorov-Arnold Networks (KANs)&lt;/h1&gt; &#xA;&lt;p&gt;This is the github repo for the paper &lt;a href=&#34;https://arxiv.org/abs/2404.19756&#34;&gt;&#34;KAN: Kolmogorov-Arnold Networks&#34;&lt;/a&gt;. Find the documentation &lt;a href=&#34;https://kindxiaoming.github.io/pykan/&#34;&gt;here&lt;/a&gt;. Here&#39;s &lt;a href=&#34;https://github.com/KindXiaoming/pykan?tab=readme-ov-file#authors-note&#34;&gt;author&#39;s note&lt;/a&gt; responding to current hype of KANs.&lt;/p&gt; &#xA;&lt;p&gt;Kolmogorov-Arnold Networks (KANs) are promising alternatives of Multi-Layer Perceptrons (MLPs). KANs have strong mathematical foundations just like MLPs: MLPs are based on the universal approximation theorem, while KANs are based on Kolmogorov-Arnold representation theorem. KANs and MLPs are dual: KANs have activation functions on edges, while MLPs have activation functions on nodes. This simple change makes KANs better (sometimes much better!) than MLPs in terms of both model &lt;strong&gt;accuracy&lt;/strong&gt; and &lt;strong&gt;interpretability&lt;/strong&gt;. A quick intro of KANs &lt;a href=&#34;https://kindxiaoming.github.io/pykan/intro.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img width=&#34;1163&#34; alt=&#34;mlp_kan_compare&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/695adc2d-0d0b-4e4b-bcff-db2c8070f841&#34;&gt; &#xA;&lt;h2&gt;Accuracy&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;KANs have faster scaling than MLPs. KANs have better accuracy than MLPs with fewer parameters.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please set &lt;code&gt;torch.set_default_dtype(torch.float64)&lt;/code&gt; if you want high precision.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 1: fitting symbolic formulas&lt;/strong&gt; &lt;img width=&#34;1824&#34; alt=&#34;Screenshot 2024-04-30 at 10 55 30&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/e1fc3dcc-c1f6-49d5-b58e-79ff7b98a49b&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 2: fitting special functions&lt;/strong&gt; &lt;img width=&#34;1544&#34; alt=&#34;Screenshot 2024-04-30 at 11 07 20&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/b2124337-cabf-4e00-9690-938e84058a91&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 3: PDE solving&lt;/strong&gt; &lt;img width=&#34;1665&#34; alt=&#34;Screenshot 2024-04-30 at 10 57 25&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/5da94412-c409-45d1-9a60-9086e11d6ccc&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 4: avoid catastrophic forgetting&lt;/strong&gt; &lt;img width=&#34;1652&#34; alt=&#34;Screenshot 2024-04-30 at 11 04 36&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/57d81de6-7cff-4e55-b8f9-c4768ace2c77&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Interpretability&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;KANs can be intuitively visualized. KANs offer interpretability and interactivity that MLPs cannot provide. We can use KANs to potentially discover new scientific laws.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 1: Symbolic formulas&lt;/strong&gt; &lt;img width=&#34;1510&#34; alt=&#34;Screenshot 2024-04-30 at 11 04 56&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/3cfd1ca2-cd3e-4396-845e-ef8f3a7c55ef&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 2: Discovering mathematical laws of knots&lt;/strong&gt; &lt;img width=&#34;1443&#34; alt=&#34;Screenshot 2024-04-30 at 11 05 25&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/80451ac2-c5fd-45b9-89a7-1637ba8145af&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 3: Discovering physical laws of Anderson localization&lt;/strong&gt; &lt;img width=&#34;1295&#34; alt=&#34;Screenshot 2024-04-30 at 11 05 53&#34; src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/8ee507a0-d194-44a9-8837-15d7f5984301&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 4: Training of a three-layer KAN&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/KindXiaoming/pykan/assets/23551623/e9f215c7-a393-46b9-8528-c906878f015e&#34; alt=&#34;kan_training_low_res&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Pykan can be installed via PyPI or directly from GitHub.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pre-requisites:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Python 3.9.7 or higher&#xA;pip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Installation via github&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m venv pykan-env&#xA;source pykan-env/bin/activate  # On Windows use `pykan-env\Scripts\activate`&#xA;pip install git+https://github.com/KindXiaoming/pykan.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Installation via PyPI:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m venv pykan-env&#xA;source pykan-env/bin/activate  # On Windows use `pykan-env\Scripts\activate`&#xA;pip install pykan&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Requirements&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# python==3.9.7&#xA;matplotlib==3.6.2&#xA;numpy==1.24.4&#xA;scikit_learn==1.1.3&#xA;setuptools==65.5.0&#xA;sympy==1.11.1&#xA;torch==2.2.2&#xA;tqdm==4.66.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After activating the virtual environment, you can install specific package requirements as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optional: Conda Environment Setup&lt;/strong&gt; For those who prefer using Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create --name pykan-env python=3.9.7&#xA;conda activate pykan-env&#xA;pip install git+https://github.com/KindXiaoming/pykan.git  # For GitHub installation&#xA;# or&#xA;pip install pykan  # For PyPI installation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Computation requirements&lt;/h2&gt; &#xA;&lt;p&gt;Examples in &lt;a href=&#34;https://raw.githubusercontent.com/KindXiaoming/pykan/master/tutorials&#34;&gt;tutorials&lt;/a&gt; are runnable on a single CPU typically less than 10 minutes. All examples in the paper are runnable on a single CPU in less than one day. Training KANs for PDE is the most expensive and may take hours to days on a single CPU. We use CPUs to train our models because we carried out parameter sweeps (both for MLPs and KANs) to obtain Pareto Frontiers. There are thousands of small models which is why we use CPUs rather than GPUs. Admittedly, our problem scales are smaller than typical machine learning tasks, but are typical for science-related tasks. In case the scale of your task is large, it is advisable to use GPUs.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The documentation can be found &lt;a href=&#34;https://kindxiaoming.github.io/pykan/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Get started with &lt;a href=&#34;https://raw.githubusercontent.com/KindXiaoming/pykan/master/hellokan.ipynb&#34;&gt;hellokan.ipynb&lt;/a&gt; notebook.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;More demos&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;More Notebook tutorials can be found in &lt;a href=&#34;https://raw.githubusercontent.com/KindXiaoming/pykan/master/tutorials&#34;&gt;tutorials&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Advice on hyperparameter tuning&lt;/h2&gt; &#xA;&lt;p&gt;Many intuition about MLPs and other networks may not directy transfer to KANs. So how can I tune the hyperparameters effectively? Here is my general advice based on my experience playing with the problems reported in the paper. Since these problems are relatively small-scale and science-oriented, it is likely that my advice is not suitable to your case. But I want to at least share my experience such that users can have better clues where to start and what to expect from tuning hyperparameters.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Start from a simple setup (small KAN shape, small grid size, small data, no reguralization &lt;code&gt;lamb=0&lt;/code&gt;). This is very different from MLP literature, where people by default use widths of order &lt;code&gt;O(10^2)&lt;/code&gt; or higher. For example, if you have a task with 5 inputs and 1 outputs, I would try something as simple as &lt;code&gt;KAN(width=[5,1,1], grid=3, k=3)&lt;/code&gt;. If it doesn&#39;t work, I would gradually first increase width. If that still doesn&#39;t work, I would consider increasing depth. You don&#39;t need to be this extreme, if you have better understanding about the complexity of your task.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once an acceptable performance is achieved, you could then try refining your KAN (more accurate or more interpretable).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you care about accuracy, try grid extention technique. An example is &lt;a href=&#34;https://kindxiaoming.github.io/pykan/Examples/Example_1_function_fitting.html&#34;&gt;here&lt;/a&gt;. But watch out for overfitting, see below.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you care about interpretability, try sparsifying the network with, e.g., &lt;code&gt;model.train(lamb=0.01)&lt;/code&gt;. It would also be advisable to try increasing lamb gradually. After training with sparsification, plot it, if you see some neurons that are obvious useless, you may call &lt;code&gt;pruned_model = model.prune()&lt;/code&gt; to get the pruned model. You can then further train (either to encourage accuracy or encouarge sparsity), or do symbolic regression.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;I also want to emphasize that accuracy and interpretability (and also parameter efficiency) are not necessarily contradictory, e.g., Figure 2.3 in &lt;a href=&#34;https://arxiv.org/pdf/2404.19756&#34;&gt;our paper&lt;/a&gt;. They can be positively correlated in some cases but in other cases may dispaly some tradeoff. So it would be good not to be greedy and aim for one goal at a time. However, if you have a strong reason why you believe pruning (interpretability) can also help accuracy, you may want to plan ahead, such that even if your end goal is accuracy, you want to push interpretability first.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once you get a quite good result, try increasing data size and have a final run, which should give you even better results!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Disclaimer: Try the simplest thing first is the mindset of physicists, which could be personal/biased but I find this mindset quite effective and make things well-controlled for me. Also, The reason why I tend to choose a small dataset at first is to get faster feedback in the debugging stage (my initial implementation is slow, after all!). The hidden assumption is that a small dataset behaves qualitatively similar to a large dataset, which is not necessarily true in general, but usually true in small-scale problems that I have tried. To know if your data is sufficient, see the next paragraph.&lt;/p&gt; &#xA;&lt;p&gt;Another thing that would be good to keep in mind is that please constantly checking if your model is in underfitting or overfitting regime. If there is a large gap between train/test losses, you probably want to increase data or reduce model (&lt;code&gt;grid&lt;/code&gt; is more important than &lt;code&gt;width&lt;/code&gt;, so first try decreasing &lt;code&gt;grid&lt;/code&gt;, then &lt;code&gt;width&lt;/code&gt;). This is also the reason why I&#39;d love to start from simple models to make sure that the model is first in underfitting regime and then gradually expands to the &#34;Goldilocks zone&#34;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@article{liu2024kan,&#xA;  title={KAN: Kolmogorov-Arnold Networks},&#xA;  author={Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Solja{\v{c}}i{\&#39;c}, Marin and Hou, Thomas Y and Tegmark, Max},&#xA;  journal={arXiv preprint arXiv:2404.19756},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please contact &lt;a href=&#34;mailto:zmliu@mit.edu&#34;&gt;zmliu@mit.edu&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Author&#39;s note&lt;/h2&gt; &#xA;&lt;p&gt;I would like to thank everyone who&#39;s interested in KANs. When I designed KANs and wrote codes, I have math &amp;amp; physics examples (which are quite small scale!) in mind, so did not consider much optimization in efficiency or reusability. It&#39;s so honored to receive this unwarranted attention, which is way beyond my expectation. So I accept any criticism from people complaning about the efficiency and resuability of the codes, my apology. My only hope is that you find &lt;code&gt;model.plot()&lt;/code&gt; fun to play with :).&lt;/p&gt; &#xA;&lt;p&gt;For users who are interested in scientific discoveries and scientific computing (the orginal users intended for), I&#39;m happy to hear your applications and collaborate. This repo will continue remaining mostly for this purpose, probably without signifiant updates for efficiency. In fact, there are already implmentations like &lt;a href=&#34;https://github.com/Blealtan/efficient-kan&#34;&gt;efficientkan&lt;/a&gt; or &lt;a href=&#34;https://github.com/GistNoesis/FourierKAN/&#34;&gt;fouierkan&lt;/a&gt; that look promising for improving efficiency.&lt;/p&gt; &#xA;&lt;p&gt;For users who are machine learning focus, I have to be honest that KANs are likely not a simple plug-in that can be used out-of-the box (yet). Hyperparameters need tuning, and more tricks special to your applications should be introduced. For example, &lt;a href=&#34;https://github.com/WillHua127/GraphKAN-Graph-Kolmogorov-Arnold-Networks&#34;&gt;GraphKAN&lt;/a&gt; suggests that KANs should better be used in latent space (need embedding and unembedding linear layers after inputs and before outputs). &lt;a href=&#34;https://github.com/riiswa/kanrl&#34;&gt;KANRL&lt;/a&gt; suggests that some trainable parameters should better be fixed in reinforcement learning to increase training stability. The extra tricks required by KAN (e.g., grid updates and grid extension) beyond MLPs make it sometimes confusing on how to use them so we should be extra careful, e.g., &lt;a href=&#34;https://www.linkedin.com/feed/update/urn:li:activity:7196684191479070721/&#34;&gt;Prof. George Karniadakis&#39; post on LinkedIn&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/feed/update/urn:li:activity:7197097659017379840/&#34;&gt;my response&lt;/a&gt; is an example.&lt;/p&gt; &#xA;&lt;p&gt;The most common question I&#39;ve been asked lately is whether KANs will be next-gen LLMs. I don&#39;t have good intuition about this. KANs are designed for applications where one cares about high accuracy and/or interpretability. We do care about LLM interpretability for sure, but interpretability can mean wildly different things for LLM and for science. Do we care about high accuracy for LLMs? I don&#39;t know, scaling laws seem to imply so, but probably not too high precision. Also, accuracy can also mean different things for LLM and for science. This subtlety makes it hard to directly transfer conclusions in our paper to LLMs, or machine learning tasks in general. However, I would be very happy if you have enjoyed the high-level idea (learnable activation functions on edges, or interacting with AI for scientific discoveries), which is not necessariy &lt;em&gt;the future&lt;/em&gt;, but can hopefully inspire and impact &lt;em&gt;many possible futures&lt;/em&gt;. As a physicist, the message I want to convey is less of &#34;KANs are great&#34;, but more of &#34;try thinking of current architectures critically and seeking fundamentally different alternatives that can do fun and/or useful stuff&#34;.&lt;/p&gt; &#xA;&lt;p&gt;I would like to welcome people to be critical of KANs, but also to be critical of critiques as well. Practice is the only criterion for testing understanding (ÂÆûË∑µÊòØÊ£ÄÈ™åÁúüÁêÜÁöÑÂîØ‰∏ÄÊ†áÂáÜ). We don&#39;t know many things beforehand until they are really tried and shown to be succeeding or failing. As much as I&#39;m willing to see success mode of KANs, I&#39;m equally curious about failure modes of KANs, to better understand the boundaries. KANs and MLPs cannot replace each other (as far as I can tell); they each have advantages in some settings and limitations in others. I would be intrigued by a theoretical framework that encompasses both and could even suggest new alternatives (physicists love unified theories, sorry :).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mlabonne/llm-course</title>
    <updated>2024-06-01T01:46:44Z</updated>
    <id>tag:github.com,2024-06-01:/mlabonne/llm-course</id>
    <link href="https://github.com/mlabonne/llm-course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;üó£Ô∏è Large Language Model Course&lt;/h1&gt; &#xA; &lt;p align=&#34;center&#34;&gt; üê¶ &lt;a href=&#34;https://twitter.com/maximelabonne&#34;&gt;Follow me on X&lt;/a&gt; ‚Ä¢ ü§ó &lt;a href=&#34;https://huggingface.co/mlabonne&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ üíª &lt;a href=&#34;https://mlabonne.github.io/blog&#34;&gt;Blog&lt;/a&gt; ‚Ä¢ üìô &lt;a href=&#34;https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python&#34;&gt;Hands-on GNN&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;The LLM course is divided into three parts:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;üß© &lt;strong&gt;LLM Fundamentals&lt;/strong&gt; covers essential knowledge about mathematics, Python, and neural networks.&lt;/li&gt; &#xA; &lt;li&gt;üßë‚Äçüî¨ &lt;strong&gt;The LLM Scientist&lt;/strong&gt; focuses on building the best possible LLMs using the latest techniques.&lt;/li&gt; &#xA; &lt;li&gt;üë∑ &lt;strong&gt;The LLM Engineer&lt;/strong&gt; focuses on creating LLM-based applications and deploying them.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For an interactive version of this course, I created two &lt;strong&gt;LLM assistants&lt;/strong&gt; that will answer questions and test your knowledge in a personalized way:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ü§ó &lt;a href=&#34;https://hf.co/chat/assistant/66029d2e5f4a884f7aabc9d1&#34;&gt;&lt;strong&gt;HuggingChat Assistant&lt;/strong&gt;&lt;/a&gt;: Free version using Mixtral-8x7B.&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ &lt;a href=&#34;https://chat.openai.com/g/g-yviLuLqvI-llm-course&#34;&gt;&lt;strong&gt;ChatGPT Assistant&lt;/strong&gt;&lt;/a&gt;: Requires a premium account.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìù Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;A list of notebooks and articles related to large language models.&lt;/p&gt; &#xA;&lt;h3&gt;Tools&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üßê &lt;a href=&#34;https://github.com/mlabonne/llm-autoeval&#34;&gt;LLM AutoEval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Automatically evaluate your LLMs using RunPod&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ü•± LazyMergekit&lt;/td&gt; &#xA;   &lt;td&gt;Easily merge models using MergeKit in one click.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ü¶é LazyAxolotl&lt;/td&gt; &#xA;   &lt;td&gt;Fine-tune models in the cloud using Axolotl in one click.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;‚ö° AutoQuant&lt;/td&gt; &#xA;   &lt;td&gt;Quantize LLMs in GGUF, GPTQ, EXL2, AWQ, and HQQ formats in one click.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üå≥ Model Family Tree&lt;/td&gt; &#xA;   &lt;td&gt;Visualize the family tree of merged models.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1s2eQlolcI1VGgDhqWIANfkfKvcKrMyNr?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üöÄ ZeroSpace&lt;/td&gt; &#xA;   &lt;td&gt;Automatically create a Gradio chat interface using a free ZeroGPU.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Fine-tuning&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Article&lt;/th&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fine-tune Llama 2 with SFT&lt;/td&gt; &#xA;   &lt;td&gt;Step-by-step guide to supervised fine-tune Llama 2 in Google Colab.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fine-tune CodeLlama using Axolotl&lt;/td&gt; &#xA;   &lt;td&gt;End-to-end guide to the state-of-the-art tool for fine-tuning.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Xu0BrCB7IShwSWKVcfAfhehwjDrDMH5m?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fine-tune Mistral-7b with SFT&lt;/td&gt; &#xA;   &lt;td&gt;Supervised fine-tune Mistral-7b in a free-tier Google Colab with TRL.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1o_w0KastmEJNVwT5GoqMCciH-18ca5WS?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fine-tune Mistral-7b with DPO&lt;/td&gt; &#xA;   &lt;td&gt;Boost the performance of supervised fine-tuned models with DPO.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fine-tune Llama 3 with ORPO&lt;/td&gt; &#xA;   &lt;td&gt;Cheaper and faster fine-tuning in a single stage with ORPO.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/2024-04-19_Fine_tune_Llama_3_with_ORPO.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Article&lt;/th&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1. Introduction to Quantization&lt;/td&gt; &#xA;   &lt;td&gt;Large language model optimization using 8-bit quantization.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2. 4-bit Quantization using GPTQ&lt;/td&gt; &#xA;   &lt;td&gt;Quantize your own open-source LLMs to run them on consumer hardware.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/4bit_quantization/&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3. Quantization with GGUF and llama.cpp&lt;/td&gt; &#xA;   &lt;td&gt;Quantize Llama 2 models with llama.cpp and upload GGUF versions to the HF Hub.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4. ExLlamaV2: The Fastest Library to Run&amp;nbsp;LLMs&lt;/td&gt; &#xA;   &lt;td&gt;Quantize and run EXL2&amp;nbsp;models and upload them to the HF Hub.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Other&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Article&lt;/th&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Decoding Strategies in Large Language Models&lt;/td&gt; &#xA;   &lt;td&gt;A guide to text generation from beam search to nucleus sampling&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/2022-06-07-Decoding_strategies.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Improve ChatGPT with Knowledge Graphs&lt;/td&gt; &#xA;   &lt;td&gt;Augment ChatGPT&#39;s answers with knowledge graphs.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1mwhOSw9Y9bgEaIFKT4CLi0n18pXRM4cj?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Merge LLMs with MergeKit&lt;/td&gt; &#xA;   &lt;td&gt;Create your own models easily, no GPU required!&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit%20copy.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Create MoEs with MergeKit&lt;/td&gt; &#xA;   &lt;td&gt;Combine multiple experts into a single frankenMoE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/2024-03-28_Create_Mixture_of_Experts_with_MergeKit.html&#34;&gt;Article&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üß© LLM Fundamentals&lt;/h2&gt; &#xA;&lt;p&gt;This section introduces essential knowledge about mathematics, Python, and neural networks. You might not want to start here, but refer to it as needed.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Toggle section&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/roadmap_fundamentals.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;h3&gt;1. Mathematics for Machine Learning&lt;/h3&gt; &#xA; &lt;p&gt;Before mastering machine learning, it is important to understand the fundamental mathematical concepts that power these algorithms.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Linear Algebra&lt;/strong&gt;: This is crucial for understanding many algorithms, especially those used in deep learning. Key concepts include vectors, matrices, determinants, eigenvalues and eigenvectors, vector spaces, and linear transformations.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Calculus&lt;/strong&gt;: Many machine learning algorithms involve the optimization of continuous functions, which requires an understanding of derivatives, integrals, limits, and series. Multivariable calculus and the concept of gradients are also important.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Probability and Statistics&lt;/strong&gt;: These are crucial for understanding how models learn from data and make predictions. Key concepts include probability theory, random variables, probability distributions, expectations, variance, covariance, correlation, hypothesis testing, confidence intervals, maximum likelihood estimation, and Bayesian inference.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;üìö Resources:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&#34;&gt;3Blue1Brown - The Essence of Linear Algebra&lt;/a&gt;: Series of videos that give a geometric intuition to these concepts.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qBigTkBLU6g&amp;amp;list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&#34;&gt;StatQuest with Josh Starmer - Statistics Fundamentals&lt;/a&gt;: Offers simple and clear explanations for many statistical concepts.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://automata88.medium.com/list/cacc224d5e7d&#34;&gt;AP Statistics Intuition by Ms Aerin&lt;/a&gt;: List of Medium articles that provide the intuition behind every probability distribution.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://immersivemath.com/ila/learnmore.html&#34;&gt;Immersive Linear Algebra&lt;/a&gt;: Another visual interpretation of linear algebra.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.khanacademy.org/math/linear-algebra&#34;&gt;Khan Academy - Linear Algebra&lt;/a&gt;: Great for beginners as it explains the concepts in a very intuitive way.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.khanacademy.org/math/calculus-1&#34;&gt;Khan Academy - Calculus&lt;/a&gt;: An interactive course that covers all the basics of calculus.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.khanacademy.org/math/statistics-probability&#34;&gt;Khan Academy - Probability and Statistics&lt;/a&gt;: Delivers the material in an easy-to-understand format.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;hr&gt; &#xA; &lt;h3&gt;2. Python for Machine Learning&lt;/h3&gt; &#xA; &lt;p&gt;Python is a powerful and flexible programming language that&#39;s particularly good for machine learning, thanks to its readability, consistency, and robust ecosystem of data science libraries.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Python Basics&lt;/strong&gt;: Python programming requires a good understanding of the basic syntax, data types, error handling, and object-oriented programming.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Data Science Libraries&lt;/strong&gt;: It includes familiarity with NumPy for numerical operations, Pandas for data manipulation and analysis, Matplotlib and Seaborn for data visualization.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Data Preprocessing&lt;/strong&gt;: This involves feature scaling and normalization, handling missing data, outlier detection, categorical data encoding, and splitting data into training, validation, and test sets.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Machine Learning Libraries&lt;/strong&gt;: Proficiency with Scikit-learn, a library providing a wide selection of supervised and unsupervised learning algorithms, is vital. Understanding how to implement algorithms like linear regression, logistic regression, decision trees, random forests, k-nearest neighbors (K-NN), and K-means clustering is important. Dimensionality reduction techniques like PCA and t-SNE are also helpful for visualizing high-dimensional data.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;üìö Resources:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://realpython.com/&#34;&gt;Real Python&lt;/a&gt;: A comprehensive resource with articles and tutorials for both beginner and advanced Python concepts.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=rfscVS0vtbw&#34;&gt;freeCodeCamp - Learn Python&lt;/a&gt;: Long video that provides a full introduction into all of the core concepts in Python.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://jakevdp.github.io/PythonDataScienceHandbook/&#34;&gt;Python Data Science Handbook&lt;/a&gt;: Free digital book that is a great resource for learning pandas, NumPy, Matplotlib, and Seaborn.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://youtu.be/i_LwzRVP7bg&#34;&gt;freeCodeCamp - Machine Learning for Everybody&lt;/a&gt;: Practical introduction to different machine learning algorithms for beginners.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning--ud120&#34;&gt;Udacity - Intro to Machine Learning&lt;/a&gt;: Free course that covers PCA and several other machine learning concepts.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;hr&gt; &#xA; &lt;h3&gt;3. Neural Networks&lt;/h3&gt; &#xA; &lt;p&gt;Neural networks are a fundamental part of many machine learning models, particularly in the realm of deep learning. To utilize them effectively, a comprehensive understanding of their design and mechanics is essential.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Fundamentals&lt;/strong&gt;: This includes understanding the structure of a neural network such as layers, weights, biases, and activation functions (sigmoid, tanh, ReLU, etc.)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Training and Optimization&lt;/strong&gt;: Familiarize yourself with backpropagation and different types of loss functions, like Mean Squared Error (MSE) and Cross-Entropy. Understand various optimization algorithms like Gradient Descent, Stochastic Gradient Descent, RMSprop, and Adam.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Overfitting&lt;/strong&gt;: Understand the concept of overfitting (where a model performs well on training data but poorly on unseen data) and learn various regularization techniques (dropout, L1/L2 regularization, early stopping, data augmentation) to prevent it.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Implement a Multilayer Perceptron (MLP)&lt;/strong&gt;: Build an MLP, also known as a fully connected network, using PyTorch.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;üìö Resources:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=aircAruvnKk&#34;&gt;3Blue1Brown - But what is a Neural Network?&lt;/a&gt;: This video gives an intuitive explanation of neural networks and their inner workings.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VyWAvY2CF9c&#34;&gt;freeCodeCamp - Deep Learning Crash Course&lt;/a&gt;: This video efficiently introduces all the most important concepts in deep learning.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://course.fast.ai/&#34;&gt;Fast.ai - Practical Deep Learning&lt;/a&gt;: Free course designed for people with coding experience who want to learn about deep learning.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&#34;&gt;Patrick Loeber - PyTorch Tutorials&lt;/a&gt;: Series of videos for complete beginners to learn about PyTorch.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;hr&gt; &#xA; &lt;h3&gt;4. Natural Language Processing (NLP)&lt;/h3&gt; &#xA; &lt;p&gt;NLP is a fascinating branch of artificial intelligence that bridges the gap between human language and machine understanding. From simple text processing to understanding linguistic nuances, NLP plays a crucial role in many applications like translation, sentiment analysis, chatbots, and much more.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Text Preprocessing&lt;/strong&gt;: Learn various text preprocessing steps like tokenization (splitting text into words or sentences), stemming (reducing words to their root form), lemmatization (similar to stemming but considers the context), stop word removal, etc.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Feature Extraction Techniques&lt;/strong&gt;: Become familiar with techniques to convert text data into a format that can be understood by machine learning algorithms. Key methods include Bag-of-words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and n-grams.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Word Embeddings&lt;/strong&gt;: Word embeddings are a type of word representation that allows words with similar meanings to have similar representations. Key methods include Word2Vec, GloVe, and FastText.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Recurrent Neural Networks (RNNs)&lt;/strong&gt;: Understand the working of RNNs, a type of neural network designed to work with sequence data. Explore LSTMs and GRUs, two RNN variants that are capable of learning long-term dependencies.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;üìö Resources:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://realpython.com/natural-language-processing-spacy-python/&#34;&gt;RealPython - NLP with spaCy in Python&lt;/a&gt;: Exhaustive guide about the spaCy library for NLP tasks in Python.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/learn-guide/natural-language-processing&#34;&gt;Kaggle - NLP Guide&lt;/a&gt;: A few notebooks and resources for a hands-on explanation of NLP in Python.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-word2vec/&#34;&gt;Jay Alammar - The Illustration Word2Vec&lt;/a&gt;: A good reference to understand the famous Word2Vec architecture.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://jaketae.github.io/study/pytorch-rnn/&#34;&gt;Jake Tae - PyTorch RNN from Scratch&lt;/a&gt;: Practical and simple implementation of RNN, LSTM, and GRU models in PyTorch.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34;&gt;colah&#39;s blog - Understanding LSTM Networks&lt;/a&gt;: A more theoretical article about the LSTM network.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üßë‚Äçüî¨ The LLM Scientist&lt;/h2&gt; &#xA;&lt;p&gt;This section of the course focuses on learning how to build the best possible LLMs using the latest techniques.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/roadmap_scientist.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1. The LLM architecture&lt;/h3&gt; &#xA;&lt;p&gt;While an in-depth knowledge about the Transformer architecture is not required, it is important to have a good understanding of its inputs (tokens) and outputs (logits). The vanilla attention mechanism is another crucial component to master, as improved versions of it are introduced later on.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-level view&lt;/strong&gt;: Revisit the encoder-decoder Transformer architecture, and more specifically the decoder-only GPT architecture, which is used in every modern LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tokenization&lt;/strong&gt;: Understand how to convert raw text data into a format that the model can understand, which involves splitting the text into tokens (usually words or subwords).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Attention mechanisms&lt;/strong&gt;: Grasp the theory behind attention mechanisms, including self-attention and scaled dot-product attention, which allows the model to focus on different parts of the input when producing an output.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text generation&lt;/strong&gt;: Learn about the different ways the model can generate output sequences. Common strategies include greedy decoding, beam search, top-k sampling, and nucleus sampling.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;The Illustrated Transformer&lt;/a&gt; by Jay Alammar: A visual and intuitive explanation of the Transformer model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-gpt2/&#34;&gt;The Illustrated GPT-2&lt;/a&gt; by Jay Alammar: Even more important than the previous article, it is focused on the GPT architecture, which is very similar to Llama&#39;s.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=wjZofJX0v4M&amp;amp;t=187s&#34;&gt;Visual intro to Transformers&lt;/a&gt; by 3Blue1Brown: Simple easy to understand visual intro to Transformers&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bbycroft.net/llm&#34;&gt;LLM Visualization&lt;/a&gt; by Brendan Bycroft: Incredible 3D visualization of what happens inside of an LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kCc8FmEb1nY&#34;&gt;nanoGPT&lt;/a&gt; by Andrej Karpathy: A 2h-long YouTube video to reimplement GPT from scratch (for programmers).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2018-06-24-attention/&#34;&gt;Attention? Attention!&lt;/a&gt; by Lilian Weng: Introduce the need for attention in a more formal way.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html&#34;&gt;Decoding Strategies in LLMs&lt;/a&gt;: Provide code and a visual introduction to the different decoding strategies to generate text.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2. Building an instruction dataset&lt;/h3&gt; &#xA;&lt;p&gt;While it&#39;s easy to find raw data from Wikipedia and other websites, it&#39;s difficult to collect pairs of instructions and answers in the wild. Like in traditional machine learning, the quality of the dataset will directly influence the quality of the model, which is why it might be the most important component in the fine-tuning process.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://crfm.stanford.edu/2023/03/13/alpaca.html&#34;&gt;Alpaca&lt;/a&gt;-like dataset&lt;/strong&gt;: Generate synthetic data from scratch with the OpenAI API (GPT). You can specify seeds and system prompts to create a diverse dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced techniques&lt;/strong&gt;: Learn how to improve existing datasets with &lt;a href=&#34;https://arxiv.org/abs/2304.12244&#34;&gt;Evol-Instruct&lt;/a&gt;, how to generate high-quality synthetic data like in the &lt;a href=&#34;https://arxiv.org/abs/2306.02707&#34;&gt;Orca&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2306.11644&#34;&gt;phi-1&lt;/a&gt; papers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Filtering data&lt;/strong&gt;: Traditional techniques involving regex, removing near-duplicates, focusing on answers with a high number of tokens, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt templates&lt;/strong&gt;: There&#39;s no true standard way of formatting instructions and answers, which is why it&#39;s important to know about the different chat templates, such as &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt?tabs=python&amp;amp;pivots=programming-language-chat-ml&#34;&gt;ChatML&lt;/a&gt;, &lt;a href=&#34;https://crfm.stanford.edu/2023/03/13/alpaca.html&#34;&gt;Alpaca&lt;/a&gt;, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2&#34;&gt;Preparing a Dataset for Instruction tuning&lt;/a&gt; by Thomas Capelle: Exploration of the Alpaca and Alpaca-GPT4 datasets and how to format them.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/mlearning-ai/generating-a-clinical-instruction-dataset-in-portuguese-with-langchain-and-gpt-4-6ee9abfa41ae&#34;&gt;Generating a Clinical Instruction Dataset&lt;/a&gt; by Solano Todeschini: Tutorial on how to create a synthetic instruction dataset using GPT-4.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f&#34;&gt;GPT 3.5 for news classification&lt;/a&gt; by Kshitiz Sahay: Use GPT 3.5 to create an instruction dataset to fine-tune Llama 2 for news classification.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1GH8PW9-zAe4cXEZyOIE-T9uHXblIldAg?usp=sharing&#34;&gt;Dataset creation for fine-tuning LLM&lt;/a&gt;: Notebook that contains a few techniques to filter a dataset and upload the result.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/chat-templates&#34;&gt;Chat Template&lt;/a&gt; by Matthew Carrigan: Hugging Face&#39;s page about prompt templates&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;3. Pre-training models&lt;/h3&gt; &#xA;&lt;p&gt;Pre-training is a very long and costly process, which is why this is not the focus of this course. It&#39;s good to have some level of understanding of what happens during pre-training, but hands-on experience is not required.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data pipeline&lt;/strong&gt;: Pre-training requires huge datasets (e.g., &lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;Llama 2&lt;/a&gt; was trained on 2 trillion tokens) that need to be filtered, tokenized, and collated with a pre-defined vocabulary.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Causal language modeling&lt;/strong&gt;: Learn the difference between causal and masked language modeling, as well as the loss function used in this case. For efficient pre-training, learn more about &lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;Megatron-LM&lt;/a&gt; or &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;gpt-neox&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scaling laws&lt;/strong&gt;: The &lt;a href=&#34;https://arxiv.org/pdf/2001.08361.pdf&#34;&gt;scaling laws&lt;/a&gt; describe the expected model performance based on the model size, dataset size, and the amount of compute used for training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-Performance Computing&lt;/strong&gt;: Out of scope here, but more knowledge about HPC is fundamental if you&#39;re planning to create your own LLM from scratch (hardware, distributed workload, etc.).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Zjh-819/LLMDataHub&#34;&gt;LLMDataHub&lt;/a&gt; by Junhao Zhao: Curated list of datasets for pre-training, fine-tuning, and RLHF.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt&#34;&gt;Training a causal language model from scratch&lt;/a&gt; by Hugging Face: Pre-train a GPT-2 model from scratch using the transformers library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jzhang38/TinyLlama&#34;&gt;TinyLlama&lt;/a&gt; by Zhang et al.: Check this project to get a good understanding of how a Llama model is trained from scratch.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/tasks/language_modeling&#34;&gt;Causal language modeling&lt;/a&gt; by Hugging Face: Explain the difference between causal and masked language modeling and how to quickly fine-tune a DistilGPT-2 model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications&#34;&gt;Chinchilla&#39;s wild implications&lt;/a&gt; by nostalgebraist: Discuss the scaling laws and explain what they mean to LLMs in general.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bigscience.notion.site/BLOOM-BigScience-176B-Model-ad073ca07cdf479398d5f95d88e218c4&#34;&gt;BLOOM&lt;/a&gt; by BigScience: Notion page that describes how the BLOOM model was built, with a lot of useful information about the engineering part and the problems that were encountered.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/metaseq/raw/main/projects/OPT/chronicles/OPT175B_Logbook.pdf&#34;&gt;OPT-175 Logbook&lt;/a&gt; by Meta: Research logs showing what went wrong and what went right. Useful if you&#39;re planning to pre-train a very large language model (in this case, 175B parameters).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.llm360.ai/&#34;&gt;LLM 360&lt;/a&gt;: A framework for open-source LLMs with training and data preparation code, data, metrics, and models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;4. Supervised Fine-Tuning&lt;/h3&gt; &#xA;&lt;p&gt;Pre-trained models are only trained on a next-token prediction task, which is why they&#39;re not helpful assistants. SFT allows you to tweak them to respond to instructions. Moreover, it allows you to fine-tune your model on any data (private, not seen by GPT-4, etc.) and use it without having to pay for an API like OpenAI&#39;s.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Full fine-tuning&lt;/strong&gt;: Full fine-tuning refers to training all the parameters in the model. It is not an efficient technique, but it produces slightly better results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;&lt;strong&gt;LoRA&lt;/strong&gt;&lt;/a&gt;: A parameter-efficient technique (PEFT) based on low-rank adapters. Instead of training all the parameters, we only train these adapters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14314&#34;&gt;&lt;strong&gt;QLoRA&lt;/strong&gt;&lt;/a&gt;: Another PEFT based on LoRA, which also quantizes the weights of the model in 4 bits and introduce paged optimizers to manage memory spikes. Combine it with &lt;a href=&#34;https://github.com/unslothai/unsloth&#34;&gt;Unsloth&lt;/a&gt; to run it efficiently on a free Colab notebook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/OpenAccess-AI-Collective/axolotl&#34;&gt;Axolotl&lt;/a&gt;&lt;/strong&gt;: A user-friendly and powerful fine-tuning tool that is used in a lot of state-of-the-art open-source models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/&#34;&gt;&lt;strong&gt;DeepSpeed&lt;/strong&gt;&lt;/a&gt;: Efficient pre-training and fine-tuning of LLMs for multi-GPU and multi-node settings (implemented in Axolotl).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rentry.org/llm-training&#34;&gt;The Novice&#39;s LLM Training Guide&lt;/a&gt; by Alpin: Overview of the main concepts and parameters to consider when fine-tuning LLMs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning.ai/pages/community/lora-insights/&#34;&gt;LoRA insights&lt;/a&gt; by Sebastian Raschka: Practical insights about LoRA and how to select the best parameters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html&#34;&gt;Fine-Tune Your Own Llama 2 Model&lt;/a&gt;: Hands-on tutorial on how to fine-tune a Llama 2 model using Hugging Face libraries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff&#34;&gt;Padding Large Language Models&lt;/a&gt; by Benjamin Marie: Best practices to pad training examples for causal LLMs&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html&#34;&gt;A Beginner&#39;s Guide to LLM Fine-Tuning&lt;/a&gt;: Tutorial on how to fine-tune a CodeLlama model using Axolotl.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;5. Preference Alignment&lt;/h3&gt; &#xA;&lt;p&gt;After supervised fine-tuning, RLHF is a step used to align the LLM&#39;s answers with human expectations. The idea is to learn preferences from human (or artificial) feedback, which can be used to reduce biases, censor models, or make them act in a more useful way. It is more complex than SFT and often seen as optional.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Preference datasets&lt;/strong&gt;: These datasets typically contain several answers with some kind of ranking, which makes them more difficult to produce than instruction datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1707.06347&#34;&gt;&lt;strong&gt;Proximal Policy Optimization&lt;/strong&gt;&lt;/a&gt;: This algorithm leverages a reward model that predicts whether a given text is highly ranked by humans. This prediction is then used to optimize the SFT model with a penalty based on KL divergence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.18290&#34;&gt;Direct Preference Optimization&lt;/a&gt;&lt;/strong&gt;: DPO simplifies the process by reframing it as a classification problem. It uses a reference model instead of a reward model (no training needed) and only requires one hyperparameter, making it more stable and efficient.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/argilla-io/distilabel&#34;&gt;Distilabel&lt;/a&gt; by Argilla: Excellent tool to create your own datasets. It was especially designed for preference datasets but can also do SFT.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wandb.ai/ayush-thakur/Intro-RLAIF/reports/An-Introduction-to-Training-LLMs-Using-Reinforcement-Learning-From-Human-Feedback-RLHF---VmlldzozMzYyNjcy&#34;&gt;An Introduction to Training LLMs using RLHF&lt;/a&gt; by Ayush Thakur: Explain why RLHF is desirable to reduce bias and increase performance in LLMs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/rlhf&#34;&gt;Illustration RLHF&lt;/a&gt; by Hugging Face: Introduction to RLHF with reward model training and fine-tuning with reinforcement learning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/pref-tuning&#34;&gt;Preference Tuning LLMs&lt;/a&gt; by Hugging Face: Comparison of the DPO, IPO, and KTO algorithms to perform preference alignment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives&#34;&gt;LLM Training: RLHF and Its Alternatives&lt;/a&gt; by Sebastian Rashcka: Overview of the RLHF process and alternatives like RLAIF.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/dpo-trl&#34;&gt;Fine-tune Mistral-7b with DPO&lt;/a&gt;: Tutorial to fine-tune a Mistral-7b model with DPO and reproduce &lt;a href=&#34;https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B&#34;&gt;NeuralHermes-2.5&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;6. Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Evaluating LLMs is an undervalued part of the pipeline, which is time-consuming and moderately reliable. Your downstream task should dictate what you want to evaluate, but always remember Goodhart&#39;s law: &#34;When a measure becomes a target, it ceases to be a good measure.&#34;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Traditional metrics&lt;/strong&gt;: Metrics like perplexity and BLEU score are not as popular as they were because they&#39;re flawed in most contexts. It is still important to understand them and when they can be applied.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;General benchmarks&lt;/strong&gt;: Based on the &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;Language Model Evaluation Harness&lt;/a&gt;, the &lt;a href=&#34;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#34;&gt;Open LLM Leaderboard&lt;/a&gt; is the main benchmark for general-purpose LLMs (like ChatGPT). There are other popular benchmarks like &lt;a href=&#34;https://github.com/google/BIG-bench&#34;&gt;BigBench&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2306.05685&#34;&gt;MT-Bench&lt;/a&gt;, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Task-specific benchmarks&lt;/strong&gt;: Tasks like summarization, translation, and question answering have dedicated benchmarks, metrics, and even subdomains (medical, financial, etc.), such as &lt;a href=&#34;https://pubmedqa.github.io/&#34;&gt;PubMedQA&lt;/a&gt; for biomedical question answering.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Human evaluation&lt;/strong&gt;: The most reliable evaluation is the acceptance rate by users or comparisons made by humans. Logging user feedback in addition to the chat traces (e.g., using &lt;a href=&#34;https://docs.smith.langchain.com/evaluation/capturing-feedback&#34;&gt;LangSmith&lt;/a&gt;) helps to identify potential areas for improvement.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/perplexity&#34;&gt;Perplexity of fixed-length models&lt;/a&gt; by Hugging Face: Overview of perplexity with code to implement it with the transformers library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213&#34;&gt;BLEU at your own risk&lt;/a&gt; by Rachael Tatman: Overview of the BLEU score and its many issues with examples.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.03109&#34;&gt;A Survey on Evaluation of LLMs&lt;/a&gt; by Chang et al.: Comprehensive paper about what to evaluate, where to evaluate, and how to evaluate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard&#34;&gt;Chatbot Arena Leaderboard&lt;/a&gt; by lmsys: Elo rating of general-purpose LLMs, based on comparisons made by humans.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;7. Quantization&lt;/h3&gt; &#xA;&lt;p&gt;Quantization is the process of converting the weights (and activations) of a model using a lower precision. For example, weights stored using 16 bits can be converted into a 4-bit representation. This technique has become increasingly important to reduce the computational and memory costs associated with LLMs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Base techniques&lt;/strong&gt;: Learn the different levels of precision (FP32, FP16, INT8, etc.) and how to perform na√Øve quantization with absmax and zero-point techniques.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GGUF and llama.cpp&lt;/strong&gt;: Originally designed to run on CPUs, &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; and the GGUF format have become the most popular tools to run LLMs on consumer-grade hardware.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPTQ and EXL2&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt; and, more specifically, the &lt;a href=&#34;https://github.com/turboderp/exllamav2&#34;&gt;EXL2&lt;/a&gt; format offer an incredible speed but can only run on GPUs. Models also take a long time to be quantized.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AWQ&lt;/strong&gt;: This new format is more accurate than GPTQ (lower perplexity) but uses a lot more VRAM and is not necessarily faster.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html&#34;&gt;Introduction to quantization&lt;/a&gt;: Overview of quantization, absmax and zero-point quantization, and LLM.int8() with code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html&#34;&gt;Quantize Llama models with llama.cpp&lt;/a&gt;: Tutorial on how to quantize a Llama 2 model using llama.cpp and the GGUF format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html&#34;&gt;4-bit LLM Quantization with GPTQ&lt;/a&gt;: Tutorial on how to quantize an LLM using the GPTQ algorithm with AutoGPTQ.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html&#34;&gt;ExLlamaV2: The Fastest Library to Run LLMs&lt;/a&gt;: Guide on how to quantize a Mistral model using the EXL2 format and run it with the ExLlamaV2 library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8&#34;&gt;Understanding Activation-Aware Weight Quantization&lt;/a&gt; by FriendliAI: Overview of the AWQ technique and its benefits.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;8. New Trends&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Positional embeddings&lt;/strong&gt;: Learn how LLMs encode positions, especially relative positional encoding schemes like &lt;a href=&#34;https://arxiv.org/abs/2104.09864&#34;&gt;RoPE&lt;/a&gt;. Implement &lt;a href=&#34;https://arxiv.org/abs/2309.00071&#34;&gt;YaRN&lt;/a&gt; (multiplies the attention matrix by a temperature factor) or &lt;a href=&#34;https://arxiv.org/abs/2108.12409&#34;&gt;ALiBi&lt;/a&gt; (attention penalty based on token distance) to extend the context length.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model merging&lt;/strong&gt;: Merging trained models has become a popular way of creating performant models without any fine-tuning. The popular &lt;a href=&#34;https://github.com/cg123/mergekit&#34;&gt;mergekit&lt;/a&gt; library implements the most popular merging methods, like SLERP, &lt;a href=&#34;https://arxiv.org/abs/2311.03099&#34;&gt;DARE&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2311.03099&#34;&gt;TIES&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mixture of Experts&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2401.04088&#34;&gt;Mixtral&lt;/a&gt; re-popularized the MoE architecture thanks to its excellent performance. In parallel, a type of frankenMoE emerged in the OSS community by merging models like &lt;a href=&#34;https://huggingface.co/mlabonne/phixtral-2x2_8&#34;&gt;Phixtral&lt;/a&gt;, which is a cheaper and performant option.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multimodal models&lt;/strong&gt;: These models (like &lt;a href=&#34;https://openai.com/research/clip&#34;&gt;CLIP&lt;/a&gt;, &lt;a href=&#34;https://stability.ai/stable-image&#34;&gt;Stable Diffusion&lt;/a&gt;, or &lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;LLaVA&lt;/a&gt;) process multiple types of inputs (text, images, audio, etc.) with a unified embedding space, which unlocks powerful applications like text-to-image.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.eleuther.ai/yarn/&#34;&gt;Extending the RoPE&lt;/a&gt; by EleutherAI: Article that summarizes the different position-encoding techniques.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@rcrajatchawla/understanding-yarn-extending-context-window-of-llms-3f21e3522465&#34;&gt;Understanding YaRN&lt;/a&gt; by Rajat Chawla: Introduction to YaRN.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html&#34;&gt;Merge LLMs with mergekit&lt;/a&gt;: Tutorial about model merging using mergekit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/moe&#34;&gt;Mixture of Experts Explained&lt;/a&gt; by Hugging Face: Exhaustive guide about MoEs and how they work.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huyenchip.com/2023/10/10/multimodal.html&#34;&gt;Large Multimodal Models&lt;/a&gt; by Chip Huyen: Overview of multimodal systems and the recent history of this field.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üë∑ The LLM Engineer&lt;/h2&gt; &#xA;&lt;p&gt;This section of the course focuses on learning how to build LLM-powered applications that can be used in production, with a focus on augmenting models and deploying them.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlabonne/llm-course/main/img/roadmap_engineer.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1. Running LLMs&lt;/h3&gt; &#xA;&lt;p&gt;Running LLMs can be difficult due to high hardware requirements. Depending on your use case, you might want to simply consume a model through an API (like GPT-4) or run it locally. In any case, additional prompting and guidance techniques can improve and constrain the output for your applications.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM APIs&lt;/strong&gt;: APIs are a convenient way to deploy LLMs. This space is divided between private LLMs (&lt;a href=&#34;https://platform.openai.com/&#34;&gt;OpenAI&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview&#34;&gt;Google&lt;/a&gt;, &lt;a href=&#34;https://docs.anthropic.com/claude/reference/getting-started-with-the-api&#34;&gt;Anthropic&lt;/a&gt;, &lt;a href=&#34;https://docs.cohere.com/docs&#34;&gt;Cohere&lt;/a&gt;, etc.) and open-source LLMs (&lt;a href=&#34;https://openrouter.ai/&#34;&gt;OpenRouter&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/inference-api&#34;&gt;Hugging Face&lt;/a&gt;, &lt;a href=&#34;https://www.together.ai/&#34;&gt;Together AI&lt;/a&gt;, etc.).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open-source LLMs&lt;/strong&gt;: The &lt;a href=&#34;https://huggingface.co/models&#34;&gt;Hugging Face Hub&lt;/a&gt; is a great place to find LLMs. You can directly run some of them in &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Hugging Face Spaces&lt;/a&gt;, or download and run them locally in apps like &lt;a href=&#34;https://lmstudio.ai/&#34;&gt;LM Studio&lt;/a&gt; or through the CLI with &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; or &lt;a href=&#34;https://ollama.ai/&#34;&gt;Ollama&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt engineering&lt;/strong&gt;: Common techniques include zero-shot prompting, few-shot prompting, chain of thought, and ReAct. They work better with bigger models, but can be adapted to smaller ones.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Structuring outputs&lt;/strong&gt;: Many tasks require a structured output, like a strict template or a JSON format. Libraries like &lt;a href=&#34;https://lmql.ai/&#34;&gt;LMQL&lt;/a&gt;, &lt;a href=&#34;https://github.com/outlines-dev/outlines&#34;&gt;Outlines&lt;/a&gt;, &lt;a href=&#34;https://github.com/guidance-ai/guidance&#34;&gt;Guidance&lt;/a&gt;, etc. can be used to guide the generation and respect a given structure.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio&#34;&gt;Run an LLM locally with LM Studio&lt;/a&gt; by Nisha Arya: Short guide on how to use LM Studio.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.promptingguide.ai/&#34;&gt;Prompt engineering guide&lt;/a&gt; by DAIR.AI: Exhaustive list of prompt techniques with examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://outlines-dev.github.io/outlines/quickstart/&#34;&gt;Outlines - Quickstart&lt;/a&gt;: List of guided generation techniques enabled by Outlines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lmql.ai/docs/language/overview.html&#34;&gt;LMQL - Overview&lt;/a&gt;: Introduction to the LMQL language.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2. Building a Vector Storage&lt;/h3&gt; &#xA;&lt;p&gt;Creating a vector storage is the first step to build a Retrieval Augmented Generation (RAG) pipeline. Documents are loaded, split, and relevant chunks are used to produce vector representations (embeddings) that are stored for future use during inference.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ingesting documents&lt;/strong&gt;: Document loaders are convenient wrappers that can handle many formats: PDF, JSON, HTML, Markdown, etc. They can also directly retrieve data from some databases and APIs (GitHub, Reddit, Google Drive, etc.).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Splitting documents&lt;/strong&gt;: Text splitters break down documents into smaller, semantically meaningful chunks. Instead of splitting text after &lt;em&gt;n&lt;/em&gt; characters, it&#39;s often better to split by header or recursively, with some additional metadata.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Embedding models&lt;/strong&gt;: Embedding models convert text into vector representations. It allows for a deeper and more nuanced understanding of language, which is essential to perform semantic search.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vector databases&lt;/strong&gt;: Vector databases (like &lt;a href=&#34;https://www.trychroma.com/&#34;&gt;Chroma&lt;/a&gt;, &lt;a href=&#34;https://www.pinecone.io/&#34;&gt;Pinecone&lt;/a&gt;, &lt;a href=&#34;https://milvus.io/&#34;&gt;Milvus&lt;/a&gt;, &lt;a href=&#34;https://faiss.ai/&#34;&gt;FAISS&lt;/a&gt;, &lt;a href=&#34;https://github.com/spotify/annoy&#34;&gt;Annoy&lt;/a&gt;, etc.) are designed to store embedding vectors. They enable efficient retrieval of data that is &#39;most similar&#39; to a query based on vector similarity.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/docs/modules/data_connection/document_transformers/&#34;&gt;LangChain - Text splitters&lt;/a&gt;: List of different text splitters implemented in LangChain.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sbert.net/&#34;&gt;Sentence Transformers library&lt;/a&gt;: Popular library for embedding models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/mteb/leaderboard&#34;&gt;MTEB Leaderboard&lt;/a&gt;: Leaderboard for embedding models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/blog/the-top-5-vector-databases&#34;&gt;The Top 5 Vector Databases&lt;/a&gt; by Moez Ali: A comparison of the best and most popular vector databases.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;3. Retrieval Augmented Generation&lt;/h3&gt; &#xA;&lt;p&gt;With RAG, LLMs retrieves contextual documents from a database to improve the accuracy of their answers. RAG is a popular way of augmenting the model&#39;s knowledge without any fine-tuning.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Orchestrators&lt;/strong&gt;: Orchestrators (like &lt;a href=&#34;https://python.langchain.com/docs/get_started/introduction&#34;&gt;LangChain&lt;/a&gt;, &lt;a href=&#34;https://docs.llamaindex.ai/en/stable/&#34;&gt;LlamaIndex&lt;/a&gt;, &lt;a href=&#34;https://github.com/IntelLabs/fastRAG&#34;&gt;FastRAG&lt;/a&gt;, etc.) are popular frameworks to connect your LLMs with tools, databases, memories, etc. and augment their abilities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Retrievers&lt;/strong&gt;: User instructions are not optimized for retrieval. Different techniques (e.g., multi-query retriever, &lt;a href=&#34;https://arxiv.org/abs/2212.10496&#34;&gt;HyDE&lt;/a&gt;, etc.) can be applied to rephrase/expand them and improve performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: To remember previous instructions and answers, LLMs and chatbots like ChatGPT add this history to their context window. This buffer can be improved with summarization (e.g., using a smaller LLM), a vector store + RAG, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: We need to evaluate both the document retrieval (context precision and recall) and generation stages (faithfulness and answer relevancy). It can be simplified with tools &lt;a href=&#34;https://github.com/explodinggradients/ragas/tree/main&#34;&gt;Ragas&lt;/a&gt; and &lt;a href=&#34;https://github.com/confident-ai/deepeval&#34;&gt;DeepEval&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.llamaindex.ai/en/stable/getting_started/concepts.html&#34;&gt;Llamaindex - High-level concepts&lt;/a&gt;: Main concepts to know when building RAG pipelines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/&#34;&gt;Pinecone - Retrieval Augmentation&lt;/a&gt;: Overview of the retrieval augmentation process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/docs/use_cases/question_answering/quickstart&#34;&gt;LangChain - Q&amp;amp;A with RAG&lt;/a&gt;: Step-by-step tutorial to build a typical RAG pipeline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/docs/modules/memory/types/&#34;&gt;LangChain - Memory types&lt;/a&gt;: List of different types of memories with relevant usage.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.ragas.io/en/stable/concepts/metrics/index.html&#34;&gt;RAG pipeline - Metrics&lt;/a&gt;: Overview of the main metrics used to evaluate RAG pipelines.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;4. Advanced RAG&lt;/h3&gt; &#xA;&lt;p&gt;Real-life applications can require complex pipelines, including SQL or graph databases, as well as automatically selecting relevant tools and APIs. These advanced techniques can improve a baseline solution and provide additional features.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Query construction&lt;/strong&gt;: Structured data stored in traditional databases requires a specific query language like SQL, Cypher, metadata, etc. We can directly translate the user instruction into a query to access the data with query construction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agents and tools&lt;/strong&gt;: Agents augment LLMs by automatically selecting the most relevant tools to provide an answer. These tools can be as simple as using Google or Wikipedia, or more complex like a Python interpreter or Jira.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Post-processing&lt;/strong&gt;: Final step that processes the inputs that are fed to the LLM. It enhances the relevance and diversity of documents retrieved with re-ranking, &lt;a href=&#34;https://github.com/Raudaschl/rag-fusion&#34;&gt;RAG-fusion&lt;/a&gt;, and classification.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Program LLMs&lt;/strong&gt;: Frameworks like &lt;a href=&#34;https://github.com/stanfordnlp/dspy&#34;&gt;DSPy&lt;/a&gt; allow you to optimize prompts and weights based on automated evaluations in a programmatic way.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.langchain.dev/query-construction/&#34;&gt;LangChain - Query Construction&lt;/a&gt;: Blog post about different types of query construction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/docs/use_cases/qa_structured/sql&#34;&gt;LangChain - SQL&lt;/a&gt;: Tutorial on how to interact with SQL databases with LLMs, involving Text-to-SQL and an optional SQL agent.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pinecone.io/learn/series/langchain/langchain-agents/&#34;&gt;Pinecone - LLM agents&lt;/a&gt;: Introduction to agents and tools with different types.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2023-06-23-agent/&#34;&gt;LLM Powered Autonomous Agents&lt;/a&gt; by Lilian Weng: More theoretical article about LLM agents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.langchain.dev/applying-openai-rag/&#34;&gt;LangChain - OpenAI&#39;s RAG&lt;/a&gt;: Overview of the RAG strategies employed by OpenAI, including post-processing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task&#34;&gt;DSPy in 8 Steps&lt;/a&gt;: General-purpose guide to DSPy introducing modules, signatures, and optimizers.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;5. Inference optimization&lt;/h3&gt; &#xA;&lt;p&gt;Text generation is a costly process that requires expensive hardware. In addition to quantization, various techniques have been proposed to maximize throughput and reduce inference costs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flash Attention&lt;/strong&gt;: Optimization of the attention mechanism to transform its complexity from quadratic to linear, speeding up both training and inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Key-value cache&lt;/strong&gt;: Understand the key-value cache and the improvements introduced in &lt;a href=&#34;https://arxiv.org/abs/1911.02150&#34;&gt;Multi-Query Attention&lt;/a&gt; (MQA) and &lt;a href=&#34;https://arxiv.org/abs/2305.13245&#34;&gt;Grouped-Query Attention&lt;/a&gt; (GQA).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speculative decoding&lt;/strong&gt;: Use a small model to produce drafts that are then reviewed by a larger model to speed up text generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one&#34;&gt;GPU Inference&lt;/a&gt; by Hugging Face: Explain how to optimize inference on GPUs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices&#34;&gt;LLM Inference&lt;/a&gt; by Databricks: Best practices for how to optimize LLM inference in production.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization&#34;&gt;Optimizing LLMs for Speed and Memory&lt;/a&gt; by Hugging Face: Explain three main techniques to optimize speed and memory, namely quantization, Flash Attention, and architectural innovations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/assisted-generation&#34;&gt;Assisted Generation&lt;/a&gt; by Hugging Face: HF&#39;s version of speculative decoding, it&#39;s an interesting blog post about how it works with code to implement it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;6. Deploying LLMs&lt;/h3&gt; &#xA;&lt;p&gt;Deploying LLMs at scale is an engineering feat that can require multiple clusters of GPUs. In other scenarios, demos and local apps can be achieved with a much lower complexity.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local deployment&lt;/strong&gt;: Privacy is an important advantage that open-source LLMs have over private ones. Local LLM servers (&lt;a href=&#34;https://lmstudio.ai/&#34;&gt;LM Studio&lt;/a&gt;, &lt;a href=&#34;https://ollama.ai/&#34;&gt;Ollama&lt;/a&gt;, &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui&#34;&gt;oobabooga&lt;/a&gt;, &lt;a href=&#34;https://github.com/LostRuins/koboldcpp&#34;&gt;kobold.cpp&lt;/a&gt;, etc.) capitalize on this advantage to power local apps.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Demo deployment&lt;/strong&gt;: Frameworks like &lt;a href=&#34;https://www.gradio.app/&#34;&gt;Gradio&lt;/a&gt; and &lt;a href=&#34;https://docs.streamlit.io/&#34;&gt;Streamlit&lt;/a&gt; are helpful to prototype applications and share demos. You can also easily host them online, for example using &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Hugging Face Spaces&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Server deployment&lt;/strong&gt;: Deploy LLMs at scale requires cloud (see also &lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/&#34;&gt;SkyPilot&lt;/a&gt;) or on-prem infrastructure and often leverage optimized text generation frameworks like &lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt;TGI&lt;/a&gt;, &lt;a href=&#34;https://github.com/vllm-project/vllm/tree/main&#34;&gt;vLLM&lt;/a&gt;, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Edge deployment&lt;/strong&gt;: In constrained environments, high-performance frameworks like &lt;a href=&#34;https://github.com/mlc-ai/mlc-llm&#34;&gt;MLC LLM&lt;/a&gt; and &lt;a href=&#34;https://github.com/wangzhaode/mnn-llm/raw/master/README_en.md&#34;&gt;mnn-llm&lt;/a&gt; can deploy LLM in web browsers, Android, and iOS.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps&#34;&gt;Streamlit - Build a basic LLM app&lt;/a&gt;: Tutorial to make a basic ChatGPT-like app using Streamlit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/sagemaker-huggingface-llm&#34;&gt;HF LLM Inference Container&lt;/a&gt;: Deploy LLMs on Amazon SageMaker using Hugging Face&#39;s inference container.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.philschmid.de/&#34;&gt;Philschmid&amp;nbsp;blog&lt;/a&gt; by Philipp Schmid: Collection of high-quality articles about LLM deployment using Amazon SageMaker.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hamel.dev/notes/llm/inference/03_inference.html&#34;&gt;Optimizing latence&lt;/a&gt; by Hamel Husain: Comparison of TGI, vLLM, CTranslate2, and mlc in terms of throughput and latency.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;7. Securing LLMs&lt;/h3&gt; &#xA;&lt;p&gt;In addition to traditional security problems associated with software, LLMs have unique weaknesses due to the way they are trained and prompted.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt hacking&lt;/strong&gt;: Different techniques related to prompt engineering, including prompt injection (additional instruction to hijack the model&#39;s answer), data/prompt leaking (retrieve its original data/prompt), and jailbreaking (craft prompts to bypass safety features).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Backdoors&lt;/strong&gt;: Attack vectors can target the training data itself, by poisoning the training data (e.g., with false information) or creating backdoors (secret triggers to change the model&#39;s behavior during inference).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Defensive measures&lt;/strong&gt;: The best way to protect your LLM applications is to test them against these vulnerabilities (e.g., using red teaming and checks like &lt;a href=&#34;https://github.com/leondz/garak/&#34;&gt;garak&lt;/a&gt;) and observe them in production (with a framework like &lt;a href=&#34;https://github.com/langfuse/langfuse&#34;&gt;langfuse&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://owasp.org/www-project-top-10-for-large-language-model-applications/&#34;&gt;OWASP LLM Top 10&lt;/a&gt; by HEGO Wiki: List of the 10 most critic vulnerabilities seen in LLM applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jthack/PIPE&#34;&gt;Prompt Injection Primer&lt;/a&gt; by Joseph Thacker: Short guide dedicated to prompt injection for engineers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://llmsecurity.net/&#34;&gt;LLM Security&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/llm_sec&#34;&gt;@llm_sec&lt;/a&gt;: Extensive list of resources related to LLM security.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming&#34;&gt;Red teaming LLMs&lt;/a&gt; by Microsoft: Guide on how to perform red teaming with LLMs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This roadmap was inspired by the excellent &lt;a href=&#34;https://github.com/milanm/DevOps-Roadmap&#34;&gt;DevOps Roadmap&lt;/a&gt; from Milan Milanoviƒá and Romano Roth.&lt;/p&gt; &#xA;&lt;p&gt;Special thanks to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thomas Thelen for motivating me to create a roadmap&lt;/li&gt; &#xA; &lt;li&gt;Andr√© Frade for his input and review of the first draft&lt;/li&gt; &#xA; &lt;li&gt;Dino Dunn for providing resources about LLM security&lt;/li&gt; &#xA; &lt;li&gt;Magdalena Kuhn for improving the &#34;human evaluation&#34; part&lt;/li&gt; &#xA; &lt;li&gt;Odoverdose for suggesting 3Blue1Brown&#39;s video about Transformers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Disclaimer: I am not affiliated with any sources listed here.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://star-history.com/#mlabonne/llm-course&amp;amp;Date&#34;&gt; &lt;img src=&#34;https://api.star-history.com/svg?repos=mlabonne/llm-course&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt; &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>harvardnlp/annotated-transformer</title>
    <updated>2024-06-01T01:46:44Z</updated>
    <id>tag:github.com,2024-06-01:/harvardnlp/annotated-transformer</id>
    <link href="https://github.com/harvardnlp/annotated-transformer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An annotated implementation of the Transformer paper.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Code for The Annotated Transformer blog post:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/annotated-transformer/&#34;&gt;http://nlp.seas.harvard.edu/annotated-transformer/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/harvardnlp/annotated-transformer/blob/master/AnnotatedTransformer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/35882/166251887-9da909a9-660b-45a9-ae72-0aae89fb38d4.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Package Dependencies&lt;/h1&gt; &#xA;&lt;p&gt;Use &lt;code&gt;requirements.txt&lt;/code&gt; to install library dependencies with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Notebook Setup&lt;/h1&gt; &#xA;&lt;p&gt;The Annotated Transformer is created using &lt;a href=&#34;https://github.com/mwouts/jupytext&#34;&gt;jupytext&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Regular notebooks pose problems for source control - cell outputs end up in the repo history and diffs between commits are difficult to examine. Using jupytext, there is a python script (&lt;code&gt;.py&lt;/code&gt; file) that is automatically kept in sync with the notebook file by the jupytext plugin.&lt;/p&gt; &#xA;&lt;p&gt;The python script is committed contains all the cell content and can be used to generate the notebook file. The python script is a regular python source file, markdown sections are included using a standard comment convention, and outputs are not saved. The notebook itself is treated as a build artifact and is not commited to the git repository.&lt;/p&gt; &#xA;&lt;p&gt;Prior to using this repo, make sure jupytext is installed by following the &lt;a href=&#34;https://github.com/mwouts/jupytext/raw/main/docs/install.md&#34;&gt;installation instructions here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To produce the &lt;code&gt;.ipynb&lt;/code&gt; notebook file using the markdown source, run (under the hood, the &lt;code&gt;notebook&lt;/code&gt; build target simply runs &lt;code&gt;jupytext --to ipynb the_annotated_transformer.py&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To produce the html version of the notebook, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;make html&lt;/code&gt; is just a shortcut for for generating the notebook with &lt;code&gt;jupytext --to ipynb the_annotated_transformer.py&lt;/code&gt; followed by using the jupyter nbconvert command to produce html using &lt;code&gt;jupyter nbconvert --to html the_annotated_transformer.ipynb&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Formatting and Linting&lt;/h1&gt; &#xA;&lt;p&gt;To keep the code formatting clean, the annotated transformer git repo has a git action to check that the code conforms to &lt;a href=&#34;https://www.python.org/dev/peps/pep-0008/&#34;&gt;PEP8 coding standards&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To make this easier, there are two &lt;code&gt;Makefile&lt;/code&gt; build targets to run automatic code formatting with black and flake8.&lt;/p&gt; &#xA;&lt;p&gt;Be sure to &lt;a href=&#34;https://github.com/psf/black#installation&#34;&gt;install black&lt;/a&gt; and &lt;a href=&#34;https://flake8.pycqa.org/en/latest/&#34;&gt;flake8&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make black&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(or alternatively manually call black &lt;code&gt;black --line-length 79 the_annotated_transformer.py&lt;/code&gt;) to format code automatically using black and:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make flake&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(or manually call flake8 `flake8 --show-source the_annotated_transformer.py) to check for PEP8 violations.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s recommended to run these two commands and fix any flake8 errors that arise, when submitting a PR, otherwise the github actions CI will report an error.&lt;/p&gt;</summary>
  </entry>
</feed>