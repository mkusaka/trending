<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-01T02:18:24Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>aamini/introtodeeplearning</title>
    <updated>2023-05-01T02:18:24Z</updated>
    <id>tag:github.com,2023-05-01:/aamini/introtodeeplearning</id>
    <link href="https://github.com/aamini/introtodeeplearning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Lab Materials for MIT 6.S191: Introduction to Deep Learning&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aamini/introtodeeplearning/master/assets/banner.png&#34; alt=&#34;banner&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains all of the code and software labs for &lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;MIT Introduction to Deep Learning&lt;/a&gt;! All lecture slides and videos are available on the program website.&lt;/p&gt; &#xA;&lt;h1&gt;Instructions&lt;/h1&gt; &#xA;&lt;p&gt;MIT Introduction to Deep Learning software labs are designed to be completed at your own pace. At the end of each of the labs, there will be instructions on how you can submit your materials as part of the lab competitions. These instructions include what information must be submitted and in what format.&lt;/p&gt; &#xA;&lt;h2&gt;Opening the labs in Google Colaboratory:&lt;/h2&gt; &#xA;&lt;p&gt;The 2023 Introduction to Deep Learning labs will be run in Google&#39;s Colaboratory, a Jupyter notebook environment that runs entirely in the cloud, so you don&#39;t need to download anything. To run these labs, you must have a Google account.&lt;/p&gt; &#xA;&lt;p&gt;On this Github repo, navigate to the lab folder you want to run (&lt;code&gt;lab1&lt;/code&gt;, &lt;code&gt;lab2&lt;/code&gt;, &lt;code&gt;lab3&lt;/code&gt;) and open the appropriate python notebook (*.ipynb). Click the &#34;Run in Colab&#34; link on the top of the lab. That&#39;s it!&lt;/p&gt; &#xA;&lt;h2&gt;Running the labs&lt;/h2&gt; &#xA;&lt;p&gt;Now, to run the labs, open the Jupyter notebook on Colab. Navigate to the &#34;Runtime&#34; tab --&amp;gt; &#34;Change runtime type&#34;. In the pop-up window, under &#34;Runtime type&#34; select &#34;Python 3&#34;, and under &#34;Hardware accelerator&#34; select &#34;GPU&#34;. Go through the notebooks and fill in the &lt;code&gt;#TODO&lt;/code&gt; cells to get the code to compile for yourself!&lt;/p&gt; &#xA;&lt;h3&gt;MIT Deep Learning package&lt;/h3&gt; &#xA;&lt;p&gt;You might notice that inside the labs we install the &lt;code&gt;mitdeeplearning&lt;/code&gt; python package from the Python Package repository:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install mitdeeplearning&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;This package contains convienence functions that we use throughout the course and can be imported like any other Python package.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import mitdeeplearning as mdl&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;We do this for you in each of the labs, but the package is also open source under the same license so you can also use it outside the class.&lt;/p&gt; &#xA;&lt;h2&gt;Lecture Videos&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=njKP3FqW3Sk&amp;amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;amp;index=1&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aamini/introtodeeplearning/master/assets/video_play.png&#34; width=&#34;500&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All lecture videos are available publicly online and linked above! Use and/or modification of lecture slides outside of MIT Introduction to Deep Learning must reference:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;© MIT Introduction to Deep Learning&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;http://introtodeeplearning.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;All code in this repository is copyright 2023 &lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;MIT Introduction to Deep Learning&lt;/a&gt;. All Rights Reserved.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the MIT License. You may not use this file except in compliance with the License. Use and/or modification of this code outside of MIT Introduction to Deep Learning must reference:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;© MIT Introduction to Deep Learning&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;http://introtodeeplearning.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>hemansnation/God-Level-Data-Science-ML-Full-Stack</title>
    <updated>2023-05-01T02:18:24Z</updated>
    <id>tag:github.com,2023-05-01:/hemansnation/God-Level-Data-Science-ML-Full-Stack</id>
    <link href="https://github.com/hemansnation/God-Level-Data-Science-ML-Full-Stack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of scientific methods, processes, algorithms, and systems to build stories &amp; models. This roadmap contains 16 Chapters, whether you are a fresher in the field or an experienced professional who wants to transition into Data Science &amp; AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;God-Level Data Science ML Full Stack&lt;/h1&gt; &#xA;&lt;p&gt;A collection of scientific methods, processes, algorithms, and systems to build stories &amp;amp; models. This roadmap contains 16 Chapters, whether you are a fresher in the field or an experienced professional who wants to transition into Data Science &amp;amp; AI&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/hemansnation/Data-Science-ML-Full-Stack/raw/master/images/components.png&#34;&gt; &#xA;&lt;h3&gt;The‌ ‌Roadmap‌ ‌is‌ ‌divided‌ ‌into‌ ‌16 ‌Sections‌&lt;/h3&gt; &#xA;&lt;p&gt;Duration:‌ ‌256‌ ‌Hours‌ of Learning ‌(8 ‌Months)‌ ‌and many more hours for practice and project building.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#1--python-programming-and-logic-building&#34;&gt;Python‌ ‌Programming‌ ‌and‌ ‌Logic‌ ‌Building‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#2--data-structure--algorithms&#34;&gt;Data‌ ‌Structure‌ ‌&amp;amp;‌ ‌Algorithms‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#3--pandas-numpy-matplotlib&#34;&gt;Pandas‌ ‌Numpy‌ ‌Matplotlib‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#4--statistics&#34;&gt;Statistics‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#5--machine-learning&#34;&gt;Machine‌ ‌Learning‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#6--mlops&#34;&gt;ML Operations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#7--natural-language-processing&#34;&gt;Natural‌ ‌Language‌ ‌Processing‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#8--computer-vision&#34;&gt;Computer‌ ‌Vision‌‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#9--data-visualization-with-tableau&#34;&gt;Data‌ ‌Visualization‌ ‌with‌ ‌Tableau‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#10--structured-query-language-sql&#34;&gt;Structure‌d ‌Query‌ ‌Language‌ ‌(SQL)‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#11--data-engineering&#34;&gt;Data Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#12--data-system-design&#34;&gt;Data System Design&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#13--five-major-projects-and-git&#34;&gt;Five‌ ‌Major‌ Capstone ‌Projects‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#14--interview-preperation&#34;&gt;Interview Preparations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#15--git--github&#34;&gt;Git &amp;amp; GitHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#16--personal-profile--portfolio&#34;&gt;Personal Branding and portfolio&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#resources&#34;&gt;Resources&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;Dataset Collection&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Technology‌ ‌Stack‌&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python‌&lt;/li&gt; &#xA; &lt;li&gt;Data‌ ‌Structures‌&lt;/li&gt; &#xA; &lt;li&gt;NumPy‌&lt;/li&gt; &#xA; &lt;li&gt;Pandas‌&lt;/li&gt; &#xA; &lt;li&gt;Matplotlib‌&lt;/li&gt; &#xA; &lt;li&gt;Seaborn‌&lt;/li&gt; &#xA; &lt;li&gt;Scikit-Learn‌&lt;/li&gt; &#xA; &lt;li&gt;Statsmodels‌&lt;/li&gt; &#xA; &lt;li&gt;Natural‌ ‌Language‌ ‌Toolkit‌ ‌(‌ ‌NLTK‌ ‌)‌&lt;/li&gt; &#xA; &lt;li&gt;PyTorch‌&lt;/li&gt; &#xA; &lt;li&gt;OpenCV‌&lt;/li&gt; &#xA; &lt;li&gt;Tableau‌&lt;/li&gt; &#xA; &lt;li&gt;Structure‌ ‌Query‌ ‌Language‌ ‌(‌ ‌SQL‌ ‌)‌&lt;/li&gt; &#xA; &lt;li&gt;PySpark‌&lt;/li&gt; &#xA; &lt;li&gt;Azure‌ ‌Fundamentals‌&lt;/li&gt; &#xA; &lt;li&gt;Azure‌ ‌Data‌ ‌Factory‌&lt;/li&gt; &#xA; &lt;li&gt;Databricks‌&lt;/li&gt; &#xA; &lt;li&gt;5‌ ‌Major‌ ‌Projects‌&lt;/li&gt; &#xA; &lt;li&gt;Git‌ ‌and‌ ‌GitHub‌ ‌&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;1 | Python Programming and Logic Building&lt;/h1&gt; &#xA;&lt;p&gt;I will prefer Python Programming Language. Python is the best for starting your programming journey. Here is the roadmap of python for logic building.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python basics, Variables, Operators, Conditional Statements&lt;/li&gt; &#xA; &lt;li&gt;List and Strings&lt;/li&gt; &#xA; &lt;li&gt;While Loop, Nested Loops, Loop Else&lt;/li&gt; &#xA; &lt;li&gt;For Loop, Break, and Continue statements&lt;/li&gt; &#xA; &lt;li&gt;Functions, Return Statement, Recursion&lt;/li&gt; &#xA; &lt;li&gt;Dictionary, Tuple, Set&lt;/li&gt; &#xA; &lt;li&gt;File Handling, Exception Handling&lt;/li&gt; &#xA; &lt;li&gt;Object-Oriented Programming&lt;/li&gt; &#xA; &lt;li&gt;Modules and Packages&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hemansnation/Python-Roadmap-2022&#34;&gt;In-Depth Roadmap of Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;2 | Data Structure &amp;amp; Algorithms&lt;/h1&gt; &#xA;&lt;p&gt;Data Structure is the most important thing to learn not only for data scientists but for all the people working in computer science. With data structure, you get an internal understanding of the working of everything in software.&lt;/p&gt; &#xA;&lt;p&gt;Understand these topics&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Types of Algorithm Analysis&lt;/li&gt; &#xA; &lt;li&gt;Asymptotic Notation, Big-O, Omega, Theta&lt;/li&gt; &#xA; &lt;li&gt;Stacks&lt;/li&gt; &#xA; &lt;li&gt;Queues&lt;/li&gt; &#xA; &lt;li&gt;Linked List&lt;/li&gt; &#xA; &lt;li&gt;Trees&lt;/li&gt; &#xA; &lt;li&gt;Graphs&lt;/li&gt; &#xA; &lt;li&gt;Sorting&lt;/li&gt; &#xA; &lt;li&gt;Searching&lt;/li&gt; &#xA; &lt;li&gt;Hashing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;3 | Pandas Numpy Matplotlib&lt;/h1&gt; &#xA;&lt;p&gt;Python supports n-dimensional arrays with Numpy. For data in 2-dimensions, Pandas is the best library for analysis. You can use other tools but tools have drag-and-drop features and have limitations. Pandas can be customized as per the need as we can code depending upon the real-life problem.&lt;/p&gt; &#xA;&lt;h3&gt;Numpy&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vectors, Matrix&lt;/li&gt; &#xA; &lt;li&gt;Operations on Matrix&lt;/li&gt; &#xA; &lt;li&gt;Mean, Variance, and Standard Deviation&lt;/li&gt; &#xA; &lt;li&gt;Reshaping Arrays&lt;/li&gt; &#xA; &lt;li&gt;Transpose and Determinant of Matrix&lt;/li&gt; &#xA; &lt;li&gt;Diagonal Operations, Trace&lt;/li&gt; &#xA; &lt;li&gt;Add, Subtract, Multiply, Dot, and Cross Product.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pandas&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Series and DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Slicing, Rows, and Columns&lt;/li&gt; &#xA; &lt;li&gt;Operations on DataFrame&lt;/li&gt; &#xA; &lt;li&gt;Different ways to create DataFrame&lt;/li&gt; &#xA; &lt;li&gt;Read, Write Operations with CSV files&lt;/li&gt; &#xA; &lt;li&gt;Handling Missing values, replace values, and Regular Expression&lt;/li&gt; &#xA; &lt;li&gt;GroupBy and Concatenation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Matplotlib&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Graph Basics&lt;/li&gt; &#xA; &lt;li&gt;Format Strings in Plots&lt;/li&gt; &#xA; &lt;li&gt;Label Parameters, Legend&lt;/li&gt; &#xA; &lt;li&gt;Bar Chart, Pie Chart, Histogram, Scatter Plot&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;4 | Statistics&lt;/h1&gt; &#xA;&lt;h3&gt;Descriptive Statistics&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Measure of Frequency and Central Tendency&lt;/li&gt; &#xA; &lt;li&gt;Measure of Dispersion&lt;/li&gt; &#xA; &lt;li&gt;Probability Distribution&lt;/li&gt; &#xA; &lt;li&gt;Gaussian Normal Distribution&lt;/li&gt; &#xA; &lt;li&gt;Skewness and Kurtosis&lt;/li&gt; &#xA; &lt;li&gt;Regression Analysis&lt;/li&gt; &#xA; &lt;li&gt;Continuous and Discrete Functions&lt;/li&gt; &#xA; &lt;li&gt;Goodness of Fit&lt;/li&gt; &#xA; &lt;li&gt;Normality Test&lt;/li&gt; &#xA; &lt;li&gt;ANOVA&lt;/li&gt; &#xA; &lt;li&gt;Homoscedasticity&lt;/li&gt; &#xA; &lt;li&gt;Linear and Non-Linear Relationship with Regression&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Inferential Statistics&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;t-Test&lt;/li&gt; &#xA; &lt;li&gt;z-Test&lt;/li&gt; &#xA; &lt;li&gt;Hypothesis Testing&lt;/li&gt; &#xA; &lt;li&gt;Type I and Type II errors&lt;/li&gt; &#xA; &lt;li&gt;t-Test and its types&lt;/li&gt; &#xA; &lt;li&gt;One way ANOVA&lt;/li&gt; &#xA; &lt;li&gt;Two way ANOVA&lt;/li&gt; &#xA; &lt;li&gt;Chi-Square Test&lt;/li&gt; &#xA; &lt;li&gt;Implementation of continuous and categorical data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;5 | Machine Learning&lt;/h1&gt; &#xA;&lt;p&gt;The best way to master machine learning algorithms is to work with the Scikit-Learn framework. Scikit-Learn contains predefined algorithms and you can work with them just by generating the object of the class. These are the algorithm you must know including the types of Supervised and Unsupervised Machine Learning:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linear Regression&lt;/li&gt; &#xA; &lt;li&gt;Logistic Regression&lt;/li&gt; &#xA; &lt;li&gt;Decision Tree&lt;/li&gt; &#xA; &lt;li&gt;Gradient Descent&lt;/li&gt; &#xA; &lt;li&gt;Random Forest&lt;/li&gt; &#xA; &lt;li&gt;Ridge and Lasso Regression&lt;/li&gt; &#xA; &lt;li&gt;Naive Bayes&lt;/li&gt; &#xA; &lt;li&gt;Support Vector Machine&lt;/li&gt; &#xA; &lt;li&gt;KMeans Clustering&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Other Concepts and Topics for ML&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Measuring Accuracy&lt;/li&gt; &#xA; &lt;li&gt;Bias-Variance Trade-off&lt;/li&gt; &#xA; &lt;li&gt;Applying Regularization&lt;/li&gt; &#xA; &lt;li&gt;Elastic Net Regression&lt;/li&gt; &#xA; &lt;li&gt;Predictive Analytics&lt;/li&gt; &#xA; &lt;li&gt;Exploratory Data Analysis&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;6 | MLOps&lt;/h1&gt; &#xA;&lt;p&gt;You can master any one of the cloud services provider from AWS, GCP and Azure. You can switch easily once you understand one of them.&lt;/p&gt; &#xA;&lt;p&gt;We will focus on AWS - Amazon Web Services first&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deploy ML models using Flask&lt;/li&gt; &#xA; &lt;li&gt;Amazon Lex - Natural Language Understanding&lt;/li&gt; &#xA; &lt;li&gt;AWS Polly - Voice Analysis&lt;/li&gt; &#xA; &lt;li&gt;Amazon Transcribe - Speech to Text&lt;/li&gt; &#xA; &lt;li&gt;Amazon Textract - Extract Text&lt;/li&gt; &#xA; &lt;li&gt;Amazon Rekognition - Image Applications&lt;/li&gt; &#xA; &lt;li&gt;Amazon SageMaker - Building and deploying models&lt;/li&gt; &#xA; &lt;li&gt;Working with Deep Learning on AWS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;7 | Natural Language Processing&lt;/h1&gt; &#xA;&lt;p&gt;If you are interested in working with Text, you should do some of the work an NLP Engineer do and understand the working of Language models.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sentiment analysis&lt;/li&gt; &#xA; &lt;li&gt;POS Tagging, Parsing,&lt;/li&gt; &#xA; &lt;li&gt;Text preprocessing&lt;/li&gt; &#xA; &lt;li&gt;Stemming and Lemmatization&lt;/li&gt; &#xA; &lt;li&gt;Sentiment classification using Naive Bayes&lt;/li&gt; &#xA; &lt;li&gt;TF-IDF, N-gram,&lt;/li&gt; &#xA; &lt;li&gt;Machine Translation, BLEU Score&lt;/li&gt; &#xA; &lt;li&gt;Text Generation, Summarization, ROUGE Score&lt;/li&gt; &#xA; &lt;li&gt;Language Modeling, Perplexity&lt;/li&gt; &#xA; &lt;li&gt;Building a text classifier&lt;/li&gt; &#xA; &lt;li&gt;Identifying the gender&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;8 | Computer Vision&lt;/h1&gt; &#xA;&lt;p&gt;To work on image and video analytics we can master computer vision. To work on computer vision we have to understand images.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch Tensors&lt;/li&gt; &#xA; &lt;li&gt;Understanding Pretrained models like AlexNet, ImageNet, ResNet.&lt;/li&gt; &#xA; &lt;li&gt;Neural Networks&lt;/li&gt; &#xA; &lt;li&gt;Building a perceptron&lt;/li&gt; &#xA; &lt;li&gt;Building a single layer neural network&lt;/li&gt; &#xA; &lt;li&gt;Building a deep neural network&lt;/li&gt; &#xA; &lt;li&gt;Recurrent neural network for sequential data analysis&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Convolutional Neural Networks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understanding the ConvNet topology&lt;/li&gt; &#xA; &lt;li&gt;Convolution layers&lt;/li&gt; &#xA; &lt;li&gt;Pooling layers&lt;/li&gt; &#xA; &lt;li&gt;Image Content Analysis&lt;/li&gt; &#xA; &lt;li&gt;Operating on images using OpenCV-Python&lt;/li&gt; &#xA; &lt;li&gt;Detecting edges&lt;/li&gt; &#xA; &lt;li&gt;Histogram equalization&lt;/li&gt; &#xA; &lt;li&gt;Detecting corners&lt;/li&gt; &#xA; &lt;li&gt;Detecting SIFT feature points&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;9 | Data Visualization with Tableau&lt;/h1&gt; &#xA;&lt;p&gt;How to use it Visual Perception&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What is it, How it works, Why Tableau&lt;/li&gt; &#xA; &lt;li&gt;Connecting to Data&lt;/li&gt; &#xA; &lt;li&gt;Building charts&lt;/li&gt; &#xA; &lt;li&gt;Calculations&lt;/li&gt; &#xA; &lt;li&gt;Dashboards&lt;/li&gt; &#xA; &lt;li&gt;Sharing our work&lt;/li&gt; &#xA; &lt;li&gt;Advanced Charts, Calculated Fields, Calculated Aggregations&lt;/li&gt; &#xA; &lt;li&gt;Conditional Calculation, Parameterized Calculation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;10 | Structured Query Language (SQL)&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fundamental to SQL syntax and Installation&lt;/li&gt; &#xA; &lt;li&gt;Creating Tables, Modifiers&lt;/li&gt; &#xA; &lt;li&gt;Inserting and Retrieving Data, SELECT INSERT UPDATE DELETE&lt;/li&gt; &#xA; &lt;li&gt;Aggregating Data using Functions, Filtering and RegEX&lt;/li&gt; &#xA; &lt;li&gt;Subqueries, retrieve data based on conditions, grouping of Data.&lt;/li&gt; &#xA; &lt;li&gt;Practice Questions&lt;/li&gt; &#xA; &lt;li&gt;JOINs&lt;/li&gt; &#xA; &lt;li&gt;Advanced SQL concepts such as transactions, views, stored procedures, and functions.&lt;/li&gt; &#xA; &lt;li&gt;Database Design principles, normalization, and ER diagrams.&lt;/li&gt; &#xA; &lt;li&gt;Practice, Practice, Practice: Practice writing SQL queries on real-world datasets, and work on projects to apply your knowledge.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;11 | Data Engineering&lt;/h1&gt; &#xA;&lt;h3&gt;BigData&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What is BigData?&lt;/li&gt; &#xA; &lt;li&gt;How is BigData applied within Business?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;PySpark&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Resilient Distributed Datasets&lt;/li&gt; &#xA; &lt;li&gt;Schema&lt;/li&gt; &#xA; &lt;li&gt;Lambda Expressions&lt;/li&gt; &#xA; &lt;li&gt;Transformations&lt;/li&gt; &#xA; &lt;li&gt;Actions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data Modeling&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Duplicate Data&lt;/li&gt; &#xA; &lt;li&gt;Descriptive Analysis on Data&lt;/li&gt; &#xA; &lt;li&gt;Visualizations&lt;/li&gt; &#xA; &lt;li&gt;ML lib&lt;/li&gt; &#xA; &lt;li&gt;ML Packages&lt;/li&gt; &#xA; &lt;li&gt;Pipelines&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Streaming&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Packaging Spark Applications&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;12 | Data System Design&lt;/h1&gt; &#xA;&lt;h3&gt;What is system design?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;IP and OSI Model&lt;/li&gt; &#xA; &lt;li&gt;Domain Name System (DNS)&lt;/li&gt; &#xA; &lt;li&gt;Load Balancing&lt;/li&gt; &#xA; &lt;li&gt;Clustering&lt;/li&gt; &#xA; &lt;li&gt;Caching&lt;/li&gt; &#xA; &lt;li&gt;Availability, Scalability, Storage&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Databases and DBMS&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SQL databases&lt;/li&gt; &#xA; &lt;li&gt;NoSQL databases&lt;/li&gt; &#xA; &lt;li&gt;SQL vs NoSQL databases&lt;/li&gt; &#xA; &lt;li&gt;Database Replication&lt;/li&gt; &#xA; &lt;li&gt;Indexes&lt;/li&gt; &#xA; &lt;li&gt;Normalization and Denormalization&lt;/li&gt; &#xA; &lt;li&gt;CAP theorem&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;System Design Interview&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;URL Shortener&lt;/li&gt; &#xA; &lt;li&gt;Whatsapp, Twitter, Netflix, Uber&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;13 | Five Major Projects and Git&lt;/h1&gt; &#xA;&lt;p&gt;We follow project-based learning and we will work on all the projects in parallel.&lt;/p&gt; &#xA;&lt;h1&gt;14 | Interview Preperation&lt;/h1&gt; &#xA;&lt;h1&gt;15 | Git &amp;amp; GitHub&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://god-level-python.notion.site/Git-GitHub-Course-Make-Recruiters-reach-You-Build-your-stunning-profile-First-open-source-cont-1d4d70450aa94dd7ad2c062c0fec3cb8&#34;&gt;Git &amp;amp; GitHub Course&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understanding Git&lt;/li&gt; &#xA; &lt;li&gt;Commands and How to commit your first code?&lt;/li&gt; &#xA; &lt;li&gt;How to use GitHub?&lt;/li&gt; &#xA; &lt;li&gt;How to make your first open-source contribution?&lt;/li&gt; &#xA; &lt;li&gt;How to work with a team? - Part 1&lt;/li&gt; &#xA; &lt;li&gt;How to create your stunning GitHub profile?&lt;/li&gt; &#xA; &lt;li&gt;How to build your own viral repository?&lt;/li&gt; &#xA; &lt;li&gt;Building a personal landing page for your Portfolio for FREE&lt;/li&gt; &#xA; &lt;li&gt;How to grow followers on GitHub?&lt;/li&gt; &#xA; &lt;li&gt;How to work with a team? Part 2 - issues, milestone and projects&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;16 | Personal Profile &amp;amp; Portfolio&lt;/h1&gt; &#xA;&lt;h1&gt;Resources&lt;/h1&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;1️⃣ &lt;a href=&#34;https://github.com/awesomedata/awesome-public-datasets&#34;&gt;Awesome Public Datasets&lt;/a&gt; This list of a topic-centric public data sources in high quality.&lt;/p&gt; &#xA;&lt;p&gt;2️⃣&lt;a href=&#34;https://github.com/niderhoff/nlp-datasets&#34;&gt;NLP Datasets&lt;/a&gt; Alphabetical list of free/public domain datasets with text data for use in NLP.&lt;/p&gt; &#xA;&lt;p&gt;3️⃣&lt;a href=&#34;https://github.com/jsbroks/awesome-dataset-tools&#34;&gt;Awesome Dataset Tools&lt;/a&gt; A curated list of awesome dataset tools.&lt;/p&gt; &#xA;&lt;p&gt;4️⃣&lt;a href=&#34;https://github.com/xephonhq/awesome-time-series-database&#34;&gt;Awesome time series database&lt;/a&gt; A curated list of time series databases.&lt;/p&gt; &#xA;&lt;p&gt;5️⃣&lt;a href=&#34;https://github.com/shramos/Awesome-Cybersecurity-Datasets&#34;&gt;Awesome-Cybersecurity-Datasets&lt;/a&gt; A curated list of amazingly awesome Cybersecurity datasets.&lt;/p&gt; &#xA;&lt;p&gt;6️⃣&lt;a href=&#34;https://github.com/mint-lab/awesome-robotics-datasets&#34;&gt;Awesome Robotics Datasets&lt;/a&gt; Robotics Dataset Collections.&lt;/p&gt; &#xA;&lt;h1&gt;Research Starting Point&lt;/h1&gt; &#xA;&lt;h3&gt;Machine Learning&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ime.unicamp.br/~dias/Intoduction%20to%20Statistical%20Learning.pdf&#34;&gt;Introduction to Statistical Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Deep Learning&lt;/h3&gt; &#xA;&lt;h3&gt;Reinforcement Learning&lt;/h3&gt; &#xA;&lt;h1&gt;Projects&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/tree/master/projects&#34;&gt;Here is the list of project ideas&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Data Science ML Full Stack -&amp;gt; &lt;a href=&#34;https://god-level-python.notion.site/Data-Science-ML-Full-Stack-Roadmap-05e6c16389c845d1a438ed4cff2b9952&#34;&gt;Notion Template&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h3&gt;Join the WhatsApp Community Group&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chat.whatsapp.com/BSUPbYhzzM1BcJplcTTIxb&#34;&gt;https://chat.whatsapp.com/BSUPbYhzzM1BcJplcTTIxb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Socials&lt;/h1&gt; &#xA;&lt;h3&gt;Join Telegram for Data Science ML AI Resources:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://t.me/+sREuRiFssMo4YWJl&#34;&gt;https://t.me/+sREuRiFssMo4YWJl&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Connect with me on these platforms:&lt;/h3&gt; &#xA;&lt;p&gt;LinkedIn: &lt;a href=&#34;https://www.linkedin.com/in/hemansnation/&#34;&gt;https://www.linkedin.com/in/hemansnation/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;YouTube: &lt;a href=&#34;https://www.youtube.com/@Himanshu-Ramchandani&#34;&gt;https://www.youtube.com/@Himanshu-Ramchandani&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Twitter: &lt;a href=&#34;https://twitter.com/hemansnation&#34;&gt;https://twitter.com/hemansnation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;GitHub: &lt;a href=&#34;https://github.com/hemansnation&#34;&gt;https://github.com/hemansnation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Instagram: &lt;a href=&#34;https://www.instagram.com/masterdexter.ai/&#34;&gt;https://www.instagram.com/masterdexter.ai/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;AI Jobs LinkedIn Group:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/groups/12540639/&#34;&gt;https://www.linkedin.com/groups/12540639/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Medium Blog:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://medium.com/@hemansnation&#34;&gt;https://medium.com/@hemansnation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Notes on Data, Product, and AI - Newsletter:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/build-relation/newsletter-follow?entityUrn=7014799989251956736&#34;&gt;https://www.linkedin.com/build-relation/newsletter-follow?entityUrn=7014799989251956736&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Any Query?&lt;/h3&gt; &#xA;&lt;p&gt;Email Me Here: &lt;a href=&#34;mailto:connect@himanshuramchandani.co&#34;&gt;connect@himanshuramchandani.co&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tloen/alpaca-lora</title>
    <updated>2023-05-01T02:18:24Z</updated>
    <id>tag:github.com,2023-05-01:/tloen/alpaca-lora</id>
    <link href="https://github.com/tloen/alpaca-lora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Instruct-tune LLaMA on consumer hardware&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🦙🌲🤏 Alpaca-LoRA&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🤗 &lt;strong&gt;Try the pretrained model out &lt;a href=&#34;https://huggingface.co/spaces/tloen/alpaca-lora&#34;&gt;here&lt;/a&gt;, courtesy of a GPU grant from Huggingface!&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Users have created a Discord server for discussion and support &lt;a href=&#34;https://discord.gg/prbq284xX5&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;4/14: Chansung Park&#39;s GPT4-Alpaca adapters: &lt;a href=&#34;https://github.com/tloen/alpaca-lora/issues/340&#34;&gt;https://github.com/tloen/alpaca-lora/issues/340&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This repository contains code for reproducing the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; results using &lt;a href=&#34;https://arxiv.org/pdf/2106.09685.pdf&#34;&gt;low-rank adaptation (LoRA)&lt;/a&gt;. We provide an Instruct model of similar quality to &lt;code&gt;text-davinci-003&lt;/code&gt; that can run &lt;a href=&#34;https://twitter.com/miolini/status/1634982361757790209&#34;&gt;on a Raspberry Pi&lt;/a&gt; (for research), and the code is easily extended to the &lt;code&gt;13b&lt;/code&gt;, &lt;code&gt;30b&lt;/code&gt;, and &lt;code&gt;65b&lt;/code&gt; models.&lt;/p&gt; &#xA;&lt;p&gt;In addition to the training code, which runs within hours on a single RTX 4090, we publish a script for downloading and inference on the foundation model and LoRA, as well as the resulting &lt;a href=&#34;https://huggingface.co/tloen/alpaca-lora-7b/tree/main&#34;&gt;LoRA weights themselves&lt;/a&gt;. To fine-tune cheaply and efficiently, we use Hugging Face&#39;s &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt; as well as Tim Dettmers&#39; &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Without hyperparameter tuning, the LoRA model produces outputs comparable to the Stanford Alpaca model. (Please see the outputs included below.) Further tuning might be able to achieve better performance; I invite interested users to give it a try and report their results.&lt;/p&gt; &#xA;&lt;h3&gt;Local Setup&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If bitsandbytes doesn&#39;t work, &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes/raw/main/compile_from_source.md&#34;&gt;install it from source.&lt;/a&gt; Windows users can follow &lt;a href=&#34;https://github.com/tloen/alpaca-lora/issues/17&#34;&gt;these instructions&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Training (&lt;code&gt;finetune.py&lt;/code&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;This file contains a straightforward application of PEFT to the LLaMA model, as well as some code related to prompt construction and tokenization. PRs adapting this code to support larger models are always welcome.&lt;/p&gt; &#xA;&lt;p&gt;Example usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune.py \&#xA;    --base_model &#39;decapoda-research/llama-7b-hf&#39; \&#xA;    --data_path &#39;yahma/alpaca-cleaned&#39; \&#xA;    --output_dir &#39;./lora-alpaca&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can also tweak our hyperparameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune.py \&#xA;    --base_model &#39;decapoda-research/llama-7b-hf&#39; \&#xA;    --data_path &#39;yahma/alpaca-cleaned&#39; \&#xA;    --output_dir &#39;./lora-alpaca&#39; \&#xA;    --batch_size 128 \&#xA;    --micro_batch_size 4 \&#xA;    --num_epochs 3 \&#xA;    --learning_rate 1e-4 \&#xA;    --cutoff_len 512 \&#xA;    --val_set_size 2000 \&#xA;    --lora_r 8 \&#xA;    --lora_alpha 16 \&#xA;    --lora_dropout 0.05 \&#xA;    --lora_target_modules &#39;[q_proj,v_proj]&#39; \&#xA;    --train_on_inputs \&#xA;    --group_by_length&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference (&lt;code&gt;generate.py&lt;/code&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;This file reads the foundation model from the Hugging Face model hub and the LoRA weights from &lt;code&gt;tloen/alpaca-lora-7b&lt;/code&gt;, and runs a Gradio interface for inference on a specified input. Users should treat this as example code for the use of the model, and modify it as needed.&lt;/p&gt; &#xA;&lt;p&gt;Example usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py \&#xA;    --load_8bit \&#xA;    --base_model &#39;decapoda-research/llama-7b-hf&#39; \&#xA;    --lora_weights &#39;tloen/alpaca-lora-7b&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Official weights&lt;/h3&gt; &#xA;&lt;p&gt;The most recent &#34;official&#34; Alpaca-LoRA adapter available at &lt;a href=&#34;https://huggingface.co/tloen/alpaca-lora-7b&#34;&gt;&lt;code&gt;tloen/alpaca-lora-7b&lt;/code&gt;&lt;/a&gt; was trained on March 26 with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune.py \&#xA;    --base_model=&#39;decapoda-research/llama-7b-hf&#39; \&#xA;    --num_epochs=10 \&#xA;    --cutoff_len=512 \&#xA;    --group_by_length \&#xA;    --output_dir=&#39;./lora-alpaca&#39; \&#xA;    --lora_target_modules=&#39;[q_proj,k_proj,v_proj,o_proj]&#39; \&#xA;    --lora_r=16 \&#xA;    --micro_batch_size=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Checkpoint export (&lt;code&gt;export_*_checkpoint.py&lt;/code&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;These files contain scripts that merge the LoRA weights back into the base model for export to Hugging Face format and to PyTorch &lt;code&gt;state_dicts&lt;/code&gt;. They should help users who want to run inference in projects like &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; or &lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;alpaca.cpp&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Docker Setup &amp;amp; Inference&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Build the container image:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t alpaca-lora .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the container (you can also use &lt;code&gt;finetune.py&lt;/code&gt; and all of its parameters as shown above for training):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus=all --shm-size 64g -p 7860:7860 -v ${HOME}/.cache:/root/.cache --rm alpaca-lora generate.py \&#xA;    --load_8bit \&#xA;    --base_model &#39;decapoda-research/llama-7b-hf&#39; \&#xA;    --lora_weights &#39;tloen/alpaca-lora-7b&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Open &lt;code&gt;https://localhost:7860&lt;/code&gt; in the browser&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Docker Compose Setup &amp;amp; Inference&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;(optional) Change desired model and weights under &lt;code&gt;environment&lt;/code&gt; in the &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build and run the container&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up -d --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Open &lt;code&gt;https://localhost:7860&lt;/code&gt; in the browser&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;See logs:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose logs -f&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Clean everything up:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose down --volumes --rmi all&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We can likely improve our model performance significantly if we had a better dataset. Consider supporting the &lt;a href=&#34;https://open-assistant.io/&#34;&gt;LAION Open Assistant&lt;/a&gt; effort to produce a high-quality dataset for supervised fine-tuning (or bugging them to release their data).&lt;/li&gt; &#xA; &lt;li&gt;We&#39;re continually fixing bugs and conducting training runs, and the weights on the Hugging Face Hub are being updated accordingly. In particular, those facing issues with response lengths should make sure that they have the latest version of the weights and code.&lt;/li&gt; &#xA; &lt;li&gt;Users with multiple GPUs should take a look &lt;a href=&#34;https://github.com/tloen/alpaca-lora/issues/8#issuecomment-1477490259&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We include the Stanford Alpaca dataset, which was made available under the ODC Attribution License.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Resources&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;alpaca.cpp&lt;/a&gt;, a native client for running Alpaca models on the CPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deep-diver/Alpaca-LoRA-Serve&#34;&gt;Alpaca-LoRA-Serve&lt;/a&gt;, a ChatGPT-style interface for Alpaca models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gururise/AlpacaDataCleaned&#34;&gt;AlpacaDataCleaned&lt;/a&gt;, a project to improve the quality of the Alpaca dataset&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;GPT-4 Alpaca Data&lt;/a&gt; a project to port synthetic data creation to GPT-4&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/c-s-ale/dolly-15k-instruction-alpaca-format&#34;&gt;dolly-15k-instruction-alpaca-format&lt;/a&gt;, an Alpaca-compatible version of &lt;a href=&#34;https://github.com/databrickslabs/dolly/tree/master/data&#34;&gt;Databricks&#39; Dolly 15k human-generated instruct dataset&lt;/a&gt; (see &lt;a href=&#34;https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm&#34;&gt;blog&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/juletx/alpaca-lora-mt&#34;&gt;Alpaca-LoRA MT&lt;/a&gt;, a project to finetune models with &lt;a href=&#34;https://huggingface.co/datasets/HiTZ/alpaca_mt&#34;&gt;machine-translated Alpaca data&lt;/a&gt; in 6 Iberian languages: Portuguese, Spanish, Catalan, Basque, Galician and Asturian.&lt;/li&gt; &#xA; &lt;li&gt;Various adapter weights (download at own risk): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;7B: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;3️⃣ &lt;a href=&#34;https://huggingface.co/tloen/alpaca-lora-7b&#34;&gt;https://huggingface.co/tloen/alpaca-lora-7b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;3️⃣ &lt;a href=&#34;https://huggingface.co/samwit/alpaca7B-lora&#34;&gt;https://huggingface.co/samwit/alpaca7B-lora&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;4️⃣ &lt;a href=&#34;https://huggingface.co/chansung/gpt4-alpaca-lora-7b&#34;&gt;https://huggingface.co/chansung/gpt4-alpaca-lora-7b&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🚀 &lt;a href=&#34;https://huggingface.co/nomic-ai/gpt4all-lora&#34;&gt;https://huggingface.co/nomic-ai/gpt4all-lora&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇧🇷 &lt;a href=&#34;https://huggingface.co/22h/cabrita-lora-v0-1&#34;&gt;https://huggingface.co/22h/cabrita-lora-v0-1&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇨🇳 &lt;a href=&#34;https://huggingface.co/qychen/luotuo-lora-7b-0.1&#34;&gt;https://huggingface.co/qychen/luotuo-lora-7b-0.1&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇨🇳 &lt;a href=&#34;https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b&#34;&gt;https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇯🇵 &lt;a href=&#34;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-7b-v0&#34;&gt;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-7b-v0&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇫🇷 &lt;a href=&#34;https://huggingface.co/bofenghuang/vigogne-lora-7b&#34;&gt;https://huggingface.co/bofenghuang/vigogne-lora-7b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇹🇭 &lt;a href=&#34;https://huggingface.co/Thaweewat/thai-buffala-lora-7b-v0-1&#34;&gt;https://huggingface.co/Thaweewat/thai-buffala-lora-7b-v0-1&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇩🇪 &lt;a href=&#34;https://huggingface.co/thisserand/alpaca_lora_german&#34;&gt;https://huggingface.co/thisserand/alpaca_lora_german&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇵🇱 &lt;a href=&#34;https://huggingface.co/mmosiolek/polpaca-lora-7b&#34;&gt;https://huggingface.co/mmosiolek/polpaca-lora-7b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇵🇱 &lt;a href=&#34;https://huggingface.co/chrisociepa/alpaca-lora-7b-pl&#34;&gt;https://huggingface.co/chrisociepa/alpaca-lora-7b-pl&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇮🇹 &lt;a href=&#34;https://huggingface.co/teelinsan/camoscio-7b-llama&#34;&gt;https://huggingface.co/teelinsan/camoscio-7b-llama&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇷🇺 &lt;a href=&#34;https://huggingface.co/IlyaGusev/llama_7b_ru_turbo_alpaca_lora&#34;&gt;https://huggingface.co/IlyaGusev/llama_7b_ru_turbo_alpaca_lora&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇺🇦 &lt;a href=&#34;https://huggingface.co/robinhad/ualpaca-7b-llama&#34;&gt;https://huggingface.co/robinhad/ualpaca-7b-llama&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇮🇹 &lt;a href=&#34;https://huggingface.co/mchl-labs/stambecco-7b-plus&#34;&gt;https://huggingface.co/mchl-labs/stambecco-7b-plus&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇪🇸 &lt;a href=&#34;https://huggingface.co/plncmm/guanaco-lora-7b&#34;&gt;https://huggingface.co/plncmm/guanaco-lora-7b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇬🇧 🇪🇸 🇵🇹 &lt;a href=&#34;https://huggingface.co/HiTZ/alpaca-lora-7b-en-pt-es-ca-eu-gl-at&#34;&gt;https://huggingface.co/HiTZ/alpaca-lora-7b-en-pt-es-ca-eu-gl-at&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;13B: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;3️⃣ &lt;a href=&#34;https://huggingface.co/Angainor/alpaca-lora-13b&#34;&gt;https://huggingface.co/Angainor/alpaca-lora-13b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;3️⃣ &lt;a href=&#34;https://huggingface.co/chansung/alpaca-lora-13b&#34;&gt;https://huggingface.co/chansung/alpaca-lora-13b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;3️⃣ &lt;a href=&#34;https://huggingface.co/mattreid/alpaca-lora-13b&#34;&gt;https://huggingface.co/mattreid/alpaca-lora-13b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;3️⃣ &lt;a href=&#34;https://huggingface.co/samwit/alpaca13B-lora&#34;&gt;https://huggingface.co/samwit/alpaca13B-lora&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;4️⃣ &lt;a href=&#34;https://huggingface.co/chansung/gpt4-alpaca-lora-13b&#34;&gt;https://huggingface.co/chansung/gpt4-alpaca-lora-13b&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇯🇵 &lt;a href=&#34;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-13b-v0&#34;&gt;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-13b-v0&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇰🇷 &lt;a href=&#34;https://huggingface.co/chansung/koalpaca-lora-13b&#34;&gt;https://huggingface.co/chansung/koalpaca-lora-13b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇨🇳 &lt;a href=&#34;https://huggingface.co/facat/alpaca-lora-cn-13b&#34;&gt;https://huggingface.co/facat/alpaca-lora-cn-13b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇨🇳 &lt;a href=&#34;https://huggingface.co/ziqingyang/chinese-alpaca-lora-13b&#34;&gt;https://huggingface.co/ziqingyang/chinese-alpaca-lora-13b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇪🇸 &lt;a href=&#34;https://huggingface.co/plncmm/guanaco-lora-13b&#34;&gt;https://huggingface.co/plncmm/guanaco-lora-13b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇮🇹 &lt;a href=&#34;https://huggingface.co/mchl-labs/stambecco-13b-plus&#34;&gt;https://huggingface.co/mchl-labs/stambecco-13b-plus&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇬🇧 🇪🇸 🇵🇹 &lt;a href=&#34;https://huggingface.co/HiTZ/alpaca-lora-13b-en-pt-es-ca-eu-gl-at&#34;&gt;https://huggingface.co/HiTZ/alpaca-lora-13b-en-pt-es-ca-eu-gl-at&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;30B: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;3️⃣ &lt;a href=&#34;https://huggingface.co/baseten/alpaca-30b&#34;&gt;https://huggingface.co/baseten/alpaca-30b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;3️⃣ &lt;a href=&#34;https://huggingface.co/chansung/alpaca-lora-30b&#34;&gt;https://huggingface.co/chansung/alpaca-lora-30b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;4️⃣ &lt;a href=&#34;https://huggingface.co/chansung/gpt4-alpaca-lora-30b&#34;&gt;https://huggingface.co/chansung/gpt4-alpaca-lora-30b&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇯🇵 &lt;a href=&#34;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-30b-v0&#34;&gt;https://huggingface.co/kunishou/Japanese-Alapaca-LoRA-30b-v0&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇬🇧 🇪🇸 🇵🇹 &lt;a href=&#34;https://huggingface.co/HiTZ/alpaca-lora-30b-en-pt-es-ca-eu-gl-at&#34;&gt;https://huggingface.co/HiTZ/alpaca-lora-30b-en-pt-es-ca-eu-gl-at&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;65B &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/chansung/alpaca-lora-65b&#34;&gt;https://huggingface.co/chansung/alpaca-lora-65b&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🇬🇧 🇪🇸 🇵🇹 &lt;a href=&#34;https://huggingface.co/HiTZ/alpaca-lora-65b-en-pt-es-ca&#34;&gt;https://huggingface.co/HiTZ/alpaca-lora-65b-en-pt-es-ca&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/chavinlo/alpaca-native&#34;&gt;alpaca-native&lt;/a&gt;, a replication using the original Alpaca code&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tpoisonooo/llama.onnx&#34;&gt;llama.onnx&lt;/a&gt;, a project to inference alpaca with onnx format&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Example outputs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Tell me about alpacas.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: Alpacas are members of the camelid family and are native to the Andes Mountains of South America. They are known for their soft, luxurious fleece, which is used to make clothing, blankets, and other items. Alpacas are herbivores and graze on grasses and other plants. They are social animals and live in herds of up to 20 individuals.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: Alpacas are small, fluffy animals related to camels and llamas. They are native to Peru and Bolivia, and were first domesticated around 5,000 years ago. They are kept mainly for their fine, soft fleece, which is used to make knitwear and other garments. Alpacas are herd animals and live in small family groups, led by an older male. They are highly social animals and can form strong bonds with their owners.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: Alpacas are a domesticated species of South American camelid, similar to llamas. They are native to the Andes Mountains in South America and are kept as livestock for their soft, luxurious wool. Alpacas are social animals and live in herds of up to 20 individuals. They are typically kept in small herds of two to five animals, and are relatively easy to care for. Alpacas are herbivores and feed on grass, hay, and other vegetation. They are also known for their gentle and friendly nature, making them popular as pets.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Tell me about the president of Mexico in 2019.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: The president of Mexico in 2019 was Andrés Manuel López Obrador, who took office on December 1, 2018. He is a member of the National Regeneration Movement (MORENA) political party and is the first left-wing president of Mexico since 1946. He is known for his anti-corruption and anti-neoliberal policies, as well as his commitment to improving the living conditions of the Mexican people.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: In 2019, Andres Manuel Lopez Obrador was sworn in as President of Mexico. He is the country’s 58th president and the first from the left-wing Morena party. He was elected in 2018 on a promise to combat corruption and reduce poverty and has faced criticism for his handling of the economy, crime and the country’s relationship with the United States. He is also known for his ambitious social welfare programs and has been praised for raising the minimum wage and providing aid to low-income families. He is seen as a champion of democracy and human rights in Mexico.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: The president of Mexico in 2019 is Andrés Manuel López Obrador, commonly known as AMLO. He is a Mexican politician who has been the President of Mexico since December 1, 2018. He is a member of the National Regeneration Movement (MORENA) party and previously served as the Head of Government of Mexico City from 2000 to 2005. He is a left-wing populist and has promised to reduce corruption and inequality in Mexico. He has also promised to increase public spending on social programs and infrastructure projects.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Tell me about the king of France in 2019.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: The King of France in 2019 is Emmanuel Macron, who was elected in May 2017. He is the youngest president in the history of the Fifth Republic and the first president to be born after World War II. He is also the first president to have never held elected office before.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: In 2019, the King of France was Emmanuel Macron. He was sworn in as President of France on May 14, 2017, and has been leading the country ever since. He is a member of the centrist La République En Marche! Party. He is known for his reformist agenda and has faced a number of protests from both the right and left of the political spectrum. He is the youngest French president in history.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: The King of France is a largely symbolic role, as France is a republic. The current President of France is Emmanuel Macron, who was elected in 2017.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: List all Canadian provinces in alphabetical order.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island, Quebec, Saskatchewan&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island, Quebec, Saskatchewan, Northwest Territories, Nunavut, Yukon.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Alberta&lt;/li&gt; &#xA; &lt;li&gt;British Columbia&lt;/li&gt; &#xA; &lt;li&gt;Manitoba&lt;/li&gt; &#xA; &lt;li&gt;New Brunswick&lt;/li&gt; &#xA; &lt;li&gt;Newfoundland and Labrador&lt;/li&gt; &#xA; &lt;li&gt;Northwest Territories&lt;/li&gt; &#xA; &lt;li&gt;Nova Scotia&lt;/li&gt; &#xA; &lt;li&gt;Nunavut&lt;/li&gt; &#xA; &lt;li&gt;Ontario&lt;/li&gt; &#xA; &lt;li&gt;Prince Edward Island&lt;/li&gt; &#xA; &lt;li&gt;Quebec&lt;/li&gt; &#xA; &lt;li&gt;Saskatchewan&lt;/li&gt; &#xA; &lt;li&gt;Yukon&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Write a Python program that prints the first 10 Fibonacci numbers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def fibonacci(n):&#xA;    if n == 0:&#xA;        return 0&#xA;    elif n == 1:&#xA;        return 1&#xA;    else:&#xA;        return fibonacci(n-1) + fibonacci(n-2)&#xA;&#xA;print(fibonacci(10))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;numbers = [0, 1]&#xA;&#xA;for i in range(2, 11):&#xA;    numbers.append(numbers[i-2] + numbers[i-1])&#xA;&#xA;print(numbers)&#xA;# Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fibonacci numbers are the numbers in the following integer sequence, called the Fibonacci sequence, and characterized by the fact that every number after the first two is the sum of the two preceding ones.&#xA;&#xA;# Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...&#xA;&#xA;# Program:&#xA;&#xA;# Initialize the first two numbers of the Fibonacci sequence&#xA;a = 0&#xA;b = 1&#xA;&#xA;# Print the first 10 numbers of the Fibonacci sequence&#xA;print(a)&#xA;print(b)&#xA;&#xA;for i in range(2,10):&#xA;    c = a + b&#xA;    a = b&#xA;    b = c&#xA;    print(c)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Write a program that prints the numbers from 1 to 100. But for multiples of three print &#39;Fizz&#39; instead of the number and for the multiples of five print &#39;Buzz&#39;. For numbers which are multiples of both three and five print &#39;FizzBuzz&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(1, 101):&#xA;    if i % 3 == 0 and i % 5 == 0:&#xA;        print(&#34;FizzBuzz&#34;)&#xA;    elif i % 3 == 0:&#xA;        print(&#34;Fizz&#34;)&#xA;    elif i % 5 == 0:&#xA;        print(&#34;Buzz&#34;)&#xA;    else:&#xA;        print(i)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(1, 101):&#xA;    if i % 3 == 0 and i % 5 == 0:&#xA;        print(&#39;FizzBuzz&#39;)&#xA;    elif i % 3 == 0:&#xA;        print(&#39;Fizz&#39;)&#xA;    elif i % 5 == 0:&#xA;        print(&#39;Buzz&#39;)&#xA;    else:&#xA;        print(i)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;for (let i = 1; i &amp;lt;= 100; i++) {&#xA;  if (i % 3 === 0 &amp;amp;&amp;amp; i % 5 === 0) {&#xA;    console.log(&#34;FizzBuzz&#34;);&#xA;  } else if (i % 3 === 0) {&#xA;    console.log(&#34;Fizz&#34;);&#xA;  } else if (i % 5 === 0) {&#xA;    console.log(&#34;Buzz&#34;);&#xA;  } else {&#xA;    console.log(i);&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Tell me five words that rhyme with &#39;shock&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: Flock, lock, rock, stock, and sock.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: Five words that rhyme with shock are: rock, pop, shock, cook, and snock.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Block&lt;/li&gt; &#xA; &lt;li&gt;Lock&lt;/li&gt; &#xA; &lt;li&gt;Dock&lt;/li&gt; &#xA; &lt;li&gt;Clock&lt;/li&gt; &#xA; &lt;li&gt;Rock&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction&lt;/strong&gt;: Translate the sentence &#39;I have no mouth but I must scream&#39; into Spanish.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: No tengo boca pero tengo que gritar.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: No tengo boca, pero debo gritar.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: No tengo boca pero debo gritar.&lt;/p&gt;</summary>
  </entry>
</feed>