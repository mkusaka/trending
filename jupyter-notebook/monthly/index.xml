<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-01T01:48:17Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVIDIA/NeMo-Curator</title>
    <updated>2024-11-01T01:48:17Z</updated>
    <id>tag:github.com,2024-11-01:/NVIDIA/NeMo-Curator</id>
    <link href="https://github.com/NVIDIA/NeMo-Curator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scalable data pre processing and curation toolkit for LLMs&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/NeMo-Curator/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/NVIDIA/NeMo-Curator&#34; alt=&#34;https://pypi.org/project/nemo-curator&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/nemo-curator/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/nemo-curator.svg?sanitize=true&#34; alt=&#34;https://pypi.org/project/nemo-curator/&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NVIDIA/NeMo-Curator/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/NVIDIA/NeMo-Curator&#34; alt=&#34;NVIDIA/NeMo-Curator&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NVIDIA/NeMo-Curator/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/NVIDIA/NeMo-Curator&#34; alt=&#34;https://github.com/NVIDIA/NeMo-Curator/releases&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/nemo-curator/&#34;&gt;&lt;img src=&#34;https://badgen.net/badge/open%20source/%E2%9D%A4/blue?icon=github&#34; alt=&#34;https://github.com/Naereen/badges/&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;NeMo Curator&lt;/h1&gt; &#xA;&lt;p&gt;üöÄ &lt;strong&gt;The GPU-Accelerated Open Source Framework for Efficient Generative AI Model Data Curation&lt;/strong&gt; üöÄ&lt;/p&gt; &#xA;&lt;p&gt;NeMo Curator is a Python library specifically designed for fast and scalable dataset preparation and curation for generative AI use cases such as foundation language model pretraining, text-to-image model training, domain-adaptive pretraining (DAPT), supervised fine-tuning (SFT) and parameter-efficient fine-tuning (PEFT). It greatly accelerates data curation by leveraging GPUs with &lt;a href=&#34;https://www.dask.org/&#34;&gt;Dask&lt;/a&gt; and &lt;a href=&#34;https://developer.nvidia.com/rapids&#34;&gt;RAPIDS&lt;/a&gt;, resulting in significant time savings. The library provides a customizable and modular interface, simplifying pipeline expansion and accelerating model convergence through the preparation of high-quality tokens.&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;p&gt;NeMo Curator provides a collection of scalable data curation modules for text and image curation.&lt;/p&gt; &#xA;&lt;h3&gt;Text Curation&lt;/h3&gt; &#xA;&lt;p&gt;All of our text pipelines have great multilingual support.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/download.html&#34;&gt;Download and Extraction&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Default implementations for Common Crawl, Wikipedia, and ArXiv sources&lt;/li&gt; &#xA;   &lt;li&gt;Easily customize and extend to other sources&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/languageidentificationunicodeformatting.html&#34;&gt;Language Identification&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/languageidentificationunicodeformatting.html&#34;&gt;Unicode Reformatting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/qualityfiltering.html&#34;&gt;Heuristic Filtering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Classifier Filtering &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/qualityfiltering.html&#34;&gt;fastText&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;GPU-Accelerated models: &lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/distributeddataclassification.html&#34;&gt;Domain, Quality, and Safety Classification&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPU-Accelerated Deduplication&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/gpudeduplication.html&#34;&gt;Exact Deduplication&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/gpudeduplication.html&#34;&gt;Fuzzy Deduplication&lt;/a&gt; via MinHash Locality Sensitive Hashing&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/semdedup.html&#34;&gt;Semantic Deduplication&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/taskdecontamination.html&#34;&gt;Downstream-task Decontamination&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/personalidentifiableinformationidentificationandremoval.html&#34;&gt;Personal Identifiable Information (PII) Redaction&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Image Curation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/image/classifiers/embedders.html&#34;&gt;Embedding Creation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Classifier Filtering &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/image/classifiers/aesthetic.html&#34;&gt;Aesthetic&lt;/a&gt; and &lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/image/classifiers/nsfw.html&#34;&gt;NSFW&lt;/a&gt; Classification&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;GPU Deduplication &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/semdedup.html&#34;&gt;Semantic&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These modules offer flexibility and permit reordering, with only a few exceptions. All the modules automatically scale to multiple nodes to increase throughput.&lt;/p&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/NeMo-Curator/main/docs/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/NeMo-Curator/main/examples/&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/NeMo-Curator/main/tutorials/&#34;&gt;Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Blog posts &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/curating-trillion-token-datasets-introducing-nemo-data-curator/&#34;&gt;Curating Trillion-Token Datasets: Introducing NVIDIA NeMo Data Curator&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/scale-and-curate-high-quality-datasets-for-llm-training-with-nemo-curator/&#34;&gt;Scale and Curate High-Quality Datasets for LLM Training with NVIDIA NeMo Curator&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/curating-custom-datasets-for-llm-training-with-nvidia-nemo-curator/&#34;&gt;Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/curating-custom-datasets-for-llm-parameter-efficient-fine-tuning-with-nvidia-nemo-curator/&#34;&gt;Curating Custom Datasets for LLM Parameter-Efficient Fine-Tuning with NVIDIA NeMo Curator&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/streamlining-data-processing-for-domain-adaptive-pretraining-with-nvidia-nemo-curator/&#34;&gt;Streamlining Data Processing for Domain Adaptive Pretraining with NVIDIA NeMo Curator&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;This section explains how to install NeMo Curator and use the Python library, Python modules, and CLI scripts. It also includes a list of tutorials to help you get started right away. Finally, this section explains how to use the NeMo Framework Launcher as an alternative method for interfacing with NeMo Curator.&lt;/p&gt; &#xA;&lt;h3&gt;Install NeMo Curator&lt;/h3&gt; &#xA;&lt;h4&gt;Requirements&lt;/h4&gt; &#xA;&lt;p&gt;Before installing NeMo Curator, ensure that the following requirements are met:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10&lt;/li&gt; &#xA; &lt;li&gt;Ubuntu 22.04/20.04&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU (optional) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Volta‚Ñ¢ or higher (&lt;a href=&#34;https://developer.nvidia.com/cuda-gpus&#34;&gt;compute capability 7.0+&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;CUDA 12 (or above)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can get NeMo-Curator in 3 ways.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;PyPi&lt;/li&gt; &#xA; &lt;li&gt;Source&lt;/li&gt; &#xA; &lt;li&gt;NeMo Framework Container&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;PyPi&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install cython&#xA;pip install --extra-index-url https://pypi.nvidia.com nemo-curator[all]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Source&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/NVIDIA/NeMo-Curator.git&#xA;pip install cython&#xA;pip install --extra-index-url https://pypi.nvidia.com &#34;./NeMo-Curator[all]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;NeMo Framework Container&lt;/h4&gt; &#xA;&lt;p&gt;The latest release of NeMo Curator comes preinstalled in the &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo/tags&#34;&gt;NeMo Framework Container&lt;/a&gt;. If you want the latest commit inside the container, you can reinstall NeMo Curator using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip uninstall nemo-curator&#xA;rm -r /opt/NeMo-Curator&#xA;git clone https://github.com/NVIDIA/NeMo-Curator.git /opt/NeMo-Curator&#xA;pip install --extra-index-url https://pypi.nvidia.com &#34;/opt/NeMo-Curator[all]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Extras&lt;/h4&gt; &#xA;&lt;p&gt;NeMo Curator has a set of extras you can use to only install the necessary modules for your workload. These extras are available for all installation methods provided.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install nemo-curator # Installs CPU-only text curation modules&#xA;pip install --extra-index-url https://pypi.nvidia.com nemo-curator[cuda12x] # Installs CPU + GPU text curation modules&#xA;pip install --extra-index-url https://pypi.nvidia.com nemo-curator[image] # Installs CPU + GPU text and image curation modules&#xA;pip install --extra-index-url https://pypi.nvidia.com nemo-curator[all] # Installs all of the above&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using Nightly Dependencies for RAPIDS&lt;/h4&gt; &#xA;&lt;p&gt;You can also install NeMo Curator using the &lt;a href=&#34;https://docs.rapids.ai/install&#34;&gt;RAPIDS Nightly Builds&lt;/a&gt;. To do so, you can set the environment variable &lt;code&gt;RAPIDS_NIGHTLY=1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# installing from pypi&#xA;RAPIDS_NIGHTLY=1 pip install --extra-index-url=https://pypi.anaconda.org/rapidsai-wheels-nightly/simple &#34;nemo-curator[cuda12x]&#34;&#xA;&#xA;# installing from source&#xA;RAPIDS_NIGHTLY=1 pip install --extra-index-url=https://pypi.anaconda.org/rapidsai-wheels-nightly/simple &#34;.[cuda12x]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When the &lt;code&gt;RAPIDS_NIGHTLY&lt;/code&gt; variable is set to 0 (which is the default), it will use the stable version of RAPIDS.&lt;/p&gt; &#xA;&lt;h2&gt;Use NeMo Curator&lt;/h2&gt; &#xA;&lt;h3&gt;Python API Quick Example&lt;/h3&gt; &#xA;&lt;p&gt;The following snippet demonstrates how to create a small data curation pipeline that downloads and curates a small subset of the Common Crawl dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;# Download your dataset&#xA;dataset = download_common_crawl(&#34;/datasets/common_crawl/&#34;, &#34;2021-04&#34;, &#34;2021-10&#34;, url_limit=10)&#xA;# Build your pipeline&#xA;curation_pipeline = Sequential([&#xA;  # Fix unicode&#xA;  Modify(UnicodeReformatter()),&#xA;  # Discard short records&#xA;  ScoreFilter(WordCountFilter(min_words=80)),&#xA;  # Discard low-quality records&#xA;  ScoreFilter(FastTextQualityFilter(model_path=&#34;model.bin&#34;)),&#xA;  # Discard records from the evaluation metrics to prevent test set leakage.&#xA;  TaskDecontamination([Winogrande(), Squad(), TriviaQA()])&#xA;])&#xA;# Execute the pipeline on your dataset&#xA;curated_dataset = curation_pipeline(dataset)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Explore NeMo Curator Tutorials&lt;/h3&gt; &#xA;&lt;p&gt;To get started with NeMo Curator, you can follow the tutorials &lt;a href=&#34;https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials&#34;&gt;available here&lt;/a&gt;. These tutorials include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/tinystories&#34;&gt;&lt;code&gt;tinystories&lt;/code&gt;&lt;/a&gt; which focuses on data curation for training LLMs from scratch.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/peft-curation&#34;&gt;&lt;code&gt;peft-curation&lt;/code&gt;&lt;/a&gt; which focuses on data curation for LLM parameter-efficient fine-tuning (PEFT) use-cases.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/distributed_data_classification&#34;&gt;&lt;code&gt;distributed_data_classification&lt;/code&gt;&lt;/a&gt; which focuses on using the quality and domain classifiers to help with data annotation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/single_node_tutorial&#34;&gt;&lt;code&gt;single_node_tutorial&lt;/code&gt;&lt;/a&gt; which demonstrates an end-to-end data curation pipeline for curating Wikipedia data in Thai.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/NeMo-Curator/raw/main/tutorials/image-curation/image-curation.ipynb&#34;&gt;&lt;code&gt;image-curation&lt;/code&gt;&lt;/a&gt; which explores the scalable image curation modules.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Access Python Modules&lt;/h3&gt; &#xA;&lt;p&gt;The NeMo Curator section of the &lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/index.html&#34;&gt;NeMo Framework User Guide&lt;/a&gt; provides in-depth information about how the Python modules work. The &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/NeMo-Curator/main/examples/&#34;&gt;examples&lt;/a&gt; directory in the GitHub repository provides scripts that showcase these modules.&lt;/p&gt; &#xA;&lt;h3&gt;Use CLI Scripts&lt;/h3&gt; &#xA;&lt;p&gt;NeMo Curator also offers CLI scripts for you to use. The scripts in &lt;code&gt;nemo_curator/scripts&lt;/code&gt; map closely to the supplied Python modules. Refer to the &lt;a href=&#34;https://docs.nvidia.com/nemo-framework/user-guide/latest/datacuration/index.html&#34;&gt;NeMo Framework User Guide&lt;/a&gt; for more information about the Python modules and scripts.&lt;/p&gt; &#xA;&lt;h3&gt;Use NeMo Framework Launcher&lt;/h3&gt; &#xA;&lt;p&gt;As an alternative method for interfacing with NeMo Curator, you can use the &lt;a href=&#34;https://github.com/NVIDIA/NeMo-Megatron-Launcher&#34;&gt;NeMo Framework Launcher&lt;/a&gt;. The launcher enables you to easily configure the parameters and cluster. It can also automatically generate the Slurm batch scripts that wrap around the CLI scripts required to run your pipeline.&lt;/p&gt; &#xA;&lt;p&gt;In addition, other methods are available to run NeMo Curator on Slurm. For example, refer to the example scripts in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/NeMo-Curator/main/examples/slurm/&#34;&gt;&lt;code&gt;examples/slurm&lt;/code&gt;&lt;/a&gt; for information on how to run NeMo Curator on Slurm without the NeMo Framework Launcher.&lt;/p&gt; &#xA;&lt;h2&gt;Module Ablation and Compute Performance&lt;/h2&gt; &#xA;&lt;p&gt;The modules within NeMo Curator were primarily designed to curate high-quality documents from Common Crawl snapshots in a scalable manner. To evaluate the quality of the curated Common Crawl documents, we conducted a series of ablation experiments. In these experiments, we trained a 357M-parameter GPT-style model using datasets generated at various stages of our data curation pipeline, which was implemented in NeMo Curator.&lt;/p&gt; &#xA;&lt;p&gt;The following figure shows that the use of different data curation modules implemented in NeMo Curator led to improved model zero-shot downstream task performance.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/NeMo-Curator/main/docs/user-guide/assets/zeroshot_ablations.png&#34; alt=&#34;drawing&#34; width=&#34;700&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;In terms of scalability and compute performance, using the combination of RAPIDS and Dask fuzzy deduplication enabled us to deduplicate the 1.1 Trillion token Red Pajama dataset in 1.8 hours with 64 NVIDIA A100 Tensor Core GPUs.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, using the CPU-based modules, the following table shows the time required and resulting data size reduction for each processing step &lt;a href=&#34;https://commoncrawl.org/2020/12/nov-dec-2020-crawl-archive-now-available/&#34;&gt;Common Crawl snapshot from November/December of 2020&lt;/a&gt; using 30 CPU nodes (with hardware similar to the &lt;code&gt;c5.24xlarge&lt;/code&gt; &lt;a href=&#34;https://aws.amazon.com/ec2/instance-types/c5/&#34;&gt;Amazon AWS C5 instance&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th style=&#34;text-align:center&#34;&gt;Dataset &lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt; Download and text extraction&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Text cleaning &lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Quality filtering&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Time &lt;/td&gt; &#xA;   &lt;td&gt; Output Size &lt;/td&gt; &#xA;   &lt;td&gt;Time &lt;/td&gt; &#xA;   &lt;td&gt; Output Size &lt;/td&gt; &#xA;   &lt;td&gt;Time &lt;/td&gt; &#xA;   &lt;td&gt; Output Size &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Common Crawl 2020-50&lt;/td&gt; &#xA;   &lt;td&gt; 36 hrs&lt;/td&gt; &#xA;   &lt;td&gt;2.8 TB&lt;/td&gt; &#xA;   &lt;td&gt; 1 hr &lt;/td&gt; &#xA;   &lt;td&gt; 2.8 TB &lt;/td&gt; &#xA;   &lt;td&gt; 0.2 hr &lt;/td&gt; &#xA;   &lt;td&gt; 0.52 TB &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contribute to NeMo Curator&lt;/h2&gt; &#xA;&lt;p&gt;We welcome community contributions! Please refer to &lt;a href=&#34;https://github.com/NVIDIA/NeMo/raw/stable/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for the process.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Arize-ai/phoenix</title>
    <updated>2024-11-01T01:48:17Z</updated>
    <id>tag:github.com,2024-11-01:/Arize-ai/phoenix</id>
    <link href="https://github.com/Arize-ai/phoenix" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI Observability &amp; Evaluation&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://phoenix.arize.com&#34; style=&#34;background:none&#34;&gt; &lt;img alt=&#34;phoenix banner&#34; src=&#34;https://github.com/Arize-ai/phoenix-assets/raw/main/images/socal/github-large-banner-phoenix.jpg?raw=true&#34; width=&#34;auto&#34; height=&#34;auto&#34;&gt; &lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://docs.arize.com/phoenix/&#34;&gt; &lt;img src=&#34;https://img.shields.io/static/v1?message=Docs&amp;amp;logo=data:image/png;base64,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&amp;amp;labelColor=grey&amp;amp;color=blue&amp;amp;logoColor=white&amp;amp;label=%20&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q&#34;&gt; &lt;img src=&#34;https://img.shields.io/static/v1?message=Community&amp;amp;logo=slack&amp;amp;labelColor=grey&amp;amp;color=blue&amp;amp;logoColor=white&amp;amp;label=%20&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://twitter.com/ArizePhoenix&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-ArizePhoenix-blue.svg?color=blue&amp;amp;labelColor=gray&amp;amp;logo=twitter&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://pypi.org/project/arize-phoenix/&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/arize-phoenix?color=blue&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://anaconda.org/conda-forge/arize-phoenix&#34;&gt; &lt;img src=&#34;https://img.shields.io/conda/vn/conda-forge/arize-phoenix.svg?color=blue&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://pypi.org/project/arize-phoenix/&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/arize-phoenix&#34;&gt; &lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://hub.docker.com/r/arizephoenix/phoenix/tags&#34;&gt; &lt;img src=&#34;https://img.shields.io/docker/v/arizephoenix/phoenix?sort=semver&amp;amp;logo=docker&amp;amp;label=image&amp;amp;color=blue&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Phoenix is an open-source AI observability platform designed for experimentation, evaluation, and troubleshooting. It provides:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/llm-traces&#34;&gt;&lt;strong&gt;&lt;em&gt;Tracing&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; - Trace your LLM application&#39;s runtime using OpenTelemetry-based instrumentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/evaluation/llm-evals&#34;&gt;&lt;strong&gt;&lt;em&gt;Evaluation&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; - Leverage LLMs to benchmark your application&#39;s performance using response and retrieval evals.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets&#34;&gt;&lt;strong&gt;&lt;em&gt;Datasets&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; - Create versioned datasets of examples for experimentation, evaluation, and fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets#experiments&#34;&gt;&lt;strong&gt;&lt;em&gt;Experiments&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; - Track and evaluate changes to prompts, LLMs, and retrieval.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Phoenix is vendor and language agnostic with out-of-the-box support for popular frameworks (ü¶ô&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/integrations-tracing/llamaindex&#34;&gt;LlamaIndex&lt;/a&gt;, ü¶ú‚õì&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/integrations-tracing/langchain&#34;&gt;LangChain&lt;/a&gt;, &lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/integrations-tracing/haystack&#34;&gt;Haystack&lt;/a&gt;, üß©&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/integrations-tracing/dspy&#34;&gt;DSPy&lt;/a&gt;) and LLM providers (&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/integrations-tracing/openai&#34;&gt;OpenAI&lt;/a&gt;, &lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/integrations-tracing/bedrock&#34;&gt;Bedrock&lt;/a&gt;, &lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/integrations-tracing/mistralai&#34;&gt;MistralAI&lt;/a&gt;, &lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/integrations-tracing/vertexai&#34;&gt;VertexAI&lt;/a&gt;, &lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/integrations-tracing/litellm&#34;&gt;LiteLLM&lt;/a&gt;, and more). For details on auto-instrumentation, check out the &lt;a href=&#34;https://github.com/Arize-ai/openinference&#34;&gt;OpenInference&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;Phoenix runs practically anywhere, including your Jupyter notebook, local machine, containerized deployment, or in the cloud.&lt;/p&gt; &#xA;&lt;img alt=&#34;phoenix_overview.gif&#34; width=&#34;100%&#34; src=&#34;https://github.com/Arize-ai/phoenix-assets/raw/main/gifs/phoenix_overview.gif?raw=true&#34;&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install Phoenix via &lt;code&gt;pip&lt;/code&gt; or &lt;code&gt;conda&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install arize-phoenix&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Phoenix container images are available via &lt;a href=&#34;https://hub.docker.com/r/arizephoenix/phoenix&#34;&gt;Docker Hub&lt;/a&gt; and can be deployed using Docker or Kubernetes.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Key Features&lt;/th&gt; &#xA;   &lt;th&gt;Availability&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/concepts-tracing/what-are-traces&#34;&gt;Tracing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/evaluation/llm-evals&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/use-cases-tracing/rag-evaluation&#34;&gt;Retrieval (RAG) Analysis&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/datasets-and-experiments/overview-datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/datasets-and-experiments/how-to-datasets/exporting-datasets&#34;&gt;Fine-Tuning Export&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/concepts-tracing/how-to-annotate-traces&#34;&gt;Annotations&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/how-to-tracing/capture-feedback&#34;&gt;Human Feedback&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/datasets-and-experiments/how-to-experiments/run-experiments&#34;&gt;Experiments&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/inferences/phoenix-inferences&#34;&gt;Embeddings Analysis&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/how-to-tracing/extract-data-from-spans&#34;&gt;Data Export&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;REST API&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GraphQL API&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Data Retention&lt;/td&gt; &#xA;   &lt;td&gt;Customizable&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Authentication&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Social Login&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RBAC&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Projects&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/deployment&#34;&gt;Self-Hosting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Jupyter Notebooks&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Arize-ai/phoenix/issues/2619&#34;&gt;Sessions&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;In Progress üöß&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Arize-ai/phoenix/issues/3435&#34;&gt;Prompt Playground&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;In Progress üöß&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Prompt Management&lt;/td&gt; &#xA;   &lt;td&gt;Coming soon ‚è±Ô∏è&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Tracing Integrations&lt;/h2&gt; &#xA;&lt;p&gt;Phoenix is built on top of OpenTelemetry and is vendor, language, and framework agnostic.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Integration&lt;/th&gt; &#xA;   &lt;th&gt;Package&lt;/th&gt; &#xA;   &lt;th&gt;Version Badge&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/openai&#34;&gt;OpenAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openinference-instrumentation-openai&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.python.org/pypi/openinference-instrumentation-openai&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openinference-instrumentation-openai.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/llamaindex&#34;&gt;LlamaIndex&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openinference-instrumentation-llama-index&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.python.org/pypi/openinference-instrumentation-llama-index&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openinference-instrumentation-llama-index.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/dspy&#34;&gt;DSPy&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openinference-instrumentation-dspy&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.python.org/pypi/openinference-instrumentation-dspy&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openinference-instrumentation-dspy.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/bedrock&#34;&gt;AWS Bedrock&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openinference-instrumentation-bedrock&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.python.org/pypi/openinference-instrumentation-bedrock&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openinference-instrumentation-bedrock.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/langchain&#34;&gt;LangChain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openinference-instrumentation-langchain&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.python.org/pypi/openinference-instrumentation-langchain&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openinference-instrumentation-langchain.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/mistralai&#34;&gt;MistralAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openinference-instrumentation-mistralai&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.python.org/pypi/openinference-instrumentation-mistralai&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openinference-instrumentation-mistralai.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/guardrails&#34;&gt;Guardrails&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openinference-instrumentation-guardrails&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.python.org/pypi/openinference-instrumentation-guardrails&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openinference-instrumentation-guardrails.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/vertexai&#34;&gt;VertexAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openinference-instrumentation-vertexai&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.python.org/pypi/openinference-instrumentation-vertexai&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openinference-instrumentation-vertexai.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/crewai&#34;&gt;CrewAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openinference-instrumentation-crewai&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.python.org/pypi/openinference-instrumentation-crewai&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openinference-instrumentation-crewai.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/haystack&#34;&gt;Haystack&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openinference-instrumentation-haystack&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.python.org/pypi/openinference-instrumentation-haystack&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openinference-instrumentation-haystack.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/litellm&#34;&gt;LiteLLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openinference-instrumentation-litellm&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.python.org/pypi/openinference-instrumentation-litellm&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openinference-instrumentation-litellm.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/groq&#34;&gt;Groq&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openinference-instrumentation-groq&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.python.org/pypi/openinference-instrumentation-groq&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openinference-instrumentation-groq.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/instructor&#34;&gt;Instructor&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openinference-instrumentation-instructor&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.python.org/pypi/openinference-instrumentation-instructor&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openinference-instrumentation-instructor.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/anthropic&#34;&gt;Anthropic&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openinference-instrumentation-anthropic&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.python.org/pypi/openinference-instrumentation-anthropic&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/openinference-instrumentation-anthropic.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;JavaScript&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Integration&lt;/th&gt; &#xA;   &lt;th&gt;Package&lt;/th&gt; &#xA;   &lt;th&gt;Version Badge&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/openai-node-sdk&#34;&gt;OpenAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;@arizeai/openinference-instrumentation-openai&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.npmjs.com/package/@arizeai/openinference-instrumentation-openai&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@arizeai/openinference-instrumentation-openai.svg?sanitize=true&#34; alt=&#34;NPM Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/langchain.js&#34;&gt;LangChain.js&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;@arizeai/openinference-instrumentation-langchain&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.npmjs.com/package/@arizeai/openinference-instrumentation-langchain&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@arizeai/openinference-instrumentation-langchain.svg?sanitize=true&#34; alt=&#34;NPM Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.arize.com/phoenix/tracing/integrations-tracing/vercel-ai-sdk&#34;&gt;Vercel AI SDK&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;@arizeai/openinference-vercel&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.npmjs.com/package/@arizeai/openinference-vercel&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@arizeai/openinference-vercel&#34; alt=&#34;NPM Version&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For details about tracing integrations and example applications, see the &lt;a href=&#34;https://github.com/Arize-ai/openinference&#34;&gt;OpenInference&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;Join our community to connect with thousands of AI builders.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üåç Join our &lt;a href=&#34;https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q&#34;&gt;Slack community&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üìö Read our &lt;a href=&#34;https://docs.arize.com/phoenix&#34;&gt;documentation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üí° Ask questions and provide feedback in the &lt;em&gt;#phoenix-support&lt;/em&gt; channel.&lt;/li&gt; &#xA; &lt;li&gt;üåü Leave a star on our &lt;a href=&#34;https://github.com/Arize-ai/phoenix&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üêû Report bugs with &lt;a href=&#34;https://github.com/Arize-ai/phoenix/issues&#34;&gt;GitHub Issues&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ùïè Follow us on &lt;a href=&#34;https://twitter.com/ArizePhoenix&#34;&gt;ùïè&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üíåÔ∏è Sign up for our &lt;a href=&#34;https://phoenix.arize.com/#updates&#34;&gt;mailing list&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üó∫Ô∏è Check out our &lt;a href=&#34;https://github.com/orgs/Arize-ai/projects/45&#34;&gt;roadmap&lt;/a&gt; to see where we&#39;re heading next.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Breaking Changes&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/Arize-ai/phoenix/main/MIGRATION.md&#34;&gt;migration guide&lt;/a&gt; for a list of breaking changes.&lt;/p&gt; &#xA;&lt;h2&gt;Copyright, Patent, and License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2024 Arize AI, Inc. All Rights Reserved.&lt;/p&gt; &#xA;&lt;p&gt;Portions of this code are patent protected by one or more U.S. Patents. See &lt;a href=&#34;https://github.com/Arize-ai/phoenix/raw/main/IP_NOTICE&#34;&gt;IP_NOTICE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This software is licensed under the terms of the Elastic License 2.0 (ELv2). See &lt;a href=&#34;https://github.com/Arize-ai/phoenix/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>langchain-ai/langchain-academy</title>
    <updated>2024-11-01T01:48:17Z</updated>
    <id>tag:github.com,2024-11-01:/langchain-ai/langchain-academy</id>
    <link href="https://github.com/langchain-ai/langchain-academy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LangChain Academy&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to LangChain Academy! This is a growing set of modules focused on foundational concepts within the LangChain ecosystem. Module 0 is basic setup and Modules 1 - 4 focus on LangGraph, progressively adding more advanced themes. In each module folder, you&#39;ll see a set of notebooks. A LangChain Academy accompanies each notebook to guide you through the topic. Each module also has a &lt;code&gt;studio&lt;/code&gt; subdirectory, with a set of relevant graphs that we will explore using the LangGraph API and Studio.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Python version&lt;/h3&gt; &#xA;&lt;p&gt;To get the most out of this course, please ensure you&#39;re using Python 3.11 or later. This version is required for optimal compatibility with LangGraph. If you&#39;re on an older version, upgrading will ensure everything runs smoothly.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 --version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Clone repo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/langchain-ai/langchain-academy.git&#xA;$ cd langchain-academy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create an environment and install dependencies&lt;/h3&gt; &#xA;&lt;h4&gt;Mac/Linux/WSL&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python3 -m venv lc-academy-env&#xA;$ source lc-academy-env/bin/activate&#xA;$ pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Windows Powershell&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;PS&amp;gt; python3 -m venv lc-academy-env&#xA;PS&amp;gt; Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process&#xA;PS&amp;gt; lc-academy-env\scripts\activate&#xA;PS&amp;gt; pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running notebooks&lt;/h3&gt; &#xA;&lt;p&gt;If you don&#39;t have Jupyter set up, follow installation instructions &lt;a href=&#34;https://jupyter.org/install&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Setting up env variables&lt;/h3&gt; &#xA;&lt;p&gt;Briefly going over how to set up environment variables. You can also use a &lt;code&gt;.env&lt;/code&gt; file with &lt;code&gt;python-dotenv&lt;/code&gt; library.&lt;/p&gt; &#xA;&lt;h4&gt;Mac/Linux/WSL&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ export API_ENV_VAR=&#34;your-api-key-here&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Windows Powershell&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;PS&amp;gt; $env:API_ENV_VAR = &#34;your-api-key-here&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Set OpenAI API key&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you don&#39;t have an OpenAI API key, you can sign up &lt;a href=&#34;https://openai.com/index/openai-api/&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in your environment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Sign up and Set LangSmith API&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sign up for LangSmith &lt;a href=&#34;https://smith.langchain.com/&#34;&gt;here&lt;/a&gt;, find out more about LangSmith&lt;/li&gt; &#xA; &lt;li&gt;and how to use it within your workflow &lt;a href=&#34;https://www.langchain.com/langsmith&#34;&gt;here&lt;/a&gt;, and relevant library &lt;a href=&#34;https://docs.smith.langchain.com/&#34;&gt;docs&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;LANGCHAIN_API_KEY&lt;/code&gt;, &lt;code&gt;LANGCHAIN_TRACING_V2=true&lt;/code&gt; in your environment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Set up Tavily API for web search&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Tavily Search API is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can sign up for an API key &lt;a href=&#34;https://tavily.com/&#34;&gt;here&lt;/a&gt;. It&#39;s easy to sign up and offers a very generous free tier. Some lessons (in Module 4) will use Tavily.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set &lt;code&gt;TAVILY_API_KEY&lt;/code&gt; in your environment.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Set up LangGraph Studio&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Currently, Studio only has macOS support and needs Docker Desktop running.&lt;/li&gt; &#xA; &lt;li&gt;Download the latest &lt;code&gt;.dmg&lt;/code&gt; file &lt;a href=&#34;https://github.com/langchain-ai/langgraph-studio?tab=readme-ov-file#download&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install Docker desktop for Mac &lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Running Studio&lt;/h3&gt; &#xA;&lt;p&gt;Graphs for LangGraph Studio are in the &lt;code&gt;module-x/studio/&lt;/code&gt; folders.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To use Studio, you will need to create a .env file with the relevant API keys&lt;/li&gt; &#xA; &lt;li&gt;Run this from the command line to create these files for module 1 to 4, as an example:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ for i in {1..4}; do&#xA;  cp module-$i/studio/.env.example module-$i/studio/.env&#xA;  echo &#34;OPENAI_API_KEY=\&#34;$OPENAI_API_KEY\&#34;&#34; &amp;gt; module-$i/studio/.env&#xA;done&#xA;$ echo &#34;TAVILY_API_KEY=\&#34;$TAVILY_API_KEY\&#34;&#34; &amp;gt;&amp;gt; module-4/studio/.env&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>