<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-05-31T02:42:40Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>graykode/nlp-tutorial</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/graykode/nlp-tutorial</id>
    <link href="https://github.com/graykode/nlp-tutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Natural Language Processing Tutorial for Deep Learning Researchers&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;nlp-tutorial&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;100&#34; src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/225px-TensorFlowLogo.svg.png&#34;&gt; &lt;img width=&#34;100&#34; src=&#34;https://media-thumbs.golden.com/OLqzmrmwAzY1P7Sl29k2T9WjJdM=/200x200/smart/golden-storage-production.s3.amazonaws.com/topic_images/e08914afa10a4179893eeb07cb5e4713.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;nlp-tutorial&lt;/code&gt; is a tutorial for who is studying NLP(Natural Language Processing) using &lt;strong&gt;Pytorch&lt;/strong&gt;. Most of the models in NLP were implemented with less than &lt;strong&gt;100 lines&lt;/strong&gt; of code.(except comments or blank lines)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[08-14-2020] Old TensorFlow v1 code is archived in &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/archive&#34;&gt;the archive folder&lt;/a&gt;. For beginner readability, only pytorch version 1.0 or higher is supported.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Curriculum - (Example Purpose)&lt;/h2&gt; &#xA;&lt;h4&gt;1. Basic Embedding Model&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/1-1.NNLM&#34;&gt;NNLM(Neural Network Language Model)&lt;/a&gt; - &lt;strong&gt;Predict Next Word&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&#34;&gt;A Neural Probabilistic Language Model(2003)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-1.NNLM/NNLM.ipynb&#34;&gt;NNLM.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;1-2. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/1-2.Word2Vec&#34;&gt;Word2Vec(Skip-gram)&lt;/a&gt; - &lt;strong&gt;Embedding Words and Show Graph&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&#34;&gt;Distributed Representations of Words and Phrases and their Compositionality(2013)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram(Softmax).ipynb&#34;&gt;Word2Vec.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;1-3. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/1-3.FastText&#34;&gt;FastText(Application Level)&lt;/a&gt; - &lt;strong&gt;Sentence Classification&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/pdf/1607.01759.pdf&#34;&gt;Bag of Tricks for Efficient Text Classification(2016)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-3.FastText/FastText.ipynb&#34;&gt;FastText.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. CNN(Convolutional Neural Network)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/2-1.TextCNN&#34;&gt;TextCNN&lt;/a&gt; - &lt;strong&gt;Binary Sentiment Classification&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;http://www.aclweb.org/anthology/D14-1181&#34;&gt;Convolutional Neural Networks for Sentence Classification(2014)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/2-1.TextCNN/TextCNN.ipynb&#34;&gt;TextCNN.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;3. RNN(Recurrent Neural Network)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;3-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/3-1.TextRNN&#34;&gt;TextRNN&lt;/a&gt; - &lt;strong&gt;Predict Next Step&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;http://psych.colorado.edu/~kimlab/Elman1990.pdf&#34;&gt;Finding Structure in Time(1990)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-1.TextRNN/TextRNN.ipynb&#34;&gt;TextRNN.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;3-2. &lt;a href=&#34;https://github.com/graykode/nlp-tutorial/tree/master/3-2.TextLSTM&#34;&gt;TextLSTM&lt;/a&gt; - &lt;strong&gt;Autocomplete&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://www.bioinf.jku.at/publications/older/2604.pdf&#34;&gt;LONG SHORT-TERM MEMORY(1997)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-2.TextLSTM/TextLSTM.ipynb&#34;&gt;TextLSTM.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;3-3. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/3-3.Bi-LSTM&#34;&gt;Bi-LSTM&lt;/a&gt; - &lt;strong&gt;Predict Next Word in Long Sentence&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-3.Bi-LSTM/Bi_LSTM.ipynb&#34;&gt;Bi_LSTM.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;4. Attention Mechanism&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;4-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/4-1.Seq2Seq&#34;&gt;Seq2Seq&lt;/a&gt; - &lt;strong&gt;Change Word&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/pdf/1406.1078.pdf&#34;&gt;Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation(2014)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-1.Seq2Seq/Seq2Seq.ipynb&#34;&gt;Seq2Seq.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;4-2. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/4-2.Seq2Seq(Attention)&#34;&gt;Seq2Seq with Attention&lt;/a&gt; - &lt;strong&gt;Translate&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34;&gt;Neural Machine Translation by Jointly Learning to Align and Translate(2014)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-2.Seq2Seq(Attention)/Seq2Seq(Attention).ipynb&#34;&gt;Seq2Seq(Attention).ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;4-3. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/4-3.Bi-LSTM(Attention)&#34;&gt;Bi-LSTM with Attention&lt;/a&gt; - &lt;strong&gt;Binary Sentiment Classification&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-3.Bi-LSTM(Attention)/Bi_LSTM(Attention).ipynb&#34;&gt;Bi_LSTM(Attention).ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;5. Model based on Transformer&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;5-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/5-1.Transformer&#34;&gt;The Transformer&lt;/a&gt; - &lt;strong&gt;Translate&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention Is All You Need(2017)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer.ipynb&#34;&gt;Transformer.ipynb&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer(Greedy_decoder).ipynb&#34;&gt;Transformer(Greedy_decoder).ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;5-2. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/5-2.BERT&#34;&gt;BERT&lt;/a&gt; - &lt;strong&gt;Classification Next Sentence &amp;amp; Predict Masked Tokens&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.ipynb&#34;&gt;BERT.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.5+&lt;/li&gt; &#xA; &lt;li&gt;Pytorch 1.0.0+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Author&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tae Hwan Jung(Jeff Jung) @graykode&lt;/li&gt; &#xA; &lt;li&gt;Author Email : &lt;a href=&#34;mailto:nlkey2022@gmail.com&#34;&gt;nlkey2022@gmail.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Acknowledgements to &lt;a href=&#34;http://mojitok.com/&#34;&gt;mojitok&lt;/a&gt; as NLP Research Internship.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>alembics/disco-diffusion</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/alembics/disco-diffusion</id>
    <link href="https://github.com/alembics/disco-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Disco Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A frankensteinian amalgamation of notebooks, models and techniques for the generation of AI Art and Animations.&lt;/p&gt; &#xA;&lt;p&gt;[to be updated with further info soon]&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project uses a special conversion tool to convert the python files into notebooks for easier development.&lt;/p&gt; &#xA;&lt;p&gt;What this means is you do not have to touch the notebook directly to make changes to it&lt;/p&gt; &#xA;&lt;p&gt;the tool being used is called &lt;a href=&#34;https://github.com/MSFTserver/colab-convert&#34;&gt;Colab-Convert&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;install using &lt;code&gt;pip install colab-convert&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;convert .py to .ipynb &lt;code&gt;colab-convert /path/to/file.py /path/to/file.ipynb&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;convert .ipynb to .py &lt;code&gt;colab-convert /path/to/file.ipynb /path/to/file.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;h4&gt;v1 Oct 29th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial QoL improvements added, including user friendly UI, settings+prompt saving and improved google drive folder organization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v1.1 Nov 13th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Now includes sizing options, intermediate saves and fixed image prompts and perlin inits. unexposed batch option since it doesn&#39;t work&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2 Update: Nov 22nd 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial addition of Katherine Crowson&#39;s Secondary Model Method (&lt;a href=&#34;https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR&#34;&gt;https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix for incorrectly named settings files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v3 Update: Dec 24th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implemented Dango&#39;s advanced cutout method&lt;/li&gt; &#xA; &lt;li&gt;Added SLIP models, thanks to NeuralDivergent&lt;/li&gt; &#xA; &lt;li&gt;Fixed issue with NaNs resulting in black images, with massive help and testing from @Softology&lt;/li&gt; &#xA; &lt;li&gt;Perlin now changes properly within batches (not sure where this perlin_regen code came from originally, but thank you)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v4 Update: Jan 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implemented Diffusion Zooming&lt;/li&gt; &#xA; &lt;li&gt;Added Chigozie keyframing&lt;/li&gt; &#xA; &lt;li&gt;Made a bunch of edits to processes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v4.1 Update: Jan 14th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added video input mode&lt;/li&gt; &#xA; &lt;li&gt;Added license that somehow went missing&lt;/li&gt; &#xA; &lt;li&gt;Added improved prompt keyframing, fixed image_prompts and multiple prompts&lt;/li&gt; &#xA; &lt;li&gt;Improved UI&lt;/li&gt; &#xA; &lt;li&gt;Significant under the hood cleanup and improvement&lt;/li&gt; &#xA; &lt;li&gt;Refined defaults for each mode&lt;/li&gt; &#xA; &lt;li&gt;Removed SLIP models for the time being due to import conflicts&lt;/li&gt; &#xA; &lt;li&gt;Added latent-diffusion SuperRes for sharpening&lt;/li&gt; &#xA; &lt;li&gt;Added resume run mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5 Update: Feb 20th 2022 - gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added 3D animation mode. Uses weighted combination of AdaBins and MiDaS depth estimation models. Uses pytorch3d for 3D transforms on Colab and/or Linux.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.1 Update: Mar 30th 2022 - zippy / Chris Allen and gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integrated Turbo+Smooth features from Disco Diffusion Turbo -- just the implementation, without its defaults.&lt;/li&gt; &#xA; &lt;li&gt;Implemented resume of turbo animations in such a way that it&#39;s now possible to resume from different batch folders and batch numbers.&lt;/li&gt; &#xA; &lt;li&gt;3D rotation parameter units are now degrees (rather than radians)&lt;/li&gt; &#xA; &lt;li&gt;Corrected name collision in sampling_mode (now diffusion_sampling_mode for plms/ddim, and sampling_mode for 3D transform sampling)&lt;/li&gt; &#xA; &lt;li&gt;Added video_init_seed_continuity option to make init video animations more continuous&lt;/li&gt; &#xA; &lt;li&gt;Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Remove Super Resolution&lt;/li&gt; &#xA; &lt;li&gt;Remove Slip Models&lt;/li&gt; &#xA; &lt;li&gt;Update for crossplatform support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.1 Update: Apr 4th 2022 - MSFTserver aka HostsServer&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Remove Super Resolution&lt;/li&gt; &#xA; &lt;li&gt;Remove Slip Models&lt;/li&gt; &#xA; &lt;li&gt;Update for crossplatform support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.2 Update: Apr 10th 2022 - nin_artificial / Tom Mason&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;VR Mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notebook Provenance&lt;/h2&gt; &#xA;&lt;p&gt;Original notebook by Katherine Crowson (&lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;https://github.com/crowsonkb&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/RiversHaveWings&#34;&gt;https://twitter.com/RiversHaveWings&lt;/a&gt;). It uses either OpenAI&#39;s 256x256 unconditional ImageNet or Katherine Crowson&#39;s fine-tuned 512x512 diffusion model (&lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;https://github.com/openai/guided-diffusion&lt;/a&gt;), together with CLIP (&lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;https://github.com/openai/CLIP&lt;/a&gt;) to connect text prompts with images.&lt;/p&gt; &#xA;&lt;p&gt;Modified by Daniel Russell (&lt;a href=&#34;https://github.com/russelldc&#34;&gt;https://github.com/russelldc&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/danielrussruss&#34;&gt;https://twitter.com/danielrussruss&lt;/a&gt;) to include (hopefully) optimal params for quick generations in 15-100 timesteps rather than 1000, as well as more robust augmentations.&lt;/p&gt; &#xA;&lt;p&gt;Further improvements from Dango233 and nsheppard helped improve the quality of diffusion in general, and especially so for shorter runs like this notebook aims to achieve.&lt;/p&gt; &#xA;&lt;p&gt;Vark added code to load in multiple Clip models at once, which all prompts are evaluated against, which may greatly improve accuracy.&lt;/p&gt; &#xA;&lt;p&gt;The latest zoom, pan, rotation, and keyframes features were taken from Chigozie Nri&#39;s VQGAN Zoom Notebook (&lt;a href=&#34;https://github.com/chigozienri&#34;&gt;https://github.com/chigozienri&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/chigozienri&#34;&gt;https://twitter.com/chigozienri&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Advanced DangoCutn Cutout method is also from Dango223.&lt;/p&gt; &#xA;&lt;p&gt;--&lt;/p&gt; &#xA;&lt;p&gt;Somnai (&lt;a href=&#34;https://twitter.com/Somnai_dreams&#34;&gt;https://twitter.com/Somnai_dreams&lt;/a&gt;) added 2D Diffusion animation techniques, QoL improvements and various implementations of tech and techniques, mostly listed in the changelog below.&lt;/p&gt; &#xA;&lt;p&gt;3D animation implementation added by Adam Letts (&lt;a href=&#34;https://twitter.com/gandamu_ml&#34;&gt;https://twitter.com/gandamu_ml&lt;/a&gt;) in collaboration with Somnai.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Pierian-Data/Complete-Python-3-Bootcamp</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/Pierian-Data/Complete-Python-3-Bootcamp</id>
    <link href="https://github.com/Pierian-Data/Complete-Python-3-Bootcamp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Complete-Python-3-Bootcamp&lt;/h1&gt; &#xA;&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt; &#xA;&lt;p&gt;Copyright(©) by Pierian Data Inc.&lt;/p&gt; &#xA;&lt;p&gt;Get it now for 95% off with the link: &lt;a href=&#34;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&#34;&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thanks!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>labmlai/annotated_deep_learning_paper_implementations</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/labmlai/annotated_deep_learning_paper_implementations</id>
    <link href="https://github.com/labmlai/annotated_deep_learning_paper_implementations" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🧑‍🏫 50! Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, ...), gans(cyclegan, stylegan2, ...), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, ... 🧠&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/labmlai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/labmlai?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://nn.labml.ai/index.html&#34;&gt;labml.ai Deep Learning Paper Implementations&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This is a collection of simple PyTorch implementations of neural networks and related algorithms. These implementations are documented with explanations,&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nn.labml.ai/index.html&#34;&gt;The website&lt;/a&gt; renders these as side-by-side formatted notes. We believe these would help you understand these algorithms better.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://nn.labml.ai/dqn-light.png&#34; alt=&#34;Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We are actively maintaining this repo and adding new implementations almost weekly. &lt;a href=&#34;https://twitter.com/labmlai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/labmlai?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; for updates.&lt;/p&gt; &#xA;&lt;h2&gt;Paper Implementations&lt;/h2&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/transformers/index.html&#34;&gt;Transformers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mha.html&#34;&gt;Multi-headed attention&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/models.html&#34;&gt;Transformer building blocks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/xl/index.html&#34;&gt;Transformer XL&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/xl/relative_mha.html&#34;&gt;Relative multi-headed attention&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/rope/index.html&#34;&gt;Rotary Positional Embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/retro/index.html&#34;&gt;RETRO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/compressive/index.html&#34;&gt;Compressive Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/gpt/index.html&#34;&gt;GPT Architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/glu_variants/simple.html&#34;&gt;GLU Variants&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/knn&#34;&gt;kNN-LM: Generalization through Memorization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/feedback/index.html&#34;&gt;Feedback Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/switch/index.html&#34;&gt;Switch Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/fast_weights/index.html&#34;&gt;Fast Weights Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/fnet/index.html&#34;&gt;FNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/aft/index.html&#34;&gt;Attention Free Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mlm/index.html&#34;&gt;Masked Language Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mlp_mixer/index.html&#34;&gt;MLP-Mixer: An all-MLP Architecture for Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/gmlp/index.html&#34;&gt;Pay Attention to MLPs (gMLP)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/vit/index.html&#34;&gt;Vision Transformer (ViT)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/primer_ez/index.html&#34;&gt;Primer EZ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/hour_glass/index.html&#34;&gt;Hourglass&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/recurrent_highway_networks/index.html&#34;&gt;Recurrent Highway Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/lstm/index.html&#34;&gt;LSTM&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/hypernetworks/hyper_lstm.html&#34;&gt;HyperNetworks - HyperLSTM&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/resnet/index.html&#34;&gt;ResNet&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/conv_mixer/index.html&#34;&gt;ConvMixer&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/capsule_networks/index.html&#34;&gt;Capsule Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/gan/index.html&#34;&gt;Generative Adversarial Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/original/index.html&#34;&gt;Original GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/dcgan/index.html&#34;&gt;GAN with deep convolutional network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/cycle_gan/index.html&#34;&gt;Cycle GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/wasserstein/index.html&#34;&gt;Wasserstein GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/wasserstein/gradient_penalty/index.html&#34;&gt;Wasserstein GAN with Gradient Penalty&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/stylegan/index.html&#34;&gt;StyleGAN 2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/diffusion/index.html&#34;&gt;Diffusion models&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/diffusion/ddpm/index.html&#34;&gt;Denoising Diffusion Probabilistic Models (DDPM)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/sketch_rnn/index.html&#34;&gt;Sketch RNN&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ Graph Neural Networks&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/graphs/gat/index.html&#34;&gt;Graph Attention Networks (GAT)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/graphs/gatv2/index.html&#34;&gt;Graph Attention Networks v2 (GATv2)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/cfr/index.html&#34;&gt;Counterfactual Regret Minimization (CFR)&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Solving games with incomplete information such as poker with CFR.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/cfr/kuhn/index.html&#34;&gt;Kuhn Poker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/rl/index.html&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/rl/ppo/index.html&#34;&gt;Proximal Policy Optimization&lt;/a&gt; with &lt;a href=&#34;https://nn.labml.ai/rl/ppo/gae.html&#34;&gt;Generalized Advantage Estimation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/rl/dqn/index.html&#34;&gt;Deep Q Networks&lt;/a&gt; with with &lt;a href=&#34;https://nn.labml.ai/rl/dqn/model.html&#34;&gt;Dueling Network&lt;/a&gt;, &lt;a href=&#34;https://nn.labml.ai/rl/dqn/replay_buffer.html&#34;&gt;Prioritized Replay&lt;/a&gt; and Double Q Network.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/optimizers/index.html&#34;&gt;Optimizers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/adam.html&#34;&gt;Adam&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/amsgrad.html&#34;&gt;AMSGrad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/adam_warmup.html&#34;&gt;Adam Optimizer with warmup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/noam.html&#34;&gt;Noam Optimizer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/radam.html&#34;&gt;Rectified Adam Optimizer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/ada_belief.html&#34;&gt;AdaBelief Optimizer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/normalization/index.html&#34;&gt;Normalization Layers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/batch_norm/index.html&#34;&gt;Batch Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/layer_norm/index.html&#34;&gt;Layer Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/instance_norm/index.html&#34;&gt;Instance Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/group_norm/index.html&#34;&gt;Group Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/weight_standardization/index.html&#34;&gt;Weight Standardization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/batch_channel_norm/index.html&#34;&gt;Batch-Channel Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/deep_norm/index.html&#34;&gt;DeepNorm&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/distillation/index.html&#34;&gt;Distillation&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/adaptive_computation/index.html&#34;&gt;Adaptive Computation&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/adaptive_computation/ponder_net/index.html&#34;&gt;PonderNet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/uncertainty/index.html&#34;&gt;Uncertainty&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/uncertainty/evidence/index.html&#34;&gt;Evidential Deep Learning to Quantify Classification Uncertainty&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/activations/index.html&#34;&gt;Activations&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/activations/fta/index.html&#34;&gt;Fuzzy Tiling Activations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Highlighted Research Paper PDFs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2204.10628.pdf&#34;&gt;Autoregressive Search Engines: Generating Substrings as Document Identifiers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2203.15556.pdf&#34;&gt;Training Compute-Optimal Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1910.02054.pdf&#34;&gt;ZeRO: Memory Optimizations Toward Training Trillion Parameter Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2204.02311.pdf&#34;&gt;PaLM: Scaling Language Modeling with Pathways&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/dall-e-2.pdf&#34;&gt;Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2203.14465.pdf&#34;&gt;STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2112.04426.pdf&#34;&gt;Improving language models by retrieving from trillions of tokens&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2003.08934.pdf&#34;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1706.03762.pdf&#34;&gt;Attention Is All You Need&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2006.11239.pdf&#34;&gt;Denoising Diffusion Probabilistic Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2109.08668.pdf&#34;&gt;Primer: Searching for Efficient Transformers for Language Modeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1803.02999.pdf&#34;&gt;On First-Order Meta-Learning Algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2103.00020.pdf&#34;&gt;Learning Transferable Visual Models From Natural Language Supervision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2109.02869.pdf&#34;&gt;The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1805.09801.pdf&#34;&gt;Meta-Gradient Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/google_maps_eta.pdf&#34;&gt;ETA Prediction with Graph Neural Networks in Google Maps&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/ponder_net.pdf&#34;&gt;PonderNet: Learning to Ponder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/muzero.pdf&#34;&gt;Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/gans_n_roses.pdf&#34;&gt;GANs N’ Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/vit.pdf&#34;&gt;An Image is Worth 16X16 Word: Transformers for Image Recognition at Scale&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/resnet.pdf&#34;&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/distillation.pdf&#34;&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install labml-nn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Citing&lt;/h3&gt; &#xA;&lt;p&gt;If you use this for academic research, please cite it using the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{labml,&#xA; author = {Varuna Jayasiri, Nipun Wijerathne},&#xA; title = {labml.ai Annotated Paper Implementations},&#xA; year = {2020},&#xA; url = {https://nn.labml.ai/},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Other Projects&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://papers.labml.ai/&#34;&gt;🚀 Trending Research Papers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This shows the most popular research papers on social media. It also aggregates links to useful resources like paper explanations videos and discussions.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://github.com/labmlai/labml&#34;&gt;🧪 labml.ai/labml&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is a library that let&#39;s you monitor deep learning model training and hardware usage from your mobile phone. It also comes with a bunch of other tools to help write deep learning code efficiently.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ageron/handson-ml2</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/ageron/handson-ml2</id>
    <link href="https://github.com/ageron/handson-ml2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Notebooks&lt;/h1&gt; &#xA;&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in python. It contains the example code and solutions to the exercises in the second edition of my O&#39;Reilly book &lt;a href=&#34;https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/&#34;&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;img src=&#34;https://images-na.ssl-images-amazon.com/images/I/51aqYc1QyrL._SX379_BO1,204,203,200_.jpg&#34; title=&#34;book&#34; width=&#34;150&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are looking for the first edition notebooks, check out &lt;a href=&#34;https://github.com/ageron/handson-ml&#34;&gt;ageron/handson-ml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Want to play with these notebooks online without having to install anything?&lt;/h3&gt; &#xA;&lt;p&gt;Use any of the following services (I recommended Colab or Kaggle, since they offer free GPUs and TPUs).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: &lt;em&gt;Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you download any data you care about.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/ageron/handson-ml2/blob/master/&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://homl.info/kaggle/&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Open in Kaggle&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/ageron/handson-ml2/HEAD?filepath=%2Findex.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Launch binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://homl.info/deepnote/&#34;&gt;&lt;img src=&#34;https://deepnote.com/buttons/launch-in-deepnote-small.svg?sanitize=true&#34; alt=&#34;Launch in Deepnote&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Just want to quickly look at some notebooks, without executing any code?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg?sanitize=true&#34; alt=&#34;Render nbviewer&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ageron/handson-ml2/raw/master/index.ipynb&#34;&gt;github.com&#39;s notebook viewer&lt;/a&gt; also works but it&#39;s not ideal: it&#39;s slower, the math equations are not always displayed correctly, and large notebooks often fail to open.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Want to run this project using a Docker image?&lt;/h3&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://github.com/ageron/handson-ml2/tree/master/docker&#34;&gt;Docker instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Want to install this project on your own machine?&lt;/h3&gt; &#xA;&lt;p&gt;Start by installing &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;Anaconda&lt;/a&gt; (or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;), &lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;git&lt;/a&gt;, and if you have a TensorFlow-compatible GPU, install the &lt;a href=&#34;https://www.nvidia.com/Download/index.aspx&#34;&gt;GPU driver&lt;/a&gt;, as well as the appropriate version of CUDA and cuDNN (see TensorFlow&#39;s documentation for more details).&lt;/p&gt; &#xA;&lt;p&gt;Next, clone this project by opening a terminal and typing the following commands (do not type the first &lt;code&gt;$&lt;/code&gt; signs on each line, they just indicate that these are terminal commands):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml2.git&#xA;$ cd handson-ml2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml&#xA;$ conda activate tf2&#xA;$ python -m ipykernel install --user --name=python3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, start Jupyter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you need further instructions, read the &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;detailed installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Which Python version should I use?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I recommend Python 3.8. If you follow the installation instructions above, that&#39;s the version you will get. Most code will work with other versions of Python 3, but some libraries do not support Python 3.9 or 3.10 yet, which is why I recommend Python 3.8.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;m getting an error when I call &lt;code&gt;load_housing_data()&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Make sure you call &lt;code&gt;fetch_housing_data()&lt;/code&gt; &lt;em&gt;before&lt;/em&gt; you call &lt;code&gt;load_housing_data()&lt;/code&gt;. If you&#39;re getting an HTTP error, make sure you&#39;re running the exact same code as in the notebook (copy/paste it if needed). If the problem persists, please check your network configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;m getting an SSL error on MacOSX&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You probably need to install the SSL certificates (see this &lt;a href=&#34;https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error&#34;&gt;StackOverflow question&lt;/a&gt;). If you downloaded Python from the official website, then run &lt;code&gt;/Applications/Python\ 3.8/Install\ Certificates.command&lt;/code&gt; in a terminal (change &lt;code&gt;3.8&lt;/code&gt; to whatever version you installed). If you installed Python using MacPorts, run &lt;code&gt;sudo port install curl-ca-bundle&lt;/code&gt; in a terminal.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;ve installed this project locally. How do I update it to the latest version?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How do I update my Python libraries to the latest versions, when using Anaconda?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;I would like to thank everyone &lt;a href=&#34;https://github.com/ageron/handson-ml2/graphs/contributors&#34;&gt;who contributed to this project&lt;/a&gt;, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park and Ian Beauregard who reviewed every notebook and submitted many PRs, including help on some of the exercise solutions. Thanks as well to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory, and to github user SuperYorio who helped on some exercise solutions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>DataTalksClub/data-engineering-zoomcamp</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/DataTalksClub/data-engineering-zoomcamp</id>
    <link href="https://github.com/DataTalksClub/data-engineering-zoomcamp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free Data Engineering course!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Data Engineering Zoomcamp&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Register in &lt;a href=&#34;https://datatalks.club/slack.html&#34;&gt;DataTalks.Club&#39;s Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Join the &lt;a href=&#34;https://app.slack.com/client/T01ATQK62F8/C01FABYF2RG&#34;&gt;&lt;code&gt;#course-data-engineering&lt;/code&gt;&lt;/a&gt; channel&lt;/li&gt; &#xA; &lt;li&gt;The videos are published to &lt;a href=&#34;https://www.youtube.com/c/DataTalksClub&#34;&gt;DataTalks.Club&#39;s YouTube channel&lt;/a&gt; in &lt;a href=&#34;https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&#34;&gt;the course playlist&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing&#34;&gt;Frequenty asked technical questions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Syllabus&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-1-introduction--prerequisites&#34;&gt;Week 1: Introduction &amp;amp; Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-2-data-ingestion&#34;&gt;Week 2: Data ingestion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-3-data-warehouse&#34;&gt;Week 3: Data Warehouse&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-4-analytics-engineering&#34;&gt;Week 4: Analytics Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-5-batch-processing&#34;&gt;Week 5: Batch processing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-6-streaming&#34;&gt;Week 6: Streaming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-7-8--9-project&#34;&gt;Week 7, 8 &amp;amp; 9: Project&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Taking the course&lt;/h2&gt; &#xA;&lt;h3&gt;Self-paced mode&lt;/h3&gt; &#xA;&lt;p&gt;All the materials of the course are freely available, so you can take the course at your own pace&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow the suggested syllabus (see below) week by week&lt;/li&gt; &#xA; &lt;li&gt;You don&#39;t need to fill in the registration form. Just start watching the videos and join Slack&lt;/li&gt; &#xA; &lt;li&gt;Check &lt;a href=&#34;https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing&#34;&gt;FAQ&lt;/a&gt; if you have problems&lt;/li&gt; &#xA; &lt;li&gt;If you can&#39;t find a solution to your problem in FAQ, ask for help in Slack&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2022 Cohort&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Start&lt;/strong&gt;: 17 January 2022&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Registration link&lt;/strong&gt;: &lt;a href=&#34;https://airtable.com/shr6oVXeQvSI5HuWD&#34;&gt;https://airtable.com/shr6oVXeQvSI5HuWD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vR9oQiYnAVvzL4dagnhvp0sngqagF0AceD0FGjhS-dnzMTBzNQIal3-hOgkTibVQvfuqbQ69b0fvRnf/pubhtml&#34;&gt;Leaderboard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Subscribe to our &lt;a href=&#34;https://calendar.google.com/calendar/?cid=ZXIxcjA1M3ZlYjJpcXU0dTFmaG02MzVxMG9AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&#34;&gt;public Google Calendar&lt;/a&gt; (it works from Desktop only)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Asking for help in Slack&lt;/h3&gt; &#xA;&lt;p&gt;The best way to get support is to use &lt;a href=&#34;https://datatalks.club/slack.html&#34;&gt;DataTalks.Club&#39;s Slack&lt;/a&gt;. Join the &lt;a href=&#34;https://app.slack.com/client/T01ATQK62F8/C01FABYF2RG&#34;&gt;&lt;code&gt;#course-data-engineering&lt;/code&gt;&lt;/a&gt; channel.&lt;/p&gt; &#xA;&lt;p&gt;To make discussions in Slack more organized:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow &lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/asking-questions.md&#34;&gt;these recommendations&lt;/a&gt; when asking for help&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://datatalks.club/slack/guidelines.html&#34;&gt;DataTalks.Club community guidelines&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Syllabus&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_1_basics_n_setup&#34;&gt;Week 1: Introduction &amp;amp; Prerequisites&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Course overview&lt;/li&gt; &#xA; &lt;li&gt;Introduction to GCP&lt;/li&gt; &#xA; &lt;li&gt;Docker and docker-compose&lt;/li&gt; &#xA; &lt;li&gt;Running Postgres locally with Docker&lt;/li&gt; &#xA; &lt;li&gt;Setting up infrastructure on GCP with Terraform&lt;/li&gt; &#xA; &lt;li&gt;Preparing the environment for the course&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_1_basics_n_setup&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_2_data_ingestion&#34;&gt;Week 2: Data ingestion&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data Lake&lt;/li&gt; &#xA; &lt;li&gt;Workflow orchestration&lt;/li&gt; &#xA; &lt;li&gt;Setting up Airflow locally&lt;/li&gt; &#xA; &lt;li&gt;Ingesting data to GCP with Airflow&lt;/li&gt; &#xA; &lt;li&gt;Ingesting data to local Postgres with Airflow&lt;/li&gt; &#xA; &lt;li&gt;Moving data from AWS to GCP (Transfer service)&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_2_data_ingestion&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_3_data_warehouse&#34;&gt;Week 3: Data Warehouse&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data Warehouse&lt;/li&gt; &#xA; &lt;li&gt;BigQuery&lt;/li&gt; &#xA; &lt;li&gt;Partitoning and clustering&lt;/li&gt; &#xA; &lt;li&gt;BigQuery best practices&lt;/li&gt; &#xA; &lt;li&gt;Internals of BigQuery&lt;/li&gt; &#xA; &lt;li&gt;Integrating BigQuery with Airflow&lt;/li&gt; &#xA; &lt;li&gt;BigQuery Machine Learning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_3_data_warehouse&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_4_analytics_engineering/&#34;&gt;Week 4: Analytics engineering&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basics of analytics engineering&lt;/li&gt; &#xA; &lt;li&gt;dbt (data build tool)&lt;/li&gt; &#xA; &lt;li&gt;BigQuery and dbt&lt;/li&gt; &#xA; &lt;li&gt;Postgres and dbt&lt;/li&gt; &#xA; &lt;li&gt;dbt models&lt;/li&gt; &#xA; &lt;li&gt;Testing and documenting&lt;/li&gt; &#xA; &lt;li&gt;Deployment to the cloud and locally&lt;/li&gt; &#xA; &lt;li&gt;Visualising the data with google data studio and metabase&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_4_analytics_engineering&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_5_batch_processing&#34;&gt;Week 5: Batch processing&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Batch processing&lt;/li&gt; &#xA; &lt;li&gt;What is Spark&lt;/li&gt; &#xA; &lt;li&gt;Spark Dataframes&lt;/li&gt; &#xA; &lt;li&gt;Spark SQL&lt;/li&gt; &#xA; &lt;li&gt;Internals: GroupBy and joins&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_5_batch_processing&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_6_stream_processing&#34;&gt;Week 6: Streaming&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Introduction to Kafka&lt;/li&gt; &#xA; &lt;li&gt;Schemas (avro)&lt;/li&gt; &#xA; &lt;li&gt;Kafka Streams&lt;/li&gt; &#xA; &lt;li&gt;Kafka Connect and KSQL&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_6_stream_processing&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_7_project&#34;&gt;Week 7, 8 &amp;amp; 9: Project&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Putting everything we learned to practice&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 7 and 8: working on your own project&lt;/li&gt; &#xA; &lt;li&gt;Week 9: reviewing your peers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_7_project&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;h3&gt;Architecture diagram&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/images/architecture/arch_1.jpg&#34;&gt; &#xA;&lt;h3&gt;Technologies&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Google Cloud Platform (GCP)&lt;/em&gt;: Cloud-based auto-scaling platform by Google &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;Google Cloud Storage (GCS)&lt;/em&gt;: Data Lake&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;BigQuery&lt;/em&gt;: Data Warehouse&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Terraform&lt;/em&gt;: Infrastructure-as-Code (IaC)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Docker&lt;/em&gt;: Containerization&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;SQL&lt;/em&gt;: Data Analysis &amp;amp; Exploration&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Airflow&lt;/em&gt;: Pipeline Orchestration&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;dbt&lt;/em&gt;: Data Transformation&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Spark&lt;/em&gt;: Distributed Processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Kafka&lt;/em&gt;: Streaming&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;To get most out of this course, you should feel comfortable with coding and command line, and know the basics of SQL. Prior experience with Python will be helpful, but you can pick Python relatively fast if you have experience with other programming languages.&lt;/p&gt; &#xA;&lt;p&gt;Prior experience with data engineering is not required.&lt;/p&gt; &#xA;&lt;h2&gt;Instructors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ankush Khanna (&lt;a href=&#34;https://linkedin.com/in/ankushkhanna2&#34;&gt;https://linkedin.com/in/ankushkhanna2&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Sejal Vaidya (&lt;a href=&#34;https://linkedin.com/in/vaidyasejal&#34;&gt;https://linkedin.com/in/vaidyasejal&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Victoria Perez Mola (&lt;a href=&#34;https://www.linkedin.com/in/victoriaperezmola/&#34;&gt;https://www.linkedin.com/in/victoriaperezmola/&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Alexey Grigorev (&lt;a href=&#34;https://linkedin.com/in/agrigorev&#34;&gt;https://linkedin.com/in/agrigorev&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tools&lt;/h2&gt; &#xA;&lt;p&gt;For this course you&#39;ll need to have the following software installed on your computer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker and Docker-Compose&lt;/li&gt; &#xA; &lt;li&gt;Python 3 (e.g. via &lt;a href=&#34;https://www.anaconda.com/products/individual&#34;&gt;Anaconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Google Cloud SDK&lt;/li&gt; &#xA; &lt;li&gt;Terraform&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_1_basics_n_setup&#34;&gt;Week 1&lt;/a&gt; for more details about installing these tools&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: I registered, but haven&#39;t received a confirmation email. Is it normal? &lt;strong&gt;A&lt;/strong&gt;: Yes, it&#39;s normal. It&#39;s not automated. But you will receive an email eventually&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: At what time of the day will it happen? &lt;strong&gt;A&lt;/strong&gt;: Office hours will happen on Mondays at 17:00 CET. But everything will be recorded, so you can watch it whenever it&#39;s convenient for you&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Will there be a certificate? &lt;strong&gt;A&lt;/strong&gt;: Yes, if you complete the project&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: I&#39;m 100% not sure I&#39;ll be able to attend. Can I still sign up? &lt;strong&gt;A&lt;/strong&gt;: Yes, please do! You&#39;ll receive all the updates and then you can watch the course at your own pace.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Do you plan to run a ML engineering course as well? &lt;strong&gt;A&lt;/strong&gt;: Glad you asked. &lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp&#34;&gt;We do&lt;/a&gt; :)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: I&#39;m stuck! I&#39;ve got a technical question! &lt;strong&gt;A&lt;/strong&gt;: Ask on Slack! And check out the &lt;a href=&#34;https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing&#34;&gt;student FAQ&lt;/a&gt;; many common issues have been answered already. If your issue is solved, please add how you solved it to the document. Thanks!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Our friends&lt;/h2&gt; &#xA;&lt;p&gt;Big thanks to other communities for helping us spread the word about the course:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dphi.tech/&#34;&gt;DPhi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlops.community/&#34;&gt;MLOps.community&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Check them out - they are cool!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/deep-rl-class</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/huggingface/deep-rl-class</id>
    <link href="https://github.com/huggingface/deep-rl-class" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repo contain the syllabus of the Hugging Face Deep Reinforcement Learning Class.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Hugging Face Deep Reinforcement Learning Class 🤗&lt;/h1&gt; &#xA;&lt;p&gt;In this free course, you will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📖 Study Deep Reinforcement Learning in &lt;strong&gt;theory and practice&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🧑‍💻 Learn to &lt;strong&gt;use famous Deep RL libraries&lt;/strong&gt; such as Stable Baselines3, RL Baselines3 Zoo, and RLlib.&lt;/li&gt; &#xA; &lt;li&gt;🤖 Train agents in &lt;strong&gt;unique environments&lt;/strong&gt; such as SnowballFight, Huggy the Doggo 🐶, and classical ones such as Space Invaders and PyBullet.&lt;/li&gt; &#xA; &lt;li&gt;💾 &lt;strong&gt;Publish your trained agents in one line of code to the Hugging Face Hub&lt;/strong&gt;. But also &lt;strong&gt;download powerful agents from the community&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🏆 &lt;strong&gt;Participate in challenges&lt;/strong&gt; where you will evaluate your agents against other teams.&lt;/li&gt; &#xA; &lt;li&gt;🖌️🎨 &lt;strong&gt;Learn to share your own environments made with Unity and Godot&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;➡️➡️➡️ Don&#39;t forget to sign up here: &lt;a href=&#34;http://eepurl.com/h1pElX&#34;&gt;http://eepurl.com/h1pElX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The best way to keep in touch is to &lt;strong&gt;join our discord server to exchange with the community and with us&lt;/strong&gt; 👉🏻 &lt;a href=&#34;https://discord.gg/aYka4Yhff9&#34;&gt;https://discord.gg/aYka4Yhff9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Are you new to Discord? Check our &lt;strong&gt;discord 101 to get the best practices&lt;/strong&gt; 👉 &lt;a href=&#34;https://github.com/huggingface/deep-rl-class/raw/main/DISCORD.Md&#34;&gt;https://github.com/huggingface/deep-rl-class/blob/main/DISCORD.Md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And don&#39;t forget to share with your friends who want to learn 🤗!&lt;/p&gt; &#xA;&lt;h2&gt;The Syllabus 🏗️&lt;/h2&gt; &#xA;&lt;p&gt;This course is &lt;strong&gt;self-paced&lt;/strong&gt; you can start when you want 🥳.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;📆 Publishing date&lt;/th&gt; &#xA;   &lt;th&gt;📘 Unit&lt;/th&gt; &#xA;   &lt;th&gt;👩‍💻 Hands-on&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/tree/main/unit1#unit-1-introduction-to-deep-reinforcement-learning&#34;&gt;Published 🥳&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/tree/main/unit1&#34;&gt;An Introduction to Deep Reinforcement Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/raw/main/unit1/unit1.ipynb&#34;&gt;Train a Deep Reinforcement Learning lander agent to land correctly on the Moon 🌕 using Stable-Baselines3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/tree/main/unit1/unit1-bonus&#34;&gt;Published 🥳&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/tree/main/unit1/unit1-bonus&#34;&gt;Bonus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/raw/main/unit2/README.md&#34;&gt;Published 🥳&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/raw/main/unit2/README.md&#34;&gt;Q-Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/raw/main/unit2/unit2.ipynb&#34;&gt;Train an agent to cross a Frozen lake ⛄ and train an autonomous taxi 🚖&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;June, the 1st&lt;/td&gt; &#xA;   &lt;td&gt;Deep Q-Learning and improvements&lt;/td&gt; &#xA;   &lt;td&gt;Train a Deep Q-Learning agent to play Space Invaders&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Policy-based methods&lt;/td&gt; &#xA;   &lt;td&gt;🏗️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Actor-Critic Methods&lt;/td&gt; &#xA;   &lt;td&gt;🏗️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Proximal Policy Optimization (PPO)&lt;/td&gt; &#xA;   &lt;td&gt;🏗️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Decision Transformers and offline Reinforcement Learning&lt;/td&gt; &#xA;   &lt;td&gt;🏗️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Towards better explorations methods&lt;/td&gt; &#xA;   &lt;td&gt;🏗️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;The library you&#39;ll learn during this course&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34;&gt;Stable-Baselines3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34;&gt;RL Baselines3 Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.ray.io/en/latest/rllib/index.html&#34;&gt;RLlib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vwxyzjn/cleanrl&#34;&gt;CleanRL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;More to come 🏗️&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;The Environments you&#39;ll use&lt;/h2&gt; &#xA;&lt;h3&gt;Custom environments made by the Hugging Face Team using Unity and Godot&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Huggy the Doggo 🐶 (Based on &lt;a href=&#34;https://blog.unity.com/technology/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit&#34;&gt;Unity&#39;s Puppo the Corgi work&lt;/a&gt;) &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/deep-rl-class/main/assets/img/huggy.jpg&#34; alt=&#34;huggy.jpg&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;SnowballFight ☃️ &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/deep-rl-class/main/assets/img/snowballfight.gif&#34; alt=&#34;snowballfight.gif&#34;&gt; 👉 Play it here: &lt;a href=&#34;https://huggingface.co/spaces/ThomasSimonini/SnowballFight&#34;&gt;https://huggingface.co/spaces/ThomasSimonini/SnowballFight&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;More to come 🚧&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Gym classic controls environments 🕹️&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lunar-Lander v2 🚀🌙&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/deep-rl-class/main/assets/img/lunarlander.gif&#34; alt=&#34;lunarlander.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;PyBullet 🤖&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More to come 🚧&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Gym Atari environments 👾&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Space Invaders 👾&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/deep-rl-class/main/assets/img/spaceinvaders.gif&#34; alt=&#34;spaceinvaders.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;MLAgents environments 🖌️&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More to come 🚧&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Good skills in Python 🐍&lt;/li&gt; &#xA; &lt;li&gt;Basics in Deep Learning and Pytorch&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If it&#39;s not the case yet, you can check these free resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python: &lt;a href=&#34;https://www.udacity.com/course/introduction-to-python--ud1110&#34;&gt;https://www.udacity.com/course/introduction-to-python--ud1110&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Intro to Deep Learning with PyTorch: &lt;a href=&#34;https://www.udacity.com/course/deep-learning-pytorch--ud188&#34;&gt;https://www.udacity.com/course/deep-learning-pytorch--ud188&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PyTorch in 60min: &lt;a href=&#34;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&#34;&gt;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is this class free?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes, totally free 🥳.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Do I need to have a Hugging Face account to follow the course?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes, to push your trained agents during the hands-on, you need an account (it&#39;s free) 🤗.&lt;/p&gt; &#xA;&lt;p&gt;You can create one here 👉 &lt;a href=&#34;https://huggingface.co/join&#34;&gt;https://huggingface.co/join&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What’s the format of the class?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The course consists of&amp;nbsp;&lt;strong&gt;8 Units.&lt;/strong&gt;&amp;nbsp;In each of the Units, we&#39;ll have:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;A theory explained part&lt;/strong&gt;: an article and a video (based on Deep Reinforcement Learning Course)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A hands-on Google Colab&lt;/strong&gt; where you&#39;ll learn to use famous Deep RL libraries such as Stable Baselines3, RL Baselines3 Zoo, and RLlib to train your agents in unique environments such as SnowballFight, Huggy the Doggo 🐶, and classical ones such as Space Invaders and PyBullet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Some optional challenges&lt;/strong&gt;: train an agent in another environment, and try to beat the results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It&#39;s not a live course video, so you can watch and read each unit when you want 🤗 You can check the syllabus here 👉 &lt;a href=&#34;https://github.com/huggingface/deep-rl-class&#34;&gt;https://github.com/huggingface/deep-rl-class&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What I will do during this course?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In this free course, you will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📖 Study Deep Reinforcement Learning in theory and practice.&lt;/li&gt; &#xA; &lt;li&gt;🧑‍💻 Learn to use famous Deep RL libraries such as Stable Baselines3, RL Baselines3 Zoo, and RLlib.&lt;/li&gt; &#xA; &lt;li&gt;🤖 Train agents in unique environments such as SnowballFight, Huggy the Doggo 🐶, and classical ones such as Space Invaders and PyBullet.&lt;/li&gt; &#xA; &lt;li&gt;💾 Publish your trained agents in one line of code to the Hub. But also download powerful agents from the community.&lt;/li&gt; &#xA; &lt;li&gt;🏆 Participate in challenges where you will evaluate your agents against other teams.&lt;/li&gt; &#xA; &lt;li&gt;🖌️🎨 Learn to share your own environments made with Unity and Godot.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Where do I sign up?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here 👉 &lt;a href=&#34;http://eepurl.com/h1pElX&#34;&gt;http://eepurl.com/h1pElX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Where can I find the course?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On this repository&lt;/strong&gt;, we&#39;ll publish every week the links (chapters, hands-ons, videos).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Where can I exchange with my classmates and with you?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We have a discord server where you &lt;strong&gt;can exchange with the community and with us&lt;/strong&gt; 👉🏻 &lt;a href=&#34;https://discord.gg/aYka4Yhff9&#34;&gt;https://discord.gg/aYka4Yhff9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Don’t forget to &lt;strong&gt;introduce yourself when you sign up 🤗&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I have some feedback&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We want to improve and update the course iteratively with your feedback. If you have some, please send a mail to &lt;a href=&#34;mailto:thomas.simonini@huggingface.co&#34;&gt;thomas.simonini@huggingface.co&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How much background knowledge is needed?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Some prerequisites:&lt;/p&gt; &#xA;&lt;p&gt;Good skills in &lt;strong&gt;Python&lt;/strong&gt; 🐍 Basics in &lt;strong&gt;Deep Learning and Pytorch&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If it&#39;s not the case yet, you can check these free resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python: &lt;a href=&#34;https://www.udacity.com/course/introduction-to-python--ud1110&#34;&gt;https://www.udacity.com/course/introduction-to-python--ud1110&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Intro to Deep Learning with PyTorch: &lt;a href=&#34;https://www.udacity.com/course/deep-learning-pytorch--ud188&#34;&gt;https://www.udacity.com/course/deep-learning-pytorch--ud188&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PyTorch in 60min: &lt;a href=&#34;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&#34;&gt;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is there a certificate?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes 🎉. You&#39;ll &lt;strong&gt;need to upload the eight models with the eight hands-on.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citing the project&lt;/h2&gt; &#xA;&lt;p&gt;To cite this repository in publications:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{deep-rl-class,&#xA;  author = {Simonini, Thomas and Sanseviero, Omar},&#xA;  title = {The Hugging Face Deep Reinforcement Learning Class},&#xA;  year = {2022},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/huggingface/deep-rl-class}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>kubernetes/community</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/kubernetes/community</id>
    <link href="https://github.com/kubernetes/community" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kubernetes community content&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kubernetes Community&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the Kubernetes community!&lt;/p&gt; &#xA;&lt;p&gt;This is the starting point for joining and contributing to the Kubernetes community - improving docs, improving code, giving talks etc.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about the project structure and organization, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/governance.md&#34;&gt;Project Governance&lt;/a&gt; information.&lt;/p&gt; &#xA;&lt;h2&gt;Communicating&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/communication/&#34;&gt;communication&lt;/a&gt; page lists communication channels like chat, issues, mailing lists, conferences, etc.&lt;/p&gt; &#xA;&lt;p&gt;For more specific topics, try a SIG.&lt;/p&gt; &#xA;&lt;h2&gt;Governance&lt;/h2&gt; &#xA;&lt;p&gt;Kubernetes has the following types of groups that are officially supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Committees&lt;/strong&gt; are named sets of people that are chartered to take on sensitive topics. This group is encouraged to be as open as possible while achieving its mission but, because of the nature of the topics discussed, private communications are allowed. Examples of committees include the steering committee and things like security or code of conduct.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Special Interest Groups (SIGs)&lt;/strong&gt; are persistent open groups that focus on a part of the project. SIGs must have open and transparent proceedings. Anyone is welcome to participate and contribute provided they follow the Kubernetes Code of Conduct. The purpose of a SIG is to own and develop a set of &lt;strong&gt;subprojects&lt;/strong&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Subprojects&lt;/strong&gt; Each SIG can have a set of subprojects. These are smaller groups that can work independently. Some subprojects will be part of the main Kubernetes deliverables while others will be more speculative and live in the &lt;code&gt;kubernetes-sigs&lt;/code&gt; github org.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Working Groups&lt;/strong&gt; are temporary groups that are formed to address issues that cross SIG boundaries. Working groups do not own any code or other long term artifacts. Working groups can report back and act through involved SIGs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User Groups&lt;/strong&gt; are groups for facilitating communication and discovery of information related to topics that have long term relevance to large groups of Kubernetes users. They do not have ownership of parts of the Kubernetes code base.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/governance.md&#34;&gt;full governance doc&lt;/a&gt; for more details on these groups.&lt;/p&gt; &#xA;&lt;p&gt;A SIG can have its own policy for contribution, described in a &lt;code&gt;README&lt;/code&gt; or &lt;code&gt;CONTRIBUTING&lt;/code&gt; file in the SIG folder in this repo (e.g. &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/sig-cli/CONTRIBUTING.md&#34;&gt;sig-cli/CONTRIBUTING.md&lt;/a&gt;), and its own mailing list, slack channel, etc.&lt;/p&gt; &#xA;&lt;p&gt;If you want to edit details about a SIG (e.g. its weekly meeting time or its leads), please follow &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/generator&#34;&gt;these instructions&lt;/a&gt; that detail how our docs are auto-generated.&lt;/p&gt; &#xA;&lt;h2&gt;Learn to Build&lt;/h2&gt; &#xA;&lt;p&gt;Links in &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/devel/README.md&#34;&gt;contributors/devel/README.md&lt;/a&gt; lead to many relevant technical topics.&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;A first step to contributing is to pick from the &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/sig-list.md&#34;&gt;list of kubernetes SIGs&lt;/a&gt;. Start attending SIG meetings, join the slack channel and subscribe to the mailing list. SIGs will often have a set of &#34;help wanted&#34; issues that can help new contributors get involved.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/README.md&#34;&gt;Contributor Guide&lt;/a&gt; provides detailed instruction on how to get your ideas and bug fixes seen and accepted, including:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;How to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/first-contribution.md#file-an-issue&#34;&gt;file an issue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;How to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/first-contribution.md#find-something-to-work-on&#34;&gt;find something to work on&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;How to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/contributing.md#opening-a-pull-request&#34;&gt;open a pull request&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Membership&lt;/h2&gt; &#xA;&lt;p&gt;We encourage all contributors to become members. We aim to grow an active, healthy community of contributors, reviewers, and code owners. Learn more about requirements and responsibilities of membership in our &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/community-membership.md&#34;&gt;Community Membership&lt;/a&gt; page.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mhamilton723/STEGO</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/mhamilton723/STEGO</id>
    <link href="https://github.com/mhamilton723/STEGO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unsupervised Semantic Segmentation by Distilling Feature Correspondences&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;STEGO: Unsupervised Semantic Segmentation by Distilling Feature Correspondences&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://mhamilton.net/stego.html&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2203.08414&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://aka.ms/stego-video&#34;&gt;Video&lt;/a&gt; | &lt;a href=&#34;https://iclr.cc/virtual/2022/poster/6068&#34;&gt;ICLR 2022&lt;/a&gt; |&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mhamilton.net/&#34;&gt;Mark Hamilton&lt;/a&gt;, &lt;a href=&#34;https://ztzhang.info/&#34;&gt;Zhoutong Zhang&lt;/a&gt;, &lt;a href=&#34;http://home.bharathh.info/&#34;&gt;Bharath Hariharan&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cornell.edu/~snavely/&#34;&gt;Noah Snavely&lt;/a&gt;, &lt;a href=&#34;https://billf.mit.edu/about/bio&#34;&gt;William T. Freeman&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official implementation of the paper &#34;Unsupervised Semantic Segmentation by Distilling Feature Correspondences&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/mhamilton723/STEGO/blob/master/src/STEGO_Colab_Demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://paperswithcode.com/sota/unsupervised-semantic-segmentation-on?p=unsupervised-semantic-segmentation-by-2&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/unsupervised-semantic-segmentation-by-2/unsupervised-semantic-segmentation-on&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://paperswithcode.com/sota/unsupervised-semantic-segmentation-on-coco-4?p=unsupervised-semantic-segmentation-by-2&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/unsupervised-semantic-segmentation-by-2/unsupervised-semantic-segmentation-on-coco-4&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://paperswithcode.com/sota/unsupervised-semantic-segmentation-on-potsdam-1?p=unsupervised-semantic-segmentation-by-2&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/unsupervised-semantic-segmentation-by-2/unsupervised-semantic-segmentation-on-potsdam-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/NPub4E4o8BA&#34;&gt;&lt;img src=&#34;https://marhamilresearch4.blob.core.windows.net/stego-public/graphics/STEGO%20Header%20video%20(2).jpg&#34; alt=&#34;Overview Video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;!--ts--&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#training&#34;&gt;Training&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#bringing-your-own-data&#34;&gt;Bringing your own data&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#understanding-stego&#34;&gt;Understanding STEGO&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#unsupervised-semantic-segmentation&#34;&gt;Unsupervised Semantic Segmentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#deep-features-connect-objects-across-images&#34;&gt;Deep features connect objects across images&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#the-stego-architecture&#34;&gt;The STEGO architecture&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#contact&#34;&gt;Contact&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--te--&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;h3&gt;Clone this repository:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/mhamilton723/STEGO.git&#xA;cd STEGO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install Conda Environment&lt;/h3&gt; &#xA;&lt;p&gt;Please visit the &lt;a href=&#34;https://docs.anaconda.com/anaconda/install/index.html&#34;&gt;Anaconda install page&lt;/a&gt; if you do not already have conda installed&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f environment.yml&#xA;conda activate stego&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download Pre-Trained Models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd src&#xA;python download_models.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download Datasets&lt;/h3&gt; &#xA;&lt;p&gt;First, change the &lt;code&gt;pytorch_data_dir&lt;/code&gt; variable to your systems pytorch data directory where datasets are stored.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python download_datasets.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once downloaded please navigate to your pytorch data dir and unzip the resulting files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /YOUR/PYTORCH/DATA/DIR&#xA;unzip cocostuff.zip&#xA;unzip cityscapes.zip&#xA;unzip potsdam.zip&#xA;unzip potsdamraw.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To evaluate our pretrained models please run the following in &lt;code&gt;STEGO/src&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python eval_segmentation.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;One can change the evaluation parameters and model by editing &lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/src/configs/eval_config.yml&#34;&gt;&lt;code&gt;STEGO/src/configs/eval_config.yml&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To train STEGO from scratch, please first generate the KNN indices for the datasets of interest:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python precompute_knns.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can run the following in &lt;code&gt;STEGO/src&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train_segmentation.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Hyperparameters can be adjusted in &lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/src/configs/train_config.yml&#34;&gt;&lt;code&gt;STEGO/src/configs/train_config.yml&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To monitor training with tensorboard run the following from &lt;code&gt;STEGO&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;tensorboard --logdir logs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Bringing your own data&lt;/h3&gt; &#xA;&lt;p&gt;To train STEGO on your own dataset please create a directory in your pytorch data root with the following structure. Note, if you do not have labels, omit the &lt;code&gt;labels&lt;/code&gt; directory from the structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dataset_name&#xA;|── imgs&#xA;|   ├── train&#xA;|   |   |── unique_img_name_1.jpg&#xA;|   |   └── unique_img_name_2.jpg&#xA;|   └── val&#xA;|       |── unique_img_name_3.jpg&#xA;|       └── unique_img_name_4.jpg&#xA;└── labels&#xA;    ├── train&#xA;    |   |── unique_img_name_1.png&#xA;    |   └── unique_img_name_2.png&#xA;    └── val&#xA;        |── unique_img_name_3.png&#xA;        └── unique_img_name_4.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next in &lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/src/configs/train_config.yml&#34;&gt;&lt;code&gt;STEGO/src/configs/train_config.yml&lt;/code&gt;&lt;/a&gt; set the following parameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;dataset_name: &#34;directory&#34;&#xA;dir_dataset_name: &#34;dataset_name&#34;&#xA;dir_dataset_n_classes: 5 # This is the number of object types to find&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to train with cropping to increase spatial resolution run our &lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/src/crop_datasets.py&#34;&gt;cropping utility&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Finally, uncomment the custom dataset code and run &lt;code&gt;python precompute_knns.py&lt;/code&gt; from &lt;code&gt;STEGO\src&lt;/code&gt; to generate the prerequisite KNN information for the custom dataset.&lt;/p&gt; &#xA;&lt;p&gt;You can now train on your custom dataset using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train_segmentation.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Understanding STEGO&lt;/h2&gt; &#xA;&lt;h3&gt;Unsupervised semantic segmentation&lt;/h3&gt; &#xA;&lt;p&gt;Real-world images can be cluttered with multiple objects making classification feel arbitrary. Furthermore, objects in the real world don&#39;t always fit in bounding boxes. Semantic segmentation methods aim to avoid these challenges by assigning each pixel of an image its own class label. Conventional semantic segmentation methods are notoriously difficult to train due to their dependence on densely labeled images, which can take 100x longer to create than bounding boxes or class annotations. This makes it hard to gather sizable and diverse datasets impossible in domains where humans don&#39;t know the structure a-priori. We sidestep these challenges by learning an ontology of objects with pixel-level semantic segmentation through only self-supervision.&lt;/p&gt; &#xA;&lt;h3&gt;Deep features connect objects across images&lt;/h3&gt; &#xA;&lt;p&gt;Self-supervised contrastive learning enables algorithms to learn intelligent representations for images without supervision. STEGO builds on this work by showing that representations from self-supervised visual transformers like Caron et. al.’s DINO are already aware of the relationships between objects. By computing the cosine similarity between image features, we can see that similar semantic regions such as grass, motorcycles, and sky are “linked” together by feature similarity.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://mhamilton.net/images/Picture3.gif&#34; alt=&#34;Feature connection GIF&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;The STEGO architecture&lt;/h3&gt; &#xA;&lt;p&gt;The STEGO unsupervised segmentation system learns by distilling correspondences between images into a set of class labels using a contrastive loss. In particular we aim to learn a segmentation that respects the induced correspondences between objects. To achieve this we train a shallow segmentation network on top of the DINO ViT backbone with three contrastive terms that distill connections between an image and itself, similar images, and random other images respectively. If two regions are strongly coupled by deep features we encourage them to share the same class.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/results/figures/stego.svg?sanitize=true&#34; alt=&#34;Architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Results&lt;/h3&gt; &#xA;&lt;p&gt;We evaluate the STEGO algorithm on the CocoStuff, Cityscapes, and Potsdam semantic segmentation datasets. Because these methods see no labels, we use a Hungarian matching algorithm to find the best mapping between clusters and dataset classes. We find that STEGO is capable of segmenting complex and cluttered scenes with much higher spatial resolution and sensitivity than the prior art, &lt;a href=&#34;https://sites.google.com/view/picie-cvpr2021/home&#34;&gt;PiCIE&lt;/a&gt;. This not only yields a substantial qualitative improvement, but also more than doubles the mean intersection over union (mIoU). For results on Cityscapes, and Potsdam see &lt;a href=&#34;https://arxiv.org/abs/2203.08414&#34;&gt;our paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/results/figures/cocostuff27_results.jpg&#34; alt=&#34;Cocostuff results&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{hamilton2022unsupervised,&#xA;  title={Unsupervised Semantic Segmentation by Distilling Feature Correspondences},&#xA;  author={Hamilton, Mark and Zhang, Zhoutong and Hariharan, Bharath and Snavely, Noah and Freeman, William T},&#xA;  journal={arXiv preprint arXiv:2203.08414},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;For feedback, questions, or press inquiries please contact &lt;a href=&#34;mailto:markth@mit.edu&#34;&gt;Mark Hamilton&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/NeMo</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/NVIDIA/NeMo</id>
    <link href="https://github.com/NVIDIA/NeMo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NeMo: a toolkit for conversational AI&lt;/p&gt;&lt;hr&gt;&lt;p&gt;|status| |documentation| |license| |lgtm_grade| |lgtm_alerts| |black|&lt;/p&gt; &#xA;&lt;p&gt;.. |status| image:: &lt;a href=&#34;http://www.repostatus.org/badges/latest/active.svg&#34;&gt;http://www.repostatus.org/badges/latest/active.svg&lt;/a&gt; :target: &lt;a href=&#34;http://www.repostatus.org/#active&#34;&gt;http://www.repostatus.org/#active&lt;/a&gt; :alt: Project Status: Active – The project has reached a stable, usable state and is being actively developed.&lt;/p&gt; &#xA;&lt;p&gt;.. |documentation| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&lt;/a&gt; :alt: Documentation :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |license| image:: &lt;a href=&#34;https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg&#34;&gt;https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/NVIDIA/NeMo/raw/master/LICENSE&#34;&gt;https://github.com/NVIDIA/NeMo/blob/master/LICENSE&lt;/a&gt; :alt: NeMo core license and license for collections in this repo&lt;/p&gt; &#xA;&lt;p&gt;.. |lgtm_grade| image:: &lt;a href=&#34;https://img.shields.io/lgtm/grade/python/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&#34;&gt;https://img.shields.io/lgtm/grade/python/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&lt;/a&gt; :target: &lt;a href=&#34;https://lgtm.com/projects/g/NVIDIA/NeMo/context:python&#34;&gt;https://lgtm.com/projects/g/NVIDIA/NeMo/context:python&lt;/a&gt; :alt: Language grade: Python&lt;/p&gt; &#xA;&lt;p&gt;.. |lgtm_alerts| image:: &lt;a href=&#34;https://img.shields.io/lgtm/alerts/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&#34;&gt;https://img.shields.io/lgtm/alerts/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&lt;/a&gt; :target: &lt;a href=&#34;https://lgtm.com/projects/g/NVIDIA/NeMo/alerts/&#34;&gt;https://lgtm.com/projects/g/NVIDIA/NeMo/alerts/&lt;/a&gt; :alt: Total alerts&lt;/p&gt; &#xA;&lt;p&gt;.. |black| image:: &lt;a href=&#34;https://img.shields.io/badge/code%20style-black-000000.svg&#34;&gt;https://img.shields.io/badge/code%20style-black-000000.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/psf/black&#34;&gt;https://github.com/psf/black&lt;/a&gt; :alt: Code style: black&lt;/p&gt; &#xA;&lt;p&gt;.. _main-readme:&lt;/p&gt; &#xA;&lt;h1&gt;&lt;strong&gt;NVIDIA NeMo&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech synthesis (TTS). The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new &lt;code&gt;conversational AI models &amp;lt;https://developer.nvidia.com/conversational-ai#started&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Pre-trained NeMo models. &amp;lt;https://catalog.ngc.nvidia.com/models?query=nemo&amp;amp;orderBy=weightPopularDESC&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Introductory video. &amp;lt;https://www.youtube.com/embed/wBgpMf_KQVw&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speech processing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Automatic Speech Recognition (ASR) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/intro.html&amp;gt;&lt;/code&gt;_ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Supported models: Jasper, QuartzNet, CitriNet, Conformer-CTC, Conformer-Transducer, ContextNet, LSTM-Transducer (RNNT), LSTM-CTC, ...&lt;/li&gt; &#xA;     &lt;li&gt;Supports CTC and Transducer/RNNT losses/decoders&lt;/li&gt; &#xA;     &lt;li&gt;Beam Search decoding&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;Language Modelling for ASR &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/asr_language_modeling.html&amp;gt;&lt;/code&gt;_: N-gram LM in fusion with Beam Search decoding, Neural Rescoring with Transformer&lt;/li&gt; &#xA;     &lt;li&gt;Streaming and Buffered ASR (CTC/Transducer) - &lt;code&gt;Chunked Inference Examples &amp;lt;https://github.com/NVIDIA/NeMo/tree/main/examples/asr/asr_chunked_inference&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speech Classification and Speech Command Recognition &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speech_classification/intro.html&amp;gt;&lt;/code&gt;_: MatchboxNet (Command Recognition)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Voice activity Detection (VAD) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/speech_classification/models.html#marblenet-vad&amp;gt;&lt;/code&gt;_: MarbleNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speaker Recognition &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speaker_recognition/intro.html&amp;gt;&lt;/code&gt;_: TitaNet, ECAPA_TDNN, SpeakerNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speaker Diarization &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speaker_diarization/intro.html&amp;gt;&lt;/code&gt;_: TitaNet, ECAPA_TDNN, SpeakerNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Pretrained models on different languages. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_asr&amp;gt;&lt;/code&gt;_: English, Spanish, German, Russian, Chinese, French, Italian, Polish, ...&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained speech processing models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_asr&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Natural Language Processing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;NeMo Megatron pre-training of Large Language Models &amp;lt;https://developer.nvidia.com/nemo-megatron-early-access&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Neural Machine Translation (NMT) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/machine_translation.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Punctuation and Capitalization &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Token classification (named entity recognition) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/token_classification.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Text classification &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/text_classification.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Joint Intent and Slot Classification &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/joint_intent_slot.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Question answering &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/question_answering.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;GLUE benchmark &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/glue_benchmark.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Information retrieval &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/information_retrieval.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Entity Linking &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/entity_linking.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Dialogue State Tracking &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/sgd_qa.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Prompt Tuning &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/prompt_learning.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained NLP models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_nlp&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Speech synthesis (TTS) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tts/intro.html#&amp;gt;&lt;/code&gt;_ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Spectrogram generation: Tacotron2, GlowTTS, TalkNet, FastPitch, FastSpeech2, Mixer-TTS, Mixer-TTS-X&lt;/li&gt; &#xA;   &lt;li&gt;Vocoders: WaveGlow, SqueezeWave, UniGlow, MelGAN, HiFiGAN, UnivNet&lt;/li&gt; &#xA;   &lt;li&gt;End-to-end speech generation: FastPitch_HifiGan_E2E, FastSpeech2_HifiGan_E2E&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained TTS models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_tts&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Tools &amp;lt;https://github.com/NVIDIA/NeMo/tree/main/tools&amp;gt;&lt;/code&gt;_ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Text Processing (text normalization and inverse text normalization) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/text_normalization/intro.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;CTC-Segmentation tool &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tools/ctc_segmentation.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speech Data Explorer &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tools/speech_data_explorer.html&amp;gt;&lt;/code&gt;_: a dash-based tool for interactive exploration of ASR/TTS datasets&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Built for speed, NeMo can utilize NVIDIA&#39;s Tensor Cores and scale out training to multiple GPUs and multiple nodes.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python 3.8 or above&lt;/li&gt; &#xA; &lt;li&gt;Pytorch 1.10.0 or above&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU for training&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;.. |main| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&lt;/a&gt; :alt: Documentation Status :scale: 100% :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |stable| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable&lt;/a&gt; :alt: Documentation Status :scale: 100% :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;+---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+ | Version | Status | Description | +=========+=============+==========================================================================================================================================+ | Latest | |main| | &lt;code&gt;Documentation of the latest (i.e. main) branch. &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&amp;gt;&lt;/code&gt;_ | +---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+ | Stable | |stable| | &lt;code&gt;Documentation of the stable (i.e. most recent release) branch. &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&amp;gt;&lt;/code&gt;_ | +---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;A great way to start with NeMo is by checking &lt;code&gt;one of our tutorials &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h2&gt;Getting help with NeMo&lt;/h2&gt; &#xA;&lt;p&gt;FAQ can be found on NeMo&#39;s &lt;code&gt;Discussions board &amp;lt;https://github.com/NVIDIA/NeMo/discussions&amp;gt;&lt;/code&gt;_. You are welcome to ask questions or start discussions there.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Conda&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;We recommend installing NeMo in a fresh Conda environment.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    conda create --name nemo python==3.8&#xA;    conda activate nemo&#xA;&#xA;Install PyTorch using their `configurator &amp;lt;https://pytorch.org/get-started/locally/&amp;gt;`_. &#xA;&#xA;.. code-block:: bash&#xA;&#xA;    conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch&#xA;&#xA;.. note::&#xA;&#xA;  The command used to install PyTorch may depend on your system.&#xA;&#xA;Pip&#xA;~~~&#xA;Use this installation mode if you want the latest released version.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;    pip install Cython&#xA;    pip install nemo_toolkit[&#39;all&#39;]&#xA;&#xA;.. note::&#xA;&#xA;    Depending on the shell used, you may need to use ``&#34;nemo_toolkit[all]&#34;`` instead in the above command.&#xA;&#xA;Pip from source&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use this installation mode if you want the a version from particular GitHub branch (e.g main).&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;pip install Cython&#xA;python -m pip install git+https://github.com/NVIDIA/NeMo.git@{BRANCH}#egg=nemo_toolkit[all]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From source&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Use this installation mode if you are contributing to NeMo.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;    git clone https://github.com/NVIDIA/NeMo&#xA;    cd NeMo&#xA;    ./reinstall.sh&#xA;&#xA;.. note::&#xA;&#xA;    If you only want the toolkit without additional conda-based dependencies, you may replace ``reinstall.sh``&#xA;    with ``pip install -e .`` when your PWD is the root of the NeMo repository.&#xA;&#xA;RNNT&#xA;~~~~&#xA;Note that RNNT requires numba to be installed from conda.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;  conda remove numba&#xA;  pip uninstall numba&#xA;  conda install -c conda-forge numba&#xA;&#xA;Megatron GPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Megatron GPT training requires NVIDIA Apex to be installed.&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/NVIDIA/apex&#xA;cd apex&#xA;git checkout 9263bc8c6c16555bd55dd759f1a1b8c0cd187d10&#xA;pip install -v --disable-pip-version-check --no-cache-dir --global-option=&#34;--cpp_ext&#34; --global-option=&#34;--cuda_ext&#34; --global-option=&#34;--fast_layer_norm&#34; ./&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker containers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;To build a nemo container with Dockerfile from a branch, please run &#xA;&#xA;.. code-block:: bash&#xA;&#xA;    DOCKER_BUILDKIT=1 docker build -f Dockerfile -t nemo:latest .&#xA;&#xA;&#xA;If you chose to work with main branch, we recommend using NVIDIA&#39;s PyTorch container version 22.04-py3 and then installing from GitHub.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    docker run --gpus all -it --rm -v &amp;lt;nemo_github_folder&amp;gt;:/NeMo --shm-size=8g \&#xA;    -p 8888:8888 -p 6006:6006 --ulimit memlock=-1 --ulimit \&#xA;    stack=67108864 --device=/dev/snd nvcr.io/nvidia/pytorch:22.04-py3&#xA;&#xA;Examples&#xA;--------&#xA;&#xA;Many examples can be found under `&#34;Examples&#34; &amp;lt;https://github.com/NVIDIA/NeMo/tree/stable/examples&amp;gt;`_ folder.&#xA;&#xA;&#xA;Contributing&#xA;------------&#xA;&#xA;We welcome community contributions! Please refer to the  `CONTRIBUTING.md &amp;lt;https://github.com/NVIDIA/NeMo/blob/stable/CONTRIBUTING.md&amp;gt;`_ CONTRIBUTING.md for the process.&#xA;&#xA;Publications&#xA;------------&#xA;&#xA;We provide an ever growing list of publications that utilize the NeMo framework. Please refer to `PUBLICATIONS.md &amp;lt;https://github.com/NVIDIA/NeMo/blob/main/PUBLICATIONS.md&amp;gt;`_. We welcome the addition of your own articles to this list !&#xA;&#xA;Citation&#xA;--------&#xA;&#xA;.. code-block:: bash&#xA;&#xA;  @article{kuchaiev2019nemo,&#xA;    title={Nemo: a toolkit for building ai applications using neural modules},&#xA;    author={Kuchaiev, Oleksii and Li, Jason and Nguyen, Huyen and Hrinchuk, Oleksii and Leary, Ryan and Ginsburg, Boris and Kriman, Samuel and Beliaev, Stanislav and Lavrukhin, Vitaly and Cook, Jack and others},&#xA;    journal={arXiv preprint arXiv:1909.09577},&#xA;    year={2019}&#xA;  }&#xA;&#xA;License&#xA;-------&#xA;NeMo is under `Apache 2.0 license &amp;lt;https://github.com/NVIDIA/NeMo/blob/stable/LICENSE&amp;gt;`_.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>deepmind/deepmind-research</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/deepmind/deepmind-research</id>
    <link href="https://github.com/deepmind/deepmind-research" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains implementations and illustrative code to accompany DeepMind publications&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepMind Research&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains implementations and illustrative code to accompany DeepMind publications. Along with publishing papers to accompany research conducted at DeepMind, we release open-source &lt;a href=&#34;https://deepmind.com/research/open-source/open-source-environments/&#34;&gt;environments&lt;/a&gt;, &lt;a href=&#34;https://deepmind.com/research/open-source/open-source-datasets/&#34;&gt;data sets&lt;/a&gt;, and &lt;a href=&#34;https://deepmind.com/research/open-source/open-source-code/&#34;&gt;code&lt;/a&gt; to enable the broader research community to engage with our work and build upon it, with the ultimate goal of accelerating scientific progress to benefit society. For example, you can build on our implementations of the &lt;a href=&#34;https://github.com/deepmind/dqn&#34;&gt;Deep Q-Network&lt;/a&gt; or &lt;a href=&#34;https://github.com/deepmind/dnc&#34;&gt;Differential Neural Computer&lt;/a&gt;, or experiment in the same environments we use for our research, such as &lt;a href=&#34;https://github.com/deepmind/lab&#34;&gt;DeepMind Lab&lt;/a&gt; or &lt;a href=&#34;https://github.com/deepmind/pysc2&#34;&gt;StarCraft II&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you enjoy building tools, environments, software libraries, and other infrastructure of the kind listed below, you can view open positions to work in related areas on our &lt;a href=&#34;https://deepmind.com/careers/&#34;&gt;careers page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a full list of our publications, please see &lt;a href=&#34;https://deepmind.com/research/publications/&#34;&gt;https://deepmind.com/research/publications/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/fusion_tcv&#34;&gt;Magnetic control of tokamak plasmas through deep reinforcement learning&lt;/a&gt;, Nature 2022&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/density_functional_approximation_dm21&#34;&gt;Pushing the Frontiers of Density Functionals by Solving the Fractional Electron Problem&lt;/a&gt;, Science 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/pitfalls_static_language_models&#34;&gt;Mind the Gap: Assessing Temporal Generalization in Neural Language Models&lt;/a&gt;, NeurIPS 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/tandem_dqn&#34;&gt;The Difficulty of Passive Learning in Deep Reinforcement Learning&lt;/a&gt;, NeurIPS 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/nowcasting&#34;&gt;Skilful precipitation nowcasting using deep generative models of radar&lt;/a&gt;, Nature 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/cadl&#34;&gt;Compute-Aided Design as Language&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/continual_learning&#34;&gt;Encoders and ensembles for continual learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/hierarchical_transformer_memory&#34;&gt;Towards mental time travel: a hierarchical memory for reinforcement learning agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver&#34;&gt;Perceiver IO: A General Architecture for Structured Inputs &amp;amp; Outputs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/neural_mip_solving&#34;&gt;Solving Mixed Integer Programs Using Neural Networks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/noisy_label&#34;&gt;A Realistic Simulation Framework for Learning with Label Noise&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/rapid_task_solving&#34;&gt;Rapid Task-Solving in Novel Environments&lt;/a&gt;, ICLR 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/wikigraphs&#34;&gt;WikiGraphs: A Wikipedia - Knowledge Graph Paired Dataset&lt;/a&gt;, TextGraphs 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/box_arrangement&#34;&gt;Behavior Priors for Efficient Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/meshgraphnets&#34;&gt;Learning Mesh-Based Simulation with Graph Networks&lt;/a&gt;, ICLR 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/ogb_lsc&#34;&gt;Open Graph Benchmark - Large-Scale Challenge (OGB-LSC)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/synthetic_returns&#34;&gt;Synthetic Returns for Long-Term Credit Assignment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/galaxy_mergers&#34;&gt;A Deep Learning Approach for Characterizing Major Galaxy Mergers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/kfac_ferminet_alpha&#34;&gt;Better, Faster Fermionic Neural Networks&lt;/a&gt; (KFAC implementation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/object_attention_for_reasoning&#34;&gt;Object-based attention for spatio-temporal reasoning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer&#34;&gt;Effective gene expression prediction from sequence by integrating long-range interactions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/satore&#34;&gt;Satore: First-order logic saturation with atom rewriting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/nfnets&#34;&gt;Characterizing signal propagation to close the performance gap in unnormalized ResNets&lt;/a&gt;, ICLR 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/adversarial_robustness&#34;&gt;Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/cmtouch&#34;&gt;Learning rich touch representations through cross-modal self-supervision&lt;/a&gt;, CoRL 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/functional_regularisation_for_continual_learning&#34;&gt;Functional Regularisation for Continual Learning&lt;/a&gt;, ICLR 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/avae&#34;&gt;The Autoencoding Variational Autoencoder&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/mmv&#34;&gt;Self-Supervised MultiModal Versatile Networks&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/ode_gan&#34;&gt;ODE-GAN: Training GANs by Solving Ordinary Differential Equations&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/causal_reasoning&#34;&gt;Algorithms for Causal Reasoning in Probability Trees&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/gated_linear_networks&#34;&gt;Gated Linear Networks&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/himo&#34;&gt;Value-driven Hindsight Modelling&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/learned_free_energy_estimation&#34;&gt;Targeted free energy estimation via learned mappings&lt;/a&gt;, Journal of Chemical Physics 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/learning_to_simulate&#34;&gt;Learning to Simulate Complex Physics with Graph Networks&lt;/a&gt;, ICML 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/physics_planning_games&#34;&gt;Physically Embedded Planning Problems&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/polygen&#34;&gt;PolyGen: PolyGen: An Autoregressive Generative Model of 3D Meshes&lt;/a&gt;, ICML 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/byol&#34;&gt;Bootstrap Your Own Latent&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/catch_carry&#34;&gt;Catch &amp;amp; Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks&lt;/a&gt;, SIGGRAPH 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/memo&#34;&gt;MEMO: A Deep Network For Flexible Combination Of Episodic Memories&lt;/a&gt;, ICLR 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/rl_unplugged&#34;&gt;RL Unplugged: Benchmarks for Offline Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/geomancer&#34;&gt;Disentangling by Subspace Diffusion (GEOMANCER)&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/affordances_theory&#34;&gt;What can I do here? A theory of affordances in reinforcement learning&lt;/a&gt;, ICML 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/sketchy&#34;&gt;Scaling data-driven robotics with reward sketching and batch reinforcement learning&lt;/a&gt;, RSS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/counterfactual_fairness&#34;&gt;Path-Specific Counterfactual Fairness&lt;/a&gt;, AAAI 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/option_keyboard&#34;&gt;The Option Keyboard: Combining Skills in Reinforcement Learning&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/visr&#34;&gt;VISR - Fast Task Inference with Variational Intrinsic Successor Features&lt;/a&gt;, ICLR 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/glassy_dynamics&#34;&gt;Unveiling the predictive power of static structure in glassy systems&lt;/a&gt;, Nature Physics 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/iodine&#34;&gt;Multi-Object Representation Learning with Iterative Variational Inference (IODINE)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/alphafold_casp13&#34;&gt;AlphaFold CASP13&lt;/a&gt;, Nature 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/unrestricted_advx&#34;&gt;Unrestricted Adversarial Challenge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/hierarchical_probabilistic_unet&#34;&gt;Hierarchical Probabilistic U-Net (HPU-Net)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/scratchgan&#34;&gt;Training Language GANs from Scratch&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/tvt&#34;&gt;Temporal Value Transport&lt;/a&gt;, Nature Communications 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/curl&#34;&gt;Continual Unsupervised Representation Learning (CURL)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/transporter&#34;&gt;Unsupervised Learning of Object Keypoints (Transporter)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/bigbigan&#34;&gt;BigBiGAN&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/cs_gan&#34;&gt;Deep Compressed Sensing&lt;/a&gt;, ICML 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/side_effects_penalties&#34;&gt;Side Effects Penalties&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/PrediNet&#34;&gt;PrediNet Architecture and Relations Game Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/unsupervised_adversarial_training&#34;&gt;Unsupervised Adversarial Training&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/graph_matching_networks&#34;&gt;Graph Matching Networks for Learning the Similarity of Graph Structured Objects&lt;/a&gt;, ICML 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/regal&#34;&gt;REGAL: Transfer Learning for Fast Optimization of Computation Graphs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/ensemble_loss_landscape&#34;&gt;Deep Ensembles: A Loss Landscape Perspective&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/powerpropagation&#34;&gt;Powerpropagation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/physics_inspired_models&#34;&gt;Physics Inspired Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;This is not an official Google product.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jeffheaton/t81_558_deep_learning</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/jeffheaton/t81_558_deep_learning</id>
    <link href="https://github.com/jeffheaton/t81_558_deep_learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Washington University (in St. Louis) Course T81-558: Applications of Deep Neural Networks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;T81 558:Applications of Deep Neural Networks&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.wustl.edu&#34;&gt;Washington University in St. Louis&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Instructor: &lt;a href=&#34;https://sites.wustl.edu/jeffheaton/&#34;&gt;Jeff Heaton&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The content of this course changes as technology evolves&lt;/strong&gt;, to keep up to date with changes &lt;a href=&#34;https://github.com/jeffheaton&#34;&gt;follow me on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Section 1. Fall 2022, Monday, 2:30 PM, Location: TBD&lt;/li&gt; &#xA; &lt;li&gt;Section 2. Fall 2022, Online&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Course Description&lt;/h1&gt; &#xA;&lt;p&gt;Deep learning is a group of exciting new technologies for neural networks. Through a combination of advanced training techniques and neural network architectural components, it is now possible to create neural networks that can handle tabular data, images, text, and audio as both input and output. Deep learning allows a neural network to learn hierarchies of information in a way that is like the function of the human brain. This course will introduce the student to classic neural network structures, Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU), General Adversarial Networks (GAN) and reinforcement learning. Application of these architectures to computer vision, time series, security, natural language processing (NLP), and data generation will be covered. High Performance Computing (HPC) aspects will demonstrate how deep learning can be leveraged both on graphical processing units (GPUs), as well as grids. Focus is primarily upon the application of deep learning to problems, with some introduction to mathematical foundations. Students will use the Python programming language to implement deep learning using Google TensorFlow and Keras. It is not necessary to know Python prior to this course; however, familiarity of at least one programming language is assumed. This course will be delivered in a hybrid format that includes both classroom and online instruction.&lt;/p&gt; &#xA;&lt;h1&gt;Textbook&lt;/h1&gt; &#xA;&lt;p&gt;The complete text for this course is here on GitHub. This same material is also available in &lt;a href=&#34;https://www.heatonresearch.com/book/applications-deep-neural-networks-keras.html&#34;&gt;book format&lt;/a&gt;. The course textbook is “Applications of Deep Neural networks with Keras“, ISBN 9798416344269.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to cite the material from this course/book, please use the following BibTex citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{heaton2020applications,&#xA;    title={Applications of Deep Neural Networks},&#xA;    author={Jeff Heaton},&#xA;    year={2020},&#xA;    eprint={2009.05673},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Objectives&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Explain how neural networks (deep and otherwise) compare to other machine learning models.&lt;/li&gt; &#xA; &lt;li&gt;Determine when a deep neural network would be a good choice for a particular problem.&lt;/li&gt; &#xA; &lt;li&gt;Demonstrate your understanding of the material through a final project uploaded to GitHub.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Syllabus&lt;/h1&gt; &#xA;&lt;p&gt;This syllabus presents the expected class schedule, due dates, and reading assignments. &lt;a href=&#34;https://data.heatonresearch.com/wustl/jheaton-t81-558-spring-2022-syllabus.pdf&#34;&gt;Download current syllabus.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Module&lt;/th&gt; &#xA;   &lt;th&gt;Content&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_01_1_overview.ipynb&#34;&gt;Module 1&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 08/29/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 1: Python Preliminaries&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 1.1: Course Overview&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.2: Introduction to Python&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.3: Python Lists, Dictionaries, Sets &amp;amp; JSON&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.4: File Handling&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.5: Functions, Lambdas, and Map/ReducePython Preliminaries&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (first meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_02_1_python_pandas.ipynb&#34;&gt;Module 2&lt;/a&gt;&lt;br&gt;Week of 09/12/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 2: Python for Machine Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; Part 2.1: Introduction to Pandas for Deep Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.2: Encoding Categorical Values in Pandas&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.3: Grouping, Sorting, and Shuffling&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.4: Using Apply and Map in Pandas&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.5: Feature Engineering in Padas&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class1.ipynb&#34;&gt;Module 1 Program&lt;/a&gt; due: 09/13/2022&lt;/li&gt;&#xA;     &lt;li&gt; Icebreaker due: 09/13/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_03_1_neural_net.ipynb&#34;&gt;Module 3&lt;/a&gt;&lt;br&gt;Week of 09/19/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 3: TensorFlow and Keras for Neural Networks&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 3.1: Deep Learning and Neural Network Introduction&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.2: Introduction to Tensorflow &amp;amp; Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.3: Saving and Loading a Keras Neural Network&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.4: Early Stopping in Keras to Prevent Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.5: Extracting Keras Weights and Manual Neural Network Calculation&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class2.ipynb&#34;&gt;Module 2: Program&lt;/a&gt; due: 09/20/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_04_1_feature_encode.ipynb&#34;&gt;Module 4&lt;/a&gt;&lt;br&gt;Week of 09/26/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 4: Training for Tabular Data&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 4.1: Encoding a Feature Vector for Keras Deep Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.2: Keras Multiclass Classification for Deep Neural Networks with ROC and AUC&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.3: Keras Regression for Deep Neural Networks with RMSE&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Training&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.5: Neural Network RMSE and Log Loss Error Calculation from Scratch&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class3.ipynb&#34;&gt;Module 3 Program&lt;/a&gt; due: 09/27/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_05_1_reg_ridge_lasso.ipynb&#34;&gt;Module 5&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 10/03/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 5: Regularization and Dropout&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 5.1: Introduction to Regularization: Ridge and Lasso&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.2: Using K-Fold Cross Validation with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.3: Using L1 and L2 Regularization with Keras to Decrease Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.4: Drop Out for Keras to Decrease Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.5: Bootstrapping and Benchmarking Hyperparameters&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class4.ipynb&#34;&gt;Module 4 Program&lt;/a&gt; due: 10/04/2022&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (second meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_06_1_python_images.ipynb&#34;&gt;Module 6&lt;/a&gt;&lt;br&gt;Week of 10/17/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 6: CNN for Vision&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;      Part 6.1: Image Processing in Python&#xA;     &lt;li&gt;Part 6.2: Using Convolutional Networks with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.3: Using Pretrained Neural Networks&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.4: Looking at Keras Generators and Image Augmentation&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.5: Recognizing Multiple Images with YOLOv5&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class5.ipynb&#34;&gt;Module 5 Program&lt;/a&gt; due: 10/18/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_07_1_gan_intro.ipynb&#34;&gt;Module 7&lt;/a&gt;&lt;br&gt;Week of 10/24/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 7: Generative Adversarial Networks (GANs)&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 7.1: Introduction to GANS for Image and Data Generation&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.2: Train StyleGAN3 with your Own Images&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.3: Exploring the StyleGAN Latent Vector&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.4: GANS to Enhance Old Photographs Deoldify&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.5: GANs for Tabular Synthetic Data Generation&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class6.ipynb&#34;&gt;Module 6 Assignment&lt;/a&gt; due: 10/25/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_08_1_kaggle_intro.ipynb&#34;&gt;Module 8&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 10/31/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 8: Kaggle&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 8.1: Introduction to Kaggle&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.2: Building Ensembles with Scikit-Learn and Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.4: Bayesian Hyperparameter Optimization for Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.5: Current Semester&#39;s Kaggle&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class7.ipynb&#34;&gt;Module 7 Assignment&lt;/a&gt; due: 11/01/2022&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (third meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_09_1_keras_transfer.ipynb&#34;&gt;Module 9&lt;/a&gt;&lt;br&gt;Week of 11/07/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 9: Transfer Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 9.1: Introduction to Keras Transfer Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.2: Keras Transfer Learning for Computer Vision&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.3: Transfer Learning for NLP with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.4: Transfer Learning for Facial Feature Recognition&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.5: Transfer Learning for Style Transfer&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class8.ipynb&#34;&gt;Module 8 Assignment&lt;/a&gt; due: 11/08/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_10_1_timeseries.ipynb&#34;&gt;Module 10&lt;/a&gt;&lt;br&gt;Week of 11/14/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 10: Time Series in Keras&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 10.1: Time Series Data Encoding for Deep Learning, Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.2: Programming LSTM with Keras and&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.3: Text Generation with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.4: Introduction to Transformers&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.5: Transformers for Timeseries&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class9.ipynb&#34;&gt;Module 9 Assignment&lt;/a&gt; due: 11/15/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_11_01_huggingface.ipynb&#34;&gt;Module 11&lt;/a&gt;&lt;br&gt;Week of 11/21/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 11: Natural Language Processing&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 11.1: Hugging Face Introduction&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.2: Hugging Face Tokenizers&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.3: Hugging Face Data Sets&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.4: Training a Model in Hugging Face&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.5: What are Embedding Layers in Keras&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class10.ipynb&#34;&gt;Module 10 Assignment&lt;/a&gt; due: 11/22/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_12_01_ai_gym.ipynb&#34;&gt;Module 12&lt;/a&gt;&lt;br&gt;Week of 11/28/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 12: Reinforcement Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Kaggle Assignment due: 11/29/2022 (approx 4-6PM, due to Kaggle GMT timezone)&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.1: Introduction to the OpenAI Gym&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.2: Introduction to Q-Learning for Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.3: Keras Q-Learning in the OpenAI Gym&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.4: Atari Games with Keras Neural Networks&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.5: Application of Reinforcement Learning&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class11.ipynb&#34;&gt;Module 11 Assignment&lt;/a&gt; due: 11/29/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_13_01_flask.ipynb&#34;&gt;Module 13&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 12/05/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 13: Deployment and Monitoring&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 13.1: Flask and Deep Learning Web Services &lt;/li&gt;&#xA;     &lt;li&gt;Part 13.2: Interrupting and Continuing Training&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.3: Using a Keras Deep Neural Network with a Web Application&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.4: When to Retrain Your Neural Network&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.5: Tensor Processing Units (TPUs)&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (fourth meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class12.ipynb&#34;&gt;Module 12 Assignment&lt;/a&gt; due: 12/06/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Datasets&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://data.heatonresearch.com/data/t81-558/index.html&#34;&gt;Datasets can be downloaded here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/Data-Science-For-Beginners</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/microsoft/Data-Science-For-Beginners</id>
    <link href="https://github.com/microsoft/Data-Science-For-Beginners" rel="alternate"></link>
    <summary type="html">&lt;p&gt;10 Weeks, 20 Lessons, Data Science for All!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Data Science for Beginners - A Curriculum&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/Data-Science-For-Beginners/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/graphs/contributors/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/issues/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/pulls/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/watchers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Watch&#34; alt=&#34;GitHub watchers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Fork&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Star&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 10-week, 20-lesson curriculum all about Data Science. Each lesson includes pre-lesson and post-lesson quizzes, written instructions to complete the lesson, a solution, and an assignment. Our project-based pedagogy allows you to learn while building, a proven way for new skills to &#39;stick&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hearty thanks to our authors:&lt;/strong&gt; &lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine Greenaway&lt;/a&gt;, &lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry Soshnikov&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya Narasimhan&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/JalenMcG&#34;&gt;Jalen McGee&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen Looper&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud Levy&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany Souterre&lt;/a&gt;, &lt;a href=&#34;https://www.twitter.com/geektrainer&#34;&gt;Christopher Harrison&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🙏 Special thanks 🙏 to our &lt;a href=&#34;https://studentambassadors.microsoft.com/&#34;&gt;Microsoft Student Ambassador&lt;/a&gt; authors, reviewers and content contributors,&lt;/strong&gt; notably Aaryan Arora, &lt;a href=&#34;https://github.com/AdityaGarg00&#34;&gt;Aditya Garg&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/alondra-sanchez-molina/&#34;&gt;Alondra Sanchez&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/ankitasingh007&#34;&gt;Ankita Singh&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/anupam--mishra/&#34;&gt;Anupam Mishra&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/arpitadas01/&#34;&gt;Arpita Das&lt;/a&gt;, ChhailBihari Dubey, &lt;a href=&#34;https://www.linkedin.com/in/dibrinsofor&#34;&gt;Dibri Nsofor&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/dishita-bhasin-7065281bb&#34;&gt;Dishita Bhasin&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/majd-s/&#34;&gt;Majd Safi&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/max-blum-6036a1186/&#34;&gt;Max Blum&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/miguelmque/&#34;&gt;Miguel Correa&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/iftu119&#34;&gt;Mohamma Iftekher (Iftu) Ebne Jalal&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/nawrin-tabassum&#34;&gt;Nawrin Tabassum&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/raymond-wp/&#34;&gt;Raymond Wangsa Putra&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/rty2423&#34;&gt;Rohit Yadav&lt;/a&gt;, Samridhi Sharma, &lt;a href=&#34;https://www.linkedin.com/mwlite/in/sanya-sinha-13aab1200&#34;&gt;Sanya Sinha&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/sheena-narua-n/&#34;&gt;Sheena Narula&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/tauqeerahmad5201/&#34;&gt;Tauqeer Ahmad&lt;/a&gt;, Yogendrasingh Pawar , &lt;a href=&#34;https://www.linkedin.com/in/vidushi-gupta07/&#34;&gt;Vidushi Gupta&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jasleen-sondhi/&#34;&gt;Jasleen Sondhi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Title.png&#34; alt=&#34; Sketchnote by (@sketchthedocs) &#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science For Beginners - &lt;em&gt;Sketchnote by &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;: we have &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/for-teachers.md&#34;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum. We&#39;d love your feedback &lt;a href=&#34;https://github.com/microsoft/Data-Science-For-Beginners/discussions&#34;&gt;in our discussion forum&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://aka.ms/student-page&#34;&gt;Students&lt;/a&gt;&lt;/strong&gt;: to use this curriculum on your own, fork the entire repo and complete the exercises on your own, starting with a pre-lecture quiz. Then read the lecture and complete the rest of the activities. Try to create the projects by comprehending the lessons rather than copying the solution code; however, that code is available in the /solutions folders in each project-oriented lesson. Another idea would be to form a study group with friends and go through the content together. For further study, we recommend &lt;a href=&#34;https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/qprpajyoy3x0g7?WT.mc_id=academic-40229-cxa&#34;&gt;Microsoft Learn&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Meet the Team&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/8mzavjQSMM4&#34; title=&#34;Promo video&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/ds-for-beginners.gif&#34; alt=&#34;Promo video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gif by&lt;/strong&gt; &lt;a href=&#34;https://www.linkedin.com/in/mohitjaisal&#34;&gt;Mohit Jaisal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🎥 Click the image above for a video about the project the folks who created it!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Pedagogy&lt;/h2&gt; &#xA;&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is project-based and that it includes frequent quizzes. By the end of this series, students will have learned basic principles of data science, including ethical concepts, data preparation, different ways of working with data, data visualization, data analysis, real-world use cases of data science, and more.&lt;/p&gt; &#xA;&lt;p&gt;In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 10 week cycle.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Find our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&#34;&gt;Translation&lt;/a&gt; guidelines. We welcome your constructive feedback!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Each lesson includes:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optional sketchnote&lt;/li&gt; &#xA; &lt;li&gt;Optional supplemental video&lt;/li&gt; &#xA; &lt;li&gt;Pre-lesson warmup quiz&lt;/li&gt; &#xA; &lt;li&gt;Written lesson&lt;/li&gt; &#xA; &lt;li&gt;For project-based lessons, step-by-step guides on how to build the project&lt;/li&gt; &#xA; &lt;li&gt;Knowledge checks&lt;/li&gt; &#xA; &lt;li&gt;A challenge&lt;/li&gt; &#xA; &lt;li&gt;Supplemental reading&lt;/li&gt; &#xA; &lt;li&gt;Assignment&lt;/li&gt; &#xA; &lt;li&gt;Post-lesson quiz&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained &lt;a href=&#34;https://red-water-0103e7a0f.azurestaticapps.net/&#34;&gt;in this app&lt;/a&gt;, for 40 total quizzes of three questions each. They are linked from within the lessons, but the quiz app can be run locally; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder. They are gradually being localized.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Lessons&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Roadmap.png&#34; alt=&#34; Sketchnote by (@sketchthedocs) &#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science For Beginners: Roadmap - &lt;em&gt;Sketchnote by &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Number&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Topic&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Grouping&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Learning Objectives&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Linked Lesson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Author&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Defining Data Science&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Learn the basic concepts behind data science and how it’s related to artificial intelligence, machine learning, and big data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/01-defining-data-science/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/beZ7Mb_oz9I&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science Ethics&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Ethics Concepts, Challenges &amp;amp; Frameworks.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/02-ethics/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Defining Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;How data is classified and its common sources.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/03-defining-data/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to Statistics &amp;amp; Probability&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The mathematical techniques of probability and statistics to understand data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/04-stats-and-probability/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/Z5Zy85g4Yjw&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with Relational Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to relational data and the basics of exploring and analyzing relational data with the Structured Query Language, also known as SQL (pronounced “see-quell”).&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/05-relational-databases/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/geektrainer&#34;&gt;Christopher&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with NoSQL Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to non-relational data, its various types and the basics of exploring and analyzing document databases.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/06-non-relational/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with Python&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Basics of using Python for data exploration with libraries such as Pandas. Foundational understanding of Python programming is recommended.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/07-python/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/dZjWOGbsN4Y&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Preparation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Topics on data techniques for cleaning and transforming the data to handle challenges of missing, inaccurate, or incomplete data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/08-data-preparation/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Quantities&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Learn how to use Matplotlib to visualize bird data 🦆&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/09-visualization-quantities/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Distributions of Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing observations and trends within an interval.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/10-visualization-distributions/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Proportions&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing discrete and grouped percentages.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/11-visualization-proportions/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Relationships&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing connections and correlations between sets of data and their variables.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/12-visualization-relationships/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meaningful Visualizations&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Techniques and guidance for making your visualizations valuable for effective problem solving and insights.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/13-meaningful-visualizations/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to the Data Science lifecycle&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to the data science lifecycle and its first step of acquiring and extracting data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/14-Introduction/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Analyzing&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This phase of the data science lifecycle focuses on techniques to analyze data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/15-analyzing/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Communication&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This phase of the data science lifecycle focuses on presenting the insights from the data in a way that makes it easier for decision makers to understand.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/16-communication/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/JalenMcG&#34;&gt;Jalen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This series of lessons introduces data science in the cloud and its benefits.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/17-Introduction/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Training models using Low Code tools.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/18-Low-Code/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Deploying models with Azure Machine Learning Studio.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/19-Azure/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Wild&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/README.md&#34;&gt;In the Wild&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data science driven projects in the real world.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/20-Real-World-Examples/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Offline access&lt;/h2&gt; &#xA;&lt;p&gt;You can run this documentation offline by using &lt;a href=&#34;https://docsify.js.org/#/&#34;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&#34;https://docsify.js.org/#/quickstart&#34;&gt;install Docsify&lt;/a&gt; on your local machine, then in the root folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note, notebooks will not be rendered via Docsify, so when you need to run a notebook, do that separately in VS Code running a Python kernel.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;PDF&lt;/h2&gt; &#xA;&lt;p&gt;A PDF of all of the lessons can be found &lt;a href=&#34;https://microsoft.github.io/Data-Science-For-Beginners/pdf/readme.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Help Wanted!&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to translate all or part of the curriculum, please follow our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&#34;&gt;Translations&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;h2&gt;Other Curricula&lt;/h2&gt; &#xA;&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ml-beginners&#34;&gt;Machine Learning for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/iot-beginners&#34;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/webdev-beginners&#34;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-beginners&#34;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>wesm/pydata-book</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/wesm/pydata-book</id>
    <link href="https://github.com/wesm/pydata-book" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Materials and IPython notebooks for &#34;Python for Data Analysis&#34; by Wes McKinney, published by O&#39;Reilly Media&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python for Data Analysis, 2nd Edition&lt;/h1&gt; &#xA;&lt;p&gt;Materials and IPython notebooks for &#34;Python for Data Analysis&#34; by Wes McKinney, published by O&#39;Reilly Media&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://amzn.to/2vvBijB&#34;&gt;Buy the book on Amazon&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://notebooks.azure.com/import/gh/wesm/pydata-book&#34;&gt;&lt;img src=&#34;https://notebooks.azure.com/launch.png&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Follow Wes on Twitter: &lt;a href=&#34;https://twitter.com/wesmckinn&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/wesmckinn.svg?style=social&amp;amp;label=Follow&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;1st Edition Readers&lt;/h1&gt; &#xA;&lt;p&gt;If you are reading the &lt;a href=&#34;http://amzn.to/2vvBijB&#34;&gt;1st Edition&lt;/a&gt; (published in 2012), please find the reorganized book materials on the &lt;a href=&#34;https://github.com/wesm/pydata-book/tree/1st-edition&#34;&gt;&lt;code&gt;1st-edition&lt;/code&gt; branch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Translations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BrambleXu/pydata-notebook&#34;&gt;Chinese&lt;/a&gt; by Xu Liang&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mbiesiad/pydata-book/tree/pl_PL&#34;&gt;Polish&lt;/a&gt; by Michal Biesiada&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;IPython Notebooks:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch02.ipynb&#34;&gt;Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch03.ipynb&#34;&gt;Chapter 3: Built-in Data Structures, Functions, and Files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch04.ipynb&#34;&gt;Chapter 4: NumPy Basics: Arrays and Vectorized Computation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch05.ipynb&#34;&gt;Chapter 5: Getting Started with pandas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch06.ipynb&#34;&gt;Chapter 6: Data Loading, Storage, and File Formats&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch07.ipynb&#34;&gt;Chapter 7: Data Cleaning and Preparation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch08.ipynb&#34;&gt;Chapter 8: Data Wrangling: Join, Combine, and Reshape&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch09.ipynb&#34;&gt;Chapter 9: Plotting and Visualization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch10.ipynb&#34;&gt;Chapter 10: Data Aggregation and Group Operations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch11.ipynb&#34;&gt;Chapter 11: Time Series&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch12.ipynb&#34;&gt;Chapter 12: Advanced pandas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch13.ipynb&#34;&gt;Chapter 13: Introduction to Modeling Libraries in Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch14.ipynb&#34;&gt;Chapter 14: Data Analysis Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/appa.ipynb&#34;&gt;Appendix A: Advanced NumPy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;h3&gt;Code&lt;/h3&gt; &#xA;&lt;p&gt;The code in this repository, including all code samples in the notebooks listed above, is released under the &lt;a href=&#34;https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/LICENSE-CODE&#34;&gt;MIT license&lt;/a&gt;. Read more at the &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>fastai/fastbook</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/fastai/fastbook</id>
    <link href="https://github.com/fastai/fastbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The fastai book, published as Jupyter Notebooks&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README.md&#34;&gt;English&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_es.md&#34;&gt;Spanish&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_ko.md&#34;&gt;Korean&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_zh.md&#34;&gt;Chinese&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_bn.md&#34;&gt;Bengali&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_id.md&#34;&gt;Indonesian&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_it.md&#34;&gt;Italian&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_pt.md&#34;&gt;Portuguese&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_vn.md&#34;&gt;Vietnamese&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;The fastai book&lt;/h1&gt; &#xA;&lt;p&gt;These notebooks cover an introduction to deep learning, &lt;a href=&#34;https://docs.fast.ai/&#34;&gt;fastai&lt;/a&gt;, and &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;. fastai is a layered API for deep learning; for more information, see &lt;a href=&#34;https://www.mdpi.com/2078-2489/11/2/108&#34;&gt;the fastai paper&lt;/a&gt;. Everything in this repo is copyright Jeremy Howard and Sylvain Gugger, 2020 onwards.&lt;/p&gt; &#xA;&lt;p&gt;These notebooks are used for &lt;a href=&#34;https://course.fast.ai&#34;&gt;a MOOC&lt;/a&gt; and form the basis of &lt;a href=&#34;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&#34;&gt;this book&lt;/a&gt;, which is currently available for purchase. It does not have the same GPL restrictions that are on this repository.&lt;/p&gt; &#xA;&lt;p&gt;The code in the notebooks and python &lt;code&gt;.py&lt;/code&gt; files is covered by the GPL v3 license; see the LICENSE file for details. The remainder (including all markdown cells in the notebooks and other prose) is not licensed for any redistribution or change of format or medium, other than making copies of the notebooks or forking this repo for your own private use. No commercial or broadcast use is allowed. We are making these materials freely available to help you learn deep learning, so please respect our copyright and these restrictions.&lt;/p&gt; &#xA;&lt;p&gt;If you see someone hosting a copy of these materials somewhere else, please let them know that their actions are not allowed and may lead to legal action. Moreover, they would be hurting the community because we&#39;re not likely to release additional materials in this way if people ignore our copyright.&lt;/p&gt; &#xA;&lt;h2&gt;Colab&lt;/h2&gt; &#xA;&lt;p&gt;Instead of cloning this repo and opening it on your machine, you can read and work with the notebooks using &lt;a href=&#34;https://research.google.com/colaboratory/&#34;&gt;Google Colab&lt;/a&gt;. This is the recommended approach for folks who are just getting started -- there&#39;s no need to set up a Python development environment on your own machine, since you can just work directly in your web-browser.&lt;/p&gt; &#xA;&lt;p&gt;You can open any chapter of the book in Colab by clicking on one of these links: &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/app_jupyter.ipynb&#34;&gt;Introduction to Jupyter&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/01_intro.ipynb&#34;&gt;Chapter 1, Intro&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/02_production.ipynb&#34;&gt;Chapter 2, Production&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/03_ethics.ipynb&#34;&gt;Chapter 3, Ethics&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Chapter 4, MNIST Basics&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb&#34;&gt;Chapter 5, Pet Breeds&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/06_multicat.ipynb&#34;&gt;Chapter 6, Multi-Category&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/07_sizing_and_tta.ipynb&#34;&gt;Chapter 7, Sizing and TTA&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/08_collab.ipynb&#34;&gt;Chapter 8, Collab&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/09_tabular.ipynb&#34;&gt;Chapter 9, Tabular&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/10_nlp.ipynb&#34;&gt;Chapter 10, NLP&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/11_midlevel_data.ipynb&#34;&gt;Chapter 11, Mid-Level API&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/12_nlp_dive.ipynb&#34;&gt;Chapter 12, NLP Deep-Dive&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/13_convolutions.ipynb&#34;&gt;Chapter 13, Convolutions&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/14_resnet.ipynb&#34;&gt;Chapter 14, Resnet&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/15_arch_details.ipynb&#34;&gt;Chapter 15, Arch Details&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/16_accel_sgd.ipynb&#34;&gt;Chapter 16, Optimizers and Callbacks&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/17_foundations.ipynb&#34;&gt;Chapter 17, Foundations&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/18_CAM.ipynb&#34;&gt;Chapter 18, GradCAM&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/19_learner.ipynb&#34;&gt;Chapter 19, Learner&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/20_conclusion.ipynb&#34;&gt;Chapter 20, conclusion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;If you make any pull requests to this repo, then you are assigning copyright of that work to Jeremy Howard and Sylvain Gugger. (Additionally, if you are making small edits to spelling or text, please specify the name of the file and a very brief description of what you&#39;re fixing. It&#39;s difficult for reviewers to know which corrections have already been made. Thank you.)&lt;/p&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you wish to cite the book, you may use the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@book{howard2020deep,&#xA;title={Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD},&#xA;author={Howard, J. and Gugger, S.},&#xA;isbn={9781492045526},&#xA;url={https://books.google.no/books?id=xd6LxgEACAAJ},&#xA;year={2020},&#xA;publisher={O&#39;Reilly Media, Incorporated}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>tensorflow/docs</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/tensorflow/docs</id>
    <link href="https://github.com/tensorflow/docs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TensorFlow documentation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TensorFlow Documentation&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://www.tensorflow.org/images/tf_logo_horizontal.png&#34;&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;These are the source files for the guide and tutorials on &lt;a href=&#34;https://www.tensorflow.org/overview&#34;&gt;tensorflow.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To contribute to the TensorFlow documentation, please read &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/docs/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;, the &lt;a href=&#34;https://www.tensorflow.org/community/contribute/docs&#34;&gt;TensorFlow docs contributor guide&lt;/a&gt;, and the &lt;a href=&#34;https://www.tensorflow.org/community/contribute/docs_style&#34;&gt;style guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To file a docs issue, use the issue tracker in the &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md&#34;&gt;tensorflow/tensorflow&lt;/a&gt; repo.&lt;/p&gt; &#xA;&lt;p&gt;And join the TensorFlow documentation contributors on the &lt;a href=&#34;https://groups.google.com/a/tensorflow.org/forum/#!forum/docs&#34;&gt;docs@tensorflow.org mailing list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community translations&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/community/contribute/docs#community_translations&#34;&gt;Community translations&lt;/a&gt; are located in the &lt;a href=&#34;https://github.com/tensorflow/docs-l10n&#34;&gt;tensorflow/docs-l10n&lt;/a&gt; repo. These docs are contributed, reviewed, and maintained by the community as &lt;em&gt;best-effort&lt;/em&gt;. To participate as a translator or reviewer, see the &lt;code&gt;site/&amp;lt;lang&amp;gt;/README.md&lt;/code&gt;, join the language mailing list, and submit a pull request.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/docs/master/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bnsreenu/python_for_microscopists</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/bnsreenu/python_for_microscopists</id>
    <link href="https://github.com/bnsreenu/python_for_microscopists" rel="alternate"></link>
    <summary type="html">&lt;p&gt;https://www.youtube.com/channel/UC34rW-HtPJulxr5wp2Xa04w?sub_confirmation=1&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;b&gt; Author: Dr. Sreenivas Bhattiprolu &lt;/b&gt;&lt;br&gt; &lt;b&gt;Twitter: @digitalsreeni &lt;/b&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Python for Microscopists and other image processing enthusiasts&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC34rW-HtPJulxr5wp2Xa04w?sub_confirmation=1&#34;&gt;https://www.youtube.com/channel/UC34rW-HtPJulxr5wp2Xa04w?sub_confirmation=1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The YouTube channel associated with this code walks you through the entire process of learning to code in Python; all the way from basics to advanced machine learning and deep learning. The primary emphasis will be on image processing and other relevant functionality.&lt;/p&gt; &#xA;&lt;p&gt;Why did I create this channel? To help you (students and researchers) gain a new skill and succeed in your respective fields.&lt;/p&gt; &#xA;&lt;p&gt;You may think coding is hard and that it&#39;s not your cup of tea, but Python made it easy to code even advanced algorithms. In addition, coding will make you self sufficient, it will teach you how to think, it improves your collaborative skills and it can take your career to new heights. Therefore, if you want to stay ahead of your peers and relevant in your field, overcome your fears and start coding!&lt;/p&gt; &#xA;&lt;p&gt;Also, checkout WWW.APEER.COM if you want free image processing in the cloud! Free for non-profits / academics / personal use.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>harvardnlp/annotated-transformer</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/harvardnlp/annotated-transformer</id>
    <link href="https://github.com/harvardnlp/annotated-transformer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An annotated implementation of the Transformer paper.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Code for The Annotated Transformer blog post:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/annotated-transformer/&#34;&gt;http://nlp.seas.harvard.edu/annotated-transformer/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/harvardnlp/annotated-transformer/blob/master/AnnotatedTransformer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/35882/166251887-9da909a9-660b-45a9-ae72-0aae89fb38d4.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Package Dependencies&lt;/h1&gt; &#xA;&lt;p&gt;Use &lt;code&gt;requirements.txt&lt;/code&gt; to install library dependencies with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Notebook Setup&lt;/h1&gt; &#xA;&lt;p&gt;The Annotated Transformer is created using &lt;a href=&#34;https://github.com/mwouts/jupytext&#34;&gt;jupytext&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Regular notebooks pose problems for source control - cell outputs end up in the repo history and diffs between commits are difficult to examine. Using jupytext, there is a python script (&lt;code&gt;.py&lt;/code&gt; file) that is automatically kept in sync with the notebook file by the jupytext plugin.&lt;/p&gt; &#xA;&lt;p&gt;The python script is committed contains all the cell content and can be used to generate the notebook file. The python script is a regular python source file, markdown sections are included using a standard comment convention, and outputs are not saved. The notebook itself is treated as a build artifact and is not commited to the git repository.&lt;/p&gt; &#xA;&lt;p&gt;Prior to using this repo, make sure jupytext is installed by following the &lt;a href=&#34;https://github.com/mwouts/jupytext/raw/main/docs/install.md&#34;&gt;installation instructions here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To produce the &lt;code&gt;.ipynb&lt;/code&gt; notebook file using the markdown source, run (under the hood, the &lt;code&gt;notebook&lt;/code&gt; build target simply runs &lt;code&gt;jupytext --to ipynb the_annotated_transformer.py&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To produce the html version of the notebook, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;make html&lt;/code&gt; is just a shortcut for for generating the notebook with &lt;code&gt;jupytext --to ipynb the_annotated_transformer.py&lt;/code&gt; followed by using the jupyter nbconvert command to produce html using &lt;code&gt;jupyter nbconvert --to html the_annotated_transformer.ipynb&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Formatting and Linting&lt;/h1&gt; &#xA;&lt;p&gt;To keep the code formatting clean, the annotated transformer git repo has a git action to check that the code conforms to &lt;a href=&#34;https://www.python.org/dev/peps/pep-0008/&#34;&gt;PEP8 coding standards&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To make this easier, there are two &lt;code&gt;Makefile&lt;/code&gt; build targets to run automatic code formatting with black and flake8.&lt;/p&gt; &#xA;&lt;p&gt;Be sure to &lt;a href=&#34;https://github.com/psf/black#installation&#34;&gt;install black&lt;/a&gt; and &lt;a href=&#34;https://flake8.pycqa.org/en/latest/&#34;&gt;flake8&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make black&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(or alternatively manually call black &lt;code&gt;black --line-length 79 the_annotated_transformer.py&lt;/code&gt;) to format code automatically using black and:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make flake&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(or manually call flake8 `flake8 --show-source the_annotated_transformer.py) to check for PEP8 violations.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s recommended to run these two commands and fix any flake8 errors that arise, when submitting a PR, otherwise the github actions CI will report an error.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure/Azure-Sentinel</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/Azure/Azure-Sentinel</id>
    <link href="https://github.com/Azure/Azure-Sentinel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cloud-native SIEM for intelligent security analytics for your entire enterprise.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Microsoft Sentinel and Microsoft 365 Defender&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the unified Microsoft Sentinel and Microsoft 365 Defender repository! This repository contains out of the box detections, exploration queries, hunting queries, workbooks, playbooks and much more to help you get ramped up with Microsoft Sentinel and provide you security content to secure your environment and hunt for threats. The hunting queries also include Microsoft 365 Defender hunting queries for advanced hunting scenarios in both Microsoft 365 Defender and Microsoft Sentinel. You can also submit to &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues&#34;&gt;issues&lt;/a&gt; for any samples or resources you would like to see here as you onboard to Microsoft Sentinel. This repository welcomes contributions and refer to this repository&#39;s &lt;a href=&#34;https://aka.ms/threathunters&#34;&gt;wiki&lt;/a&gt; to get started. For questions and feedback, please contact &lt;a href=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/AzureSentinel@microsoft.com&#34;&gt;AzureSentinel@microsoft.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Resources&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://go.microsoft.com/fwlink/?linkid=2073774&amp;amp;clcid=0x409&#34;&gt;Microsoft Sentinel documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/microsoft-365/security/defender/microsoft-365-defender?view=o365-worldwide&#34;&gt;Microsoft 365 Defender documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/securitywebinars&#34;&gt;Security Community Webinars&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.github.com/en#dotcom&#34;&gt;Getting started with GitHub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We value your feedback. Here are some channels to help surface your questions or feedback:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;General product specific Q&amp;amp;A for SIEM and SOAR - Join in the &lt;a href=&#34;https://techcommunity.microsoft.com/t5/microsoft-sentinel/bd-p/MicrosoftSentinel&#34;&gt;Microsoft Sentinel Tech Community conversations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;General product specific Q&amp;amp;A for XDR - Join in the &lt;a href=&#34;https://techcommunity.microsoft.com/t5/microsoft-365-defender/bd-p/MicrosoftThreatProtection&#34;&gt;Microsoft 365 Defender Tech Community conversations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Product specific feature requests - Upvote or post new on &lt;a href=&#34;https://feedback.azure.com/d365community/forum/37638d17-0625-ec11-b6e6-000d3a4f07b8&#34;&gt;Microsoft Sentinel feedback forums&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Report product or contribution bugs - File a GitHub Issue using &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;amp;labels=&amp;amp;template=bug_report.md&amp;amp;title=&#34;&gt;Bug template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;General feedback on community and contribution process - File a GitHub Issue using &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;amp;labels=&amp;amp;template=feature_request.md&amp;amp;title=&#34;&gt;Feature Request template&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.microsoft.com&#34;&gt;https://cla.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Add in your new or updated contributions to GitHub&lt;/h2&gt; &#xA;&lt;p&gt;Note: If you are a first time contributor to this repository, &lt;a href=&#34;https://docs.github.com/github/getting-started-with-github/fork-a-repo&#34;&gt;General GitHub Fork the repo guidance&lt;/a&gt; before cloning or &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/raw/master/GettingStarted.md&#34;&gt;Specific steps for the Sentinel repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;General Steps&lt;/h2&gt; &#xA;&lt;p&gt;Brand new or update to a contribution via these methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Submit for review directly on GitHub website &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Browse to the folder you want to upload your file to&lt;/li&gt; &#xA;   &lt;li&gt;Choose Upload Files and browse to your file.&lt;/li&gt; &#xA;   &lt;li&gt;You will be required to create your own branch and then submit the Pull Request for review.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://help.github.com/en/desktop/getting-started-with-github-desktop&#34;&gt;GitHub Desktop&lt;/a&gt; or &lt;a href=&#34;https://visualstudio.microsoft.com/vs/&#34;&gt;Visual Studio&lt;/a&gt; or &lt;a href=&#34;https://code.visualstudio.com/?wt.mc_id=DX_841432&#34;&gt;VSCode&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.github.com/github/getting-started-with-github/fork-a-repo&#34;&gt;Fork the repo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository&#34;&gt;Clone the repo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/desktop/contributing-to-projects/creating-a-branch-for-your-work&#34;&gt;Create your own branch&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Do your additions/updates in GitHub Desktop&lt;/li&gt; &#xA;   &lt;li&gt;Be sure to merge master back to your branch before you push.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/using-git/pushing-commits-to-a-remote-repository&#34;&gt;Push your changes to GitHub&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pull Request&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;After you push your changes, you will need to submit the &lt;a href=&#34;https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests&#34;&gt;Pull Request (PR)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Details about the Proposed Changes are required, be sure to include a minimal level of detail so a review can clearly understand the reason for the change and what he change is related to in the code.&lt;/li&gt; &#xA; &lt;li&gt;After submission, check the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/pulls&#34;&gt;Pull Request&lt;/a&gt; for comments&lt;/li&gt; &#xA; &lt;li&gt;Make changes as suggested and update your branch or explain why no change is needed. Resolve the comment when done.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pull Request Detection Template Structure Validation Check&lt;/h3&gt; &#xA;&lt;p&gt;As part of the PR checks we run a structure validation to make sure all required parts of the YAML structure are included. For Detections, there is a new section that must be included. See the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/wiki/Contribute-to-Sentinel-GitHub-Community-of-Queries#now-onto-the-how&#34;&gt;contribution guidelines&lt;/a&gt; for more information. If this section or any other required section is not included, then a validation error will occur similar to the below. The example is specifically if the YAML is missing the entityMappings section:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;A total of 1 test files matched the specified pattern.&#xA;[xUnit.net 00:00:00.95]     Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [FAIL]&#xA;  X Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [104ms]&#xA;  Error Message:&#xA;   Expected object to be &amp;lt;null&amp;gt;, but found System.ComponentModel.DataAnnotations.ValidationException with message &#34;An old mapping for entity &#39;AccountCustomEntity&#39; does not have a matching new mapping entry.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pull Request Kql Validation Check&lt;/h3&gt; &#xA;&lt;p&gt;As part of the PR checks we run a syntax validation of the kql queries defined in the template. If this check fails go to Azure Pipeline (by pressing on the errors link on the checks tab in your PR) &lt;img src=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/.github/Media/Azurepipeline.png&#34; alt=&#34;Azurepipeline&#34;&gt; In the pipeline you can see which test failed and what is the cause: &lt;img src=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/.github/Media/PipelineTestsTab.png&#34; alt=&#34;Pipeline Tests Tab&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Example error message:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;A total of 1 test files matched the specified pattern.&#xA;[xUnit.net 00:00:01.81]     Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [FAIL]&#xA;  X Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [21ms]&#xA;  Error Message:&#xA;   Template Id:fa0ab69c-7124-4f62-acdd-61017cf6ce89 is not valid Errors:The name &#39;SymantecEndpointProtection&#39; does not refer to any known table, tabular variable or function., Code: &#39;KS204&#39;, Severity: &#39;Error&#39;, Location: &#39;67..93&#39;,The name &#39;SymantecEndpointProtection&#39; does not refer to any known table, tabular variable or function., Code: &#39;KS204&#39;, Severity: &#39;Error&#39;, Location: &#39;289..315&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using custom logs table (a table which is not defined on all workspaces by default) you should verify your table schema is defined in json file in the folder &lt;em&gt;Azure-Sentinel\.script\tests\KqlvalidationsTests\CustomTables&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example for table tablexyz.json&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;Name&#34;: &#34;tablexyz&#34;,&#xA;  &#34;Properties&#34;: [&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeDateTimeColumn&#34;,&#xA;      &#34;Type&#34;: &#34;DateTime&#34;&#xA;    },&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeStringColumn&#34;,&#xA;      &#34;Type&#34;: &#34;String&#34;&#xA;    },&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeDynamicColumn&#34;,&#xA;      &#34;Type&#34;: &#34;Dynamic&#34;&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Kql Validation Locally&lt;/h3&gt; &#xA;&lt;p&gt;In order to run the kql validation before submitting Pull Request in you local machine:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need to have &lt;strong&gt;.Net Core 3.1 SDK&lt;/strong&gt; installed &lt;a href=&#34;https://dotnet.microsoft.com/download&#34;&gt;How to download .Net&lt;/a&gt; (Supports all platforms)&lt;/li&gt; &#xA; &lt;li&gt;Open Shell and navigate to &lt;code&gt;Azure-Sentinel\\.script\tests\KqlvalidationsTests\&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;dotnet test&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of output (in Ubuntu):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Welcome to .NET Core 3.1!&#xA;---------------------&#xA;SDK Version: 3.1.403&#xA;&#xA;Telemetry&#xA;---------&#xA;The .NET Core tools collect usage data in order to help us improve your experience. The data is anonymous. It is collected by Microsoft and shared with the community. You can opt-out of telemetry by setting the DOTNET_CLI_TELEMETRY_OPTOUT environment variable to &#39;1&#39; or &#39;true&#39; using your favorite shell.&#xA;&#xA;Read more about .NET Core CLI Tools telemetry: https://aka.ms/dotnet-cli-telemetry&#xA;&#xA;----------------&#xA;Explore documentation: https://aka.ms/dotnet-docs&#xA;Report issues and find source on GitHub: https://github.com/dotnet/core&#xA;Find out what&#39;s new: https://aka.ms/dotnet-whats-new&#xA;Learn about the installed HTTPS developer cert: https://aka.ms/aspnet-core-https&#xA;Use &#39;dotnet --help&#39; to see available commands or visit: https://aka.ms/dotnet-cli-docs&#xA;Write your first app: https://aka.ms/first-net-core-app&#xA;--------------------------------------------------------------------------------------&#xA;Test run for /mnt/c/git/Azure-Sentinel/.script/tests/KqlvalidationsTests/bin/Debug/netcoreapp3.1/Kqlvalidations.Tests.dll(.NETCoreApp,Version=v3.1)&#xA;Microsoft (R) Test Execution Command Line Tool Version 16.7.0&#xA;Copyright (c) Microsoft Corporation.  All rights reserved.&#xA;&#xA;Starting test execution, please wait...&#xA;&#xA;A total of 1 test files matched the specified pattern.&#xA;&#xA;Test Run Successful.&#xA;Total tests: 171&#xA;     Passed: 171&#xA; Total time: 25.7973 Seconds&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Detection schema validation tests&lt;/h3&gt; &#xA;&lt;p&gt;Similarly to KQL Validation, there is an automatic validation of the schema of a detection. The schema validation includes the detection&#39;s frequency and period, the detection&#39;s trigger type and threshold, validity of connectors Ids (&lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/raw/master/.script/tests/detectionTemplateSchemaValidation/ValidConnectorIds.json&#34;&gt;valid connectors Ids list&lt;/a&gt;), etc. A wrong format or missing attributes will result with an informative check failure, which should guide you through the resolution of the issue, but make sure to look into the format of already approved detection.&lt;/p&gt; &#xA;&lt;h3&gt;Run Detection Schema Validation Locally&lt;/h3&gt; &#xA;&lt;p&gt;In order to run the kql validation before submitting Pull Request in you local machine:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need to have &lt;strong&gt;.Net Core 3.1 SDK&lt;/strong&gt; installed &lt;a href=&#34;https://dotnet.microsoft.com/download&#34;&gt;How to download .Net&lt;/a&gt; (Supports all platforms)&lt;/li&gt; &#xA; &lt;li&gt;Open Shell and navigate to &lt;code&gt;Azure-Sentinel\\.script\tests\DetectionTemplateSchemaValidation\&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;dotnet test&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;p&gt;For information on what you can contribute and further details, refer to the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/wiki#get-started&#34;&gt;&#34;get started&#34;&lt;/a&gt; section on the project&#39;s &lt;a href=&#34;https://aka.ms/threathunters&#34;&gt;wiki&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google-research/google-research</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/google-research/google-research</id>
    <link href="https://github.com/google-research/google-research" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Google Research&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google Research&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains code released by &lt;a href=&#34;https://research.google&#34;&gt;Google Research&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All datasets in this repository are released under the CC BY 4.0 International license, which can be found here: &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;. All source files in this repository are released under the Apache 2.0 license, the text of which can be found in the LICENSE file.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Because the repo is large, we recommend you download only the subdirectory of interest:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;SUBDIR=foo&#xA;svn export https://github.com/google-research/google-research/trunk/$SUBDIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;d like to submit a pull request, you&#39;ll need to clone the repository; we recommend making a shallow clone (without history).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:google-research/google-research.git --depth=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;Disclaimer: This is not an official Google product.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lixin4ever/Conference-Acceptance-Rate</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/lixin4ever/Conference-Acceptance-Rate</id>
    <link href="https://github.com/lixin4ever/Conference-Acceptance-Rate" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Acceptance rates for the major AI conferences&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Statistics of acceptance rate for the main AI conferences&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lixin4ever/Conference-Acceptance-Rate/master/conference_trends.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Natural Language Processing and Computational Linguistics&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Conference &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Long Paper &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Short Paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.2% (146/572)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.1% (139/551)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.0% (173/692)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.4% (145/648)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.0% (231/825)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.0% (97/463)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.0% (195/751)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.9% (107/567)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.3% (258/1018)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.0% (126/526)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.7% (447/1737)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.2% (213/1168)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.4% (571/2244)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.6% (208/1185)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.5% (571/2327)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13.6% (139/1023)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;21 Findings&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.6% (339/2327)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.5% (118/1023)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.4% (155/510)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27.8% (70/252)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.2% (157/600)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.1% (155/700)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.8% (177/687)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.8% (87/400)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.8% (216/836)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.4% (107/582)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.5% (351/1376)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.2% (198/855)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.6% (465/1813)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.5% (218/1063)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.5% (602/2455)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.6% (150/904)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;20 Findings&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13.5% (332/2455)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;12.7% (115/904)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.6% (650/2540)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.9% (190/1060)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;21 Findings&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.8% (300/2540)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.2% (119/1060)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NAACL-HLT&#39;13&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.0% (88/293)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.1% (51/162)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NAACL-HLT&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.1% (117/402)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.1% (69/312)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NAACL-HLT&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.3% (100/396)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.9% (82/284)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NAACL-HLT&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.0% (207/647)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.4% (125/425)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NAACL-HLT&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.3% (281/1067)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.3% (142/666)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NAACL-HLT&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.2% (366/1254)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.6% (123/544)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLING&#39;12&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27% (311/1000+)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLING&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.8% (217/705)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLING&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.4% (337/1039)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLING&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;37.4% (332/888)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLING&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;33.4% (622/1862)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Computer Vision and Pattern Recognition&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Conference &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Long Paper &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Short Paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.9% (540/1807) (104 orals and 436 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.3% (602/2123) (71 orals and 531 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.9% (643/2145) (83 orals, 123 spotlights and 437 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.9% (783/2620) (71 orals, 144 spotlights and 568 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.6% (979/3303) (70 orals, 224 spotlights and 685 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.0% (1294/5160) (288 short orals and 1294 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.1% (1470/6656)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.7% (1661/7015) (295 orals and 1366 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.3% (2067/8161)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICCV&#39;13&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27.9% (454/1629) (41 orals and 413 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICCV&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.9% (525/1698)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICCV&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.0% (621/2143) (45 orals, 56 spotlights and 520 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICCV&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.0% (1077/4304) (187 short orals and 1077 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ECCV&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27.9% (363/1444) (38 orals and 325 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ECCV&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.6% (415/1561) (28 orals, 45 spotlights and 342 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ECCV&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;31.8% (776/2439) (59 orals and 717 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ECCV&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27.1% (1361/5025) (104 orals, 161 spotlights and 1096 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Machine Learning and Learning Theory&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Conference &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Long Paper &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Short Paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.0% (Cycle I), 22.0% (Cycle II)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.0% (270/1037)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.0% (322/?)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.9% (434/1676)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.1% (621/2473)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.6% (773/3424)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.8% (1088/4990)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.5% (1184/5513) (166 long talks, 1018 short talks)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.9% (1235/5630) (118 long talks, 1117 short talks)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.7% (414/1678)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.9% (403/1838)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.6% (569/2403)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.9% (678/3240) (40 orals, 112 spotlights and 526 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.8% (1011/4856) (30 orals, 168 spotlights and 813 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.1% (1428/6743) (36 orals, 164 spotlights and 1228 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.1% (1900/9454) (105 orals, 280 spotlights and 1515 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.69% (2344/9122) (55 orals, 260 spotlights and 2029 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;39.1% (198/507) (15 orals and 183 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.0% (314/981) (23 orals and 291 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;31.4% (500/1591) (24 orals and 476 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.5% (687/2594) (48 orals, 107 spotlights and 532 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.7% (860/2997) (53 orals, 114 spotlights and 693 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.9% (1095/3328) (54 orals, 176 spotlights and 865 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLT&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.1% (45/140)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLT&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;34.8% (62/178)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLT&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.1% (53/203)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLT&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.5% (74/228)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLT&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27.2% (91/335)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLT&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.0% (118/393)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLT&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.9% (120/388)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.0% (94/292)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;34.0% (99/291)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;31.0% (85/275)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;31.0% (87/282)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.8% (104/337)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.0% (118/450)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27.5% (142/515)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.3% (205/777)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.3% (230/712) (36 orals and 194 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;35.8% (120/335)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.7% (127/442)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.7% (165/537)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;31.7% (168/530)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;33.2% (214/645)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.4% (360/1111)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.8% (455/1527) (48 orals)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.2% (493/1685)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Artificial Intelligence&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Conference &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Long Paper &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Short Paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.0% (398/1406)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.7% (531/1991)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.8% (549/2132)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.6% (638/2590)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.6% (933/3800)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.2% (1150/7095)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.6% (1591/7737)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.4% (1692/7911)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.0% (1349/9020)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;13&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.0% (413/1473)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.6% (572/1996)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.0% (551/2294)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.0% (660/2540)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.5% (710/3470)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.9% (850/4752)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;12.6% (592/4717)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13.9% (587/4204)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Data Mining and Information Retrieval&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Conference &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Long Paper &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Short Paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KDD&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.6% (151/1036)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KDD&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.5% (160/819)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KDD&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13.7% (142/1115)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KDD&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.4% (130/748)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KDD&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.4% (181/983) (107 orals and 74 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KDD&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.2% (170/1200) (110 orals and 60 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KDD&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.9% (216/1279)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.0% (82/387)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;40.0% (104/263)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.0% (70/351)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;31.3% (79/252)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.0% (62/341)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.6% (104/339)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.0% (78/362)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.0% (121/398)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.0% (86/409)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.0% (98/327)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.7% (84/426)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.4% (108/443)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.5% (147/555)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.2% (153/507)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.0% (151/720)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27.6% (145/526)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.3% (161/794)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.7% (165/667)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13.0% (84/645)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.0% (131/929)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.0% (115/727)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.0% (164/966)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.0% (171/1140)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.0% (225/1247)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.9% (72/361)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.2% (217/1129)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.7% (98/397)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.6% (357/1736)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.7% (323/1822)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.0% (64/355)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.4% (39/238)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.2% (67/368)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.8% (80/505)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.1% (84/514)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.4% (84/511)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.8% (91/615)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.6% (112/603)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.8% (80/505)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.0% (175/838)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.9% (57/260)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.0% (165/646)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.0% (69/276)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.0% (160/701)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.5% (55/234)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.0% (171/855)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.4% (119/419)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.0% (147/862)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.2% (96/413)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.4% (200/1030)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.3% (100/470)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.0% (193/920)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.9% (103/397)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.7% (271/1251)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.3% (177/626)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.8% (71/727)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.8% (71/727)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;8.4% (68/807)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.7% (78/807)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;8.6% (78/904)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.0% (100/904)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.3% (72/778)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.7% (83/778)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;8.9% (84/948)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.1% (105/948)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.1% (95/1046)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.5% (99/1046)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.8% (91/930)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.9% (92/930)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.9% (98/990)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.1% (100/990)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RecSys&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.0% (35/152)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RecSys&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.2% (29/159)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RecSys&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.8% (26/125)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.4% (20/122)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RecSys&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.7% (32/181)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RecSys&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.0% (36/189)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RecSys&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.9% (39/218)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Speech and Signal Processing&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Conference &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Long Paper &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Short Paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;51.0% (~743/1458)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;50.5% (779/1541)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;52.0% (799/1582)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;54.3% (749/1320)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;49.3% (914/1855)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;~47% (?/?)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;48.4% (963/1990)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;48.0% (1709/3500)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;52.0% (1207/2322)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;47.0% (1265/2682)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;52.0% (1220/2518)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;49.7% (1406/2829)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;46.5% (1774/3815)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;48.0% (1734/3610)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;45.0% (1785/3967)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;For &lt;strong&gt;KDD&lt;/strong&gt; and &lt;strong&gt;TheWebConf&lt;/strong&gt; (formerly known as &lt;strong&gt;WWW&lt;/strong&gt;), only the papers from research track are counted.&lt;/li&gt; &#xA; &lt;li&gt;For &lt;strong&gt;ICDM&lt;/strong&gt;, submissions of short paper and those of long paper are in the same session and the decision of the paper type is made according to its quality.&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>fastai/fastai</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/fastai/fastai</id>
    <link href="https://github.com/fastai/fastai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The fastai deep learning library&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to fastai&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;fastai simplifies training fast and accurate neural nets using modern best practices&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/fastai/fastai/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt; &lt;a href=&#34;https://pypi.org/project/fastai/#description&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/fastai?color=blue&amp;amp;label=pypi%20version&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/fastai/fastai&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/vn/fastai/fastai?color=seagreen&amp;amp;label=conda%20version&#34; alt=&#34;Conda (channel only)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/fastai/docker-containers&#34;&gt;&lt;img src=&#34;https://github.com/fastai/docker-containers/workflows/Build%20fastai%20images/badge.svg?sanitize=true&#34; alt=&#34;Build fastai images&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/fastai/fastai/workflows/docs/badge.svg?sanitize=true&#34; alt=&#34;docs&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installing&lt;/h2&gt; &#xA;&lt;p&gt;You can use fastai without any installation by using &lt;a href=&#34;https://colab.research.google.com/&#34;&gt;Google Colab&lt;/a&gt;. In fact, every page of this documentation is also available as an interactive notebook - click &#34;Open in colab&#34; at the top of any page to open it (be sure to change the Colab runtime to &#34;GPU&#34; to have it run fast!) See the fast.ai documentation on &lt;a href=&#34;https://course.fast.ai/start_colab&#34;&gt;Using Colab&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;You can install fastai on your own machines with conda (highly recommended), as long as you&#39;re running Linux or Windows (NB: Mac is not supported). For Windows, please see the &#34;Running on Windows&#34; for important notes.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re using &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;miniconda&lt;/a&gt; (recommended) then run (note that if you replace &lt;code&gt;conda&lt;/code&gt; with &lt;a href=&#34;https://github.com/mamba-org/mamba&#34;&gt;mamba&lt;/a&gt; the install process will be much faster and more reliable):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c fastchan fastai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...or if you&#39;re using &lt;a href=&#34;https://www.anaconda.com/products/individual&#34;&gt;Anaconda&lt;/a&gt; then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c fastchan fastai anaconda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install with pip, use: &lt;code&gt;pip install fastai&lt;/code&gt;. If you install with pip, you should install PyTorch first by following the PyTorch &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you plan to develop fastai yourself, or want to be on the cutting edge, you can use an editable install (if you do this, you should also use an editable install of &lt;a href=&#34;https://github.com/fastai/fastcore&#34;&gt;fastcore&lt;/a&gt; to go with it.) First install PyTorch, and then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/fastai/fastai&#xA;pip install -e &#34;fastai[dev]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Learning fastai&lt;/h2&gt; &#xA;&lt;p&gt;The best way to get started with fastai (and deep learning) is to read &lt;a href=&#34;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&#34;&gt;the book&lt;/a&gt;, and complete &lt;a href=&#34;https://course.fast.ai&#34;&gt;the free course&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To see what&#39;s possible with fastai, take a look at the &lt;a href=&#34;https://docs.fast.ai/quick_start.html&#34;&gt;Quick Start&lt;/a&gt;, which shows how to use around 5 lines of code to build an image classifier, an image segmentation model, a text sentiment model, a recommendation system, and a tabular model. For each of the applications, the code is much the same.&lt;/p&gt; &#xA;&lt;p&gt;Read through the &lt;a href=&#34;https://docs.fast.ai/tutorial&#34;&gt;Tutorials&lt;/a&gt; to learn how to train your own models on your own datasets. Use the navigation sidebar to look through the fastai documentation. Every class, function, and method is documented here.&lt;/p&gt; &#xA;&lt;p&gt;To learn about the design and motivation of the library, read the &lt;a href=&#34;https://www.mdpi.com/2078-2489/11/2/108/htm&#34;&gt;peer reviewed paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;About fastai&lt;/h2&gt; &#xA;&lt;p&gt;fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A new type dispatch system for Python along with a semantic type hierarchy for tensors&lt;/li&gt; &#xA; &lt;li&gt;A GPU-optimized computer vision library which can be extended in pure Python&lt;/li&gt; &#xA; &lt;li&gt;An optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4–5 lines of code&lt;/li&gt; &#xA; &lt;li&gt;A novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training&lt;/li&gt; &#xA; &lt;li&gt;A new data block API&lt;/li&gt; &#xA; &lt;li&gt;And much more...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;fastai is organized around two main design goals: to be approachable and rapidly productive, while also being deeply hackable and configurable. It is built on top of a hierarchy of lower-level APIs which provide composable building blocks. This way, a user wanting to rewrite part of the high-level API or add particular behavior to suit their needs does not have to learn how to use the lowest level.&lt;/p&gt; &#xA;&lt;img alt=&#34;Layered API&#34; src=&#34;https://raw.githubusercontent.com/fastai/fastai/master/nbs/images/layered.png&#34; width=&#34;345&#34; style=&#34;max-width: 345px&#34;&gt; &#xA;&lt;h2&gt;Migrating from other libraries&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s very easy to migrate from plain PyTorch, Ignite, or any other PyTorch-based library, or even to use fastai in conjunction with other libraries. Generally, you&#39;ll be able to use all your existing data processing code, but will be able to reduce the amount of code you require for training, and more easily take advantage of modern best practices. Here are migration guides from some popular libraries to help you on your way:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.fast.ai/migrating_pytorch&#34;&gt;Plain PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.fast.ai/migrating_ignite&#34;&gt;Ignite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.fast.ai/migrating_lightning&#34;&gt;Lightning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.fast.ai/migrating_catalyst&#34;&gt;Catalyst&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Windows Support&lt;/h2&gt; &#xA;&lt;p&gt;When installing with &lt;code&gt;mamba&lt;/code&gt; or &lt;code&gt;conda&lt;/code&gt; replace &lt;code&gt;-c fastchan&lt;/code&gt; in the installation with &lt;code&gt;-c pytorch -c nvidia -c fastai&lt;/code&gt;, since fastchan is not currently supported on Windows.&lt;/p&gt; &#xA;&lt;p&gt;Due to python multiprocessing issues on Jupyter and Windows, &lt;code&gt;num_workers&lt;/code&gt; of &lt;code&gt;Dataloader&lt;/code&gt; is reset to 0 automatically to avoid Jupyter hanging. This makes tasks such as computer vision in Jupyter on Windows many times slower than on Linux. This limitation doesn&#39;t exist if you use fastai from a script.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/fastai/fastai/raw/master/nbs/examples/dataloader_spawn.py&#34;&gt;this example&lt;/a&gt; to fully leverage the fastai API on Windows.&lt;/p&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;To run the tests in parallel, launch:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;nbdev_test_nbs&lt;/code&gt; or &lt;code&gt;make test&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For all the tests to pass, you&#39;ll need to install the dependencies specified as part of dev_requirements in settings.ini&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install -e .[dev]&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tests are written using &lt;code&gt;nbdev&lt;/code&gt;, for example see the documentation for &lt;code&gt;test_eq&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;After you clone this repository, please run &lt;code&gt;nbdev_install_git_hooks&lt;/code&gt; in your terminal. This sets up git hooks, which clean up the notebooks to remove the extraneous stuff stored in the notebooks (e.g. which cells you ran) which causes unnecessary merge conflicts.&lt;/p&gt; &#xA;&lt;p&gt;Before submitting a PR, check that the local library and notebooks match. The script &lt;code&gt;nbdev_diff_nbs&lt;/code&gt; can let you know if there is a difference between the local library and the notebooks.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you made a change to the notebooks in one of the exported cells, you can export it to the library with &lt;code&gt;nbdev_build_lib&lt;/code&gt; or &lt;code&gt;make fastai&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you made a change to the library, you can export it back to the notebooks with &lt;code&gt;nbdev_update_lib&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker Containers&lt;/h2&gt; &#xA;&lt;p&gt;For those interested in official docker containers for this project, they can be found &lt;a href=&#34;https://github.com/fastai/docker-containers#fastai&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure/MachineLearningNotebooks</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/Azure/MachineLearningNotebooks</id>
    <link href="https://github.com/Azure/MachineLearningNotebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python notebooks with ML and deep learning examples with Azure Machine Learning Python SDK | Microsoft&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Azure Machine Learning Python SDK notebooks&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;a community-driven repository of examples using mlflow for tracking can be found at &lt;a href=&#34;https://github.com/Azure/azureml-examples&#34;&gt;https://github.com/Azure/azureml-examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Welcome to the Azure Machine Learning Python SDK notebooks repository!&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;These notebooks are recommended for use in an Azure Machine Learning &lt;a href=&#34;https://docs.microsoft.com/azure/machine-learning/concept-compute-instance&#34;&gt;Compute Instance&lt;/a&gt;, where you can run them without any additional set up.&lt;/p&gt; &#xA;&lt;p&gt;However, the notebooks can be run in any development environment with the correct &lt;code&gt;azureml&lt;/code&gt; packages installed.&lt;/p&gt; &#xA;&lt;p&gt;Install the &lt;code&gt;azureml.core&lt;/code&gt; Python package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install azureml-core&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install additional packages as needed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install azureml-mlflow&#xA;pip install azureml-dataset-runtime&#xA;pip install azureml-automl-runtime&#xA;pip install azureml-pipeline&#xA;pip install azureml-pipeline-steps&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend starting with one of the &lt;a href=&#34;https://raw.githubusercontent.com/Azure/MachineLearningNotebooks/master/tutorials/compute-instance-quickstarts&#34;&gt;quickstarts&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This repository is a push-only mirror. Pull requests are ignored.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/Azure/MachineLearningNotebooks/master/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/azure/machine-learning&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>The-Art-of-Hacking/h4cker</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/The-Art-of-Hacking/h4cker</id>
    <link href="https://github.com/The-Art-of-Hacking/h4cker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository is primarily maintained by Omar Santos and includes thousands of resources related to ethical hacking / penetration testing, digital forensics and incident response (DFIR), vulnerability research, exploit development, reverse engineering, and more.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cyber Security Resources&lt;/h1&gt; &#xA;&lt;center&gt;&#xA; &lt;img src=&#34;https://h4cker.org/img/h4cker2.PNG&#34; width=&#34;200&#34; height=&#34;300&#34;&gt; &#xA;&lt;/center&gt; &#xA;&lt;p&gt;This repository includes thousands of cybersecurity-related references and resources and it is maintained by &lt;a href=&#34;https://omarsantos.io/&#34;&gt;Omar Santos&lt;/a&gt;. This GitHub repository has been created to provide supplemental material to several books, video courses, and live training created by Omar Santos and other co-authors. It provides over 9,000 references, scripts, tools, code, and other resources that help offensive and defensive security professionals learn and develop new skills. This GitHub repository provides guidance on how build your own hacking environment, learn about offensive security (ethical hacking) techniques, vulnerability research, exploit development, reverse engineering, malware analysis, threat intelligence, threat hunting, digital forensics and incident response (DFIR), includes examples of real-life penetration testing reports, and more.&lt;/p&gt; &#xA;&lt;h2&gt;The Art of Hacking Series&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;http://theartofhacking.org&#34;&gt;Art of Hacking Series&lt;/a&gt; is a series of video courses and live training that help you get up and running with your cybersecurity career. The following are the different video courses that are part of the Art of Hacking series:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/library/view/security-penetration-testing/9780134833989&#34;&gt;Security Penetration Testing (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/library/view/wireless-networks-iot/9780134854632/&#34;&gt;Wireless Networks, IoT, and Mobile Devices Hacking (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/videos/enterprise-penetration-testing/9780134854779&#34;&gt;Enterprise Penetration Testing and Continuous Monitoring (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/videos/hacking-web-applications/9780135261422&#34;&gt;Hacking Web Applications: Security Penetration Testing for Today&#39;s DevOps and Cloud Environments (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These courses serve as comprehensive guide for any network and security professional who is starting a career in ethical hacking and penetration testing. It also can help individuals preparing for the &lt;a href=&#34;https://www.offensive-security.com/information-security-certifications/&#34;&gt;Offensive Security Certified Professional (OSCP)&lt;/a&gt;, the &lt;a href=&#34;https://www.eccouncil.org/programs/certified-ethical-hacker-ceh/&#34;&gt;Certified Ethical Hacker (CEH)&lt;/a&gt;, &lt;a href=&#34;https://certification.comptia.org/certifications/pentest&#34;&gt;CompTIA PenTest+&lt;/a&gt; and any other ethical hacking certification. This course helps any cyber security professional that want to learn the skills required to becoming a professional ethical hacker or that want to learn more about general hacking methodologies and concepts.&lt;/p&gt; &#xA;&lt;p&gt;These video courses are published by Pearson, but this GitHub repository is maintained and supported by the lead author of the series &lt;a href=&#34;https://omarsantos.io/&#34;&gt;Omar Santos&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Live Training&lt;/h2&gt; &#xA;&lt;p&gt;Other Live Training and Video Courses: &lt;a href=&#34;https://h4cker.org/training&#34;&gt;https://h4cker.org/training&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/ML-For-Beginners</title>
    <updated>2022-05-31T02:42:40Z</updated>
    <id>tag:github.com,2022-05-31:/microsoft/ML-For-Beginners</id>
    <link href="https://github.com/microsoft/ML-For-Beginners" rel="alternate"></link>
    <summary type="html">&lt;p&gt;12 weeks, 26 lessons, 52 quizzes, classic Machine Learning for all&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/ML-For-Beginners/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/issues/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/pulls/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/watchers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&amp;amp;label=Watch&#34; alt=&#34;GitHub watchers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&amp;amp;label=Fork&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&amp;amp;label=Star&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Machine Learning for Beginners - A Curriculum&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about &lt;strong&gt;Machine Learning&lt;/strong&gt;. In this curriculum, you will learn about what is sometimes called &lt;strong&gt;classic machine learning&lt;/strong&gt;, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming &#39;AI for Beginners&#39; curriculum. Pair these lessons with our &lt;a href=&#34;https://aka.ms/datascience-beginners&#34;&gt;&#39;Data Science for Beginners&#39; curriculum&lt;/a&gt;, as well!&lt;/p&gt; &#xA;&lt;p&gt;Travel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to &#39;stick&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;✍️ Hearty thanks to our authors&lt;/strong&gt; Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, and Amy Boyd&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🎨 Thanks as well to our illustrators&lt;/strong&gt; Tomomi Imura, Dasani Madipalli, and Jen Looper&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🙏 Special thanks 🙏 to our Microsoft Student Ambassador authors, reviewers, and content contributors&lt;/strong&gt;, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🤩 Extra gratitude to Microsoft Student Ambassador Eric Wanjau for our R lessons!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://aka.ms/student-page&#34;&gt;Students&lt;/a&gt;&lt;/strong&gt;, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start with a pre-lecture quiz.&lt;/li&gt; &#xA; &lt;li&gt;Read the lecture and complete the activities, pausing and reflecting at each knowledge check.&lt;/li&gt; &#xA; &lt;li&gt;Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the &lt;code&gt;/solution&lt;/code&gt; folders in each project-oriented lesson.&lt;/li&gt; &#xA; &lt;li&gt;Take the post-lecture quiz.&lt;/li&gt; &#xA; &lt;li&gt;Complete the challenge.&lt;/li&gt; &#xA; &lt;li&gt;Complete the assignment.&lt;/li&gt; &#xA; &lt;li&gt;After completing a lesson group, visit the &lt;a href=&#34;https://github.com/microsoft/ML-For-Beginners/discussions&#34;&gt;Discussion Board&lt;/a&gt; and &#34;learn out loud&#34; by filling out the appropriate PAT rubric. A &#39;PAT&#39; is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For further study, we recommend following these &lt;a href=&#34;https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa&#34;&gt;Microsoft Learn&lt;/a&gt; modules and learning paths.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;, we have &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/for-teachers.md&#34;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Meet the Team&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/Tj1XWrDSYJU&#34; title=&#34;Promo video&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/ml.gif&#34; alt=&#34;Promo video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gif by&lt;/strong&gt; &lt;a href=&#34;https://linkedin.com/in/mohitjaisal&#34;&gt;Mohit Jaisal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🎥 Click the image above for a video about the project and the folks who created it!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Pedagogy&lt;/h2&gt; &#xA;&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is hands-on &lt;strong&gt;project-based&lt;/strong&gt; and that it includes &lt;strong&gt;frequent quizzes&lt;/strong&gt;. In addition, this curriculum has a common &lt;strong&gt;theme&lt;/strong&gt; to give it cohesion.&lt;/p&gt; &#xA;&lt;p&gt;By ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12-week cycle. This curriculum also includes a postscript on real-world applications of ML, which can be used as extra credit or as a basis for discussion.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Find our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/TRANSLATIONS.md&#34;&gt;Translation&lt;/a&gt; guidelines. We welcome your constructive feedback!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Each lesson includes:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;optional sketchnote&lt;/li&gt; &#xA; &lt;li&gt;optional supplemental video&lt;/li&gt; &#xA; &lt;li&gt;pre-lecture warmup quiz&lt;/li&gt; &#xA; &lt;li&gt;written lesson&lt;/li&gt; &#xA; &lt;li&gt;for project-based lessons, step-by-step guides on how to build the project&lt;/li&gt; &#xA; &lt;li&gt;knowledge checks&lt;/li&gt; &#xA; &lt;li&gt;a challenge&lt;/li&gt; &#xA; &lt;li&gt;supplemental reading&lt;/li&gt; &#xA; &lt;li&gt;assignment&lt;/li&gt; &#xA; &lt;li&gt;post-lecture quiz&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about languages&lt;/strong&gt;: These lessons are primarily written in Python, but many are also available in R. To complete an R lesson, go to the &lt;code&gt;/solution&lt;/code&gt; folder and look for R lessons. They include an .rmd extension that represents an &lt;strong&gt;R Markdown&lt;/strong&gt; file which can be simply defined as an embedding of &lt;code&gt;code chunks&lt;/code&gt; (of R or other languages) and a &lt;code&gt;YAML header&lt;/code&gt; (that guides how to format outputs such as PDF) in a &lt;code&gt;Markdown document&lt;/code&gt;. As such, it serves as an exemplary authoring framework for data science since it allows you to combine your code, its output, and your thoughts by allowing you to write them down in Markdown. Moreover, R Markdown documents can be rendered to output formats such as PDF, HTML, or Word.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained &lt;a href=&#34;https://white-water-09ec41f0f.azurestaticapps.net/&#34;&gt;in this app&lt;/a&gt;, for 52 total quizzes of three questions each. They are linked from within the lessons but the quiz app can be run locally; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Number&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Topic&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Grouping&lt;/th&gt; &#xA;   &lt;th&gt;Learning Objectives&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Linked Lesson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Author&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn the basic concepts behind machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/1-intro-to-ML/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Muhammad&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The History of machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn the history underlying this field&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/2-history-of-ML/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Jen and Amy&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Fairness and machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;What are the important philosophical issues around fairness that students should consider when building and applying ML models?&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/3-fairness/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Tomomi&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Techniques for machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;What techniques do ML researchers use to build ML models?&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/4-techniques-of-ML/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chris and Jen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to regression&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Get started with Python and Scikit-learn for regression models&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/1-Tools/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/1-Tools/solution/R/lesson_1-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;North American pumpkin prices 🎃&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Visualize and clean data in preparation for ML&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/2-Data/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;North American pumpkin prices 🎃&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build linear and polynomial regression models&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/3-Linear/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Dmitry&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;North American pumpkin prices 🎃&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build a logistic regression model&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/4-Logistic/README.md&#34;&gt;Python&lt;/a&gt; &lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A Web App 🔌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/3-Web-App/README.md&#34;&gt;Web App&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build a web app to use your trained model&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/3-Web-App/1-Web-App/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Jen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to classification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Clean, prep, and visualize your data; introduction to classification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/1-Introduction/README.md&#34;&gt;Python&lt;/a&gt; &lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Cassie&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Delicious Asian and Indian cuisines 🍜&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduction to classifiers&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/2-Classifiers-1/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Cassie&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Delicious Asian and Indian cuisines 🍜&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;More classifiers&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/3-Classifiers-2/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Cassie&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Delicious Asian and Indian cuisines 🍜&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build a recommender web app using your model&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/4-Applied/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Jen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to clustering&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/README.md&#34;&gt;Clustering&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Clean, prep, and visualize your data; Introduction to clustering&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/1-Visualize/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Exploring Nigerian Musical Tastes 🎧&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/README.md&#34;&gt;Clustering&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Explore the K-Means clustering method&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/2-K-Means/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to natural language processing ☕️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn the basics about NLP by building a simple bot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/1-Introduction-to-NLP/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Common NLP Tasks ☕️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deepen your NLP knowledge by understanding common tasks required when dealing with language structures&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/2-Tasks/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Translation and sentiment analysis ♥️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Translation and sentiment analysis with Jane Austen&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/3-Translation-Sentiment/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Romantic hotels of Europe ♥️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sentiment analysis with hotel reviews 1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/4-Hotel-Reviews-1/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Romantic hotels of Europe ♥️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sentiment analysis with hotel reviews 2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/5-Hotel-Reviews-2/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to time series forecasting&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/README.md&#34;&gt;Time series&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduction to time series forecasting&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/1-Introduction/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Francesca&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;⚡️ World Power Usage ⚡️ - time series forecasting with ARIMA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/README.md&#34;&gt;Time series&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Time series forecasting with ARIMA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/2-ARIMA/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Francesca&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;⚡️ World Power Usage ⚡️ - time series forecasting with SVR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/README.md&#34;&gt;Time series&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Time series forecasting with Support Vector Regressor&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/3-SVR/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Anirban&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to reinforcement learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/README.md&#34;&gt;Reinforcement learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduction to reinforcement learning with Q-Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/1-QLearning/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dmitry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Help Peter avoid the wolf! 🐺&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/README.md&#34;&gt;Reinforcement learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Reinforcement learning Gym&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/2-Gym/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dmitry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Postscript&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Real-World ML scenarios and applications&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/README.md&#34;&gt;ML in the Wild&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Interesting and revealing real-world applications of classical ML&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/1-Applications/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Team&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Offline access&lt;/h2&gt; &#xA;&lt;p&gt;You can run this documentation offline by using &lt;a href=&#34;https://docsify.js.org/#/&#34;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&#34;https://docsify.js.org/#/quickstart&#34;&gt;install Docsify&lt;/a&gt; on your local machine, and then in the root folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;PDFs&lt;/h2&gt; &#xA;&lt;p&gt;Find a pdf of the curriculum with links &lt;a href=&#34;https://microsoft.github.io/ML-For-Beginners/pdf/readme.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Help Wanted!&lt;/h2&gt; &#xA;&lt;p&gt;Would you like to contribute a translation? Please read our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/TRANSLATIONS.md&#34;&gt;translation guidelines&lt;/a&gt; and add a templated issue to manage the workload &lt;a href=&#34;https://github.com/microsoft/ML-For-Beginners/issues&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Other Curricula&lt;/h2&gt; &#xA;&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/webdev-beginners&#34;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/iot-beginners&#34;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/datascience-beginners&#34;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-beginners&#34;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>