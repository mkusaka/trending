<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-01T01:48:55Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>panaversity/learn-agentic-ai</title>
    <updated>2025-06-01T01:48:55Z</updated>
    <id>tag:github.com,2025-06-01:/panaversity/learn-agentic-ai</id>
    <link href="https://github.com/panaversity/learn-agentic-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn Agentic AI using Dapr Agentic Cloud Ascent (DACA) Design Pattern and Agent-Native Cloud Technologies: OpenAI Agents SDK, Memory, MCP, A2A, Knowledge Graphs, Dapr, Rancher Desktop, and Kubernetes.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Learn Agentic AI using Dapr Agentic Cloud Ascent (DACA) Design Pattern: From Start to Scale&lt;/h1&gt; &#xA;&lt;p&gt;This repo is part of the &lt;a href=&#34;https://docs.google.com/document/d/15usu1hkrrRLRjcq_3nCTT-0ljEcgiC44iSdvdqrCprk/edit?usp=sharing&#34;&gt;Panaversity Certified Agentic &amp;amp; Robotic AI Engineer&lt;/a&gt; program. It covers AI-201, AI-202 and AI-301 courses.&lt;/p&gt; &#xA;&lt;p&gt;We have Two Hunches, the future of Pakistan depends on it, let&#39;s make sure that we are not wrong:&lt;/p&gt; &#xA;&lt;p&gt;It is very important for Pakistan that we bet on the right horses for the upcoming age of Agentic AI. We will be training millions of Agentic AI Developers all over Pakistan and online around the world and building startups, we cant afford to be wrong.&lt;/p&gt; &#xA;&lt;p&gt;Hunch #1: Dapr We feel Dapr, Dapr Actors, Dapr Workflows, and Dapr Agents will be the core technology in building the next generation multi ai agentic systems, is my hunch correct?&lt;/p&gt; &#xA;&lt;p&gt;Hunch #2: OpenAI Agents SDK We also have a hunch that OpenAI Agents SDK will be the go to framework for beginners to start learning Agentic AI?&lt;/p&gt; &#xA;&lt;p&gt;Let us see what the best AI has to say about our hunches:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chatgpt.com/share/6811b893-82cc-8001-9037-e45bcd91cc64&#34;&gt;https://chatgpt.com/share/6811b893-82cc-8001-9037-e45bcd91cc64&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://g.co/gemini/share/1f31c876520b&#34;&gt;https://g.co/gemini/share/1f31c876520b&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://grok.com/share/bGVnYWN5_4343d342-c7df-4b06-9174-487a64f59d53&#34;&gt;https://grok.com/share/bGVnYWN5_4343d342-c7df-4b06-9174-487a64f59d53&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;This Panaversity Initiative Tackles the Critical Challenge:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;“How do we design AI Agents that can handle 10 million concurrent AI Agents without failing?”&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: The challenge is intensified as we must guide our students to solve this issue with minimal financial resources available during training.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/img/cover.png&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Kubernetes with Dapr can theoretically handle 10 million concurrent agents in an agentic AI system without failing, but achieving this requires extensive optimization, significant infrastructure, and careful engineering. While direct evidence at this scale is limited, logical extrapolation from existing benchmarks, Kubernetes’ scalability, and Dapr’s actor model supports feasibility, especially with rigorous tuning and resource allocation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Condensed Argument with Proof and Logic&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Kubernetes Scalability&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: Kubernetes supports up to 5,000 nodes and 150,000 pods per cluster (Kubernetes docs), with real-world examples like PayPal scaling to 4,000 nodes and 200,000 pods (InfoQ, 2023) and KubeEdge managing 100,000 edge nodes and 1 million pods (KubeEdge case studies). OpenAI’s 2,500-node cluster for AI workloads (OpenAI blog, 2022) shows Kubernetes can handle compute-intensive tasks.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: For 10 million users, a cluster of 5,000–10,000 nodes (e.g., AWS g5 instances with GPUs) can distribute workloads. Each node can run hundreds of pods, and Kubernetes’ horizontal pod autoscaling (HPA) dynamically adjusts to demand. Bottlenecks (e.g., API server, networking) can be mitigated by tuning etcd, using high-performance CNIs like Cilium, and optimizing DNS.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dapr’s Efficiency for Agentic AI&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: Dapr’s actor model supports thousands of virtual actors per CPU core with double-digit millisecond latency (Dapr docs, 2024). Case studies show Dapr handling millions of events, e.g., Tempestive’s IoT platform processing billions of messages (Dapr blog, 2023) and DeFacto’s system managing 3,700 events/second (320 million daily) on Kubernetes with Kafka (Microsoft case study, 2022).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: Agentic AI relies on stateful, low-latency agents. Dapr Agents, built on the actor model, can represent 10 million users as actors, distributed across a Kubernetes cluster. Dapr’s state management (e.g., Redis) and pub/sub messaging (e.g., Kafka) ensure efficient coordination and resilience, with automatic retries preventing failures. Sharding state stores and message brokers scales to millions of operations/second.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Handling AI Workloads&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: LLM inference frameworks like vLLM and TGI serve thousands of requests/second per GPU (vLLM benchmarks, 2024). Kubernetes orchestrates GPU workloads effectively, as seen Kubernetes manages GPU workloads, as seen in NVIDIA’s AI platform scaling to thousands of GPUs (NVIDIA case study, 2023).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: Assuming each user generates 1 request/second requiring 0.01 GPU, 10 million users need ~100,000 GPUs. Batching, caching, and model parallelism reduce this to a feasible ~10,000–20,000 GPUs, achievable in hyperscale clouds (e.g., AWS). Kubernetes’ resource scheduling ensures optimal GPU utilization.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Networking and Storage&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: EMQX on Kubernetes handled 1 million concurrent connections with tuning (EMQX blog, 2024). C10M benchmarks (2013) achieved 10 million connections using optimized stacks. Dapr’s state stores (e.g., Redis) support millions of operations/second (Redis benchmarks, 2024).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: 10 million connections require ~100–1,000 Gbps bandwidth, supported by modern clouds. High-throughput databases (e.g., CockroachDB) and caching (e.g., Redis Cluster) handle 10 TB of state data for 10 million users (1 KB/user). Kernel bypass (e.g., DPDK) and eBPF-based CNIs (e.g., Cilium) minimize networking latency.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Resilience and Monitoring&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: Dapr’s resiliency policies (retries, circuit breakers) and Kubernetes’ self-healing (pod restarts) ensure reliability (Dapr docs, 2024). Dapr’s OpenTelemetry integration scales monitoring for millions of agents (Prometheus case studies, 2023).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: Real-time metrics (e.g., latency, error rates) and distributed tracing prevent cascading failures. Kubernetes’ liveness probes and Dapr’s workflow engine recover from crashes, ensuring 99.999% uptime.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feasibility with Constraints&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Challenge&lt;/strong&gt;: No direct benchmark exists for 10 million concurrent users with Dapr/Kubernetes in an agentic AI context. Infrastructure costs (e.g., $10M–$100M for 10,000 nodes) are prohibitive for low-budget scenarios.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Use open-source tools (e.g., Minikube, kind) for local testing and cloud credits (e.g., AWS Educate) for students. Simulate 10 million users with tools like Locust on smaller clusters (e.g., 100 nodes), extrapolating results. Optimize Dapr’s actor placement and Kubernetes’ resource quotas to maximize efficiency on limited hardware. Leverage free-tier databases (e.g., MongoDB Atlas) and message brokers (e.g., RabbitMQ).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;: Kubernetes with Dapr can handle 10 million concurrent users in an agentic AI system, supported by their proven scalability, real-world case studies, and logical extrapolation. For students with minimal budgets, small-scale simulations, open-source tools, and cloud credits make the problem tractable, though production-scale deployment requires hyperscale resources and expertise.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Agentic AI Top Trend of 2025&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/img/toptrend.webp&#34; width=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;The Dapr Agentic Cloud Ascent (DACA) Design Pattern Addresses 10 Million AI Agents Challenge&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s understand and learn about &#34;Dapr Agentic Cloud Ascent (DACA)&#34;, our winning design pattern for developing and deploying planet scale multi-agent systems.&lt;/p&gt; &#xA;&lt;h3&gt;Executive Summary: Dapr Agentic Cloud Ascent (DACA)&lt;/h3&gt; &#xA;&lt;p&gt;The Dapr Agentic Cloud Ascent (DACA) guide introduces a strategic design pattern for building and deploying sophisticated, scalable, and resilient agentic AI systems. Addressing the complexities of modern AI development, DACA integrates the OpenAI Agents SDK for core agent logic with the Model Context Protocol (MCP) for standardized tool use and the Agent2Agent (A2A) protocol for seamless inter-agent communication, all underpinned by the distributed capabilities of Dapr. &lt;strong&gt;Grounded in AI-first and cloud-first principles&lt;/strong&gt;, DACA promotes the use of stateless, containerized applications deployed on platforms like Azure Container Apps (Serverless Containers) or Kubernetes, enabling efficient scaling from local development to planetary-scale production, potentially leveraging free-tier cloud services and self-hosted LLMs for cost optimization. The pattern emphasizes modularity, context-awareness, and standardized communication, envisioning an &lt;strong&gt;Agentia World&lt;/strong&gt; where diverse AI agents collaborate intelligently. Ultimately, DACA offers a robust, flexible, and cost-effective framework for developers and architects aiming to create complex, cloud-native agentic AI applications that are built for scalability and resilience from the ground up.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/panaversity/learn-agentic-ai/raw/main/comprehensive_guide_daca.md&#34;&gt;Comprehensive Guide to Dapr Agentic Cloud Ascent (DACA) Design Pattern&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/img/ascent.png&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/img/architecture1.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Target User&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agentic AI Developer and AgentOps Professionals&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Why OpenAI Agents SDK should be the main framework for agentic development for most use cases?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Table 1: Comparison of Abstraction Levels in AI Agent Frameworks&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Framework&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Abstraction Level&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Key Characteristics&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Learning Curve&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Control Level&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Simplicity&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;OpenAI Agents SDK&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Minimal&lt;/td&gt; &#xA;   &lt;td&gt;Python-first, core primitives (Agents, Handoffs, Guardrails), direct control&lt;/td&gt; &#xA;   &lt;td&gt;Low&lt;/td&gt; &#xA;   &lt;td&gt;High&lt;/td&gt; &#xA;   &lt;td&gt;High&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CrewAI&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Moderate&lt;/td&gt; &#xA;   &lt;td&gt;Role-based agents, crews, tasks, focus on collaboration&lt;/td&gt; &#xA;   &lt;td&gt;Low-Medium&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;AutoGen&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;High&lt;/td&gt; &#xA;   &lt;td&gt;Conversational agents, flexible conversation patterns, human-in-the-loop support&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Google ADK&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Moderate&lt;/td&gt; &#xA;   &lt;td&gt;Multi-agent hierarchies, Google Cloud integration (Gemini, Vertex AI), rich tool ecosystem, bidirectional streaming&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;Medium-High&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Low-Moderate&lt;/td&gt; &#xA;   &lt;td&gt;Graph-based workflows, nodes, edges, explicit state management&lt;/td&gt; &#xA;   &lt;td&gt;Very High&lt;/td&gt; &#xA;   &lt;td&gt;Very High&lt;/td&gt; &#xA;   &lt;td&gt;Low&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Dapr Agents&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Moderate&lt;/td&gt; &#xA;   &lt;td&gt;Stateful virtual actors, event-driven multi-agent workflows, Kubernetes integration, 50+ data connectors, built-in resiliency&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;Medium-High&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The table clearly identifies why OpenAI Agents SDK should be the main framework for agentic development for most use cases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It excels in &lt;strong&gt;simplicity&lt;/strong&gt; and &lt;strong&gt;ease of use&lt;/strong&gt;, making it the best choice for rapid development and broad accessibility.&lt;/li&gt; &#xA; &lt;li&gt;It offers &lt;strong&gt;high control&lt;/strong&gt; with &lt;strong&gt;minimal abstraction&lt;/strong&gt;, providing the flexibility needed for agentic development without the complexity of frameworks like LangGraph.&lt;/li&gt; &#xA; &lt;li&gt;It outperforms most alternatives (CrewAI, AutoGen, Google ADK, Dapr Agents) in balancing usability and power, and while LangGraph offers more control, its complexity makes it less practical for general use.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If your priority is ease of use, flexibility, and quick iteration in agentic development, OpenAI Agents SDK is the clear winner based on the table. However, if your project requires enterprise-scale features (e.g., Dapr Agents) or maximum control for complex workflows (e.g., LangGraph), you might consider those alternatives despite their added complexity.&lt;/p&gt; &#xA;&lt;h2&gt;Core DACA Agentic AI Courses:&lt;/h2&gt; &#xA;&lt;h3&gt;AI-201: Fundamentals of Agentic AI and DACA AI-First Development (14 weeks)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;⁠Agentic &amp;amp; DACA Theory - 1 week&lt;/li&gt; &#xA; &lt;li&gt;UV &amp;amp; ⁠OpenAI Agents SDK - 5 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Agentic Design Patterns - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Memory [LangMem &amp;amp; mem0] 1 week&lt;/li&gt; &#xA; &lt;li&gt;Postgres/Redis (Managed Cloud) - 1 week&lt;/li&gt; &#xA; &lt;li&gt;FastAPI (Basic) - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Containerization (Rancher Desktop) - 1 week&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face Docker Spaces - 1 week&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL0vKVrkG4hWovpr0FX6Gs-06hfsPDEUe6&#34;&gt;AI-201 Video Playlist&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: These videos are for additional learning, and do not cover all the material taught in the onsite classes.&lt;/p&gt; &#xA;&lt;p&gt;Prerequisite: Successful completion of &lt;a href=&#34;https://github.com/panaversity/learn-modern-ai-python&#34;&gt;AI-101: Modern AI Python Programming - Your Launchpad into Intelligent Systems&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;AI-202: DACA Cloud-First Agentic AI Development (14 weeks)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Rancher Desktop with Local Kubernetes - 4 weeks&lt;/li&gt; &#xA; &lt;li&gt;Advanced FastAPI with Kubernetes - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;Dapr [workflows, state, pubsub, secrets] - 3 Week&lt;/li&gt; &#xA; &lt;li&gt;CockRoachdb &amp;amp; RabbitMQ Managed Services - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Model Context Protocol - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Serverless Containers Deployment (ACA) - 2 weeks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Prerequisite: Successful completion of AI-201&lt;/p&gt; &#xA;&lt;h3&gt;AI-301 DACA Planet-Scale Distributed AI Agents (14 Weeks)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;⁠Certified Kubernetes Application Developer (CKAD) - 4 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠A2A Protocol - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Voice Agents - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Dapr Agents/Google ADK - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Self-LLMs Hosting - 1 week&lt;/li&gt; &#xA; &lt;li&gt;Finetuning LLMs - 3 weeks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Prerequisite: Successful completion of AI-201 &amp;amp; AI-202&lt;/p&gt; &#xA;&lt;h2&gt;Evaluations&lt;/h2&gt; &#xA;&lt;p&gt;Quizzes + Hackathons (Everything is Onsite)&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Advanced Modern Python (including asyncio) [Q1]&lt;/li&gt; &#xA; &lt;li&gt;OpenAI Agents SDK (48 MCQ in 2 hour) [01_ai_agents_first]&lt;/li&gt; &#xA; &lt;li&gt;Protocols &amp;amp; Design Patterns (A2A and MCP) [05_ai_protocols]&lt;/li&gt; &#xA; &lt;li&gt;Hackathon1 - 8 Hours (Using Above Quiz Stack)&lt;/li&gt; &#xA; &lt;li&gt;Containerization + FastAPI [04_daca_agent_native_dev = 01 + 02 ]&lt;/li&gt; &#xA; &lt;li&gt;Kubernetes (Rancher Desktop) [Stimulations] [04_daca_agent_native_dev = 02 ]&lt;/li&gt; &#xA; &lt;li&gt;Dapr-1 - State, PubSub, Bindings, Invocation [04_daca_agent_native_dev = 03 ]&lt;/li&gt; &#xA; &lt;li&gt;Dapr-2 - Workflows, Virtual Actors [04_agent_native = 04, 05, 06]&lt;/li&gt; &#xA; &lt;li&gt;Hackathon2 - 8 Hours (Agent Native Startup)&lt;/li&gt; &#xA; &lt;li&gt;CKAD + DAPR + ArgoCD (Simulations) [06_daca_deployment_guide + 07_ckad]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Quiz Details&lt;/h2&gt; &#xA;&lt;h3&gt;Fundamentals of Agentic AI Quiz&lt;/h3&gt; &#xA;&lt;p&gt;Total Questions: 48 MCQs&lt;/p&gt; &#xA;&lt;p&gt;Duration: 120 Minutes&lt;/p&gt; &#xA;&lt;p&gt;Difficulty Level: Intermediate or Advanced (NOT beginner-level)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL0vKVrkG4hWr4V2I4P6GaDzMG_LijlGTm&#34;&gt;Quiz Preparation Playlist&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a well-constructed, comprehensive quiz that accurately tests deep knowledge of the OpenAI Agents SDK. However, it&#39;s significantly more challenging than typical beginner-level assessments.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Difficulty Level for Beginners&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The quiz is challenging for beginners due to the following factors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Technical Depth&lt;/strong&gt;: Questions require understanding the OpenAI Agents SDK’s architecture (e.g., Agents, Tools, Handoffs, Runner), Pydantic models, async programming, and prompt engineering. These are advanced topics for someone new to AI or Python.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Conceptual Complexity&lt;/strong&gt;: Topics like dynamic instructions, context management, error handling, and Chain-of-Thought prompting require familiarity with both theoretical and practical aspects of agentic AI.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Code Analysis&lt;/strong&gt;: Many questions involve analyzing code snippets, understanding execution paths, and predicting outcomes, which demand strong Python and debugging skills. Domain Knowledge: Questions on Markdown are simpler, but the majority focus on niche SDK features, making the quiz specialized.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Beginner Challenges&lt;/strong&gt;: Beginners (e.g., those with basic Python knowledge and minimal AI experience) would struggle with SDK-specific concepts like Runner.run_sync, tool_choice, and Pydantic validation, as well as async programming and multi-agent workflows.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Difficulty Rating&lt;/strong&gt;: Advanced (not beginner-friendly). Beginners would need foundational knowledge in Python, async programming, and LLMs, plus specific training on the OpenAI Agents SDK to perform well.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To excel in this quiz, focus on understanding the core components and philosophy of the OpenAI Agents SDK, such as its &#34;Python-first&#34; design for orchestration, the roles of Agents and Tools, and how primitives like &#34;Handoffs&#34; facilitate multi-agent collaboration. Pay close attention to how the SDK manages the agent loop, handles tool calls and Pydantic models for typed inputs/outputs, and uses context objects. Review concepts like dynamic instructions, agent cloning, error handling during tool execution, and the nuances of Runner.run_sync() versus streaming. Additionally, refresh your knowledge of prompt engineering techniques, including crafting clear instructions, guiding the agent&#39;s reasoning (e.g., Chain-of-Thought), and managing sensitive data through persona and careful prompting. Finally, ensure you&#39;re comfortable with basic Markdown syntax for links and images.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Preparation Guide for Beginner Students&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This OpenAI Agents SDK quiz is designed for intermediate to advanced learners and requires substantial preparation to succeed. Before attempting this assessment, ensure you have a solid foundation in Python programming, including object-oriented concepts, async/await patterns, decorators, and error handling. You&#39;ll need to thoroughly study Pydantic models for data validation, understanding field definitions, default values, and validation behavior. Dedicate significant time to the OpenAI Agents SDK documentation (&lt;a href=&#34;https://openai.github.io/openai-agents-python/&#34;&gt;https://openai.github.io/openai-agents-python/&lt;/a&gt;), focusing on core concepts like Agents, Tools, Handoffs, context management, and the agent execution loop. Practice writing and analyzing code that uses the @function_tool decorator, Runner.run_sync(), agent cloning, and multi-agent orchestration patterns. Review prompt engineering techniques from the OpenAI cookbook, particularly Chain-of-Thought prompting, system message design, and handling sensitive data. Finally, familiarize yourself with basic Markdown syntax for links and images. Plan to spend at least 2-3 weeks studying these materials, complete hands-on coding exercises with the SDK. Consider this quiz a capstone assessment that requires comprehensive understanding rather than a beginner-level introduction to the concepts.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quiz Covers&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openai.github.io/openai-agents-python/&#34;&gt;https://openai.github.io/openai-agents-python/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cookbook.openai.com/examples/gpt4-1_prompting_guide&#34;&gt;https://cookbook.openai.com/examples/gpt4-1_prompting_guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.markdownguide.org/basic-syntax/&#34;&gt;https://www.markdownguide.org/basic-syntax/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.markdownguide.org/cheat-sheet/&#34;&gt;https://www.markdownguide.org/cheat-sheet/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/panaversity/learn-agentic-ai/tree/main/01_ai_agents_first&#34;&gt;https://github.com/panaversity/learn-agentic-ai/tree/main/01_ai_agents_first&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;You Can Generate Mock Quizzes for Practice using LLMs from this Prompt:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Create a comprehensive quiz covering OpenAI Agents SDK. It should include as many MCQ Quiz Questions as required to test the material, the questions should be difficult and at the graduate level and should test both concepts and include code were required. From the following following documentation:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openai.github.io/openai-agents-python/&#34;&gt;https://openai.github.io/openai-agents-python/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>aburkov/theLMbook</title>
    <updated>2025-06-01T01:48:55Z</updated>
    <id>tag:github.com,2025-06-01:/aburkov/theLMbook</id>
    <link href="https://github.com/aburkov/theLMbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the official repository for The Hundred-Page Language Models Book by Andriy Burkov&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Hundred-Page Language Models Book&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/06a77a36-3022-4a21-a522-d5d213320bf0&#34; alt=&#34;cover&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/GenerativeAIExamples</title>
    <updated>2025-06-01T01:48:55Z</updated>
    <id>tag:github.com,2025-06-01:/NVIDIA/GenerativeAIExamples</id>
    <link href="https://github.com/NVIDIA/GenerativeAIExamples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generative AI reference workflows optimized for accelerated infrastructure and microservice architecture.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/images/apps-catalog-promo-web-banner-laptop-300@2x.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;NVIDIA Generative AI Examples&lt;/h1&gt; &#xA;&lt;p&gt;This repository is a starting point for developers looking to integrate with the NVIDIA software ecosystem to speed up their generative AI systems. Whether you are building RAG pipelines, agentic workflows, or fine-tuning models, this repository will help you integrate NVIDIA, seamlessly and natively, with your development stack.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;!-- TOC --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#whats-new&#34;&gt;What&#39;s New?&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#data-flywheel&#34;&gt;Data Flywheel&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#knowledge-graph-rag&#34;&gt;Knowledge Graph RAG&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#agentic-workflows-with-llama-31&#34;&gt;Agentic Workflows with Llama 3.1&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#rag-with-local-nim-deployment-and-langchain&#34;&gt;RAG with Local NIM Deployment and LangChain&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#vision-nim-workflows&#34;&gt;Vision NIM Workflows&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#try-it-now&#34;&gt;Try it Now!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#data-flywheel&#34;&gt;Data Flywheel&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#tool-calling-notebooks&#34;&gt;Tool-Calling Notebooks&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#rag&#34;&gt;RAG&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#rag-notebooks&#34;&gt;RAG Notebooks&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#rag-examples&#34;&gt;RAG Examples&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#rag-tools&#34;&gt;RAG Tools&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#rag-projects&#34;&gt;RAG Projects&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#documentation&#34;&gt;Documentation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#how-tos&#34;&gt;How To&#39;s&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#reference&#34;&gt;Reference&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/#community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- /TOC --&gt; &#xA;&lt;h2&gt;What&#39;s New?&lt;/h2&gt; &#xA;&lt;h3&gt;Data Flywheel&lt;/h3&gt; &#xA;&lt;p&gt;This tutorial demonstrates an end-to-end Data Flywheel implementation that uses NVIDIA NeMo Microservices. It features a tool-calling workflow with the NVIDIA NeMo Datastore, NeMo Entity Store, NeMo Customizer, NeMo Evaluator, NeMo Guardrails microservices, and NVIDIA NIMs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/nemo/data-flywheel/tool-calling&#34;&gt;Tool Calling Fine-tuning, Inference, and Evaluation with NVIDIA NeMo Microservices and NIMs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Knowledge Graph RAG&lt;/h3&gt; &#xA;&lt;p&gt;This example implements a GPU-accelerated pipeline for creating and querying knowledge graphs using RAG by leveraging NIM microservices and the RAPIDS ecosystem to process large-scale datasets efficiently.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/community/knowledge_graph_rag&#34;&gt;Knowledge Graphs for RAG with NVIDIA AI Foundation Models and Endpoints&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Agentic Workflows with Llama 3.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build an Agentic RAG Pipeline with Llama 3.1 and NVIDIA NeMo Retriever NIM microservices [&lt;a href=&#34;https://developer.nvidia.com/blog/build-an-agentic-rag-pipeline-with-llama-3-1-and-nvidia-nemo-retriever-nims/&#34;&gt;Blog&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/notebooks/langchain/agentic_rag_with_nemo_retriever_nim.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/GenerativeAIExamples/raw/v0.7.0/experimental/event-driven-rag-cve-analysis&#34;&gt;NVIDIA Morpheus, NIM microservices, and RAG pipelines integrated to create LLM-based agent pipelines&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RAG with Local NIM Deployment and LangChain&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tips for Building a RAG Pipeline with NVIDIA AI LangChain AI Endpoints by Amit Bleiweiss. [&lt;a href=&#34;https://developer.nvidia.com/blog/tips-for-building-a-rag-pipeline-with-nvidia-ai-langchain-ai-endpoints/&#34;&gt;Blog&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVIDIA/GenerativeAIExamples/raw/v0.7.0/notebooks/08_RAG_Langchain_with_Local_NIM.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more information, refer to the &lt;a href=&#34;https://github.com/NVIDIA/GenerativeAIExamples/releases/&#34;&gt;Generative AI Example releases&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Vision NIM Workflows&lt;/h3&gt; &#xA;&lt;p&gt;A collection of Jupyter notebooks, sample code and reference applications built with Vision NIMs.&lt;/p&gt; &#xA;&lt;p&gt;To pull the vision NIM workflows, clone this repository recursively:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/nvidia/GenerativeAIExamples --recurse-submodules&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The workflows will then be located at &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/vision_workflows/README.md&#34;&gt;GenerativeAIExamples/vision_workflows&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Follow the links below to learn more:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/nim_workflows/vlm_alerts/README.md&#34;&gt;Learn how to use VLMs to automatically monitor a video stream for custom events.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/nim_workflows/nvclip_multimodal_search/README.md&#34;&gt;Learn how to search images with natural language using NV-CLIP.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/nim_workflows/vision_text_extraction/README.md&#34;&gt;Learn how to combine VLMs, LLMs and CV models to build a robust text extraction pipeline.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/nim_workflows/nvdinov2_few_shot/README.md&#34;&gt;Learn how to use embeddings with NVDINOv2 and a Milvus VectorDB to build a few shot classification model.&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Try it Now!&lt;/h2&gt; &#xA;&lt;p&gt;Experience NVIDIA RAG Pipelines with just a few steps!&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Get your NVIDIA API key.&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Go to the &lt;a href=&#34;https://build.ngc.nvidia.com/explore/&#34;&gt;NVIDIA API Catalog&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Select any model.&lt;/li&gt; &#xA;   &lt;li&gt;Click &lt;strong&gt;Get API Key&lt;/strong&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Run: &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;export NVIDIA_API_KEY=nvapi-...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;git clone https://github.com/nvidia/GenerativeAIExamples.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build and run the basic RAG pipeline.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;cd GenerativeAIExamples/RAG/examples/basic_rag/langchain/&#xA;docker compose up -d --build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Go to &lt;a href=&#34;https://localhost:8090/&#34;&gt;https://localhost:8090/&lt;/a&gt; and submit queries to the sample RAG Playground.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Stop containers when done.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;docker compose down&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Data Flywheel&lt;/h2&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://www.nvidia.com/en-us/glossary/data-flywheel/&#34;&gt;Data Flywheel&lt;/a&gt; is a self-reinforcing cycle where user interactions generate data that improves AI models or products, leading to better outcomes that attract more users and further enhance data quality. This feedback loop relies on continuous data processing, model refinement, and guardrails to ensure accuracy and compliance while compounding value over time. Real-world applications range from personalized customer experiences to operational systems like inventory management, where improved predictions drive efficiency and growth.&lt;/p&gt; &#xA;&lt;h3&gt;Tool-Calling Notebooks&lt;/h3&gt; &#xA;&lt;p&gt;Tool calling empowers Large Language Models (LLMs) to integrate with external APIs, execute dynamic workflows, and retrieve real-time data beyond their training scope. The NVIDIA NeMo microservices platform offers a modular infrastructure for deploying AI pipelines that includes fine-tuning, evaluation, inference, and guardrail enforcement—across Kubernetes clusters in cloud or on-premises environments.&lt;/p&gt; &#xA;&lt;p&gt;This end-to-end &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/nemo/data-flywheel/tool-calling&#34;&gt;tutorial&lt;/a&gt; demonstrates how to leverage NeMo Microservices to customize &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct&#34;&gt;Llama-3.2-1B-Instruct&lt;/a&gt; by using the &lt;a href=&#34;https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k&#34;&gt;xLAM&lt;/a&gt; function-calling dataset, assess its accuracy, and implement safety constraints to govern its behavior.&lt;/p&gt; &#xA;&lt;h2&gt;RAG&lt;/h2&gt; &#xA;&lt;h3&gt;RAG Notebooks&lt;/h3&gt; &#xA;&lt;p&gt;NVIDIA has first-class support for popular generative AI developer frameworks like &lt;a href=&#34;https://python.langchain.com/v0.2/docs/integrations/chat/nvidia_ai_endpoints/&#34;&gt;LangChain&lt;/a&gt;, &lt;a href=&#34;https://docs.llamaindex.ai/en/stable/examples/llm/nvidia/&#34;&gt;LlamaIndex&lt;/a&gt;, and &lt;a href=&#34;https://haystack.deepset.ai/integrations/nvidia&#34;&gt;Haystack&lt;/a&gt;. These end-to-end notebooks show how to integrate NIM microservices using your preferred generative AI development framework.&lt;/p&gt; &#xA;&lt;p&gt;Use these &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/notebooks/README.md&#34;&gt;notebooks&lt;/a&gt; to learn about the LangChain and LlamaIndex connectors.&lt;/p&gt; &#xA;&lt;h4&gt;LangChain Notebooks&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;RAG &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/notebooks/langchain/langchain_basic_RAG.ipynb&#34;&gt;Basic RAG with CHATNVIDIA LangChain Integration&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/notebooks/langchain/RAG_Langchain_with_Local_NIM.ipynb&#34;&gt;RAG using local NIM microservices for LLMs and Retrieval&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/notebooks/langchain/RAG_for_HTML_docs_with_Langchain_NVIDIA_AI_Endpoints.ipynb&#34;&gt;RAG for HTML Documents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/notebooks/langchain/Chat_with_nvidia_financial_reports.ipynb&#34;&gt;Chat with NVIDIA Financial Reports&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Agents &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain-nvidia/raw/main/cookbook/nvidia_nim_agents_llama3.1.ipynb&#34;&gt;NIM Tool Calling 101&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/notebooks/langchain/agentic_rag_with_nemo_retriever_nim.ipynb&#34;&gt;Agentic RAG with NeMo Retriever&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/notebooks/langchain/LangGraph_HandlingAgent_IntermediateSteps.ipynb&#34;&gt;Agents with Human in the Loop&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;LlamaIndex Notebooks&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/notebooks/llamaindex/llamaindex_basic_RAG.ipynb&#34;&gt;Basic RAG with LlamaIndex Integration&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RAG Examples&lt;/h3&gt; &#xA;&lt;p&gt;By default, these end-to-end &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/examples/README.md&#34;&gt;examples&lt;/a&gt; use preview NIM endpoints on &lt;a href=&#34;https://catalog.ngc.nvidia.com&#34;&gt;NVIDIA API Catalog&lt;/a&gt;. Alternatively, you can run any of the examples &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/examples/local_deploy/&#34;&gt;on premises&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Basic RAG Examples&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/examples/basic_rag/langchain/README.md&#34;&gt;LangChain example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/examples/basic_rag/llamaindex/README.md&#34;&gt;LlamaIndex example&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Advanced RAG Examples&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/examples/advanced_rag/multi_turn_rag/README.md&#34;&gt;Multi-Turn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/examples/advanced_rag/multimodal_rag/README.md&#34;&gt;Multimodal Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/examples/advanced_rag/structured_data_rag/README.md&#34;&gt;Structured Data&lt;/a&gt; (CSV)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/examples/advanced_rag/query_decomposition_rag/README.md&#34;&gt;Query Decomposition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RAG Tools&lt;/h3&gt; &#xA;&lt;p&gt;Example tools and tutorials to enhance LLM development and productivity when using NVIDIA RAG pipelines.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/tools/evaluation/README.md&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/RAG/tools/observability/README.md&#34;&gt;Observability&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RAG Projects&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/ace/latest/workflows/tokkio/text/Tokkio_LLM_RAG_Bot.html&#34;&gt;NVIDIA Tokkio LLM-RAG&lt;/a&gt;: Use Tokkio to add avatar animation for RAG responses.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/workbench-example-hybrid-rag&#34;&gt;Hybrid RAG Project on AI Workbench&lt;/a&gt;: Run an NVIDIA AI Workbench example project for RAG.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/common-prerequisites.md&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How To&#39;s&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/change-model.md&#34;&gt;Changing the Inference or Embedded Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/vector-database.md&#34;&gt;Customizing the Vector Database&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/chain-server.md&#34;&gt;Customizing the Chain Server&lt;/a&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/text-splitter.md&#34;&gt;Chunking Strategy&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/prompt-customization.md&#34;&gt;Prompting Template Engineering&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/llm-params.md&#34;&gt;Configuring LLM Parameters at Runtime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/multiturn.md&#34;&gt;Supporting Multi-Turn Conversations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/riva-asr-tts.md&#34;&gt;Speaking Queries and Listening to Responses with NVIDIA Riva&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Reference&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/support-matrix.md&#34;&gt;Support Matrix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/architecture.md&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/using-sample-web-application.md&#34;&gt;Using the Sample Chat Web Application&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/frontend.md&#34;&gt;RAG Playground Web Application&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/configuration.md&#34;&gt;Software Component Configuration&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re posting these examples on GitHub to support the NVIDIA LLM community and facilitate feedback. We invite contributions! Open a GitHub issue or pull request! See &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/docs/contributing.md&#34;&gt;contributing&lt;/a&gt; Check out the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/community/README.md&#34;&gt;community&lt;/a&gt; examples and notebooks.&lt;/p&gt;</summary>
  </entry>
</feed>