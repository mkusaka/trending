<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-01T02:43:39Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Pierian-Data/Complete-Python-3-Bootcamp</title>
    <updated>2022-07-01T02:43:39Z</updated>
    <id>tag:github.com,2022-07-01:/Pierian-Data/Complete-Python-3-Bootcamp</id>
    <link href="https://github.com/Pierian-Data/Complete-Python-3-Bootcamp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Complete-Python-3-Bootcamp&lt;/h1&gt; &#xA;&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt; &#xA;&lt;p&gt;Copyright(©) by Pierian Data Inc.&lt;/p&gt; &#xA;&lt;p&gt;Get it now for 95% off with the link: &lt;a href=&#34;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&#34;&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thanks!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>snakers4/silero-models</title>
    <updated>2022-07-01T02:43:39Z</updated>
    <id>tag:github.com,2022-07-01:/snakers4/silero-models</id>
    <link href="https://github.com/snakers4/silero-models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Silero Models: pre-trained speech-to-text, text-to-speech and text-enhancement models made embarrassingly simple&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;mailto:hello@silero.ai&#34;&gt;&lt;img src=&#34;http://img.shields.io/badge/Email-gray.svg?style=for-the-badge&amp;amp;logo=gmail&#34; alt=&#34;Mailing list : test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://t.me/silero_speech&#34;&gt;&lt;img src=&#34;http://img.shields.io/badge/Telegram-blue.svg?style=for-the-badge&amp;amp;logo=telegram&#34; alt=&#34;Mailing list : test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/snakers4/silero-models/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg?style=for-the-badge&#34; alt=&#34;License: CC BY-NC 4.0&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opencollective.com/open_stt&#34;&gt;&lt;img src=&#34;https://opencollective.com/open_stt/tiers/donation/badge.svg?label=donations&amp;amp;color=brightgreen&#34; alt=&#34;Donations&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/open_stt&#34;&gt;&lt;img src=&#34;https://opencollective.com/open_stt/tiers/backer/badge.svg?label=backers&amp;amp;color=brightgreen&#34; alt=&#34;Backers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/open_stt&#34;&gt;&lt;img src=&#34;https://opencollective.com/open_stt/tiers/sponsor/badge.svg?label=sponsors&amp;amp;color=brightgreen&#34; alt=&#34;Sponsors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/snakers4/silero-models/actions/workflows/build_deploy.yml&#34;&gt;&lt;img src=&#34;https://github.com/snakers4/silero-models/actions/workflows/build_deploy.yml/badge.svg?sanitize=true&#34; alt=&#34;Build and Deploy to PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/silero&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/silero.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/12515440/89997349-b3523080-dc94-11ea-9906-ca2e8bc50535.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#silero-models&#34;&gt;Silero Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#installation-and-basics&#34;&gt;Installation and Basics&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#speech-to-text&#34;&gt;Speech-To-Text&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#dependencies&#34;&gt;Dependencies&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#pytorch&#34;&gt;PyTorch&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#onnx&#34;&gt;ONNX&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#tensorflow&#34;&gt;TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#text-to-speech&#34;&gt;Text-To-Speech&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#models-and-speakers&#34;&gt;Models and Speakers&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#dependencies-1&#34;&gt;Dependencies&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#pytorch-1&#34;&gt;PyTorch&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#standalone-use&#34;&gt;Standalone Use&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#SSML&#34;&gt;SSML&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#indic-languages&#34;&gt;Indic languages&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#text-enhancement&#34;&gt;Text-Enhancement&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#dependencies-2&#34;&gt;Dependencies&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#standalone-use-1&#34;&gt;Standalone Use&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#faq&#34;&gt;FAQ&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#wiki&#34;&gt;Wiki&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#performance-and-quality&#34;&gt;Performance and Quality&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#adding-new-languages&#34;&gt;Adding new Languages&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#contact&#34;&gt;Contact&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#get-in-touch&#34;&gt;Get in Touch&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#commercial-inquiries&#34;&gt;Commercial Inquiries&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#citations&#34;&gt;Citations&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#further-reading&#34;&gt;Further reading&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#english&#34;&gt;English&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#chinese&#34;&gt;Chinese&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#russian&#34;&gt;Russian&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#donations&#34;&gt;Donations&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Silero Models&lt;/h1&gt; &#xA;&lt;p&gt;Silero Models: pre-trained enterprise-grade STT / TTS models and benchmarks.&lt;/p&gt; &#xA;&lt;p&gt;Enterprise-grade STT made refreshingly simple (seriously, see &lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/Quality-Benchmarks&#34;&gt;benchmarks&lt;/a&gt;). We provide quality comparable to Google&#39;s STT (and sometimes even better) and we are not Google.&lt;/p&gt; &#xA;&lt;p&gt;As a bonus:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No Kaldi;&lt;/li&gt; &#xA; &lt;li&gt;No compilation;&lt;/li&gt; &#xA; &lt;li&gt;No 20-step instructions;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also we have published TTS models that satisfy the following criteria:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;One-line usage;&lt;/li&gt; &#xA; &lt;li&gt;A large library of voices;&lt;/li&gt; &#xA; &lt;li&gt;A fully end-to-end pipeline;&lt;/li&gt; &#xA; &lt;li&gt;Natural-sounding speech;&lt;/li&gt; &#xA; &lt;li&gt;No GPU or training required;&lt;/li&gt; &#xA; &lt;li&gt;Minimalism and lack of dependencies;&lt;/li&gt; &#xA; &lt;li&gt;Faster than real-time on one CPU thread (!!!);&lt;/li&gt; &#xA; &lt;li&gt;Support for 16kHz and 8kHz out of the box;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also we have published a model for text repunctuation and recapitalization that:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inserts capital letters and basic punctuation marks, e.g., dots, commas, hyphens, question marks, exclamation points, and dashes (for Russian);&lt;/li&gt; &#xA; &lt;li&gt;Works for 4 languages (Russian, English, German, and Spanish) and can be extended;&lt;/li&gt; &#xA; &lt;li&gt;Domain-agnostic by design and not based on any hard-coded rules;&lt;/li&gt; &#xA; &lt;li&gt;Has non-trivial metrics and succeeds in the task of improving text readability;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation and Basics&lt;/h2&gt; &#xA;&lt;p&gt;You can basically use our models in 3 flavours:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Via PyTorch Hub: &lt;code&gt;torch.hub.load()&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Via pip: &lt;code&gt;pip install silero&lt;/code&gt; and then &lt;code&gt;import silero&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Via caching the required models and utils manually and modifying if necessary;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Models are downloaded on demand both by pip and PyTorch Hub. If you need caching, do it manually or via invoking a necessary model once (it will be downloaded to a cache folder). Please see these &lt;a href=&#34;https://pytorch.org/docs/stable/hub.html#loading-models-from-hub&#34;&gt;docs&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;PyTorch Hub and pip package are based on the same code. All of the &lt;code&gt;torch.hub.load&lt;/code&gt; examples can be used with the pip package via this basic change:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python3&#34;&gt;# before&#xA;torch.hub.load(repo_or_dir=&#39;snakers4/silero-models&#39;,&#xA;               model=&#39;silero_stt&#39;,  # or silero_tts or silero_te&#xA;               **kwargs)&#xA;&#xA;# after&#xA;from silero import silero_stt, silero_tts, silero_te&#xA;silero_stt(**kwargs)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Speech-To-Text&lt;/h2&gt; &#xA;&lt;p&gt;All of the provided models are listed in the &lt;a href=&#34;https://github.com/snakers4/silero-models/raw/master/models.yml&#34;&gt;models.yml&lt;/a&gt; file. Any metadata and newer versions will be added there.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/36505480/132320823-f0c5b774-44f7-4375-9c46-3acbcc548b76.png&#34; alt=&#34;Screenshot_1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Currently we provide the following checkpoints:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;PyTorch&lt;/th&gt; &#xA;   &lt;th&gt;ONNX&lt;/th&gt; &#xA;   &lt;th&gt;Quantization&lt;/th&gt; &#xA;   &lt;th&gt;Quality&lt;/th&gt; &#xA;   &lt;th&gt;Colab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;English (&lt;code&gt;en_v6&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/Quality-Benchmarks#en-v6&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;English (&lt;code&gt;en_v5&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/Quality-Benchmarks#en-v5&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;German (&lt;code&gt;de_v4&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;⌛&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/Quality-Benchmarks#de-v4&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;English (&lt;code&gt;en_v3&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/Quality-Benchmarks#en-v3&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;German (&lt;code&gt;de_v3&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;⌛&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;⌛&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/Quality-Benchmarks#de-v3&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;German (&lt;code&gt;de_v1&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;⌛&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/Quality-Benchmarks#de-v1&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spanish (&lt;code&gt;es_v1&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;⌛&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/Quality-Benchmarks#es-v1&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ukrainian (&lt;code&gt;ua_v3&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Model flavours:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;jit&lt;/th&gt; &#xA;   &lt;th&gt;jit&lt;/th&gt; &#xA;   &lt;th&gt;jit&lt;/th&gt; &#xA;   &lt;th&gt;jit&lt;/th&gt; &#xA;   &lt;th&gt;jit_q&lt;/th&gt; &#xA;   &lt;th&gt;jit_q&lt;/th&gt; &#xA;   &lt;th&gt;onnx&lt;/th&gt; &#xA;   &lt;th&gt;onnx&lt;/th&gt; &#xA;   &lt;th&gt;onnx&lt;/th&gt; &#xA;   &lt;th&gt;onnx&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;xsmall&lt;/td&gt; &#xA;   &lt;td&gt;small&lt;/td&gt; &#xA;   &lt;td&gt;large&lt;/td&gt; &#xA;   &lt;td&gt;xlarge&lt;/td&gt; &#xA;   &lt;td&gt;xsmall&lt;/td&gt; &#xA;   &lt;td&gt;small&lt;/td&gt; &#xA;   &lt;td&gt;xsmall&lt;/td&gt; &#xA;   &lt;td&gt;small&lt;/td&gt; &#xA;   &lt;td&gt;large&lt;/td&gt; &#xA;   &lt;td&gt;xlarge&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;English &lt;code&gt;en_v6&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;English &lt;code&gt;en_v5&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;English &lt;code&gt;en_v4_0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;English &lt;code&gt;en_v3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;German &lt;code&gt;de_v4&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;German &lt;code&gt;de_v3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;German &lt;code&gt;de_v1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spanish &lt;code&gt;es_v1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ukrainian &lt;code&gt;ua_v3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;torch&lt;/code&gt;, 1.8+ (used to clone the repo in TensorFlow and ONNX examples), breaking changes for versions older than 1.6&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;torchaudio&lt;/code&gt;, latest version bound to PyTorch should just work&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;omegaconf&lt;/code&gt;, latest should just work&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Additional dependencies for ONNX examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;onnx&lt;/code&gt;, latest should just work&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;onnxruntime&lt;/code&gt;, latest should just work&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Additional for TensorFlow examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;tensorflow&lt;/code&gt;, latest should just work&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;tensorflow_hub&lt;/code&gt;, latest should just work&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please see the provided Colab for details for each example below. All examples are maintained to work with the latest major packaged versions of the installed libraries.&lt;/p&gt; &#xA;&lt;h3&gt;PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/hub/snakers4_silero-models_stt/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Torch-Hub-red?logo=pytorch&amp;amp;style=for-the-badge&#34; alt=&#34;Open on Torch Hub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import zipfile&#xA;import torchaudio&#xA;from glob import glob&#xA;&#xA;device = torch.device(&#39;cpu&#39;)  # gpu also works, but our models are fast enough for CPU&#xA;model, decoder, utils = torch.hub.load(repo_or_dir=&#39;snakers4/silero-models&#39;,&#xA;                                       model=&#39;silero_stt&#39;,&#xA;                                       language=&#39;en&#39;, # also available &#39;de&#39;, &#39;es&#39;&#xA;                                       device=device)&#xA;(read_batch, split_into_batches,&#xA; read_audio, prepare_model_input) = utils  # see function signature for details&#xA;&#xA;# download a single file in any format compatible with TorchAudio&#xA;torch.hub.download_url_to_file(&#39;https://opus-codec.org/static/examples/samples/speech_orig.wav&#39;,&#xA;                               dst =&#39;speech_orig.wav&#39;, progress=True)&#xA;test_files = glob(&#39;speech_orig.wav&#39;)&#xA;batches = split_into_batches(test_files, batch_size=10)&#xA;input = prepare_model_input(read_batch(batches[0]),&#xA;                            device=device)&#xA;&#xA;output = model(input)&#xA;for example in output:&#xA;    print(decoder(example.cpu()))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ONNX&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our model will run anywhere that can import the ONNX model or that supports the ONNX runtime.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import onnx&#xA;import torch&#xA;import onnxruntime&#xA;from omegaconf import OmegaConf&#xA;&#xA;language = &#39;en&#39; # also available &#39;de&#39;, &#39;es&#39;&#xA;&#xA;# load provided utils&#xA;_, decoder, utils = torch.hub.load(repo_or_dir=&#39;snakers4/silero-models&#39;, model=&#39;silero_stt&#39;, language=language)&#xA;(read_batch, split_into_batches,&#xA; read_audio, prepare_model_input) = utils&#xA;&#xA;# see available models&#xA;torch.hub.download_url_to_file(&#39;https://raw.githubusercontent.com/snakers4/silero-models/master/models.yml&#39;, &#39;models.yml&#39;)&#xA;models = OmegaConf.load(&#39;models.yml&#39;)&#xA;available_languages = list(models.stt_models.keys())&#xA;assert language in available_languages&#xA;&#xA;# load the actual ONNX model&#xA;torch.hub.download_url_to_file(models.stt_models.en.latest.onnx, &#39;model.onnx&#39;, progress=True)&#xA;onnx_model = onnx.load(&#39;model.onnx&#39;)&#xA;onnx.checker.check_model(onnx_model)&#xA;ort_session = onnxruntime.InferenceSession(&#39;model.onnx&#39;)&#xA;&#xA;# download a single file in any format compatible with TorchAudio&#xA;torch.hub.download_url_to_file(&#39;https://opus-codec.org/static/examples/samples/speech_orig.wav&#39;, dst =&#39;speech_orig.wav&#39;, progress=True)&#xA;test_files = [&#39;speech_orig.wav&#39;]&#xA;batches = split_into_batches(test_files, batch_size=10)&#xA;input = prepare_model_input(read_batch(batches[0]))&#xA;&#xA;# actual ONNX inference and decoding&#xA;onnx_input = input.detach().cpu().numpy()&#xA;ort_inputs = {&#39;input&#39;: onnx_input}&#xA;ort_outs = ort_session.run(None, ort_inputs)&#xA;decoded = decoder(torch.Tensor(ort_outs[0])[0])&#xA;print(decoded)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;TensorFlow&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SavedModel example&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import torch&#xA;import subprocess&#xA;import tensorflow as tf&#xA;import tensorflow_hub as tf_hub&#xA;from omegaconf import OmegaConf&#xA;&#xA;language = &#39;en&#39; # also available &#39;de&#39;, &#39;es&#39;&#xA;&#xA;# load provided utils using torch.hub for brevity&#xA;_, decoder, utils = torch.hub.load(repo_or_dir=&#39;snakers4/silero-models&#39;, model=&#39;silero_stt&#39;, language=language)&#xA;(read_batch, split_into_batches,&#xA; read_audio, prepare_model_input) = utils&#xA;&#xA;# see available models&#xA;torch.hub.download_url_to_file(&#39;https://raw.githubusercontent.com/snakers4/silero-models/master/models.yml&#39;, &#39;models.yml&#39;)&#xA;models = OmegaConf.load(&#39;models.yml&#39;)&#xA;available_languages = list(models.stt_models.keys())&#xA;assert language in available_languages&#xA;&#xA;# load the actual tf model&#xA;torch.hub.download_url_to_file(models.stt_models.en.latest.tf, &#39;tf_model.tar.gz&#39;)&#xA;subprocess.run(&#39;rm -rf tf_model &amp;amp;&amp;amp; mkdir tf_model &amp;amp;&amp;amp; tar xzfv tf_model.tar.gz -C tf_model&#39;,  shell=True, check=True)&#xA;tf_model = tf.saved_model.load(&#39;tf_model&#39;)&#xA;&#xA;# download a single file in any format compatible with TorchAudio&#xA;torch.hub.download_url_to_file(&#39;https://opus-codec.org/static/examples/samples/speech_orig.wav&#39;, dst =&#39;speech_orig.wav&#39;, progress=True)&#xA;test_files = [&#39;speech_orig.wav&#39;]&#xA;batches = split_into_batches(test_files, batch_size=10)&#xA;input = prepare_model_input(read_batch(batches[0]))&#xA;&#xA;# tf inference&#xA;res = tf_model.signatures[&#34;serving_default&#34;](tf.constant(input.numpy()))[&#39;output_0&#39;]&#xA;print(decoder(torch.Tensor(res.numpy())[0]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Text-To-Speech&lt;/h2&gt; &#xA;&lt;h3&gt;Models and Speakers&lt;/h3&gt; &#xA;&lt;p&gt;All of the provided models are listed in the &lt;a href=&#34;https://github.com/snakers4/silero-models/raw/master/models.yml&#34;&gt;models.yml&lt;/a&gt; file. Any metadata and newer versions will be added there.&lt;/p&gt; &#xA;&lt;h4&gt;V3&lt;/h4&gt; &#xA;&lt;p&gt;V3 models support &lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/SSML&#34;&gt;SSML&lt;/a&gt;. Also see Colab examples for main SSML tag usage.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ID&lt;/th&gt; &#xA;   &lt;th&gt;Speakers&lt;/th&gt; &#xA;   &lt;th&gt;Auto-stress&lt;/th&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;SR&lt;/th&gt; &#xA;   &lt;th&gt;Colab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v3_1_ru&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;aidar&lt;/code&gt;, &lt;code&gt;baya&lt;/code&gt;, &lt;code&gt;kseniya&lt;/code&gt;, &lt;code&gt;xenia&lt;/code&gt;, &lt;code&gt;eugene&lt;/code&gt;, &lt;code&gt;random&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ru&lt;/code&gt; (Russian)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;, &lt;code&gt;24000&lt;/code&gt;, &lt;code&gt;48000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v3_en&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;en_0&lt;/code&gt;, &lt;code&gt;en_1&lt;/code&gt;, ..., &lt;code&gt;en_117&lt;/code&gt;, &lt;code&gt;random&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;en&lt;/code&gt; (English)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;, &lt;code&gt;24000&lt;/code&gt;, &lt;code&gt;48000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v3_en_indic&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tamil_female&lt;/code&gt;, ..., &lt;code&gt;assamese_male&lt;/code&gt;, &lt;code&gt;random&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;en&lt;/code&gt; (English)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;, &lt;code&gt;24000&lt;/code&gt;, &lt;code&gt;48000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v3_de&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;eva_k&lt;/code&gt;, ..., &lt;code&gt;karlsson&lt;/code&gt;, &lt;code&gt;random&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;de&lt;/code&gt; (German)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;, &lt;code&gt;24000&lt;/code&gt;, &lt;code&gt;48000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v3_es&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;es_0&lt;/code&gt;, &lt;code&gt;es_1&lt;/code&gt;, &lt;code&gt;es_2&lt;/code&gt;, &lt;code&gt;random&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;es&lt;/code&gt; (Spanish)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;, &lt;code&gt;24000&lt;/code&gt;, &lt;code&gt;48000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v3_fr&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fr_0&lt;/code&gt;, ..., &lt;code&gt;fr_5&lt;/code&gt;, &lt;code&gt;random&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fr&lt;/code&gt; (French)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;, &lt;code&gt;24000&lt;/code&gt;, &lt;code&gt;48000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v3_tt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;dilyara&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tt&lt;/code&gt; (Tatar)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;, &lt;code&gt;24000&lt;/code&gt;, &lt;code&gt;48000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v3_ua&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;mykyta&lt;/code&gt;, &lt;code&gt;random&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ua&lt;/code&gt; (Ukrainian)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;, &lt;code&gt;24000&lt;/code&gt;, &lt;code&gt;48000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v3_uz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;dilnavoz&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;uz&lt;/code&gt; (Uzbek)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;, &lt;code&gt;24000&lt;/code&gt;, &lt;code&gt;48000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;v3_xal&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;erdni&lt;/code&gt;, &lt;code&gt;delghir&lt;/code&gt;, &lt;code&gt;random&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;xal&lt;/code&gt; (Kalmyk)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;, &lt;code&gt;24000&lt;/code&gt;, &lt;code&gt;48000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#indic-languages&#34;&gt;&lt;code&gt;v3_indic&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;hindi_male&lt;/code&gt;, &lt;code&gt;hindi_female&lt;/code&gt;, ..., &lt;code&gt;random&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;indic&lt;/code&gt; &lt;a href=&#34;https://raw.githubusercontent.com/snakers4/silero-models/master/#indic-languages&#34;&gt;(Hindi, Telugu, ...)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;, &lt;code&gt;24000&lt;/code&gt;, &lt;code&gt;48000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;ru_v3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;aidar&lt;/code&gt;, &lt;code&gt;baya&lt;/code&gt;, &lt;code&gt;kseniya&lt;/code&gt;, &lt;code&gt;xenia&lt;/code&gt;, &lt;code&gt;random&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ru&lt;/code&gt; (Russian)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;, &lt;code&gt;24000&lt;/code&gt;, &lt;code&gt;48000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Basic dependencies for Colab examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;torch&lt;/code&gt;, 1.10+;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;torchaudio&lt;/code&gt;, latest version bound to PyTorch should work (required only because models are hosted together with STT, not required for work);&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;omegaconf&lt;/code&gt;, latest (can be removed as well, if you do not load all of the configs);&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/hub/snakers4_silero-models_tts/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Torch-Hub-red?logo=pytorch&amp;amp;style=for-the-badge&#34; alt=&#34;Open on Torch Hub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# V3&#xA;import torch&#xA;&#xA;language = &#39;ru&#39;&#xA;model_id = &#39;v3_1_ru&#39;&#xA;sample_rate = 48000&#xA;speaker = &#39;xenia&#39;&#xA;device = torch.device(&#39;cpu&#39;)&#xA;&#xA;model, example_text = torch.hub.load(repo_or_dir=&#39;snakers4/silero-models&#39;,&#xA;                                     model=&#39;silero_tts&#39;,&#xA;                                     language=language,&#xA;                                     speaker=model_id)&#xA;model.to(device)  # gpu or cpu&#xA;&#xA;audio = model.apply_tts(text=example_text,&#xA;                        speaker=speaker,&#xA;                        sample_rate=sample_rate)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Standalone Use&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Standalone usage only requires PyTorch 1.10+ and the Python Standard Library;&lt;/li&gt; &#xA; &lt;li&gt;Please see the detailed examples in Colab;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# V3&#xA;import os&#xA;import torch&#xA;&#xA;device = torch.device(&#39;cpu&#39;)&#xA;torch.set_num_threads(4)&#xA;local_file = &#39;model.pt&#39;&#xA;&#xA;if not os.path.isfile(local_file):&#xA;    torch.hub.download_url_to_file(&#39;https://models.silero.ai/models/tts/ru/v3_1_ru.pt&#39;,&#xA;                                   local_file)  &#xA;&#xA;model = torch.package.PackageImporter(local_file).load_pickle(&#34;tts_models&#34;, &#34;model&#34;)&#xA;model.to(device)&#xA;&#xA;example_text = &#39;В недрах тундры выдры в г+етрах т+ырят в вёдра ядра кедров.&#39;&#xA;sample_rate = 48000&#xA;speaker=&#39;baya&#39;&#xA;&#xA;audio_paths = model.save_wav(text=example_text,&#xA;                             speaker=speaker,&#xA;                             sample_rate=sample_rate)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SSML&lt;/h3&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/SSML&#34;&gt;TTS Wiki page.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Indic languages&lt;/h3&gt; &#xA;&lt;h4&gt;Example&lt;/h4&gt; &#xA;&lt;p&gt;(!!!) All input sentences should be romanized to ISO format using &lt;a href=&#34;https://aksharamukha.appspot.com/python&#34;&gt;&lt;code&gt;aksharamukha&lt;/code&gt;&lt;/a&gt;. An example for &lt;code&gt;hindi&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# V3&#xA;import torch&#xA;from aksharamukha import transliterate&#xA;&#xA;# Loading model&#xA;model, example_text = torch.hub.load(repo_or_dir=&#39;snakers4/silero-models&#39;,&#xA;                                     model=&#39;silero_tts&#39;,&#xA;                                     language=&#39;indic&#39;,&#xA;                                     speaker=&#39;v3_indic&#39;)&#xA;&#xA;orig_text = &#34;प्रसिद्द कबीर अध्येता, पुरुषोत्तम अग्रवाल का यह शोध आलेख, उस रामानंद की खोज करता है&#34;&#xA;roman_text = transliterate.process(&#39;Devanagari&#39;, &#39;ISO&#39;, orig_text)&#xA;print(roman_text)&#xA;&#xA;audio = model.apply_tts(roman_text,&#xA;                        speaker=&#39;hindi_male&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Supported languages&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Speakers&lt;/th&gt; &#xA;   &lt;th&gt;Romanization function&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hindi&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;hindi_female&lt;/code&gt;, &lt;code&gt;hindi_male&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;transliterate.process(&#39;Devanagari&#39;, &#39;ISO&#39;, orig_text)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;malayalam&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;malayalam_female&lt;/code&gt;, &lt;code&gt;malayalam_male&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;transliterate.process(&#39;Malayalam&#39;, &#39;ISO&#39;, orig_text)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;manipuri&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;manipuri_female&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;transliterate.process(&#39;Bengali&#39;, &#39;ISO&#39;, orig_text)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bengali&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;bengali_female&lt;/code&gt;, &lt;code&gt;bengali_male&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;transliterate.process(&#39;Bengali&#39;, &#39;ISO&#39;, orig_text)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;rajasthani&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;rajasthani_female&lt;/code&gt;, &lt;code&gt;rajasthani_female&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;transliterate.process(&#39;Devanagari&#39;, &#39;ISO&#39;, orig_text)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tamil&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tamil_female&lt;/code&gt;, &lt;code&gt;tamil_male&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;transliterate.process(&#39;Tamil&#39;, &#39;ISO&#39;, orig_text, pre_options=[&#39;TamilTranscribe&#39;])&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;telugu&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;telugu_female&lt;/code&gt;, &lt;code&gt;telugu_male&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;transliterate.process(&#39;Telugu&#39;, &#39;ISO&#39;, orig_text)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gujarati&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;gujarati_female&lt;/code&gt;, &lt;code&gt;gujarati_male&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;transliterate.process(&#39;Gujarati&#39;, &#39;ISO&#39;, orig_text)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kannada&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;kannada_female&lt;/code&gt;, &lt;code&gt;kannada_male&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;transliterate.process(&#39;Kannada&#39;, &#39;ISO&#39;, orig_text)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Text-Enhancement&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Languages&lt;/th&gt; &#xA;   &lt;th&gt;Quantization&lt;/th&gt; &#xA;   &lt;th&gt;Quality&lt;/th&gt; &#xA;   &lt;th&gt;Colab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#39;en&#39;, &#39;de&#39;, &#39;ru&#39;, &#39;es&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/Quality-Benchmarks#te-models&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_te.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Basic dependencies for Colab examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;torch&lt;/code&gt;, 1.9+;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pyyaml&lt;/code&gt;, but it&#39;s installed with torch itself&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Standalone Use&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Standalone usage only requires PyTorch 1.9+ and the Python Standard Library;&lt;/li&gt; &#xA; &lt;li&gt;Please see the detailed examples in &lt;a href=&#34;https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_te.ipynb&#34;&gt;Colab&lt;/a&gt;;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;model, example_texts, languages, punct, apply_te = torch.hub.load(repo_or_dir=&#39;snakers4/silero-models&#39;,&#xA;                                                                  model=&#39;silero_te&#39;)&#xA;&#xA;input_text = input(&#39;Enter input text\n&#39;)&#xA;apply_te(input_text, lan=&#39;en&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Wiki&lt;/h3&gt; &#xA;&lt;p&gt;Also check out our &lt;a href=&#34;https://github.com/snakers4/silero-models/wiki&#34;&gt;wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Performance and Quality&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to these wiki sections:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/Quality-Benchmarks&#34;&gt;Quality Benchmarks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/Performance-Benchmarks&#34;&gt;Performance Benchmarks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Adding new Languages&lt;/h3&gt; &#xA;&lt;p&gt;Please refer &lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/Adding-New-Languages&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;h3&gt;Get in Touch&lt;/h3&gt; &#xA;&lt;p&gt;Try our models, create an &lt;a href=&#34;https://github.com/snakers4/silero-models/issues/new&#34;&gt;issue&lt;/a&gt;, join our &lt;a href=&#34;https://t.me/silero_speech&#34;&gt;chat&lt;/a&gt;, &lt;a href=&#34;mailto:hello@silero.ai&#34;&gt;email&lt;/a&gt; us, and read the latest &lt;a href=&#34;https://t.me/silero_news&#34;&gt;news&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Commercial Inquiries&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://github.com/snakers4/silero-models/wiki&#34;&gt;wiki&lt;/a&gt; and the &lt;a href=&#34;https://github.com/snakers4/silero-models/wiki/Licensing-and-Tiers&#34;&gt;Licensing and Tiers&lt;/a&gt; page for relevant information, and &lt;a href=&#34;mailto:hello@silero.ai&#34;&gt;email&lt;/a&gt; us.&lt;/p&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{Silero Models,&#xA;  author = {Silero Team},&#xA;  title = {Silero Models: pre-trained enterprise-grade STT / TTS models and benchmarks},&#xA;  year = {2021},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/snakers4/silero-models}},&#xA;  commit = {insert_some_commit_here},&#xA;  email = {hello@silero.ai}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Further reading&lt;/h2&gt; &#xA;&lt;h3&gt;English&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;STT:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Towards an Imagenet Moment For Speech-To-Text - &lt;a href=&#34;https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;A Speech-To-Text Practitioners Criticisms of Industry and Academia - &lt;a href=&#34;https://thegradient.pub/a-speech-to-text-practitioners-criticisms-of-industry-and-academia/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Modern Google-level STT Models Released - &lt;a href=&#34;https://habr.com/ru/post/519562/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;TTS:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Multilingual Text-to-Speech Models for Indic Languages - &lt;a href=&#34;https://www.analyticsvidhya.com/blog/2022/06/multilingual-text-to-speech-models-for-indic-languages/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Our new public speech synthesis in super-high quality, 10x faster and more stable - &lt;a href=&#34;https://habr.com/ru/post/660571/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;High-Quality Text-to-Speech Made Accessible, Simple and Fast - &lt;a href=&#34;https://habr.com/ru/post/549482/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;VAD:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;One Voice Detector to Rule Them All - &lt;a href=&#34;https://thegradient.pub/one-voice-detector-to-rule-them-all/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Modern Portable Voice Activity Detector Released - &lt;a href=&#34;https://habr.com/ru/post/537276/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Text Enhancement:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We have published a model for text repunctuation and recapitalization for four languages - &lt;a href=&#34;https://habr.com/ru/post/581960/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Chinese&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;STT: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;迈向语音识别领域的 ImageNet 时刻 - &lt;a href=&#34;https://www.infoq.cn/article/4u58WcFCs0RdpoXev1E2&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;语音领域学术界和工业界的七宗罪 - &lt;a href=&#34;https://www.infoq.cn/article/lEe6GCRjF1CNToVITvNw&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Russian&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;STT&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Наши сервисы для бесплатного распознавания речи стали лучше и удобнее - &lt;a href=&#34;https://habr.com/ru/post/654227/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Telegram-бот Silero бесплатно переводит речь в текст - &lt;a href=&#34;https://habr.com/ru/post/591563/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Бесплатное распознавание речи для всех желающих - &lt;a href=&#34;https://habr.com/ru/post/587512/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Последние обновления моделей распознавания речи из Silero Models - &lt;a href=&#34;https://habr.com/ru/post/577630/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Сжимаем трансформеры: простые, универсальные и прикладные способы cделать их компактными и быстрыми - &lt;a href=&#34;https://habr.com/ru/post/563778/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Ультимативное сравнение систем распознавания речи: Ashmanov, Google, Sber, Silero, Tinkoff, Yandex - &lt;a href=&#34;https://habr.com/ru/post/559640/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Мы опубликовали современные STT модели сравнимые по качеству с Google - &lt;a href=&#34;https://habr.com/ru/post/519564/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Понижаем барьеры на вход в распознавание речи - &lt;a href=&#34;https://habr.com/ru/post/494006/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Огромный открытый датасет русской речи версия 1.0 - &lt;a href=&#34;https://habr.com/ru/post/474462/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Насколько Быстрой Можно Сделать Систему STT? - &lt;a href=&#34;https://habr.com/ru/post/531524/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Наша система Speech-To-Text - &lt;a href=&#34;https://www.silero.ai/tag/our-speech-to-text/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Speech-To-Text - &lt;a href=&#34;https://www.silero.ai/tag/speech-to-text/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;TTS:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Может ли синтез речи обмануть систему биометрической идентификации? - &lt;a href=&#34;https://habr.com/ru/post/673996/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Теперь наш синтез на 20 языках - &lt;a href=&#34;https://habr.com/ru/post/669910/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Теперь наш публичный синтез в супер-высоком качестве, в 10 раз быстрее и без детских болячек - &lt;a href=&#34;https://habr.com/ru/post/660565/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Синтезируем голос бабушки, дедушки и Ленина + новости нашего публичного синтеза - &lt;a href=&#34;https://habr.com/ru/post/584750/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Мы сделали наш публичный синтез речи еще лучше - &lt;a href=&#34;https://habr.com/ru/post/563484/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Мы Опубликовали Качественный, Простой, Доступный и Быстрый Синтез Речи - &lt;a href=&#34;https://habr.com/ru/post/549480/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;VAD:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;А ты используешь VAD? Что это такое и зачем он нужен - &lt;a href=&#34;https://habr.com/ru/post/594745/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Модели для Детекции Речи, Чисел и Распознавания Языков - &lt;a href=&#34;https://www.silero.ai/vad-lang-classifier-number-detector/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Мы опубликовали современный Voice Activity Detector и не только -&lt;a href=&#34;https://habr.com/ru/post/537274/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Text Enhancement:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Восстановление знаков пунктуации и заглавных букв — теперь и на длинных текстах - &lt;a href=&#34;https://habr.com/ru/post/594565/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Мы опубликовали модель, расставляющую знаки препинания и заглавные буквы в тексте на четырех языках - &lt;a href=&#34;https://habr.com/ru/post/581946/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Donations&lt;/h2&gt; &#xA;&lt;p&gt;Please use the &#34;sponsor&#34; button.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kwea123/nerf_pl</title>
    <updated>2022-07-01T02:43:39Z</updated>
    <id>tag:github.com,2022-07-01:/kwea123/nerf_pl</id>
    <link href="https://github.com/kwea123/nerf_pl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NeRF (Neural Radiance Fields) and NeRF in the Wild using pytorch-lightning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nerf_pl&lt;/h1&gt; &#xA;&lt;h3&gt;Update: NVIDIA just open-sourced a lightning-fast version of NeRF: &lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34;&gt;NGP&lt;/a&gt;. I also learnt cuda recently, and will open source my pytorch implementation &lt;a href=&#34;https://github.com/kwea123/ngp_pl&#34;&gt;here&lt;/a&gt;.&lt;/h3&gt; &#xA;&lt;h3&gt;Update: an improved &lt;a href=&#34;https://www.cs.cornell.edu/~zl548/NSFF/&#34;&gt;NSFF&lt;/a&gt; implementation to handle dynamic scene is &lt;a href=&#34;https://github.com/kwea123/nsff_pl&#34;&gt;open&lt;/a&gt;!&lt;/h3&gt; &#xA;&lt;h3&gt;Update: &lt;a href=&#34;https://nerf-w.github.io/&#34;&gt;NeRF-W&lt;/a&gt; (NeRF in the Wild) implementation is added to &lt;a href=&#34;https://github.com/kwea123/nerf_pl/tree/nerfw&#34;&gt;nerfw&lt;/a&gt; branch!&lt;/h3&gt; &#xA;&lt;h3&gt;Update: The lastest code (using the latest libraries) will be updated to &lt;a href=&#34;https://github.com/kwea123/nerf_pl/tree/dev&#34;&gt;dev&lt;/a&gt; branch. The master branch remains to support the colab files. If you don&#39;t use colab, it is recommended to switch to dev branch.&lt;/h3&gt; &#xA;&lt;h3&gt;Only issues of the dev and nerfw branch will be considered currently.&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;span&gt;💎&lt;/span&gt; &lt;a href=&#34;https://kwea123.github.io/nerf_pl/&#34;&gt;&lt;strong&gt;Project page&lt;/strong&gt;&lt;/a&gt; (live demo!)&lt;/h3&gt; &#xA;&lt;p&gt;Unofficial implementation of &lt;a href=&#34;https://arxiv.org/pdf/2003.08934.pdf&#34;&gt;NeRF&lt;/a&gt; (Neural Radiance Fields) using pytorch (&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning&#34;&gt;pytorch-lightning&lt;/a&gt;). This repo doesn&#39;t aim at reproducibility, but aim at providing a simpler and faster training procedure (also simpler code with detailed comments to help to understand the work). Moreover, I try to extend much more opportunities by integrating this algorithm into game engine like Unity.&lt;/p&gt; &#xA;&lt;p&gt;Official implementation: &lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;nerf&lt;/a&gt; .. Reference pytorch implementation: &lt;a href=&#34;https://github.com/yenchenlin/nerf-pytorch&#34;&gt;nerf-pytorch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Recommend to read: A detailed NeRF extension list: &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF&#34;&gt;awesome-NeRF&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;&lt;span&gt;🌌&lt;/span&gt; Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-gpu training: Training on 8 GPUs finishes within 1 hour for the synthetic dataset!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#mortar_board-colab&#34;&gt;Colab&lt;/a&gt; notebooks to allow easy usage!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#ribbon-mesh&#34;&gt;Reconstruct&lt;/a&gt; &lt;strong&gt;colored&lt;/strong&gt; mesh!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/S5phWFTs2iM&#34;&gt;Mixed Reality&lt;/a&gt; in Unity!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/w9qTbVzCdWk&#34;&gt;REAL TIME volume rendering&lt;/a&gt; in Unity!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#portable-scenes&#34;&gt;Portable Scenes&lt;/a&gt; to let you play with other people&#39;s scenes!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;You can find the Unity project including mesh, mixed reality and volume rendering &lt;a href=&#34;https://github.com/kwea123/nerf_Unity&#34;&gt;here&lt;/a&gt;! See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/README_Unity.md&#34;&gt;README_Unity&lt;/a&gt; for generating your own data for Unity rendering!&lt;/h3&gt; &#xA;&lt;h2&gt;&lt;span&gt;🔰&lt;/span&gt; Tutorial&lt;/h2&gt; &#xA;&lt;h3&gt;What can NeRF do?&lt;/h3&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/11364490/82124460-1ccbbb80-97da-11ea-88ad-25e22868a5c1.png&#34; style=&#34;max-width:100%&#34;&gt; &#xA;&lt;h3&gt;Tutorial videos&lt;/h3&gt; &#xA;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLDV2CyUo4q-K02pNEyDr7DYpTQuka3mbV&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80913471-d5781080-8d7f-11ea-9f72-9d68402b8271.png&#34;&gt; &lt;/a&gt; &#xA;&lt;h1&gt;&lt;span&gt;💻&lt;/span&gt; Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Hardware&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OS: Ubuntu 18.04&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU with &lt;strong&gt;CUDA&amp;gt;=10.1&lt;/strong&gt; (tested with 1 RTX2080Ti)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Software&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repo by &lt;code&gt;git clone --recursive https://github.com/kwea123/nerf_pl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Python&amp;gt;=3.6 (installation via &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;anaconda&lt;/a&gt; is recommended, use &lt;code&gt;conda create -n nerf_pl python=3.6&lt;/code&gt; to create a conda environment and activate it by &lt;code&gt;conda activate nerf_pl&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Python libraries &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Install core requirements by &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Install &lt;code&gt;torchsearchsorted&lt;/code&gt; by &lt;code&gt;cd torchsearchsorted&lt;/code&gt; then &lt;code&gt;pip install .&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;span&gt;🔑&lt;/span&gt; Training&lt;/h1&gt; &#xA;&lt;p&gt;Please see each subsection for training on different datasets. Available training datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#blender&#34;&gt;Blender&lt;/a&gt; (Realistic Synthetic 360)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#llff&#34;&gt;LLFF&lt;/a&gt; (Real Forward-Facing)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#your-own-data&#34;&gt;Your own data&lt;/a&gt; (Forward-Facing/360 inward-facing)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Blender&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Steps&lt;/summary&gt; &#xA; &lt;h3&gt;Data download&lt;/h3&gt; &#xA; &lt;p&gt;Download &lt;code&gt;nerf_synthetic.zip&lt;/code&gt; from &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Training model&lt;/h3&gt; &#xA; &lt;p&gt;Run (example)&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python train.py \&#xA;   --dataset_name blender \&#xA;   --root_dir $BLENDER_DIR \&#xA;   --N_importance 64 --img_wh 400 400 --noise_std 0 \&#xA;   --num_epochs 16 --batch_size 1024 \&#xA;   --optimizer adam --lr 5e-4 \&#xA;   --lr_scheduler steplr --decay_step 2 4 8 --decay_gamma 0.5 \&#xA;   --exp_name exp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;These parameters are chosen to best mimic the training settings in the original repo. See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/opt.py&#34;&gt;opt.py&lt;/a&gt; for all configurations.&lt;/p&gt; &#xA; &lt;p&gt;NOTE: the above configuration doesn&#39;t work for some scenes like &lt;code&gt;drums&lt;/code&gt;, &lt;code&gt;ship&lt;/code&gt;. In that case, consider increasing the &lt;code&gt;batch_size&lt;/code&gt; or change the &lt;code&gt;optimizer&lt;/code&gt; to &lt;code&gt;radam&lt;/code&gt;. I managed to train on all scenes with these modifications.&lt;/p&gt; &#xA; &lt;p&gt;You can monitor the training process by &lt;code&gt;tensorboard --logdir logs/&lt;/code&gt; and go to &lt;code&gt;localhost:6006&lt;/code&gt; in your browser.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;LLFF&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Steps&lt;/summary&gt; &#xA; &lt;h3&gt;Data download&lt;/h3&gt; &#xA; &lt;p&gt;Download &lt;code&gt;nerf_llff_data.zip&lt;/code&gt; from &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Training model&lt;/h3&gt; &#xA; &lt;p&gt;Run (example)&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python train.py \&#xA;   --dataset_name llff \&#xA;   --root_dir $LLFF_DIR \&#xA;   --N_importance 64 --img_wh 504 378 \&#xA;   --num_epochs 30 --batch_size 1024 \&#xA;   --optimizer adam --lr 5e-4 \&#xA;   --lr_scheduler steplr --decay_step 10 20 --decay_gamma 0.5 \&#xA;   --exp_name exp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;These parameters are chosen to best mimic the training settings in the original repo. See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/opt.py&#34;&gt;opt.py&lt;/a&gt; for all configurations.&lt;/p&gt; &#xA; &lt;p&gt;You can monitor the training process by &lt;code&gt;tensorboard --logdir logs/&lt;/code&gt; and go to &lt;code&gt;localhost:6006&lt;/code&gt; in your browser.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Your own data&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Steps&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install &lt;a href=&#34;https://github.com/colmap/colmap&#34;&gt;COLMAP&lt;/a&gt; following &lt;a href=&#34;https://colmap.github.io/install.html&#34;&gt;installation guide&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Prepare your images in a folder (around 20 to 30 for forward facing, and 40 to 50 for 360 inward-facing)&lt;/li&gt; &#xA;  &lt;li&gt;Clone &lt;a href=&#34;https://github.com/Fyusion/LLFF&#34;&gt;LLFF&lt;/a&gt; and run &lt;code&gt;python img2poses.py $your-images-folder&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Train the model using the same command as in &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#llff&#34;&gt;LLFF&lt;/a&gt;. If the scene is captured in a 360 inward-facing manner, add &lt;code&gt;--spheric&lt;/code&gt; argument.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;For more details of training a good model, please see the video &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#colab&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Pretrained models and logs&lt;/h2&gt; &#xA;&lt;p&gt;Download the pretrained models and training logs in &lt;a href=&#34;https://github.com/kwea123/nerf_pl/releases&#34;&gt;release&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Comparison with other repos&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;training GPU memory in GB&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Speed (1 step)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;Original&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.177s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yenchenlin/nerf-pytorch&#34;&gt;Ref pytorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.147s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This repo&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.12s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The speed is measured on 1 RTX2080Ti. Detailed profile can be found in &lt;a href=&#34;https://github.com/kwea123/nerf_pl/releases&#34;&gt;release&lt;/a&gt;. Training memory is largely reduced, since the original repo loads the whole data to GPU at the beginning, while we only pass batches to GPU every step.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;🔎&lt;/span&gt; Testing&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/test.ipynb&#34;&gt;test.ipynb&lt;/a&gt; for a simple view synthesis and depth prediction on 1 image.&lt;/p&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/eval.py&#34;&gt;eval.py&lt;/a&gt; to create the whole sequence of moving views. E.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python eval.py \&#xA;   --root_dir $BLENDER \&#xA;   --dataset_name blender --scene_name lego \&#xA;   --img_wh 400 400 --N_importance 64 --ckpt_path $CKPT_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt; : Don&#39;t forget to add &lt;code&gt;--spheric_poses&lt;/code&gt; if the model is trained under &lt;code&gt;--spheric&lt;/code&gt; setting!&lt;/p&gt; &#xA;&lt;p&gt;It will create folder &lt;code&gt;results/{dataset_name}/{scene_name}&lt;/code&gt; and run inference on all test data, finally create a gif out of them.&lt;/p&gt; &#xA;&lt;p&gt;Example of lego scene using pretrained model and the reconstructed &lt;strong&gt;colored&lt;/strong&gt; mesh: (PSNR=31.39, paper=32.54)&lt;/p&gt; &#xA;&lt;p&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/79932648-f8a1e680-8488-11ea-98fe-c11ec22fc8a1.gif&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80813179-822d8300-8c04-11ea-84e6-142f04714c58.png&#34; width=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Example of fern scene using pretrained model:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/11364490/79932650-f9d31380-8488-11ea-8dad-b70a6a3daa6e.gif&#34; alt=&#34;fern&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Example of own scene (&lt;a href=&#34;https://www.youtube.com/watch?v=hVQIvEq_Av0&#34;&gt;Silica GGO figure&lt;/a&gt;) and the reconstructed &lt;strong&gt;colored&lt;/strong&gt; mesh. Click to link to youtube video.&lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://youtu.be/yH1ZBcdNsUY&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80279695-324d4880-873a-11ea-961a-d6350e149ece.gif&#34; height=&#34;252&#34;&gt; &lt;/a&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80813184-83f74680-8c04-11ea-8606-40580f753355.png&#34; height=&#34;252&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Portable scenes&lt;/h2&gt; &#xA;&lt;p&gt;The concept of NeRF is that the whole scene is compressed into a NeRF model, then we can render from any pose we want. To render from plausible poses, we can leverage the training poses; therefore, you can generate video with &lt;strong&gt;only&lt;/strong&gt; the trained model and the poses (hence the name of portable scenes). I provided my silica model in &lt;a href=&#34;https://github.com/kwea123/nerf_pl/releases&#34;&gt;release&lt;/a&gt;, feel free to play around with it!&lt;/p&gt; &#xA;&lt;p&gt;If you trained some interesting scenes, you are also welcomed to share the model (and the &lt;code&gt;poses_bounds.npy&lt;/code&gt;) by sending me an email, or post in issues! After all, a model is just around &lt;strong&gt;5MB&lt;/strong&gt;! Please run &lt;code&gt;python utils/save_weights_only.py --ckpt_path $YOUR_MODEL_PATH&lt;/code&gt; to extract the final model.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;🎀&lt;/span&gt; Mesh&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/README_mesh.md&#34;&gt;README_mesh&lt;/a&gt; for reconstruction of &lt;strong&gt;colored&lt;/strong&gt; mesh. Only supported for blender dataset and 360 inward-facing data!&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;⚠&lt;/span&gt; Notes on differences with the original repo&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The learning rate decay in the original repo is &lt;strong&gt;by step&lt;/strong&gt;, which means it decreases every step, here I use learning rate decay &lt;strong&gt;by epoch&lt;/strong&gt;, which means it changes only at the end of 1 epoch.&lt;/li&gt; &#xA; &lt;li&gt;The validation image for LLFF dataset is chosen as the most centered image here, whereas the original repo chooses every 8th image.&lt;/li&gt; &#xA; &lt;li&gt;The rendering spiral path is slightly different from the original repo (I use approximate values to simplify the code).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;span&gt;🎓&lt;/span&gt; COLAB&lt;/h1&gt; &#xA;&lt;p&gt;I also prepared colab notebooks that allow you to run the algorithm on any machine without GPU requirement.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/kwea123/f0e8f38ff2aa94495dbfe7ae9219f75c&#34;&gt;colmap&lt;/a&gt; to prepare camera poses for your own training data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/kwea123/a3c541a325e895ef79ecbc0d2e6d7221&#34;&gt;nerf&lt;/a&gt; to train on your data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/kwea123/77ed1640f9bc9550136dc13a6a419e88&#34;&gt;extract_mesh&lt;/a&gt; to extract colored mesh&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://www.youtube.com/playlist?list=PLDV2CyUo4q-K02pNEyDr7DYpTQuka3mbV&#34;&gt;this playlist&lt;/a&gt; for the detailed tutorials.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;🎃&lt;/span&gt; SHOWOFF&lt;/h1&gt; &#xA;&lt;p&gt;We can incorporate &lt;em&gt;ray tracing&lt;/em&gt; techniques into the volume rendering pipeline, and realize realistic scene editing (following is the &lt;code&gt;materials&lt;/code&gt; scene with an object removed, and a mesh is inserted and rendered with ray tracing). The code &lt;strong&gt;will not&lt;/strong&gt; be released.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/11364490/90312710-92face00-df41-11ea-9eea-10f24849b407.gif&#34; alt=&#34;add&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/90360796-92744b80-e097-11ea-859d-159aa2519375.gif&#34; alt=&#34;add2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;With my integration in Unity, I can realize realistic mixed reality photos (note my character casts shadow on the scene, &lt;strong&gt;zero&lt;/strong&gt; post- image editing required): &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/140264589-295acebe-8ace-4d61-b871-26eb8ae10ab0.png&#34; alt=&#34;defer&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/140264596-59daebe5-b88d-48e7-82bd-5ccaaff2283f.png&#34; alt=&#34;defer2&#34;&gt; BTW, I would like to visit the museum one day...&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;📖&lt;/span&gt; Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use (part of) my code or find my work helpful, please consider citing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{queianchen_nerf,&#xA;  author={Quei-An, Chen},&#xA;  title={Nerf_pl: a pytorch-lightning implementation of NeRF},&#xA;  url={https://github.com/kwea123/nerf_pl/},&#xA;  year={2020},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>