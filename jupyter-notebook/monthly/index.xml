<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-05-29T02:42:18Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Pierian-Data/Complete-Python-3-Bootcamp</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/Pierian-Data/Complete-Python-3-Bootcamp</id>
    <link href="https://github.com/Pierian-Data/Complete-Python-3-Bootcamp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Complete-Python-3-Bootcamp&lt;/h1&gt; &#xA;&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt; &#xA;&lt;p&gt;Copyright(¬©) by Pierian Data Inc.&lt;/p&gt; &#xA;&lt;p&gt;Get it now for 95% off with the link: &lt;a href=&#34;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&#34;&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thanks!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>alembics/disco-diffusion</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/alembics/disco-diffusion</id>
    <link href="https://github.com/alembics/disco-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Disco Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A frankensteinian amalgamation of notebooks, models and techniques for the generation of AI Art and Animations.&lt;/p&gt; &#xA;&lt;p&gt;[to be updated with further info soon]&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project uses a special conversion tool to convert the python files into notebooks for easier development.&lt;/p&gt; &#xA;&lt;p&gt;What this means is you do not have to touch the notebook directly to make changes to it&lt;/p&gt; &#xA;&lt;p&gt;the tool being used is called &lt;a href=&#34;https://github.com/MSFTserver/colab-convert&#34;&gt;Colab-Convert&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;install using &lt;code&gt;pip install colab-convert&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;convert .py to .ipynb &lt;code&gt;colab-convert /path/to/file.py /path/to/file.ipynb&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;convert .ipynb to .py &lt;code&gt;colab-convert /path/to/file.ipynb /path/to/file.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;h4&gt;v1 Oct 29th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial QoL improvements added, including user friendly UI, settings+prompt saving and improved google drive folder organization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v1.1 Nov 13th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Now includes sizing options, intermediate saves and fixed image prompts and perlin inits. unexposed batch option since it doesn&#39;t work&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2 Update: Nov 22nd 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial addition of Katherine Crowson&#39;s Secondary Model Method (&lt;a href=&#34;https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR&#34;&gt;https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix for incorrectly named settings files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v3 Update: Dec 24th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implemented Dango&#39;s advanced cutout method&lt;/li&gt; &#xA; &lt;li&gt;Added SLIP models, thanks to NeuralDivergent&lt;/li&gt; &#xA; &lt;li&gt;Fixed issue with NaNs resulting in black images, with massive help and testing from @Softology&lt;/li&gt; &#xA; &lt;li&gt;Perlin now changes properly within batches (not sure where this perlin_regen code came from originally, but thank you)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v4 Update: Jan 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implemented Diffusion Zooming&lt;/li&gt; &#xA; &lt;li&gt;Added Chigozie keyframing&lt;/li&gt; &#xA; &lt;li&gt;Made a bunch of edits to processes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v4.1 Update: Jan 14th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added video input mode&lt;/li&gt; &#xA; &lt;li&gt;Added license that somehow went missing&lt;/li&gt; &#xA; &lt;li&gt;Added improved prompt keyframing, fixed image_prompts and multiple prompts&lt;/li&gt; &#xA; &lt;li&gt;Improved UI&lt;/li&gt; &#xA; &lt;li&gt;Significant under the hood cleanup and improvement&lt;/li&gt; &#xA; &lt;li&gt;Refined defaults for each mode&lt;/li&gt; &#xA; &lt;li&gt;Removed SLIP models for the time being due to import conflicts&lt;/li&gt; &#xA; &lt;li&gt;Added latent-diffusion SuperRes for sharpening&lt;/li&gt; &#xA; &lt;li&gt;Added resume run mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5 Update: Feb 20th 2022 - gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added 3D animation mode. Uses weighted combination of AdaBins and MiDaS depth estimation models. Uses pytorch3d for 3D transforms on Colab and/or Linux.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.1 Update: Mar 30th 2022 - zippy / Chris Allen and gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integrated Turbo+Smooth features from Disco Diffusion Turbo -- just the implementation, without its defaults.&lt;/li&gt; &#xA; &lt;li&gt;Implemented resume of turbo animations in such a way that it&#39;s now possible to resume from different batch folders and batch numbers.&lt;/li&gt; &#xA; &lt;li&gt;3D rotation parameter units are now degrees (rather than radians)&lt;/li&gt; &#xA; &lt;li&gt;Corrected name collision in sampling_mode (now diffusion_sampling_mode for plms/ddim, and sampling_mode for 3D transform sampling)&lt;/li&gt; &#xA; &lt;li&gt;Added video_init_seed_continuity option to make init video animations more continuous&lt;/li&gt; &#xA; &lt;li&gt;Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Remove Super Resolution&lt;/li&gt; &#xA; &lt;li&gt;Remove Slip Models&lt;/li&gt; &#xA; &lt;li&gt;Update for crossplatform support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.1 Update: Apr 4th 2022 - MSFTserver aka HostsServer&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Remove Super Resolution&lt;/li&gt; &#xA; &lt;li&gt;Remove Slip Models&lt;/li&gt; &#xA; &lt;li&gt;Update for crossplatform support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.2 Update: Apr 10th 2022 - nin_artificial / Tom Mason&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;VR Mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notebook Provenance&lt;/h2&gt; &#xA;&lt;p&gt;Original notebook by Katherine Crowson (&lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;https://github.com/crowsonkb&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/RiversHaveWings&#34;&gt;https://twitter.com/RiversHaveWings&lt;/a&gt;). It uses either OpenAI&#39;s 256x256 unconditional ImageNet or Katherine Crowson&#39;s fine-tuned 512x512 diffusion model (&lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;https://github.com/openai/guided-diffusion&lt;/a&gt;), together with CLIP (&lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;https://github.com/openai/CLIP&lt;/a&gt;) to connect text prompts with images.&lt;/p&gt; &#xA;&lt;p&gt;Modified by Daniel Russell (&lt;a href=&#34;https://github.com/russelldc&#34;&gt;https://github.com/russelldc&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/danielrussruss&#34;&gt;https://twitter.com/danielrussruss&lt;/a&gt;) to include (hopefully) optimal params for quick generations in 15-100 timesteps rather than 1000, as well as more robust augmentations.&lt;/p&gt; &#xA;&lt;p&gt;Further improvements from Dango233 and nsheppard helped improve the quality of diffusion in general, and especially so for shorter runs like this notebook aims to achieve.&lt;/p&gt; &#xA;&lt;p&gt;Vark added code to load in multiple Clip models at once, which all prompts are evaluated against, which may greatly improve accuracy.&lt;/p&gt; &#xA;&lt;p&gt;The latest zoom, pan, rotation, and keyframes features were taken from Chigozie Nri&#39;s VQGAN Zoom Notebook (&lt;a href=&#34;https://github.com/chigozienri&#34;&gt;https://github.com/chigozienri&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/chigozienri&#34;&gt;https://twitter.com/chigozienri&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Advanced DangoCutn Cutout method is also from Dango223.&lt;/p&gt; &#xA;&lt;p&gt;--&lt;/p&gt; &#xA;&lt;p&gt;Somnai (&lt;a href=&#34;https://twitter.com/Somnai_dreams&#34;&gt;https://twitter.com/Somnai_dreams&lt;/a&gt;) added 2D Diffusion animation techniques, QoL improvements and various implementations of tech and techniques, mostly listed in the changelog below.&lt;/p&gt; &#xA;&lt;p&gt;3D animation implementation added by Adam Letts (&lt;a href=&#34;https://twitter.com/gandamu_ml&#34;&gt;https://twitter.com/gandamu_ml&lt;/a&gt;) in collaboration with Somnai.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>DataTalksClub/data-engineering-zoomcamp</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/DataTalksClub/data-engineering-zoomcamp</id>
    <link href="https://github.com/DataTalksClub/data-engineering-zoomcamp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free Data Engineering course!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Data Engineering Zoomcamp&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Register in &lt;a href=&#34;https://datatalks.club/slack.html&#34;&gt;DataTalks.Club&#39;s Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Join the &lt;a href=&#34;https://app.slack.com/client/T01ATQK62F8/C01FABYF2RG&#34;&gt;&lt;code&gt;#course-data-engineering&lt;/code&gt;&lt;/a&gt; channel&lt;/li&gt; &#xA; &lt;li&gt;The videos are published to &lt;a href=&#34;https://www.youtube.com/c/DataTalksClub&#34;&gt;DataTalks.Club&#39;s YouTube channel&lt;/a&gt; in &lt;a href=&#34;https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&#34;&gt;the course playlist&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing&#34;&gt;Frequenty asked technical questions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Syllabus&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-1-introduction--prerequisites&#34;&gt;Week 1: Introduction &amp;amp; Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-2-data-ingestion&#34;&gt;Week 2: Data ingestion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-3-data-warehouse&#34;&gt;Week 3: Data Warehouse&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-4-analytics-engineering&#34;&gt;Week 4: Analytics Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-5-batch-processing&#34;&gt;Week 5: Batch processing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-6-streaming&#34;&gt;Week 6: Streaming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-7-8--9-project&#34;&gt;Week 7, 8 &amp;amp; 9: Project&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Taking the course&lt;/h2&gt; &#xA;&lt;h3&gt;Self-paced mode&lt;/h3&gt; &#xA;&lt;p&gt;All the materials of the course are freely available, so you can take the course at your own pace&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow the suggested syllabus (see below) week by week&lt;/li&gt; &#xA; &lt;li&gt;You don&#39;t need to fill in the registration form. Just start watching the videos and join Slack&lt;/li&gt; &#xA; &lt;li&gt;Check &lt;a href=&#34;https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing&#34;&gt;FAQ&lt;/a&gt; if you have problems&lt;/li&gt; &#xA; &lt;li&gt;If you can&#39;t find a solution to your problem in FAQ, ask for help in Slack&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2022 Cohort&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Start&lt;/strong&gt;: 17 January 2022&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Registration link&lt;/strong&gt;: &lt;a href=&#34;https://airtable.com/shr6oVXeQvSI5HuWD&#34;&gt;https://airtable.com/shr6oVXeQvSI5HuWD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vR9oQiYnAVvzL4dagnhvp0sngqagF0AceD0FGjhS-dnzMTBzNQIal3-hOgkTibVQvfuqbQ69b0fvRnf/pubhtml&#34;&gt;Leaderboard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Subscribe to our &lt;a href=&#34;https://calendar.google.com/calendar/?cid=ZXIxcjA1M3ZlYjJpcXU0dTFmaG02MzVxMG9AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&#34;&gt;public Google Calendar&lt;/a&gt; (it works from Desktop only)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Asking for help in Slack&lt;/h3&gt; &#xA;&lt;p&gt;The best way to get support is to use &lt;a href=&#34;https://datatalks.club/slack.html&#34;&gt;DataTalks.Club&#39;s Slack&lt;/a&gt;. Join the &lt;a href=&#34;https://app.slack.com/client/T01ATQK62F8/C01FABYF2RG&#34;&gt;&lt;code&gt;#course-data-engineering&lt;/code&gt;&lt;/a&gt; channel.&lt;/p&gt; &#xA;&lt;p&gt;To make discussions in Slack more organized:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow &lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/asking-questions.md&#34;&gt;these recommendations&lt;/a&gt; when asking for help&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://datatalks.club/slack/guidelines.html&#34;&gt;DataTalks.Club community guidelines&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Syllabus&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_1_basics_n_setup&#34;&gt;Week 1: Introduction &amp;amp; Prerequisites&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Course overview&lt;/li&gt; &#xA; &lt;li&gt;Introduction to GCP&lt;/li&gt; &#xA; &lt;li&gt;Docker and docker-compose&lt;/li&gt; &#xA; &lt;li&gt;Running Postgres locally with Docker&lt;/li&gt; &#xA; &lt;li&gt;Setting up infrastructure on GCP with Terraform&lt;/li&gt; &#xA; &lt;li&gt;Preparing the environment for the course&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_1_basics_n_setup&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_2_data_ingestion&#34;&gt;Week 2: Data ingestion&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data Lake&lt;/li&gt; &#xA; &lt;li&gt;Workflow orchestration&lt;/li&gt; &#xA; &lt;li&gt;Setting up Airflow locally&lt;/li&gt; &#xA; &lt;li&gt;Ingesting data to GCP with Airflow&lt;/li&gt; &#xA; &lt;li&gt;Ingesting data to local Postgres with Airflow&lt;/li&gt; &#xA; &lt;li&gt;Moving data from AWS to GCP (Transfer service)&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_2_data_ingestion&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_3_data_warehouse&#34;&gt;Week 3: Data Warehouse&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data Warehouse&lt;/li&gt; &#xA; &lt;li&gt;BigQuery&lt;/li&gt; &#xA; &lt;li&gt;Partitoning and clustering&lt;/li&gt; &#xA; &lt;li&gt;BigQuery best practices&lt;/li&gt; &#xA; &lt;li&gt;Internals of BigQuery&lt;/li&gt; &#xA; &lt;li&gt;Integrating BigQuery with Airflow&lt;/li&gt; &#xA; &lt;li&gt;BigQuery Machine Learning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_3_data_warehouse&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_4_analytics_engineering/&#34;&gt;Week 4: Analytics engineering&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basics of analytics engineering&lt;/li&gt; &#xA; &lt;li&gt;dbt (data build tool)&lt;/li&gt; &#xA; &lt;li&gt;BigQuery and dbt&lt;/li&gt; &#xA; &lt;li&gt;Postgres and dbt&lt;/li&gt; &#xA; &lt;li&gt;dbt models&lt;/li&gt; &#xA; &lt;li&gt;Testing and documenting&lt;/li&gt; &#xA; &lt;li&gt;Deployment to the cloud and locally&lt;/li&gt; &#xA; &lt;li&gt;Visualising the data with google data studio and metabase&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_4_analytics_engineering&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_5_batch_processing&#34;&gt;Week 5: Batch processing&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Batch processing&lt;/li&gt; &#xA; &lt;li&gt;What is Spark&lt;/li&gt; &#xA; &lt;li&gt;Spark Dataframes&lt;/li&gt; &#xA; &lt;li&gt;Spark SQL&lt;/li&gt; &#xA; &lt;li&gt;Internals: GroupBy and joins&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_5_batch_processing&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_6_stream_processing&#34;&gt;Week 6: Streaming&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Introduction to Kafka&lt;/li&gt; &#xA; &lt;li&gt;Schemas (avro)&lt;/li&gt; &#xA; &lt;li&gt;Kafka Streams&lt;/li&gt; &#xA; &lt;li&gt;Kafka Connect and KSQL&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_6_stream_processing&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_7_project&#34;&gt;Week 7, 8 &amp;amp; 9: Project&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Putting everything we learned to practice&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 7 and 8: working on your own project&lt;/li&gt; &#xA; &lt;li&gt;Week 9: reviewing your peers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_7_project&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;h3&gt;Architecture diagram&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/images/architecture/arch_1.jpg&#34;&gt; &#xA;&lt;h3&gt;Technologies&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Google Cloud Platform (GCP)&lt;/em&gt;: Cloud-based auto-scaling platform by Google &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;Google Cloud Storage (GCS)&lt;/em&gt;: Data Lake&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;BigQuery&lt;/em&gt;: Data Warehouse&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Terraform&lt;/em&gt;: Infrastructure-as-Code (IaC)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Docker&lt;/em&gt;: Containerization&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;SQL&lt;/em&gt;: Data Analysis &amp;amp; Exploration&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Airflow&lt;/em&gt;: Pipeline Orchestration&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;dbt&lt;/em&gt;: Data Transformation&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Spark&lt;/em&gt;: Distributed Processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Kafka&lt;/em&gt;: Streaming&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;To get most out of this course, you should feel comfortable with coding and command line, and know the basics of SQL. Prior experience with Python will be helpful, but you can pick Python relatively fast if you have experience with other programming languages.&lt;/p&gt; &#xA;&lt;p&gt;Prior experience with data engineering is not required.&lt;/p&gt; &#xA;&lt;h2&gt;Instructors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ankush Khanna (&lt;a href=&#34;https://linkedin.com/in/ankushkhanna2&#34;&gt;https://linkedin.com/in/ankushkhanna2&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Sejal Vaidya (&lt;a href=&#34;https://linkedin.com/in/vaidyasejal&#34;&gt;https://linkedin.com/in/vaidyasejal&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Victoria Perez Mola (&lt;a href=&#34;https://www.linkedin.com/in/victoriaperezmola/&#34;&gt;https://www.linkedin.com/in/victoriaperezmola/&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Alexey Grigorev (&lt;a href=&#34;https://linkedin.com/in/agrigorev&#34;&gt;https://linkedin.com/in/agrigorev&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tools&lt;/h2&gt; &#xA;&lt;p&gt;For this course you&#39;ll need to have the following software installed on your computer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker and Docker-Compose&lt;/li&gt; &#xA; &lt;li&gt;Python 3 (e.g. via &lt;a href=&#34;https://www.anaconda.com/products/individual&#34;&gt;Anaconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Google Cloud SDK&lt;/li&gt; &#xA; &lt;li&gt;Terraform&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_1_basics_n_setup&#34;&gt;Week 1&lt;/a&gt; for more details about installing these tools&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: I registered, but haven&#39;t received a confirmation email. Is it normal? &lt;strong&gt;A&lt;/strong&gt;: Yes, it&#39;s normal. It&#39;s not automated. But you will receive an email eventually&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: At what time of the day will it happen? &lt;strong&gt;A&lt;/strong&gt;: Office hours will happen on Mondays at 17:00 CET. But everything will be recorded, so you can watch it whenever it&#39;s convenient for you&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Will there be a certificate? &lt;strong&gt;A&lt;/strong&gt;: Yes, if you complete the project&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: I&#39;m 100% not sure I&#39;ll be able to attend. Can I still sign up? &lt;strong&gt;A&lt;/strong&gt;: Yes, please do! You&#39;ll receive all the updates and then you can watch the course at your own pace.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Do you plan to run a ML engineering course as well? &lt;strong&gt;A&lt;/strong&gt;: Glad you asked. &lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp&#34;&gt;We do&lt;/a&gt; :)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: I&#39;m stuck! I&#39;ve got a technical question! &lt;strong&gt;A&lt;/strong&gt;: Ask on Slack! And check out the &lt;a href=&#34;https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing&#34;&gt;student FAQ&lt;/a&gt;; many common issues have been answered already. If your issue is solved, please add how you solved it to the document. Thanks!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Our friends&lt;/h2&gt; &#xA;&lt;p&gt;Big thanks to other communities for helping us spread the word about the course:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dphi.tech/&#34;&gt;DPhi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlops.community/&#34;&gt;MLOps.community&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Check them out - they are cool!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ageron/handson-ml2</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/ageron/handson-ml2</id>
    <link href="https://github.com/ageron/handson-ml2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Notebooks&lt;/h1&gt; &#xA;&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in python. It contains the example code and solutions to the exercises in the second edition of my O&#39;Reilly book &lt;a href=&#34;https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/&#34;&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;img src=&#34;https://images-na.ssl-images-amazon.com/images/I/51aqYc1QyrL._SX379_BO1,204,203,200_.jpg&#34; title=&#34;book&#34; width=&#34;150&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are looking for the first edition notebooks, check out &lt;a href=&#34;https://github.com/ageron/handson-ml&#34;&gt;ageron/handson-ml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Want to play with these notebooks online without having to install anything?&lt;/h3&gt; &#xA;&lt;p&gt;Use any of the following services (I recommended Colab or Kaggle, since they offer free GPUs and TPUs).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: &lt;em&gt;Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you download any data you care about.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/ageron/handson-ml2/blob/master/&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://homl.info/kaggle/&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Open in Kaggle&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/ageron/handson-ml2/HEAD?filepath=%2Findex.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Launch binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://homl.info/deepnote/&#34;&gt;&lt;img src=&#34;https://deepnote.com/buttons/launch-in-deepnote-small.svg?sanitize=true&#34; alt=&#34;Launch in Deepnote&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Just want to quickly look at some notebooks, without executing any code?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg?sanitize=true&#34; alt=&#34;Render nbviewer&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ageron/handson-ml2/raw/master/index.ipynb&#34;&gt;github.com&#39;s notebook viewer&lt;/a&gt; also works but it&#39;s not ideal: it&#39;s slower, the math equations are not always displayed correctly, and large notebooks often fail to open.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Want to run this project using a Docker image?&lt;/h3&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://github.com/ageron/handson-ml2/tree/master/docker&#34;&gt;Docker instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Want to install this project on your own machine?&lt;/h3&gt; &#xA;&lt;p&gt;Start by installing &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;Anaconda&lt;/a&gt; (or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;), &lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;git&lt;/a&gt;, and if you have a TensorFlow-compatible GPU, install the &lt;a href=&#34;https://www.nvidia.com/Download/index.aspx&#34;&gt;GPU driver&lt;/a&gt;, as well as the appropriate version of CUDA and cuDNN (see TensorFlow&#39;s documentation for more details).&lt;/p&gt; &#xA;&lt;p&gt;Next, clone this project by opening a terminal and typing the following commands (do not type the first &lt;code&gt;$&lt;/code&gt; signs on each line, they just indicate that these are terminal commands):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml2.git&#xA;$ cd handson-ml2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml&#xA;$ conda activate tf2&#xA;$ python -m ipykernel install --user --name=python3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, start Jupyter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you need further instructions, read the &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;detailed installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Which Python version should I use?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I recommend Python 3.8. If you follow the installation instructions above, that&#39;s the version you will get. Most code will work with other versions of Python 3, but some libraries do not support Python 3.9 or 3.10 yet, which is why I recommend Python 3.8.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;m getting an error when I call &lt;code&gt;load_housing_data()&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Make sure you call &lt;code&gt;fetch_housing_data()&lt;/code&gt; &lt;em&gt;before&lt;/em&gt; you call &lt;code&gt;load_housing_data()&lt;/code&gt;. If you&#39;re getting an HTTP error, make sure you&#39;re running the exact same code as in the notebook (copy/paste it if needed). If the problem persists, please check your network configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;m getting an SSL error on MacOSX&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You probably need to install the SSL certificates (see this &lt;a href=&#34;https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error&#34;&gt;StackOverflow question&lt;/a&gt;). If you downloaded Python from the official website, then run &lt;code&gt;/Applications/Python\ 3.8/Install\ Certificates.command&lt;/code&gt; in a terminal (change &lt;code&gt;3.8&lt;/code&gt; to whatever version you installed). If you installed Python using MacPorts, run &lt;code&gt;sudo port install curl-ca-bundle&lt;/code&gt; in a terminal.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;ve installed this project locally. How do I update it to the latest version?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How do I update my Python libraries to the latest versions, when using Anaconda?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;I would like to thank everyone &lt;a href=&#34;https://github.com/ageron/handson-ml2/graphs/contributors&#34;&gt;who contributed to this project&lt;/a&gt;, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park and Ian Beauregard who reviewed every notebook and submitted many PRs, including help on some of the exercise solutions. Thanks as well to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory, and to github user SuperYorio who helped on some exercise solutions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>labmlai/annotated_deep_learning_paper_implementations</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/labmlai/annotated_deep_learning_paper_implementations</id>
    <link href="https://github.com/labmlai/annotated_deep_learning_paper_implementations" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üßë‚Äçüè´ 50! Implementations/tutorials of deep learning papers with side-by-side notes üìù; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, ...), gans(cyclegan, stylegan2, ...), üéÆ reinforcement learning (ppo, dqn), capsnet, distillation, ... üß†&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/labmlai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/labmlai?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://nn.labml.ai/index.html&#34;&gt;labml.ai Deep Learning Paper Implementations&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This is a collection of simple PyTorch implementations of neural networks and related algorithms. These implementations are documented with explanations,&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nn.labml.ai/index.html&#34;&gt;The website&lt;/a&gt; renders these as side-by-side formatted notes. We believe these would help you understand these algorithms better.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://nn.labml.ai/dqn-light.png&#34; alt=&#34;Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We are actively maintaining this repo and adding new implementations almost weekly. &lt;a href=&#34;https://twitter.com/labmlai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/labmlai?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; for updates.&lt;/p&gt; &#xA;&lt;h2&gt;Paper Implementations&lt;/h2&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/transformers/index.html&#34;&gt;Transformers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mha.html&#34;&gt;Multi-headed attention&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/models.html&#34;&gt;Transformer building blocks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/xl/index.html&#34;&gt;Transformer XL&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/xl/relative_mha.html&#34;&gt;Relative multi-headed attention&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/rope/index.html&#34;&gt;Rotary Positional Embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/retro/index.html&#34;&gt;RETRO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/compressive/index.html&#34;&gt;Compressive Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/gpt/index.html&#34;&gt;GPT Architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/glu_variants/simple.html&#34;&gt;GLU Variants&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/knn&#34;&gt;kNN-LM: Generalization through Memorization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/feedback/index.html&#34;&gt;Feedback Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/switch/index.html&#34;&gt;Switch Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/fast_weights/index.html&#34;&gt;Fast Weights Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/fnet/index.html&#34;&gt;FNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/aft/index.html&#34;&gt;Attention Free Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mlm/index.html&#34;&gt;Masked Language Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mlp_mixer/index.html&#34;&gt;MLP-Mixer: An all-MLP Architecture for Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/gmlp/index.html&#34;&gt;Pay Attention to MLPs (gMLP)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/vit/index.html&#34;&gt;Vision Transformer (ViT)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/primer_ez/index.html&#34;&gt;Primer EZ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/hour_glass/index.html&#34;&gt;Hourglass&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/recurrent_highway_networks/index.html&#34;&gt;Recurrent Highway Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/lstm/index.html&#34;&gt;LSTM&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/hypernetworks/hyper_lstm.html&#34;&gt;HyperNetworks - HyperLSTM&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/resnet/index.html&#34;&gt;ResNet&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/conv_mixer/index.html&#34;&gt;ConvMixer&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/capsule_networks/index.html&#34;&gt;Capsule Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/gan/index.html&#34;&gt;Generative Adversarial Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/original/index.html&#34;&gt;Original GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/dcgan/index.html&#34;&gt;GAN with deep convolutional network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/cycle_gan/index.html&#34;&gt;Cycle GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/wasserstein/index.html&#34;&gt;Wasserstein GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/wasserstein/gradient_penalty/index.html&#34;&gt;Wasserstein GAN with Gradient Penalty&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/stylegan/index.html&#34;&gt;StyleGAN 2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/diffusion/index.html&#34;&gt;Diffusion models&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/diffusion/ddpm/index.html&#34;&gt;Denoising Diffusion Probabilistic Models (DDPM)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/sketch_rnn/index.html&#34;&gt;Sketch RNN&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® Graph Neural Networks&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/graphs/gat/index.html&#34;&gt;Graph Attention Networks (GAT)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/graphs/gatv2/index.html&#34;&gt;Graph Attention Networks v2 (GATv2)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/cfr/index.html&#34;&gt;Counterfactual Regret Minimization (CFR)&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Solving games with incomplete information such as poker with CFR.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/cfr/kuhn/index.html&#34;&gt;Kuhn Poker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/rl/index.html&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/rl/ppo/index.html&#34;&gt;Proximal Policy Optimization&lt;/a&gt; with &lt;a href=&#34;https://nn.labml.ai/rl/ppo/gae.html&#34;&gt;Generalized Advantage Estimation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/rl/dqn/index.html&#34;&gt;Deep Q Networks&lt;/a&gt; with with &lt;a href=&#34;https://nn.labml.ai/rl/dqn/model.html&#34;&gt;Dueling Network&lt;/a&gt;, &lt;a href=&#34;https://nn.labml.ai/rl/dqn/replay_buffer.html&#34;&gt;Prioritized Replay&lt;/a&gt; and Double Q Network.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/optimizers/index.html&#34;&gt;Optimizers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/adam.html&#34;&gt;Adam&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/amsgrad.html&#34;&gt;AMSGrad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/adam_warmup.html&#34;&gt;Adam Optimizer with warmup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/noam.html&#34;&gt;Noam Optimizer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/radam.html&#34;&gt;Rectified Adam Optimizer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/ada_belief.html&#34;&gt;AdaBelief Optimizer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/normalization/index.html&#34;&gt;Normalization Layers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/batch_norm/index.html&#34;&gt;Batch Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/layer_norm/index.html&#34;&gt;Layer Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/instance_norm/index.html&#34;&gt;Instance Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/group_norm/index.html&#34;&gt;Group Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/weight_standardization/index.html&#34;&gt;Weight Standardization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/batch_channel_norm/index.html&#34;&gt;Batch-Channel Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/deep_norm/index.html&#34;&gt;DeepNorm&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/distillation/index.html&#34;&gt;Distillation&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/adaptive_computation/index.html&#34;&gt;Adaptive Computation&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/adaptive_computation/ponder_net/index.html&#34;&gt;PonderNet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/uncertainty/index.html&#34;&gt;Uncertainty&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/uncertainty/evidence/index.html&#34;&gt;Evidential Deep Learning to Quantify Classification Uncertainty&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;‚ú® &lt;a href=&#34;https://nn.labml.ai/activations/index.html&#34;&gt;Activations&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/activations/fta/index.html&#34;&gt;Fuzzy Tiling Activations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Highlighted Research Paper PDFs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2204.10628.pdf&#34;&gt;Autoregressive Search Engines: Generating Substrings as Document Identifiers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2203.15556.pdf&#34;&gt;Training Compute-Optimal Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1910.02054.pdf&#34;&gt;ZeRO: Memory Optimizations Toward Training Trillion Parameter Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2204.02311.pdf&#34;&gt;PaLM: Scaling Language Modeling with Pathways&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/dall-e-2.pdf&#34;&gt;Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2203.14465.pdf&#34;&gt;STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2112.04426.pdf&#34;&gt;Improving language models by retrieving from trillions of tokens&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2003.08934.pdf&#34;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1706.03762.pdf&#34;&gt;Attention Is All You Need&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2006.11239.pdf&#34;&gt;Denoising Diffusion Probabilistic Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2109.08668.pdf&#34;&gt;Primer: Searching for Efficient Transformers for Language Modeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1803.02999.pdf&#34;&gt;On First-Order Meta-Learning Algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2103.00020.pdf&#34;&gt;Learning Transferable Visual Models From Natural Language Supervision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2109.02869.pdf&#34;&gt;The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1805.09801.pdf&#34;&gt;Meta-Gradient Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/google_maps_eta.pdf&#34;&gt;ETA Prediction with Graph Neural Networks in Google Maps&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/ponder_net.pdf&#34;&gt;PonderNet: Learning to Ponder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/muzero.pdf&#34;&gt;Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/gans_n_roses.pdf&#34;&gt;GANs N‚Äô Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/vit.pdf&#34;&gt;An Image is Worth 16X16 Word: Transformers for Image Recognition at Scale&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/resnet.pdf&#34;&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/distillation.pdf&#34;&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install labml-nn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Citing&lt;/h3&gt; &#xA;&lt;p&gt;If you use this for academic research, please cite it using the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{labml,&#xA; author = {Varuna Jayasiri, Nipun Wijerathne},&#xA; title = {labml.ai Annotated Paper Implementations},&#xA; year = {2020},&#xA; url = {https://nn.labml.ai/},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Other Projects&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://papers.labml.ai/&#34;&gt;üöÄ Trending Research Papers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This shows the most popular research papers on social media. It also aggregates links to useful resources like paper explanations videos and discussions.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://github.com/labmlai/labml&#34;&gt;üß™ labml.ai/labml&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is a library that let&#39;s you monitor deep learning model training and hardware usage from your mobile phone. It also comes with a bunch of other tools to help write deep learning code efficiently.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>fastai/fastai</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/fastai/fastai</id>
    <link href="https://github.com/fastai/fastai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The fastai deep learning library&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to fastai&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;fastai simplifies training fast and accurate neural nets using modern best practices&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/fastai/fastai/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt; &lt;a href=&#34;https://pypi.org/project/fastai/#description&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/fastai?color=blue&amp;amp;label=pypi%20version&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/fastai/fastai&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/vn/fastai/fastai?color=seagreen&amp;amp;label=conda%20version&#34; alt=&#34;Conda (channel only)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/fastai/docker-containers&#34;&gt;&lt;img src=&#34;https://github.com/fastai/docker-containers/workflows/Build%20fastai%20images/badge.svg?sanitize=true&#34; alt=&#34;Build fastai images&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/fastai/fastai/workflows/docs/badge.svg?sanitize=true&#34; alt=&#34;docs&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installing&lt;/h2&gt; &#xA;&lt;p&gt;You can use fastai without any installation by using &lt;a href=&#34;https://colab.research.google.com/&#34;&gt;Google Colab&lt;/a&gt;. In fact, every page of this documentation is also available as an interactive notebook - click &#34;Open in colab&#34; at the top of any page to open it (be sure to change the Colab runtime to &#34;GPU&#34; to have it run fast!) See the fast.ai documentation on &lt;a href=&#34;https://course.fast.ai/start_colab&#34;&gt;Using Colab&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;You can install fastai on your own machines with conda (highly recommended), as long as you&#39;re running Linux or Windows (NB: Mac is not supported). For Windows, please see the &#34;Running on Windows&#34; for important notes.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re using &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;miniconda&lt;/a&gt; (recommended) then run (note that if you replace &lt;code&gt;conda&lt;/code&gt; with &lt;a href=&#34;https://github.com/mamba-org/mamba&#34;&gt;mamba&lt;/a&gt; the install process will be much faster and more reliable):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c fastchan fastai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...or if you&#39;re using &lt;a href=&#34;https://www.anaconda.com/products/individual&#34;&gt;Anaconda&lt;/a&gt; then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c fastchan fastai anaconda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install with pip, use: &lt;code&gt;pip install fastai&lt;/code&gt;. If you install with pip, you should install PyTorch first by following the PyTorch &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you plan to develop fastai yourself, or want to be on the cutting edge, you can use an editable install (if you do this, you should also use an editable install of &lt;a href=&#34;https://github.com/fastai/fastcore&#34;&gt;fastcore&lt;/a&gt; to go with it.) First install PyTorch, and then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/fastai/fastai&#xA;pip install -e &#34;fastai[dev]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Learning fastai&lt;/h2&gt; &#xA;&lt;p&gt;The best way to get started with fastai (and deep learning) is to read &lt;a href=&#34;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&#34;&gt;the book&lt;/a&gt;, and complete &lt;a href=&#34;https://course.fast.ai&#34;&gt;the free course&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To see what&#39;s possible with fastai, take a look at the &lt;a href=&#34;https://docs.fast.ai/quick_start.html&#34;&gt;Quick Start&lt;/a&gt;, which shows how to use around 5 lines of code to build an image classifier, an image segmentation model, a text sentiment model, a recommendation system, and a tabular model. For each of the applications, the code is much the same.&lt;/p&gt; &#xA;&lt;p&gt;Read through the &lt;a href=&#34;https://docs.fast.ai/tutorial&#34;&gt;Tutorials&lt;/a&gt; to learn how to train your own models on your own datasets. Use the navigation sidebar to look through the fastai documentation. Every class, function, and method is documented here.&lt;/p&gt; &#xA;&lt;p&gt;To learn about the design and motivation of the library, read the &lt;a href=&#34;https://www.mdpi.com/2078-2489/11/2/108/htm&#34;&gt;peer reviewed paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;About fastai&lt;/h2&gt; &#xA;&lt;p&gt;fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A new type dispatch system for Python along with a semantic type hierarchy for tensors&lt;/li&gt; &#xA; &lt;li&gt;A GPU-optimized computer vision library which can be extended in pure Python&lt;/li&gt; &#xA; &lt;li&gt;An optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4‚Äì5 lines of code&lt;/li&gt; &#xA; &lt;li&gt;A novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training&lt;/li&gt; &#xA; &lt;li&gt;A new data block API&lt;/li&gt; &#xA; &lt;li&gt;And much more...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;fastai is organized around two main design goals: to be approachable and rapidly productive, while also being deeply hackable and configurable. It is built on top of a hierarchy of lower-level APIs which provide composable building blocks. This way, a user wanting to rewrite part of the high-level API or add particular behavior to suit their needs does not have to learn how to use the lowest level.&lt;/p&gt; &#xA;&lt;img alt=&#34;Layered API&#34; src=&#34;https://raw.githubusercontent.com/fastai/fastai/master/nbs/images/layered.png&#34; width=&#34;345&#34; style=&#34;max-width: 345px&#34;&gt; &#xA;&lt;h2&gt;Migrating from other libraries&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s very easy to migrate from plain PyTorch, Ignite, or any other PyTorch-based library, or even to use fastai in conjunction with other libraries. Generally, you&#39;ll be able to use all your existing data processing code, but will be able to reduce the amount of code you require for training, and more easily take advantage of modern best practices. Here are migration guides from some popular libraries to help you on your way:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.fast.ai/migrating_pytorch&#34;&gt;Plain PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.fast.ai/migrating_ignite&#34;&gt;Ignite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.fast.ai/migrating_lightning&#34;&gt;Lightning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.fast.ai/migrating_catalyst&#34;&gt;Catalyst&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Windows Support&lt;/h2&gt; &#xA;&lt;p&gt;When installing with &lt;code&gt;mamba&lt;/code&gt; or &lt;code&gt;conda&lt;/code&gt; replace &lt;code&gt;-c fastchan&lt;/code&gt; in the installation with &lt;code&gt;-c pytorch -c nvidia -c fastai&lt;/code&gt;, since fastchan is not currently supported on Windows.&lt;/p&gt; &#xA;&lt;p&gt;Due to python multiprocessing issues on Jupyter and Windows, &lt;code&gt;num_workers&lt;/code&gt; of &lt;code&gt;Dataloader&lt;/code&gt; is reset to 0 automatically to avoid Jupyter hanging. This makes tasks such as computer vision in Jupyter on Windows many times slower than on Linux. This limitation doesn&#39;t exist if you use fastai from a script.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/fastai/fastai/raw/master/nbs/examples/dataloader_spawn.py&#34;&gt;this example&lt;/a&gt; to fully leverage the fastai API on Windows.&lt;/p&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;To run the tests in parallel, launch:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;nbdev_test_nbs&lt;/code&gt; or &lt;code&gt;make test&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For all the tests to pass, you&#39;ll need to install the dependencies specified as part of dev_requirements in settings.ini&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install -e .[dev]&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tests are written using &lt;code&gt;nbdev&lt;/code&gt;, for example see the documentation for &lt;code&gt;test_eq&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;After you clone this repository, please run &lt;code&gt;nbdev_install_git_hooks&lt;/code&gt; in your terminal. This sets up git hooks, which clean up the notebooks to remove the extraneous stuff stored in the notebooks (e.g. which cells you ran) which causes unnecessary merge conflicts.&lt;/p&gt; &#xA;&lt;p&gt;Before submitting a PR, check that the local library and notebooks match. The script &lt;code&gt;nbdev_diff_nbs&lt;/code&gt; can let you know if there is a difference between the local library and the notebooks.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you made a change to the notebooks in one of the exported cells, you can export it to the library with &lt;code&gt;nbdev_build_lib&lt;/code&gt; or &lt;code&gt;make fastai&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you made a change to the library, you can export it back to the notebooks with &lt;code&gt;nbdev_update_lib&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker Containers&lt;/h2&gt; &#xA;&lt;p&gt;For those interested in official docker containers for this project, they can be found &lt;a href=&#34;https://github.com/fastai/docker-containers#fastai&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>The-Art-of-Hacking/h4cker</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/The-Art-of-Hacking/h4cker</id>
    <link href="https://github.com/The-Art-of-Hacking/h4cker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository is primarily maintained by Omar Santos and includes thousands of resources related to ethical hacking / penetration testing, digital forensics and incident response (DFIR), vulnerability research, exploit development, reverse engineering, and more.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cyber Security Resources&lt;/h1&gt; &#xA;&lt;center&gt;&#xA; &lt;img src=&#34;https://h4cker.org/img/h4cker2.PNG&#34; width=&#34;200&#34; height=&#34;300&#34;&gt; &#xA;&lt;/center&gt; &#xA;&lt;p&gt;This repository includes thousands of cybersecurity-related references and resources and it is maintained by &lt;a href=&#34;https://omarsantos.io/&#34;&gt;Omar Santos&lt;/a&gt;. This GitHub repository has been created to provide supplemental material to several books, video courses, and live training created by Omar Santos and other co-authors. It provides over 9,000 references, scripts, tools, code, and other resources that help offensive and defensive security professionals learn and develop new skills. This GitHub repository provides guidance on how build your own hacking environment, learn about offensive security (ethical hacking) techniques, vulnerability research, exploit development, reverse engineering, malware analysis, threat intelligence, threat hunting, digital forensics and incident response (DFIR), includes examples of real-life penetration testing reports, and more.&lt;/p&gt; &#xA;&lt;h2&gt;The Art of Hacking Series&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;http://theartofhacking.org&#34;&gt;Art of Hacking Series&lt;/a&gt; is a series of video courses and live training that help you get up and running with your cybersecurity career. The following are the different video courses that are part of the Art of Hacking series:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/library/view/security-penetration-testing/9780134833989&#34;&gt;Security Penetration Testing (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/library/view/wireless-networks-iot/9780134854632/&#34;&gt;Wireless Networks, IoT, and Mobile Devices Hacking (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/videos/enterprise-penetration-testing/9780134854779&#34;&gt;Enterprise Penetration Testing and Continuous Monitoring (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/videos/hacking-web-applications/9780135261422&#34;&gt;Hacking Web Applications: Security Penetration Testing for Today&#39;s DevOps and Cloud Environments (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These courses serve as comprehensive guide for any network and security professional who is starting a career in ethical hacking and penetration testing. It also can help individuals preparing for the &lt;a href=&#34;https://www.offensive-security.com/information-security-certifications/&#34;&gt;Offensive Security Certified Professional (OSCP)&lt;/a&gt;, the &lt;a href=&#34;https://www.eccouncil.org/programs/certified-ethical-hacker-ceh/&#34;&gt;Certified Ethical Hacker (CEH)&lt;/a&gt;, &lt;a href=&#34;https://certification.comptia.org/certifications/pentest&#34;&gt;CompTIA PenTest+&lt;/a&gt; and any other ethical hacking certification. This course helps any cyber security professional that want to learn the skills required to becoming a professional ethical hacker or that want to learn more about general hacking methodologies and concepts.&lt;/p&gt; &#xA;&lt;p&gt;These video courses are published by Pearson, but this GitHub repository is maintained and supported by the lead author of the series &lt;a href=&#34;https://omarsantos.io/&#34;&gt;Omar Santos&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Live Training&lt;/h2&gt; &#xA;&lt;p&gt;Other Live Training and Video Courses: &lt;a href=&#34;https://h4cker.org/training&#34;&gt;https://h4cker.org/training&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>wesm/pydata-book</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/wesm/pydata-book</id>
    <link href="https://github.com/wesm/pydata-book" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Materials and IPython notebooks for &#34;Python for Data Analysis&#34; by Wes McKinney, published by O&#39;Reilly Media&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python for Data Analysis, 2nd Edition&lt;/h1&gt; &#xA;&lt;p&gt;Materials and IPython notebooks for &#34;Python for Data Analysis&#34; by Wes McKinney, published by O&#39;Reilly Media&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://amzn.to/2vvBijB&#34;&gt;Buy the book on Amazon&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://notebooks.azure.com/import/gh/wesm/pydata-book&#34;&gt;&lt;img src=&#34;https://notebooks.azure.com/launch.png&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Follow Wes on Twitter: &lt;a href=&#34;https://twitter.com/wesmckinn&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/wesmckinn.svg?style=social&amp;amp;label=Follow&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;1st Edition Readers&lt;/h1&gt; &#xA;&lt;p&gt;If you are reading the &lt;a href=&#34;http://amzn.to/2vvBijB&#34;&gt;1st Edition&lt;/a&gt; (published in 2012), please find the reorganized book materials on the &lt;a href=&#34;https://github.com/wesm/pydata-book/tree/1st-edition&#34;&gt;&lt;code&gt;1st-edition&lt;/code&gt; branch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Translations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BrambleXu/pydata-notebook&#34;&gt;Chinese&lt;/a&gt; by Xu Liang&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mbiesiad/pydata-book/tree/pl_PL&#34;&gt;Polish&lt;/a&gt; by Michal Biesiada&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;IPython Notebooks:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch02.ipynb&#34;&gt;Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch03.ipynb&#34;&gt;Chapter 3: Built-in Data Structures, Functions, and Files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch04.ipynb&#34;&gt;Chapter 4: NumPy Basics: Arrays and Vectorized Computation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch05.ipynb&#34;&gt;Chapter 5: Getting Started with pandas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch06.ipynb&#34;&gt;Chapter 6: Data Loading, Storage, and File Formats&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch07.ipynb&#34;&gt;Chapter 7: Data Cleaning and Preparation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch08.ipynb&#34;&gt;Chapter 8: Data Wrangling: Join, Combine, and Reshape&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch09.ipynb&#34;&gt;Chapter 9: Plotting and Visualization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch10.ipynb&#34;&gt;Chapter 10: Data Aggregation and Group Operations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch11.ipynb&#34;&gt;Chapter 11: Time Series&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch12.ipynb&#34;&gt;Chapter 12: Advanced pandas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch13.ipynb&#34;&gt;Chapter 13: Introduction to Modeling Libraries in Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch14.ipynb&#34;&gt;Chapter 14: Data Analysis Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/appa.ipynb&#34;&gt;Appendix A: Advanced NumPy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;h3&gt;Code&lt;/h3&gt; &#xA;&lt;p&gt;The code in this repository, including all code samples in the notebooks listed above, is released under the &lt;a href=&#34;https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/LICENSE-CODE&#34;&gt;MIT license&lt;/a&gt;. Read more at the &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>fastai/fastbook</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/fastai/fastbook</id>
    <link href="https://github.com/fastai/fastbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The fastai book, published as Jupyter Notebooks&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README.md&#34;&gt;English&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_es.md&#34;&gt;Spanish&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_ko.md&#34;&gt;Korean&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_zh.md&#34;&gt;Chinese&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_bn.md&#34;&gt;Bengali&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_id.md&#34;&gt;Indonesian&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_it.md&#34;&gt;Italian&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_pt.md&#34;&gt;Portuguese&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_vn.md&#34;&gt;Vietnamese&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;The fastai book&lt;/h1&gt; &#xA;&lt;p&gt;These notebooks cover an introduction to deep learning, &lt;a href=&#34;https://docs.fast.ai/&#34;&gt;fastai&lt;/a&gt;, and &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;. fastai is a layered API for deep learning; for more information, see &lt;a href=&#34;https://www.mdpi.com/2078-2489/11/2/108&#34;&gt;the fastai paper&lt;/a&gt;. Everything in this repo is copyright Jeremy Howard and Sylvain Gugger, 2020 onwards.&lt;/p&gt; &#xA;&lt;p&gt;These notebooks are used for &lt;a href=&#34;https://course.fast.ai&#34;&gt;a MOOC&lt;/a&gt; and form the basis of &lt;a href=&#34;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&#34;&gt;this book&lt;/a&gt;, which is currently available for purchase. It does not have the same GPL restrictions that are on this repository.&lt;/p&gt; &#xA;&lt;p&gt;The code in the notebooks and python &lt;code&gt;.py&lt;/code&gt; files is covered by the GPL v3 license; see the LICENSE file for details. The remainder (including all markdown cells in the notebooks and other prose) is not licensed for any redistribution or change of format or medium, other than making copies of the notebooks or forking this repo for your own private use. No commercial or broadcast use is allowed. We are making these materials freely available to help you learn deep learning, so please respect our copyright and these restrictions.&lt;/p&gt; &#xA;&lt;p&gt;If you see someone hosting a copy of these materials somewhere else, please let them know that their actions are not allowed and may lead to legal action. Moreover, they would be hurting the community because we&#39;re not likely to release additional materials in this way if people ignore our copyright.&lt;/p&gt; &#xA;&lt;h2&gt;Colab&lt;/h2&gt; &#xA;&lt;p&gt;Instead of cloning this repo and opening it on your machine, you can read and work with the notebooks using &lt;a href=&#34;https://research.google.com/colaboratory/&#34;&gt;Google Colab&lt;/a&gt;. This is the recommended approach for folks who are just getting started -- there&#39;s no need to set up a Python development environment on your own machine, since you can just work directly in your web-browser.&lt;/p&gt; &#xA;&lt;p&gt;You can open any chapter of the book in Colab by clicking on one of these links: &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/app_jupyter.ipynb&#34;&gt;Introduction to Jupyter&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/01_intro.ipynb&#34;&gt;Chapter 1, Intro&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/02_production.ipynb&#34;&gt;Chapter 2, Production&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/03_ethics.ipynb&#34;&gt;Chapter 3, Ethics&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Chapter 4, MNIST Basics&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb&#34;&gt;Chapter 5, Pet Breeds&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/06_multicat.ipynb&#34;&gt;Chapter 6, Multi-Category&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/07_sizing_and_tta.ipynb&#34;&gt;Chapter 7, Sizing and TTA&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/08_collab.ipynb&#34;&gt;Chapter 8, Collab&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/09_tabular.ipynb&#34;&gt;Chapter 9, Tabular&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/10_nlp.ipynb&#34;&gt;Chapter 10, NLP&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/11_midlevel_data.ipynb&#34;&gt;Chapter 11, Mid-Level API&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/12_nlp_dive.ipynb&#34;&gt;Chapter 12, NLP Deep-Dive&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/13_convolutions.ipynb&#34;&gt;Chapter 13, Convolutions&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/14_resnet.ipynb&#34;&gt;Chapter 14, Resnet&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/15_arch_details.ipynb&#34;&gt;Chapter 15, Arch Details&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/16_accel_sgd.ipynb&#34;&gt;Chapter 16, Optimizers and Callbacks&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/17_foundations.ipynb&#34;&gt;Chapter 17, Foundations&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/18_CAM.ipynb&#34;&gt;Chapter 18, GradCAM&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/19_learner.ipynb&#34;&gt;Chapter 19, Learner&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/20_conclusion.ipynb&#34;&gt;Chapter 20, conclusion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;If you make any pull requests to this repo, then you are assigning copyright of that work to Jeremy Howard and Sylvain Gugger. (Additionally, if you are making small edits to spelling or text, please specify the name of the file and a very brief description of what you&#39;re fixing. It&#39;s difficult for reviewers to know which corrections have already been made. Thank you.)&lt;/p&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you wish to cite the book, you may use the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@book{howard2020deep,&#xA;title={Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD},&#xA;author={Howard, J. and Gugger, S.},&#xA;isbn={9781492045526},&#xA;url={https://books.google.no/books?id=xd6LxgEACAAJ},&#xA;year={2020},&#xA;publisher={O&#39;Reilly Media, Incorporated}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>kubernetes/community</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/kubernetes/community</id>
    <link href="https://github.com/kubernetes/community" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kubernetes community content&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kubernetes Community&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the Kubernetes community!&lt;/p&gt; &#xA;&lt;p&gt;This is the starting point for joining and contributing to the Kubernetes community - improving docs, improving code, giving talks etc.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about the project structure and organization, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/governance.md&#34;&gt;Project Governance&lt;/a&gt; information.&lt;/p&gt; &#xA;&lt;h2&gt;Communicating&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/communication/&#34;&gt;communication&lt;/a&gt; page lists communication channels like chat, issues, mailing lists, conferences, etc.&lt;/p&gt; &#xA;&lt;p&gt;For more specific topics, try a SIG.&lt;/p&gt; &#xA;&lt;h2&gt;Governance&lt;/h2&gt; &#xA;&lt;p&gt;Kubernetes has the following types of groups that are officially supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Committees&lt;/strong&gt; are named sets of people that are chartered to take on sensitive topics. This group is encouraged to be as open as possible while achieving its mission but, because of the nature of the topics discussed, private communications are allowed. Examples of committees include the steering committee and things like security or code of conduct.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Special Interest Groups (SIGs)&lt;/strong&gt; are persistent open groups that focus on a part of the project. SIGs must have open and transparent proceedings. Anyone is welcome to participate and contribute provided they follow the Kubernetes Code of Conduct. The purpose of a SIG is to own and develop a set of &lt;strong&gt;subprojects&lt;/strong&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Subprojects&lt;/strong&gt; Each SIG can have a set of subprojects. These are smaller groups that can work independently. Some subprojects will be part of the main Kubernetes deliverables while others will be more speculative and live in the &lt;code&gt;kubernetes-sigs&lt;/code&gt; github org.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Working Groups&lt;/strong&gt; are temporary groups that are formed to address issues that cross SIG boundaries. Working groups do not own any code or other long term artifacts. Working groups can report back and act through involved SIGs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User Groups&lt;/strong&gt; are groups for facilitating communication and discovery of information related to topics that have long term relevance to large groups of Kubernetes users. They do not have ownership of parts of the Kubernetes code base.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/governance.md&#34;&gt;full governance doc&lt;/a&gt; for more details on these groups.&lt;/p&gt; &#xA;&lt;p&gt;A SIG can have its own policy for contribution, described in a &lt;code&gt;README&lt;/code&gt; or &lt;code&gt;CONTRIBUTING&lt;/code&gt; file in the SIG folder in this repo (e.g. &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/sig-cli/CONTRIBUTING.md&#34;&gt;sig-cli/CONTRIBUTING.md&lt;/a&gt;), and its own mailing list, slack channel, etc.&lt;/p&gt; &#xA;&lt;p&gt;If you want to edit details about a SIG (e.g. its weekly meeting time or its leads), please follow &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/generator&#34;&gt;these instructions&lt;/a&gt; that detail how our docs are auto-generated.&lt;/p&gt; &#xA;&lt;h2&gt;Learn to Build&lt;/h2&gt; &#xA;&lt;p&gt;Links in &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/devel/README.md&#34;&gt;contributors/devel/README.md&lt;/a&gt; lead to many relevant technical topics.&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;A first step to contributing is to pick from the &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/sig-list.md&#34;&gt;list of kubernetes SIGs&lt;/a&gt;. Start attending SIG meetings, join the slack channel and subscribe to the mailing list. SIGs will often have a set of &#34;help wanted&#34; issues that can help new contributors get involved.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/README.md&#34;&gt;Contributor Guide&lt;/a&gt; provides detailed instruction on how to get your ideas and bug fixes seen and accepted, including:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;How to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/first-contribution.md#file-an-issue&#34;&gt;file an issue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;How to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/first-contribution.md#find-something-to-work-on&#34;&gt;find something to work on&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;How to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/contributing.md#opening-a-pull-request&#34;&gt;open a pull request&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Membership&lt;/h2&gt; &#xA;&lt;p&gt;We encourage all contributors to become members. We aim to grow an active, healthy community of contributors, reviewers, and code owners. Learn more about requirements and responsibilities of membership in our &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/community-membership.md&#34;&gt;Community Membership&lt;/a&gt; page.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lixin4ever/Conference-Acceptance-Rate</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/lixin4ever/Conference-Acceptance-Rate</id>
    <link href="https://github.com/lixin4ever/Conference-Acceptance-Rate" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Acceptance rates for the major AI conferences&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Statistics of acceptance rate for the main AI conferences&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lixin4ever/Conference-Acceptance-Rate/master/conference_trends.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Natural Language Processing and Computational Linguistics&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Conference &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Long Paper &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Short Paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.2% (146/572)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.1% (139/551)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.0% (173/692)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.4% (145/648)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.0% (231/825)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.0% (97/463)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.0% (195/751)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.9% (107/567)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.3% (258/1018)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.0% (126/526)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.7% (447/1737)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.2% (213/1168)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.4% (571/2244)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.6% (208/1185)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.5% (571/2327)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13.6% (139/1023)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACL&#39;21 Findings&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.6% (339/2327)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.5% (118/1023)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.4% (155/510)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27.8% (70/252)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.2% (157/600)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.1% (155/700)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.8% (177/687)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.8% (87/400)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.8% (216/836)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.4% (107/582)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.5% (351/1376)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.2% (198/855)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.6% (465/1813)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.5% (218/1063)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.5% (602/2455)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.6% (150/904)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;20 Findings&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13.5% (332/2455)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;12.7% (115/904)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.6% (650/2540)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.9% (190/1060)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EMNLP&#39;21 Findings&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.8% (300/2540)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.2% (119/1060)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NAACL-HLT&#39;13&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.0% (88/293)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.1% (51/162)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NAACL-HLT&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.1% (117/402)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.1% (69/312)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NAACL-HLT&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.3% (100/396)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.9% (82/284)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NAACL-HLT&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.0% (207/647)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.4% (125/425)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NAACL-HLT&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.3% (281/1067)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.3% (142/666)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NAACL-HLT&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.2% (366/1254)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.6% (123/544)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLING&#39;12&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27% (311/1000+)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLING&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.8% (217/705)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLING&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.4% (337/1039)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLING&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;37.4% (332/888)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLING&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;33.4% (622/1862)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Computer Vision and Pattern Recognition&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Conference &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Long Paper &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Short Paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.9% (540/1807) (104 orals and 436 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.3% (602/2123) (71 orals and 531 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.9% (643/2145) (83 orals, 123 spotlights and 437 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.9% (783/2620) (71 orals, 144 spotlights and 568 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.6% (979/3303) (70 orals, 224 spotlights and 685 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.0% (1294/5160) (288 short orals and 1294 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.1% (1470/6656)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.7% (1661/7015) (295 orals and 1366 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CVPR&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.3% (2067/8161)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICCV&#39;13&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27.9% (454/1629) (41 orals and 413 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICCV&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.9% (525/1698)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICCV&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.0% (621/2143) (45 orals, 56 spotlights and 520 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICCV&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.0% (1077/4304) (187 short orals and 1077 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ECCV&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27.9% (363/1444) (38 orals and 325 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ECCV&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.6% (415/1561) (28 orals, 45 spotlights and 342 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ECCV&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;31.8% (776/2439) (59 orals and 717 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ECCV&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27.1% (1361/5025) (104 orals, 161 spotlights and 1096 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Machine Learning and Learning Theory&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Conference &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Long Paper &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Short Paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.0% (Cycle I), 22.0% (Cycle II)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.0% (270/1037)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.0% (322/?)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.9% (434/1676)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.1% (621/2473)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.6% (773/3424)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.8% (1088/4990)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.5% (1184/5513) (166 long talks, 1018 short talks)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICML&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.9% (1235/5630) (118 long talks, 1117 short talks)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.7% (414/1678)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.9% (403/1838)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.6% (569/2403)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.9% (678/3240) (40 orals, 112 spotlights and 526 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.8% (1011/4856) (30 orals, 168 spotlights and 813 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.1% (1428/6743) (36 orals, 164 spotlights and 1228 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.1% (1900/9454) (105 orals, 280 spotlights and 1515 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeurIPS&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.69% (2344/9122) (55 orals, 260 spotlights and 2029 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;39.1% (198/507) (15 orals and 183 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.0% (314/981) (23 orals and 291 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;31.4% (500/1591) (24 orals and 476 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.5% (687/2594) (48 orals, 107 spotlights and 532 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.7% (860/2997) (53 orals, 114 spotlights and 693 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICLR&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.9% (1095/3328) (54 orals, 176 spotlights and 865 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLT&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.1% (45/140)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLT&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;34.8% (62/178)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLT&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.1% (53/203)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLT&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.5% (74/228)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLT&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27.2% (91/335)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLT&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.0% (118/393)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COLT&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.9% (120/388)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.0% (94/292)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;34.0% (99/291)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;31.0% (85/275)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;31.0% (87/282)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.8% (104/337)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.0% (118/450)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27.5% (142/515)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.3% (205/777)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UAI&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.3% (230/712) (36 orals and 194 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;35.8% (120/335)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.7% (127/442)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.7% (165/537)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;31.7% (168/530)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;33.2% (214/645)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;32.4% (360/1111)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.8% (455/1527) (48 orals)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AISTATS&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;29.2% (493/1685)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Artificial Intelligence&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Conference &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Long Paper &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Short Paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.0% (398/1406)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.7% (531/1991)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.8% (549/2132)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.6% (638/2590)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.6% (933/3800)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.2% (1150/7095)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.6% (1591/7737)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.4% (1692/7911)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AAAI&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.0% (1349/9020)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;13&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.0% (413/1473)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.6% (572/1996)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.0% (551/2294)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.0% (660/2540)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.5% (710/3470)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.9% (850/4752)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;12.6% (592/4717)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IJCAI&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13.9% (587/4204)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Data Mining and Information Retrieval&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Conference &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Long Paper &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Short Paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KDD&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.6% (151/1036)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KDD&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.5% (160/819)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KDD&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13.7% (142/1115)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KDD&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.4% (130/748)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KDD&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.4% (181/983) (107 orals and 74 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KDD&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.2% (170/1200) (110 orals and 60 posters)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KDD&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.9% (216/1279)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.0% (82/387)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;40.0% (104/263)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.0% (70/351)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;31.3% (79/252)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.0% (62/341)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.6% (104/339)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.0% (78/362)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.0% (121/398)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.0% (86/409)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.0% (98/327)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.7% (84/426)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.4% (108/443)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.5% (147/555)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.2% (153/507)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.0% (151/720)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;27.6% (145/526)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SIGIR&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.3% (161/794)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.7% (165/667)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13.0% (84/645)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.0% (131/929)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.0% (115/727)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.0% (164/966)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.0% (171/1140)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.0% (225/1247)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.9% (72/361)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.2% (217/1129)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;24.7% (98/397)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.6% (357/1736)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheWebConf&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.7% (323/1822)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.0% (64/355)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.4% (39/238)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.2% (67/368)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.8% (80/505)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.1% (84/514)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.4% (84/511)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.8% (91/615)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.6% (112/603)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSDM&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.8% (80/505)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.0% (175/838)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.9% (57/260)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;26.0% (165/646)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.0% (69/276)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.0% (160/701)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.5% (55/234)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.0% (171/855)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.4% (119/419)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.0% (147/862)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.2% (96/413)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.4% (200/1030)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.3% (100/470)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.0% (193/920)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.9% (103/397)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CIKM&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.7% (271/1251)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28.3% (177/626)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.8% (71/727)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.8% (71/727)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;8.4% (68/807)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.7% (78/807)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;8.6% (78/904)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.0% (100/904)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.3% (72/778)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.7% (83/778)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;8.9% (84/948)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.1% (105/948)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.1% (95/1046)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.5% (99/1046)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.8% (91/930)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.9% (92/930)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICDM&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.9% (98/990)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.1% (100/990)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RecSys&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.0% (35/152)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RecSys&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.2% (29/159)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RecSys&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;20.8% (26/125)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.4% (20/122)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RecSys&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.7% (32/181)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RecSys&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.0% (36/189)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RecSys&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.9% (39/218)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Speech and Signal Processing&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Conference &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Long Paper &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Short Paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;51.0% (~743/1458)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;50.5% (779/1541)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;52.0% (799/1582)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;54.3% (749/1320)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;49.3% (914/1855)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;~47% (?/?)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;INTERSPEECH&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;48.4% (963/1990)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;48.0% (1709/3500)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;52.0% (1207/2322)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;47.0% (1265/2682)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;52.0% (1220/2518)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;49.7% (1406/2829)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;19&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;46.5% (1774/3815)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;48.0% (1734/3610)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ICASSP&#39;22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;45.0% (1785/3967)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;For &lt;strong&gt;KDD&lt;/strong&gt; and &lt;strong&gt;TheWebConf&lt;/strong&gt; (formerly known as &lt;strong&gt;WWW&lt;/strong&gt;), only the papers from research track are counted.&lt;/li&gt; &#xA; &lt;li&gt;For &lt;strong&gt;ICDM&lt;/strong&gt;, submissions of short paper and those of long paper are in the same session and the decision of the paper type is made according to its quality.&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>jeffheaton/t81_558_deep_learning</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/jeffheaton/t81_558_deep_learning</id>
    <link href="https://github.com/jeffheaton/t81_558_deep_learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Washington University (in St. Louis) Course T81-558: Applications of Deep Neural Networks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;T81 558:Applications of Deep Neural Networks&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.wustl.edu&#34;&gt;Washington University in St. Louis&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Instructor: &lt;a href=&#34;https://sites.wustl.edu/jeffheaton/&#34;&gt;Jeff Heaton&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The content of this course changes as technology evolves&lt;/strong&gt;, to keep up to date with changes &lt;a href=&#34;https://github.com/jeffheaton&#34;&gt;follow me on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Section 1. Fall 2022, Monday, 2:30 PM, Location: TBD&lt;/li&gt; &#xA; &lt;li&gt;Section 2. Fall 2022, Online&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Course Description&lt;/h1&gt; &#xA;&lt;p&gt;Deep learning is a group of exciting new technologies for neural networks. Through a combination of advanced training techniques and neural network architectural components, it is now possible to create neural networks that can handle tabular data, images, text, and audio as both input and output. Deep learning allows a neural network to learn hierarchies of information in a way that is like the function of the human brain. This course will introduce the student to classic neural network structures, Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU), General Adversarial Networks (GAN) and reinforcement learning. Application of these architectures to computer vision, time series, security, natural language processing (NLP), and data generation will be covered. High Performance Computing (HPC) aspects will demonstrate how deep learning can be leveraged both on graphical processing units (GPUs), as well as grids. Focus is primarily upon the application of deep learning to problems, with some introduction to mathematical foundations. Students will use the Python programming language to implement deep learning using Google TensorFlow and Keras. It is not necessary to know Python prior to this course; however, familiarity of at least one programming language is assumed. This course will be delivered in a hybrid format that includes both classroom and online instruction.&lt;/p&gt; &#xA;&lt;h1&gt;Textbook&lt;/h1&gt; &#xA;&lt;p&gt;The complete text for this course is here on GitHub. This same material is also available in &lt;a href=&#34;https://www.heatonresearch.com/book/applications-deep-neural-networks-keras.html&#34;&gt;book format&lt;/a&gt;. The course textbook is ‚ÄúApplications of Deep Neural networks with Keras‚Äú, ISBN 9798416344269.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to cite the material from this course/book, please use the following BibTex citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{heaton2020applications,&#xA;    title={Applications of Deep Neural Networks},&#xA;    author={Jeff Heaton},&#xA;    year={2020},&#xA;    eprint={2009.05673},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Objectives&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Explain how neural networks (deep and otherwise) compare to other machine learning models.&lt;/li&gt; &#xA; &lt;li&gt;Determine when a deep neural network would be a good choice for a particular problem.&lt;/li&gt; &#xA; &lt;li&gt;Demonstrate your understanding of the material through a final project uploaded to GitHub.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Syllabus&lt;/h1&gt; &#xA;&lt;p&gt;This syllabus presents the expected class schedule, due dates, and reading assignments. &lt;a href=&#34;https://data.heatonresearch.com/wustl/jheaton-t81-558-spring-2022-syllabus.pdf&#34;&gt;Download current syllabus.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Module&lt;/th&gt; &#xA;   &lt;th&gt;Content&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_01_1_overview.ipynb&#34;&gt;Module 1&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 08/29/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 1: Python Preliminaries&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 1.1: Course Overview&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.2: Introduction to Python&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.3: Python Lists, Dictionaries, Sets &amp;amp; JSON&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.4: File Handling&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.5: Functions, Lambdas, and Map/ReducePython Preliminaries&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (first meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_02_1_python_pandas.ipynb&#34;&gt;Module 2&lt;/a&gt;&lt;br&gt;Week of 09/12/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 2: Python for Machine Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; Part 2.1: Introduction to Pandas for Deep Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.2: Encoding Categorical Values in Pandas&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.3: Grouping, Sorting, and Shuffling&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.4: Using Apply and Map in Pandas&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.5: Feature Engineering in Padas&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class1.ipynb&#34;&gt;Module 1 Program&lt;/a&gt; due: 09/13/2022&lt;/li&gt;&#xA;     &lt;li&gt; Icebreaker due: 09/13/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_03_1_neural_net.ipynb&#34;&gt;Module 3&lt;/a&gt;&lt;br&gt;Week of 09/19/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 3: TensorFlow and Keras for Neural Networks&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 3.1: Deep Learning and Neural Network Introduction&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.2: Introduction to Tensorflow &amp;amp; Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.3: Saving and Loading a Keras Neural Network&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.4: Early Stopping in Keras to Prevent Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.5: Extracting Keras Weights and Manual Neural Network Calculation&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class2.ipynb&#34;&gt;Module 2: Program&lt;/a&gt; due: 09/20/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_04_1_feature_encode.ipynb&#34;&gt;Module 4&lt;/a&gt;&lt;br&gt;Week of 09/26/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 4: Training for Tabular Data&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 4.1: Encoding a Feature Vector for Keras Deep Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.2: Keras Multiclass Classification for Deep Neural Networks with ROC and AUC&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.3: Keras Regression for Deep Neural Networks with RMSE&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Training&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.5: Neural Network RMSE and Log Loss Error Calculation from Scratch&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class3.ipynb&#34;&gt;Module 3 Program&lt;/a&gt; due: 09/27/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_05_1_reg_ridge_lasso.ipynb&#34;&gt;Module 5&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 10/03/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 5: Regularization and Dropout&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 5.1: Introduction to Regularization: Ridge and Lasso&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.2: Using K-Fold Cross Validation with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.3: Using L1 and L2 Regularization with Keras to Decrease Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.4: Drop Out for Keras to Decrease Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.5: Bootstrapping and Benchmarking Hyperparameters&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class4.ipynb&#34;&gt;Module 4 Program&lt;/a&gt; due: 10/04/2022&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (second meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_06_1_python_images.ipynb&#34;&gt;Module 6&lt;/a&gt;&lt;br&gt;Week of 10/17/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 6: CNN for Vision&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;      Part 6.1: Image Processing in Python&#xA;     &lt;li&gt;Part 6.2: Using Convolutional Networks with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.3: Using Pretrained Neural Networks&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.4: Looking at Keras Generators and Image Augmentation&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.5: Recognizing Multiple Images with YOLOv5&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class5.ipynb&#34;&gt;Module 5 Program&lt;/a&gt; due: 10/18/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_07_1_gan_intro.ipynb&#34;&gt;Module 7&lt;/a&gt;&lt;br&gt;Week of 10/24/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 7: Generative Adversarial Networks (GANs)&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 7.1: Introduction to GANS for Image and Data Generation&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.2: Train StyleGAN3 with your Own Images&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.3: Exploring the StyleGAN Latent Vector&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.4: GANS to Enhance Old Photographs Deoldify&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.5: GANs for Tabular Synthetic Data Generation&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class6.ipynb&#34;&gt;Module 6 Assignment&lt;/a&gt; due: 10/25/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_08_1_kaggle_intro.ipynb&#34;&gt;Module 8&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 10/31/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 8: Kaggle&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 8.1: Introduction to Kaggle&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.2: Building Ensembles with Scikit-Learn and Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.4: Bayesian Hyperparameter Optimization for Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.5: Current Semester&#39;s Kaggle&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class7.ipynb&#34;&gt;Module 7 Assignment&lt;/a&gt; due: 11/01/2022&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (third meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_09_1_keras_transfer.ipynb&#34;&gt;Module 9&lt;/a&gt;&lt;br&gt;Week of 11/07/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 9: Transfer Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 9.1: Introduction to Keras Transfer Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.2: Keras Transfer Learning for Computer Vision&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.3: Transfer Learning for NLP with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.4: Transfer Learning for Facial Feature Recognition&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.5: Transfer Learning for Style Transfer&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class8.ipynb&#34;&gt;Module 8 Assignment&lt;/a&gt; due: 11/08/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_10_1_timeseries.ipynb&#34;&gt;Module 10&lt;/a&gt;&lt;br&gt;Week of 11/14/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 10: Time Series in Keras&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 10.1: Time Series Data Encoding for Deep Learning, Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.2: Programming LSTM with Keras and&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.3: Text Generation with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.4: Introduction to Transformers&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.5: Transformers for Timeseries&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class9.ipynb&#34;&gt;Module 9 Assignment&lt;/a&gt; due: 11/15/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_11_01_huggingface.ipynb&#34;&gt;Module 11&lt;/a&gt;&lt;br&gt;Week of 11/21/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 11: Natural Language Processing&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 11.1: Hugging Face Introduction&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.2: Hugging Face Tokenizers&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.3: Hugging Face Data Sets&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.4: Training a Model in Hugging Face&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.5: What are Embedding Layers in Keras&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class10.ipynb&#34;&gt;Module 10 Assignment&lt;/a&gt; due: 11/22/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_12_01_ai_gym.ipynb&#34;&gt;Module 12&lt;/a&gt;&lt;br&gt;Week of 11/28/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 12: Reinforcement Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Kaggle Assignment due: 11/29/2022 (approx 4-6PM, due to Kaggle GMT timezone)&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.1: Introduction to the OpenAI Gym&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.2: Introduction to Q-Learning for Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.3: Keras Q-Learning in the OpenAI Gym&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.4: Atari Games with Keras Neural Networks&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.5: Application of Reinforcement Learning&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class11.ipynb&#34;&gt;Module 11 Assignment&lt;/a&gt; due: 11/29/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_13_01_flask.ipynb&#34;&gt;Module 13&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 12/05/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 13: Deployment and Monitoring&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 13.1: Flask and Deep Learning Web Services &lt;/li&gt;&#xA;     &lt;li&gt;Part 13.2: Interrupting and Continuing Training&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.3: Using a Keras Deep Neural Network with a Web Application&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.4: When to Retrain Your Neural Network&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.5: Tensor Processing Units (TPUs)&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (fourth meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class12.ipynb&#34;&gt;Module 12 Assignment&lt;/a&gt; due: 12/06/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Datasets&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://data.heatonresearch.com/data/t81-558/index.html&#34;&gt;Datasets can be downloaded here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/deep-rl-class</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/huggingface/deep-rl-class</id>
    <link href="https://github.com/huggingface/deep-rl-class" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repo contain the syllabus of the Hugging Face Deep Reinforcement Learning Class.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Hugging Face Deep Reinforcement Learning Class ü§ó&lt;/h1&gt; &#xA;&lt;p&gt;In this free course, you will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìñ Study Deep Reinforcement Learning in &lt;strong&gt;theory and practice&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üßë‚Äçüíª Learn to &lt;strong&gt;use famous Deep RL libraries&lt;/strong&gt; such as Stable Baselines3, RL Baselines3 Zoo, and RLlib.&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ Train agents in &lt;strong&gt;unique environments&lt;/strong&gt; such as SnowballFight, Huggy the Doggo üê∂, and classical ones such as Space Invaders and PyBullet.&lt;/li&gt; &#xA; &lt;li&gt;üíæ &lt;strong&gt;Publish your trained agents in one line of code to the Hugging Face Hub&lt;/strong&gt;. But also &lt;strong&gt;download powerful agents from the community&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üèÜ &lt;strong&gt;Participate in challenges&lt;/strong&gt; where you will evaluate your agents against other teams.&lt;/li&gt; &#xA; &lt;li&gt;üñåÔ∏èüé® &lt;strong&gt;Learn to share your own environments made with Unity and Godot&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;‚û°Ô∏è‚û°Ô∏è‚û°Ô∏è Don&#39;t forget to sign up here: &lt;a href=&#34;http://eepurl.com/h1pElX&#34;&gt;http://eepurl.com/h1pElX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The best way to keep in touch is to &lt;strong&gt;join our discord server to exchange with the community and with us&lt;/strong&gt; üëâüèª &lt;a href=&#34;https://discord.gg/aYka4Yhff9&#34;&gt;https://discord.gg/aYka4Yhff9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Are you new to Discord? Check our &lt;strong&gt;discord 101 to get the best practices&lt;/strong&gt; üëâ &lt;a href=&#34;https://github.com/huggingface/deep-rl-class/raw/main/DISCORD.Md&#34;&gt;https://github.com/huggingface/deep-rl-class/blob/main/DISCORD.Md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And don&#39;t forget to share with your friends who want to learn ü§ó!&lt;/p&gt; &#xA;&lt;h2&gt;The Syllabus üèóÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;This course is &lt;strong&gt;self-paced&lt;/strong&gt; you can start when you want ü•≥.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;üìÜ Publishing date&lt;/th&gt; &#xA;   &lt;th&gt;üìò Unit&lt;/th&gt; &#xA;   &lt;th&gt;üë©‚Äçüíª Hands-on&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/tree/main/unit1#unit-1-introduction-to-deep-reinforcement-learning&#34;&gt;Published ü•≥&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/tree/main/unit1&#34;&gt;An Introduction to Deep Reinforcement Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/raw/main/unit1/unit1.ipynb&#34;&gt;Train a Deep Reinforcement Learning lander agent to land correctly on the Moon üåï using Stable-Baselines3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;May, the 11th&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://discord.com/channels/879548962464493619/968114737655214080/973937495546925056&#34;&gt;Bonus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;May, the 18th/May, the 20th&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/raw/main/unit2/README.md&#34;&gt;Q-Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Train an agent to cross a Frozen lake in this new version of the environment.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;June, the 1st&lt;/td&gt; &#xA;   &lt;td&gt;Deep Q-Learning and improvements&lt;/td&gt; &#xA;   &lt;td&gt;Train a Deep Q-Learning agent to play Space Invaders&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Policy-based methods&lt;/td&gt; &#xA;   &lt;td&gt;üèóÔ∏è&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Actor-Critic Methods&lt;/td&gt; &#xA;   &lt;td&gt;üèóÔ∏è&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Proximal Policy Optimization (PPO)&lt;/td&gt; &#xA;   &lt;td&gt;üèóÔ∏è&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Decision Transformers and offline Reinforcement Learning&lt;/td&gt; &#xA;   &lt;td&gt;üèóÔ∏è&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Towards better explorations methods&lt;/td&gt; &#xA;   &lt;td&gt;üèóÔ∏è&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;The library you&#39;ll learn during this course&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34;&gt;Stable-Baselines3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34;&gt;RL Baselines3 Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.ray.io/en/latest/rllib/index.html&#34;&gt;RLlib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vwxyzjn/cleanrl&#34;&gt;CleanRL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;More to come üèóÔ∏è&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;The Environments you&#39;ll use&lt;/h2&gt; &#xA;&lt;h3&gt;Custom environments made by the Hugging Face Team using Unity and Godot&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Huggy the Doggo üê∂ (Based on &lt;a href=&#34;https://blog.unity.com/technology/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit&#34;&gt;Unity&#39;s Puppo the Corgi work&lt;/a&gt;) &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/deep-rl-class/main/assets/img/huggy.jpg&#34; alt=&#34;huggy.jpg&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;SnowballFight ‚òÉÔ∏è &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/deep-rl-class/main/assets/img/snowballfight.gif&#34; alt=&#34;snowballfight.gif&#34;&gt; üëâ Play it here: &lt;a href=&#34;https://huggingface.co/spaces/ThomasSimonini/SnowballFight&#34;&gt;https://huggingface.co/spaces/ThomasSimonini/SnowballFight&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;More to come üöß&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Gym classic controls environments üïπÔ∏è&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lunar-Lander v2 üöÄüåô&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/deep-rl-class/main/assets/img/lunarlander.gif&#34; alt=&#34;lunarlander.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;PyBullet ü§ñ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More to come üöß&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Gym Atari environments üëæ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Space Invaders üëæ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/deep-rl-class/main/assets/img/spaceinvaders.gif&#34; alt=&#34;spaceinvaders.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;MLAgents environments üñåÔ∏è&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More to come üöß&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Good skills in Python üêç&lt;/li&gt; &#xA; &lt;li&gt;Basics in Deep Learning and Pytorch&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If it&#39;s not the case yet, you can check these free resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python: &lt;a href=&#34;https://www.udacity.com/course/introduction-to-python--ud1110&#34;&gt;https://www.udacity.com/course/introduction-to-python--ud1110&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Intro to Deep Learning with PyTorch: &lt;a href=&#34;https://www.udacity.com/course/deep-learning-pytorch--ud188&#34;&gt;https://www.udacity.com/course/deep-learning-pytorch--ud188&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PyTorch in 60min: &lt;a href=&#34;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&#34;&gt;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is this class free?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes, totally free ü•≥.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Do I need to have a Hugging Face account to follow the course?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes, to push your trained agents during the hands-on, you need an account (it&#39;s free) ü§ó.&lt;/p&gt; &#xA;&lt;p&gt;You can create one here üëâ &lt;a href=&#34;https://huggingface.co/join&#34;&gt;https://huggingface.co/join&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What‚Äôs the format of the class?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The course consists of&amp;nbsp;&lt;strong&gt;8 Units.&lt;/strong&gt;&amp;nbsp;In each of the Units, we&#39;ll have:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;A theory explained part&lt;/strong&gt;: an article and a video (based on Deep Reinforcement Learning Course)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A hands-on Google Colab&lt;/strong&gt; where you&#39;ll learn to use famous Deep RL libraries such as Stable Baselines3, RL Baselines3 Zoo, and RLlib to train your agents in unique environments such as SnowballFight, Huggy the Doggo üê∂, and classical ones such as Space Invaders and PyBullet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Some optional challenges&lt;/strong&gt;: train an agent in another environment, and try to beat the results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It&#39;s not a live course video, so you can watch and read each unit when you want ü§ó You can check the syllabus here üëâ &lt;a href=&#34;https://github.com/huggingface/deep-rl-class&#34;&gt;https://github.com/huggingface/deep-rl-class&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What I will do during this course?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In this free course, you will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìñ Study Deep Reinforcement Learning in theory and practice.&lt;/li&gt; &#xA; &lt;li&gt;üßë‚Äçüíª Learn to use famous Deep RL libraries such as Stable Baselines3, RL Baselines3 Zoo, and RLlib.&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ Train agents in unique environments such as SnowballFight, Huggy the Doggo üê∂, and classical ones such as Space Invaders and PyBullet.&lt;/li&gt; &#xA; &lt;li&gt;üíæ Publish your trained agents in one line of code to the Hub. But also download powerful agents from the community.&lt;/li&gt; &#xA; &lt;li&gt;üèÜ Participate in challenges where you will evaluate your agents against other teams.&lt;/li&gt; &#xA; &lt;li&gt;üñåÔ∏èüé® Learn to share your own environments made with Unity and Godot.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Where do I sign up?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here üëâ &lt;a href=&#34;http://eepurl.com/h1pElX&#34;&gt;http://eepurl.com/h1pElX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Where can I find the course?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On this repository&lt;/strong&gt;, we&#39;ll publish every week the links (chapters, hands-ons, videos).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Where can I exchange with my classmates and with you?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We have a discord server where you &lt;strong&gt;can exchange with the community and with us&lt;/strong&gt; üëâüèª &lt;a href=&#34;https://discord.gg/aYka4Yhff9&#34;&gt;https://discord.gg/aYka4Yhff9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Don‚Äôt forget to &lt;strong&gt;introduce yourself when you sign up ü§ó&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I have some feedback&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We want to improve and update the course iteratively with your feedback. If you have some, please send a mail to &lt;a href=&#34;mailto:thomas.simonini@huggingface.co&#34;&gt;thomas.simonini@huggingface.co&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How much background knowledge is needed?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Some prerequisites:&lt;/p&gt; &#xA;&lt;p&gt;Good skills in &lt;strong&gt;Python&lt;/strong&gt; üêç Basics in &lt;strong&gt;Deep Learning and Pytorch&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If it&#39;s not the case yet, you can check these free resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python: &lt;a href=&#34;https://www.udacity.com/course/introduction-to-python--ud1110&#34;&gt;https://www.udacity.com/course/introduction-to-python--ud1110&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Intro to Deep Learning with PyTorch: &lt;a href=&#34;https://www.udacity.com/course/deep-learning-pytorch--ud188&#34;&gt;https://www.udacity.com/course/deep-learning-pytorch--ud188&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PyTorch in 60min: &lt;a href=&#34;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&#34;&gt;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is there a certificate?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes üéâ. You&#39;ll &lt;strong&gt;need to upload the eight models with the eight hands-on.&lt;/strong&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google-research/google-research</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/google-research/google-research</id>
    <link href="https://github.com/google-research/google-research" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Google Research&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google Research&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains code released by &lt;a href=&#34;https://research.google&#34;&gt;Google Research&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All datasets in this repository are released under the CC BY 4.0 International license, which can be found here: &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;. All source files in this repository are released under the Apache 2.0 license, the text of which can be found in the LICENSE file.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Because the repo is large, we recommend you download only the subdirectory of interest:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;SUBDIR=foo&#xA;svn export https://github.com/google-research/google-research/trunk/$SUBDIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;d like to submit a pull request, you&#39;ll need to clone the repository; we recommend making a shallow clone (without history).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:google-research/google-research.git --depth=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;Disclaimer: This is not an official Google product.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/ML-For-Beginners</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/microsoft/ML-For-Beginners</id>
    <link href="https://github.com/microsoft/ML-For-Beginners" rel="alternate"></link>
    <summary type="html">&lt;p&gt;12 weeks, 26 lessons, 52 quizzes, classic Machine Learning for all&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/ML-For-Beginners/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/issues/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/pulls/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/watchers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&amp;amp;label=Watch&#34; alt=&#34;GitHub watchers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&amp;amp;label=Fork&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&amp;amp;label=Star&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Machine Learning for Beginners - A Curriculum&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üåç Travel around the world as we explore Machine Learning by means of world cultures üåç&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about &lt;strong&gt;Machine Learning&lt;/strong&gt;. In this curriculum, you will learn about what is sometimes called &lt;strong&gt;classic machine learning&lt;/strong&gt;, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming &#39;AI for Beginners&#39; curriculum. Pair these lessons with our &lt;a href=&#34;https://aka.ms/datascience-beginners&#34;&gt;&#39;Data Science for Beginners&#39; curriculum&lt;/a&gt;, as well!&lt;/p&gt; &#xA;&lt;p&gt;Travel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to &#39;stick&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;‚úçÔ∏è Hearty thanks to our authors&lt;/strong&gt; Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, and Amy Boyd&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üé® Thanks as well to our illustrators&lt;/strong&gt; Tomomi Imura, Dasani Madipalli, and Jen Looper&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üôè Special thanks üôè to our Microsoft Student Ambassador authors, reviewers, and content contributors&lt;/strong&gt;, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ü§© Extra gratitude to Microsoft Student Ambassador Eric Wanjau for our R lessons!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://aka.ms/student-page&#34;&gt;Students&lt;/a&gt;&lt;/strong&gt;, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start with a pre-lecture quiz.&lt;/li&gt; &#xA; &lt;li&gt;Read the lecture and complete the activities, pausing and reflecting at each knowledge check.&lt;/li&gt; &#xA; &lt;li&gt;Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the &lt;code&gt;/solution&lt;/code&gt; folders in each project-oriented lesson.&lt;/li&gt; &#xA; &lt;li&gt;Take the post-lecture quiz.&lt;/li&gt; &#xA; &lt;li&gt;Complete the challenge.&lt;/li&gt; &#xA; &lt;li&gt;Complete the assignment.&lt;/li&gt; &#xA; &lt;li&gt;After completing a lesson group, visit the &lt;a href=&#34;https://github.com/microsoft/ML-For-Beginners/discussions&#34;&gt;Discussion Board&lt;/a&gt; and &#34;learn out loud&#34; by filling out the appropriate PAT rubric. A &#39;PAT&#39; is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For further study, we recommend following these &lt;a href=&#34;https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa&#34;&gt;Microsoft Learn&lt;/a&gt; modules and learning paths.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;, we have &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/for-teachers.md&#34;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Meet the Team&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/Tj1XWrDSYJU&#34; title=&#34;Promo video&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/ml.gif&#34; alt=&#34;Promo video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gif by&lt;/strong&gt; &lt;a href=&#34;https://linkedin.com/in/mohitjaisal&#34;&gt;Mohit Jaisal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üé• Click the image above for a video about the project and the folks who created it!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Pedagogy&lt;/h2&gt; &#xA;&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is hands-on &lt;strong&gt;project-based&lt;/strong&gt; and that it includes &lt;strong&gt;frequent quizzes&lt;/strong&gt;. In addition, this curriculum has a common &lt;strong&gt;theme&lt;/strong&gt; to give it cohesion.&lt;/p&gt; &#xA;&lt;p&gt;By ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12-week cycle. This curriculum also includes a postscript on real-world applications of ML, which can be used as extra credit or as a basis for discussion.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Find our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/TRANSLATIONS.md&#34;&gt;Translation&lt;/a&gt; guidelines. We welcome your constructive feedback!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Each lesson includes:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;optional sketchnote&lt;/li&gt; &#xA; &lt;li&gt;optional supplemental video&lt;/li&gt; &#xA; &lt;li&gt;pre-lecture warmup quiz&lt;/li&gt; &#xA; &lt;li&gt;written lesson&lt;/li&gt; &#xA; &lt;li&gt;for project-based lessons, step-by-step guides on how to build the project&lt;/li&gt; &#xA; &lt;li&gt;knowledge checks&lt;/li&gt; &#xA; &lt;li&gt;a challenge&lt;/li&gt; &#xA; &lt;li&gt;supplemental reading&lt;/li&gt; &#xA; &lt;li&gt;assignment&lt;/li&gt; &#xA; &lt;li&gt;post-lecture quiz&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about languages&lt;/strong&gt;: These lessons are primarily written in Python, but many are also available in R. To complete an R lesson, go to the &lt;code&gt;/solution&lt;/code&gt; folder and look for R lessons. They include an .rmd extension that represents an &lt;strong&gt;R Markdown&lt;/strong&gt; file which can be simply defined as an embedding of &lt;code&gt;code chunks&lt;/code&gt; (of R or other languages) and a &lt;code&gt;YAML header&lt;/code&gt; (that guides how to format outputs such as PDF) in a &lt;code&gt;Markdown document&lt;/code&gt;. As such, it serves as an exemplary authoring framework for data science since it allows you to combine your code, its output, and your thoughts by allowing you to write them down in Markdown. Moreover, R Markdown documents can be rendered to output formats such as PDF, HTML, or Word.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained &lt;a href=&#34;https://white-water-09ec41f0f.azurestaticapps.net/&#34;&gt;in this app&lt;/a&gt;, for 52 total quizzes of three questions each. They are linked from within the lessons but the quiz app can be run locally; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Number&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Topic&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Grouping&lt;/th&gt; &#xA;   &lt;th&gt;Learning Objectives&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Linked Lesson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Author&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn the basic concepts behind machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/1-intro-to-ML/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Muhammad&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The History of machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn the history underlying this field&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/2-history-of-ML/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Jen and Amy&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Fairness and machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;What are the important philosophical issues around fairness that students should consider when building and applying ML models?&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/3-fairness/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Tomomi&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Techniques for machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;What techniques do ML researchers use to build ML models?&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/4-techniques-of-ML/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chris and Jen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to regression&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Get started with Python and Scikit-learn for regression models&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/1-Tools/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/1-Tools/solution/R/lesson_1-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;North American pumpkin prices üéÉ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Visualize and clean data in preparation for ML&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/2-Data/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;North American pumpkin prices üéÉ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build linear and polynomial regression models&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/3-Linear/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Dmitry&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;North American pumpkin prices üéÉ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build a logistic regression model&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/4-Logistic/README.md&#34;&gt;Python&lt;/a&gt; &lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A Web App üîå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/3-Web-App/README.md&#34;&gt;Web App&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build a web app to use your trained model&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/3-Web-App/1-Web-App/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Jen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to classification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Clean, prep, and visualize your data; introduction to classification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/1-Introduction/README.md&#34;&gt;Python&lt;/a&gt; &lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Cassie&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Delicious Asian and Indian cuisines üçú&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduction to classifiers&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/2-Classifiers-1/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Cassie&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Delicious Asian and Indian cuisines üçú&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;More classifiers&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/3-Classifiers-2/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Cassie&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Delicious Asian and Indian cuisines üçú&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build a recommender web app using your model&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/4-Applied/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Jen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to clustering&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/README.md&#34;&gt;Clustering&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Clean, prep, and visualize your data; Introduction to clustering&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/1-Visualize/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Exploring Nigerian Musical Tastes üéß&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/README.md&#34;&gt;Clustering&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Explore the K-Means clustering method&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/2-K-Means/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to natural language processing ‚òïÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn the basics about NLP by building a simple bot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/1-Introduction-to-NLP/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Common NLP Tasks ‚òïÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deepen your NLP knowledge by understanding common tasks required when dealing with language structures&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/2-Tasks/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Translation and sentiment analysis ‚ô•Ô∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Translation and sentiment analysis with Jane Austen&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/3-Translation-Sentiment/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Romantic hotels of Europe ‚ô•Ô∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sentiment analysis with hotel reviews 1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/4-Hotel-Reviews-1/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Romantic hotels of Europe ‚ô•Ô∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sentiment analysis with hotel reviews 2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/5-Hotel-Reviews-2/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to time series forecasting&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/README.md&#34;&gt;Time series&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduction to time series forecasting&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/1-Introduction/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Francesca&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ö°Ô∏è World Power Usage ‚ö°Ô∏è - time series forecasting with ARIMA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/README.md&#34;&gt;Time series&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Time series forecasting with ARIMA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/2-ARIMA/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Francesca&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ö°Ô∏è World Power Usage ‚ö°Ô∏è - time series forecasting with SVR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/README.md&#34;&gt;Time series&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Time series forecasting with Support Vector Regressor&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/3-SVR/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Anirban&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to reinforcement learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/README.md&#34;&gt;Reinforcement learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduction to reinforcement learning with Q-Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/1-QLearning/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dmitry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Help Peter avoid the wolf! üê∫&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/README.md&#34;&gt;Reinforcement learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Reinforcement learning Gym&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/2-Gym/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dmitry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Postscript&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Real-World ML scenarios and applications&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/README.md&#34;&gt;ML in the Wild&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Interesting and revealing real-world applications of classical ML&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/1-Applications/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Team&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Offline access&lt;/h2&gt; &#xA;&lt;p&gt;You can run this documentation offline by using &lt;a href=&#34;https://docsify.js.org/#/&#34;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&#34;https://docsify.js.org/#/quickstart&#34;&gt;install Docsify&lt;/a&gt; on your local machine, and then in the root folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;PDFs&lt;/h2&gt; &#xA;&lt;p&gt;Find a pdf of the curriculum with links &lt;a href=&#34;https://microsoft.github.io/ML-For-Beginners/pdf/readme.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Help Wanted!&lt;/h2&gt; &#xA;&lt;p&gt;Would you like to contribute a translation? Please read our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/TRANSLATIONS.md&#34;&gt;translation guidelines&lt;/a&gt; and add a templated issue to manage the workload &lt;a href=&#34;https://github.com/microsoft/ML-For-Beginners/issues&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Other Curricula&lt;/h2&gt; &#xA;&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/webdev-beginners&#34;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/iot-beginners&#34;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/datascience-beginners&#34;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-beginners&#34;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>bnsreenu/python_for_microscopists</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/bnsreenu/python_for_microscopists</id>
    <link href="https://github.com/bnsreenu/python_for_microscopists" rel="alternate"></link>
    <summary type="html">&lt;p&gt;https://www.youtube.com/channel/UC34rW-HtPJulxr5wp2Xa04w?sub_confirmation=1&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;b&gt; Author: Dr. Sreenivas Bhattiprolu &lt;/b&gt;&lt;br&gt; &lt;b&gt;Twitter: @digitalsreeni &lt;/b&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Python for Microscopists and other image processing enthusiasts&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC34rW-HtPJulxr5wp2Xa04w?sub_confirmation=1&#34;&gt;https://www.youtube.com/channel/UC34rW-HtPJulxr5wp2Xa04w?sub_confirmation=1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The YouTube channel associated with this code walks you through the entire process of learning to code in Python; all the way from basics to advanced machine learning and deep learning. The primary emphasis will be on image processing and other relevant functionality.&lt;/p&gt; &#xA;&lt;p&gt;Why did I create this channel? To help you (students and researchers) gain a new skill and succeed in your respective fields.&lt;/p&gt; &#xA;&lt;p&gt;You may think coding is hard and that it&#39;s not your cup of tea, but Python made it easy to code even advanced algorithms. In addition, coding will make you self sufficient, it will teach you how to think, it improves your collaborative skills and it can take your career to new heights. Therefore, if you want to stay ahead of your peers and relevant in your field, overcome your fears and start coding!&lt;/p&gt; &#xA;&lt;p&gt;Also, checkout WWW.APEER.COM if you want free image processing in the cloud! Free for non-profits / academics / personal use.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>codebasics/py</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/codebasics/py</id>
    <link href="https://github.com/codebasics/py" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repository to store sample python programs for python learning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;py&lt;/h1&gt; &#xA;&lt;p&gt;Repository to store sample Python programs.&lt;/p&gt; &#xA;&lt;p&gt;This repository is meant for beginners to assist them in their learning of Python. The repository covers a wide range of algorithms and other programs, and would prove immensely helpful for everybody interested in Python programming.&lt;/p&gt; &#xA;&lt;p&gt;If this is your first time coding in Python, I would love to suggest you begin from the &lt;a href=&#34;https://github.com/codebasics/py/tree/master/Basics&#34;&gt;Basics&lt;/a&gt;. They are simple to understand and hopefully will prove fun to you.&lt;/p&gt; &#xA;&lt;p&gt;You can also pay a visit to my very own &lt;a href=&#34;https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ&#34;&gt;Youtube channel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Contributions to the repository are welcome.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ&#34;&gt;&lt;img src=&#34;https://yt3.ggpht.com/ytc/AAUvwnihwx4a5idwBTE5JFpXHb-ykyh-i1gXtFiGJYV1=s176-c-k-c0x00ffffff-no-rj&#34; alt=&#34;CodeBasics&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Happy coding!&lt;/h4&gt;</summary>
  </entry>
  <entry>
    <title>tensorflow/docs</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/tensorflow/docs</id>
    <link href="https://github.com/tensorflow/docs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TensorFlow documentation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TensorFlow Documentation&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://www.tensorflow.org/images/tf_logo_horizontal.png&#34;&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;These are the source files for the guide and tutorials on &lt;a href=&#34;https://www.tensorflow.org/overview&#34;&gt;tensorflow.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To contribute to the TensorFlow documentation, please read &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/docs/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;, the &lt;a href=&#34;https://www.tensorflow.org/community/contribute/docs&#34;&gt;TensorFlow docs contributor guide&lt;/a&gt;, and the &lt;a href=&#34;https://www.tensorflow.org/community/contribute/docs_style&#34;&gt;style guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To file a docs issue, use the issue tracker in the &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md&#34;&gt;tensorflow/tensorflow&lt;/a&gt; repo.&lt;/p&gt; &#xA;&lt;p&gt;And join the TensorFlow documentation contributors on the &lt;a href=&#34;https://groups.google.com/a/tensorflow.org/forum/#!forum/docs&#34;&gt;docs@tensorflow.org mailing list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community translations&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/community/contribute/docs#community_translations&#34;&gt;Community translations&lt;/a&gt; are located in the &lt;a href=&#34;https://github.com/tensorflow/docs-l10n&#34;&gt;tensorflow/docs-l10n&lt;/a&gt; repo. These docs are contributed, reviewed, and maintained by the community as &lt;em&gt;best-effort&lt;/em&gt;. To participate as a translator or reviewer, see the &lt;code&gt;site/&amp;lt;lang&amp;gt;/README.md&lt;/code&gt;, join the language mailing list, and submit a pull request.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/docs/master/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/Data-Science-For-Beginners</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/microsoft/Data-Science-For-Beginners</id>
    <link href="https://github.com/microsoft/Data-Science-For-Beginners" rel="alternate"></link>
    <summary type="html">&lt;p&gt;10 Weeks, 20 Lessons, Data Science for All!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Data Science for Beginners - A Curriculum&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/Data-Science-For-Beginners/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/graphs/contributors/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/issues/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/pulls/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/watchers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Watch&#34; alt=&#34;GitHub watchers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Fork&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Star&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 10-week, 20-lesson curriculum all about Data Science. Each lesson includes pre-lesson and post-lesson quizzes, written instructions to complete the lesson, a solution, and an assignment. Our project-based pedagogy allows you to learn while building, a proven way for new skills to &#39;stick&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hearty thanks to our authors:&lt;/strong&gt; &lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine Greenaway&lt;/a&gt;, &lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry Soshnikov&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya Narasimhan&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/JalenMcG&#34;&gt;Jalen McGee&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen Looper&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud Levy&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany Souterre&lt;/a&gt;, &lt;a href=&#34;https://www.twitter.com/geektrainer&#34;&gt;Christopher Harrison&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üôè Special thanks üôè to our &lt;a href=&#34;https://studentambassadors.microsoft.com/&#34;&gt;Microsoft Student Ambassador&lt;/a&gt; authors, reviewers and content contributors,&lt;/strong&gt; notably Aaryan Arora, &lt;a href=&#34;https://github.com/AdityaGarg00&#34;&gt;Aditya Garg&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/alondra-sanchez-molina/&#34;&gt;Alondra Sanchez&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/ankitasingh007&#34;&gt;Ankita Singh&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/anupam--mishra/&#34;&gt;Anupam Mishra&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/arpitadas01/&#34;&gt;Arpita Das&lt;/a&gt;, ChhailBihari Dubey, &lt;a href=&#34;https://www.linkedin.com/in/dibrinsofor&#34;&gt;Dibri Nsofor&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/dishita-bhasin-7065281bb&#34;&gt;Dishita Bhasin&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/majd-s/&#34;&gt;Majd Safi&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/max-blum-6036a1186/&#34;&gt;Max Blum&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/miguelmque/&#34;&gt;Miguel Correa&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/iftu119&#34;&gt;Mohamma Iftekher (Iftu) Ebne Jalal&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/nawrin-tabassum&#34;&gt;Nawrin Tabassum&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/raymond-wp/&#34;&gt;Raymond Wangsa Putra&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/rty2423&#34;&gt;Rohit Yadav&lt;/a&gt;, Samridhi Sharma, &lt;a href=&#34;https://www.linkedin.com/mwlite/in/sanya-sinha-13aab1200&#34;&gt;Sanya Sinha&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/sheena-narua-n/&#34;&gt;Sheena Narula&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/tauqeerahmad5201/&#34;&gt;Tauqeer Ahmad&lt;/a&gt;, Yogendrasingh Pawar , &lt;a href=&#34;https://www.linkedin.com/in/vidushi-gupta07/&#34;&gt;Vidushi Gupta&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jasleen-sondhi/&#34;&gt;Jasleen Sondhi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Title.png&#34; alt=&#34; Sketchnote by (@sketchthedocs) &#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science For Beginners - &lt;em&gt;Sketchnote by &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;: we have &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/for-teachers.md&#34;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum. We&#39;d love your feedback &lt;a href=&#34;https://github.com/microsoft/Data-Science-For-Beginners/discussions&#34;&gt;in our discussion forum&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://aka.ms/student-page&#34;&gt;Students&lt;/a&gt;&lt;/strong&gt;: to use this curriculum on your own, fork the entire repo and complete the exercises on your own, starting with a pre-lecture quiz. Then read the lecture and complete the rest of the activities. Try to create the projects by comprehending the lessons rather than copying the solution code; however, that code is available in the /solutions folders in each project-oriented lesson. Another idea would be to form a study group with friends and go through the content together. For further study, we recommend &lt;a href=&#34;https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/qprpajyoy3x0g7?WT.mc_id=academic-40229-cxa&#34;&gt;Microsoft Learn&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Meet the Team&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/8mzavjQSMM4&#34; title=&#34;Promo video&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/ds-for-beginners.gif&#34; alt=&#34;Promo video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gif by&lt;/strong&gt; &lt;a href=&#34;https://www.linkedin.com/in/mohitjaisal&#34;&gt;Mohit Jaisal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üé• Click the image above for a video about the project the folks who created it!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Pedagogy&lt;/h2&gt; &#xA;&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is project-based and that it includes frequent quizzes. By the end of this series, students will have learned basic principles of data science, including ethical concepts, data preparation, different ways of working with data, data visualization, data analysis, real-world use cases of data science, and more.&lt;/p&gt; &#xA;&lt;p&gt;In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 10 week cycle.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Find our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&#34;&gt;Translation&lt;/a&gt; guidelines. We welcome your constructive feedback!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Each lesson includes:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optional sketchnote&lt;/li&gt; &#xA; &lt;li&gt;Optional supplemental video&lt;/li&gt; &#xA; &lt;li&gt;Pre-lesson warmup quiz&lt;/li&gt; &#xA; &lt;li&gt;Written lesson&lt;/li&gt; &#xA; &lt;li&gt;For project-based lessons, step-by-step guides on how to build the project&lt;/li&gt; &#xA; &lt;li&gt;Knowledge checks&lt;/li&gt; &#xA; &lt;li&gt;A challenge&lt;/li&gt; &#xA; &lt;li&gt;Supplemental reading&lt;/li&gt; &#xA; &lt;li&gt;Assignment&lt;/li&gt; &#xA; &lt;li&gt;Post-lesson quiz&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained &lt;a href=&#34;https://red-water-0103e7a0f.azurestaticapps.net/&#34;&gt;in this app&lt;/a&gt;, for 40 total quizzes of three questions each. They are linked from within the lessons, but the quiz app can be run locally; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder. They are gradually being localized.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Lessons&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Roadmap.png&#34; alt=&#34; Sketchnote by (@sketchthedocs) &#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science For Beginners: Roadmap - &lt;em&gt;Sketchnote by &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Number&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Topic&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Grouping&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Learning Objectives&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Linked Lesson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Author&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Defining Data Science&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Learn the basic concepts behind data science and how it‚Äôs related to artificial intelligence, machine learning, and big data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/01-defining-data-science/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/beZ7Mb_oz9I&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science Ethics&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Ethics Concepts, Challenges &amp;amp; Frameworks.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/02-ethics/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Defining Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;How data is classified and its common sources.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/03-defining-data/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to Statistics &amp;amp; Probability&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The mathematical techniques of probability and statistics to understand data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/04-stats-and-probability/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/Z5Zy85g4Yjw&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with Relational Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to relational data and the basics of exploring and analyzing relational data with the Structured Query Language, also known as SQL (pronounced ‚Äúsee-quell‚Äù).&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/05-relational-databases/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/geektrainer&#34;&gt;Christopher&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with NoSQL Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to non-relational data, its various types and the basics of exploring and analyzing document databases.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/06-non-relational/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with Python&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Basics of using Python for data exploration with libraries such as Pandas. Foundational understanding of Python programming is recommended.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/07-python/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/dZjWOGbsN4Y&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Preparation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Topics on data techniques for cleaning and transforming the data to handle challenges of missing, inaccurate, or incomplete data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/08-data-preparation/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Quantities&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Learn how to use Matplotlib to visualize bird data ü¶Ü&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/09-visualization-quantities/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Distributions of Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing observations and trends within an interval.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/10-visualization-distributions/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Proportions&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing discrete and grouped percentages.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/11-visualization-proportions/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Relationships&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing connections and correlations between sets of data and their variables.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/12-visualization-relationships/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meaningful Visualizations&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Techniques and guidance for making your visualizations valuable for effective problem solving and insights.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/13-meaningful-visualizations/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to the Data Science lifecycle&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to the data science lifecycle and its first step of acquiring and extracting data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/14-Introduction/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Analyzing&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This phase of the data science lifecycle focuses on techniques to analyze data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/15-analyzing/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Communication&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This phase of the data science lifecycle focuses on presenting the insights from the data in a way that makes it easier for decision makers to understand.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/16-communication/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/JalenMcG&#34;&gt;Jalen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This series of lessons introduces data science in the cloud and its benefits.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/17-Introduction/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Training models using Low Code tools.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/18-Low-Code/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Deploying models with Azure Machine Learning Studio.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/19-Azure/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Wild&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/README.md&#34;&gt;In the Wild&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data science driven projects in the real world.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/20-Real-World-Examples/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Offline access&lt;/h2&gt; &#xA;&lt;p&gt;You can run this documentation offline by using &lt;a href=&#34;https://docsify.js.org/#/&#34;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&#34;https://docsify.js.org/#/quickstart&#34;&gt;install Docsify&lt;/a&gt; on your local machine, then in the root folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note, notebooks will not be rendered via Docsify, so when you need to run a notebook, do that separately in VS Code running a Python kernel.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;PDF&lt;/h2&gt; &#xA;&lt;p&gt;A PDF of all of the lessons can be found &lt;a href=&#34;https://microsoft.github.io/Data-Science-For-Beginners/pdf/readme.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Help Wanted!&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to translate all or part of the curriculum, please follow our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&#34;&gt;Translations&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;h2&gt;Other Curricula&lt;/h2&gt; &#xA;&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ml-beginners&#34;&gt;Machine Learning for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/iot-beginners&#34;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/webdev-beginners&#34;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-beginners&#34;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>graykode/nlp-tutorial</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/graykode/nlp-tutorial</id>
    <link href="https://github.com/graykode/nlp-tutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Natural Language Processing Tutorial for Deep Learning Researchers&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;nlp-tutorial&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;100&#34; src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/225px-TensorFlowLogo.svg.png&#34;&gt; &lt;img width=&#34;100&#34; src=&#34;https://media-thumbs.golden.com/OLqzmrmwAzY1P7Sl29k2T9WjJdM=/200x200/smart/golden-storage-production.s3.amazonaws.com/topic_images/e08914afa10a4179893eeb07cb5e4713.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;nlp-tutorial&lt;/code&gt; is a tutorial for who is studying NLP(Natural Language Processing) using &lt;strong&gt;Pytorch&lt;/strong&gt;. Most of the models in NLP were implemented with less than &lt;strong&gt;100 lines&lt;/strong&gt; of code.(except comments or blank lines)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[08-14-2020] Old TensorFlow v1 code is archived in &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/archive&#34;&gt;the archive folder&lt;/a&gt;. For beginner readability, only pytorch version 1.0 or higher is supported.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Curriculum - (Example Purpose)&lt;/h2&gt; &#xA;&lt;h4&gt;1. Basic Embedding Model&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/1-1.NNLM&#34;&gt;NNLM(Neural Network Language Model)&lt;/a&gt; - &lt;strong&gt;Predict Next Word&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&#34;&gt;A Neural Probabilistic Language Model(2003)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-1.NNLM/NNLM.ipynb&#34;&gt;NNLM.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;1-2. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/1-2.Word2Vec&#34;&gt;Word2Vec(Skip-gram)&lt;/a&gt; - &lt;strong&gt;Embedding Words and Show Graph&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&#34;&gt;Distributed Representations of Words and Phrases and their Compositionality(2013)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram(Softmax).ipynb&#34;&gt;Word2Vec.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;1-3. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/1-3.FastText&#34;&gt;FastText(Application Level)&lt;/a&gt; - &lt;strong&gt;Sentence Classification&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/pdf/1607.01759.pdf&#34;&gt;Bag of Tricks for Efficient Text Classification(2016)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-3.FastText/FastText.ipynb&#34;&gt;FastText.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. CNN(Convolutional Neural Network)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/2-1.TextCNN&#34;&gt;TextCNN&lt;/a&gt; - &lt;strong&gt;Binary Sentiment Classification&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;http://www.aclweb.org/anthology/D14-1181&#34;&gt;Convolutional Neural Networks for Sentence Classification(2014)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/2-1.TextCNN/TextCNN.ipynb&#34;&gt;TextCNN.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;3. RNN(Recurrent Neural Network)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;3-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/3-1.TextRNN&#34;&gt;TextRNN&lt;/a&gt; - &lt;strong&gt;Predict Next Step&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;http://psych.colorado.edu/~kimlab/Elman1990.pdf&#34;&gt;Finding Structure in Time(1990)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-1.TextRNN/TextRNN.ipynb&#34;&gt;TextRNN.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;3-2. &lt;a href=&#34;https://github.com/graykode/nlp-tutorial/tree/master/3-2.TextLSTM&#34;&gt;TextLSTM&lt;/a&gt; - &lt;strong&gt;Autocomplete&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://www.bioinf.jku.at/publications/older/2604.pdf&#34;&gt;LONG SHORT-TERM MEMORY(1997)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-2.TextLSTM/TextLSTM.ipynb&#34;&gt;TextLSTM.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;3-3. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/3-3.Bi-LSTM&#34;&gt;Bi-LSTM&lt;/a&gt; - &lt;strong&gt;Predict Next Word in Long Sentence&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-3.Bi-LSTM/Bi_LSTM.ipynb&#34;&gt;Bi_LSTM.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;4. Attention Mechanism&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;4-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/4-1.Seq2Seq&#34;&gt;Seq2Seq&lt;/a&gt; - &lt;strong&gt;Change Word&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/pdf/1406.1078.pdf&#34;&gt;Learning Phrase Representations using RNN Encoder‚ÄìDecoder for Statistical Machine Translation(2014)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-1.Seq2Seq/Seq2Seq.ipynb&#34;&gt;Seq2Seq.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;4-2. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/4-2.Seq2Seq(Attention)&#34;&gt;Seq2Seq with Attention&lt;/a&gt; - &lt;strong&gt;Translate&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34;&gt;Neural Machine Translation by Jointly Learning to Align and Translate(2014)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-2.Seq2Seq(Attention)/Seq2Seq(Attention).ipynb&#34;&gt;Seq2Seq(Attention).ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;4-3. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/4-3.Bi-LSTM(Attention)&#34;&gt;Bi-LSTM with Attention&lt;/a&gt; - &lt;strong&gt;Binary Sentiment Classification&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-3.Bi-LSTM(Attention)/Bi_LSTM(Attention).ipynb&#34;&gt;Bi_LSTM(Attention).ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;5. Model based on Transformer&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;5-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/5-1.Transformer&#34;&gt;The Transformer&lt;/a&gt; - &lt;strong&gt;Translate&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention Is All You Need(2017)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer.ipynb&#34;&gt;Transformer.ipynb&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer(Greedy_decoder).ipynb&#34;&gt;Transformer(Greedy_decoder).ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;5-2. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/5-2.BERT&#34;&gt;BERT&lt;/a&gt; - &lt;strong&gt;Classification Next Sentence &amp;amp; Predict Masked Tokens&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.ipynb&#34;&gt;BERT.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.5+&lt;/li&gt; &#xA; &lt;li&gt;Pytorch 1.0.0+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Author&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tae Hwan Jung(Jeff Jung) @graykode&lt;/li&gt; &#xA; &lt;li&gt;Author Email : &lt;a href=&#34;mailto:nlkey2022@gmail.com&#34;&gt;nlkey2022@gmail.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Acknowledgements to &lt;a href=&#34;http://mojitok.com/&#34;&gt;mojitok&lt;/a&gt; as NLP Research Internship.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Azure/MachineLearningNotebooks</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/Azure/MachineLearningNotebooks</id>
    <link href="https://github.com/Azure/MachineLearningNotebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python notebooks with ML and deep learning examples with Azure Machine Learning Python SDK | Microsoft&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Azure Machine Learning Python SDK notebooks&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;a community-driven repository of examples using mlflow for tracking can be found at &lt;a href=&#34;https://github.com/Azure/azureml-examples&#34;&gt;https://github.com/Azure/azureml-examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Welcome to the Azure Machine Learning Python SDK notebooks repository!&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;These notebooks are recommended for use in an Azure Machine Learning &lt;a href=&#34;https://docs.microsoft.com/azure/machine-learning/concept-compute-instance&#34;&gt;Compute Instance&lt;/a&gt;, where you can run them without any additional set up.&lt;/p&gt; &#xA;&lt;p&gt;However, the notebooks can be run in any development environment with the correct &lt;code&gt;azureml&lt;/code&gt; packages installed.&lt;/p&gt; &#xA;&lt;p&gt;Install the &lt;code&gt;azureml.core&lt;/code&gt; Python package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install azureml-core&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install additional packages as needed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install azureml-mlflow&#xA;pip install azureml-dataset-runtime&#xA;pip install azureml-automl-runtime&#xA;pip install azureml-pipeline&#xA;pip install azureml-pipeline-steps&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend starting with one of the &lt;a href=&#34;https://raw.githubusercontent.com/Azure/MachineLearningNotebooks/master/tutorials/compute-instance-quickstarts&#34;&gt;quickstarts&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This repository is a push-only mirror. Pull requests are ignored.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/Azure/MachineLearningNotebooks/master/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/azure/machine-learning&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Azure/Azure-Sentinel</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/Azure/Azure-Sentinel</id>
    <link href="https://github.com/Azure/Azure-Sentinel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cloud-native SIEM for intelligent security analytics for your entire enterprise.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Microsoft Sentinel and Microsoft 365 Defender&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the unified Microsoft Sentinel and Microsoft 365 Defender repository! This repository contains out of the box detections, exploration queries, hunting queries, workbooks, playbooks and much more to help you get ramped up with Microsoft Sentinel and provide you security content to secure your environment and hunt for threats. The hunting queries also include Microsoft 365 Defender hunting queries for advanced hunting scenarios in both Microsoft 365 Defender and Microsoft Sentinel. You can also submit to &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues&#34;&gt;issues&lt;/a&gt; for any samples or resources you would like to see here as you onboard to Microsoft Sentinel. This repository welcomes contributions and refer to this repository&#39;s &lt;a href=&#34;https://aka.ms/threathunters&#34;&gt;wiki&lt;/a&gt; to get started. For questions and feedback, please contact &lt;a href=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/AzureSentinel@microsoft.com&#34;&gt;AzureSentinel@microsoft.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Resources&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://go.microsoft.com/fwlink/?linkid=2073774&amp;amp;clcid=0x409&#34;&gt;Microsoft Sentinel documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/microsoft-365/security/defender/microsoft-365-defender?view=o365-worldwide&#34;&gt;Microsoft 365 Defender documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/securitywebinars&#34;&gt;Security Community Webinars&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.github.com/en#dotcom&#34;&gt;Getting started with GitHub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We value your feedback. Here are some channels to help surface your questions or feedback:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;General product specific Q&amp;amp;A for SIEM and SOAR - Join in the &lt;a href=&#34;https://techcommunity.microsoft.com/t5/microsoft-sentinel/bd-p/MicrosoftSentinel&#34;&gt;Microsoft Sentinel Tech Community conversations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;General product specific Q&amp;amp;A for XDR - Join in the &lt;a href=&#34;https://techcommunity.microsoft.com/t5/microsoft-365-defender/bd-p/MicrosoftThreatProtection&#34;&gt;Microsoft 365 Defender Tech Community conversations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Product specific feature requests - Upvote or post new on &lt;a href=&#34;https://feedback.azure.com/d365community/forum/37638d17-0625-ec11-b6e6-000d3a4f07b8&#34;&gt;Microsoft Sentinel feedback forums&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Report product or contribution bugs - File a GitHub Issue using &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;amp;labels=&amp;amp;template=bug_report.md&amp;amp;title=&#34;&gt;Bug template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;General feedback on community and contribution process - File a GitHub Issue using &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;amp;labels=&amp;amp;template=feature_request.md&amp;amp;title=&#34;&gt;Feature Request template&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.microsoft.com&#34;&gt;https://cla.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Add in your new or updated contributions to GitHub&lt;/h2&gt; &#xA;&lt;p&gt;Note: If you are a first time contributor to this repository, &lt;a href=&#34;https://docs.github.com/github/getting-started-with-github/fork-a-repo&#34;&gt;General GitHub Fork the repo guidance&lt;/a&gt; before cloning or &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/raw/master/GettingStarted.md&#34;&gt;Specific steps for the Sentinel repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;General Steps&lt;/h2&gt; &#xA;&lt;p&gt;Brand new or update to a contribution via these methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Submit for review directly on GitHub website &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Browse to the folder you want to upload your file to&lt;/li&gt; &#xA;   &lt;li&gt;Choose Upload Files and browse to your file.&lt;/li&gt; &#xA;   &lt;li&gt;You will be required to create your own branch and then submit the Pull Request for review.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://help.github.com/en/desktop/getting-started-with-github-desktop&#34;&gt;GitHub Desktop&lt;/a&gt; or &lt;a href=&#34;https://visualstudio.microsoft.com/vs/&#34;&gt;Visual Studio&lt;/a&gt; or &lt;a href=&#34;https://code.visualstudio.com/?wt.mc_id=DX_841432&#34;&gt;VSCode&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.github.com/github/getting-started-with-github/fork-a-repo&#34;&gt;Fork the repo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository&#34;&gt;Clone the repo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/desktop/contributing-to-projects/creating-a-branch-for-your-work&#34;&gt;Create your own branch&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Do your additions/updates in GitHub Desktop&lt;/li&gt; &#xA;   &lt;li&gt;Be sure to merge master back to your branch before you push.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/using-git/pushing-commits-to-a-remote-repository&#34;&gt;Push your changes to GitHub&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pull Request&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;After you push your changes, you will need to submit the &lt;a href=&#34;https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests&#34;&gt;Pull Request (PR)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Details about the Proposed Changes are required, be sure to include a minimal level of detail so a review can clearly understand the reason for the change and what he change is related to in the code.&lt;/li&gt; &#xA; &lt;li&gt;After submission, check the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/pulls&#34;&gt;Pull Request&lt;/a&gt; for comments&lt;/li&gt; &#xA; &lt;li&gt;Make changes as suggested and update your branch or explain why no change is needed. Resolve the comment when done.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pull Request Detection Template Structure Validation Check&lt;/h3&gt; &#xA;&lt;p&gt;As part of the PR checks we run a structure validation to make sure all required parts of the YAML structure are included. For Detections, there is a new section that must be included. See the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/wiki/Contribute-to-Sentinel-GitHub-Community-of-Queries#now-onto-the-how&#34;&gt;contribution guidelines&lt;/a&gt; for more information. If this section or any other required section is not included, then a validation error will occur similar to the below. The example is specifically if the YAML is missing the entityMappings section:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;A total of 1 test files matched the specified pattern.&#xA;[xUnit.net 00:00:00.95]     Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [FAIL]&#xA;  X Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [104ms]&#xA;  Error Message:&#xA;   Expected object to be &amp;lt;null&amp;gt;, but found System.ComponentModel.DataAnnotations.ValidationException with message &#34;An old mapping for entity &#39;AccountCustomEntity&#39; does not have a matching new mapping entry.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pull Request Kql Validation Check&lt;/h3&gt; &#xA;&lt;p&gt;As part of the PR checks we run a syntax validation of the kql queries defined in the template. If this check fails go to Azure Pipeline (by pressing on the errors link on the checks tab in your PR) &lt;img src=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/.github/Media/Azurepipeline.png&#34; alt=&#34;Azurepipeline&#34;&gt; In the pipeline you can see which test failed and what is the cause: &lt;img src=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/.github/Media/PipelineTestsTab.png&#34; alt=&#34;Pipeline Tests Tab&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Example error message:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;A total of 1 test files matched the specified pattern.&#xA;[xUnit.net 00:00:01.81]     Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [FAIL]&#xA;  X Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [21ms]&#xA;  Error Message:&#xA;   Template Id:fa0ab69c-7124-4f62-acdd-61017cf6ce89 is not valid Errors:The name &#39;SymantecEndpointProtection&#39; does not refer to any known table, tabular variable or function., Code: &#39;KS204&#39;, Severity: &#39;Error&#39;, Location: &#39;67..93&#39;,The name &#39;SymantecEndpointProtection&#39; does not refer to any known table, tabular variable or function., Code: &#39;KS204&#39;, Severity: &#39;Error&#39;, Location: &#39;289..315&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using custom logs table (a table which is not defined on all workspaces by default) you should verify your table schema is defined in json file in the folder &lt;em&gt;Azure-Sentinel\.script\tests\KqlvalidationsTests\CustomTables&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example for table tablexyz.json&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;Name&#34;: &#34;tablexyz&#34;,&#xA;  &#34;Properties&#34;: [&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeDateTimeColumn&#34;,&#xA;      &#34;Type&#34;: &#34;DateTime&#34;&#xA;    },&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeStringColumn&#34;,&#xA;      &#34;Type&#34;: &#34;String&#34;&#xA;    },&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeDynamicColumn&#34;,&#xA;      &#34;Type&#34;: &#34;Dynamic&#34;&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Kql Validation Locally&lt;/h3&gt; &#xA;&lt;p&gt;In order to run the kql validation before submitting Pull Request in you local machine:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need to have &lt;strong&gt;.Net Core 3.1 SDK&lt;/strong&gt; installed &lt;a href=&#34;https://dotnet.microsoft.com/download&#34;&gt;How to download .Net&lt;/a&gt; (Supports all platforms)&lt;/li&gt; &#xA; &lt;li&gt;Open Shell and navigate to &lt;code&gt;Azure-Sentinel\\.script\tests\KqlvalidationsTests\&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;dotnet test&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of output (in Ubuntu):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Welcome to .NET Core 3.1!&#xA;---------------------&#xA;SDK Version: 3.1.403&#xA;&#xA;Telemetry&#xA;---------&#xA;The .NET Core tools collect usage data in order to help us improve your experience. The data is anonymous. It is collected by Microsoft and shared with the community. You can opt-out of telemetry by setting the DOTNET_CLI_TELEMETRY_OPTOUT environment variable to &#39;1&#39; or &#39;true&#39; using your favorite shell.&#xA;&#xA;Read more about .NET Core CLI Tools telemetry: https://aka.ms/dotnet-cli-telemetry&#xA;&#xA;----------------&#xA;Explore documentation: https://aka.ms/dotnet-docs&#xA;Report issues and find source on GitHub: https://github.com/dotnet/core&#xA;Find out what&#39;s new: https://aka.ms/dotnet-whats-new&#xA;Learn about the installed HTTPS developer cert: https://aka.ms/aspnet-core-https&#xA;Use &#39;dotnet --help&#39; to see available commands or visit: https://aka.ms/dotnet-cli-docs&#xA;Write your first app: https://aka.ms/first-net-core-app&#xA;--------------------------------------------------------------------------------------&#xA;Test run for /mnt/c/git/Azure-Sentinel/.script/tests/KqlvalidationsTests/bin/Debug/netcoreapp3.1/Kqlvalidations.Tests.dll(.NETCoreApp,Version=v3.1)&#xA;Microsoft (R) Test Execution Command Line Tool Version 16.7.0&#xA;Copyright (c) Microsoft Corporation.  All rights reserved.&#xA;&#xA;Starting test execution, please wait...&#xA;&#xA;A total of 1 test files matched the specified pattern.&#xA;&#xA;Test Run Successful.&#xA;Total tests: 171&#xA;     Passed: 171&#xA; Total time: 25.7973 Seconds&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Detection schema validation tests&lt;/h3&gt; &#xA;&lt;p&gt;Similarly to KQL Validation, there is an automatic validation of the schema of a detection. The schema validation includes the detection&#39;s frequency and period, the detection&#39;s trigger type and threshold, validity of connectors Ids (&lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/raw/master/.script/tests/detectionTemplateSchemaValidation/ValidConnectorIds.json&#34;&gt;valid connectors Ids list&lt;/a&gt;), etc. A wrong format or missing attributes will result with an informative check failure, which should guide you through the resolution of the issue, but make sure to look into the format of already approved detection.&lt;/p&gt; &#xA;&lt;h3&gt;Run Detection Schema Validation Locally&lt;/h3&gt; &#xA;&lt;p&gt;In order to run the kql validation before submitting Pull Request in you local machine:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need to have &lt;strong&gt;.Net Core 3.1 SDK&lt;/strong&gt; installed &lt;a href=&#34;https://dotnet.microsoft.com/download&#34;&gt;How to download .Net&lt;/a&gt; (Supports all platforms)&lt;/li&gt; &#xA; &lt;li&gt;Open Shell and navigate to &lt;code&gt;Azure-Sentinel\\.script\tests\DetectionTemplateSchemaValidation\&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;dotnet test&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;p&gt;For information on what you can contribute and further details, refer to the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/wiki#get-started&#34;&gt;&#34;get started&#34;&lt;/a&gt; section on the project&#39;s &lt;a href=&#34;https://aka.ms/threathunters&#34;&gt;wiki&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mhamilton723/STEGO</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/mhamilton723/STEGO</id>
    <link href="https://github.com/mhamilton723/STEGO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unsupervised Semantic Segmentation by Distilling Feature Correspondences&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;STEGO: Unsupervised Semantic Segmentation by Distilling Feature Correspondences&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://mhamilton.net/stego.html&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2203.08414&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://aka.ms/stego-video&#34;&gt;Video&lt;/a&gt; | &lt;a href=&#34;https://iclr.cc/virtual/2022/poster/6068&#34;&gt;ICLR 2022&lt;/a&gt; |&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mhamilton.net/&#34;&gt;Mark Hamilton&lt;/a&gt;, &lt;a href=&#34;https://ztzhang.info/&#34;&gt;Zhoutong Zhang&lt;/a&gt;, &lt;a href=&#34;http://home.bharathh.info/&#34;&gt;Bharath Hariharan&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cornell.edu/~snavely/&#34;&gt;Noah Snavely&lt;/a&gt;, &lt;a href=&#34;https://billf.mit.edu/about/bio&#34;&gt;William T. Freeman&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official implementation of the paper &#34;Unsupervised Semantic Segmentation by Distilling Feature Correspondences&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/mhamilton723/STEGO/blob/master/src/STEGO_Colab_Demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://paperswithcode.com/sota/unsupervised-semantic-segmentation-on?p=unsupervised-semantic-segmentation-by-2&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/unsupervised-semantic-segmentation-by-2/unsupervised-semantic-segmentation-on&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://paperswithcode.com/sota/unsupervised-semantic-segmentation-on-coco-4?p=unsupervised-semantic-segmentation-by-2&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/unsupervised-semantic-segmentation-by-2/unsupervised-semantic-segmentation-on-coco-4&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://paperswithcode.com/sota/unsupervised-semantic-segmentation-on-potsdam-1?p=unsupervised-semantic-segmentation-by-2&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/unsupervised-semantic-segmentation-by-2/unsupervised-semantic-segmentation-on-potsdam-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/NPub4E4o8BA&#34;&gt;&lt;img src=&#34;https://marhamilresearch4.blob.core.windows.net/stego-public/graphics/STEGO%20Header%20video%20(2).jpg&#34; alt=&#34;Overview Video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;!--ts--&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#training&#34;&gt;Training&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#bringing-your-own-data&#34;&gt;Bringing your own data&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#understanding-stego&#34;&gt;Understanding STEGO&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#unsupervised-semantic-segmentation&#34;&gt;Unsupervised Semantic Segmentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#deep-features-connect-objects-across-images&#34;&gt;Deep features connect objects across images&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#the-stego-architecture&#34;&gt;The STEGO architecture&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/#contact&#34;&gt;Contact&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--te--&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;h3&gt;Clone this repository:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/mhamilton723/STEGO.git&#xA;cd STEGO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install Conda Environment&lt;/h3&gt; &#xA;&lt;p&gt;Please visit the &lt;a href=&#34;https://docs.anaconda.com/anaconda/install/index.html&#34;&gt;Anaconda install page&lt;/a&gt; if you do not already have conda installed&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f environment.yml&#xA;conda activate stego&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download Pre-Trained Models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd src&#xA;python download_models.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download Datasets&lt;/h3&gt; &#xA;&lt;p&gt;First, change the &lt;code&gt;pytorch_data_dir&lt;/code&gt; variable to your systems pytorch data directory where datasets are stored.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python download_datasets.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once downloaded please navigate to your pytorch data dir and unzip the resulting files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd /YOUR/PYTORCH/DATA/DIR&#xA;unzip cocostuff.zip&#xA;unzip cityscapes.zip&#xA;unzip potsdam.zip&#xA;unzip potsdamraw.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To evaluate our pretrained models please run the following in &lt;code&gt;STEGO/src&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python eval_segmentation.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;One can change the evaluation parameters and model by editing &lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/src/configs/eval_config.yml&#34;&gt;&lt;code&gt;STEGO/src/configs/eval_config.yml&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To train STEGO from scratch, please first generate the KNN indices for the datasets of interest:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python precompute_knns.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can run the following in &lt;code&gt;STEGO/src&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train_segmentation.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Hyperparameters can be adjusted in &lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/src/configs/train_config.yml&#34;&gt;&lt;code&gt;STEGO/src/configs/train_config.yml&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To monitor training with tensorboard run the following from &lt;code&gt;STEGO&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;tensorboard --logdir logs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Bringing your own data&lt;/h3&gt; &#xA;&lt;p&gt;To train STEGO on your own dataset please create a directory in your pytorch data root with the following structure. Note, if you do not have labels, omit the &lt;code&gt;labels&lt;/code&gt; directory from the structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dataset_name&#xA;|‚îÄ‚îÄ imgs&#xA;|   ‚îú‚îÄ‚îÄ train&#xA;|   |   |‚îÄ‚îÄ unique_img_name_1.jpg&#xA;|   |   ‚îî‚îÄ‚îÄ unique_img_name_2.jpg&#xA;|   ‚îî‚îÄ‚îÄ val&#xA;|       |‚îÄ‚îÄ unique_img_name_3.jpg&#xA;|       ‚îî‚îÄ‚îÄ unique_img_name_4.jpg&#xA;‚îî‚îÄ‚îÄ labels&#xA;    ‚îú‚îÄ‚îÄ train&#xA;    |   |‚îÄ‚îÄ unique_img_name_1.png&#xA;    |   ‚îî‚îÄ‚îÄ unique_img_name_2.png&#xA;    ‚îî‚îÄ‚îÄ val&#xA;        |‚îÄ‚îÄ unique_img_name_3.png&#xA;        ‚îî‚îÄ‚îÄ unique_img_name_4.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next in &lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/src/configs/train_config.yml&#34;&gt;&lt;code&gt;STEGO/src/configs/train_config.yml&lt;/code&gt;&lt;/a&gt; set the following parameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;dataset_name: &#34;directory&#34;&#xA;dir_dataset_name: &#34;dataset_name&#34;&#xA;dir_dataset_n_classes: 5 # This is the number of object types to find&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to train with cropping to increase spatial resolution run our &lt;a href=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/src/crop_datasets.py&#34;&gt;cropping utility&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Finally, uncomment the custom dataset code and run &lt;code&gt;python precompute_knns.py&lt;/code&gt; from &lt;code&gt;STEGO\src&lt;/code&gt; to generate the prerequisite KNN information for the custom dataset.&lt;/p&gt; &#xA;&lt;p&gt;You can now train on your custom dataset using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train_segmentation.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Understanding STEGO&lt;/h2&gt; &#xA;&lt;h3&gt;Unsupervised semantic segmentation&lt;/h3&gt; &#xA;&lt;p&gt;Real-world images can be cluttered with multiple objects making classification feel arbitrary. Furthermore, objects in the real world don&#39;t always fit in bounding boxes. Semantic segmentation methods aim to avoid these challenges by assigning each pixel of an image its own class label. Conventional semantic segmentation methods are notoriously difficult to train due to their dependence on densely labeled images, which can take 100x longer to create than bounding boxes or class annotations. This makes it hard to gather sizable and diverse datasets impossible in domains where humans don&#39;t know the structure a-priori. We sidestep these challenges by learning an ontology of objects with pixel-level semantic segmentation through only self-supervision.&lt;/p&gt; &#xA;&lt;h3&gt;Deep features connect objects across images&lt;/h3&gt; &#xA;&lt;p&gt;Self-supervised contrastive learning enables algorithms to learn intelligent representations for images without supervision. STEGO builds on this work by showing that representations from self-supervised visual transformers like Caron et. al.‚Äôs DINO are already aware of the relationships between objects. By computing the cosine similarity between image features, we can see that similar semantic regions such as grass, motorcycles, and sky are ‚Äúlinked‚Äù together by feature similarity.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://mhamilton.net/images/Picture3.gif&#34; alt=&#34;Feature connection GIF&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;The STEGO architecture&lt;/h3&gt; &#xA;&lt;p&gt;The STEGO unsupervised segmentation system learns by distilling correspondences between images into a set of class labels using a contrastive loss. In particular we aim to learn a segmentation that respects the induced correspondences between objects. To achieve this we train a shallow segmentation network on top of the DINO ViT backbone with three contrastive terms that distill connections between an image and itself, similar images, and random other images respectively. If two regions are strongly coupled by deep features we encourage them to share the same class.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/results/figures/stego.svg?sanitize=true&#34; alt=&#34;Architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Results&lt;/h3&gt; &#xA;&lt;p&gt;We evaluate the STEGO algorithm on the CocoStuff, Cityscapes, and Potsdam semantic segmentation datasets. Because these methods see no labels, we use a Hungarian matching algorithm to find the best mapping between clusters and dataset classes. We find that STEGO is capable of segmenting complex and cluttered scenes with much higher spatial resolution and sensitivity than the prior art, &lt;a href=&#34;https://sites.google.com/view/picie-cvpr2021/home&#34;&gt;PiCIE&lt;/a&gt;. This not only yields a substantial qualitative improvement, but also more than doubles the mean intersection over union (mIoU). For results on Cityscapes, and Potsdam see &lt;a href=&#34;https://arxiv.org/abs/2203.08414&#34;&gt;our paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mhamilton723/STEGO/master/results/figures/cocostuff27_results.jpg&#34; alt=&#34;Cocostuff results&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{hamilton2022unsupervised,&#xA;  title={Unsupervised Semantic Segmentation by Distilling Feature Correspondences},&#xA;  author={Hamilton, Mark and Zhang, Zhoutong and Hariharan, Bharath and Snavely, Noah and Freeman, William T},&#xA;  journal={arXiv preprint arXiv:2203.08414},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;For feedback, questions, or press inquiries please contact &lt;a href=&#34;mailto:markth@mit.edu&#34;&gt;Mark Hamilton&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/NeMo</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/NVIDIA/NeMo</id>
    <link href="https://github.com/NVIDIA/NeMo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NeMo: a toolkit for conversational AI&lt;/p&gt;&lt;hr&gt;&lt;p&gt;|status| |documentation| |license| |lgtm_grade| |lgtm_alerts| |black|&lt;/p&gt; &#xA;&lt;p&gt;.. |status| image:: &lt;a href=&#34;http://www.repostatus.org/badges/latest/active.svg&#34;&gt;http://www.repostatus.org/badges/latest/active.svg&lt;/a&gt; :target: &lt;a href=&#34;http://www.repostatus.org/#active&#34;&gt;http://www.repostatus.org/#active&lt;/a&gt; :alt: Project Status: Active ‚Äì The project has reached a stable, usable state and is being actively developed.&lt;/p&gt; &#xA;&lt;p&gt;.. |documentation| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&lt;/a&gt; :alt: Documentation :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |license| image:: &lt;a href=&#34;https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg&#34;&gt;https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/NVIDIA/NeMo/raw/master/LICENSE&#34;&gt;https://github.com/NVIDIA/NeMo/blob/master/LICENSE&lt;/a&gt; :alt: NeMo core license and license for collections in this repo&lt;/p&gt; &#xA;&lt;p&gt;.. |lgtm_grade| image:: &lt;a href=&#34;https://img.shields.io/lgtm/grade/python/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&#34;&gt;https://img.shields.io/lgtm/grade/python/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&lt;/a&gt; :target: &lt;a href=&#34;https://lgtm.com/projects/g/NVIDIA/NeMo/context:python&#34;&gt;https://lgtm.com/projects/g/NVIDIA/NeMo/context:python&lt;/a&gt; :alt: Language grade: Python&lt;/p&gt; &#xA;&lt;p&gt;.. |lgtm_alerts| image:: &lt;a href=&#34;https://img.shields.io/lgtm/alerts/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&#34;&gt;https://img.shields.io/lgtm/alerts/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&lt;/a&gt; :target: &lt;a href=&#34;https://lgtm.com/projects/g/NVIDIA/NeMo/alerts/&#34;&gt;https://lgtm.com/projects/g/NVIDIA/NeMo/alerts/&lt;/a&gt; :alt: Total alerts&lt;/p&gt; &#xA;&lt;p&gt;.. |black| image:: &lt;a href=&#34;https://img.shields.io/badge/code%20style-black-000000.svg&#34;&gt;https://img.shields.io/badge/code%20style-black-000000.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/psf/black&#34;&gt;https://github.com/psf/black&lt;/a&gt; :alt: Code style: black&lt;/p&gt; &#xA;&lt;p&gt;.. _main-readme:&lt;/p&gt; &#xA;&lt;h1&gt;&lt;strong&gt;NVIDIA NeMo&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech synthesis (TTS). The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new &lt;code&gt;conversational AI models &amp;lt;https://developer.nvidia.com/conversational-ai#started&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Pre-trained NeMo models. &amp;lt;https://catalog.ngc.nvidia.com/models?query=nemo&amp;amp;orderBy=weightPopularDESC&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Introductory video. &amp;lt;https://www.youtube.com/embed/wBgpMf_KQVw&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speech processing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Automatic Speech Recognition (ASR) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/intro.html&amp;gt;&lt;/code&gt;_ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Supported models: Jasper, QuartzNet, CitriNet, Conformer-CTC, Conformer-Transducer, ContextNet, LSTM-Transducer (RNNT), LSTM-CTC, ...&lt;/li&gt; &#xA;     &lt;li&gt;Supports CTC and Transducer/RNNT losses/decoders&lt;/li&gt; &#xA;     &lt;li&gt;Beam Search decoding&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;Language Modelling for ASR &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/asr_language_modeling.html&amp;gt;&lt;/code&gt;_: N-gram LM in fusion with Beam Search decoding, Neural Rescoring with Transformer&lt;/li&gt; &#xA;     &lt;li&gt;Streaming and Buffered ASR (CTC/Transducer) - &lt;code&gt;Chunked Inference Examples &amp;lt;https://github.com/NVIDIA/NeMo/tree/main/examples/asr/asr_chunked_inference&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speech Classification and Speech Command Recognition &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speech_classification/intro.html&amp;gt;&lt;/code&gt;_: MatchboxNet (Command Recognition)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Voice activity Detection (VAD) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/speech_classification/models.html#marblenet-vad&amp;gt;&lt;/code&gt;_: MarbleNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speaker Recognition &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speaker_recognition/intro.html&amp;gt;&lt;/code&gt;_: TitaNet, ECAPA_TDNN, SpeakerNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speaker Diarization &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speaker_diarization/intro.html&amp;gt;&lt;/code&gt;_: TitaNet, ECAPA_TDNN, SpeakerNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Pretrained models on different languages. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_asr&amp;gt;&lt;/code&gt;_: English, Spanish, German, Russian, Chinese, French, Italian, Polish, ...&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained speech processing models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_asr&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Natural Language Processing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;NeMo Megatron pre-training of Large Language Models &amp;lt;https://developer.nvidia.com/nemo-megatron-early-access&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Neural Machine Translation (NMT) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/machine_translation.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Punctuation and Capitalization &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Token classification (named entity recognition) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/token_classification.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Text classification &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/text_classification.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Joint Intent and Slot Classification &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/joint_intent_slot.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Question answering &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/question_answering.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;GLUE benchmark &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/glue_benchmark.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Information retrieval &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/information_retrieval.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Entity Linking &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/entity_linking.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Dialogue State Tracking &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/sgd_qa.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Prompt Tuning &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/prompt_learning.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained NLP models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_nlp&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Speech synthesis (TTS) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tts/intro.html#&amp;gt;&lt;/code&gt;_ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Spectrogram generation: Tacotron2, GlowTTS, TalkNet, FastPitch, FastSpeech2, Mixer-TTS, Mixer-TTS-X&lt;/li&gt; &#xA;   &lt;li&gt;Vocoders: WaveGlow, SqueezeWave, UniGlow, MelGAN, HiFiGAN, UnivNet&lt;/li&gt; &#xA;   &lt;li&gt;End-to-end speech generation: FastPitch_HifiGan_E2E, FastSpeech2_HifiGan_E2E&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained TTS models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_tts&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Tools &amp;lt;https://github.com/NVIDIA/NeMo/tree/main/tools&amp;gt;&lt;/code&gt;_ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Text Processing (text normalization and inverse text normalization) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/text_normalization/intro.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;CTC-Segmentation tool &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tools/ctc_segmentation.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speech Data Explorer &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tools/speech_data_explorer.html&amp;gt;&lt;/code&gt;_: a dash-based tool for interactive exploration of ASR/TTS datasets&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Built for speed, NeMo can utilize NVIDIA&#39;s Tensor Cores and scale out training to multiple GPUs and multiple nodes.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python 3.8 or above&lt;/li&gt; &#xA; &lt;li&gt;Pytorch 1.10.0 or above&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU for training&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;.. |main| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&lt;/a&gt; :alt: Documentation Status :scale: 100% :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |stable| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable&lt;/a&gt; :alt: Documentation Status :scale: 100% :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;+---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+ | Version | Status | Description | +=========+=============+==========================================================================================================================================+ | Latest | |main| | &lt;code&gt;Documentation of the latest (i.e. main) branch. &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&amp;gt;&lt;/code&gt;_ | +---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+ | Stable | |stable| | &lt;code&gt;Documentation of the stable (i.e. most recent release) branch. &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&amp;gt;&lt;/code&gt;_ | +---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;A great way to start with NeMo is by checking &lt;code&gt;one of our tutorials &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h2&gt;Getting help with NeMo&lt;/h2&gt; &#xA;&lt;p&gt;FAQ can be found on NeMo&#39;s &lt;code&gt;Discussions board &amp;lt;https://github.com/NVIDIA/NeMo/discussions&amp;gt;&lt;/code&gt;_. You are welcome to ask questions or start discussions there.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Conda&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;We recommend installing NeMo in a fresh Conda environment.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    conda create --name nemo python==3.8&#xA;    conda activate nemo&#xA;&#xA;Install PyTorch using their `configurator &amp;lt;https://pytorch.org/get-started/locally/&amp;gt;`_. &#xA;&#xA;.. code-block:: bash&#xA;&#xA;    conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch&#xA;&#xA;.. note::&#xA;&#xA;  The command used to install PyTorch may depend on your system.&#xA;&#xA;Pip&#xA;~~~&#xA;Use this installation mode if you want the latest released version.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;    pip install Cython&#xA;    pip install nemo_toolkit[&#39;all&#39;]&#xA;&#xA;.. note::&#xA;&#xA;    Depending on the shell used, you may need to use ``&#34;nemo_toolkit[all]&#34;`` instead in the above command.&#xA;&#xA;Pip from source&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use this installation mode if you want the a version from particular GitHub branch (e.g main).&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;pip install Cython&#xA;python -m pip install git+https://github.com/NVIDIA/NeMo.git@{BRANCH}#egg=nemo_toolkit[all]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From source&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Use this installation mode if you are contributing to NeMo.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;    git clone https://github.com/NVIDIA/NeMo&#xA;    cd NeMo&#xA;    ./reinstall.sh&#xA;&#xA;.. note::&#xA;&#xA;    If you only want the toolkit without additional conda-based dependencies, you may replace ``reinstall.sh``&#xA;    with ``pip install -e .`` when your PWD is the root of the NeMo repository.&#xA;&#xA;RNNT&#xA;~~~~&#xA;Note that RNNT requires numba to be installed from conda.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;  conda remove numba&#xA;  pip uninstall numba&#xA;  conda install -c conda-forge numba&#xA;&#xA;Megatron GPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Megatron GPT training requires NVIDIA Apex to be installed.&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/NVIDIA/apex&#xA;cd apex&#xA;git checkout 9263bc8c6c16555bd55dd759f1a1b8c0cd187d10&#xA;pip install -v --disable-pip-version-check --no-cache-dir --global-option=&#34;--cpp_ext&#34; --global-option=&#34;--cuda_ext&#34; --global-option=&#34;--fast_layer_norm&#34; ./&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker containers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;To build a nemo container with Dockerfile from a branch, please run &#xA;&#xA;.. code-block:: bash&#xA;&#xA;    DOCKER_BUILDKIT=1 docker build -f Dockerfile -t nemo:latest .&#xA;&#xA;&#xA;If you chose to work with main branch, we recommend using NVIDIA&#39;s PyTorch container version 22.04-py3 and then installing from GitHub.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    docker run --gpus all -it --rm -v &amp;lt;nemo_github_folder&amp;gt;:/NeMo --shm-size=8g \&#xA;    -p 8888:8888 -p 6006:6006 --ulimit memlock=-1 --ulimit \&#xA;    stack=67108864 --device=/dev/snd nvcr.io/nvidia/pytorch:22.04-py3&#xA;&#xA;Examples&#xA;--------&#xA;&#xA;Many examples can be found under `&#34;Examples&#34; &amp;lt;https://github.com/NVIDIA/NeMo/tree/stable/examples&amp;gt;`_ folder.&#xA;&#xA;&#xA;Contributing&#xA;------------&#xA;&#xA;We welcome community contributions! Please refer to the  `CONTRIBUTING.md &amp;lt;https://github.com/NVIDIA/NeMo/blob/stable/CONTRIBUTING.md&amp;gt;`_ CONTRIBUTING.md for the process.&#xA;&#xA;Publications&#xA;------------&#xA;&#xA;We provide an ever growing list of publications that utilize the NeMo framework. Please refer to `PUBLICATIONS.md &amp;lt;https://github.com/NVIDIA/NeMo/blob/main/PUBLICATIONS.md&amp;gt;`_. We welcome the addition of your own articles to this list !&#xA;&#xA;Citation&#xA;--------&#xA;&#xA;.. code-block:: bash&#xA;&#xA;  @article{kuchaiev2019nemo,&#xA;    title={Nemo: a toolkit for building ai applications using neural modules},&#xA;    author={Kuchaiev, Oleksii and Li, Jason and Nguyen, Huyen and Hrinchuk, Oleksii and Leary, Ryan and Ginsburg, Boris and Kriman, Samuel and Beliaev, Stanislav and Lavrukhin, Vitaly and Cook, Jack and others},&#xA;    journal={arXiv preprint arXiv:1909.09577},&#xA;    year={2019}&#xA;  }&#xA;&#xA;License&#xA;-------&#xA;NeMo is under `Apache 2.0 license &amp;lt;https://github.com/NVIDIA/NeMo/blob/stable/LICENSE&amp;gt;`_.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>jaakkopasanen/AutoEq</title>
    <updated>2022-05-29T02:42:18Z</updated>
    <id>tag:github.com,2022-05-29:/jaakkopasanen/AutoEq</id>
    <link href="https://github.com/jaakkopasanen/AutoEq" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Automatic headphone equalization from frequency responses&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AutoEQ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; If you are here just looking to make your headphones sound better, find your headphone model in &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/results&#34;&gt;results&lt;/a&gt; folder&#39;s recommended headphones list and follow instructions in &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/#usage&#34;&gt;Usage&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;About This Project&lt;/h2&gt; &#xA;&lt;p&gt;AutoEQ is a project for equalizing headphone frequency responses automatically and it achieves this by parsing frequency response measurements and producing equalization settings which correct the headphone to a neutral sound. This project currently has over 2500 headphones covered in the &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/results&#34;&gt;results&lt;/a&gt; folder. See &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/#usage&#34;&gt;Usage&lt;/a&gt; for instructions how to use the results with different equalizer softwares and &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/#results&#34;&gt;Results&lt;/a&gt; section for details about parameters and how the results were obtained.&lt;/p&gt; &#xA;&lt;p&gt;AutoEQ is not just a collection of automatically produced headphone equalization settings but also a tool for equalizing headphones for yourself. &lt;code&gt;autoeq.py&lt;/code&gt; provides methods for reading data, equalizing it to a given target response and saving the results for usage with equalizers. It&#39;s possible to use different compensation (target) curves, apply tilt for making the headphones brighter/darker and adding a bass boost. It&#39;s even possible to make one headphone sound (roughly) like another headphone. For more info about equalizing see &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/#equalizing&#34;&gt;Equalizing&lt;/a&gt;. If you&#39;re looking for something light weight to install as a dependency for your own project, you&#39;ll find &lt;a href=&#34;https://github.com/jaakkopasanen/autoeq-pkg&#34;&gt;autoeq-pkg&lt;/a&gt; much more suited for your needs.&lt;/p&gt; &#xA;&lt;p&gt;Third major contribution of this project is the measurement data and compensation curves all in a numerical format except for Crinacle&#39;s raw data. Everything is stored as CSV files so they are easy to process with any programming language or even Microsoft Excel.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/results/oratory1990/harman_over-ear_2018/Sennheiser%20HD%20800/Sennheiser%20HD%20800.png&#34; alt=&#34;Sennheiser HD 800&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Sennheiser HD 800 equalization results plotted&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;AutoEQ produces settings for basically all types of equalizer apps.&lt;/p&gt; &#xA;&lt;h3&gt;Convolution Equalizers&lt;/h3&gt; &#xA;&lt;p&gt;Convolution equalizer is the most powerful type of equalizer software. These equalizers allow extremly precise control over the frequency response and the results are the same on all devices and platforms when using the same FIR filter. Convolution equalizer is the preferred way to use AutoEq results.&lt;/p&gt; &#xA;&lt;p&gt;AutoEq supports convolution equalizers with FIR filters as WAV files and with EqualizerAPO&#39;s GraphicEQ filter type. The default results contain FIR filters for both 44.1 kHz and 48 kHz sampling rates. Other sampling rates are supported but not given in the default results. EqualizerAPO&#39;s GraphicEQ works with any sampling rate.&lt;/p&gt; &#xA;&lt;p&gt;To use the FIR filters, download the appropriate WAV file and import it to the EQ software of your choice. Please keep in mind that not all EQ softwares support convolution. Some equalizers can load multiple FIR filters at the same time. Download both WAV files, create a Zip file containing both and load the Zip file to for example Roon.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/#EqualizerAPO&#34;&gt;EqualizerApo&lt;/a&gt; for instructions on how to use the GraphicEQ.&lt;/p&gt; &#xA;&lt;h3&gt;Parametric Equalizers&lt;/h3&gt; &#xA;&lt;p&gt;Parametric equalizers have filters (bands) with user adjustable gain, center frequency and quality Q. Keep in mind that parametric eq accuracy depends on the number of filters available. Usually 10 filters produce very good results but as little as 5 can be good enough. Keep in mind that different parametric equalizers will produce different outcomes with the same parameter values. Parameters produced by AutoEq are equal with EqualizerAPO using 48 kHz sampling rate. When using other equalizers or sampling rates, it&#39;s always highly recommended to check that the frequency response of the equalizer matches the parametric eq curve in the graphs.&lt;/p&gt; &#xA;&lt;p&gt;All parametric equalizer except Peace require you to configure the filter parameters manually with the software user interface. Some parametric equalizer use filter width (band width) instead of Q. Filter width can be calculated as: &lt;code&gt;bw = Fc / Q&lt;/code&gt; where &lt;code&gt;bw&lt;/code&gt; is the band width in Herts, &lt;code&gt;Fc&lt;/code&gt; is center frequency and &lt;code&gt;Q&lt;/code&gt; is quality. Filter width in octaves can be calculated as: &lt;code&gt;N = ln(1 + 1/(2*Q^2) + sqrt(((2*Q^2 + 1) / Q^2 )^2 / 4 - 1)) / ln(2)&lt;/code&gt; where &lt;code&gt;ln&lt;/code&gt; is the natural logarithm. See &lt;a href=&#34;http://www.sengpielaudio.com/calculator-bandwidth.htm&#34;&gt;http://www.sengpielaudio.com/calculator-bandwidth.htm&lt;/a&gt; for an online calculator.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s very important to set preamp according to the value given in the result README.md document. Parametric eq filters will produce positive gains and to avoid clipping a preamp with negative gain is required.&lt;/p&gt; &#xA;&lt;p&gt;Parametric eq settings can be used with Peace or any other parametric eq which has at least 5 bands available. Even fewer bands is possible but pre-computed results require to use minimum five first of the filters. Parametric equalizer filter parameters look like this:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Fc&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Q&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Gain&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;28 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.46&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6.3 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;162 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.91&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-2.3 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2237 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.94&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-4.6 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6093 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.26&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-4.7 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;8251 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.71&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-2.9 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Fixed Band Equalizers&lt;/h3&gt; &#xA;&lt;p&gt;Fixed band eq is more commonly known as graphic equalizer but in order not to confuse with EqualizerAPO GraphicEQ it is called like that in this project. Fixed band equalizer is like parametric equalizer with several peaking filters but don&#39;t have adjustable frequency information, only gain. All other types are preferred over fixed band equalizers but on some devices these are the only available ones.&lt;/p&gt; &#xA;&lt;p&gt;Fixed band equalizers have trouble compensating for narrow notches and peaks that fall between two bands. Good example is &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/results/oratory1990/harman_over-ear_2018/Sennheiser%20HD%20800&#34;&gt;Sennheiser HD 800&lt;/a&gt; with it&#39;s 6 kHz peak that is right in between 4 kHz and 8 kHz bands of standard 10-band equalizer. When using 10-band equalizer check if the fixed band equalization curve is very different than the desired equalization curve at some frequency and adjust the nearby filters by ear for best results.&lt;/p&gt; &#xA;&lt;p&gt;Fixed band equalizer settings look like this:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Fc&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Q&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Gain&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;31 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.41&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6.1 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;62 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.41&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.0 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;125 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.41&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-1.1 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;250 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.41&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-2.2 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;500 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.41&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-0.9 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1000 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.41&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.1 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2000 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.41&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.6 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4000 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.41&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-1.0 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;8000 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.41&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-4.1 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16000 Hz&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.41&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-7.5 dB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;has &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/#equalizerapo&#34;&gt;EqualizerAPO&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/#peace&#34;&gt;Peace&lt;/a&gt; and many media players with parametric equalizers such as &lt;a href=&#34;https://www.microsoft.com/en-us/p/neutron-music-player/9nblggh4vp2h?activetab=pivot:overviewtab&#34;&gt;Neutron&lt;/a&gt;, &lt;a href=&#34;https://roonlabs.com/&#34;&gt;Roon&lt;/a&gt; and &lt;a href=&#34;https://www.foobar2000.org/&#34;&gt;Foobar2000&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;EqualizerAPO&lt;/h4&gt; &#xA;&lt;p&gt;It&#39;s possible to use plain &lt;a href=&#34;https://sourceforge.net/projects/equalizerapo/&#34;&gt;EqualizerAPO&lt;/a&gt; and edit configuration file in &lt;code&gt;C:\Program Files\EqualizerAPO\config\config.txt&lt;/code&gt;. Replace contents of the file with the GraphicEQ.txt file found in results. Preamp is not needed because it is incorporated into the GraphicEQ line. Using &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/results/oratory1990/harman_over-ear_2018/Sennheiser%20HD%20650&#34;&gt;Sennheiser HD 650&lt;/a&gt; would make config file look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;GraphicEQ: 20 -0.5; 21 -0.5; 22 -0.5; 23 -0.5; 24 -0.5; 26 -0.5; 27 -0.5; 29 -0.5; 30 -0.5; 32 -0.9; 34 -1.2; 36 -1.5; 38 -1.8; 40 -2.1; 43 -2.4; 45 -2.4; 48 -2.5; 50 -2.6; 53 -3.0; 56 -3.2; 59 -3.3; 63 -3.6; 66 -4.0; 70 -4.5; 74 -5.0; 78 -5.5; 83 -5.9; 87 -6.3; 92 -6.7; 97 -7.0; 103 -7.3; 109 -7.6; 115 -7.8; 121 -8.0; 128 -8.1; 136 -8.4; 143 -8.6; 151 -8.7; 160 -8.8; 169 -8.8; 178 -8.9; 188 -8.9; 199 -9.0; 210 -9.0; 222 -9.0; 235 -8.9; 248 -8.8; 262 -8.6; 277 -8.5; 292 -8.4; 309 -8.3; 326 -8.2; 345 -8.1; 364 -7.9; 385 -7.8; 406 -7.7; 429 -7.6; 453 -7.6; 479 -7.5; 506 -7.4; 534 -7.2; 565 -7.1; 596 -7.0; 630 -7.0; 665 -7.0; 703 -7.0; 743 -7.0; 784 -7.1; 829 -7.1; 875 -7.1; 924 -7.0; 977 -7.1; 1032 -7.2; 1090 -7.3; 1151 -7.2; 1216 -7.0; 1284 -6.9; 1357 -6.7; 1433 -6.4; 1514 -6.2; 1599 -6.1; 1689 -5.9; 1784 -5.6; 1885 -5.4; 1991 -5.2; 2103 -5.1; 2221 -5.2; 2347 -5.5; 2479 -5.8; 2618 -6.2; 2766 -6.6; 2921 -6.9; 3086 -7.0; 3260 -6.8; 3443 -6.2; 3637 -5.6; 3842 -5.1; 4058 -4.6; 4287 -4.2; 4528 -4.2; 4783 -4.8; 5052 -5.2; 5337 -4.8; 5637 -4.3; 5955 -4.7; 6290 -4.9; 6644 -4.7; 7018 -4.8; 7414 -5.7; 7831 -6.3; 8272 -6.5; 8738 -6.5; 9230 -6.5; 9749 -6.5; 10298 -6.5; 10878 -6.5; 11490 -6.5; 12137 -6.5; 12821 -7.8; 13543 -9.9; 14305 -9.7; 15110 -9.0; 15961 -11.0; 16860 -13.5; 17809 -14.5; 18812 -15.2; 19871 -15.7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;EqualizerAPO has a graphical user interface for adjusting configurations. Launch the editor from &lt;code&gt;C:\Program Files\EqualizerAPO\Editor.exe&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/lHhRBuA.png&#34; alt=&#34;equalizerapo-editor&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;EqualizerAPO Editor GUI&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Peace&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://sourceforge.net/projects/peace-equalizer-apo-extension/&#34;&gt;Peace&lt;/a&gt; is a GUI for manipulating parametric eq filters with EqualizerAPO. Peace also has visualization for the end result equalization frequency response, profile manager for multiple different eq settings and a switch for disabling everything among other features. Load eq settings into Peace by clicking &lt;em&gt;Import&lt;/em&gt; button and select the &lt;em&gt;&#xA;  &lt;model&gt;&#xA;    ParametricEQ.txt&#xA;  &lt;/model&gt;&lt;/em&gt; file. Set the preamp to value mentioned in the results.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/e0POEbF.png&#34; alt=&#34;peace&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Peace with full GUI for EqualizerAPO&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Android&lt;/h3&gt; &#xA;&lt;p&gt;Android has several different equalizer options but not too many powerful apps which work with all apps. Wavelet is the best option for newer Androids (version 9 and up) but older devices have a built-in fixed band equalizer which works system wide but the center frequencies and Q values vary so might need to &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/#equalizing&#34;&gt;produce your own results&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Wavelet&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://play.google.com/store/apps/details?id=com.pittvandewitt.wavelet&#34;&gt;Wavelet&lt;/a&gt; is an Android app which comes with all the AutoEq eq profiles built in. The app works with all music apps so is closest to system-wide equalizer one can have on Android without rooting. The equalizer built into this app is very powerful and can represent the AutoEq profiles very accurately. There is also an option to tune the sound with graphic equalizer. Wavelet has the best Bluetooth device compatibility of all the tested eq apps on Android.&lt;/p&gt; &#xA;&lt;p&gt;The main functionalities of Wavelet are free (including AutoEq profiles and graphic eq) but some extra features can be unlocked with an in-app purchase.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/UGiBwFX.png&#34; alt=&#34;Wavelet&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Neutron&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://play.google.com/store/apps/details?id=com.neutroncode.mp&#34;&gt;Neutron&lt;/a&gt; is a music player with parametric equalizer and comes with all of the AutoEq profiles built in but is not free.&lt;/p&gt; &#xA;&lt;h4&gt;USB Audio Player PRO&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://play.google.com/store/apps/details?id=com.extreamsd.usbaudioplayerpro&#34;&gt;USB Audio Player PRO&lt;/a&gt; is an Android app with improved USB audio drivers for usage with USB DACs. USB Audio Player PRO is not system-wide but works with local files and many streaming services though not with Spotify. USB Audio Player has Toneboosters Morphit plugin which has parametric equalizer. This app and the plugin are not free.&lt;/p&gt; &#xA;&lt;h4&gt;Music EQ Equalizer&lt;/h4&gt; &#xA;&lt;p&gt;The best app for system wide equalization on older Android phones (without rooting) is &lt;a href=&#34;https://play.google.com/store/apps/details?id=mediam.music.equalizer&#34;&gt;Music Equalizer EQ&lt;/a&gt; which is a 10-band standard equalizer. Gains for each band can be adjusted with only 1 dB resolution but this isn&#39;t a problem because the average error is then only 0.25 dB, hardly noticeable. Bigger problem is the potential narrow peaks and notches between the bands&#39; center frequencies since there isn&#39;t really anything that can be done for those. See notes about &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/#fixed-band-equalizers&#34;&gt;fixed band equalizers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The app starts in the presets view so you need to click the left arrow in the top left corner to get to the manual view. Here you can adjust the bands. Set each band level to the closest value to what the equalization settings ask. Pre-computed results only support standard 10-band equalizers which have band center frequencies at 31, 63, 125, 250, 500, 1000, 2000, 4000, 8000 and 16000 Hz. Q values are not adjustable so you don&#39;t have to worry about those even though they are given in the result settings.&lt;/p&gt; &#xA;&lt;h4&gt;Viper4Android&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://forum.xda-developers.com/showthread.php?t=2191223&#34;&gt;Viper4Android&lt;/a&gt; is a system-wide convolution based equalizer (and much more) on Android but it requires rooting of the device. Viper4Android is supported with impulse response (WAV) files. For rooted users this is the best option.&lt;/p&gt; &#xA;&lt;h4&gt;JamesDSP&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://forum.xda-developers.com/android/apps-games/app-reformed-dsp-manager-t3607970&#34;&gt;JamesDSP&lt;/a&gt; is an alternative to Viper4Android. It provides a system wide solution, has a convolution engine but requires rooting.&lt;/p&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;h4&gt;PulseEffects / EasyEffects&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wwmm/easyeffects&#34;&gt;PulseEffects / EasyEffects&lt;/a&gt; is a Linux module with wide variety of signal processing tools including convolution and parametric equalizers.&lt;/p&gt; &#xA;&lt;p&gt;From version 4.7.2 onwards PulseEffects added support for convolution FIR filters. This is the recommended way to apply AutoEq presets. Navigate to the plugins tab and add the convolver plugin, then click the waveform button above the stereo width controls (or just the &#39;Impulses&#39; button as of 6.1.x), click &#34;Import impulse&#34; and select the AutoEq generated WAV file. You may also need to manually click &#39;load&#39; in the Impulses menu for the filter to be fully loaded. PulseEffects&#39; convolver requires you to set the input gain to prevent clipping. The gain required by parametric eq should be sufficient, maybe 0.5 dB of negative gain more.&lt;/p&gt; &#xA;&lt;p&gt;To use parametric eq, from version 6.0.0 onwards, first select the &lt;code&gt;plugins&lt;/code&gt; tab at the bottom of the screen, add the equalizer plugin, and load APO settings by clicking &#34;Load APO Preset&#34; and selecting the ParametricEQ.txt file. For EasyEffects &amp;lt;= 6.1.3, Pre-amp can be adjusted with the input slider. Later versions support reading this from ParametricEQ.txt.&lt;/p&gt; &#xA;&lt;p&gt;From version 5.0.0 onwards, PulseEffects was renamed to EasyEffects and uses PipeWire instead of PulseAudio as backend. Load eq settings by clicking the top center cog &amp;amp; clicking &lt;em&gt;Import ACO Presets&lt;/em&gt; button and select the ParametricEQ.txt file. Pre-amp can be adjusted with the input slider.&lt;/p&gt; &#xA;&lt;p&gt;For versions prior to v4.8.0, adjust filter parameters by clicking the cog button on each filter and set type to &#34;Bell&#34;, mode to &#34;APO&#34; and adjust the gain with the slider. Number of filters can be changed by clicking the screwdriver and wrench button.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/32952512/112381638-6cd3b280-8d08-11eb-844a-b83600c6c02a.png&#34; alt=&#34;pulseeffects&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;OSX / MacOS&lt;/h3&gt; &#xA;&lt;p&gt;System wide parametric EQ solutions on OSX typically rely on separate plugin hosting software and the actual plugin which does the actual equalization.&lt;/p&gt; &#xA;&lt;p&gt;Pardon the lack of documentation for these. I have not tested any of the methods myself but they have been suggested by helpful AutoEQ users.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://rogueamoeba.com/soundsource/&#34;&gt;SoundSource&lt;/a&gt; is the easiest way to use AutoEq on Mac since it comes with all of the profiles built in. The software is however not free.&lt;/p&gt; &#xA;&lt;p&gt;Audio plugin hosts include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apple&#39;s own &lt;a href=&#34;https://www.apple.com/apple-music/apple-digital-masters/&#34;&gt;AU Lab&lt;/a&gt; hosts AU plugins and can be used as a system-wide audio output via &lt;a href=&#34;https://github.com/ExistentialAudio/BlackHole&#34;&gt;BlackHole&lt;/a&gt; or &lt;a href=&#34;https://github.com/mattingalls/Soundflower&#34;&gt;Soundflower&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.menubus.audio/versions&#34;&gt;MenuBus&lt;/a&gt; has a free version but is no longer actively developed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ju-x.com/hostingau.html&#34;&gt;Hosting AU&lt;/a&gt; with &lt;a href=&#34;https://github.com/ExistentialAudio/BlackHole&#34;&gt;BlackHole&lt;/a&gt; or &lt;a href=&#34;https://github.com/mattingalls/Soundflower&#34;&gt;Soundflower&lt;/a&gt; can be used as a system wide AU plugin host.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;EQ plugins include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.voxengo.com/product/primeeq/&#34;&gt;Voxengo PrimeEQ&lt;/a&gt; is a parametric EQ plugin but is not free.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.fabfilter.com/products/pro-q-3-equalizer-plug-in&#34;&gt;Fabfilter Pro Q3&lt;/a&gt; is another parametric EQ plugin, more expensive than Voxengo but might be easier to install and use. Note: Pro Q3 uses a different system and all Q values need to be multiplied by 1.41!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://audio.lernvall.com/&#34;&gt;LAConvolver plugin&lt;/a&gt; is a free convolver EQ which works with impulse response WAV files.&lt;/li&gt; &#xA; &lt;li&gt;AUNBandEq comes built in with Mac OSX. Works at least with HostingAU + BlackHole&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/38220377/71527191-9706ac80-28da-11ea-8f70-88caf57c4821.png&#34; alt=&#34;hostingau+blackhole&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.superbestaudiofriends.org/index.php?threads/systemwide-eq-on-mac.7435/&#34;&gt;Apple AU Lab + Soundflower + AUNBandEQ Tutorial&lt;/a&gt; &lt;a href=&#34;https://discussions.apple.com/thread/8552731&#34;&gt;AU Lab Permission Issue&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;eqMac&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://eqmac.app&#34;&gt;eqMac&lt;/a&gt; is a Free &amp;amp; &lt;a href=&#34;https://github.com/bitgapp/eqmac&#34;&gt;Open Source&lt;/a&gt; System Wide equalizer for macOS. eqMac has a Free 10 Band EQ and an Unlimited Band EQ (paid) with built-in AutoEQ Integration! (Expert EQ)&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;512&#34; src=&#34;https://raw.githubusercontent.com/bitgapp/eqMac/master/assets/screenshots/autoeq-promo.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;iOS&lt;/h3&gt; &#xA;&lt;p&gt;iOS unfortunately doesn&#39;t allow system-wide equalizers, so the only options are either music players with built-in equalizer or &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/#Hardware&#34;&gt;hardware solutions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Neutron&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://apps.apple.com/app/neutron-music-player/id766858884&#34;&gt;Neutron&lt;/a&gt; is a music player with parametric equalizer and comes with all of the AutoEq profiles built in but is not free.&lt;/p&gt; &#xA;&lt;h4&gt;EQE&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rweichler/EQE&#34;&gt;EQE&lt;/a&gt; is a system wide parametric equalizer on iOS but requires jailbreaking. Here are instructions on how to set it up: &lt;a href=&#34;https://www.reddit.com/r/headphones/comments/dqbt81/psa_if_you_have_a_jailbroken_iphone_you_can/&#34;&gt;https://www.reddit.com/r/headphones/comments/dqbt81/psa_if_you_have_a_jailbroken_iphone_you_can/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Hardware&lt;/h3&gt; &#xA;&lt;p&gt;Some devices have built-in equalizers and since they do the processing in the device, they work with any source which can connect to the device.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.qudelix.com/products/qudelix-5k-dac-amp&#34;&gt;Qudelix 5K&lt;/a&gt; is a portable DAC and amplifier with wired and Bluetooth connectivity and 10 band parametric equalizer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.radsone.com/earstudio&#34;&gt;Radsone EasStudio ES100&lt;/a&gt; is a Bluetooth DAC and amp with built-in 10 band equalizer. Since this is a hardware solution it will work with practically any source.&lt;/p&gt; &#xA;&lt;h2&gt;Equalizing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;autoeq.py&lt;/code&gt; is the tool used to produce the equalization results from measurement data. There is no fancy graphical user interface but instead it is used from command line.&lt;/p&gt; &#xA;&lt;h3&gt;Installing&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download and install Git: &lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;https://git-scm.com/downloads&lt;/a&gt;. When installing Git on Windows, use Windows SSL verification instead of Open SSL or you might run into problems when installing project dependencies.&lt;/li&gt; &#xA; &lt;li&gt;Download and install 64-bit &lt;strong&gt;&lt;a href=&#34;https://www.python.org/getit/&#34;&gt;Python 3.8&lt;/a&gt;&lt;/strong&gt;. Make sure to check &lt;em&gt;Add Python 3.8 to PATH&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;You may need to install &lt;a href=&#34;http://www.mega-nerd.com/libsndfile/&#34;&gt;libsndfile&lt;/a&gt; if you&#39;re having problems with &lt;code&gt;soundfile&lt;/code&gt; when installing &lt;code&gt;requirements.txt&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;On Linux you may need to install Python dev packages&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install python3-dev python3-pip python3-venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;On Linux you may need to install &lt;a href=&#34;https://pip.pypa.io/en/stable/installing/&#34;&gt;pip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;On Windows you may need to install &lt;a href=&#34;https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads&#34;&gt;Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017, and 2019&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open a terminal / command prompt. On Windows, search &lt;code&gt;cmd&lt;/code&gt; in the start menu.&lt;/li&gt; &#xA; &lt;li&gt;Clone AutoEq&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/jaakkopasanen/AutoEq.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go to AutoEq location&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd AutoEq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check Python version. You should see Python 3.8.x printed out. If you see for example 3.9.x, you need to install Python &lt;strong&gt;3.8&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python --version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a python virtual environment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Activate virtualenv&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# On Windows&#xA;venv\Scripts\activate.bat&#xA;# On Linux and Mac&#xA;. venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Update pip&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install -U pip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install required packages&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install -U -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Verify installation. If everything went well, you&#39;ll see the list of command line parameters AutoEq accepts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python autoeq.py --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When coming back at a later time you&#39;ll only need to activate virtual environment again&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# On Windows&#xA;cd AutoEq&#xA;venv\Scripts\activate.bat&#xA;# On Linux and Mac&#xA;cd AutoEq&#xA;. venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To learn more about virtual environments, read &lt;a href=&#34;https://docs.python.org/3.8/library/venv.html&#34;&gt;Python&#39; venv documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Updating&lt;/h4&gt; &#xA;&lt;p&gt;AutoEq is in active development and gets new measurements, results and features all the time. You can get the latest version from git&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git pull&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Dependencies may change from time to time, you can update to the latest with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install -U -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Command Line Arguments&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: autoeq.py [-h] --input_dir INPUT_DIR [--output_dir OUTPUT_DIR] [--standardize_input] [--new_only] [--compensation COMPENSATION] [--equalize] [--parametric_eq] [--fixed_band_eq] [--rockbox] [--fc FC] [--q Q]&#xA;                 [--ten_band_eq] [--max_filters MAX_FILTERS] [--convolution_eq] [--fs FS] [--bit_depth BIT_DEPTH] [--phase PHASE] [--f_res F_RES] [--bass_boost BASS_BOOST] [--iem_bass_boost IEM_BASS_BOOST] [--tilt TILT]&#xA;                 [--sound_signature SOUND_SIGNATURE] [--max_gain MAX_GAIN] [--treble_f_lower TREBLE_F_LOWER] [--treble_f_upper TREBLE_F_UPPER] [--treble_gain_k TREBLE_GAIN_K] [--show_plot]&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --input_dir INPUT_DIR&#xA;                        Path to input data directory. Will look for CSV files in the data directory and recursively in sub-directories.&#xA;  --output_dir OUTPUT_DIR&#xA;                        Path to results directory. Will keep the same relative paths for files found in input_dir.&#xA;  --standardize_input   Overwrite input data in standardized sampling and bias?&#xA;  --new_only            Only process input files which don&#39;t have results in output directory.&#xA;  --compensation COMPENSATION&#xA;                        File path to CSV containing compensation (target) curve. Compensation is necessary when equalizing because all input data is raw microphone data. See &#34;compensation&#34;, &#34;innerfidelity/resources&#34; and&#xA;                        &#34;headphonecom/resources&#34;.&#xA;  --equalize            Will run equalization if this parameter exists, no value needed.&#xA;  --parametric_eq       Will produce parametric eq settings if this parameter exists, no value needed.&#xA;  --fixed_band_eq       Will produce fixed band eq settings if this parameter exists, no value needed.&#xA;  --rockbox             Will produce a Rockbox .cfg file with 10 band eq settings if this parameter exists,no value needed.&#xA;  --fc FC               Comma separated list of center frequencies for fixed band eq.&#xA;  --q Q                 Comma separated list of Q values for fixed band eq. If only one value is passed it is used for all bands. Q value can be calculated from bandwidth in N octaves by Q = 2^(N/2)/(2^N-1).&#xA;  --ten_band_eq         Shortcut parameter for activating standard ten band eq optimization.&#xA;  --max_filters MAX_FILTERS&#xA;                        Maximum number of filters for parametric EQ. Multiple cumulative optimization runs can be done by giving multiple filter counts separated by &#34;+&#34;. &#34;5+5&#34; would create 10 filters where the first 5 are&#xA;                        usable independently from the rest 5 and the last 5 can only be used with the first 5. This allows to have muliple configurations for equalizers with different number of bands available. Not limited&#xA;                        by default.&#xA;  --convolution_eq      Will produce impulse response for convolution equalizers if this parameter exists, no value needed.&#xA;  --fs FS               Sampling frequency in Hertz for impulse response and parametric eq filters. Single value or multiple values separated by commas eg 44100,48000. When multiple values are given only the first one will&#xA;                        be used for parametric eq. Defaults to 44100.&#xA;  --bit_depth BIT_DEPTH&#xA;                        Number of bits for every sample in impulse response. Defaults to 16.&#xA;  --phase PHASE         Impulse response phase characteristic. &#34;minimum&#34;, &#34;linear&#34; or &#34;both&#34;. Defaults to &#34;minimum&#34;&#xA;  --f_res F_RES         Frequency resolution for impulse responses. If this is 20 then impulse response frequency domain will be sampled every 20 Hz. Filter length for impulse responses will be fs/f_res. Defaults to 10.&#xA;  --bass_boost BASS_BOOST&#xA;                        Bass boost shelf. Sub-bass frequencies will be boosted by this amount. Can be either a single value for a gain in dB or a comma separated list of three values for parameters of a low shelf filter,&#xA;                        where the first is gain in dB, second is center frequency (Fc) in Hz and the last is quality (Q). When only a single value (gain) is given, default values for Fc and Q are used which are 105.0 Hz and&#xA;                        0.71, respectively. For example &#34;--bass_boost=6&#34; or &#34;--bass_boost=9.5,150,0.69&#34;.&#xA;  --iem_bass_boost IEM_BASS_BOOST&#xA;                        iem_bass_boost argument has been removed, use &#34;--bass_boost&#34; instead!&#xA;  --tilt TILT           Target tilt in dB/octave. Positive value (upwards slope) will result in brighter frequency response and negative value (downwards slope) will result in darker frequency response. 1 dB/octave will&#xA;                        produce nearly 10 dB difference in desired value between 20 Hz and 20 kHz. Tilt is applied with bass boost and both will affect the bass gain.&#xA;  --sound_signature SOUND_SIGNATURE&#xA;                        File path to a sound signature CSV file. Sound signature is added to the compensation curve. Error data will be used as the sound signature target if the CSV file contains an error column and&#xA;                        otherwise the raw column will be used. This means there are two different options for using sound signature: 1st is pointing it to a result CSV file of a previous run and the 2nd is to create a CSV&#xA;                        file with just frequency and raw columns by hand (or other means). The Sound signature graph will be interpolated so any number of point at any frequencies will do, making it easy to create simple&#xA;                        signatures with as little as two or three points.&#xA;  --max_gain MAX_GAIN   Maximum positive gain in equalization. Higher max gain allows to equalize deeper dips in frequency response but will limit output volume if no analog gain is available because positive gain requires&#xA;                        negative digital preamp equal to maximum positive gain. Defaults to 6.0.&#xA;  --treble_f_lower TREBLE_F_LOWER&#xA;                        Lower bound for transition region between normal and treble frequencies. Treble frequencies can have different max gain and gain K. Defaults to 6000.0.&#xA;  --treble_f_upper TREBLE_F_UPPER&#xA;                        Upper bound for transition region between normal and treble frequencies. Treble frequencies can have different max gain and gain K. Defaults to 8000.0.&#xA;  --treble_gain_k TREBLE_GAIN_K&#xA;                        Coefficient for treble gain, affects both positive and negative gain. Useful for disabling or reducing equalization power in treble region. Defaults to 1.0.&#xA;  --show_plot           Plot will be shown if this parameter exists, no value needed.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;h4&gt;Reproducing Results&lt;/h4&gt; &#xA;&lt;p&gt;Reproducing pre-computed results for oratory1990 measured on-ear headphones:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python autoeq.py --input_dir=&#34;measurements/oratory1990/data/onear&#34; --output_dir=&#34;my_results/oratory1990/harman_over-ear_2018&#34; --compensation=&#34;compensation/harman_over-ear_2018_wo_bass.csv&#34; --equalize --parametric_eq --max_filters=5+5 --ten_band_eq --bass_boost=4.0 --convolution_eq --fs=44100,48000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Reproducing pre-computed results for Rtings measured IEMs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python autoeq.py --input_dir=&#34;measurements/rtings/data/inear&#34; --output_dir=&#34;my_results/rtings/avg&#34; --compensation=&#34;measurements/rtings/resources/rtings_compensation_avg.csv&#34; --equalize --parametric_eq --max_filters=5+5 --ten_band_eq --bass_boost=6.0 --convolution_eq --fs=44100,48000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All parameters used for pre-computed results can be found in the &lt;code&gt;results/update.py&lt;/code&gt; script.&lt;/p&gt; &#xA;&lt;h4&gt;Equalizing Individual Headphones&lt;/h4&gt; &#xA;&lt;p&gt;Equalizing Sennheiser HD 650 and saving results to &lt;code&gt;my_results/HD650&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python autoeq.py --input_dir=&#34;measurements/innerfidelity/data/onear/Sennheiser HD 650&#34; --output_dir=&#34;my_results/HD650&#34; --compensation=&#34;measurements/innerfidelity/resources/innerfidelity_harman_over-ear_2018_wo_bass.csv&#34; --equalize --bass_boost=4 --show_plot --convolution_eq --fs=44100,48000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Fixed Band Equalizers&lt;/h4&gt; &#xA;&lt;p&gt;Filter parameters for fixed band equalizers can be adjusted with &lt;code&gt;--q&lt;/code&gt; and &lt;code&gt;--fc&lt;/code&gt; parameters. Producing fixed band equalizer settings for Sony WH-1000XM3 app:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python autoeq.py --input_dir=&#34;measurements/oratory1990/data/onear/Sony WH-1000XM3&#34; --output_dir=&#34;my_results/Sony WH-1000XM3 (app)&#34; --compensation=&#34;compensation/harman_over-ear_2018_wo_bass.csv&#34; --equalize --bass_boost=4.0 --fixed_band_eq --fc=400,1000,2500,6300,16000 --q=1.05&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using Sound Signatures&lt;/h4&gt; &#xA;&lt;p&gt;AutoEQ provides a way to play around with different sound signatures easily. The use-cases include making headphones deviate from the neutral target or making one headphone sound like another.&lt;/p&gt; &#xA;&lt;p&gt;Equalizing Sennheiser HD 800 to sound like Sennheiser HD 650 using pre-computed results. Both have been measured by oratory1990 so we&#39;ll use those measurements. Pre-computed results include 4dB of bass boost for over-ear headphones and therefore we need to apply a bass boost of 4dB here as well.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python autoeq.py --input_dir=&#34;measurements/oratory1990/data/onear/Sennheiser HD 800&#34; --output_dir=&#34;my_results/Sennheiser HD 800 (HD 650)&#34; --compensation=&#34;compensation/harman_over-ear_2018_wo_bass.csv&#34; --sound_signature=&#34;results/oratory1990/harman_over-ear_2018/Sennheiser HD 650/Sennheiser HD 650.csv&#34; --equalize --parametric_eq --max_filters=5+5 --ten_band_eq --bass_boost=4 --convolution_eq --fs=44100,48000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Equalizing Massdrop x Sennheiser HD 800 to sound like AKG K701. There is no K701 measurement made by oratory1990 so we&#39;ll use Innerfidelity&#39;s measurement for the sound signature. The list of recommended results always points to best measurement so you can check there which one to use (measurement system can be found in the URL).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python autoeq.py --input_dir=&#34;measurements/oratory1990/data/onear/Sennheiser HD 800&#34; --output_dir=&#34;my_results/Sennheiser HD 800 (K701)&#34; --compensation=&#34;compensation/harman_over-ear_2018_wo_bass.csv&#34; --sound_signature=&#34;results/innerfidelity/innerfidelity_harman_over-ear_2018/AKG K701/AKG K701.csv&#34; --equalize --parametric_eq --max_filters=5+5 --ten_band_eq --bass_boost=4 --convolution_eq --fs=44100,48000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Equalizing HiFiMAN HE400S to sound like Massdrop x Meze 99 Noir. HE400S is measured only by Innerfidelity so we&#39;ll point compensation file pointing to Innerfidelity&#39;s calibrated Harman target. Meze 99 Noir has massive natural bass boost and to capture that we need to relax max gain to +12dB.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python autoeq.py --input_dir=&#34;measurements/innerfidelity/data/onear/HiFiMAN HE400S&#34; --output_dir=&#34;my_results/HE400S (99 Noir)&#34; --compensation=&#34;measurements/innerfidelity/resources/innerfidelity_harman_over-ear_2018_wo_bass.csv&#34; --sound_signature=&#34;results/oratory1990/harman_over-ear_2018/Meze 99 Noir/Meze 99 Noir.csv&#34; --equalize --parametric_eq --max_filters=5+5 --ten_band_eq --bass_boost=4 --max_gain=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Applying V-shaped sound signature to Audeze Mobius. First step is to create the sound signature file. Save this to &lt;code&gt;my_data/v.csv&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-csv&#34;&gt;frequency,raw&#xA;20,4.0&#xA;1000,-4.0&#xA;10000,4.0&#xA;20000,0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then use it by providing the path to &lt;code&gt;--sound_signature&lt;/code&gt; parameter. We&#39;ll set bass boost to 0dB because the sound signature already has a significant bass boost. Of course it&#39;s possible to add bass boost on top of the sound signature file if you want even more bass.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python autoeq.py --input_dir=&#34;measurements/rtings/data/onear/Audeze Mobius&#34; --output_dir=&#34;my_results/Audeze Mobius (V-signature)&#34; --compensation=&#34;measurements/rtings/resources/rtings_compensation_avg.csv&#34; --sound_signature=&#34;my_data/v.csv&#34; --equalize --parametric_eq --max_filters=5+5 --ten_band_eq --bass_boost=4.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;The main principle used by AutoEQ for producing the equalization function is to invert the error curve. Error is the difference between raw microphone data and the compensation (target) curve. If headphone&#39;s frequency response is 4 dB below the target at 20 Hz equalization function will have +4 dB boost at 20 Hz. In reality simply inverting the error is not sufficient since measurements and equalization have several problems that need to be addressed, see &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/#technical-challenges&#34;&gt;Technical Challenges&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;Results provided in this project currently have all the headphone measurements from&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://crinacle.com/&#34;&gt;Crinacle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://graphs.headphone.com/&#34;&gt;Headphone.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.innerfidelity.com/headphone-measurements&#34;&gt;Innerfidelity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/oratory1990&#34;&gt;oratory1990&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://reference-audio-analyzer.pro/en/catalog-reports.php?sp_1=1&amp;amp;tp=1&#34;&gt;Reference Audio Analyzer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rtings.com/headphones&#34;&gt;Rtings&lt;/a&gt; with the exception of Reference Audio Analyzer measurements done on the SF1 system.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Results are organized by &lt;code&gt;source/target/headphone&lt;/code&gt; so a Sennheiser HD 650 measured by Innerfidelity and tuned to a calibrated Harman target would be found in &lt;a href=&#34;https://raw.githubusercontent.com/jaakkopasanen/AutoEq/master/results/innerfidelity/innerfidelity_harman_over-ear_2018/Sennheiser%20HD%20650&#34;&gt;innerfidelity/innerfidelity_harman_over-ear_2018/Sennheiser HD 650&lt;/a&gt;. Multiple measurements of the same headphone by the same measurement entity are averaged. All different measurements for averaging have been renamed with snXXX (serial number) or sample X in the end of the name to distinguish from the averaged data which has no suffixes in the name.&lt;/p&gt; &#xA;&lt;p&gt;oratory1990 measurements have been done on Gras 43AG and 43AC couplers, the same which were used to develop Harman target responses by Olive et al. and therefore use Harman target responses for the equalization targets. These results are recommended over all other measurements because of this reason. Harman target data is in the &lt;code&gt;compensation&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;Crinacle&#39;s in-ear measurements have been performed with IEC 60318-4 coupler and are therefore compatible with Harman in-ear targets. This fact also earns Crinacle&#39;s measurements second highest ranking recommendation after oratory1990. Crinacle&#39;s over-ear measurements use the same ear simulator attached to a MiniDSP ears pinna. The measurements done on this system are not as accurate as oratory1990&#39;s but because of the high quality ear simulator, these are a bit better than rest.&lt;/p&gt; &#xA;&lt;p&gt;Innerfidelity, Rtings and Headphone.com measurements have been performed on Head Acoustics HMSII.3 measurement system. This system is not an industry standard anymore because of the rigid pinnae. The Headphone.com measurements are the old ones which are no longer available. These are not to be consfused with the new measurements Resolve is producing using GRAS system.&lt;/p&gt; &#xA;&lt;p&gt;Reference Audio Analyzer have &lt;a href=&#34;https://reference-audio-analyzer.pro/en/stands.php&#34;&gt;three different measurement systems&lt;/a&gt; none of which seem to represent human hearing particularly well. The most recent HDM-X system is close to the Head Acoustics HMSII.3 systems but seems to suffer a bit more in the bass range. HDM1 is clearly worse than other systems and the measurements done on the SF1 system are not included at all because that is a flat plate coupler. IEM measurements are done with what looks like a tubing coupler and these don&#39;t look very accurate. Reference Audio Analyzer measurements and results are a last resort.&lt;/p&gt; &#xA;&lt;p&gt;All of the results use frequency response targets that were specifically developed for this project except oratory1990 and Crinacle&#39;s IEM measurements which use standard Harman targets. The target curves were developed by calibrating measurements against reference measurements by oratory1990 and Crinacle (IEMs) and modifying the Harman 2018 over-ear and 2019 in-ear targets with the calibration data.&lt;/p&gt; &#xA;&lt;p&gt;None of these targets have bass boost seen in Harman target responses and therefore a +4dB boost was applied for all over-ear headphones, +6dB for in-ear headphones and no boost for earbuds. Harman targets actually ask for about +6dB for over-ears and +9dB for in-ears but since some headphones cannot achieve this with positive gain limited to +6dB, a smaller boost was selected. Above 6 to 12 kHz data is filtered more heavily to avoid equalizing the narrow dips and notches that depend heavily on the listener&#39;s own ears.&lt;/p&gt; &#xA;&lt;h3&gt;oratory1990 IEM Target&lt;/h3&gt; &#xA;&lt;p&gt;In-ear results with oratory1990 target (formerly &#34;Usound&#34; target) are not longer given because the new 2019 Harman in-ear fixes the +10 kHz problems of the 2017 target. Also it is easy to transform results created for Harman 2019 to oratory1990 target without running the processing yourself if you are using parametric equalizer and have two filters (bands) available by adding these two to your eq software:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Fc&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Q&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Gain&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;113&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.75&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Peaking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3766&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.63&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-2.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The results will be remarkably similar to results produced with the actual oratory1990 target:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/kGYBOev.png&#34; alt=&#34;oratory1990 vs Harman in-ear 2019&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Of course it&#39;s still possible to produce native results with oratory1990 target by pointing compensation to the oratory1990 target file: &lt;code&gt;--compensation=&#34;compensation/oratory1990.csv&lt;/code&gt; or &lt;code&gt;--compensation=&#34;compensation/oratory1990_wo_bass.csv&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Innerfidelity Target by Super Best Audio Friends Forum User &#34;Serious&#34;&lt;/h3&gt; &#xA;&lt;p&gt;Innerfidelity and Headphone.com measured headphones previously used &lt;a href=&#34;https://www.superbestaudiofriends.org/index.php?threads/innerfidelity-fr-target.5560/&#34;&gt;SBAF-Serious target&lt;/a&gt; only. The SBAF-Serious curve is no longer used for these measurements since a new targets were developed by calibrating Harman targets. This is a modified version of Innerfidelity target curve produced by a user called Serious on Super Best Audio Friends forum. This curve doesn&#39;t have any glaring problems and is quite well balanced overall. Curve was turned into a compensation for raw microphone data and tilted 0.2 dB / octave brighter. Innerfidelity measurements are recommended over Headphone.com measurements because SBAF-Serious target was developed for Innerfidelity. SBAF-Serious curve was modified to be suitable for Headphone.com measurements. CSV data files for Innerfidelity and Headphone.com are at &lt;code&gt;innerfidelity/resources/innerfidelity_compensation_sbaf-serious.csv&lt;/code&gt; and &lt;code&gt;headphonecom/resources/headphonecom_compensation_sbaf-serious.csv&lt;/code&gt;, respectively.&lt;/p&gt; &#xA;&lt;h3&gt;Rtings Targets&lt;/h3&gt; &#xA;&lt;p&gt;Rtings measured headphones have a frequency response target made for this project. This treble average target is using an average of frequency responses of all Rtings measured headphones in the treble range with small manual reduction of the 9kHz peak and the Rtings native response below 2500 Hz without bass boost. Three different targets were compared in listening tests and the treble average target was found to sound the best. Other two were the Rtings native target curve and calibrated and uncalibrated versions of SBAF Serious target curve. Rtings uses the same measurement system as Innerfidelity uses so in theory the uncalibrated SBAF Serious target should work similarly with Rtings but listening tests found the treble average target to be slightly better. Rtings have &lt;a href=&#34;https://www.youtube.com/watch?v=HNEI3qLZEKo&#34;&gt;a very informative video&lt;/a&gt; about how they are doing the measurements and how they came up with the target they use.&lt;/p&gt; &#xA;&lt;p&gt;All of these Rtings targets retired when new calibrated Harman targets were developed for Rtings measurements.&lt;/p&gt; &#xA;&lt;h2&gt;Technical Challenges&lt;/h2&gt; &#xA;&lt;p&gt;Simply inverting headphone frequency response deviation from target response does not usually produce sufficient results. Some problems are caused by imperfections in measurements, some are reliability issues and some are practical end-user problems. Rtings has a good &lt;a href=&#34;https://www.youtube.com/watch?v=HNEI3qLZEKo&#34;&gt;video on Youtube&lt;/a&gt; about measurement system challenges and solutions which is definitely worth checking out. Innerfidelity also has a very educational &lt;a href=&#34;https://www.youtube.com/watch?v=SDRHFNfFCFU&#34;&gt;video on Youtube&lt;/a&gt; about measurements and what constitutes as a neutral sound. Main takeoffs are that bass and treble measurements are very inconsistent, neutral sound is not very well defined yet and on-ear headphones have big reliability problems in 8 to 9kHz range due to resonances which move when headphone placement is changed. Harman international has done some solid research into preferred headphone frequency response but since that research was done on a different measurement system the target does not apply directly to Innerfidelity (Summer 2018) and Headphone.com measurements.&lt;/p&gt; &#xA;&lt;p&gt;There is very little that can be done for fighting bass inconsistencies because the same problems will be there whether equalization is used or not. Headphones simply have different bass responses on different listeners (heads). Therefore bass is taken as is in AutoEQ and equalized as if there was nothing wrong with it. Your mileage may vary. Luckily bass has smaller impact on music and having too much bass (especially sub-bass) doesn&#39;t create problems of the same magnitude as having too much treble.&lt;/p&gt; &#xA;&lt;p&gt;Moving resonances around 8 to 9kHz may cause big problems if not taken into account. Spikes and dips in this range are of great amplitude and very narrow. If one equalizes these spikes and dips according to frequency response measurement in worst case scenario a spike will move in a place of dip when headphone is moved, and therefore the spike is amplified significantly, leading to a very sharp and piercing sound signature. To counter these problems by default AutoEQ uses heavy smoothing and limited positive gain above 6 to 8kHz. This way the equalization will follow a broader trend of the region and will not care so much about narrow spikes and dips. Also positive gain is limited to 0dB as an extra safety measure against amplifying spikes due to moving the headphone. Suppressing a narrow dip even further is not an optimal thing to do but in practice has little negative effect on the sound. Both of these measures will also alleviate upper treble measurement inconsistencies above 11 to 12 kHz.&lt;/p&gt; &#xA;&lt;p&gt;A practical end-user problem is if too high positive gain is allowed which asks for equal amount of negative digital pre-amp to prevent clipping. This negative preamp will limit maximum volume produced by the system if there is no analog gain available. If a dedicated headphone amplifier is available or if the motherboard/soundcard can drive the headphones loud enough even when using high negative preamp larger &lt;code&gt;--max_gain&lt;/code&gt; values can be used. By default &lt;code&gt;--max_gain&lt;/code&gt; is set to +6dB so as not to cripple the user&#39;s volume too much. Max gain will clip the equalization curve which produces sharp kinks in it. Sharp changes in equalization may produce unwanted equalization artifacts. To counter this AutoEQ rounds the corners whenever max gain clips the curve.&lt;/p&gt; &#xA;&lt;h2&gt;Parametric Equalizer Optimization&lt;/h2&gt; &#xA;&lt;p&gt;AutoEQ has an optimizer to fit several peaking filters to the desired equalization curve. Optimization is part heuristic initialization and part mathematical optimization.&lt;/p&gt; &#xA;&lt;p&gt;In the initialization phase peaks are detected from the target curve and a peaking filter is created to match the peak&#39;s height (gain) and location (frequency). This way, the optimizer finds a suitable number of filters to optimize. If the bass region has no peaks and therefore is missing filters entirely, a maximum of two filters will be added at 20 Hz and 60 Hz.&lt;/p&gt; &#xA;&lt;p&gt;A way to limit the number of filters used is provided with &lt;code&gt;max_filters&lt;/code&gt; parameter. If there are too many filters after initialization, some filters are removed. First filters with small gain (&amp;lt; 0.2 dB and &amp;lt; 0.33 dB) are removed. If there are too many filters after reduction of small gain filters, nearby filters are attempted to merge. Merged filter will be in the mid point of the merged filters. If merging filters did not reduce the count enough, smallest filters are removed until count matches maximum allowed number of filters. Image below shows initialization for 1More MK801 headphone. Red dots are the peaks of filters before reduction and green dots are the peaks after reduction.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/UlMb2jK.png&#34; alt=&#34;filter-initialization&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Equalization target and initial peak filters for optimization before and after filter number limitation&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;After suitable number of filters have been achieved and filter center frequencies and gains have been set to appropriate values a mathematical optimization is performed to fit sum frequency response of all filters to match as close as possible the desired curve. Optimization is based on gradient descent and will attempt to minimize mean squared error between the sum frequency response of the filters and the target. When improvements in the error are getting too small to make a practical difference the optimization is stopped. Animation below shows progress from the initialization to a close finished curve.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/pM7JYAb.gif&#34; alt=&#34;optimization-animation&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Optimization of parametric eq filters (click to play)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jaakkopasanen/AutoEq/issues&#34;&gt;Issues&lt;/a&gt; are the way to go if you are experiencing problems, have ideas or if there is something unclear about how things are done or documented.&lt;/p&gt; &#xA;&lt;p&gt;You can find me in &lt;a href=&#34;https://www.reddit.com/user/jaakkopasanen&#34;&gt;Reddit&lt;/a&gt; and &lt;a href=&#34;https://www.head-fi.org/members/jaakkopasanen.491235/&#34;&gt;Head-fi&lt;/a&gt; if you just want to say hello.&lt;/p&gt;</summary>
  </entry>
</feed>