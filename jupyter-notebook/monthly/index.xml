<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-01T02:12:49Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>facebookresearch/co-tracker</title>
    <updated>2023-09-01T02:12:49Z</updated>
    <id>tag:github.com,2023-09-01:/facebookresearch/co-tracker</id>
    <link href="https://github.com/facebookresearch/co-tracker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CoTracker is a model for tracking any point (pixel) on a video.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CoTracker: It is Better to Track Together&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ai.facebook.com/research/&#34;&gt;Meta AI Research, GenAI&lt;/a&gt;&lt;/strong&gt;; &lt;strong&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/&#34;&gt;University of Oxford, VGG&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nikitakaraevv.github.io/&#34;&gt;Nikita Karaev&lt;/a&gt;, &lt;a href=&#34;https://www.irocco.info/&#34;&gt;Ignacio Rocco&lt;/a&gt;, &lt;a href=&#34;https://ai.facebook.com/people/benjamin-graham/&#34;&gt;Benjamin Graham&lt;/a&gt;, &lt;a href=&#34;https://nneverova.github.io/&#34;&gt;Natalia Neverova&lt;/a&gt;, &lt;a href=&#34;https://www.robots.ox.ac.uk/~vedaldi/&#34;&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href=&#34;https://chrirupp.github.io/&#34;&gt;Christian Rupprecht&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2307.07635&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://co-tracker.github.io/&#34;&gt;&lt;code&gt;Project&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/#citing-cotracker&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/facebookresearch/co-tracker/blob/main/notebooks/demo.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://huggingface.co/spaces/facebook/cotracker&#34;&gt; &lt;img alt=&#34;Spaces&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt; &lt;/a&gt; &#xA;&lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/assets/bmx-bumps.gif&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoTracker&lt;/strong&gt; is a fast transformer-based model that can track any point in a video. It brings to tracking some of the benefits of Optical Flow.&lt;/p&gt; &#xA;&lt;p&gt;CoTracker can track:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Every pixel&lt;/strong&gt; in a video&lt;/li&gt; &#xA; &lt;li&gt;Points sampled on a regular grid on any video frame&lt;/li&gt; &#xA; &lt;li&gt;Manually selected points&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Try these tracking modes for yourself with our &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/co-tracker/blob/master/notebooks/demo.ipynb&#34;&gt;Colab demo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation Instructions&lt;/h2&gt; &#xA;&lt;p&gt;Ensure you have both PyTorch and TorchVision installed on your system. Follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; for the installation. We strongly recommend installing both PyTorch and TorchVision with CUDA support.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained models via PyTorch Hub&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to use CoTracker is to load a pretrained model from torch.hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install einops timm tqdm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;import timm&#xA;import einops&#xA;import tqdm&#xA;&#xA;cotracker = torch.hub.load(&#34;facebookresearch/co-tracker&#34;, &#34;cotracker_w8&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Another option is to install it from this gihub repo. That&#39;s the best way if you need to run our demo or evaluate / train CoTracker:&lt;/p&gt; &#xA;&lt;h3&gt;Steps to Install CoTracker and its dependencies:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/facebookresearch/co-tracker&#xA;cd co-tracker&#xA;pip install -e .&#xA;pip install opencv-python einops timm matplotlib moviepy flow_vis &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download Model Weights:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir checkpoints&#xA;cd checkpoints&#xA;wget https://dl.fbaipublicfiles.com/cotracker/cotracker_stride_4_wind_8.pth&#xA;wget https://dl.fbaipublicfiles.com/cotracker/cotracker_stride_4_wind_12.pth&#xA;wget https://dl.fbaipublicfiles.com/cotracker/cotracker_stride_8_wind_16.pth&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running the Demo:&lt;/h2&gt; &#xA;&lt;p&gt;Try our &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/co-tracker/blob/master/notebooks/demo.ipynb&#34;&gt;Colab demo&lt;/a&gt; or run a local demo with 10*10 points sampled on a grid on the first frame of a video:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo.py --grid_size 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To reproduce the results presented in the paper, download the following datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepmind/tapnet&#34;&gt;TAP-Vid&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/benjiebob/BADJA&#34;&gt;BADJA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.11898&#34;&gt;ZJU-Mocap (FastCapture)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And install the necessary dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install hydra-core==1.1.0 mediapy &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, execute the following command to evaluate on BADJA:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python ./cotracker/evaluation/evaluate.py --config-name eval_badja exp_dir=./eval_outputs dataset_root=your/badja/path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, evaluation will be slow since it is done for one target point at a time, which ensures robustness and fairness, as described in the paper.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To train the CoTracker as described in our paper, you first need to generate annotations for &lt;a href=&#34;https://github.com/google-research/kubric&#34;&gt;Google Kubric&lt;/a&gt; MOVI-f dataset. Instructions for annotation generation can be found &lt;a href=&#34;https://github.com/deepmind/tapnet&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once you have the annotated dataset, you need to make sure you followed the steps for evaluation setup and install the training dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install pytorch_lightning==1.6.0 tensorboard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can launch training on Kubric. Our model was trained for 50000 iterations on 32 GPUs (4 nodes with 8 GPUs). Modify &lt;em&gt;dataset_root&lt;/em&gt; and &lt;em&gt;ckpt_path&lt;/em&gt; accordingly before running this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train.py --batch_size 1 --num_workers 28 \&#xA;--num_steps 50000 --ckpt_path ./ --dataset_root ./datasets --model_name cotracker \&#xA;--save_freq 200 --sequence_len 24 --eval_datasets tapvid_davis_first badja \&#xA;--traj_per_sample 256 --sliding_window_len 8 --updateformer_space_depth 6 --updateformer_time_depth 6 \&#xA;--save_every_n_epoch 10 --evaluate_every_n_epoch 10 --model_stride 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The majority of CoTracker is licensed under CC-BY-NC, however portions of the project are available under separate license terms: Particle Video Revisited is licensed under the MIT license, TAP-Vid is licensed under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank &lt;a href=&#34;https://github.com/aharley/pips&#34;&gt;PIPs&lt;/a&gt; and &lt;a href=&#34;https://github.com/deepmind/tapnet&#34;&gt;TAP-Vid&lt;/a&gt; for publicly releasing their code and data. We also want to thank &lt;a href=&#34;https://lukemelas.github.io/&#34;&gt;Luke Melas-Kyriazi&lt;/a&gt; for proofreading the paper, &lt;a href=&#34;https://jytime.github.io/&#34;&gt;Jianyuan Wang&lt;/a&gt;, &lt;a href=&#34;https://shapovalov.ro/&#34;&gt;Roman Shapovalov&lt;/a&gt; and &lt;a href=&#34;https://adamharley.com/&#34;&gt;Adam W. Harley&lt;/a&gt; for the insightful discussions.&lt;/p&gt; &#xA;&lt;h2&gt;Citing CoTracker&lt;/h2&gt; &#xA;&lt;p&gt;If you find our repository useful, please consider giving it a star ‚≠ê and citing our paper in your work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{karaev2023cotracker,&#xA;  title={CoTracker: It is Better to Track Together},&#xA;  author={Nikita Karaev and Ignacio Rocco and Benjamin Graham and Natalia Neverova and Andrea Vedaldi and Christian Rupprecht},&#xA;  journal={arXiv:2307.07635},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>aws-samples/amazon-bedrock-workshop</title>
    <updated>2023-09-01T02:12:49Z</updated>
    <id>tag:github.com,2023-09-01:/aws-samples/amazon-bedrock-workshop</id>
    <link href="https://github.com/aws-samples/amazon-bedrock-workshop" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is a workshop designed for Amazon Bedrock a foundational model service.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Amazon Bedrock Workshop&lt;/h1&gt; &#xA;&lt;p&gt;This hands-on workshop, aimed at developers and solution builders, introduces how to leverage foundation models (FMs) through &lt;a href=&#34;https://aws.amazon.com/bedrock/&#34;&gt;Amazon Bedrock&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Amazon Bedrock is a fully managed service that provides access to FMs from third-party providers and Amazon; available via an API. With Bedrock, you can choose from a variety of models to find the one that‚Äôs best suited for your use case.&lt;/p&gt; &#xA;&lt;p&gt;Within this series of labs, you&#39;ll explore some of the most common usage patterns we are seeing with our customers for Generative AI. We will show techniques for generating text and images, creating value for organizations by improving productivity. This is achieved by leveraging foundation models to help in composing emails, summarizing text, answering questions, building chatbots, and creating images. You will gain hands-on experience implementing these patterns via Bedrock APIs and SDKs, as well as open-source software like &lt;a href=&#34;https://python.langchain.com/docs/get_started/introduction&#34;&gt;LangChain&lt;/a&gt; and &lt;a href=&#34;https://faiss.ai/index.html&#34;&gt;FAISS&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Labs include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text Generation&lt;/strong&gt; [Estimated time to complete - 30 mins]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text Summarization&lt;/strong&gt; [Estimated time to complete - 30 mins]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Questions Answering&lt;/strong&gt; [Estimated time to complete - 45 mins]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chatbot&lt;/strong&gt; [Estimated time to complete - 45 mins]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Image Generation&lt;/strong&gt; [Estimated time to complete - 30 mins]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/imgs/10-overview.png&#34; alt=&#34;imgs/10-overview&#34; title=&#34;Overview of the different labs in the workshop&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;You can also refer to these &lt;a href=&#34;https://catalog.us-east-1.prod.workshops.aws/workshops/a4bdb007-5600-4368-81c5-ff5b4154f518/en-US&#34;&gt;Step-by-step guided instructions on the workshop website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;Choose a notebook environment&lt;/h3&gt; &#xA;&lt;p&gt;This workshop is presented as a series of &lt;strong&gt;Python notebooks&lt;/strong&gt;, which you can run from the environment of your choice:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For a fully-managed environment with rich AI/ML features, we&#39;d recommend using &lt;a href=&#34;https://aws.amazon.com/sagemaker/studio/&#34;&gt;SageMaker Studio&lt;/a&gt;. To get started quickly, you can refer to the &lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html&#34;&gt;instructions for domain quick setup&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For a fully-managed but more basic experience, you could instead &lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html&#34;&gt;create a SageMaker Notebook Instance&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you prefer to use your existing (local or other) notebook environment, make sure it has &lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html&#34;&gt;credentials for calling AWS&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Enable AWS IAM permissions for Bedrock&lt;/h3&gt; &#xA;&lt;p&gt;The AWS identity you assume from your notebook environment (which is the &lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html&#34;&gt;&lt;em&gt;Studio/notebook Execution Role&lt;/em&gt;&lt;/a&gt; from SageMaker, or could be a role or IAM User for self-managed notebooks), must have sufficient &lt;a href=&#34;https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html&#34;&gt;AWS IAM permissions&lt;/a&gt; to call the Amazon Bedrock service.&lt;/p&gt; &#xA;&lt;p&gt;To grant Bedrock access to your identity, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open the &lt;a href=&#34;https://us-east-1.console.aws.amazon.com/iam/home?#&#34;&gt;AWS IAM Console&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Find your &lt;a href=&#34;https://us-east-1.console.aws.amazon.com/iamv2/home?#/roles&#34;&gt;Role&lt;/a&gt; (if using SageMaker or otherwise assuming an IAM Role), or else &lt;a href=&#34;https://us-east-1.console.aws.amazon.com/iamv2/home?#/users&#34;&gt;User&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Select &lt;em&gt;Add Permissions &amp;gt; Create Inline Policy&lt;/em&gt; to attach new inline permissions, open the &lt;em&gt;JSON&lt;/em&gt; editor and paste in the below example policy:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;    &#34;Version&#34;: &#34;2012-10-17&#34;,&#xA;    &#34;Statement&#34;: [&#xA;        {&#xA;            &#34;Sid&#34;: &#34;BedrockFullAccess&#34;,&#xA;            &#34;Effect&#34;: &#34;Allow&#34;,&#xA;            &#34;Action&#34;: [&#34;bedrock:*&#34;],&#xA;            &#34;Resource&#34;: &#34;*&#34;&#xA;        }&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; With Amazon SageMaker, your notebook execution role will typically be &lt;em&gt;separate&lt;/em&gt; from the user or role that you log in to the AWS Console with. If you&#39;d like to explore the AWS Console for Amazon Bedrock, you&#39;ll need to grant permissions to your Console user/role too.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For more information on the fine-grained action and resource permissions in Bedrock, check out the Bedrock Developer Guide.&lt;/p&gt; &#xA;&lt;h3&gt;Clone and use the notebooks&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ÑπÔ∏è &lt;strong&gt;Note:&lt;/strong&gt; In SageMaker Studio, you can open a &#34;System Terminal&#34; to run these commands by clicking &lt;em&gt;File &amp;gt; New &amp;gt; Terminal&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Once your notebook environment is set up, clone this workshop repository into it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/aws-samples/amazon-bedrock-workshop.git&#xA;cd amazon-bedrock-workshop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Because the service is in preview, the Amazon Bedrock SDK is not yet included in standard releases of the &lt;a href=&#34;https://boto3.amazonaws.com/v1/documentation/api/latest/index.html&#34;&gt;AWS SDK for Python - boto3&lt;/a&gt;. Run the following script to download and extract custom SDK wheels for testing Bedrock:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash ./download-dependencies.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script will create a &lt;code&gt;dependencies&lt;/code&gt; folder and download the relevant SDKs, but will not &lt;code&gt;pip install&lt;/code&gt; them just yet.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;re now ready to explore the lab notebooks! Start with &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/00_Intro/bedrock_boto3_setup.ipynb&#34;&gt;00_Intro/bedrock_boto3_setup.ipynb&lt;/a&gt; for details on how to install the Bedrock SDKs, create a client, and start calling the APIs from Python.&lt;/p&gt; &#xA;&lt;h2&gt;Content&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains notebook examples for the Bedrock Architecture Patterns workshop. The notebooks are organised by module as follows:&lt;/p&gt; &#xA;&lt;h3&gt;Intro&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/00_Intro/bedrock_boto3_setup.ipynb&#34;&gt;Simple Bedrock Usage&lt;/a&gt;: This notebook shows setting up the boto3 client and some basic usage of bedrock.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/01_Generation/00_generate_w_bedrock.ipynb&#34;&gt;Simple use case with boto3&lt;/a&gt;: In this notebook, you generate text using Amazon Bedrock. We demonstrate consuming the Amazon Titan model directly with boto3&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/01_Generation/01_zero_shot_generation.ipynb&#34;&gt;Simple use case with LangChain&lt;/a&gt;: We then perform the same task but using the popular framework LangChain&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/01_Generation/02_contextual_generation.ipynb&#34;&gt;Generation with additional context&lt;/a&gt;: We then take this further by enhancing the prompt with additional context in order to improve the response.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Summarization&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/02_Summarization/01.small-text-summarization-claude.ipynb&#34;&gt;Small text summarization&lt;/a&gt;: In this notebook, you use use Bedrock to perform a simple task of summarizing a small piece of text.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/02_Summarization/02.long-text-summarization-titan.ipynb&#34;&gt;Long text summarization&lt;/a&gt;: The above approach may not work as the content to be summarized gets larger and exceeds the max tokens of the model. In this notebook we show an approach of breaking the file up into smaller chunks, summarizing each chunk, and then summarizing the summaries.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Question Answering&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/03_QuestionAnswering/00_qa_w_bedrock_titan.ipynb&#34;&gt;Simple questions with context&lt;/a&gt;: This notebook shows a simple example answering a question with given context by calling the model directly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/03_QuestionAnswering/01_qa_w_rag_claude.ipynb&#34;&gt;Answering questions with Retrieval Augmented Generation&lt;/a&gt;: We can improve the above process by implementing an architecure called Retreival Augmented Generation (RAG). RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Chatbot&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/04_Chatbot/00_Chatbot_Claude.ipynb&#34;&gt;Chatbot using Claude&lt;/a&gt;: This notebook shows a chatbot using Claude&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/04_Chatbot/00_Chatbot_Titan.ipynb&#34;&gt;Chatbot using Titan&lt;/a&gt;: This notebook shows a chatbot using Titan&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Text to Image&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/05_Image/Bedrock%20Stable%20Diffusion%20XL.ipynb&#34;&gt;Image Generation with Stable Diffusion&lt;/a&gt;: This notebook demonstrates image generation with using the Stable Diffusion model&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ageron/handson-ml3</title>
    <updated>2023-09-01T02:12:49Z</updated>
    <id>tag:github.com,2023-09-01:/ageron/handson-ml3</id>
    <link href="https://github.com/ageron/handson-ml3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Notebooks, 3rd edition&lt;/h1&gt; &#xA;&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in python. It contains the example code and solutions to the exercises in the third edition of my O&#39;Reilly book &lt;a href=&#34;https://homl.info/er3&#34;&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow (3rd edition)&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://homl.info/er3&#34;&gt;&lt;img src=&#34;https://learning.oreilly.com/library/cover/9781098125967/300w/&#34; title=&#34;book&#34; width=&#34;150&#34; border=&#34;0&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are looking for the second edition notebooks, check out &lt;a href=&#34;https://github.com/ageron/handson-ml2&#34;&gt;ageron/handson-ml2&lt;/a&gt;. For the first edition, see &lt;a href=&#34;https://github.com/ageron/handson-ml&#34;&gt;ageron/handson-ml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Want to play with these notebooks online without having to install anything?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/ageron/handson-ml3/blob/main/&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; (recommended)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;‚ö† &lt;em&gt;Colab provides a temporary environment: anything you do will be deleted after a while, so make sure you download any data you care about.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;Other services may work as well, but I have not fully tested them:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://homl.info/kaggle3/&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Open in Kaggle&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/ageron/handson-ml3/HEAD?filepath=%2Findex.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Launch binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://homl.info/deepnote3/&#34;&gt;&lt;img src=&#34;https://deepnote.com/buttons/launch-in-deepnote-small.svg?sanitize=true&#34; alt=&#34;Launch in Deepnote&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Just want to quickly look at some notebooks, without executing any code?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ageron/handson-ml3/blob/main/index.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg?sanitize=true&#34; alt=&#34;Render nbviewer&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ageron/handson-ml3/raw/main/index.ipynb&#34;&gt;github.com&#39;s notebook viewer&lt;/a&gt; also works but it&#39;s not ideal: it&#39;s slower, the math equations are not always displayed correctly, and large notebooks often fail to open.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Want to run this project using a Docker image?&lt;/h3&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://github.com/ageron/handson-ml3/tree/main/docker&#34;&gt;Docker instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Want to install this project on your own machine?&lt;/h3&gt; &#xA;&lt;p&gt;Start by installing &lt;a href=&#34;https://www.anaconda.com/products/distribution&#34;&gt;Anaconda&lt;/a&gt; (or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;), &lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;git&lt;/a&gt;, and if you have a TensorFlow-compatible GPU, install the &lt;a href=&#34;https://www.nvidia.com/Download/index.aspx&#34;&gt;GPU driver&lt;/a&gt;, as well as the appropriate version of CUDA and cuDNN (see TensorFlow&#39;s documentation for more details).&lt;/p&gt; &#xA;&lt;p&gt;Next, clone this project by opening a terminal and typing the following commands (do not type the first &lt;code&gt;$&lt;/code&gt; signs on each line, they just indicate that these are terminal commands):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml3.git&#xA;$ cd handson-ml3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml&#xA;$ conda activate homl3&#xA;$ python -m ipykernel install --user --name=python3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, start Jupyter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you need further instructions, read the &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml3/main/INSTALL.md&#34;&gt;detailed installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Which Python version should I use?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I recommend Python 3.10. If you follow the installation instructions above, that&#39;s the version you will get. Any version ‚â•3.7 should work as well.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;m getting an error when I call &lt;code&gt;load_housing_data()&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re getting an HTTP error, make sure you&#39;re running the exact same code as in the notebook (copy/paste it if needed). If the problem persists, please check your network configuration. If it&#39;s an SSL error, see the next question.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;m getting an SSL error on MacOSX&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You probably need to install the SSL certificates (see this &lt;a href=&#34;https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error&#34;&gt;StackOverflow question&lt;/a&gt;). If you downloaded Python from the official website, then run &lt;code&gt;/Applications/Python\ 3.10/Install\ Certificates.command&lt;/code&gt; in a terminal (change &lt;code&gt;3.10&lt;/code&gt; to whatever version you installed). If you installed Python using MacPorts, run &lt;code&gt;sudo port install curl-ca-bundle&lt;/code&gt; in a terminal.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;ve installed this project locally. How do I update it to the latest version?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml3/main/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How do I update my Python libraries to the latest versions, when using Anaconda?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml3/main/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;I would like to thank everyone &lt;a href=&#34;https://github.com/ageron/handson-ml3/graphs/contributors&#34;&gt;who contributed to this project&lt;/a&gt;, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park and Ian Beauregard who reviewed every notebook and submitted many PRs, including help on some of the exercise solutions. Thanks as well to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory, and to github user SuperYorio who helped on some exercise solutions. Thanks a lot to Victor Khaustov who submitted plenty of excellent PRs, fixing many errors. And lastly, thanks to Google ML Developer Programs team who supported this work by providing Google Cloud Credit.&lt;/p&gt;</summary>
  </entry>
</feed>