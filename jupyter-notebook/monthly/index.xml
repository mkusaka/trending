<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-01T02:06:37Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>guipsamora/pandas_exercises</title>
    <updated>2024-04-01T02:06:37Z</updated>
    <id>tag:github.com,2024-04-01:/guipsamora/pandas_exercises</id>
    <link href="https://github.com/guipsamora/pandas_exercises" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Practice your pandas skills!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Pandas Exercises&lt;/h1&gt; &#xA;&lt;p&gt;Fed up with a ton of tutorials but no easy way to find exercises I decided to create a repo just with exercises to practice pandas. Don&#39;t get me wrong, tutorials are great resources, but to learn is to do. So unless you practice you won&#39;t learn.&lt;/p&gt; &#xA;&lt;p&gt;There will be three different types of files:&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1. Exercise instructions&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;2. Solutions without code&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;3. Solutions with code and comments&lt;/p&gt; &#xA;&lt;p&gt;My suggestion is that you learn a topic in a tutorial, video or documentation and then do the first exercises. Learn one more topic and do more exercises. If you are stuck, don&#39;t go directly to the solution with code files. Check the solutions only and try to get the correct answer.&lt;/p&gt; &#xA;&lt;p&gt;Suggestions and collaborations are more than welcome.🙂 Please open an issue or make a PR indicating the exercise and your problem/solution.&lt;/p&gt; &#xA;&lt;h1&gt;Lessons&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#getting-and-knowing&#34;&gt;Getting and knowing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#merge&#34;&gt;Merge&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#time-series&#34;&gt;Time Series&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#filtering-and-sorting&#34;&gt;Filtering and Sorting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#stats&#34;&gt;Stats&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#deleting&#34;&gt;Deleting&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#grouping&#34;&gt;Grouping&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#visualization&#34;&gt;Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Indexing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#apply&#34;&gt;Apply&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/#creating-series-and-dataframes&#34;&gt;Creating Series and DataFrames&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Exporting&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data&#34;&gt;Getting and knowing&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/Chipotle&#34;&gt;Chipotle&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/Occupation&#34;&gt;Occupation&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/World%20Food%20Facts&#34;&gt;World Food Facts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting&#34;&gt;Filtering and Sorting&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Chipotle&#34;&gt;Chipotle&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Euro12&#34;&gt;Euro12&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Fictional%20Army&#34;&gt;Fictional Army&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping&#34;&gt;Grouping&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Alcohol_Consumption&#34;&gt;Alcohol Consumption&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Occupation&#34;&gt;Occupation&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Regiment&#34;&gt;Regiment&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply&#34;&gt;Apply&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply/Students_Alcohol_Consumption&#34;&gt;Students Alcohol Consumption&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply/US_Crime_Rates&#34;&gt;US_Crime_Rates&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge&#34;&gt;Merge&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Auto_MPG&#34;&gt;Auto_MPG&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Fictitous%20Names&#34;&gt;Fictitious Names&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Housing%20Market&#34;&gt;House Market&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats&#34;&gt;Stats&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats/US_Baby_Names&#34;&gt;US_Baby_Names&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats/Wind_Stats&#34;&gt;Wind_Stats&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization&#34;&gt;Visualization&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Chipotle&#34;&gt;Chipotle&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Titanic_Desaster&#34;&gt;Titanic Disaster&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Scores&#34;&gt;Scores&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Online_Retail&#34;&gt;Online Retail&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Tips&#34;&gt;Tips&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/08_Creating_Series_and_DataFrames&#34;&gt;Creating Series and DataFrames&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/08_Creating_Series_and_DataFrames/Pokemon&#34;&gt;Pokemon&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series&#34;&gt;Time Series&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Apple_Stock&#34;&gt;Apple_Stock&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Getting_Financial_Data&#34;&gt;Getting_Financial_Data&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Getting_Financial_Data&#34;&gt;Investor_Flow_of_Funds_US&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting&#34;&gt;Deleting&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting/Iris&#34;&gt;Iris&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting/Wine&#34;&gt;Wine&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Video Solutions&lt;/h1&gt; &#xA;&lt;p&gt;Video tutorials of data scientists working through the above exercises:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=pu3IpU937xs&amp;amp;list=PLgJhDSE2ZLxaY_DigHeiIDC1cD09rXgJv&#34;&gt;Data Talks - Pandas Learning By Doing&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>gkamradt/LLMTest_NeedleInAHaystack</title>
    <updated>2024-04-01T02:06:37Z</updated>
    <id>tag:github.com,2024-04-01:/gkamradt/LLMTest_NeedleInAHaystack</id>
    <link href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Doing simple retrieval from LLM models at various context lengths to measure accuracy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Needle In A Haystack - Pressure Testing LLMs&lt;/h1&gt; &#xA;&lt;p&gt;A simple &#39;needle in a haystack&#39; analysis to test in-context retrieval ability of long context LLMs.&lt;/p&gt; &#xA;&lt;p&gt;Supported model providers: OpenAI, Anthropic&lt;/p&gt; &#xA;&lt;p&gt;Get the behind the scenes on the &lt;a href=&#34;https://youtu.be/KwRRuiCCdmc&#34;&gt;overview video&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gkamradt/LLMTest_NeedleInAHaystack/main/img/NeedleHaystackCodeSnippet.png&#34; alt=&#34;GPT-4-128 Context Testing&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;The Test&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Place a random fact or statement (the &#39;needle&#39;) in the middle of a long context window (the &#39;haystack&#39;)&lt;/li&gt; &#xA; &lt;li&gt;Ask the model to retrieve this statement&lt;/li&gt; &#xA; &lt;li&gt;Iterate over various document depths (where the needle is placed) and context lengths to measure performance&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This is the code that backed &lt;a href=&#34;https://twitter.com/GregKamradt/status/1722386725635580292&#34;&gt;this OpenAI&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/GregKamradt/status/1727018183608193393&#34;&gt;Anthropic analysis&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The results from the original tests are in &lt;code&gt;/original_results&lt;/code&gt;. The script has upgraded a lot since those test were ran so the data formats may not match your script results.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Setup Virtual Environment&lt;/h3&gt; &#xA;&lt;p&gt;We recommend setting up a virtual environment to isolate Python dependencies, ensuring project-specific packages without conflicting with system-wide installations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-zsh&#34;&gt;python3 -m venv venv&#xA;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Environment Variables&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;NIAH_MODEL_API_KEY&lt;/code&gt; - API key for interacting with the model. Depending on the provider, this gets used appropriately with the correct sdk.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;NIAH_EVALUATOR_API_KEY&lt;/code&gt; - API key to use if &lt;code&gt;openai&lt;/code&gt; evaluation strategy is used.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Install Package&lt;/h3&gt; &#xA;&lt;p&gt;Install the package from PyPi:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-zsh&#34;&gt;pip install needlehaystack&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Test&lt;/h3&gt; &#xA;&lt;p&gt;Start using the package by calling the entry point &lt;code&gt;needlehaystack.run_test&lt;/code&gt; from command line.&lt;/p&gt; &#xA;&lt;p&gt;You can then run the analysis on OpenAI or Anthropic models with the following command line arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;provider&lt;/code&gt; - The provider of the model, available options are &lt;code&gt;openai&lt;/code&gt; and &lt;code&gt;anthropic&lt;/code&gt;. Defaults to &lt;code&gt;openai&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;evaluator&lt;/code&gt; - The evaluator, which can either be a &lt;code&gt;model&lt;/code&gt; or &lt;code&gt;LangSmith&lt;/code&gt;. See more on &lt;code&gt;LangSmith&lt;/code&gt; below. If using a &lt;code&gt;model&lt;/code&gt;, only &lt;code&gt;openai&lt;/code&gt; is currently supported. Defaults to &lt;code&gt;openai&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model_name&lt;/code&gt; - Model name of the language model accessible by the provider. Defaults to &lt;code&gt;gpt-3.5-turbo-0125&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;evaluator_model_name&lt;/code&gt; - Model name of the language model accessible by the evaluator. Defaults to &lt;code&gt;gpt-3.5-turbo-0125&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, &lt;code&gt;LLMNeedleHaystackTester&lt;/code&gt; parameters can also be passed as command line arguments, except &lt;code&gt;model_to_test&lt;/code&gt; and &lt;code&gt;evaluator&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here are some example use cases.&lt;/p&gt; &#xA;&lt;p&gt;Following command runs the test for openai model &lt;code&gt;gpt-3.5-turbo-0125&lt;/code&gt; for a single context length of 2000 and single document depth of 50%.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-zsh&#34;&gt;needlehaystack.run_test --provider openai --model_name &#34;gpt-3.5-turbo-0125&#34; --document_depth_percents &#34;[50]&#34; --context_lengths &#34;[2000]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Following command runs the test for anthropic model &lt;code&gt;claude-2.1&lt;/code&gt; for a single context length of 2000 and single document depth of 50%.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-zsh&#34;&gt;needlehaystack.run_test --provider anthropic --model_name &#34;claude-2.1&#34; --document_depth_percents &#34;[50]&#34; --context_lengths &#34;[2000]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;For Contributors&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork and clone the repository.&lt;/li&gt; &#xA; &lt;li&gt;Create and activate the virtual environment as described above.&lt;/li&gt; &#xA; &lt;li&gt;Set the environment variables as described above.&lt;/li&gt; &#xA; &lt;li&gt;Install the package in editable mode by running the following command from repository root:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-zsh&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The package &lt;code&gt;needlehaystack&lt;/code&gt; is available for import in your test cases. Develop, make changes and test locally.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;LLMNeedleHaystackTester&lt;/code&gt; parameters:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model_to_test&lt;/code&gt; - The model to run the needle in a haystack test on. Default is None.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;evaluator&lt;/code&gt; - An evaluator to evaluate the model&#39;s response. Default is None.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;needle&lt;/code&gt; - The statement or fact which will be placed in your context (&#39;haystack&#39;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;haystack_dir&lt;/code&gt; - The directory which contains the text files to load as background context. Only text files are supported&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;retrieval_question&lt;/code&gt; - The question with which to retrieve your needle in the background context&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;results_version&lt;/code&gt; - You may want to run your test multiple times for the same combination of length/depth, change the version number if so&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;num_concurrent_requests&lt;/code&gt; - Default: 1. Set higher if you&#39;d like to run more requests in parallel. Keep in mind rate limits.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;save_results&lt;/code&gt; - Whether or not you&#39;d like to save your results to file. They will be temporarily saved in the object regardless. True/False. If &lt;code&gt;save_results = True&lt;/code&gt;, then this script will populate a &lt;code&gt;result/&lt;/code&gt; directory with evaluation information. Due to potential concurrent requests each new test will be saved as a few file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;save_contexts&lt;/code&gt; - Whether or not you&#39;d like to save your contexts to file. &lt;strong&gt;Warning&lt;/strong&gt; these will get very long. True/False&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;final_context_length_buffer&lt;/code&gt; - The amount of context to take off each input to account for system messages and output tokens. This can be more intelligent but using a static value for now. Default 200 tokens.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;context_lengths_min&lt;/code&gt; - The starting point of your context lengths list to iterate&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;context_lengths_max&lt;/code&gt; - The ending point of your context lengths list to iterate&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;context_lengths_num_intervals&lt;/code&gt; - The number of intervals between your min/max to iterate through&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;context_lengths&lt;/code&gt; - A custom set of context lengths. This will override the values set for &lt;code&gt;context_lengths_min&lt;/code&gt;, max, and intervals if set&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;document_depth_percent_min&lt;/code&gt; - The starting point of your document depths. Should be int &amp;gt; 0&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;document_depth_percent_max&lt;/code&gt; - The ending point of your document depths. Should be int &amp;lt; 100&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;document_depth_percent_intervals&lt;/code&gt; - The number of iterations to do between your min/max points&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;document_depth_percents&lt;/code&gt; - A custom set of document depths lengths. This will override the values set for &lt;code&gt;document_depth_percent_min&lt;/code&gt;, max, and intervals if set&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;document_depth_percent_interval_type&lt;/code&gt; - Determines the distribution of depths to iterate over. &#39;linear&#39; or &#39;sigmoid&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;seconds_to_sleep_between_completions&lt;/code&gt; - Default: None, set # of seconds if you&#39;d like to slow down your requests&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;print_ongoing_status&lt;/code&gt; - Default: True, whether or not to print the status of test as they complete&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;LLMMultiNeedleHaystackTester&lt;/code&gt; parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;multi_needle&lt;/code&gt; - True or False, whether to run multi-needle&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;needles&lt;/code&gt; - List of needles to insert in the context&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other Parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model_name&lt;/code&gt; - The name of the model you&#39;d like to use. Should match the exact value which needs to be passed to the api. Ex: For OpenAI inference and evaluator models it would be &lt;code&gt;gpt-3.5-turbo-0125&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Results Visualization&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;LLMNeedleInHaystackVisualization.ipynb&lt;/code&gt; holds the code to make the pivot table visualization. The pivot table was then transferred to Google Slides for custom annotations and formatting. See the &lt;a href=&#34;https://docs.google.com/presentation/d/15JEdEBjm32qBbqeYM6DK6G-3mUJd7FAJu-qEzj8IYLQ/edit?usp=sharing&#34;&gt;google slides version&lt;/a&gt;. See an overview of how this viz was created &lt;a href=&#34;https://twitter.com/GregKamradt/status/1729573848893579488&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;OpenAI&#39;s GPT-4-128K (Run 11/8/2023)&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/gkamradt/LLMTest_NeedleInAHaystack/main/img/GPT_4_testing.png&#34; alt=&#34;GPT-4-128 Context Testing&#34; width=&#34;800&#34;&gt; &#xA;&lt;h2&gt;Anthropic&#39;s Claude 2.1 (Run 11/21/2023)&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/gkamradt/LLMTest_NeedleInAHaystack/main/img/Claude_2_1_testing.png&#34; alt=&#34;GPT-4-128 Context Testing&#34; width=&#34;800&#34;&gt; &#xA;&lt;h2&gt;Multi Needle Evaluator&lt;/h2&gt; &#xA;&lt;p&gt;To enable multi-needle insertion into our context, use &lt;code&gt;--multi_needle True&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This inserts the first needle at the specified &lt;code&gt;depth_percent&lt;/code&gt;, then evenly distributes subsequent needles through the remaining context after this depth.&lt;/p&gt; &#xA;&lt;p&gt;For even spacing, it calculates the &lt;code&gt;depth_percent_interval&lt;/code&gt; as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;depth_percent_interval = (100 - depth_percent) / len(self.needles)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;So, the first needle is placed at a depth percent of &lt;code&gt;depth_percent&lt;/code&gt;, the second at &lt;code&gt;depth_percent + depth_percent_interval&lt;/code&gt;, the third at &lt;code&gt;depth_percent + 2 * depth_percent_interval&lt;/code&gt;, and so on.&lt;/p&gt; &#xA;&lt;p&gt;Following example shows the depth percents for the case of 10 needles and depth_percent of 40%.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;depth_percent_interval = (100 - 40) / 10 = 6&#xA;&#xA;Needle 1: 40&#xA;Needle 2: 40 + 6 = 46&#xA;Needle 3: 40 + 2 * 6 = 52&#xA;Needle 4: 40 + 3 * 6 = 58&#xA;Needle 5: 40 + 4 * 6 = 64&#xA;Needle 6: 40 + 5 * 6 = 70&#xA;Needle 7: 40 + 6 * 6 = 76&#xA;Needle 8: 40 + 7 * 6 = 82&#xA;Needle 9: 40 + 8 * 6 = 88&#xA;Needle 10: 40 + 9 * 6 = 94&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;LangSmith Evaluator&lt;/h2&gt; &#xA;&lt;p&gt;You can use LangSmith to orchestrate evals and store results.&lt;/p&gt; &#xA;&lt;p&gt;(1) Sign up for &lt;a href=&#34;https://docs.smith.langchain.com/setup&#34;&gt;LangSmith&lt;/a&gt; (2) Set env variables for LangSmith as specified in the setup. (3) In the &lt;code&gt;Datasets + Testing&lt;/code&gt; tab, use &lt;code&gt;+ Dataset&lt;/code&gt; to create a new dataset, call it &lt;code&gt;multi-needle-eval-sf&lt;/code&gt; to start. (4) Populate the dataset with a test question:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;question: What are the 5 best things to do in San Franscisco?&#xA;answer: &#34;The 5 best things to do in San Francisco are: 1) Go to Dolores Park. 2) Eat at Tony&#39;s Pizza Napoletana. 3) Visit Alcatraz. 4) Hike up Twin Peaks. 5) Bike across the Golden Gate Bridge&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/rlancemartin/LLMTest_NeedleInAHaystack/assets/122662504/2f903955-ed1d-49cc-b995-ed0407d6212a&#34; alt=&#34;Screenshot 2024-03-05 at 4 54 15 PM&#34;&gt; (5) Run with &lt;code&gt; --evaluator langsmith&lt;/code&gt; and &lt;code&gt;--eval_set multi-needle-eval-sf&lt;/code&gt; to run against our recently created eval set.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s see all these working together on a new dataset, &lt;code&gt;multi-needle-eval-pizza&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here is the &lt;code&gt;multi-needle-eval-pizza&lt;/code&gt; eval set, which has a question and reference answer. You can also and resulting runs: &lt;a href=&#34;https://smith.langchain.com/public/74d2af1c-333d-4a73-87bc-a837f8f0f65c/d&#34;&gt;https://smith.langchain.com/public/74d2af1c-333d-4a73-87bc-a837f8f0f65c/d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here is the command to run this using multi-needle eval and passing the relevant needles:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;needlehaystack.run_test --evaluator langsmith --context_lengths_num_intervals 3 --document_depth_percent_intervals 3 --provider openai --model_name &#34;gpt-4-0125-preview&#34; --multi_needle True --eval_set multi-needle-eval-pizza --needles &#39;[&#34;Figs are one of the three most delicious pizza toppings.&#34;, &#34;Prosciutto is one of the three most delicious pizza toppings.&#34;, &#34;Goat cheese is one of the three most delicious pizza toppings.&#34;]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/gkamradt/LLMTest_NeedleInAHaystack/main/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt; file for details. Use of this software requires attribution to the original author and project, as detailed in the license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>chenzomi12/AISystem</title>
    <updated>2024-04-01T02:06:37Z</updated>
    <id>tag:github.com,2024-04-01:/chenzomi12/AISystem</id>
    <link href="https://github.com/chenzomi12/AISystem" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AISystem 主要是指AI系统，包括AI芯片、AI编译器、AI推理和训练框架等AI全栈底层技术&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deep Learning System &amp;amp; AI System&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;Continuous Integration&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/d2l-ai/d2l-en/actions/workflows/build-docker.yml&#34;&gt;&lt;img src=&#34;https://github.com/d2l-ai/d2l-en/actions/workflows/build-docker.yml/badge.svg?sanitize=true&#34; alt=&#34;Build Docker Image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;文字课程内容正在一节节补充更新，尽可能抽空继续更新正在 &lt;a href=&#34;https://chenzomi12.github.io/&#34;&gt;AISys&lt;/a&gt; ，希望您多多鼓励和参与进来！！！&lt;/p&gt; &#xA;&lt;p&gt;文字课程开源在 &lt;a href=&#34;https://chenzomi12.github.io/&#34;&gt;AISys&lt;/a&gt;，系列视频托管&lt;a href=&#34;https://space.bilibili.com/517221395&#34;&gt;B站&lt;/a&gt;和&lt;a href=&#34;https://www.youtube.com/@zomi6222/videos&#34;&gt;油管&lt;/a&gt;，PPT开源在&lt;a href=&#34;https://github.com/chenzomi12/DeepLearningSystem&#34;&gt;github&lt;/a&gt;，欢迎取用！！！&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;非常希望您也参与到这个开源项目中，B站给ZOMI留言哦！&lt;/p&gt; &#xA; &lt;p&gt;欢迎大家使用的过程中发现bug或者勘误直接提交代码PR到开源社区哦！&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;项目背景&lt;/h2&gt; &#xA;&lt;p&gt;这个开源项目英文名字叫做 &lt;strong&gt;Deep Learning System&lt;/strong&gt; 或者 &lt;strong&gt;AI System(AISys)&lt;/strong&gt;，中文名字叫做 &lt;strong&gt;深度学习系统&lt;/strong&gt; 或者 &lt;strong&gt;AI系统&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;p&gt;本开源项目主要是跟大家一起探讨和学习人工智能、深度学习的系统设计，而整个系统是围绕着 ZOMI 在工作当中所积累、梳理、构建 AI 系统全栈的内容。希望跟所有关注 AI 开源项目的好朋友一起探讨研究，共同促进学习讨论。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/images/ai_system02.png&#34; alt=&#34;AI系统全栈&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;课程内容大纲&lt;/h2&gt; &#xA;&lt;p&gt;课程主要包括以下六大模块：&lt;/p&gt; &#xA;&lt;p&gt;第一部分，AI基础知识和AI系统的全栈概述的&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/01Introduction/&#34;&gt;&lt;u&gt;&lt;strong&gt;AI系统概述&lt;/strong&gt;&lt;/u&gt;&lt;/a&gt;，以及深度学习系统的系统性设计和方法论，主要是整体了解AI训练和推理全栈的体系结构内容。&lt;/p&gt; &#xA;&lt;p&gt;第二部分，硬核篇介绍&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/02Hardware/&#34;&gt;&lt;u&gt;&lt;strong&gt;AI芯片概况&lt;/strong&gt;&lt;/u&gt;&lt;/a&gt;，这里就很硬核了，从芯片基础到AI芯片的范围都会涉及，芯片设计需要考虑上面AI框架的前端、后端编译，而不是停留在天天喊着吊打英伟达，被现实打趴。&lt;/p&gt; &#xA;&lt;p&gt;第三部分，进阶篇介绍&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/03Compiler/&#34;&gt;&lt;u&gt;&lt;strong&gt;AI编译器原理&lt;/strong&gt;&lt;/u&gt;&lt;/a&gt;，将站在系统设计的角度，思考在设计现代机器学习系统中需要考虑的编译器问题，特别是中间表达乃至后端优化。&lt;/p&gt; &#xA;&lt;p&gt;第四部分，实际应用&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/04Inference/&#34;&gt;&lt;u&gt;&lt;strong&gt;推理系统与引擎&lt;/strong&gt;&lt;/u&gt;&lt;/a&gt;，讲了太多原理身体太虚容易消化不良，还是得回归到业务本质，让行业、企业能够真正应用起来，而推理系统涉及一些核心算法和注意的事情也分享下。&lt;/p&gt; &#xA;&lt;p&gt;第五部分，介绍&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/05Framework/&#34;&gt;&lt;u&gt;&lt;strong&gt;AI框架核心技术&lt;/strong&gt;&lt;/u&gt;&lt;/a&gt;，首先介绍任何一个AI框架都离不开的自动微分，通过自动微分功能后就会产生表示神经网络的图和算子，然后介绍AI框架前端的优化，还有最近很火的大模型分布式训练在AI框架中的关键技术。&lt;/p&gt; &#xA;&lt;p&gt;第六部分，汇总篇介绍&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/06Foundation/&#34;&gt;&lt;u&gt;&lt;strong&gt;大模型与AI系统&lt;/strong&gt;&lt;/u&gt;&lt;/a&gt;，大模型是基于AI集群的全栈软硬件性能优化，通过最小的每一块AI芯片组成的AI集群，编译器使能到上层的AI框架，训练过程需要分布式并行、集群通信等算法支持，而且在大模型领域最近持续演进如智能体等新技术。&lt;/p&gt; &#xA;&lt;h2&gt;课程设立目的&lt;/h2&gt; &#xA;&lt;p&gt;本课程主要为本科生高年级、硕博研究生、AI系统从业者设计，帮助大家：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;完整了解AI的计算机系统架构，并通过实际问题和案例，来了解AI完整生命周期下的系统设计。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;介绍前沿系统架构和AI相结合的研究工作，了解主流框架、平台和工具来了解AI系统。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;先修课程:&lt;/strong&gt;&amp;nbsp;C++/Python，计算机体系结构，人工智能基础&lt;/p&gt; &#xA;&lt;h2&gt;课程部分&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/01Introduction/&#34;&gt;一. AI系统概述&lt;/a&gt;&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;编号&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;名称&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;具体内容&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/01Introduction/&#34;&gt;AI 系统&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;算法、框架、体系结构的结合，形成AI系统&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/02Hardware/&#34;&gt;二. AI芯片体系结构&lt;/a&gt;&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;编号&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;名称&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;具体内容&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/02Hardware/01Foundation/&#34;&gt;AI 计算体系&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;神经网络等AI技术的计算模式和计算体系架构&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/02Hardware/02ChipBase/&#34;&gt;AI 芯片基础&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;CPU、GPU、NPU等芯片体系架构基础原理&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/02Hardware/03GPUBase/&#34;&gt;图形处理器 GPU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;GPU的基本原理，英伟达GPU的架构发展&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/02Hardware/04NVIDIA/&#34;&gt;英伟达 GPU 详解&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;英伟达GPU的TensorCore、NVLink深度剖析&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/02Hardware/05Abroad/&#34;&gt;国外 AI 处理器&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;谷歌、特斯拉等专用AI处理器核心原理&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/02Hardware/06Domestic/&#34;&gt;国内 AI 处理器&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;寒武纪、燧原科技等专用AI处理器核心原理&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/02Hardware/07Thought/&#34;&gt;AI 芯片黄金10年&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;对 AI 芯片的编程模式和发展进行总结&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/03Compiler/&#34;&gt;三. AI编译原理&lt;/a&gt;&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;编号&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;名称&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;具体内容&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/03Compiler/01Tradition/&#34;&gt;传统编译器&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;传统编译器GCC与LLVM，LLVM详细架构&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/03Compiler/02AICompiler/&#34;&gt;AI 编译器&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;AI编译器发展与架构定义，未来挑战与思考&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/03Compiler/03Frontend/&#34;&gt;前端优化&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;AI编译器的前端优化(算子融合、内存优化等)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/03Compiler/04Backend/&#34;&gt;后端优化&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;AI编译器的后端优化(Kernel优化、AutoTuning)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;多面体&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;待更ing...&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/03Compiler/06PyTorch/&#34;&gt;PyTorch2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;PyTorch2.0最重要的新特性：编译技术栈&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/04Inference/&#34;&gt;四. AI推理系统&lt;/a&gt;&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;编号&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;名称&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;具体内容&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/04Inference/01Inference/&#34;&gt;推理系统&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;推理系统整体介绍，推理引擎架构梳理&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/04Inference/02Mobilenet/&#34;&gt;轻量网络&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;轻量化主干网络，MobileNet等SOTA模型介绍&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/04Inference/03Slim/&#34;&gt;模型压缩&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;模型压缩4件套，量化、蒸馏、剪枝和二值化&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/04Inference/04Converter/&#34;&gt;转换&amp;amp;优化&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;AI框架训练后模型进行转换，并对计算图优化&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/04Inference/05Kernel/&#34;&gt;Kernel优化&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Kernel层、算子层优化，对算子、内存、调度优化&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/05Framework/&#34;&gt;五. AI框架核心技术&lt;/a&gt;&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;编号&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;名称&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;具体内容&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/05Framework/01Foundation/&#34;&gt;AI框架基础&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;AI框架的作用、发展、编程范式&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/05Framework/02AutoDiff/&#34;&gt;自动微分&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;自动微分的实现方式和原理&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/05Framework/03DataFlow/&#34;&gt;计算图&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;计算图的概念，图优化、图执行、控制流表达&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/06Foundation/&#34;&gt;六. 大模型训练&lt;/a&gt;&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;编号&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;名称&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;具体内容&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/06Foundation/01Introduce/&#34;&gt;大模型全流程&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;大模型整体架构和大模型全流程介绍&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/06Foundation/02AICluster/&#34;&gt;AI 集群简介&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;AI集群服务器整体组成相关技术初体验&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/06Foundation/03Storage/&#34;&gt;AI 集群存储&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;数据存储在AI集群中，具体的存储优化方案&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/06Foundation/04Network/&#34;&gt;AI 集群通信&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;更新中&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/06Foundation/05Dataset/&#34;&gt;数据处理&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;更新中&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/06Foundation/06Algorithm/&#34;&gt;大模型算法&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;更新中&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/06Foundation/07Train/&#34;&gt;大模型训练&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;更新中&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/06Foundation/08Parallel/&#34;&gt;分布式并行&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;更新中&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/06Foundation/09Finetune/&#34;&gt;大模型微调&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;更新中&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/06Foundation/10Evaluate/&#34;&gt;大模型验证&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;更新中&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/06Foundation/11Inference/&#34;&gt;大模型推理&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;更新中&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/06Foundation/12Agent/&#34;&gt;AI Agent&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;AI Agent 智能体，通过大模型走向GAI&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;知识清单&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chenzomi12/AISystem/main/images/knowledge_list.png&#34; alt=&#34;知识清单&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;贡献者&lt;/h2&gt; &#xA;&lt;!-- readme: collaborators,contributors -start --&gt; &#xA;&lt;a href=&#34;https://github.com/chenzomi12/DeepLearningSystem/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=chenzomi12/DeepLearningSystem&#34;&gt; &lt;/a&gt; &#xA;&lt;!-- readme: collaborators,contributors -end --&gt; &#xA;&lt;h2&gt;备注&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;这个仓已经到达疯狂的10G啦（ZOMI把所有制作过程、高清图片都原封不动提供），如果你要git clone会非常的慢，因此建议优先到 &lt;a href=&#34;https://github.com/chenzomi12/DeepLearningSystem/releases&#34;&gt;Releases · chenzomi12/DeepLearningSystem&lt;/a&gt; 来下载你需要的内容。&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
</feed>