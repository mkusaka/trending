<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-01T01:47:15Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>facebookresearch/dinov2</title>
    <updated>2025-05-01T01:47:15Z</updated>
    <id>tag:github.com,2025-05-01:/facebookresearch/dinov2</id>
    <link href="https://github.com/facebookresearch/dinov2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch code and models for the DINOv2 self-supervised learning method.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;span&gt;üÜï&lt;/span&gt; [2023-10-26] &lt;em&gt;Added DINOv2 backbones with registers, following &lt;a href=&#34;https://arxiv.org/abs/2309.16588&#34;&gt;Vision Transformers Need Registers&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h1&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ai.facebook.com/research/&#34;&gt;Meta AI Research, FAIR&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Patrick Labatut, Armand Joulin, Piotr Bojanowski&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2304.07193&#34;&gt;&lt;code&gt;Paper #1&lt;/code&gt;&lt;/a&gt;] &lt;a href=&#34;https://arxiv.org/abs/2309.16588&#34;&gt;&lt;code&gt;Paper #2&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/&#34;&gt;&lt;code&gt;Blog&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://dinov2.metademolab.com&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/#citing-dinov2&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;PyTorch implementation and pretrained models for DINOv2. For details, see the papers: &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.07193&#34;&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.16588&#34;&gt;Vision Transformers Need Registers&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;DINOv2 models produce high-performance visual features that can be directly employed with classifiers as simple as linear layers on a variety of computer vision tasks; these visual features are robust and perform well across domains without any requirement for fine-tuning. The models were pretrained on a dataset of 142 M images without using any labels or annotations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/dinov2/assets/60359573/f168823e-7922-415a-b429-578badf5c356&#34;&gt;https://github.com/facebookresearch/dinov2/assets/60359573/f168823e-7922-415a-b429-578badf5c356&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  Visualization of the three first principal components of the patch features of all frames, mapped to RGB values. &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Pretrained models&lt;/h2&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;# of&lt;br&gt;params&lt;/th&gt; &#xA;   &lt;th&gt;with&lt;br&gt;registers&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet&lt;br&gt;k-NN&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet&lt;br&gt;linear&lt;/th&gt; &#xA;   &lt;th&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;21 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.0%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;81.1%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;21 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.1%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;80.9%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;82.1%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;82.0%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84.6%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;300 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;300 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.8%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.7%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1,100 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1,100 M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.7%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;87.1%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pretrained backbones (via PyTorch Hub)&lt;/h3&gt; &#xA;&lt;p&gt;Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install PyTorch (the only required dependency for loading the model). Installing PyTorch with CUDA support is strongly recommended.&lt;/p&gt; &#xA;&lt;p&gt;A corresponding &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/MODEL_CARD.md&#34;&gt;model card&lt;/a&gt; is included in the repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;# DINOv2&#xA;dinov2_vits14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14&#39;)&#xA;dinov2_vitb14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14&#39;)&#xA;dinov2_vitl14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14&#39;)&#xA;dinov2_vitg14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14&#39;)&#xA;&#xA;# DINOv2 with registers&#xA;dinov2_vits14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14_reg&#39;)&#xA;dinov2_vitb14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14_reg&#39;)&#xA;dinov2_vitl14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_reg&#39;)&#xA;dinov2_vitg14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14_reg&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pretrained heads - Image classification&lt;/h3&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;backbone&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;with&lt;br&gt;registers&lt;/th&gt; &#xA;   &lt;th&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ImageNet&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt; linear head (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_lreg4_inear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The (full) classifier models can be loaded via PyTorch Hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;# DINOv2&#xA;dinov2_vits14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14_lc&#39;)&#xA;dinov2_vitb14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14_lc&#39;)&#xA;dinov2_vitl14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_lc&#39;)&#xA;dinov2_vitg14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14_lc&#39;)&#xA;&#xA;# DINOv2 with registers&#xA;dinov2_vits14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14_reg_lc&#39;)&#xA;dinov2_vitb14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14_reg_lc&#39;)&#xA;dinov2_vitl14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_reg_lc&#39;)&#xA;dinov2_vitg14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14_reg_lc&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pretrained heads - Depth estimation&lt;/h3&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;backbone&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;download head&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;NYUd&lt;/th&gt; &#xA;   &lt;th&gt;KITTI&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_nyu_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_nyu_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_nyu_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_nyu_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_nyu_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_nyu_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; linear (&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_linear_head.pth&#34;&gt;1 layer&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_linear4_head.pth&#34;&gt;4 layers&lt;/a&gt;), &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_dpt_head.pth&#34;&gt;DPT&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pretrained heads - Semantic segmentation&lt;/h3&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;backbone&lt;/th&gt; &#xA;   &lt;th&gt;download model&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;download head&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ADE20K&lt;/th&gt; &#xA;   &lt;th&gt;ADE20K&lt;/th&gt; &#xA;   &lt;th&gt;VOC2012&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_ade20k_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_ade20k_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_voc2012_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_voc2012_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_ade20k_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_ade20k_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_voc2012_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_voc2012_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_ade20k_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_ade20k_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_voc2012_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_voc2012_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_m2f.pth&#34;&gt;Mask2Former&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_voc2012_linear_head.pth&#34;&gt;linear&lt;/a&gt;, &lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_voc2012_ms_head.pth&#34;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The training and evaluation code requires PyTorch 2.0 and &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;xFormers&lt;/a&gt; 0.0.18 as well as a number of other 3rd party packages. Note that the code has only been tested with the specified versions and also expects a Linux environment. To setup all the required dependencies for training and evaluation, please follow the instructions below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html&#34;&gt;conda&lt;/a&gt;&lt;/em&gt; &lt;strong&gt;(Recommended)&lt;/strong&gt; - Clone the repository and then create and activate a &lt;code&gt;dinov2&lt;/code&gt; conda environment using the provided environment definition:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f conda.yaml&#xA;conda activate dinov2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://pip.pypa.io/en/stable/getting-started/&#34;&gt;pip&lt;/a&gt;&lt;/em&gt; - Clone the repository and then use the provided &lt;code&gt;requirements.txt&lt;/code&gt; to install the dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For dense tasks (depth estimation and semantic segmentation), there are additional dependencies (specific versions of &lt;code&gt;mmcv&lt;/code&gt; and &lt;code&gt;mmsegmentation&lt;/code&gt;) which are captured in the &lt;code&gt;extras&lt;/code&gt; dependency specifications:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html&#34;&gt;conda&lt;/a&gt;&lt;/em&gt; &lt;strong&gt;(Recommended)&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f conda-extras.yaml&#xA;conda activate dinov2-extras&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://pip.pypa.io/en/stable/getting-started/&#34;&gt;pip&lt;/a&gt;&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt -r requirements-extras.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;h3&gt;ImageNet-1k&lt;/h3&gt; &#xA;&lt;p&gt;The root directory of the dataset should hold the following contents:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/ILSVRC2012_test_00000001.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/[..]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/ILSVRC2012_test_00100000.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/n01440764/n01440764_10026.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/[...]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/n15075141/n15075141_9993.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/n01440764/ILSVRC2012_val_00000293.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/[...]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/n15075141/ILSVRC2012_val_00049174.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/labels.txt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The provided dataset implementation expects a few additional metadata files to be present under the extra directory:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-ids-TRAIN.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-ids-VAL.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-names-TRAIN.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-names-VAL.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-TEST.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-TRAIN.npy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-VAL.npy&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These metadata files can be generated (once) with the following lines of Python code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from dinov2.data.datasets import ImageNet&#xA;&#xA;for split in ImageNet.Split:&#xA;    dataset = ImageNet(split=split, root=&#34;&amp;lt;ROOT&amp;gt;&#34;, extra=&#34;&amp;lt;EXTRA&amp;gt;&#34;)&#xA;    dataset.dump_extra()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the root and extra directories do not have to be distinct directories.&lt;/p&gt; &#xA;&lt;h3&gt;ImageNet-22k&lt;/h3&gt; &#xA;&lt;p&gt;Please adapt the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/dinov2/data/datasets/image_net_22k.py&#34;&gt;dataset class&lt;/a&gt; to match your local setup.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; To execute the commands provided in the next sections for training and evaluation, the &lt;code&gt;dinov2&lt;/code&gt; package should be included in the Python module search path, i.e. simply prefix the command to run with &lt;code&gt;PYTHONPATH=.&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Fast setup: training DINOv2 ViT-L/16 on ImageNet-1k&lt;/h3&gt; &#xA;&lt;p&gt;Run DINOv2 training on 4 A100-80GB nodes (32 GPUs) in a SLURM cluster environment with submitit:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/train/train.py \&#xA;    --nodes 4 \&#xA;    --config-file dinov2/configs/train/vitl16_short.yaml \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt; \&#xA;    train.dataset_path=ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training time is approximately 1 day and the resulting checkpoint should reach 81.6% on k-NN eval and 82.9% on linear eval.&lt;/p&gt; &#xA;&lt;p&gt;The training code saves the weights of the teacher in the &lt;code&gt;eval&lt;/code&gt; folder every 12500 iterations for evaluation.&lt;/p&gt; &#xA;&lt;h3&gt;Long setup: training DINOv2 ViT-L/14 on ImageNet-22k&lt;/h3&gt; &#xA;&lt;p&gt;Run DINOv2 training on 12 A100-80GB nodes (96 GPUs) in a SLURM cluster environment with submitit:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/train/train.py \&#xA;    --nodes 12 \&#xA;    --config-file dinov2/configs/train/vitl14.yaml \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt; \&#xA;    train.dataset_path=ImageNet22k:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training time is approximately 3.3 days and the resulting checkpoint should reach 82.0% on k-NN eval and 84.5% on linear eval.&lt;/p&gt; &#xA;&lt;p&gt;The training code saves the weights of the teacher in the &lt;code&gt;eval&lt;/code&gt; folder every 12500 iterations for evaluation.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;The training code regularly saves the teacher weights. In order to evaluate the model, run the following evaluation on a single node:&lt;/p&gt; &#xA;&lt;h3&gt;k-NN classification on ImageNet-1k&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/knn.py \&#xA;    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \&#xA;    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/knn \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Logistic regression classification on ImageNet-1k&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/log_regression.py \&#xA;    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \&#xA;    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/logreg \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Linear classification with data augmentation on ImageNet-1k&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/linear.py \&#xA;    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \&#xA;    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/linear \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We release the weights from evaluating the different models:&lt;/p&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;with&lt;br&gt;registers&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet&lt;br&gt;top-1&lt;/th&gt; &#xA;   &lt;th&gt;linear evaluation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;81.1%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;80.8%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84.4%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;87.0%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;The performance of the provided pretrained model weights can be evaluated as follows on ImageNet-1k:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/linear.py \&#xA;    --config-file dinov2/configs/eval/vitg14_pretrain.yaml \&#xA;    --pretrained-weights https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;A few notebooks are provided to help the community leverage the models and code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/dinov2/raw/main/notebooks/depth_estimation.ipynb&#34;&gt;Depth estimation&lt;/a&gt; - How to load and use the depth heads in combination with a matching backbone via mmcv&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/dinov2/raw/main/notebooks/semantic_segmentation.ipynb&#34;&gt;Semantic segmentation&lt;/a&gt; - How to load and use the segmentation heads in combination with a matching backbone via mmcv, and also how to load and use the Mask2Former-based segmentation model trained on ADE20K&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;DINOv2 code and model weights are released under the Apache License 2.0. See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for additional details.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citing DINOv2&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful, please consider giving a star &lt;span&gt;‚≠ê&lt;/span&gt; and citation &lt;span&gt;ü¶ñ&lt;/span&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{oquab2023dinov2,&#xA;  title={DINOv2: Learning Robust Visual Features without Supervision},&#xA;  author={Oquab, Maxime and Darcet, Timoth√©e and Moutakanni, Theo and Vo, Huy V. and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Howes, Russell and Huang, Po-Yao and Xu, Hu and Sharma, Vasu and Li, Shang-Wen and Galuba, Wojciech and Rabbat, Mike and Assran, Mido and Ballas, Nicolas and Synnaeve, Gabriel and Misra, Ishan and Jegou, Herve and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},&#xA;  journal={arXiv:2304.07193},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{darcet2023vitneedreg,&#xA;  title={Vision Transformers Need Registers},&#xA;  author={Darcet, Timoth√©e and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},&#xA;  journal={arXiv:2309.16588},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/ai-agents-for-beginners</title>
    <updated>2025-05-01T01:47:15Z</updated>
    <id>tag:github.com,2025-05-01:/microsoft/ai-agents-for-beginners</id>
    <link href="https://github.com/microsoft/ai-agents-for-beginners" rel="alternate"></link>
    <summary type="html">&lt;p&gt;10 Lessons to Get Started Building AI Agents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Agents for Beginners - A Course&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/images/repo-thumbnail.png&#34; alt=&#34;Generative AI For Beginners&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;10 Lessons teaching everything you need to know to start building AI Agents&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/ai-agents-for-beginners/raw/master/LICENSE?WT.mc_id=academic-105485-koreyst&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/microsoft/ai-agents-for-beginners.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ai-agents-for-beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/microsoft/ai-agents-for-beginners.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ai-agents-for-beginners/issues/?WT.mc_id=academic-105485-koreyst&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/microsoft/ai-agents-for-beginners.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ai-agents-for-beginners/pulls/?WT.mc_id=academic-105485-koreyst&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/microsoft/ai-agents-for-beginners.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Language Support&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/English-brightgreen.svg?style=flat-square&#34; alt=&#34;English&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/zh/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Chinese_Simplified-brightgreen.svg?style=flat-square&#34; alt=&#34;Chinese Simplified&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/tw/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Chinese_Traditional-brightgreen.svg?style=flat-square&#34; alt=&#34;Chinese Traditional&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hk/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Chinese_Hong_Kong-brightgreen.svg?style=flat-square&#34; alt=&#34;Chinese Hong Kong&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fr/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/French-brightgreen.svg?style=flat-square&#34; alt=&#34;French&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ja/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Japanese-brightgreen.svg?style=flat-square&#34; alt=&#34;Japanese&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ko/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Korean-brightgreen.svg?style=flat-square&#34; alt=&#34;Korean&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pt/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Portuguese_Brazilian-brightgreen.svg?style=flat-square&#34; alt=&#34;Portuguese Brazilian&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/es/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Spanish-brightgreen.svg?style=flat-square&#34; alt=&#34;Spanish&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/de/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/German-brightgreen.svg?style=flat-square&#34; alt=&#34;German&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fa/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Persian-brightgreen.svg?style=flat-square&#34; alt=&#34;Persian&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pl/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Polish-brightgreen.svg?style=flat-square&#34; alt=&#34;Polish&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hi/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hindi-brightgreen.svg?style=flat-square&#34; alt=&#34;Hindi&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/microsoft/ai-agents-for-beginners/watchers/?WT.mc_id=academic-105485-koreyst&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Watch&#34; alt=&#34;GitHub watchers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ai-agents-for-beginners/network/?WT.mc_id=academic-105485-koreyst&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Fork&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ai-agents-for-beginners/stargazers/?WT.mc_id=academic-105485-koreyst&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Star&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/kzRShWzttr&#34;&gt;&lt;img src=&#34;https://dcbadge.limes.pink/api/server/kzRShWzttr&#34; alt=&#34;Azure AI Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üå± Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;This course has 10 lessons covering the fundamentals of building AI Agents. Each lesson covers its own topic so start wherever you like!&lt;/p&gt; &#xA;&lt;p&gt;There is multi-language support for this course. Go to our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/#-multi-language-support&#34;&gt;available languages here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If this is your first time building with Generative AI models, check out our &lt;a href=&#34;https://aka.ms/genai-beginners&#34;&gt;Generative AI For Beginners&lt;/a&gt; course, which includes 21 lessons on building with GenAI.&lt;/p&gt; &#xA;&lt;p&gt;Don&#39;t forget to &lt;a href=&#34;https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst&#34;&gt;star (üåü) this repo&lt;/a&gt; and &lt;a href=&#34;https://github.com/microsoft/ai-agents-for-beginners/fork&#34;&gt;fork this repo&lt;/a&gt; to run the code.&lt;/p&gt; &#xA;&lt;h3&gt;What You Need&lt;/h3&gt; &#xA;&lt;p&gt;Each lesson in this course includes code examples, which can be found in the code_samples folder. You can &lt;a href=&#34;https://github.com/microsoft/ai-agents-for-beginners/fork&#34;&gt;fork this repo&lt;/a&gt; to create your own copy.&lt;/p&gt; &#xA;&lt;p&gt;The code example in these exercises, utilize Azure AI Foundry and GitHub Model Catalogs for interacting with Language Models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-agents-beginners/github-models&#34;&gt;Github Models&lt;/a&gt; - Free / Limited&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-agents-beginners/ai-foundry&#34;&gt;Azure AI Foundry&lt;/a&gt; - Azure Account Required&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This course also uses the following AI Agent frameworks and services from Microsoft:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-agents-beginners/ai-agent-service&#34;&gt;Azure AI Agent Service&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-agents-beginners/semantic-kernel&#34;&gt;Semantic Kernel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-agents/autogen&#34;&gt;AutoGen&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more information on running the code for this course, go to the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/00-course-setup/README.md&#34;&gt;Course Setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üôè Want to help?&lt;/h2&gt; &#xA;&lt;p&gt;Do you have suggestions or found spelling or code errors? &lt;a href=&#34;https://github.com/microsoft/ai-agents-for-beginners/issues?WT.mc_id=academic-105485-koreyst&#34;&gt;Raise an issue&lt;/a&gt; or &lt;a href=&#34;https://github.com/microsoft/ai-agents-for-beginners/pulls?WT.mc_id=academic-105485-koreyst&#34;&gt;Create a pull request&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you get stuck or have any questions about building AI Agents, join our &lt;a href=&#34;https://discord.gg/kzRShWzttr&#34;&gt;Azure AI Community Discord&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üìÇ Each lesson includes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A written lesson located in the README and a short video&lt;/li&gt; &#xA; &lt;li&gt;Python code samples supporting Azure AI Foundry and Github Models (Free)&lt;/li&gt; &#xA; &lt;li&gt;Links to extra resources to continue your learning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üóÉÔ∏è Lessons&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Lesson&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Text &amp;amp; Code&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Video&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Extra Learning&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intro to AI Agents and Agent Use Cases&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/01-intro-to-ai-agents/README.md&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://youtu.be/3zgm60bXmQk?si=z8QygFvYQv-9WtO1&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Exploring AI Agentic Frameworks&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/02-explore-agentic-frameworks/README.md&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://youtu.be/ODwF-EZo_O8?si=Vawth4hzVaHv-u0H&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Understanding AI Agentic Design Patterns&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/03-agentic-design-patterns/README.md&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://youtu.be/m9lM8qqoOEA?si=BIzHwzstTPL8o9GF&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tool Use Design Pattern&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/04-tool-use/README.md&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://youtu.be/vieRiPRx-gI?si=2z6O2Xu2cu_Jz46N&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Agentic RAG&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/05-agentic-rag/README.md&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://youtu.be/WcjAARvdL7I?si=gKPWsQpKiIlDH9A3&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Building Trustworthy AI Agents&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/06-building-trustworthy-agents/README.md&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://youtu.be/iZKkMEGBCUQ?si=jZjpiMnGFOE9L8OK&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Planning Design Pattern&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/07-planning-design/README.md&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://youtu.be/kPfJ2BrBCMY?si=6SC_iv_E5-mzucnC&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi-Agent Design Pattern&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/08-multi-agent/README.md&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://youtu.be/V6HpE9hZEx0?si=rMgDhEu7wXo2uo6g&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Metacognition Design Pattern&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/09-metacognition/README.md&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://youtu.be/His9R6gw6Ec?si=8gck6vvdSNCt6OcF&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AI Agents in Production&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/10-ai-agents-production/README.md&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://youtu.be/l4TP6IyJxmQ?si=31dnhexRo6yLRJDl&#34;&gt;Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üåê Multi-Language Support&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Code&lt;/th&gt; &#xA;   &lt;th&gt;Link to Translated README&lt;/th&gt; &#xA;   &lt;th&gt;Last Updated&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese (Simplified)&lt;/td&gt; &#xA;   &lt;td&gt;zh&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/zh/README.md&#34;&gt;Chinese Translation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-24&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese (Traditional)&lt;/td&gt; &#xA;   &lt;td&gt;tw&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/tw/README.md&#34;&gt;Chinese Translation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese (Hong Kong)&lt;/td&gt; &#xA;   &lt;td&gt;hk&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hk/README.md&#34;&gt;Chinese (Hong Kong) Translation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;French&lt;/td&gt; &#xA;   &lt;td&gt;fr&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fr/README.md&#34;&gt;French Translation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Japanese&lt;/td&gt; &#xA;   &lt;td&gt;ja&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ja/README.md&#34;&gt;Japanese Translation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Korean&lt;/td&gt; &#xA;   &lt;td&gt;ko&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ko/README.md&#34;&gt;Korean Translation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Portuguese&lt;/td&gt; &#xA;   &lt;td&gt;pt&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pt/README.md&#34;&gt;Portuguese Translation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spanish&lt;/td&gt; &#xA;   &lt;td&gt;es&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/es/README.md&#34;&gt;Spanish Translation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;German&lt;/td&gt; &#xA;   &lt;td&gt;de&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/de/README.md&#34;&gt;German Translation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Persian&lt;/td&gt; &#xA;   &lt;td&gt;fa&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fa/README.md&#34;&gt;Persian Translation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Polish&lt;/td&gt; &#xA;   &lt;td&gt;pl&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pl/README.md&#34;&gt;Polish Translation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2025-03-28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hindi&lt;/td&gt; &#xA;   &lt;td&gt;hi&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hi/README.md&#34;&gt;Hindi Translation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2025-04-05&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üéí Other Courses&lt;/h2&gt; &#xA;&lt;p&gt;Our team produces other courses! Check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst&#34;&gt;&lt;strong&gt;NEW&lt;/strong&gt; Generative AI for Beginners using .NET&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst&#34;&gt;Generative AI for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst&#34;&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst&#34;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst&#34;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung&#34;&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst&#34;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst&#34;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst&#34;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst&#34;&gt;Mastering GitHub Copilot for AI Paired Programming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst&#34;&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst&#34;&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üåü Community Thanks&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://www.linkedin.com/in/shivam2003/&#34;&gt;Shivam Goyal&lt;/a&gt; for contributing important code samples demonstrating Agentic RAG.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos is subject to those third-parties&#39; policies.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>chiphuyen/aie-book</title>
    <updated>2025-05-01T01:47:15Z</updated>
    <id>tag:github.com,2025-05-01:/chiphuyen/aie-book</id>
    <link href="https://github.com/chiphuyen/aie-book" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[WIP] Resources for AI engineers. Also contains supporting materials for the book AI Engineering (Chip Huyen, 2025)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Engineering book and other resources&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;This repo will be updated with more resources in the next few weeks.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chiphuyen/aie-book/main/#about-the-book&#34;&gt;About the book AI Engineering&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chiphuyen/aie-book/main/ToC.md&#34;&gt;Table of contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chiphuyen/aie-book/main/chapter-summaries.md&#34;&gt;Chapter summaries&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chiphuyen/aie-book/main/study-notes.md&#34;&gt;Study notes&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chiphuyen/aie-book/main/resources.md&#34;&gt;AI engineering resources&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chiphuyen/aie-book/main/prompt-examples.md&#34;&gt;Prompt examples&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chiphuyen/aie-book/main/case-studies.md&#34;&gt;Case studies&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chiphuyen/aie-book/main/misalignment.md&#34;&gt;Misalignment AI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chiphuyen/aie-book/main/appendix.md&#34;&gt;Appendix&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Fun tools:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chiphuyen/aie-book/main/scripts/ai-heatmap.ipynb&#34;&gt;ChatGPT and Claude conversation heatmap generator&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;And more ...&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;About the book&lt;/h2&gt; &#xA;&lt;p&gt;The availability of foundation models has transformed AI from a specialized discipline into a powerful development tool everyone can use. This book covers the end-to-end process of adapting foundation models to solve real-world problems, encompassing tried-and-true techniques from other engineering fields and techniques emerging with foundation models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://amzn.to/49j1cGS&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chiphuyen/aie-book/main/assets/aie-cover.png&#34; width=&#34;250&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://amzn.to/49j1cGS&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chiphuyen/aie-book/main/assets/aie-cover-back.png&#34; width=&#34;250&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The book is available on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://amzn.to/49j1cGS&#34;&gt;Amazon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://oreillymedia.pxf.io/c/5719111/2146021/15173&#34;&gt;O&#39;Reilly&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://amzn.to/3Vq2ryu&#34;&gt;Kindle&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;and most places where technical books are sold.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;This is NOT a tutorial book, so it doesn&#39;t have a lot of code snippets.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What this book is about&lt;/h2&gt; &#xA;&lt;p&gt;This book provides a framework for adapting foundation models, which include both large language models (LLMs) and large multimodal models (LMMs), to specific applications. It not only outlines various solutions for building an AI application but also raises questions you can ask to evaluate the best solution for your needs. Here are just some of the many questions that this book can help you answer:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Should I build this AI application?&lt;/li&gt; &#xA; &lt;li&gt;How do I evaluate my application? Can I use AI to evaluate AI outputs?&lt;/li&gt; &#xA; &lt;li&gt;What causes hallucinations? How do I detect and mitigate hallucinations?&lt;/li&gt; &#xA; &lt;li&gt;What are the best practices for prompt engineering?&lt;/li&gt; &#xA; &lt;li&gt;Why does RAG work? What are the strategies for doing RAG?&lt;/li&gt; &#xA; &lt;li&gt;What‚Äôs an agent? How do I build and evaluate an agent?&lt;/li&gt; &#xA; &lt;li&gt;When to finetune a model? When not to finetune a model?&lt;/li&gt; &#xA; &lt;li&gt;How much data do I need? How do I validate the quality of my data?&lt;/li&gt; &#xA; &lt;li&gt;How do I make my model faster, cheaper, and secure?&lt;/li&gt; &#xA; &lt;li&gt;How do I create a feedback loop to improve my application continually?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The book will also help you navigate the overwhelming AI landscape: types of models, evaluation benchmarks, and a seemingly infinite number of use cases and application patterns.&lt;/p&gt; &#xA;&lt;p&gt;The content in this book is illustrated using actual case studies, many of which I‚Äôve worked on, backed by ample references and extensively reviewed by experts from a wide range of backgrounds. Even though the book took two years to write, it draws from my experience working with language models and ML systems from the last decade.&lt;/p&gt; &#xA;&lt;p&gt;Like my previous book, &lt;em&gt;&lt;a href=&#34;https://amzn.to/4fXVZH2&#34;&gt;Designing Machine Learning Systems (DMLS)&lt;/a&gt;&lt;/em&gt;, this book focuses on the fundamentals of AI engineering instead of any specific tool or API. Tools become outdated quickly, but fundamentals should last longer.&lt;/p&gt; &#xA;&lt;h3&gt;Reading &lt;em&gt;AI Engineering&lt;/em&gt; (AIE) with &lt;em&gt;Designing Machine Learning Systems&lt;/em&gt; (DMLS)&lt;/h3&gt; &#xA;&lt;p&gt;AIE can be a companion to DMLS. DMLS focuses on building applications on top of traditional ML models, which involves more tabular data annotations, feature engineering, and model training. AIE focuses on building applications on top of foundation models, which involves more prompt engineering, context construction, and parameter-efficient finetuning. Both books are self-contained and modular, so you can read either book independently.&lt;/p&gt; &#xA;&lt;p&gt;Since foundation models are ML models, some concepts are relevant to working with both. If a topic is relevant to AIE but has been discussed extensively in DMLS, it‚Äôll still be covered in this book, but to a lesser extent, with pointers to relevant resources.&lt;/p&gt; &#xA;&lt;p&gt;Note that many topics are covered in DMLS but not in AIE, and vice versa. The first chapter of this book also covers the differences between traditional ML engineering and AI engineering.&lt;/p&gt; &#xA;&lt;p&gt;A real-world system often involves both traditional ML models and foundation models, so knowledge about working with both is often necessary.&lt;/p&gt; &#xA;&lt;h2&gt;Who this book is for&lt;/h2&gt; &#xA;&lt;p&gt;This book is for anyone who wants to leverage foundation models to solve real-world problems. This is a technical book, so the language of this book is geared towards technical roles, including AI engineers, ML engineers, data scientists, engineering managers, and technical product managers. This book is for you if you can relate to one of the following scenarios:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You‚Äôre building or optimizing an AI application, whether you‚Äôre starting from scratch or looking to move beyond the demo phase into a production-ready stage. You may also be facing issues like hallucinations, security, latency, or costs, and need targeted solutions.&lt;/li&gt; &#xA; &lt;li&gt;You want to streamline your team‚Äôs AI development process, making it more systematic, faster, and reliable.&lt;/li&gt; &#xA; &lt;li&gt;You want to understand how your organization can leverage foundation models to improve the business‚Äôs bottom line and how to build a team to do so.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also benefit from the book if you belong to one of the following groups:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tool developers who want to identify underserved areas in AI engineering to position your products in the ecosystem.&lt;/li&gt; &#xA; &lt;li&gt;Researchers who want to understand better AI use cases.&lt;/li&gt; &#xA; &lt;li&gt;Job candidates seeking clarity on the skills needed to pursue a career as an AI engineer.&lt;/li&gt; &#xA; &lt;li&gt;Anyone wanting to better understand AI&#39;s capabilities and limitations, and how it might affect different roles.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I love getting to the bottom of things, so some sections dive a bit deeper into the technical side. While many early readers like the detail, I know it might not be for everyone. I‚Äôll give you a heads-up before things get too technical. Feel free to skip ahead if it feels a little too in the weeds!&lt;/p&gt; &#xA;&lt;h2&gt;Reviews&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;This book offers a comprehensive, well-structured guide to the essential aspects of building generative AI systems. A must-read for any professional looking to scale AI across the enterprise.&#34;&lt;/em&gt; - Vittorio Cretella, former global CIO at P&amp;amp;G and Mars&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;Chip Huyen gets generative AI. She is a remarkable teacher and writer whose work has been instrumental in helping teams bring AI into production. Drawing on her deep expertise, AI Engineering is a comprehensive and holistic guide to building generative AI applications in production.&#34;&lt;/em&gt; - Luke Metz, co-creator of ChatGPT, ex-research manager @ OpenAI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;Every AI engineer building real-world applications should read this book. It‚Äôs a vital guide to end-to-end AI system design, from model development and evaluation to large-scale deployment and operation.&#34;&lt;/em&gt; - Andrei Lopatenko, Director Search and AI, Neuron7&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;This book serves as an essential guide for building AI products that can scale. Unlike other books that focus on tools or current trends that are constantly changing, Chip delivers timeless foundational knowledge. Whether you&#39;re a product manager or an engineer, this book effectively bridges the collaboration gap between cross-functional teams, making it a must-read for anyone involved in AI development.&#34;&lt;/em&gt; - Aileen Bui, AI Product Operations Manager, Google&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;This is the definitive segue into AI Engineering from one of the greats of ML Engineering! Chip has seen through successful projects and careers at every stage of a company and for the first time ever condensed her expertise for new AI Engineers entering the field.&#34;&lt;/em&gt; - swyx, Curator, AI.Engineer&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;AI Engineering is a practical guide that provides the most up-to-date information on AI development, making it approachable for novice and expert leaders alike. This book is an essential resource for anyone looking to build robust and scalable AI systems.&#34;&lt;/em&gt; - Vicki Reyzelman, Chief AI Solutions Architect, Mave Sparks&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;AI Engineering is a comprehensive guide that serves as an essential reference for both understanding and implementing AI systems in practice.&#34;&lt;/em&gt; - Han Lee, Director - Data Science, Moody&#39;s.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;AI Engineering is an essential guide for anyone building software with Generative AI! It demystifies the technology, highlights the importance of evaluation, and shares what should be done to achieve quality before starting with costly fine-tuning.&#34;&lt;/em&gt; - Rafal Kawala, Senior AI Engineering Director, 16 years of experience working in a Fortune 500 company&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See what people are talking about the book on Twitter &lt;a href=&#34;https://twitter.com/aisysbooks/likes&#34;&gt;@aisysbooks&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;This book would&#39;ve taken a lot longer to write and missed many important topics if it wasn&#39;t for so many wonderful people who helped me through the process.&lt;/p&gt; &#xA;&lt;p&gt;Because the timeline for the project was tight‚Äîtwo years for a 150,000-word book that covers so much ground‚ÄîI&#39;m grateful to the technical reviewers who put aside their precious time to review this book so quickly.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://x.com/luke_metz&#34;&gt;Luke Metz&lt;/a&gt; is an amazing soundboard who checked my assumptions and prevented me from going down the wrong path. &lt;a href=&#34;https://www.linkedin.com/in/hanchunglee/&#34;&gt;Han-chung Lee&lt;/a&gt;, always up to date with the latest AI news and community development, pointed me toward resources that I missed. Luke and Han were the first to review my drafts before I sent them to the next round of technical reviewers, and I&#39;m forever indebted to them for tolerating my follies and mistakes.&lt;/p&gt; &#xA;&lt;p&gt;Having led AI innovation at Fortune 500 companies, &lt;a href=&#34;https://www.linkedin.com/in/vittorio-cretella/&#34;&gt;Vittorio Cretella&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/lopatenko/&#34;&gt;Andrei Lopatenko&lt;/a&gt; provided invaluable feedback that combined deep technical expertise with executive insights. &lt;a href=&#34;https://www.linkedin.com/in/vickireyzelman/&#34;&gt;Vicki Reyzelman&lt;/a&gt; helped me ground my content and keep it relevant for readers with a software engineering background.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://eugeneyan.com/&#34;&gt;Eugene Yan&lt;/a&gt;, a dear friend and amazing applied scientist, provided me with technical and emotional support. Shawn Wang (&lt;a href=&#34;https://x.com/swyx&#34;&gt;swyx&lt;/a&gt;), provided an important vibe check that helped me feel more confident about the book. &lt;a href=&#34;https://x.com/bhutanisanyam1&#34;&gt;Sanyam Bhutani&lt;/a&gt; is one of the best learners and most humble souls I know, who not only gave thoughtful written feedback but also recorded videos to explain his feedback.&lt;/p&gt; &#xA;&lt;p&gt;Kyle Krannen is a star deep learning lead who interviewed his colleagues and shared with me an amazing writeup about their finetuning process, which guided the finetuning chapter. &lt;a href=&#34;https://x.com/marksaroufim&#34;&gt;Mark Saroufim&lt;/a&gt;, an inquisitive mind who always has his pulse on the most interesting problems, introduced me to great resources on efficiency. Both Kyle and Mark&#39;s feedback was critical in writing Chapters 7 and 9.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/kittipat-bot-kampa-1b1965/&#34;&gt;Kittipat &#34;Bot&#34; Kampa&lt;/a&gt;, on top of answering my many questions, shared with me a detailed visualization of how he thinks about AI platform. I appreciate &lt;a href=&#34;https://www.linkedin.com/in/denyslinkov/&#34;&gt;Denys Linkov&lt;/a&gt;&#39;s systematic approach to evaluation and platform development. &lt;a href=&#34;https://www.linkedin.com/in/chetantekur/&#34;&gt;Chetan Tekur&lt;/a&gt; gave great examples that helped me structure AI application patterns. I&#39;d also like to thank &lt;a href=&#34;https://www.linkedin.com/in/findalexli/&#34;&gt;Alex (Shengzhi Li) Li&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/hienluu/&#34;&gt;Hien Luu&lt;/a&gt; for their thoughtful feedback on my draft on AI architecture.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/aileenbui/&#34;&gt;Aileen Bui&lt;/a&gt; is a treasure who shared unique feedback and examples from a product manager&#39;s perspective. Thanks &lt;a href=&#34;https://www.linkedin.com/in/todor-markov-4aa38a67/&#34;&gt;Todor Markov&lt;/a&gt; for the actionable advice on the RAG and Agents chapter. Thanks &lt;a href=&#34;https://www.linkedin.com/in/tal-kachman/&#34;&gt;Tal Kachman&lt;/a&gt; for jumping in at the last minute to push the finetuning chapter over the finish line.&lt;/p&gt; &#xA;&lt;p&gt;There are so many wonderful people whose company and conversations gave me ideas that guide the content of this book. I tried my best to include the names of everyone who has helped me here, but due to the inherent faultiness of human memory, I undoubtedly neglected to mention many. If I forgot to include your name, please know that it wasn&#39;t because I don&#39;t appreciate your contribution, and please kindly remind me so that I can rectify as soon as possible!&lt;/p&gt; &#xA;&lt;p&gt;Andrew Francis, Anish Nag, &lt;a href=&#34;https://www.linkedin.com/in/wgalczak/&#34;&gt;Anthony Galczak&lt;/a&gt;, &lt;a href=&#34;https://x.com/abacaj&#34;&gt;Anton Bacaj&lt;/a&gt;, Bal√°zs Galambosi, Charles Frye, Charles Packer, Chris Brousseau, Eric Hartford, Goku Mohandas, Hamel Husain, Harpreet Sahota, Hassan El Mghari, Huu Nguyen, Jeremy Howard, Jesse Silver, John Cook, &lt;a href=&#34;https://www.linkedin.com/in/juan-pablo-bottaro/&#34;&gt;Juan Pablo Bottaro&lt;/a&gt;, Kyle Gallatin, Lance Martin, Lucio Dery, Matt Ross, Maxime Labonne, Miles Brundage, Nathan Lambert, Omar Khattab, &lt;a href=&#34;https://www.linkedin.com/in/xphongvn/&#34;&gt;Phong Nguyen&lt;/a&gt;, Purnendu Mukherjee, Sam Reiswig, Sebastian Raschka, Shahul ES, Sharif Shameem, Soumith Chintala, Teknium, Tim Dettmers, Undi5, Val Andrei Fajardo, Vern Liang, Victor Sanh, Wing Lian, Xiquan Cui, Ying Sheng, and Kristofer.&lt;/p&gt; &#xA;&lt;p&gt;I&#39;d like to thank all early readers who have also reached out with feedback. Douglas Bailley is a super reader who shared so much thoughtful feedback. Nutan Sahoo for suggesting an elegant way to explain perplexity.&lt;/p&gt; &#xA;&lt;p&gt;I learned so much from the online discussions with so many. Thanks to everyone who&#39;s ever answered my questions, commented on my posts, or sent me an email with your thoughts.&lt;/p&gt; &#xA;&lt;p&gt;Of course, the book wouldn&#39;t have been possible without the team at O&#39;Reilly, especially my development editors (Melissa Potter, Corbin Collins, Jill Leonard) and my production editors (Kristen Brown and Elizabeth Kelly). Liz Wheeler is the most discerning editor I&#39;ve ever worked with. Nicole Butterfield is a force who oversaw this book from an idea to a final product.&lt;/p&gt; &#xA;&lt;p&gt;This book, after all, is an accumulation of invaluable lessons I learned throughout my career. I owe these lessons to my extremely competent and patient coworkers and former coworkers. Every person I&#39;ve worked with has taught me something new about bringing ML into the world.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Chip Huyen, &lt;em&gt;AI Engineering&lt;/em&gt;. O&#39;Reilly Media, 2025.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@book{aiebook2025,  &#xA;    address = {USA},  &#xA;    author = {Chip Huyen},  &#xA;    isbn = {978-1801819312},   &#xA;    publisher = {O&#39;Reilly Media},  &#xA;    title = {{AI Engineering}},  &#xA;    year = {2025}  &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>