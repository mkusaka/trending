<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-01T02:03:53Z</updated>
  <subtitle>Monthly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AccumulateMore/CV</title>
    <updated>2023-11-01T02:03:53Z</updated>
    <id>tag:github.com,2023-11-01:/AccumulateMore/CV</id>
    <link href="https://github.com/AccumulateMore/CV" rel="alternate"></link>
    <summary type="html">&lt;p&gt;✔（已完结）最全面的 深度学习 笔记【土堆 Pytorch】【李沐 动手学深度学习】【吴恩达 深度学习】&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;最全面的 深度学习 笔记&lt;/h1&gt; &#xA;&lt;p&gt;Pytorch 视频讲解【主讲人：土堆】【对应笔记：100-122】&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1hE411t7RN?spm_id_from=333.337.search-card.all.click&#34;&gt;https://www.bilibili.com/video/BV1hE411t7RN?spm_id_from=333.337.search-card.all.click&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;深度学习 视频讲解【主讲人：李沐】【对应笔记：200-268】&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497&#34;&gt;https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;深度学习 视频讲解【主讲人：吴恩达】【对应笔记：300-354】&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=c9745e4447536b28b2b0735071d30bd6&#34;&gt;https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=c9745e4447536b28b2b0735071d30bd6&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;土堆&amp;amp;李沐&amp;amp;吴恩达 数据集：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;链接：&lt;a href=&#34;https://pan.baidu.com/s/1Fjw3o1Jr_-yOabpXyLXLNA&#34;&gt;https://pan.baidu.com/s/1Fjw3o1Jr_-yOabpXyLXLNA&lt;/a&gt; 提取码：eal7&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;备注：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;在线观看笔记时，有时会出现图片（或公式）显示不完整，这是Github网站没有解析好，笔记下载到本地观看就正常了。不会下载笔记的，百度查一下&#34;Github如何下载文件&#34;。&lt;/li&gt; &#xA; &lt;li&gt;笔记是用 Anaconda 的 Jupyter Notebook 打开的，不会打开的，百度查一下&#34;Anaconda如何打开jupyter notebook文件&#34;，或者我的主页Python仓库里面&#34;00_Python编辑器&#34;里面有写。&lt;/li&gt; &#xA; &lt;li&gt;安装 Jupyter Notebook 的目录插件，可以快速通过目录，跳转到相应的章节，如下图所示。不会安装目录的，百度查一下&#34;jupyter notebook如何安装目录&#34;，或者我的主页Python仓库里面&#34;00_Python编辑器&#34;里面有相关链接。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AccumulateMore/CV/assets/60348867/20b6a8e9-ad05-4940-93b9-ab63dc75bb7b&#34; alt=&#34;ae3cce2d56a4953972ed4201c085722&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;补充：&lt;/p&gt; &#xA;&lt;p&gt;我的Github主页，还有其他优秀视频的笔记，希望能帮助到你~&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AccumulateMore&#34;&gt;https://github.com/AccumulateMore&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;我的知乎主页，还有其他领域的笔记，希望能帮助到你~&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/people/bao-bei-ru-huai&#34;&gt;https://www.zhihu.com/people/bao-bei-ru-huai&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;我的哔哩哔哩 （未来更新项目~）&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://space.bilibili.com/402601153?spm_id_from=333.788.0.0&#34;&gt;https://space.bilibili.com/402601153?spm_id_from=333.788.0.0&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&#34;♥我的笔记，希望对你有帮助♥&#34;&lt;/p&gt; &#xA;&lt;p&gt;♥小声哔哔：你的star，是我更新的动力~♥&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;搭建交流群，帮助孤单的自学者交流&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;【深度学习 学习交流①群】&lt;/th&gt; &#xA;   &lt;th&gt;【深度学习 学习交流②群】&lt;/th&gt; &#xA;   &lt;th&gt;微信&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/AccumulateMore/CV/assets/60348867/c99750a2-89c0-45ed-bf42-e8f63a222d60&#34; alt=&#34;312f346ad393a2f617f21da7ffec9d8&#34;&gt;&lt;br&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/AccumulateMore/CV/assets/60348867/d6c44e7b-8349-4de3-b91b-ed62ee7c1544&#34; alt=&#34;2f44c2648aaf04f393162501e9e4e0a&#34;&gt;&lt;br&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/AccumulateMore/CV/assets/60348867/0d3f05f6-ba72-4a1d-9c5b-6d5f0c8eb8f8&#34; alt=&#34;ad9bc1ef4eccf11a0e521dec10968d3&#34;&gt;&lt;br&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;看人之短，天下无一可交之人。看人之长，世间一切皆是吾师。&lt;/p&gt; &#xA;&lt;p&gt;补充说明：本人仅搭建并管理群【发广告踢】，不在群内答疑，群友互相交流答疑。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;帮你们就业，有意向的可以投简历&lt;/p&gt; &#xA;&lt;p&gt;联影医疗 地点（自选）：上海、北京、深圳、武汉、广州、成都、西安、沈阳、三亚等&lt;/p&gt; &#xA;&lt;p&gt;内推人：联影老员工&lt;/p&gt; &#xA;&lt;p&gt;内推入口（校招、社招）：&lt;a href=&#34;https://neitui.italent.cn/united-imaging/sharejobs?shareId=9d052226-87e4-4fb9-942b-f6f83abfb1cd&#34;&gt;https://neitui.italent.cn/united-imaging/sharejobs?shareId=9d052226-87e4-4fb9-942b-f6f83abfb1cd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;岗位职责【图像方向】：&lt;/p&gt; &#xA;&lt;p&gt;参与联影集团产品的AI算法开发工作或图形算法开发工作，负责下列至少一项工作：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;负责提供产品级高端算法解决方案，包括：联影通用软件平台、联影高级应用后处理工作站、联影智能uAl平台、联影机器人手术规划与导航、MR/CT/RT/MI/US事业部的算法需求、科研院所和医院的前瞻性研究项目等；参与创新技术的产品化工作。&lt;/li&gt; &#xA; &lt;li&gt;医学图像3D渲染算法和应用维护与开发，利用图像处理和人工智能技术，从事医学图像处理领域相关算法的设计和研发。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;任职要求：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;熟悉机器视觉原理，熟悉以下领域之一：3D重建、立体视觉、SLAM、通用图像视频分类、目标检测、人体姿态估计。&lt;/li&gt; &#xA; &lt;li&gt;熟悉以下医学图像后处理算法之一：图形算法、重建算法、分割算法、检测算法、分类算法、配准算法、深度学习算法、流体力学算法、灌注算法、纹理分析算法等。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;岗位职责【语音方向】：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;负责音频信号处理和识别相关算法的研究和实现，包含但不限于回声消除、麦克风阵列、盲源分离等技术。&lt;/li&gt; &#xA; &lt;li&gt;负责深度学习、神经网络等AI音频算法的研究和实现。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;任职要求：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;扎实的数学理论及数字信号处理基础，掌握Matlab及C/C++语言编程。&lt;/li&gt; &#xA; &lt;li&gt;有语音、音频信号处理(降噪，回声消除、麦阵、音效等)相关经验。&lt;/li&gt; &#xA; &lt;li&gt;有深度学习、神经网络、智能语音识别相关研究。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;岗位职责【大模型方向】：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;负责大语言模型方面的算法开发、优化、应用落地。&lt;/li&gt; &#xA; &lt;li&gt;负责相应AI解决方案设计，参与关键技术研发，攻关技术难点。&lt;/li&gt; &#xA; &lt;li&gt;负责设计和实现大语言模型相关的算法和模型实现，研究并设计新是算法和模型，解决大语言模型应用问题。&lt;/li&gt; &#xA; &lt;li&gt;负责开发和优化大语言模型的训练过程，设计并实现大语言模型的训练算法和策略，配置和优化训练的超参数和计算资源，保证模型的训练效果和效率。&lt;/li&gt; &#xA; &lt;li&gt;负责构建和管理大规模医疗文本数据集，用于模型预训练和微调，完成不用场景下的下游任务。&lt;/li&gt; &#xA; &lt;li&gt;负责进行大语言模型的评估和验证，设计评估指标和实验;设计和实施评估指标和实验，对训练好的大语言模型进行性能评估和分析。识别模型的弱点和改进空间，提出相应的改进策略和方法。&lt;/li&gt; &#xA; &lt;li&gt;参与高校、科研、医疗机构科研合作，协助科研成果落地转化。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;任职要求：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;具有机器学习、自然语言处理、医学影像分析，或相关领域的学习和研究。&lt;/li&gt; &#xA; &lt;li&gt;有预训练大语言模型或GPT模型等相关研究开发经验。&lt;/li&gt; &#xA; &lt;li&gt;在机器学习(ICML，NeurlPS、ICLR等)、计算机视觉(CVPR、ICCV、ECCV等)、自然语言处理(ACL，EMN LP等)和医疗影像分析(MICCAI、IPMI)等顶级会议，或者顶级期刊(IEEE T-PAMI，IEEE TMI、Medical lmage An alysis)发表过相关论文。&lt;/li&gt; &#xA; &lt;li&gt;具有人工智能相关专业(计算机视觉、机器学习、医疗图像分析等)硕士及以上学位。&lt;/li&gt; &#xA; &lt;li&gt;能熟练使用英语。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;合众汽车（自动驾驶部门） 地点：上海&lt;/p&gt; &#xA;&lt;p&gt;内推人：项目组某leader&lt;/p&gt; &#xA;&lt;p&gt;岗位职责【图像方向】：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;负责智能驾驶检测跟踪及相关方向的算法研发工作。&lt;/li&gt; &#xA; &lt;li&gt;工作内容包括但不限于：基于图像的物体检测跟踪，交通标志/标线检测。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;任职要求：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;熟悉常用的深度学习框架，有实际的深度学习模型设计训练经验。&lt;/li&gt; &#xA; &lt;li&gt;具备扎实的编程能力，熟练掌握C++编程语言。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;备注：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;初学者也可以根据市场上就业需求，去学习自己。&lt;/li&gt; &#xA; &lt;li&gt;能内推简历的，也可以联系我，把岗位职责、任职要求发给我【你收公司内推奖金、ta们就业，双赢】。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;提供毕设、论文指导服务，有完整的论文文档，数据集、和代码可复现，稍加修改可发论文&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;《基于深度学习的疲劳驾驶检测系统》&lt;/p&gt; &lt;p&gt;关键字：疲劳驾驶检测、人脸关键点检测、长宽比优化算法、相机标定、头部姿态估计&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;提供商业级项目，有数据集和代码可复现，可写简历上&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;行人检测&lt;/li&gt; &#xA; &lt;li&gt;车辆检测&lt;/li&gt; &#xA; &lt;li&gt;高速公路 车人摩托车检测 跟踪&lt;/li&gt; &#xA; &lt;li&gt;人脸关键点检测 106点&lt;/li&gt; &#xA; &lt;li&gt;抽烟打电话检测&lt;/li&gt; &#xA; &lt;li&gt;宠物识别&lt;/li&gt; &#xA; &lt;li&gt;情绪7类别识别&lt;/li&gt; &#xA; &lt;li&gt;手势识别 18种手势&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;更多项目，敬请期待..........&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;提供简历指导服务，指导写项目经历（或者说：如何去学习项目）&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AccumulateMore/CV/assets/60348867/5b13bb46-b426-4144-a5fd-340f75f668f3&#34; alt=&#34;UVCkSLQLig&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;以前我毕业时，技能书写，可以借鉴&lt;/p&gt; &#xA;&lt;p&gt;用了几个技巧，分享给大家：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;技能只写掌握了XX，没掌握不要写，掌握且更熟练的放前面，掌握但不熟练的放后面。&lt;/li&gt; &#xA; &lt;li&gt;写掌握某项技能时，后面要论证自己真的掌握了，提供相关经历，方便面试官切入询问。&lt;/li&gt; &#xA; &lt;li&gt;围绕算法岗展开技能阐述，从编程语言、到深度学习框架、到图像处理算法、服务器数据库存储图像等。&lt;/li&gt; &#xA; &lt;li&gt;只写岗位需要的技能，与算法岗不需要的硬件知识、组织能力等不要写，让简历更高匹配算法岗。&lt;/li&gt; &#xA; &lt;li&gt;掌握一种类别技能，只占一行，不要有的两行、有的一行，看着不整齐。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;面试官及HR会看着感觉整齐、可信、岗位匹配度高。&lt;/p&gt; &#xA;&lt;p&gt;&#34;♥君子藏器于身，待时而动……♥&#34;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&#34;♥我们读书是为了成为提灯人去照亮黑暗，而不是为了自己有灯而沾沾自喜还要去吹灭别人的蜡烛♥&#34;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/autogen</title>
    <updated>2023-11-01T02:03:53Z</updated>
    <id>tag:github.com,2023-11-01:/microsoft/autogen</id>
    <link href="https://github.com/microsoft/autogen" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Enable Next-Gen Large Language Model Applications. Join our Discord: https://discord.gg/pAbnFJrkgZ&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/pyautogen&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/pyautogen.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/autogen/actions/workflows/python-package.yml&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/autogen/actions/workflows/python-package.yml/badge.svg?sanitize=true&#34; alt=&#34;Build&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11-blue&#34; alt=&#34;Python Version&#34;&gt; &lt;a href=&#34;https://pepy.tech/project/pyautogen&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/pyautogen/week&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/pAbnFJrkgZ&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1153072414184452236?logo=discord&amp;amp;style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project is a spinoff from &lt;a href=&#34;https://github.com/microsoft/FLAML&#34;&gt;FLAML&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;AutoGen&lt;/h1&gt; &#xA;&lt;!-- &lt;p align=&#34;center&#34;&gt;&#xA;    &lt;img src=&#34;https://github.com/microsoft/autogen/blob/main/website/static/img/flaml.svg&#34;  width=200&gt;&#xA;    &lt;br&gt;&#xA;&lt;/p&gt; --&gt; &#xA;&lt;p&gt;&lt;span&gt;🔥&lt;/span&gt; Heads-up: pyautogen v0.2 will switch to using openai v1.&lt;/p&gt; &#xA;&lt;!--&#xA;&lt;span&gt;🔥&lt;/span&gt; FLAML is highlighted in OpenAI&#39;s [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).&#xA;&#xA;&lt;span&gt;🔥&lt;/span&gt; [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).&#xA;&#xA;&lt;span&gt;🔥&lt;/span&gt; FLAML supports Code-First AutoML &amp; Tuning – Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/). --&gt; &#xA;&lt;h2&gt;What is AutoGen&lt;/h2&gt; &#xA;&lt;p&gt;AutoGen is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/microsoft/autogen/raw/main/website/static/img/autogen_agentchat.png&#34; alt=&#34;AutoGen Overview&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AutoGen enables building next-gen LLM applications based on &lt;strong&gt;multi-agent conversations&lt;/strong&gt; with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.&lt;/li&gt; &#xA; &lt;li&gt;It supports &lt;strong&gt;diverse conversation patterns&lt;/strong&gt; for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy, the number of agents, and agent conversation topology.&lt;/li&gt; &#xA; &lt;li&gt;It provides a collection of working systems with different complexities. These systems span a &lt;strong&gt;wide range of applications&lt;/strong&gt; from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.&lt;/li&gt; &#xA; &lt;li&gt;AutoGen provides &lt;strong&gt;enhanced LLM inference&lt;/strong&gt;. It offers easy performance tuning, plus utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;AutoGen is powered by collaborative &lt;a href=&#34;https://microsoft.github.io/autogen/docs/Research&#34;&gt;research studies&lt;/a&gt; from Microsoft, Penn State University, and the University of Washington.&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to start playing is&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Click below to use the Github Codespace&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://codespaces.new/microsoft/autogen?quickstart=1&#34;&gt;&lt;img src=&#34;https://github.com/codespaces/badge.svg?sanitize=true&#34; alt=&#34;Open in GitHub Codespaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Copy OAI_CONFIG_LIST_sample to /notebook folder, name to OAI_CONFIG_LIST, and set the correct config.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start playing with the notebooks!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;AutoGen requires &lt;strong&gt;Python version &amp;gt;= 3.8&lt;/strong&gt;. It can be installed from pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pyautogen&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Minimal dependencies are installed without extra options. You can install extra options based on the feature you need.&lt;/p&gt; &#xA;&lt;!-- For example, use the following to install the dependencies needed by the [`blendsearch`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#blendsearch-economical-hyperparameter-optimization-with-blended-search-strategy) option.&#xA;```bash&#xA;pip install &#34;pyautogen[blendsearch]&#34;&#xA;``` --&gt; &#xA;&lt;p&gt;Find more options in &lt;a href=&#34;https://microsoft.github.io/autogen/docs/Installation&#34;&gt;Installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- Each of the [`notebook examples`](https://github.com/microsoft/autogen/tree/main/notebook) may require a specific option to be installed. --&gt; &#xA;&lt;p&gt;For &lt;a href=&#34;https://microsoft.github.io/autogen/docs/FAQ/#code-execution&#34;&gt;code execution&lt;/a&gt;, we strongly recommend installing the python docker package, and using docker.&lt;/p&gt; &#xA;&lt;p&gt;For LLM inference configurations, check the &lt;a href=&#34;https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints&#34;&gt;FAQs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Multi-Agent Conversation Framework&lt;/h2&gt; &#xA;&lt;p&gt;Autogen enables the next-gen LLM applications with a generic multi-agent conversation framework. It offers customizable and conversable agents that integrate LLMs, tools, and humans. By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.&lt;/p&gt; &#xA;&lt;p&gt;Features of this use case include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-agent conversations&lt;/strong&gt;: AutoGen agents can communicate with each other to solve tasks. This allows for more complex and sophisticated applications than would be possible with a single LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customization&lt;/strong&gt;: AutoGen agents can be customized to meet the specific needs of an application. This includes the ability to choose the LLMs to use, the types of human input to allow, and the tools to employ.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Human participation&lt;/strong&gt;: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For &lt;a href=&#34;https://github.com/microsoft/autogen/raw/main/test/twoagent.py&#34;&gt;example&lt;/a&gt;,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from autogen import AssistantAgent, UserProxyAgent, config_list_from_json&#xA;# Load LLM inference endpoints from an env variable or a file&#xA;# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints&#xA;# and OAI_CONFIG_LIST_sample&#xA;config_list = config_list_from_json(env_or_file=&#34;OAI_CONFIG_LIST&#34;)&#xA;# You can also set config_list directly as a list, for example, config_list = [{&#39;model&#39;: &#39;gpt-4&#39;, &#39;api_key&#39;: &#39;&amp;lt;your OpenAI API key here&amp;gt;&#39;},]&#xA;assistant = AssistantAgent(&#34;assistant&#34;, llm_config={&#34;config_list&#34;: config_list})&#xA;user_proxy = UserProxyAgent(&#34;user_proxy&#34;, code_execution_config={&#34;work_dir&#34;: &#34;coding&#34;})&#xA;user_proxy.initiate_chat(assistant, message=&#34;Plot a chart of NVDA and TESLA stock price change YTD.&#34;)&#xA;# This initiates an automated chat between the two agents to solve the task&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This example can be run with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python test/twoagent.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the repo is cloned. The figure below shows an example conversation flow with AutoGen. &lt;img src=&#34;https://github.com/microsoft/autogen/raw/main/website/static/img/chat_example.png&#34; alt=&#34;Agent Chat Example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please find more &lt;a href=&#34;https://microsoft.github.io/autogen/docs/Examples/AutoGen-AgentChat&#34;&gt;code examples&lt;/a&gt; for this feature.&lt;/p&gt; &#xA;&lt;h2&gt;Enhanced LLM Inferences&lt;/h2&gt; &#xA;&lt;p&gt;Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalities like tuning, caching, error handling, and templating. For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# perform tuning&#xA;config, analysis = autogen.Completion.tune(&#xA;    data=tune_data,&#xA;    metric=&#34;success&#34;,&#xA;    mode=&#34;max&#34;,&#xA;    eval_func=eval_func,&#xA;    inference_budget=0.05,&#xA;    optimization_budget=3,&#xA;    num_samples=-1,&#xA;)&#xA;# perform inference for a test instance&#xA;response = autogen.Completion.create(context=test_instance, **config)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please find more &lt;a href=&#34;https://microsoft.github.io/autogen/docs/Examples/AutoGen-Inference&#34;&gt;code examples&lt;/a&gt; for this feature.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;You can find detailed documentation about AutoGen &lt;a href=&#34;https://microsoft.github.io/autogen/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In addition, you can find:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://microsoft.github.io/autogen/docs/Research&#34;&gt;Research&lt;/a&gt;, &lt;a href=&#34;https://microsoft.github.io/autogen/blog&#34;&gt;blogposts&lt;/a&gt; around AutoGen, and &lt;a href=&#34;https://github.com/microsoft/autogen/raw/main/TRANSPARENCY_FAQS.md&#34;&gt;Transparency FAQs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://discord.gg/pAbnFJrkgZ&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://microsoft.github.io/autogen/docs/Contribute&#34;&gt;Contributing guide&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/orgs/microsoft/projects/989/views/3&#34;&gt;Roadmap&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.08155&#34;&gt;AutoGen&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{wu2023autogen,&#xA;      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},&#xA;      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Shaokun Zhang and Erkang Zhu and Beibin Li and Li Jiang and Xiaoyun Zhang and Chi Wang},&#xA;      year={2023},&#xA;      eprint={2308.08155},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.04673&#34;&gt;EcoOptiGen&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{wang2023EcoOptiGen,&#xA;    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},&#xA;    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},&#xA;    year={2023},&#xA;    booktitle={AutoML&#39;23},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.01337&#34;&gt;MathChat&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{wu2023empirical,&#xA;    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},&#xA;    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},&#xA;    year={2023},&#xA;    booktitle={ArXiv preprint arXiv:2306.01337},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are new to GitHub &lt;a href=&#34;https://help.github.com/categories/collaborating-with-issues-and-pull-requests/&#34;&gt;here&lt;/a&gt; is a detailed help source on getting involved with development on GitHub.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information, see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors Wall&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/microsoft/autogen/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=microsoft/autogen&#34;&gt; &lt;/a&gt; &#xA;&lt;h1&gt;Legal Notices&lt;/h1&gt; &#xA;&lt;p&gt;Microsoft and any contributors grant you a license to the Microsoft documentation and other content in this repository under the &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;Creative Commons Attribution 4.0 International Public License&lt;/a&gt;, see the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/autogen/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file, and grant you a license to any code in the repository under the &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;MIT License&lt;/a&gt;, see the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/autogen/main/LICENSE-CODE&#34;&gt;LICENSE-CODE&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;Microsoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries. The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks. Microsoft&#39;s general trademark guidelines can be found at &lt;a href=&#34;http://go.microsoft.com/fwlink/?LinkID=254653&#34;&gt;http://go.microsoft.com/fwlink/?LinkID=254653&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Privacy information can be found at &lt;a href=&#34;https://privacy.microsoft.com/en-us/&#34;&gt;https://privacy.microsoft.com/en-us/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents, or trademarks, whether by implication, estoppel, or otherwise.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>iusztinpaul/hands-on-llms</title>
    <updated>2023-11-01T02:03:53Z</updated>
    <id>tag:github.com,2023-11-01:/iusztinpaul/hands-on-llms</id>
    <link href="https://github.com/iusztinpaul/hands-on-llms" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn how to engineer your end-to-end LLM ecosystem: training, streaming, and inference pipelines | deploy &amp; automate | work in progress...&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;Hands-on LLMOps&lt;/h2&gt; &#xA; &lt;h1&gt;Train and Deploy a Real-Time Financial Advisor&lt;/h1&gt; &#xA; &lt;i&gt;by &lt;a href=&#34;https://github.com/iusztinpaul&#34;&gt;Paul Iusztin&lt;/a&gt; and &lt;a href=&#34;https://github.com/Paulescu&#34;&gt;Pau Labarta Bajo&lt;/a&gt;&lt;/i&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/#1-building-blocks&#34;&gt;1. Building Blocks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/#2-setup-external-services&#34;&gt;2. Setup External Services&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/#3-install--usage&#34;&gt;3. Install &amp;amp; Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/#4-video-lectures&#34;&gt;4. Video lectures&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;1. Building Blocks&lt;/h2&gt; &#xA;&lt;h3&gt;Training pipeline&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fine-tune Falcon 7B using our own &lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/modules/q_and_a_dataset_generator/&#34;&gt;Q&amp;amp;A generated dataset&lt;/a&gt; containing investing questions and answers based on Alpaca News. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;It seems that 1 GPU is enough if we use &lt;a href=&#34;https://lightning.ai/pages/blog/falcon-a-guide-to-finetune-and-inference/&#34;&gt;Lit-Parrot&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Real-time data pipeline&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Build real-time feature pipeline, that ingests data form Alpaca, computes embeddings, and stores them into a serverless Vector DB.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Inference pipeline&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; REST API for inference, that &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;receives a question (e.g. &#34;Is it a good time to invest in renewable energy?&#34;),&lt;/li&gt; &#xA;   &lt;li&gt;finds the most relevant documents in the VectorDB (aka context)&lt;/li&gt; &#xA;   &lt;li&gt;sends a prompt with question and context to our fine-tuned Falcon and return response.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/media/architecture.png&#34; alt=&#34;architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2. Setup External Services&lt;/h2&gt; &#xA;&lt;p&gt;Before diving into the modules, you have to set up a couple of additional tools for the course.&lt;/p&gt; &#xA;&lt;h3&gt;2.1. Alpaca&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;financial news data source&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Follow this &lt;a href=&#34;https://alpaca.markets/docs/market-data/getting-started/&#34;&gt;document&lt;/a&gt;, showing you how to create a FREE account, generate the API Keys, and put them somewhere safe.&lt;/p&gt; &#xA;&lt;h3&gt;2.2. Qdrant&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;vector DB&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;https://qdrant.tech/?utm_source=thepauls&amp;amp;utm_medium=partner&amp;amp;utm_content=github&#34;&gt;Qdrant&lt;/a&gt;, create a FREE account, and follow &lt;a href=&#34;https://qdrant.tech/documentation/cloud/authentication/?utm_source=thepauls&amp;amp;utm_medium=partner&amp;amp;utm_content=github&#34;&gt;this document&lt;/a&gt; on how to generate the API Keys.&lt;/p&gt; &#xA;&lt;h3&gt;2.3. Comet ML&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;ML platform&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;https://www.comet.com/signup?utm_source=thepauls&amp;amp;utm_medium=partner&amp;amp;utm_content=github&#34;&gt;Comet ML&lt;/a&gt;, create a FREE account, a project, and an API KEY. We will show you in every module how to add these credentials.&lt;/p&gt; &#xA;&lt;h3&gt;2.4. Beam&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;cloud compute&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;https://www.beam.cloud?utm_source=thepauls&amp;amp;utm_medium=partner&amp;amp;utm_content=github&#34;&gt;Beam&lt;/a&gt; and follow their quick setup/get started tutorial. You must install their CLI and configure your credentials on your local machine.&lt;/p&gt; &#xA;&lt;p&gt;When using Poetry, we had issues locating the Beam CLI when using the Poetry virtual environment. To fix this, create a symlink using the following command - replace &lt;code&gt;&amp;lt;your-poetry-env-name&amp;gt;&lt;/code&gt; with your Poetry env name:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export POETRY_ENV_NAME=&amp;lt;your-poetry-env-name&amp;gt;&#xA; ln -s /usr/local/bin/beam ~/.cache/pypoetry/virtualenvs/${POETRY_ENV_NAME}/bin/beam&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2.5. AWS&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;cloud compute&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;https://aws.amazon.com/console/&#34;&gt;AWS&lt;/a&gt;, create an account, and generate a pair of credentials.&lt;/p&gt; &#xA;&lt;p&gt;After, download and install their &lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html&#34;&gt;AWS CLI&lt;/a&gt; and &lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html&#34;&gt;configure it&lt;/a&gt; with your credentials.&lt;/p&gt; &#xA;&lt;h2&gt;3. Install &amp;amp; Usage&lt;/h2&gt; &#xA;&lt;p&gt;Every module has its dependencies and scripts. In a production setup, every module would have its repository, but in this use case, for learning purposes, we put everything in one place:&lt;/p&gt; &#xA;&lt;p&gt;Thus, check out the README for every module individually to see how to install &amp;amp; use it:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/modules/q_and_a_dataset_generator/&#34;&gt;q_and_a_dataset_generator&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/modules/training_pipeline/&#34;&gt;training_pipeline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/modules/streaming_pipeline/&#34;&gt;streaming_pipeline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/modules/financial_bot/&#34;&gt;inference_pipeline&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;3.1 Run Notebooks Server&lt;/h3&gt; &#xA;&lt;p&gt;If you want to run a notebook server inside a virtual environment, follow the next steps.&lt;/p&gt; &#xA;&lt;p&gt;First, expose the virtual environment as a notebook kernel:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m ipykernel install --user --name hands-on-llms --display-name &#34;hands-on-llms&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now run the notebook server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;jupyter notebook notebooks/ --ip 0.0.0.0 --port 8888&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;4. Video lectures&lt;/h2&gt; &#xA;&lt;h3&gt;4.0 Intro to the course&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.youtube.com/watch?v=l4HTEf0_s70&#34;&gt; &lt;p&gt;Click here to watch the video 🎬&lt;/p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/media/youtube_thumbnails/00_intro.png&#34; alt=&#34;Intro to the course&#34; style=&#34;width:75%;&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;4.1 Fine-tuning our open-source LLM (overview)&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.youtube.com/watch?v=HcxwOYMmj40&#34;&gt; &lt;p&gt;Click here to watch the video 🎬&lt;/p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/media/youtube_thumbnails/01_fine_tuning_pipeline_overview.png&#34; alt=&#34;Intro to the course&#34; style=&#34;width:75%;&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;4.2 Fine-tuning our open-source LLM (Hands-on!)&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.youtube.com/watch?v=RS96R0dH0uE&#34;&gt; &lt;p&gt;Click here to watch the video 🎬&lt;/p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/iusztinpaul/hands-on-llms/main/media/youtube_thumbnails/02_fine_tuning_pipeline_hands_on.png&#34; alt=&#34;Hands-on Fine Tuning an LLM&#34; style=&#34;width:75%;&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>