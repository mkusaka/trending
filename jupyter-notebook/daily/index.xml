<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-03T01:32:58Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tiepvupsu/ebookMLCB</title>
    <updated>2023-11-03T01:32:58Z</updated>
    <id>tag:github.com,2023-11-03:/tiepvupsu/ebookMLCB</id>
    <link href="https://github.com/tiepvupsu/ebookMLCB" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ebook Machine Learning cơ bản&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Mã nguồn cuốn ebook &#34;Machine Learning cơ bản&#34;, Vũ Hữu Tiệp.&lt;/h2&gt; &#xA;&lt;p&gt;ebook Machine Learning cơ bản &lt;a href=&#34;https://github.com/tiepvupsu/ebookMLCB/raw/master/book_ML.pdf&#34;&gt;pdf-black_white&lt;/a&gt;, &lt;a href=&#34;https://github.com/tiepvupsu/ebookMLCB/raw/master/book_ML_color.pdf&#34;&gt;pdf-color&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Mọi hình thức sao chép, in ấn đều cần được sự đồng ý của tác giả. Mọi chia sẻ đều cần được dẫn nguồn tới &lt;a href=&#34;https://github.com/tiepvupsu/ebookMLCB&#34;&gt;https://github.com/tiepvupsu/ebookMLCB&lt;/a&gt; hoặc &lt;a href=&#34;https://machinelearningcoban.com&#34;&gt;https://machinelearningcoban.com&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hiện sách giấy không còn được bán nữa.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Nếu bạn gặp bất cứ lỗi nào hoặc cho rằng nội dung có thể được cải thiện, bạn có thể tạo một issue &lt;a href=&#34;https://github.com/tiepvupsu/ebookMLCB/issues&#34;&gt;tại đây&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Click Star nếu bạn thấy nội dung cuốn sách có ích. Cảm ơn bạn.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google-research/big_vision</title>
    <updated>2023-11-03T01:32:58Z</updated>
    <id>tag:github.com,2023-11-03:/google-research/big_vision</id>
    <link href="https://github.com/google-research/big_vision" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official codebase used to develop Vision Transformer, SigLIP, MLP-Mixer, LiT and more.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Big Vision&lt;/h1&gt; &#xA;&lt;p&gt;This codebase is designed for training large-scale vision models using &lt;a href=&#34;https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms&#34;&gt;Cloud TPU VMs&lt;/a&gt; or GPU machines. It is based on &lt;a href=&#34;https://github.com/google/jax&#34;&gt;Jax&lt;/a&gt;/&lt;a href=&#34;https://github.com/google/flax&#34;&gt;Flax&lt;/a&gt; libraries, and uses &lt;a href=&#34;https://www.tensorflow.org/guide/data&#34;&gt;tf.data&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/datasets&#34;&gt;TensorFlow Datasets&lt;/a&gt; for scalable and reproducible input pipelines.&lt;/p&gt; &#xA;&lt;p&gt;The open-sourcing of this codebase has two main purposes:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Publishing the code of research projects developed in this codebase (see a list below).&lt;/li&gt; &#xA; &lt;li&gt;Providing a strong starting point for running large-scale vision experiments on GPU machines and Google Cloud TPUs, which should scale seamlessly and out-of-the box from a single TPU core to a distributed setup with up to 2048 TPU cores.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;code&gt;big_vision&lt;/code&gt; aims to support research projects at Google. We are unlikely to work on feature requests or accept external contributions, unless they were pre-approved (ask in an issue first). For a well-supported transfer-only codebase, see also &lt;a href=&#34;https://github.com/google-research/vision_transformer&#34;&gt;vision_transformer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;big_vision&lt;/code&gt; is quite dynamic codebase and, while we intend to keep the core code fully-functional at all times, we can not guarantee timely updates of the project-specific code that lives in the &lt;code&gt;.../proj/...&lt;/code&gt; subfolders. However, we provide a &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/#project-specific-commits&#34;&gt;table&lt;/a&gt; with last known commits where specific projects were known to work.&lt;/p&gt; &#xA;&lt;p&gt;The following research projects were originally conducted in the &lt;code&gt;big_vision&lt;/code&gt; codebase:&lt;/p&gt; &#xA;&lt;h3&gt;Architecture research&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34;&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a&gt;, by Alexey Dosovitskiy*, Lucas Beyer*, Alexander Kolesnikov*, Dirk Weissenborn*, Xiaohua Zhai*, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby*&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.04560&#34;&gt;Scaling Vision Transformers&lt;/a&gt;, by Xiaohua Zhai*, Alexander Kolesnikov*, Neil Houlsby, and Lucas Beyer*&lt;br&gt; Resources: &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/configs/proj/scaling_laws/train_vit_g.py&#34;&gt;config&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.10270&#34;&gt;How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers&lt;/a&gt;, by Andreas Steiner*, Alexander Kolesnikov*, Xiaohua Zhai*, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer*&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.01601&#34;&gt;MLP-Mixer: An all-MLP Architecture for Vision&lt;/a&gt;, by Ilya Tolstikhin*, Neil Houlsby*, Alexander Kolesnikov*, Lucas Beyer*, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy&lt;br&gt; Resources: &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/configs/mlp_mixer_i1k.py&#34;&gt;config&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.01580&#34;&gt;Better plain ViT baselines for ImageNet-1k&lt;/a&gt;, by Lucas Beyer, Xiaohua Zhai, Alexander Kolesnikov&lt;br&gt; Resources: &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/configs/vit_s16_i1k.py&#34;&gt;config&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.10337&#34;&gt;UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes&lt;/a&gt;, by Alexander Kolesnikov^&lt;em&gt;, André Susano Pinto^&lt;/em&gt;, Lucas Beyer*, Xiaohua Zhai*, Jeremiah Harmsen*, Neil Houlsby*&lt;br&gt; Resources: &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/configs/proj/uvim/README.md&#34;&gt;readme&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/configs/proj/uvim&#34;&gt;configs&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/configs/proj/uvim&#34;&gt;colabs&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.08013&#34;&gt;FlexiViT: One Model for All Patch Sizes&lt;/a&gt;, by Lucas Beyer*, Pavel Izmailov*, Alexander Kolesnikov*, Mathilde Caron*, Simon Kornblith*, Xiaohua Zhai*, Matthias Minderer*, Michael Tschannen*, Ibrahim Alabdulmohsin*, Filip Pavetic*&lt;br&gt; Resources: &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/configs/proj/flexivit/README.md&#34;&gt;readme&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/configs/proj/flexivit&#34;&gt;configs&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.01327&#34;&gt;Dual PatchNorm&lt;/a&gt;, by Manoj Kumar, Mostafa Dehghani, Neil Houlsby.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.13035&#34;&gt;Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design&lt;/a&gt;, by Ibrahim Alabdulmohsin*, Xiaohua Zhai*, Alexander Kolesnikov, Lucas Beyer*.&lt;/li&gt; &#xA; &lt;li&gt;(partial) &lt;a href=&#34;https://arxiv.org/abs/2302.05442&#34;&gt;Scaling Vision Transformers to 22 Billion Parameters&lt;/a&gt;, by Mostafa Dehghani*, Josip Djolonga*, Basil Mustafa*, Piotr Padlewski*, Jonathan Heek*, &lt;em&gt;wow many middle authors&lt;/em&gt;, Neil Houlsby*.&lt;/li&gt; &#xA; &lt;li&gt;(partial) &lt;a href=&#34;https://arxiv.org/abs/2309.15505&#34;&gt;Finite Scalar Quantization: VQ-VAE Made Simple&lt;/a&gt;, by Fabian Mentzer, David Minnen, Eirikur Agustsson, Michael Tschannen.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Multimodal research&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.07991&#34;&gt;LiT: Zero-Shot Transfer with Locked-image Text Tuning&lt;/a&gt;, by Xiaohua Zhai*, Xiao Wang*, Basil Mustafa*, Andreas Steiner*, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer*&lt;br&gt; Resources: &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/trainers/proj/image_text/contrastive.py&#34;&gt;trainer&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/configs/proj/image_text/lit_coco.py&#34;&gt;config&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/lit.ipynb&#34;&gt;colab&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.08045&#34;&gt;Image-and-Language Understanding from Pixels Only&lt;/a&gt;, by Michael Tschannen, Basil Mustafa, Neil Houlsby&lt;br&gt; Resources: &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/configs/proj/clippo/README.md&#34;&gt;readme&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/configs/proj/clippo/train_clippo.py&#34;&gt;config&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/clippo/clippo_colab.ipynb&#34;&gt;colab&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.15343&#34;&gt;Sigmoid Loss for Language Image Pre-Training&lt;/a&gt;, by Xiaohua Zhai*, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer*&lt;br&gt; Resources: &lt;a href=&#34;https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/SigLIP_demo.ipynb&#34;&gt;colab and models&lt;/a&gt;, code TODO.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.17376&#34;&gt;A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision&lt;/a&gt;, by Lucas Beyer*, Bo Wan*, Gagan Madan*, Filip Pavetic*, Andreas Steiner*, Alexander Kolesnikov, André Susano Pinto, Emanuele Bugliarello, Xiao Wang, Qihang Yu, Liang-Chieh Chen, Xiaohua Zhai*.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.07915&#34;&gt;Image Captioners Are Scalable Vision Learners Too&lt;/a&gt;, by Michael Tschannen*, Manoj Kumar*, Andreas Steiner*, Xiaohua Zhai, Neil Houlsby, Lucas Beyer*.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.16999&#34;&gt;Three Towers: Flexible Contrastive Learning with Pretrained Image Models&lt;/a&gt;, by Jannik Kossen, Mark Collier, Basil Mustafa, Xiao Wang, Xiaohua Zhai, Lucas Beyer, Andreas Steiner, Jesse Berent, Rodolphe Jenatton, Efi Kokiopoulou.&lt;/li&gt; &#xA; &lt;li&gt;(partial) &lt;a href=&#34;https://arxiv.org/abs/2209.06794&#34;&gt;PaLI: A Jointly-Scaled Multilingual Language-Image Model&lt;/a&gt;, by Xi Chen, Xiao Wang, Soravit Changpinyo, &lt;em&gt;wow so many middle authors&lt;/em&gt;, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut.&lt;/li&gt; &#xA; &lt;li&gt;(partial) &lt;a href=&#34;https://arxiv.org/abs/2310.09199&#34;&gt;PaLI-3 Vision Language Models: Smaller, Faster, Stronger&lt;/a&gt;, by Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, Radu Soricut.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.05237&#34;&gt;Knowledge distillation: A good teacher is patient and consistent&lt;/a&gt;, by Lucas Beyer*, Xiaohua Zhai*, Amélie Royer*, Larisa Markeeva*, Rohan Anil, and Alexander Kolesnikov*&lt;br&gt; Resources: &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/configs/proj/distill/README.md&#34;&gt;README&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/trainers/proj/distill/distill.py&#34;&gt;trainer&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/drive/1nMykzUzsfQ_uAxfj3k35DYsATnG_knPl?usp=sharing&#34;&gt;colab&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.01412&#34;&gt;Sharpness-Aware Minimization for Efficiently Improving Generalization&lt;/a&gt;, by Pierre Foret, Ariel Kleiner, Hossein Mobahi, Behnam Neyshabur&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.08065&#34;&gt;Surrogate Gap Minimization Improves Sharpness-Aware Training&lt;/a&gt;, by Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha Dvornek, Sekhar Tatikonda, James Duncan and Ting Liu &lt;br&gt; Resources: &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/trainers/proj/gsam/gsam.py&#34;&gt;trainer&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/configs/proj/gsam/vit_i1k_gsam_no_aug.py&#34;&gt;config&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research/big_vision/pull/8#pullrequestreview-1078557411&#34;&gt;reproduced results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.08242&#34;&gt;Tuning computer vision models with task rewards&lt;/a&gt;, by André Susano Pinto*, Alexander Kolesnikov*, Yuge Shi, Lucas Beyer, Xiaohua Zhai.&lt;/li&gt; &#xA; &lt;li&gt;(partial) &lt;a href=&#34;https://arxiv.org/abs/2211.09760&#34;&gt;VeLO: Training Versatile Learned Optimizers by Scaling Up&lt;/a&gt; by Luke Metz, James Harrison, C. Daniel Freeman, Amil Merchant, Lucas Beyer, James Bradbury, Naman Agrawal, Ben Poole, Igor Mordatch, Adam Roberts, Jascha Sohl-Dickstein.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Misc&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.07159&#34;&gt;Are we done with ImageNet?&lt;/a&gt;, by Lucas Beyer*, Olivier J. Hénaff*, Alexander Kolesnikov*, Xiaohua Zhai*, and Aäron van den Oord*&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Codebase high-level organization and principles in a nutshell&lt;/h1&gt; &#xA;&lt;p&gt;The main entry point is a trainer module, which typically does all the boilerplate related to creating a model and an optimizer, loading the data, checkpointing and training/evaluating the model inside a loop. We provide the canonical trainer &lt;code&gt;train.py&lt;/code&gt; in the root folder. Normally, individual projects within &lt;code&gt;big_vision&lt;/code&gt; fork and customize this trainer.&lt;/p&gt; &#xA;&lt;p&gt;All models, evaluators and preprocessing operations live in the corresponding subdirectories and can often be reused between different projects. We encourage compatible APIs within these directories to facilitate reusability, but it is not strictly enforced, as individual projects may need to introduce their custom APIs.&lt;/p&gt; &#xA;&lt;p&gt;We have a powerful configuration system, with the configs living in the &lt;code&gt;configs/&lt;/code&gt; directory. Custom trainers and modules can directly extend/modify the configuration options.&lt;/p&gt; &#xA;&lt;p&gt;Project-specific code resides in the &lt;code&gt;.../proj/...&lt;/code&gt; namespace. It is not always possible to keep project-specific in sync with the core &lt;code&gt;big_vision&lt;/code&gt; libraries, Below we provide the &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/#project-specific-commits&#34;&gt;last known commit&lt;/a&gt; for each project where the project code is expected to work.&lt;/p&gt; &#xA;&lt;p&gt;Training jobs are robust to interruptions and will resume seamlessly from the last saved checkpoint (assuming a user provides the correct &lt;code&gt;--workdir&lt;/code&gt; path).&lt;/p&gt; &#xA;&lt;p&gt;Each configuration file contains a comment at the top with a &lt;code&gt;COMMAND&lt;/code&gt; snippet to run it, and some hint of expected runtime and results. See below for more details, but generally speaking, running on a GPU machine involves calling &lt;code&gt;python -m COMMAND&lt;/code&gt; while running on TPUs, including multi-host, involves&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gcloud compute tpus tpu-vm ssh $NAME --zone=$ZONE --worker=all&#xA;  --command &#34;bash big_vision/run_tpu.sh COMMAND&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See instructions below for more details on how to run &lt;code&gt;big_vision&lt;/code&gt; code on a GPU machine or Google Cloud TPU.&lt;/p&gt; &#xA;&lt;p&gt;By default we write checkpoints and logfiles. The logfiles are a list of JSON objects, and we provide a short and straightforward &lt;a href=&#34;https://colab.research.google.com/drive/1R_lvV542WUp8Q2y8sbyooZOGCplkn7KI?usp=sharing&#34;&gt;example colab to read and display the logs and checkpoints&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Current and future contents&lt;/h1&gt; &#xA;&lt;p&gt;The first release contains the core part of pre-training, transferring, and evaluating classification models at scale on Cloud TPU VMs.&lt;/p&gt; &#xA;&lt;p&gt;We have since added the following key features and projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Contrastive Image-Text model training and evaluation as in LiT and CLIP.&lt;/li&gt; &#xA; &lt;li&gt;Patient and consistent distillation.&lt;/li&gt; &#xA; &lt;li&gt;Scaling ViT.&lt;/li&gt; &#xA; &lt;li&gt;MLP-Mixer.&lt;/li&gt; &#xA; &lt;li&gt;UViM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Features and projects we plan to release in the near future, in no particular order:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ImageNet-21k in TFDS.&lt;/li&gt; &#xA; &lt;li&gt;Loading misc public models used in our publications (NFNet, MoCov3, DINO).&lt;/li&gt; &#xA; &lt;li&gt;Memory-efficient Polyak-averaging implementation.&lt;/li&gt; &#xA; &lt;li&gt;Advanced JAX compute and memory profiling. We are using internal tools for this, but may eventually add support for the publicly available ones.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We will continue releasing code of our future publications developed within &lt;code&gt;big_vision&lt;/code&gt; here.&lt;/p&gt; &#xA;&lt;h3&gt;Non-content&lt;/h3&gt; &#xA;&lt;p&gt;The following exist in the internal variant of this codebase, and there is no plan for their release:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Regular regression tests for both quality and speed. They rely heavily on internal infrastructure.&lt;/li&gt; &#xA; &lt;li&gt;Advanced logging, monitoring, and plotting of experiments. This also relies heavily on internal infrastructure. However, we are open to ideas on this and may add some in the future, especially if implemented in a self-contained manner.&lt;/li&gt; &#xA; &lt;li&gt;Not yet published, ongoing research projects.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;GPU Setup&lt;/h1&gt; &#xA;&lt;p&gt;We first discuss how to setup and run &lt;code&gt;big_vision&lt;/code&gt; on a (local) GPU machine, and then discuss the setup for Cloud TPUs. Note that data preparation step for (local) GPU setup can be largely reused for the Cloud TPU setup. While the instructions skip this for brevity, we highly recommend using a &lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34;&gt;virtual environment&lt;/a&gt; when installing python dependencies.&lt;/p&gt; &#xA;&lt;h2&gt;Setting up python packages&lt;/h2&gt; &#xA;&lt;p&gt;The first step is to checkout &lt;code&gt;big_vision&lt;/code&gt; and install relevant python dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/google-research/big_vision&#xA;cd big_vision/&#xA;pip3 install --upgrade pip&#xA;pip3 install -r big_vision/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The latest version of &lt;code&gt;jax&lt;/code&gt; library can be fetched as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install --upgrade &#34;jax[cuda]&#34; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may need a different &lt;code&gt;jax&lt;/code&gt; package, depending on CUDA and cuDNN libraries installed on your machine. Please consult &lt;a href=&#34;https://github.com/google/jax#pip-installation-gpu-cuda&#34;&gt;official jax documentation&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Preparing tfds data&lt;/h2&gt; &#xA;&lt;p&gt;For unified and reproducible access to standard datasets we opted to use the &lt;code&gt;tensorflow_datasets&lt;/code&gt; (&lt;code&gt;tfds&lt;/code&gt;) library. It requires each dataset to be downloaded, preprocessed and then to be stored on a hard drive (or, if you use &#34;Google Cloud&#34;, preferably stored in a &#34;GCP bucket&#34;.).&lt;/p&gt; &#xA;&lt;p&gt;Many datasets can be downloaded and preprocessed automatically when used for the first time. Nevertheless, we intentionally disable this feature and recommend doing dataset preparation step separately, ahead of the first run. It will make debugging easier if problems arise and some datasets, like &lt;code&gt;imagenet2012&lt;/code&gt;, require manually downloaded data.&lt;/p&gt; &#xA;&lt;p&gt;Most of the datasets, e.g. &lt;code&gt;cifar100&lt;/code&gt;, &lt;code&gt;oxford_iiit_pet&lt;/code&gt; or &lt;code&gt;imagenet_v2&lt;/code&gt; can be fully automatically downloaded and prepared by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd big_vision/&#xA;python3 -m big_vision.tools.download_tfds_datasets cifar100 oxford_iiit_pet imagenet_v2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A full list of datasets is available at &lt;a href=&#34;https://www.tensorflow.org/datasets/catalog/overview#all_datasets&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Some datasets, like &lt;code&gt;imagenet2012&lt;/code&gt; or &lt;code&gt;imagenet2012_real&lt;/code&gt;, require the data to be downloaded manually and placed into &lt;code&gt;$TFDS_DATA_DIR/downloads/manual/&lt;/code&gt;, which defaults to &lt;code&gt;~/tensorflow_datasets/downloads/manual/&lt;/code&gt;. For example, for &lt;code&gt;imagenet2012&lt;/code&gt; and &lt;code&gt;imagenet2012_real&lt;/code&gt; one needs to place the official &lt;code&gt;ILSVRC2012_img_train.tar&lt;/code&gt; and &lt;code&gt;ILSVRC2012_img_val.tar&lt;/code&gt; files in that directory and then run &lt;code&gt;python3 -m big_vision.tools.download_tfds_datasets imagenet2012 imagenet2012_real&lt;/code&gt; (which may take ~1 hour).&lt;/p&gt; &#xA;&lt;p&gt;If you use &lt;code&gt;Google Cloud&lt;/code&gt; and, TPUs in particular, you can then upload the preprocessed data (stored in &lt;code&gt;$TFDS_DATA_DIR&lt;/code&gt;) to &#34;Google Cloud Bucket&#34; and use the bucket on any of your (TPU) virtual machines to access the data.&lt;/p&gt; &#xA;&lt;h2&gt;Running on a GPU machine&lt;/h2&gt; &#xA;&lt;p&gt;Finally, after installing all python dependencies and preparing &lt;code&gt;tfds&lt;/code&gt; data, the user can run the job using config of their choice, e.g. to train &lt;code&gt;ViT-S/16&lt;/code&gt; model on ImageNet data, one should run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m big_vision.train --config big_vision/configs/vit_s16_i1k.py --workdir workdirs/`date &#39;+%m-%d_%H%M&#39;`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to train MLP-Mixer-B/16, run (note the &lt;code&gt;gpu8&lt;/code&gt; config param that reduces the default batch size and epoch count):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m big_vision.train --config big_vision/configs/mlp_mixer_i1k.py:gpu8 --workdir workdirs/`date &#39;+%m-%d_%H%M&#39;`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Cloud TPU VM setup&lt;/h1&gt; &#xA;&lt;h2&gt;Create TPU VMs&lt;/h2&gt; &#xA;&lt;p&gt;To create a single machine with 8 TPU cores, follow the following Cloud TPU JAX document: &lt;a href=&#34;https://cloud.google.com/tpu/docs/run-calculation-jax&#34;&gt;https://cloud.google.com/tpu/docs/run-calculation-jax&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To support large-scale vision research, more cores with multiple hosts are recommended. Below we provide instructions on how to do it.&lt;/p&gt; &#xA;&lt;p&gt;First, create some useful variables, which we be reused:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export NAME=&#34;a name of the TPU deployment, e.g. my-tpu-machine&#34;&#xA;export ZONE=&#34;GCP geographical zone, e.g. europe-west4-a&#34;&#xA;export GS_BUCKET_NAME=&#34;Name of the storage bucket, e.g. my_bucket&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following command line will create TPU VMs with 32 cores, 4 hosts.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gcloud compute tpus tpu-vm create $NAME --zone $ZONE --accelerator-type v3-32 --version tpu-ubuntu2204-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Install &lt;code&gt;big_vision&lt;/code&gt; on TPU VMs&lt;/h2&gt; &#xA;&lt;p&gt;Fetch the &lt;code&gt;big_vision&lt;/code&gt; repository, copy it to all TPU VM hosts, and install dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/google-research/big_vision&#xA;gcloud compute tpus tpu-vm scp --recurse big_vision/big_vision $NAME: --zone=$ZONE --worker=all&#xA;gcloud compute tpus tpu-vm ssh $NAME --zone=$ZONE --worker=all --command &#34;bash big_vision/run_tpu.sh&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download and prepare TFDS datasets&lt;/h2&gt; &#xA;&lt;p&gt;We recommend preparing &lt;code&gt;tfds&lt;/code&gt; data locally as described above and then uploading the data to &lt;code&gt;Google Cloud&lt;/code&gt; bucket. However, if you prefer, the datasets which do not require manual downloads can be prepared automatically using a TPU machine as described below.&lt;/p&gt; &#xA;&lt;p&gt;Specifically, the seven TFDS datasets used during evaluations will be generated under &lt;code&gt;~/tensorflow_datasets&lt;/code&gt; on TPU machine with this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gcloud compute tpus tpu-vm ssh $NAME --zone=$ZONE --worker=0 --command &#34;TFDS_DATA_DIR=~/tensorflow_datasets bash big_vision/run_tpu.sh big_vision.tools.download_tfds_datasets cifar10 cifar100 oxford_iiit_pet oxford_flowers102 cars196 dtd uc_merced&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then copy the datasets to GS bucket, to make them accessible to all TPU workers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gcloud compute tpus tpu-vm ssh $NAME --zone=$ZONE --worker=0 --command &#34;rm -r ~/tensorflow_datasets/downloads &amp;amp;&amp;amp; gsutil cp -r ~/tensorflow_datasets gs://$GS_BUCKET_NAME&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to integrate other public or custom datasets, i.e. imagenet2012, please follow &lt;a href=&#34;https://www.tensorflow.org/datasets/catalog/overview&#34;&gt;the official guideline&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Pre-trained models&lt;/h2&gt; &#xA;&lt;p&gt;For the full list of pre-trained models check out the &lt;code&gt;load&lt;/code&gt; function defined in the same module as the model code. And for example config on how to use these models, see &lt;code&gt;configs/transfer.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Run the transfer script on TPU VMs&lt;/h2&gt; &#xA;&lt;p&gt;The following command line fine-tunes a pre-trained &lt;code&gt;vit-i21k-augreg-b/32&lt;/code&gt; model on &lt;code&gt;cifar10&lt;/code&gt; dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gcloud compute tpus tpu-vm ssh $NAME --zone=$ZONE --worker=all --command &#34;TFDS_DATA_DIR=gs://$GS_BUCKET_NAME/tensorflow_datasets bash big_vision/run_tpu.sh big_vision.train --config big_vision/configs/transfer.py:model=vit-i21k-augreg-b/32,dataset=cifar10,crop=resmall_crop --workdir gs://$GS_BUCKET_NAME/big_vision/workdir/`date &#39;+%m-%d_%H%M&#39;` --config.lr=0.03&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run the train script on TPU VMs&lt;/h2&gt; &#xA;&lt;p&gt;To train your own big_vision models on a large dataset, e.g. &lt;code&gt;imagenet2012&lt;/code&gt; (&lt;a href=&#34;https://www.tensorflow.org/datasets/catalog/imagenet2012&#34;&gt;prepare the TFDS dataset&lt;/a&gt;), run the following command line.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gcloud compute tpus tpu-vm ssh $NAME --zone=$ZONE --worker=all --command &#34;TFDS_DATA_DIR=gs://$GS_BUCKET_NAME/tensorflow_datasets bash big_vision/run_tpu.sh big_vision.train --config big_vision/configs/bit_i1k.py  --workdir gs://$GS_BUCKET_NAME/big_vision/workdir/`date &#39;+%m-%d_%H%M&#39;`&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FSDP training.&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;big_vision&lt;/code&gt; supports flexible parameter and model sharding strategies. Currently, we support the popular sharding strategy, name FSDP, via a simple config change, see &lt;a href=&#34;https://raw.githubusercontent.com/google-research/big_vision/main/big_vision/configs/transfer.py&#34;&gt;this config example&lt;/a&gt;. For example, to run FSDP finetuning of a pretrained ViT-L model, run the following command (possibly adjusting batch size depending on your hardware):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gcloud compute tpus tpu-vm ssh $NAME --zone=$ZONE --worker=all --command &#34;TFDS_DATA_DIR=gs://$GS_BUCKET_NAME/tensorflow_datasets bash big_vision/run_tpu.sh big_vision.train --config big_vision/configs/transfer.py:model=vit-i21k-augreg-l/16,dataset=oxford_iiit_pet,crop=resmall_crop,fsdp=True,batch_size=256 --workdir gs://$GS_BUCKET_NAME/big_vision/workdir/`date &#39;+%m-%d_%H%M&#39;` --config.lr=0.03&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Sometimes useful gcloud commands&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Destroy the TPU machines: &lt;code&gt;gcloud compute tpus tpu-vm delete $NAME --zone $ZONE&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Remove all big_vision-related folders on all hosts: &lt;code&gt;gcloud compute tpus tpu-vm ssh $NAME --zone $ZONE --worker=all --command &#39;rm -rf ~/big_vision ~/bv_venv&#39;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;ViT baseline&lt;/h1&gt; &#xA;&lt;p&gt;We provide a well-tuned ViT-S/16 baseline in the config file named &lt;code&gt;vit_s16_i1k.py&lt;/code&gt;. It achieves 76.5% accuracy on ImageNet validation split in 90 epochs of training, being a strong and simple starting point for research on the ViT models.&lt;/p&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://arxiv.org/abs/2205.01580&#34;&gt;arXiv note&lt;/a&gt; for more details and if this baseline happens to by useful for your research, consider citing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{vit_baseline,&#xA;  url = {https://arxiv.org/abs/2205.01580},&#xA;  author = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},&#xA;  title = {Better plain ViT baselines for ImageNet-1k},&#xA;  journal={arXiv preprint arXiv:2205.01580},&#xA;  year = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Project specific commits&lt;/h1&gt; &#xA;&lt;p&gt;The last known commit where the specific project code is expected to work. The core code and configs are expected to work at head.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Project&lt;/th&gt; &#xA;   &lt;th&gt;Commit&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UViM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/big_vision/commit/21bd6ebe253f070f584d8b777ad76f4abce51bef&#34;&gt;https://github.com/google-research/big_vision/commit/21bd6ebe253f070f584d8b777ad76f4abce51bef&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;image_text&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/big_vision/commit/8921d5141504390a8a4f7b2dacb3b3c042237290&#34;&gt;https://github.com/google-research/big_vision/commit/8921d5141504390a8a4f7b2dacb3b3c042237290&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;distill&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/big_vision/commit/2f3f493af048dbfd97555ff6060f31a0e686f17f&#34;&gt;https://github.com/google-research/big_vision/commit/2f3f493af048dbfd97555ff6060f31a0e686f17f&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GSAM&lt;/td&gt; &#xA;   &lt;td&gt;WIP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CLIPPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/big_vision/commit/fd2d3bd2efc9d89ea959f16cd2f58ae8a495cd44&#34;&gt;https://github.com/google-research/big_vision/commit/fd2d3bd2efc9d89ea959f16cd2f58ae8a495cd44&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Citing the codebase&lt;/h1&gt; &#xA;&lt;p&gt;If you found this codebase useful for your research, please consider using the following BibTEX to cite it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{big_vision,&#xA;  author = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},&#xA;  title = {Big Vision},&#xA;  year = {2022},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/google-research/big_vision}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;This is not an official Google Product.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Unless explicitly noted otherwise, everything in the big_vision codebase (including models and colabs) is released under the Apache2 license. See the LICENSE file for the full license text.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rajshah4/LLM-Evaluation</title>
    <updated>2023-11-03T01:32:58Z</updated>
    <id>tag:github.com,2023-11-03:/rajshah4/LLM-Evaluation</id>
    <link href="https://github.com/rajshah4/LLM-Evaluation" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Sample notebooks and prompts for LLM evaluation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Resources for Evaluation of LLMs / Generative AI&lt;/h1&gt; &#xA;&lt;p&gt;This repository includes the slides and some of the notebooks that are used in my Evaluation workshops.&lt;/p&gt; &#xA;&lt;p&gt;Some of the notebooks do require an OpenAI API key.&lt;/p&gt; &#xA;&lt;p&gt;These notebooks are intended for explaining key points of the talk, please don&#39;t try to bring them to production use. If you want to dig deeper or have issues, go to the source for each of these projects.&lt;/p&gt; &#xA;&lt;h2&gt;About the workshop&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rajshah4/LLM-Evaluation/main/workshop_one_pager.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Notebook links&lt;/h2&gt; &#xA;&lt;p&gt;Prompting a Chatbot: &lt;a href=&#34;https://colab.research.google.com/github/minimaxir/chatgpt_api_test/blob/main/glados_chatbot.ipynb&#34;&gt;Colab notebook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Testing Properties of a System: &lt;a href=&#34;https://github.com/guidance-ai/guidance/raw/main/notebooks/testing_lms.ipynb&#34;&gt;Guidance AI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thumb: Prompt Testing Libary for LLMs: &lt;a href=&#34;https://github.com/hammer-mt/thumb&#34;&gt;Github&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Langtest tutorials from John Snow Labs: &lt;a href=&#34;http://langtest.org/docs/pages/tutorials/tutorials&#34;&gt;Colab Notebooks&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;LLM Evaluation Harness from EleutherAI: &lt;a href=&#34;https://raw.githubusercontent.com/rajshah4/LLM-Evaluation/main/LLM_evaluation_harness_for_Arc_Easy_and_SST.ipynb&#34;&gt;Github&lt;/a&gt; or &lt;a href=&#34;https://colab.research.google.com/drive/1lPHO8wosT72jkhfBbcESsSD56IvpYk9u#scrollTo=asj6HXacKfc_&#34;&gt;Colab notebook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ragas showing Model as an evaluator: &lt;a href=&#34;https://raw.githubusercontent.com/rajshah4/LLM-Evaluation/main/ragas_quickstart.ipynb&#34;&gt;Github&lt;/a&gt; or &lt;a href=&#34;https://colab.research.google.com/drive/1i78-peTBdhK5y4ZskFzC_NtLRaqvySXM&#34;&gt;Colab notebook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Evaluate LLMs and RAG a practical example using Langchain and Hugging Face: &lt;a href=&#34;https://github.com/philschmid/evaluate-llms/raw/main/notebooks/01-getting-started.ipynb&#34;&gt;Github&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;MLFlow Automated Evaluation: &lt;a href=&#34;https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluation-rag-applications-part&#34;&gt;Blog&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Argilla for Annotation: &lt;a href=&#34;https://huggingface.co/spaces/argilla/llm-eval&#34;&gt;Spaces&lt;/a&gt; login: admin password: 12345678&lt;/p&gt; &#xA;&lt;h2&gt;Presentation Slides&lt;/h2&gt; &#xA;&lt;p&gt;Generative AI Summit, Austin (Oct 2023) - &lt;a href=&#34;https://raw.githubusercontent.com/rajshah4/LLM-Evaluation/main/presentation_slides/EvaluatingLLMs_GenAI_Oct2023_Shah.pdf&#34;&gt;Slides&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ODSC West, San Francisco (Nov 2023) - &lt;a href=&#34;https://raw.githubusercontent.com/rajshah4/LLM-Evaluation/main/presentation_slides/EvaluatingLLMs_ODSC_Nov2023_Shah.pdf&#34;&gt;Slides&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Other Additional Resources&lt;/h2&gt; &#xA;&lt;p&gt;Josh Tobin&#39;s Evaluation talk &lt;a href=&#34;https://youtu.be/r-HUnht-Gns?si=5vU3RzXf7Jkprwn1&#34;&gt;YouTube&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Mahesh Deshwal&#39;s LLM Evaluation &lt;a href=&#34;https://docs.google.com/document/d/1ndYxbN9O7dGKeVXR53B3xHFszniSyho6KLaq-aniDRo/edit#heading=h.j5cyenlrao7z&#34;&gt;Google Doc&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>