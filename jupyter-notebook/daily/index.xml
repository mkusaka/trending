<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-03T01:37:41Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>zgana/fpp3-python-readalong</title>
    <updated>2022-09-03T01:37:41Z</updated>
    <id>tag:github.com,2022-09-03:/zgana/fpp3-python-readalong</id>
    <link href="https://github.com/zgana/fpp3-python-readalong" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python-centered read-along of Forecasting: Principles and Practice&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;fpp3-python-readalong&lt;/h1&gt; &#xA;&lt;p&gt;These notes are a Python-centered read-along of the excellent &lt;a href=&#34;https://otexts.com/fpp3/index.html&#34;&gt;Forecasting: Principles and Practice&lt;/a&gt; by Rob J Hyndman and George Athanasopoulos [1].&lt;/p&gt; &#xA;&lt;p&gt;Please find the &lt;a href=&#34;https://nbviewer.jupyter.org/github/zgana/fpp3-python-readalong/blob/master/Contents.ipynb&#34;&gt;table of contents&lt;/a&gt; on Jupyter nbviewer.&lt;/p&gt; &#xA;&lt;p&gt;[1] Hyndman, R.J., &amp;amp; Athanasopoulos, G. (2019) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 2020-07-20.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>nousr/koi</title>
    <updated>2022-09-03T01:37:41Z</updated>
    <id>tag:github.com,2022-09-03:/nousr/koi</id>
    <link href="https://github.com/nousr/koi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A plug-in for Krita that enables the use of AI models for img2img generation.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;koi üé£&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/nousr/koi/blob/main/koi_colab_backend.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;koi is an open source plug-in that allows you to use AI to accelerate your art workflow!&lt;/p&gt; &#xA;&lt;h4&gt;Disclaimer ‚úã&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;In the interest of getting the open source community on board--I have released this plug-in early. In its current state you may run into issues (particularly during the setup process). If you do, I encourage you to open an issue here on GitHub and describe your problems so that it can be fixed it for you and others!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Overview üòÑ&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The goal of this repository is to serve as a starting point for building increasingly useful tools for artists of all levels of experience to use.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This plug-in serves as a working example of how new A.I. models like Stable Diffusion can lower the barrier of entry to art so that anyone can enjoy making their dreams a reality!&lt;/p&gt; &#xA;&lt;p&gt;Because this is an open source project I encourage you to try it out, break things, and come back with suggestions!&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started üèÅ&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to get started is to follow the plug-in installation process for krita. Then use the google colab backend server &lt;em&gt;(button at the top of this readme)&lt;/em&gt;! This should give you a good introduction to the setup process and get you up and running fast!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installation üî®&lt;/h2&gt; &#xA;&lt;h3&gt;Krita has a few plug-in installation methods, however, I will refer you to the one I use.&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Find your operating system&#39;s &lt;code&gt;pykrita&lt;/code&gt; folder &lt;a href=&#34;https://docs.krita.org/en/reference_manual/resource_management.html#resource-management&#34;&gt;reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Copy the &lt;code&gt;koi&lt;/code&gt; folder, as well as &lt;code&gt;koi.desktop&lt;/code&gt; to &lt;code&gt;pykrita&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Open Krita and navigate to the python plug-in menu &lt;a href=&#34;https://scripting.krita.org/lessons/plugins-introduction&#34;&gt;reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Enable the &lt;code&gt;koi&lt;/code&gt; plugin and restart Krita to load the plug-in.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;The next thing you will need to do is setup the backend server that do all the computation!&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Step 0&lt;/strong&gt;: Ensure you have a GPU-accelerated installation of &lt;code&gt;pytorch&lt;/code&gt;. &lt;em&gt;(You can skip this step if you are using colab or already have it)&lt;/em&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Follow the installation instructions on pytorch&#39;s official &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;getting started&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Get the latest version of huggingface&#39;s &lt;code&gt;diffusers&lt;/code&gt; from source by going to the &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;github repo&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;From here you can clone the repo &lt;code&gt;git clone https://github.com/huggingface/diffusers.git&lt;/code&gt; &amp;amp; &lt;code&gt;cd diffusers&lt;/code&gt; to move into the directory.&lt;/li&gt; &#xA;   &lt;li&gt;Install the package with &lt;code&gt;pip install -e .&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Install this package! I recommend moving out of the diffusers folder if you haven&#39;t already (eg. &lt;code&gt;cd ..&lt;/code&gt;)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;git clone https://github.com/nousr/koi.git&lt;/code&gt;, then &lt;code&gt;cd koi&lt;/code&gt; and &lt;code&gt;pip install -e .&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Note &lt;span&gt;üôã&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;Before continuing, make sure you accept the terms of service for the &lt;code&gt;diffusers&lt;/code&gt; model &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;link to do so here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Next, inside your terminal run the &lt;code&gt;huggingface-cli login&lt;/code&gt; command and paste a token generated from &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;here&lt;/a&gt;. If you don&#39;t want to repeat this step in the future you can then run &lt;code&gt;git config --global credential.helper store&lt;/code&gt;. &lt;em&gt;(note: only do this on a computer you trust)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Run the server by typing &lt;code&gt;python server.py&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you did everything correctly you should see an adress spit out after some time (eg. 127.0.0.1:8888)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Open Krita, if you haven&#39;t already, and paste your address into the &lt;code&gt;endpoint&lt;/code&gt; field of the plugin &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You will also need to append the actual API endpoit you are using. By default this is &lt;code&gt;/api/img2img&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;If you are using all of the default settings your endpoint field will look something like this &lt;code&gt;http://127.0.0.1:8888/api/img2img&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Inference üñåÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;This part is easy!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Step 1: Create a new canvas that is &lt;strong&gt;512 x 512&lt;/strong&gt; (px) in size and make a &lt;strong&gt;single-layer&lt;/strong&gt; sketch &lt;em&gt;(note: these are temporary restrictions)&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Step 2: Fill out the prompt field in the &lt;code&gt;koi&lt;/code&gt; panel (default location is somewhere on the right of your screen).&lt;/li&gt; &#xA; &lt;li&gt;Step 3: Make any additional changes you would like to the inference parameters (strength, steps, etc)&lt;/li&gt; &#xA; &lt;li&gt;Step 4: Copy and paste your server&#39;s endpoint to the associated field&lt;/li&gt; &#xA; &lt;li&gt;Step 5: Click &lt;code&gt;dream&lt;/code&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;FAQ ‚ùî&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;What does &lt;code&gt;koi&lt;/code&gt; stand for?&lt;/strong&gt;&lt;/em&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;Krita Open(source) Img2Img: While support for StableDiffusion is first, the goal is to have this plug-in be compatible with any model!&lt;/em&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Why the client/server setup?&lt;/strong&gt;&lt;/em&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;The goal is to make this as widely available as possible. The server can be run anywhere with a GPU (i.e. colab) and allow those with low-powered hardware to still use the plug-in!&lt;/em&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;TODO:&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add colab backend example&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Refactor code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add DreamStudio API support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add CI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve documentation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add support for arbitrary canvas size &amp;amp; selection-based img2img&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add support for masks&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve UI/UX&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Offer more configuration options&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Abstract away drop-in models for the server&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>iamtrask/Grokking-Deep-Learning</title>
    <updated>2022-09-03T01:37:41Z</updated>
    <id>tag:github.com,2022-09-03:/iamtrask/Grokking-Deep-Learning</id>
    <link href="https://github.com/iamtrask/Grokking-Deep-Learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;this repository accompanies the book &#34;Grokking Deep Learning&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Grokking-Deep-Learning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://floydhub.com/run&#34;&gt;&lt;img src=&#34;https://static.floydhub.com/button/button-small.svg?sanitize=true&#34; alt=&#34;Run on FloydHub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository accompanies the book &#34;Grokking Deep Learning&#34;, &lt;a href=&#34;https://manning.com/books/grokking-deep-learning?a_aid=grokkingdl&amp;amp;a_bid=32715258&#34; title=&#34;Grokking Deep Learning&#34;&gt;available here&lt;/a&gt;. Also, the coupon code &#34;trask40&#34; is good for a 40% discount.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamtrask/Grokking-Deep-Learning/raw/master/Chapter3%20-%20%20Forward%20Propagation%20-%20Intro%20to%20Neural%20Prediction.ipynb&#34;&gt;Chapter 3 - Forward Propagation - Intro to Neural Prediction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamtrask/Grokking-Deep-Learning/raw/master/Chapter4%20-%20Gradient%20Descent%20-%20Intro%20to%20Neural%20Learning.ipynb&#34;&gt;Chapter 4 - Gradient Descent - Into to Neural Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamtrask/Grokking-Deep-Learning/raw/master/Chapter5%20-%20Generalizing%20Gradient%20Descent%20-%20Learning%20Multiple%20Weights%20at%20a%20Time.ipynb&#34;&gt;Chapter 5 - Generalizing Gradient Descent - Learning Multiple Weights at a Time&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamtrask/Grokking-Deep-Learning/raw/master/Chapter6%20-%20Intro%20to%20Backpropagation%20-%20Building%20Your%20First%20DEEP%20Neural%20Network.ipynb&#34;&gt;Chapter 6 - Intro to Backpropagation - Building your first DEEP Neural Network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamtrask/Grokking-Deep-Learning/raw/master/Chapter8%20-%20Intro%20to%20Regularization%20-%20Learning%20Signal%20and%20Ignoring%20Noise.ipynb&#34;&gt;Chapter 8 - Intro to Regularization - Learning Signal and Ignoring Noise&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamtrask/Grokking-Deep-Learning/raw/master/Chapter9%20-%20Intro%20to%20Activation%20Functions%20-%20Modeling%20Probabilities.ipynb&#34;&gt;Chapter 9 - Intro to Activation Functions - Learning to Model Probabilities&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamtrask/Grokking-Deep-Learning/raw/master/Chapter10%20-%20Intro%20to%20Convolutional%20Neural%20Networks%20-%20Learning%20Edges%20and%20Corners.ipynb&#34;&gt;Chapter 10 - Intro to Convolutional Neural Networks - Learning Edges and Corners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamtrask/Grokking-Deep-Learning/raw/master/Chapter11%20-%20Intro%20to%20Word%20Embeddings%20-%20Neural%20Networks%20that%20Understand%20Language.ipynb&#34;&gt;Chapter 11 - Intro to Word Embeddings - Neural Networks which Understand Language&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamtrask/Grokking-Deep-Learning/raw/master/Chapter12%20-%20Intro%20to%20Recurrence%20-%20Predicting%20the%20Next%20Word.ipynb&#34;&gt;Chapter 12 - Intro to Recurrence (RNNs) - Predicting the Next Word&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamtrask/Grokking-Deep-Learning/raw/master/Chapter13%20-%20Intro%20to%20Automatic%20Differentiation%20-%20Let&#39;s%20Build%20A%20Deep%20Learning%20Framework.ipynb&#34;&gt;Chapter 13 - Intro to Automatic Differentiation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamtrask/Grokking-Deep-Learning/raw/master/Chapter14%20-%20Exploding%20Gradients%20Examples.ipynb&#34;&gt;Chapter 14 - Exploding Gradients Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamtrask/Grokking-Deep-Learning/raw/master/Chapter14%20-%20Intro%20to%20LSTMs%20-%20Learn%20to%20Write%20Like%20Shakespeare.ipynb&#34;&gt;Chapter 14 - Intro to LSTMs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamtrask/Grokking-Deep-Learning/raw/master/Chapter14%20-%20Intro%20to%20LSTMs%20-%20Part%202%20-%20Learn%20to%20Write%20Like%20Shakespeare.ipynb&#34;&gt;Chapter 14 - Intro to LSTMs - Part 2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamtrask/Grokking-Deep-Learning/raw/master/Chapter15%20-%20Intro%20to%20Federated%20Learning%20-%20Deep%20Learning%20on%20Unseen%20Data.ipynb&#34;&gt;Chapter 15 - Intro to Federated Learning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>