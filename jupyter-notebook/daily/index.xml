<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-10T01:39:09Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>emmethalm/youtube-to-chatbot</title>
    <updated>2023-06-10T01:39:09Z</updated>
    <id>tag:github.com,2023-06-10:/emmethalm/youtube-to-chatbot</id>
    <link href="https://github.com/emmethalm/youtube-to-chatbot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Train a chatbot on an entire YouTube channel using OpenAI &amp; Pinecone.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;YouTube-to-Chatbot 🤖🎥&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/emmethalm/youtube-to-chatbot/assets/69861956/898523a0-ce60-41b3-8ed5-7bf47aa7771b&#34; alt=&#34;YouTube-to-chatbot logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;👉 &lt;a href=&#34;https://twitter.com/ehalm_/status/1660914850107883520?s=20&#34;&gt;Original launch announcement&lt;/a&gt; 🚀&lt;/h4&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to YouTube-to-Chatbot, a Python notebook that allows you to train a chatbot on an entire YouTube channel. 🌟&lt;/p&gt; &#xA;&lt;p&gt;This repository provides a notebook that utilizes the power of YouTube, OpenAI, Langchain, and Pinecone to build a conversational agent capable of mimicking the content, knowledge, and tone of any YouTube channel. By extracting information from the channel&#39;s videos and training a chatbot, we can create an AI-powered assistant that engages in meaningful conversations with users.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/emmethalm/youtube-to-chatbot/raw/main/images/demo.gif&#34; alt=&#34;YouTube-to-Chatbot Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🧑‍🏫 How it Works&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Start by adding the YouTube ID of the channel you&#39;d like to clone.&lt;/li&gt; &#xA; &lt;li&gt;Obtain API keys for OpenAI, YouTube, and Pinecone.&lt;/li&gt; &#xA; &lt;li&gt;Run each step of the notebook to extract data from YouTube, train the chatbot, and deploy the model.&lt;/li&gt; &#xA; &lt;li&gt;Interact with your newly created chatbot and witness its ability to hold intelligent conversations based on the channel&#39;s content.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;💬 Benefits for Creators and Communities&lt;/h2&gt; &#xA;&lt;p&gt;YouTube-to-Chatbot aims to unlock new possibilities for content creators and foster community growth. With this project, creators can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Provide an interactive and engaging experience for their audience.&lt;/li&gt; &#xA; &lt;li&gt;Offer personalized recommendations and responses to viewers.&lt;/li&gt; &#xA; &lt;li&gt;Automate routine tasks such as answering frequently asked questions.&lt;/li&gt; &#xA; &lt;li&gt;Expand their reach by enabling chatbot interactions across various platforms.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🎯 Early Access and Contributions&lt;/h2&gt; &#xA;&lt;p&gt;To get early access to new features and updates, follow me at &lt;a href=&#34;https://twitter.com/ehalm_&#34;&gt;@ehalm_&lt;/a&gt; on Twitter. This is just the start! If you are a creator interested in a custom model or a developer eager to contribute to this project, feel free to shoot me a DM.&lt;/p&gt; &#xA;&lt;h2&gt;🏁 Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To start using YouTube-to-Chatbot, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository to your local machine.&lt;/li&gt; &#xA; &lt;li&gt;Access the notebook &lt;a href=&#34;https://colab.research.google.com/github/emmethalm/youtube-to-chatbot/blob/main/YouTube_to_chatbot_notebook.ipynb&#34;&gt;here&lt;/a&gt; to run it on Google Colab.&lt;/li&gt; &#xA; &lt;li&gt;Make sure you have the necessary API keys and permissions.&lt;/li&gt; &#xA; &lt;li&gt;Fill in the required information in the notebook, such as the YouTube ID and API keys.&lt;/li&gt; &#xA; &lt;li&gt;Run each step in the notebook to train and deploy your chatbot.&lt;/li&gt; &#xA; &lt;li&gt;Engage in conversations with your AI assistant and explore its capabilities!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Let&#39;s empower creators and revolutionize community engagement together! 🚀✨&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>shibing624/MedicalGPT</title>
    <updated>2023-06-10T01:39:09Z</updated>
    <id>tag:github.com,2023-06-10:/shibing624/MedicalGPT</id>
    <link href="https://github.com/shibing624/MedicalGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline. 训练医疗大模型，实现包括二次预训练、有监督微调、奖励建模、强化学习训练。&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/README.md&#34;&gt;&lt;strong&gt;🇨🇳中文&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/README_EN.md&#34;&gt;&lt;strong&gt;🌐English&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/shibing624/MedicalGPT/wiki&#34;&gt;&lt;strong&gt;📖文档/Docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/shibing624&#34;&gt;&lt;strong&gt;🤖模型/Models&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/shibing624/MedicalGPT&#34;&gt; &lt;img src=&#34;https://github.com/shibing624/MedicalGPT/raw/main/docs/logo.png&#34; height=&#34;100&#34; alt=&#34;Logo&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;MedicalGPT: Training Medical GPT Model&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/shibing624&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hugging%20Face-shibing624-green&#34; alt=&#34;HF Models&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://star-history.com/#shibing624/MedicalGPT&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/shibing624/MedicalGPT?color=yellow&#34; alt=&#34;Github Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/shibing624/MedicalGPT/main/CONTRIBUTING.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/contributions-welcome-brightgreen.svg?sanitize=true&#34; alt=&#34;Contributions welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/shibing624/MedicalGPT/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License Apache 2.0&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/shibing624/MedicalGPT/main/requirements.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.8%2B-green.svg?sanitize=true&#34; alt=&#34;python_version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/shibing624/MedicalGPT/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/shibing624/MedicalGPT.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/shibing624/MedicalGPT/main/#Contact&#34;&gt;&lt;img src=&#34;http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20&#34; alt=&#34;Wechat Group&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📖 Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedicalGPT&lt;/strong&gt; training medical GPT model with ChatGPT training pipeline, implemantation of Pretraining, Supervised Finetuning, Reward Modeling and Reinforcement Learning.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedicalGPT&lt;/strong&gt; 训练医疗大模型，实现包括二次预训练、有监督微调、奖励建模、强化学习训练。&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/shibing624/MedicalGPT/raw/main/docs/GPT_Training.jpg&#34; width=&#34;860&#34;&gt; &#xA;&lt;p&gt;分四阶段训练GPT模型，来自Andrej Karpathy的演讲PDF&lt;a href=&#34;https://karpathy.ai/stateofgpt.pdf&#34;&gt;State of GPT&lt;/a&gt;，视频&lt;a href=&#34;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&#34;&gt;Video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;😊 Feature&lt;/h2&gt; &#xA;&lt;p&gt;基于ChatGPT Training Pipeline，本项目实现了领域模型--医疗模型的四阶段训练：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文档数据上二次预训练GPT模型，以注入领域知识&lt;/li&gt; &#xA; &lt;li&gt;第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图&lt;/li&gt; &#xA; &lt;li&gt;第三阶段：RM(Reward Model)奖励模型建模，构造人类偏好排序数据集，训练奖励模型，用来对齐人类偏好，主要是&#34;HHH&#34;原则，具体是&#34;helpful, honest, harmless&#34;&lt;/li&gt; &#xA; &lt;li&gt;第四阶段：RL(Reinforcement Learning)基于人类反馈的强化学习(RLHF)，用奖励模型来训练SFT模型，生成模型使用奖励或惩罚来更新其策略，以便生成更高质量、更符合人类偏好的文本&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🔥 Release Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Base Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Introduction&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/shibing624/ziya-llama-13b-medical-lora&#34;&gt;shibing624/ziya-llama-13b-medical-lora&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1&#34;&gt;IDEA-CCNL/Ziya-LLaMA-13B-v1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;在240万条中英文医疗数据集&lt;a href=&#34;https://huggingface.co/datasets/shibing624/medical&#34;&gt;shibing624/medical&lt;/a&gt;上SFT微调了一版Ziya-LLaMA-13B模型，医疗问答效果有提升，发布微调后的LoRA权重&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;▶️ Demo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hugging Face Demo: doing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;我们提供了一个简洁的基于gradio的交互式web界面，启动服务后，可通过浏览器访问，输入问题，模型会返回答案。&lt;/p&gt; &#xA;&lt;p&gt;启动服务，命令如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python scripts/gradio_demo.py --model_type base_model_type --base_model path_to_llama_hf_dir --lora_model path_to_lora_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;参数说明：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type {base_model_type}&lt;/code&gt;：预训练模型类型，如llama、bloom、chatglm等&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--base_model {base_model}&lt;/code&gt;：存放HF格式的LLaMA模型权重和配置文件的目录，也可使用HF Model Hub模型调用名称&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--lora_model {lora_model}&lt;/code&gt;：LoRA文件所在目录，也可使用HF Model Hub模型调用名称。若lora权重已经合并到预训练模型，则删除--lora_model参数&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--tokenizer_path {tokenizer_path}&lt;/code&gt;：存放对应tokenizer的目录。若不提供此参数，则其默认值与--base_model相同&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--use_cpu&lt;/code&gt;: 仅使用CPU进行推理&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--gpus {gpu_ids}&lt;/code&gt;: 指定使用的GPU设备编号，默认为0。如使用多张GPU，以逗号分隔，如0,1,2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🚀 Training Pipeline&lt;/h2&gt; &#xA;&lt;p&gt;Training Stage:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Stage&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Introduction&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Notebook&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Open In Colab&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Python script&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Shell script&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Stage 1: Continue Pretraining&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;增量预训练&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/notebook/run_pretraining.ipynb&#34;&gt;run_pretraining.ipynb&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/notebook/run_pretraining.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/scripts/pretraining.py&#34;&gt;pretraining.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/scripts/run_pt.sh&#34;&gt;run_pt.sh&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Stage 2: Supervised Fine-tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;有监督微调&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/notebook/run_supervised_finetuning.ipynb&#34;&gt;run_supervised_finetuning.ipynb&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/notebook/run_supervised_finetuning.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/scripts/supervised_finetuning.py&#34;&gt;supervised_finetuning.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/scripts/run_sft.sh&#34;&gt;run_sft.sh&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Stage 3: Reward Modeling&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;奖励模型建模&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/notebook/run_reward_modeling.ipynb&#34;&gt;run_reward_modeling.ipynb&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/notebook/run_reward_modeling.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/scripts/reward_modeling.py&#34;&gt;reward_modeling.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/scripts/run_rm.sh&#34;&gt;run_rm.sh&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Stage 4: Reinforcement Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;强化学习&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/notebook/run_rl_training.ipynb&#34;&gt;run_rl_training.ipynb&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/notebook/run_rl_training.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/scripts/rl_training.py&#34;&gt;rl_training.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/scripts/run_rl.sh&#34;&gt;run_rl.sh&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/wiki/%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82%E8%AF%B4%E6%98%8E&#34;&gt;训练参数说明wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;💻 Inference&lt;/h2&gt; &#xA;&lt;p&gt;训练完成后，现在我们加载训练好的模型，验证模型生成文本的效果。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python scripts/inference.py \&#xA;    --model_type base_model_type \&#xA;    --base_model path_to_llama_hf_dir \&#xA;    --lora_model path_to_lora \&#xA;    --with_prompt \&#xA;    --interactive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;参数说明：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type {base_model_type}&lt;/code&gt;：预训练模型类型，如llama、bloom、chatglm等&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--base_model {base_model}&lt;/code&gt;：存放HF格式的LLaMA模型权重和配置文件的目录&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--lora_model {lora_model}&lt;/code&gt;：LoRA解压后文件所在目录，也可使用HF Model Hub模型调用名称。如果已经合并了LoRA权重到预训练模型，则可以不提供此参数&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--tokenizer_path {tokenizer_path}&lt;/code&gt;：存放对应tokenizer的目录。若不提供此参数，则其默认值与--base_model相同&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--with_prompt&lt;/code&gt;：是否将输入与prompt模版进行合并。如果加载Alpaca模型，请务必启用此选项！&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--interactive&lt;/code&gt;：以交互方式启动，以便进行多次单轮问答&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--data_file {file_name}&lt;/code&gt;：非交互方式启动下，按行读取file_name中的的内容进行预测&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--predictions_file {file_name}&lt;/code&gt;：非交互式方式下，将预测的结果以json格式写入file_name&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--use_cpu&lt;/code&gt;: 仅使用CPU进行推理&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--gpus {gpu_ids}&lt;/code&gt;: 指定使用的GPU设备编号，默认为0。如使用多张GPU，以逗号分隔，如0,1,2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Inference Examples&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/shibing624/ziya-llama-13b-medical-lora&#34;&gt;shibing624/ziya-llama-13b-medical-lora&lt;/a&gt; inference examples:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Human&lt;/th&gt; &#xA;   &lt;th&gt;Assitant&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;小孩发烧怎么办&lt;/td&gt; &#xA;   &lt;td&gt;宝宝的抵抗力是比较差的，所以很容易就会出现感冒或者其他疾病。如果宝宝在平时没有注意保暖，那么也可能导致身体受凉而引起了感冒等症状，这样不仅对于健康造成影响，还可能让宝宝出现高热的情况。因此当家长们发现自己的宝宝出现了高热的问题之后一定要及早进行治疗和护理才好。&lt;br&gt;1、首先我们应该给宝宝服用退烧药物来帮助降温，但是千万不要盲目使用退烧药物，否则只会加重宝宝的病情，最好带着宝宝到医院去检查清楚具体原因之后再针对性地采取措施处理。&lt;br&gt;2、另外大人们也需要做好预防工作，尤其是天气变化的时候更应该多关心下宝宝的身体状态，避免宝宝受寒受凉从而诱发各种疾病，同时也要尽量减少宝宝接触细菌病毒的机会，增强宝宝的免疫力。&lt;br&gt;3、除了上面两点之外，大人们还可以通过按摩推拿穴位的方式来缓解宝宝的病痛，促进血液循环，提升新陈代谢速度，达到快速消炎止疼的效果&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;一岁宝宝发烧能吃啥药？&lt;/td&gt; &#xA;   &lt;td&gt;孩子的身体是很脆弱的，如果不注意保护好自己就会出现各种疾病。有些家长在看到孩子生病后都非常着急，想要立刻给他们用药治疗，但其实并没有那么简单，因为每个人对于药物的敏感度和耐受性都是不同的，所以我们需要根据具体情况来选择合适的方法进行治疗才可以达到最佳效果。&lt;br&gt;1、首先大多数小儿退热药中含有解热镇痛成分阿司匹林或布洛芬等，这类药品虽然副作用较少，但也存在过量使用引起胃肠道损伤甚至消化道溃疡出血的风险；&lt;br&gt;2、还有部分小儿退热药中含有扑尔敏等抗组胺药，它们本身属于处方药，应该严格按照医嘱服用，切勿擅自加减剂量或者延长用药时间;&lt;br&gt;3、此外，某些小儿退热药中含有维生素C及葡萄糖酸钙等营养补充剂，只有当患儿伴随脱水症状（如口渴、尿色深）时才考虑添加，否则容易造成电解质紊乱。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;肛门病变可能是什么疾病的症状&lt;/td&gt; &#xA;   &lt;td&gt;痔疮、直肠息肉等。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医生，您好白带增多泛黄怎么回事？&lt;/td&gt; &#xA;   &lt;td&gt;你这种情况考虑可能会是阴道炎症引起的建议平时留意卫生清洁，勤换洗衣物内裤，必要时去正规公立医院妇科复诊做分泌物常规等相关检查明确病因后对症治疗，不同原因所致的疾病在临床表现上也存在差异性，如果患者出现了类似于的症状应当及早到专业、权威的医院进行详细的检验和救治&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;药流后有巧克力白带是没流干净吗&lt;/td&gt; &#xA;   &lt;td&gt;在日常生活中，我们要或多或少了解一些常见疾病的相关知识，不仅能起到增加知识水平的作用，更能让自己更好地预防此类疾病的产生。很多女性都会出现月经紊乱、腹痛等症状，这时候就需要考虑妇科炎症可能，比如说宫颈糜烂和盆腔积液。那么下面小编来给大家介绍一种情况：药物流产后白带呈巧克力色怎么回事？&lt;br&gt;1、首先，对于药物流产之后发现阴道分泌物异常，应该及时去医院做检查确诊具体原因，然后再根据结果进行治疗调理。&lt;br&gt;2、其次，由于每个人身体素质以及患病程度存在差别，所以采取的措施也不尽相同，但无论选择哪种方式处理，都必须严格按照规定剂量服用，并且注意观察副作用反映情况，若出现明显不适感觉，则应立即停止使用。&lt;br&gt;3、最后，建议你保持外阴清洁卫生，勤换内裤，避免吃辛辣刺激食品，而且还要禁忌房事，否则容易引起交叉感染影响恢复健康。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;维胺酯维E乳膏能治理什么疾病&lt;/td&gt; &#xA;   &lt;td&gt;皮肤淀粉样变性；脂溢性角化症&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;痤疮是什么原因导致的?&lt;/td&gt; &#xA;   &lt;td&gt;痤疮，又称青春痘、粉刺。它主要发生在面部和胸背部位，表现为黑头粉刺、白头粉刺、丘疹、脓疱、结节等多种形式。其中以黑头粉刺最常见，约占全身皮肤病患者的70%左右。那么，痤疮是什么原因引起的呢？下面就给大家介绍一些相关内容。&lt;br&gt;1、雄性激素分泌过旺：男女都会长痘痘，但是女孩子比较喜欢用化妆品来掩盖脸上的问题，而且很多人不知道这样做反而加重了自己的毛囊炎症状，所以说平时应该少使用含有雌激素类的产品。&lt;br&gt;2、饮食习惯差：经常吃辛辣油腻的东西或者甜食也可能造成体内毒素堆积，从而诱发痤疮。&lt;br&gt;3、压力太大：如果你总是处于紧张焦虑的情绪之中，同时还没有及时调整好心态，那么很容易出现内分泌失调的情况，进而影响到肌肤健康。&lt;br&gt;4、睡眠质量差：熬夜对我们的身体伤害非常严重，尤其是对于年轻人群更甚，他们需要保证充足的睡眠才行，否则很容易让肌肤变得暗沉无光泽。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;脸上长青春痘怎么办,能吃啥药,需要注意啥&lt;/td&gt; &#xA;   &lt;td&gt;你好，根据您的描述考虑是痤疮。建议外用维胺酯乳膏或者阿达帕林凝胶进行治疗看看，平时禁忌辛辣和刺激性食物，如海鲜、狗肉、羊肉等“发”动品，以及葱蒜韭姜等刺激性调料。多喝水，少吃油腻高热量食物。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;由于输精管堵塞会不会导致男性不育？，医生您好，我同事这几天不开心，我问她怎么了，她说她老公输精管堵塞，所以一直没孩子，不是不想要，请问输精管堵塞会不会导致男性不育？&lt;/td&gt; &#xA;   &lt;td&gt;你好！根据你的叙述及症状推测：如果有输精管阻塞或者狭窄等情况都可能会引来无法排出来精液而影响怀上。建议到正规专业的快孕医院实施进行咨询和治疗为宜。对于输精管患者来说，及时救治是可以减低病发，如果不及时救治会影响到受孕情况。因此患者要及时药物救治，并且日常生活中要多看重自身护理&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;心肌梗塞能治疗吗？&lt;/td&gt; &#xA;   &lt;td&gt;心脏病是一种危害性极大的疾病，我们要对它有足够多的了解。在生活中很多人都会患上这个疾病，而且还不知道怎么去预防和护理，所以就导致自己的身体越来越差，那么下面小编给你介绍几点关于心脏病的常识吧！&lt;br&gt;1、冠状动脉粥样硬化性心脏病：又称为冠心病或缺血型心脏病（IHD）,是世界各地最重要的死亡原因之一。&lt;br&gt;2、风湿热：主要侵犯心脏瓣膜，可引起二尖瓣狭窄及其他类型的心脏瓣膜病变；&lt;br&gt;3、先天性心脏病：包括房间隔缺损、室间隔缺损等；&lt;br&gt;4、高脂蛋白血症：由于胆固醇与低密度脂蛋白升高，使得动脉壁增厚，管腔缩小，易形成斑块并脱落造成急性栓塞；&lt;br&gt;5、肥胖：过量摄入食物，特别是含糖饮料，加速脂肪堆积，促进动脉粥样硬化发展；&lt;br&gt;6、吸烟：尼古丁刺激交感神经兴奋，使外周阻力增强，收缩压升高，舒张期减短，从而降低左右心室射血分数，诱发心律失常；&lt;br&gt;7、酗酒：长期嗜酒者，肝功受到影响，代谢紊乱，容易出现脂质代谢异常。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;怎么让自己精力充沛，列5点建议&lt;/td&gt; &#xA;   &lt;td&gt;1、保证睡眠质量：每天晚上7-8小时的高质量睡眠是提升身体和大脑活跃度最有效的方法。&lt;br&gt;2、合理饮食：均衡营养摄入对于维持人体正常运转至关重要，多吃蔬菜水果，少油腻辛辣刺激性食物。&lt;br&gt;3、适当锻炼：坚持一定强度的锻炼可以促进血液循环，增加肌肉耐力，改善心肺功能等。&lt;br&gt;4、学习新知识：不断接触新事物，开拓视野，思考问题，都会使得我们更具创造力与灵感。&lt;br&gt;5、放松心情：通过冥想或者瑜伽来缓解压力，也可以听音乐或看电影来调节情绪。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;写个春天的诗&lt;/td&gt; &#xA;   &lt;td&gt;春风吹拂，花儿绽放。&lt;br&gt;阳光明媚，鸟语鸣叫。&lt;br&gt;树木抽芽，草地变绿。&lt;br&gt;万物复苏，生机勃发。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;南极洲为啥没有北极熊?&lt;/td&gt; &#xA;   &lt;td&gt;南极是世界上最大的冰盖，覆盖着98％以上的地球表面积。它位于南纬60度至74度之间，环绕在海洋中心周围。由于其独特的气候条件和生态系统，南极被称为“真正的荒野”。南极洲拥有丰富多样的动植物资源，但这里并不存在北极熊。 &lt;br&gt;那么，为什么南极洲没有北极熊呢？&lt;br&gt;首先，我们需要了解一下两者栖息地的差异。北极熊主要分布在北半球寒冷的高纬度地区，如加拿大、格陵兰岛等地。而南极洲则处于南半球，属于温带或寒带气候类型。虽然南极洲也有很低的平均气温（-25℃左右），但与北极相比还是太热了。因此，即使北极熊能够适应更严酷的气候条件，它们也无法在南极找到合适的栖息地。&lt;br&gt;另外，南极洲缺乏陆地哺乳动物食物来源，包括鱼类、鲸鱼和企鹅等。尽管南极洲的水域中也有各种鱼类，但数量远少于北极圈内。&lt;br&gt;同时，南极洲的土著居民——企鹅群体繁殖季节期间会消耗掉大部分可用的食物资源，导致当地的鱼类数量减少甚至枯竭。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;📚 Dataset&lt;/h2&gt; &#xA;&lt;h3&gt;医疗数据集&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;240万条中文医疗数据集(包括预训练、指令微调和奖励数据集)：&lt;a href=&#34;https://huggingface.co/datasets/shibing624/medical&#34;&gt;shibing624/medical&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;22万条中文医疗对话数据集(华佗项目)：&lt;a href=&#34;https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1&#34;&gt;FreedomIntelligence/HuatuoGPT-sft-data-v1&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;通用数据集&lt;/h3&gt; &#xA;&lt;h4&gt;SFT datasets&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;50万条中文ChatGPT指令Belle数据集：&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_0.5M_CN&#34;&gt;BelleGroup/train_0.5M_CN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;100万条中文ChatGPT指令Belle数据集：&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_1M_CN&#34;&gt;BelleGroup/train_1M_CN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;5万条英文ChatGPT指令Alpaca数据集：&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca#data-release&#34;&gt;50k English Stanford Alpaca dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;2万条中文ChatGPT指令Alpaca数据集：&lt;a href=&#34;https://huggingface.co/datasets/shibing624/alpaca-zh&#34;&gt;shibing624/alpaca-zh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;69万条中文指令Guanaco数据集(Belle50万条+Guanaco19万条)：&lt;a href=&#34;https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0&#34;&gt;Chinese-Vicuna/guanaco_belle_merge_v1.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;5万条英文ChatGPT多轮对话数据集：&lt;a href=&#34;https://huggingface.co/datasets/RyokoAI/ShareGPT52K&#34;&gt;RyokoAI/ShareGPT52K&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;80万条中文ChatGPT多轮对话数据集：&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M&#34;&gt;BelleGroup/multiturn_chat_0.8M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;116万条中文ChatGPT多轮对话数据集：&lt;a href=&#34;https://huggingface.co/datasets/fnlp/moss-002-sft-data&#34;&gt;fnlp/moss-002-sft-data&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Reward Model datasets&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;原版的oasst1数据集：&lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1&#34;&gt;OpenAssistant/oasst1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;2万条多语言oasst1的reward数据集：&lt;a href=&#34;https://huggingface.co/datasets/tasksource/oasst1_pairwise_rlhf_reward&#34;&gt;tasksource/oasst1_pairwise_rlhf_reward&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;11万条英文hh-rlhf的reward数据集：&lt;a href=&#34;https://huggingface.co/datasets/Dahoas/full-hh-rlhf&#34;&gt;Dahoas/full-hh-rlhf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;9万条英文reward数据集(来自Anthropic&#39;s Helpful Harmless dataset)：&lt;a href=&#34;https://huggingface.co/datasets/Dahoas/static-hh&#34;&gt;Dahoas/static-hh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;7万条英文reward数据集（来源同上）：&lt;a href=&#34;https://huggingface.co/datasets/Dahoas/rm-static&#34;&gt;Dahoas/rm-static&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;7万条繁体中文的reward数据集（翻译自rm-static）&lt;a href=&#34;https://huggingface.co/datasets/liswei/rm-static-m2m100-zh&#34;&gt;liswei/rm-static-m2m100-zh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;7万条英文Reward数据集：&lt;a href=&#34;https://huggingface.co/datasets/yitingxie/rlhf-reward-datasets&#34;&gt;yitingxie/rlhf-reward-datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;3千条中文知乎问答偏好数据集：&lt;a href=&#34;https://huggingface.co/datasets/liyucheng/zhihu_rlhf_3k&#34;&gt;liyucheng/zhihu_rlhf_3k&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;✅ Todo&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; add multi-round dialogue data fine-tuning method&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add reward model fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add rl fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add medical reward dataset&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add llama in8/int4 training&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add all training and predict demo in colab&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;☎️ Contact&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Issue(建议) ：&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/shibing624/MedicalGPT.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;邮件我：xuming: &lt;a href=&#34;mailto:xuming624@qq.com&#34;&gt;xuming624@qq.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;微信我： 加我&lt;em&gt;微信号：xuming624, 备注：姓名-公司名-NLP&lt;/em&gt; 进NLP交流群。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://github.com/shibing624/MedicalGPT/raw/main/docs/wechat.jpeg&#34; width=&#34;200&#34;&gt; &#xA;&lt;h2&gt;⚠️ 局限性、使用限制与免责声明&lt;/h2&gt; &#xA;&lt;p&gt;基于当前数据和基础模型训练得到的SFT模型，在效果上仍存在以下问题：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;在涉及事实性的指令上可能会产生违背事实的错误回答。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;对于具备危害性的指令无法很好的鉴别，由此会产生危害性言论。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;在一些涉及推理、代码、多轮对话等场景下模型的能力仍有待提高。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;基于以上模型局限性，我们要求开发者仅将我们开源的模型权重及后续用此项目生成的衍生物用于研究目的，不得用于商业，以及其他会对社会带来危害的用途。&lt;/p&gt; &#xA;&lt;p&gt;本项目仅可应用于研究目的，项目开发者不承担任何因使用本项目（包含但不限于数据、模型、代码等）导致的危害或损失。详细请参考&lt;a href=&#34;https://github.com/shibing624/MedicalGPT/raw/main/DISCLAIMER&#34;&gt;免责声明&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;项目代码的授权协议为 &lt;a href=&#34;https://raw.githubusercontent.com/shibing624/MedicalGPT/main/LICENSE&#34;&gt;The Apache License 2.0&lt;/a&gt;，代码可免费用做商业用途，模型权重和数据只能用于研究目的。请在产品说明中附加MedicalGPT的链接和授权协议。&lt;/p&gt; &#xA;&lt;h2&gt;😇 Citation&lt;/h2&gt; &#xA;&lt;p&gt;如果你在研究中使用了MedicalGPT，请按如下格式引用：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@misc{MedicalGPT,&#xA;  title={MedicalGPT: Training Medical GPT Model},&#xA;  author={Ming Xu},&#xA;  year={2023},&#xA;  howpublished={\url{https://github.com/shibing624/MedicalGPT}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;😍 Contribute&lt;/h2&gt; &#xA;&lt;p&gt;项目代码还很粗糙，如果大家对代码有所改进，欢迎提交回本项目，在提交之前，注意以下两点：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;在&lt;code&gt;tests&lt;/code&gt;添加相应的单元测试&lt;/li&gt; &#xA; &lt;li&gt;使用&lt;code&gt;python -m pytest&lt;/code&gt;来运行所有单元测试，确保所有单测都是通过的&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;之后即可提交PR。&lt;/p&gt; &#xA;&lt;h2&gt;💕 Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tloen/alpaca-lora/raw/main/finetune.py&#34;&gt;tloen/alpaca-lora&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;ymcui/Chinese-LLaMA-Alpaca&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks for their great work!&lt;/p&gt;</summary>
  </entry>
</feed>