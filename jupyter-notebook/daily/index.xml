<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-07T01:34:39Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lastmile-ai/aiconfig</title>
    <updated>2023-12-07T01:34:39Z</updated>
    <id>tag:github.com,2023-12-07:/lastmile-ai/aiconfig</id>
    <link href="https://github.com/lastmile-ai/aiconfig" rel="alternate"></link>
    <summary type="html">&lt;p&gt;aiconfig -- config-driven, source control friendly AI application development&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;picture&gt; &#xA;  &lt;img alt=&#34;aiconfig&#34; src=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/aiconfig-docs/static/img/readme_logo.png&#34;&gt; &#xA; &lt;/picture&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lastmile-ai/aiconfig/actions/workflows/main_python.yml/badge.svg?sanitize=true&#34; alt=&#34;Python&#34;&gt; &lt;img src=&#34;https://github.com/lastmile-ai/aiconfig/actions/workflows/main-typescript.yml/badge.svg?sanitize=true&#34; alt=&#34;Node&#34;&gt; &lt;img src=&#34;https://github.com/lastmile-ai/aiconfig/actions/workflows/test-deploy-docs.yml/badge.svg?sanitize=true&#34; alt=&#34;Docs&#34;&gt; &lt;a href=&#34;https://discord.gg/qMqgzDae&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-LastMile%20AI-Blue?color=rgb(37%2C%20150%2C%20190)&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Full documentation: &lt;strong&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/&#34;&gt;aiconfig.lastmileai.dev&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;AIConfig saves prompts, models and model parameters as source control friendly configs. This allows you to iterate on prompts and model parameters &lt;em&gt;separately from your application code&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompts as configs&lt;/strong&gt;: a &lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/ai-config-format&#34;&gt;standardized JSON format&lt;/a&gt; to store generative AI model settings, prompt inputs/outputs, and flexible metadata.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model-agnostic SDK&lt;/strong&gt;: Python &amp;amp; Node SDKs to use &lt;code&gt;aiconfig&lt;/code&gt; in your application code. AIConfig is designed to be &lt;strong&gt;model-agnostic&lt;/strong&gt; and &lt;strong&gt;multi-modal&lt;/strong&gt;, so you can extend it to work with any generative AI model, including text, image and audio.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AI Workbook editor&lt;/strong&gt;: A &lt;a href=&#34;https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9&#34;&gt;notebook-like playground&lt;/a&gt; to edit &lt;code&gt;aiconfig&lt;/code&gt; files visually, run prompts, tweak models and model settings, and chain things together.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;What problem it solves&lt;/h3&gt; &#xA;&lt;p&gt;Today, application code is tightly coupled with the gen AI settings for the application -- prompts, parameters, and model-specific logic is all jumbled in with app code.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;results in increased complexity&lt;/li&gt; &#xA; &lt;li&gt;makes it hard to iterate on the prompts or try different models easily&lt;/li&gt; &#xA; &lt;li&gt;makes it hard to evaluate prompt/model performance&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;AIConfig helps unwind complexity by separating prompts, model parameters, and model-specific logic from your application.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;simplifies application code -- simply call &lt;code&gt;config.run()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;open the &lt;code&gt;aiconfig&lt;/code&gt; in a playground to iterate quickly&lt;/li&gt; &#xA; &lt;li&gt;version control and evaluate the &lt;code&gt;aiconfig&lt;/code&gt; - it&#39;s the AI artifact for your application.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/aiconfig-docs/static/img/aiconfig_dataflow.png&#34; alt=&#34;AIConfig flow&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Quicknav&lt;/h3&gt; &#xA;&lt;ul style=&#34;margin-bottom:0; padding-bottom:0;&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/#install&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;ul style=&#34;margin-bottom:0; padding-bottom:0;&#34;&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig&#34;&gt;Create an AIConfig&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig&#34;&gt;Run a prompt&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/parameters&#34;&gt;Pass data into prompts&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain&#34;&gt;Prompt Chains&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig&#34;&gt;Callbacks and monitoring&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/#aiconfig-sdk&#34;&gt;SDK Cheatsheet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/#cookbooks&#34;&gt;Cookbooks and guides&lt;/a&gt;&lt;/li&gt; &#xA; &lt;ul style=&#34;margin-bottom:0; padding-bottom:0;&#34;&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT&#34;&gt;CLI Chatbot&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig&#34;&gt;RAG with AIConfig&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing&#34;&gt;Prompt routing&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI&#34;&gt;OpenAI function calling&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification&#34;&gt;Chain of Verification&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/#supported-models&#34;&gt;Supported models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;ul style=&#34;margin-bottom:0; padding-bottom:0;&#34;&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama&#34;&gt;LLaMA2 example&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace&#34;&gt;Hugging Face (Mistral-7B) example&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency&#34;&gt;PaLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/#extensibility&#34;&gt;Extensibility&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/#contributing-to-aiconfig&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/#faqs&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Source-control friendly&lt;/strong&gt; &lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/ai-config-format&#34;&gt;&lt;code&gt;aiconfig&lt;/code&gt; format&lt;/a&gt; to save prompts and model settings, which you can use for evaluation, reproducibility and simplifying your application code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Multi-modal and model agnostic&lt;/strong&gt;. Use with any model, and serialize/deserialize data with the same &lt;code&gt;aiconfig&lt;/code&gt; format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Prompt chaining and parameterization&lt;/strong&gt; with &lt;a href=&#34;https://handlebarsjs.com/&#34;&gt;{{handlebars}}&lt;/a&gt; templating syntax, allowing you to pass dynamic data into prompts (as well as between prompts).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Streaming&lt;/strong&gt; supported out of the box, allowing you to get playground-like streaming wherever you use &lt;code&gt;aiconfig&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Notebook editor&lt;/strong&gt;. &lt;a href=&#34;https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9&#34;&gt;AI Workbooks editor&lt;/a&gt; to visually create your &lt;code&gt;aiconfig&lt;/code&gt;, and use the SDK to connect it to your application code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Install with your favorite package manager for Node or Python.&lt;/p&gt; &#xA;&lt;h3&gt;Node.js&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;code&gt;npm&lt;/code&gt; or &lt;code&gt;yarn&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install aiconfig&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yarn add aiconfig&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;code&gt;pip3&lt;/code&gt; or &lt;code&gt;poetry&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install python-aiconfig&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry add python-aiconfig&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/getting-started/#installation&#34;&gt;Detailed installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Set your OpenAI API Key&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Make sure to specify the API keys (such as &lt;a href=&#34;https://platform.openai.com/api-keys&#34;&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/a&gt;) in your environment before proceeding.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;In your CLI, set the environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=my_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We cover Python instructions here, for Node.js please see the &lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/getting-started&#34;&gt;detailed Getting Started guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;In this quickstart, you will create a customizable NYC travel itinerary using &lt;code&gt;aiconfig&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This AIConfig contains a prompt chain to get a list of travel activities from an LLM and then generate an itinerary in an order specified by the user.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Link to tutorial code: &lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Getting-Started&#34;&gt;here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66&#34;&gt;https://github.com/lastmile-ai/aiconfig/assets/81494782/805173d1-0f83-44c5-b570-c776bb7dba66&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Download &lt;code&gt;travel.aiconfig.json&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Don&#39;t worry if you don&#39;t understand all the pieces of this yet, we&#39;ll go over it step by step.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;name&#34;: &#34;NYC Trip Planner&#34;,&#xA;  &#34;description&#34;: &#34;Intrepid explorer with ChatGPT and AIConfig&#34;,&#xA;  &#34;schema_version&#34;: &#34;latest&#34;,&#xA;  &#34;metadata&#34;: {&#xA;    &#34;models&#34;: {&#xA;      &#34;gpt-3.5-turbo&#34;: {&#xA;        &#34;model&#34;: &#34;gpt-3.5-turbo&#34;,&#xA;        &#34;top_p&#34;: 1,&#xA;        &#34;temperature&#34;: 1&#xA;      },&#xA;      &#34;gpt-4&#34;: {&#xA;        &#34;model&#34;: &#34;gpt-4&#34;,&#xA;        &#34;max_tokens&#34;: 3000,&#xA;        &#34;system_prompt&#34;: &#34;You are an expert travel coordinator with exquisite taste.&#34;&#xA;      }&#xA;    },&#xA;    &#34;default_model&#34;: &#34;gpt-3.5-turbo&#34;&#xA;  },&#xA;  &#34;prompts&#34;: [&#xA;    {&#xA;      &#34;name&#34;: &#34;get_activities&#34;,&#xA;      &#34;input&#34;: &#34;Tell me 10 fun attractions to do in NYC.&#34;&#xA;    },&#xA;    {&#xA;      &#34;name&#34;: &#34;gen_itinerary&#34;,&#xA;      &#34;input&#34;: &#34;Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.&#34;,&#xA;      &#34;metadata&#34;: {&#xA;        &#34;model&#34;: &#34;gpt-4&#34;,&#xA;        &#34;parameters&#34;: {&#xA;          &#34;order_by&#34;: &#34;geographic location&#34;&#xA;        }&#xA;      }&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run the &lt;code&gt;get_activities&lt;/code&gt; prompt.&lt;/h3&gt; &#xA;&lt;p&gt;You don&#39;t need to worry about how to run inference for the model; it&#39;s all handled by AIConfig. The prompt runs with gpt-3.5-turbo since that is the &lt;code&gt;default_model&lt;/code&gt; for this AIConfig.&lt;/p&gt; &#xA;&lt;p&gt;Create a new file called &lt;code&gt;app.py&lt;/code&gt; and and enter the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from aiconfig import AIConfigRuntime, InferenceOptions&#xA;&#xA;async def main():&#xA;  # Load the aiconfig&#xA;  config = AIConfigRuntime.load(&#39;travel.aiconfig.json&#39;)&#xA;&#xA;  # Run a single prompt (with streaming)&#xA;  inference_options = InferenceOptions(stream=True)&#xA;  await config.run(&#34;get_activities&#34;, options=inference_options)&#xA;&#xA;asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now run this in your terminal with the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run the &lt;code&gt;gen_itinerary&lt;/code&gt; prompt.&lt;/h3&gt; &#xA;&lt;p&gt;In your &lt;code&gt;app.py&lt;/code&gt; file, change the last line to below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;await config.run(&#34;gen_itinerary&#34;, params=None, options=inference_options)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Re-run the command in your terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This prompt depends on the output of &lt;code&gt;get_activities&lt;/code&gt;. It also takes in parameters (user input) to determine the customized itinerary.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s take a closer look:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;gen_itinerary&lt;/code&gt; prompt:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;Generate an itinerary ordered by {{order_by}} for these activities: {{get_activities.output}}.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;prompt metadata:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;metadata&#34;: {&#xA;    &#34;model&#34;: &#34;gpt-4&#34;,&#xA;    &#34;parameters&#34;: {&#xA;      &#34;order_by&#34;: &#34;geographic location&#34;&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Observe the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The prompt depends on the output of the &lt;code&gt;get_activities&lt;/code&gt; prompt.&lt;/li&gt; &#xA; &lt;li&gt;It also depends on an &lt;code&gt;order_by&lt;/code&gt; parameter (using {{handlebars}} syntax)&lt;/li&gt; &#xA; &lt;li&gt;It uses &lt;strong&gt;gpt-4&lt;/strong&gt;, whereas the &lt;code&gt;get_activities&lt;/code&gt; prompt it depends on uses &lt;strong&gt;gpt-3.5-turbo&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Effectively, this is a prompt chain between &lt;code&gt;gen_itinerary&lt;/code&gt; and &lt;code&gt;get_activities&lt;/code&gt; prompts, &lt;em&gt;as well as&lt;/em&gt; as a model chain between &lt;strong&gt;gpt-3.5-turbo&lt;/strong&gt; and &lt;strong&gt;gpt-4&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Let&#39;s run this with AIConfig:&lt;/p&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;config.run&lt;/code&gt; above with this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;await config.run(&#34;gen_itinerary&#34;, params={&#34;order_by&#34;: &#34;duration&#34;}, options=inference_options, run_with_dependencies=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice how simple the syntax is to perform a fairly complex task - running 2 different prompts across 2 different models and chaining one&#39;s output as part of the input of another.&lt;/p&gt; &#xA;&lt;p&gt;The code will just run &lt;code&gt;get_activities&lt;/code&gt;, then pipe its output as an input to &lt;code&gt;gen_itinerary&lt;/code&gt;, and finally run &lt;code&gt;gen_itinerary&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Save the AIConfig&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s save the AIConfig back to disk, and serialize the outputs from the latest inference run as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Save the aiconfig to disk. and serialize outputs from the model run&#xA;config.save(&#39;updated.aiconfig.json&#39;, include_outputs=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Edit &lt;code&gt;aiconfig&lt;/code&gt; in a notebook editor&lt;/h3&gt; &#xA;&lt;p&gt;We can iterate on an &lt;code&gt;aiconfig&lt;/code&gt; using a notebook-like editor called an &lt;strong&gt;AI Workbook&lt;/strong&gt;. Now that we have an &lt;code&gt;aiconfig&lt;/code&gt; file artifact that encapsulates the generative AI part of our application, we can iterate on it separately from the application code that uses it.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go to &lt;a href=&#34;https://lastmileai.dev&#34;&gt;https://lastmileai.dev&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Go to Workbooks page: &lt;a href=&#34;https://lastmileai.dev/workbooks&#34;&gt;https://lastmileai.dev/workbooks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Click dropdown from &#39;+ New Workbook&#39; and select &#39;Create from AIConfig&#39;&lt;/li&gt; &#xA; &lt;li&gt;Upload &lt;code&gt;travel.aiconfig.json&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e&#34;&gt;https://github.com/lastmile-ai/aiconfig/assets/81494782/5d901493-bbda-4f8e-93c7-dd9a91bf242e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Try out the workbook playground here: &lt;strong&gt;&lt;a href=&#34;https://lastmileai.dev/workbooks/clooqs3p200kkpe53u6n2rhr9&#34;&gt;NYC Travel Workbook&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;We are working on a local editor that you can run yourself. For now, please use the hosted version on &lt;a href=&#34;https://lastmileai.dev&#34;&gt;https://lastmileai.dev&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Additional Guides&lt;/h3&gt; &#xA;&lt;p&gt;There is a lot you can do with &lt;code&gt;aiconfig&lt;/code&gt;. We have several other tutorials to help get you started:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig&#34;&gt;Create an AIConfig from scratch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/run-aiconfig&#34;&gt;Run a prompt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/parameters&#34;&gt;Pass data into prompts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/define-prompt-chain&#34;&gt;Prompt chains&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig&#34;&gt;Callbacks and monitoring&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here are some example uses:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT&#34;&gt;CLI Chatbot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig&#34;&gt;RAG with AIConfig&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing&#34;&gt;Prompt routing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI&#34;&gt;OpenAI function calling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification&#34;&gt;Chain of thought&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;OpenAI Introspection API&lt;/h3&gt; &#xA;&lt;p&gt;If you are already using OpenAI completion API&#39;s in your application, you can get started very quickly to start saving the messages in an &lt;code&gt;aiconfig&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Usage: see openai_wrapper.ipynb.&lt;/p&gt; &#xA;&lt;p&gt;Now you can continue using &lt;code&gt;openai&lt;/code&gt; completion API as normal. When you want to save the config, just call &lt;code&gt;new_config.save()&lt;/code&gt; and all your openai completion calls will get serialized to disk.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/create-an-aiconfig#openai-api-python-wrapper&#34;&gt;&lt;strong&gt;Detailed guide here&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;AIConfig supports the following models out of the box:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI chat models (GPT-3, GPT-3.5, GPT-4)&lt;/li&gt; &#xA; &lt;li&gt;LLaMA2 (running locally)&lt;/li&gt; &#xA; &lt;li&gt;Google PaLM models (PaLM chat)&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face text generation models (e.g. Mistral-7B)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI&#34;&gt;OpenAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama&#34;&gt;LLaMA example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace&#34;&gt;Hugging Face (Mistral-7B) example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency&#34;&gt;PaLM&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you need to use a model that isn&#39;t provided out of the box, you can implement a &lt;code&gt;ModelParser&lt;/code&gt; for it (see &lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/#extending-aiconfig&#34;&gt;Extending AIConfig&lt;/a&gt;). &lt;strong&gt;We welcome &lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/contributing&#34;&gt;contributions&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;AIConfig Schema&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/ai-config-format&#34;&gt;AIConfig specification&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;AIConfig SDK&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Read the &lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/usage-guide&#34;&gt;Usage Guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The AIConfig SDK supports CRUD operations for prompts, models, parameters and metadata. Here are some common examples.&lt;/p&gt; &#xA;&lt;p&gt;The root interface is the &lt;code&gt;AIConfigRuntime&lt;/code&gt; object. That is the entrypoint for interacting with an AIConfig programmatically.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s go over a few key CRUD operations to give a glimpse.&lt;/p&gt; &#xA;&lt;h3&gt;AIConfig &lt;code&gt;create&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;config = AIConfigRuntime.create(&#34;aiconfig name&#34;, &#34;description&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Prompt &lt;code&gt;resolve&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;resolve&lt;/code&gt; deserializes an existing &lt;code&gt;Prompt&lt;/code&gt; into the data object that its model expects.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;config.resolve(&#34;prompt_name&#34;, params)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;params&lt;/code&gt; are overrides you can specify to resolve any &lt;code&gt;{{handlebars}}&lt;/code&gt; templates in the prompt. See the &lt;code&gt;gen_itinerary&lt;/code&gt; prompt in the Getting Started example.&lt;/p&gt; &#xA;&lt;h3&gt;Prompt &lt;code&gt;serialize&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;serialize&lt;/code&gt; is the inverse of &lt;code&gt;resolve&lt;/code&gt; -- it serializes the data object that a model understands into a &lt;code&gt;Prompt&lt;/code&gt; object that can be serialized into the &lt;code&gt;aiconfig&lt;/code&gt; format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;config.serialize(&#34;model_name&#34;, data, &#34;prompt_name&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Prompt &lt;code&gt;run&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;run&lt;/code&gt; is used to run inference for the specified &lt;code&gt;Prompt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;config.run(&#34;prompt_name&#34;, params)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;run_with_dependencies&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This is a variant of &lt;code&gt;run&lt;/code&gt; -- this re-runs all prompt dependencies. For example, in &lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/#download-travelaiconfigjson&#34;&gt;&lt;code&gt;travel.aiconfig.json&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;gen_itinerary&lt;/code&gt; prompt references the output of the &lt;code&gt;get_activities&lt;/code&gt; prompt using &lt;code&gt;{{get_activities.output}}&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Running this function will first execute &lt;code&gt;get_activities&lt;/code&gt;, and use its output to resolve the &lt;code&gt;gen_itinerary&lt;/code&gt; prompt before executing it. This is transitive, so it computes the Directed Acyclic Graph of dependencies to execute. Complex relationships can be modeled this way.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;config.run_with_dependencies(&#34;gen_itinerary&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Updating metadata and parameters&lt;/h3&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;get/set_metadata&lt;/code&gt; and &lt;code&gt;get/set_parameter&lt;/code&gt; methods to interact with metadata and parameters (&lt;code&gt;set_parameter&lt;/code&gt; is just syntactic sugar to update &lt;code&gt;&#34;metadata.parameters&#34;&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;config.set_metadata(&#34;key&#34;, data, &#34;prompt_name&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: if &lt;code&gt;&#34;prompt_name&#34;&lt;/code&gt; is specified, the metadata is updated specifically for that prompt. Otherwise, the global metadata is updated.&lt;/p&gt; &#xA;&lt;h3&gt;Register new &lt;code&gt;ModelParser&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;AIConfigRuntime.register_model_parser&lt;/code&gt; if you want to use a different &lt;code&gt;ModelParser&lt;/code&gt;, or configure AIConfig to work with an additional model.&lt;/p&gt; &#xA;&lt;p&gt;AIConfig uses the model name string to retrieve the right &lt;code&gt;ModelParser&lt;/code&gt; for a given Prompt (see &lt;code&gt;AIConfigRuntime.get_model_parser&lt;/code&gt;), so you can register a different ModelParser for the same ID to override which &lt;code&gt;ModelParser&lt;/code&gt; handles a Prompt.&lt;/p&gt; &#xA;&lt;p&gt;For example, suppose I want to use &lt;code&gt;MyOpenAIModelParser&lt;/code&gt; to handle &lt;code&gt;gpt-4&lt;/code&gt; prompts. I can do the following at the start of my application:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;AIConfigRuntime.register_model_parser(myModelParserInstance, [&#34;gpt-4&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Callback events&lt;/h3&gt; &#xA;&lt;p&gt;Use callback events to trace and monitor what&#39;s going on -- helpful for debugging and observability.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from aiconfig import AIConfigRuntime, CallbackEvent, CallbackManager&#xA;config = AIConfigRuntime.load(&#39;aiconfig.json&#39;)&#xA;&#xA;async def my_custom_callback(event: CallbackEvent) -&amp;gt; None:&#xA;  print(f&#34;Event triggered: {event.name}&#34;, event)&#xA;&#xA;callback_manager = CallbackManager([my_custom_callback])&#xA;config.set_callback_manager(callback_manager)&#xA;&#xA;await config.run(&#34;prompt_name&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/overview/monitoring-aiconfig&#34;&gt;&lt;strong&gt;Read more&lt;/strong&gt; here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Extensibility&lt;/h2&gt; &#xA;&lt;p&gt;AIConfig is designed to be customized and extended for your use-case. The &lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/docs/extensibility&#34;&gt;Extensibility&lt;/a&gt; guide goes into more detail.&lt;/p&gt; &#xA;&lt;p&gt;Currently, there are 3 core ways to extend AIConfig:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/extensibility#1-bring-your-own-model&#34;&gt;Supporting other models&lt;/a&gt; - define a ModelParser extension&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/extensibility#2-callback-handlers&#34;&gt;Callback event handlers&lt;/a&gt; - tracing and monitoring&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/extensibility#3-custom-metadata&#34;&gt;Custom metadata&lt;/a&gt; - save custom fields in &lt;code&gt;aiconfig&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributing to &lt;code&gt;aiconfig&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This is our first open-source project and we&#39;d love your help.&lt;/p&gt; &#xA;&lt;p&gt;See our &lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/contributing&#34;&gt;contributing guidelines&lt;/a&gt; -- we would especially love help adding support for additional models that the community wants.&lt;/p&gt; &#xA;&lt;h2&gt;Cookbooks&lt;/h2&gt; &#xA;&lt;p&gt;We provide several guides to demonstrate the power of &lt;code&gt;aiconfig&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;See the &lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks&#34;&gt;&lt;code&gt;cookbooks&lt;/code&gt;&lt;/a&gt; folder for examples to clone.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Chatbot&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Wizard-GPT&#34;&gt;Wizard GPT&lt;/a&gt; - speak to a wizard on your CLI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Cli-Mate&#34;&gt;CLI-mate&lt;/a&gt; - help you make code-mods interactively on your codebase.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Retrieval Augmented Generated (RAG)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/RAG-with-AIConfig&#34;&gt;RAG with AIConfig&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;At its core, RAG is about passing data into prompts. Read how to &lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/docs/overview/parameters&#34;&gt;pass data&lt;/a&gt; with AIConfig.&lt;/p&gt; &#xA;&lt;h3&gt;Function calling&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI&#34;&gt;OpenAI function calling&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Prompt routing&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Basic-Prompt-Routing&#34;&gt;Prompt routing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Chain of Thought&lt;/h3&gt; &#xA;&lt;p&gt;A variant of chain-of-thought is Chain of Verification, used to help reduce hallucinations. Check out the aiconfig cookbook for CoVe:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Chain-of-Verification&#34;&gt;Chain of Verification&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using local LLaMA2 with &lt;code&gt;aiconfig&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/llama&#34;&gt;LLaMA example&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Hugging Face text generation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/HuggingFace&#34;&gt;Hugging Face (Mistral-7B) example&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Google PaLM&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency&#34;&gt;PaLM&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;This project is under active development.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to help, please see the &lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/#contributing-to-aiconfig&#34;&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please create issues for additional capabilities you&#39;d like to see.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s what&#39;s already on our roadmap:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Evaluation interfaces: allow &lt;code&gt;aiconfig&lt;/code&gt; artifacts to be evaluated with user-defined eval functions. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We are also considering integrating with existing evaluation frameworks.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Local editor for &lt;code&gt;aiconfig&lt;/code&gt;: enable you to interact with aiconfigs more intuitively.&lt;/li&gt; &#xA; &lt;li&gt;OpenAI Assistants API support&lt;/li&gt; &#xA; &lt;li&gt;Multi-modal ModelParsers: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GPT4-V support&lt;/li&gt; &#xA;   &lt;li&gt;DALLE-3&lt;/li&gt; &#xA;   &lt;li&gt;Whisper&lt;/li&gt; &#xA;   &lt;li&gt;HuggingFace image generation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQs&lt;/h2&gt; &#xA;&lt;h3&gt;How should I edit an &lt;code&gt;aiconfig&lt;/code&gt; file?&lt;/h3&gt; &#xA;&lt;p&gt;Editing a configshould be done either programmatically via SDK or via the UI (workbooks):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/raw/main/cookbooks/Create-AIConfig-Programmatically/create_aiconfig_programmatically.ipynb&#34;&gt;Programmatic&lt;/a&gt; editing.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/#edit-aiconfig-in-a-notebook-editor&#34;&gt;Edit with a workbook&lt;/a&gt; editor: this is similar to editing an ipynb file as a notebook (most people never touch the json ipynb directly)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You should only edit the &lt;code&gt;aiconfig&lt;/code&gt; by hand for minor modifications, like tweaking a prompt string or updating some metadata.&lt;/p&gt; &#xA;&lt;h3&gt;Does this support custom endpoints?&lt;/h3&gt; &#xA;&lt;p&gt;Out of the box, AIConfig already supports all OpenAI GPT* models, Google’s PaLM model and any “textgeneration” model on Hugging Face (like Mistral). See &lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/#supported-models&#34;&gt;Supported Models&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, you can install &lt;code&gt;aiconfig&lt;/code&gt; &lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/extensions&#34;&gt;extensions&lt;/a&gt; for additional models (see question below).&lt;/p&gt; &#xA;&lt;h3&gt;Is OpenAI function calling supported?&lt;/h3&gt; &#xA;&lt;p&gt;Yes. &lt;a href=&#34;https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Function-Calling-OpenAI&#34;&gt;This example&lt;/a&gt; goes through how to do it.&lt;/p&gt; &#xA;&lt;p&gt;We are also working on adding support for the Assistants API.&lt;/p&gt; &#xA;&lt;h3&gt;How can I use aiconfig with my own model endpoint?&lt;/h3&gt; &#xA;&lt;p&gt;Model support is implemented as “ModelParser”s in the AIConfig SDK, and the idea is that anyone, including you, can define a ModelParser (and even publish it as an extension package).&lt;/p&gt; &#xA;&lt;p&gt;All that’s needed to use a model with AIConfig is a ModelParser that knows&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;how to serialize data from a model into the aiconfig format&lt;/li&gt; &#xA; &lt;li&gt;how to deserialize data from an aiconfig into the type the model expects&lt;/li&gt; &#xA; &lt;li&gt;how to run inference for model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more details, see &lt;a href=&#34;https://aiconfig.lastmileai.dev/docs/extensibility&#34;&gt;Extensibility&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;When should I store outputs in an &lt;code&gt;aiconfig&lt;/code&gt;?&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;AIConfigRuntime&lt;/code&gt; object is used to interact with an aiconfig programmatically (see &lt;a href=&#34;https://raw.githubusercontent.com/lastmile-ai/aiconfig/main/#aiconfig-sdk&#34;&gt;SDK usage guide&lt;/a&gt;). As you run prompts, this object keeps track of the outputs returned from the model.&lt;/p&gt; &#xA;&lt;p&gt;You can choose to serialize these outputs back into the &lt;code&gt;aiconfig&lt;/code&gt; by using the &lt;code&gt;config.save(include_outputs=True)&lt;/code&gt; API. This can be useful for preserving context -- think of it like session state.&lt;/p&gt; &#xA;&lt;p&gt;For example, you can use aiconfig to create a chatbot, and use the same format to save the chat history so it can be resumed for the next session.&lt;/p&gt; &#xA;&lt;p&gt;You can also choose to save outputs to a &lt;em&gt;different&lt;/em&gt; file than the original config -- &lt;code&gt;config.save(&#34;history.aiconfig.json&#34;, include_outputs=True)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Why should I use &lt;code&gt;aiconfig&lt;/code&gt; instead of things like &lt;a href=&#34;https://pypi.org/project/configurator/&#34;&gt;configurator&lt;/a&gt;?&lt;/h3&gt; &#xA;&lt;p&gt;It helps to have a &lt;a href=&#34;http://aiconfig.lastmileai.dev/docs/overview/ai-config-format&#34;&gt;standardized format&lt;/a&gt; specifically for storing generative AI prompts, inference results, model parameters and arbitrary metadata, as opposed to a general-purpose configuration schema.&lt;/p&gt; &#xA;&lt;p&gt;With that standardization, you just need a layer that knows how to serialize/deserialize from that format into whatever the inference endpoints require.&lt;/p&gt; &#xA;&lt;h3&gt;This looks similar to &lt;code&gt;ipynb&lt;/code&gt; for Jupyter notebooks&lt;/h3&gt; &#xA;&lt;p&gt;We believe that notebooks are a perfect iteration environment for generative AI -- they are flexible, multi-modal, and collaborative.&lt;/p&gt; &#xA;&lt;p&gt;The multi-modality and flexibility offered by notebooks and &lt;a href=&#34;https://ipython.org/ipython-doc/3/notebook/nbformat.html&#34;&gt;&lt;code&gt;ipynb&lt;/code&gt;&lt;/a&gt; offers a good interaction model for generative AI. The &lt;code&gt;aiconfig&lt;/code&gt; file format is extensible like &lt;code&gt;ipynb&lt;/code&gt;, and AI Workbook editor allows rapid iteration in a notebook-like IDE.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;AI Workbooks are to AIConfig what Jupyter notebooks are to &lt;code&gt;ipynb&lt;/code&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are 2 areas where we are going beyond what notebooks offer:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;aiconfig&lt;/code&gt; is more &lt;strong&gt;source-control friendly&lt;/strong&gt; than &lt;code&gt;ipynb&lt;/code&gt;. &lt;code&gt;ipynb&lt;/code&gt; stores binary data (images, etc.) by encoding it in the file, while &lt;code&gt;aiconfig&lt;/code&gt; recommends using file URI references instead.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;aiconfig&lt;/code&gt; can be imported and &lt;strong&gt;connected to application code&lt;/strong&gt; using the AIConfig SDK.&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>