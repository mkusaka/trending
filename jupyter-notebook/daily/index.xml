<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-14T01:32:34Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Srijith-rkr/Whispering-LLaMA</title>
    <updated>2023-10-14T01:32:34Z</updated>
    <id>tag:github.com,2023-10-14:/Srijith-rkr/Whispering-LLaMA</id>
    <link href="https://github.com/Srijith-rkr/Whispering-LLaMA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;EMNLP 23 - Integrating Whisper Encoder to LLaMA Decoder for Generative ASR Error Correction&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Whispering-LLaMA: Integrate Whisper Encoder to LLaMA Decoder&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Accepted at &lt;strong&gt;EMNLP 2023 (Main Track)&lt;/strong&gt; | &lt;a href=&#34;https://arxiv.org/abs/2310.06434&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; | &lt;img src=&#34;https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&amp;amp;logo=PyTorch&amp;amp;logoColor=white&#34; alt=&#34;PyTorch&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;ASR Generative Error Correction by leveraging foundational Audio (Whisper) and Langugae (LLaMA) models.&lt;/li&gt; &#xA; &lt;li&gt;Fusing Whisper Encoder and LLaMA decoder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Srijith-rkr/Whispering-LLaMA/raw/main/images/model%20overview.svg?sanitize=true&#34; height=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;We introduce a novel cross-modal fusion technique designed for generative error correction for Automatic Speech Recognition. In an oversimplified sense, We leverage In-Context learning to feed the n-best hypothesis produced by an Acoustic model into a Large Language model and prompt it to predict the most accurate sentence, as shown below.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Srijith-rkr/WHISPERing-LLaMA/raw/main/images/Prompt%20overview.svg?sanitize=true&#34; height=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We propose a novel mechanism to fuse the acoustic features from the audio input into the LLM to significantly enhance the performance (28.83% -&amp;gt; 37.66% WERR) by leveraging an Audio Foundational model as a feature extractor. We further design our system in a parameter-efficient manner with only 7.97M trainable parameters as shown below. Please refer to the paper [YET] for further information.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Srijith-rkr/Whispering-LLaMA/raw/main/images/Adapter_mechanism.svg?sanitize=true&#34; width=&#34;700&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Setup&lt;/h1&gt; &#xA;&lt;p&gt;Clone the repo&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Srijith-rkr/Whispering-LLaMA&#xA;cd WHISPERing-LLaMA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And use the environment.yml file to install dependencies with Anaconda.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To obtain the pre-trained Alpaca weights, please refer &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca#recovering-alpaca-weights&#34;&gt;here&lt;/a&gt;. You can then use convert_hf_checkpoint.py to rename the state_dict the &lt;a href=&#34;https://github.com/Lightning-AI/lit-llama&#34;&gt;lit-llama&lt;/a&gt; implementation&lt;/li&gt; &#xA; &lt;li&gt;Hosted Alpaca weights for easy implementation coming soon&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You are all set! ðŸŽ‰&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h1&gt;Dataset&lt;/h1&gt; &#xA;&lt;p&gt;We have uploaded our N-best Hypotheses dataset generated using Whisper-Tiny on &lt;a href=&#34;https://huggingface.co/datasets/PeacefulData/HyPoradise-v1-GigaSpeech&#34;&gt;Hugging Face PeacefulData&lt;/a&gt;. The hypotheses were generated using the Hugging Face &lt;a href=&#34;https://huggingface.co/datasets/speechcolab/gigaspeech&#34;&gt;GigaSpeech dataset&lt;/a&gt; M subset. You will be able to map the hypothesis on our dataset with the audio clips from the Gigaspeeh dataset using the &#39;ID&#39; tag.&lt;/p&gt; &#xA;&lt;h1&gt;Model Weights&lt;/h1&gt; &#xA;&lt;p&gt;We are working on hosting the model weights in &lt;a href=&#34;https://huggingface.co/Srijith-rkr/Whispering-LLaMA&#34;&gt;Huggin Face/Whispering-LLaMA&lt;/a&gt; for easier setup.&lt;/p&gt; &#xA;&lt;h1&gt;Training &amp;amp; Inference&lt;/h1&gt; &#xA;&lt;p&gt;Please refer to :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;data_preparation to generate your custom n-best hypothesis dataset&lt;/li&gt; &#xA; &lt;li&gt;training/WL-M.py to train the best our best model on your dataset&lt;/li&gt; &#xA; &lt;li&gt;Inference/WL-M.py to run inference&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;This implementation builds on&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Lightning-AI/lit-llama&#34;&gt;lit-llama&lt;/a&gt; for the Training pipeline.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;stanford_alpaca&lt;/a&gt; for the pre-trained instruction following Language model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt; to obtain acoustic embeddings.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h3&gt;Reference&lt;/h3&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you consider this work would be related or useful for your research, please consider to cite this paper. Thank you!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@inproceedings{radhakrishnan2023whispering,&#xA;  title={Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition},&#xA;  author={Srijith Radhakrishnan, Chao-Han Huck Yang, Sumeer Ahmad Khan, Rohit Kumar, Narsis A. Kiani, David Gomez-Cabrero, Jesper N. Tegner},&#xA;  booktitle={Proc. of EMNLP},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>