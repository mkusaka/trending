<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-13T01:32:26Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>RonaldJEN/FinanceChatGLM</title>
    <updated>2023-08-13T01:32:26Z</updated>
    <id>tag:github.com,2023-08-13:/RonaldJEN/FinanceChatGLM</id>
    <link href="https://github.com/RonaldJEN/FinanceChatGLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SMP 2023 ChatGLMé‡‘èå¤§æ¨¡å‹æŒ‘æˆ˜èµ› 60 åˆ†baselineæ€è·¯ä»‹ç»&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SMP 2023 ChatGLM é‡‘èå¤§æ¨¡å‹æŒ‘æˆ˜èµ› 60 åˆ† Baseline æ€è·¯&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;é˜Ÿä¼:&lt;/strong&gt; å°æ‰“å°é—¹&lt;/p&gt; &#xA;&lt;h2&gt;æ•°æ®æå–æ•´ä½“è¿‡ç¨‹&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/001.png&#34; alt=&#34;æ•°æ®æå–æ•´ä½“è¿‡ç¨‹&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;PDF è¡¨æ ¼è¯†åˆ«ç»“æœå¯¹æ¯”&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/002.jpeg&#34; alt=&#34;è¯†åˆ«ç»“æœ1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/003.jpeg&#34; alt=&#34;è¯†åˆ«ç»“æœ2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;è‡ªç ”PDFè¡¨æ ¼è¯†åˆ«é€»è¾‘&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/004.png&#34; alt=&#34;é€»è¾‘1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/005.png&#34; alt=&#34;é€»è¾‘2&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/006.png&#34; alt=&#34;é€»è¾‘3&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ä¸»è¦æ­¥éª¤:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;å®šä½è¡¨æ ¼åŒºåŸŸ&lt;/li&gt; &#xA; &lt;li&gt;è¯†åˆ«å•å…ƒæ ¼&lt;/li&gt; &#xA; &lt;li&gt;è·¨é¡µè¡¨æ ¼åˆå¹¶&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;è¯†åˆ«å•å…ƒæ ¼å¹¶ç”Ÿæˆè¡¨æ ¼ç®—æ³•ä¼ªä»£ç &lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/007.jpeg&#34; alt=&#34;ä¼ªä»£ç 1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/008.jpeg&#34; alt=&#34;ä¼ªä»£ç 2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;åŸºäºæœ‰é™çŠ¶æ€æœºçš„æ•°æ®æå–&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/009.png&#34; alt=&#34;æœ‰é™çŠ¶æ€æœº&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;èµ„äº§è´Ÿå€ºè¡¨ç¤ºæ„å›¾ (ä¸‰å¤§è¡¨ä¹‹ä¸€) æ•°æ®å…¥åº“&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/010.jpeg&#34; alt=&#34;æ•°æ®å…¥åº“&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ä¸»è¦å†…å®¹:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å…¬å¸åŸºæœ¬ä¿¡æ¯&lt;/li&gt; &#xA; &lt;li&gt;èµ„äº§è´Ÿå€ºè¡¨&lt;/li&gt; &#xA; &lt;li&gt;ç°é‡‘æµé‡è¡¨&lt;/li&gt; &#xA; &lt;li&gt;åˆ©æ¶¦è¡¨&lt;/li&gt; &#xA; &lt;li&gt;å…¬å¸å‘˜å·¥ä¿¡æ¯&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å…¬å¸å…¨ç§°ä¸ç®€ç§°åŠä»£ç å¯¹ç…§&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/011.jpeg&#34; alt=&#34;å¯¹ç…§è¡¨&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ä¿¡æ¯ä»è¡¨æ ¼è½¬ä¸ºæ–‡æœ¬æè¿°&lt;/h2&gt; &#xA;&lt;h3&gt;å…¬å¸å‘˜å·¥ä¿¡æ¯&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/012.png&#34; alt=&#34;å‘˜å·¥ä¿¡æ¯&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;æ–‡æœ¬æè¿°ç¤ºä¾‹: å®‰é æ™ºç”µ&lt;/h3&gt; &#xA;&lt;p&gt;å®‰é æ™ºç”µï¼ˆè‚¡ç¥¨ä»£ç ï¼š300617ï¼‰åœ¨2019å¹´å…±æœ‰642åèŒå·¥ï¼Œå…¶ä¸­74äººæ˜¯ç ”å‘äººå‘˜ï¼Œç ”å‘äººå‘˜å æ¯”11.53%ã€‚è¯¥å…¬å¸æœ‰10åç¡•å£«å­¦å†ä»¥ä¸Šå­¦å†çš„å‘˜å·¥ï¼Œä½†æ²¡æœ‰åšå£«å­¦å†çš„å‘˜å·¥ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;æ•´ä½“æ¨ç†æµç¨‹&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/013.jpeg&#34; alt=&#34;æ¨ç†æµç¨‹&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;âš ï¸ ä¸è¦ç›¸ä¿¡å¤§æ¨¡å‹çš„æ•°å­¦èƒ½åŠ›&lt;/h2&gt; &#xA;&lt;h3&gt;æ”¹è¿›å‰&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/014.png&#34; alt=&#34;æ”¹è¿›å‰&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;æ”¹è¿›å&lt;/h3&gt; &#xA;&lt;p&gt;ç›´æ¥å¸®ä»–ç®—å¥½ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å®‰è®°é£Ÿå“2019å¹´è¥ä¸šåˆ©æ¶¦ä¸º49072627.15å…ƒ, 2019å¹´è¥ä¸šæ”¶å…¥ä¸º421296738.60å…ƒã€‚æ ¹æ®å…¬å¼:&lt;/p&gt; &#xA;&lt;p&gt;[ \text{è¥ä¸šåˆ©æ¶¦ç‡} = \frac{\text{è¥ä¸šåˆ©æ¶¦}}{\text{è¥ä¸šæ”¶å…¥}} \times 100 ]&lt;/p&gt; &#xA;&lt;p&gt;å¾—å‡ºç»“æœå®‰è®°é£Ÿå“2019å¹´è¥ä¸šåˆ©æ¶¦ç‡ä¸º11.65%ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;âš ï¸ ä¸è¦ç›¸ä¿¡å¤§æ¨¡å‹çš„æ¨ç†èƒ½åŠ›&lt;/h2&gt; &#xA;&lt;h3&gt;å»ºè®® å‰”é™¤å†—ä½™ä¿¡æ¯ï¼Œå¦åˆ™å¯èƒ½æ— æ³•å¾—åˆ°æ­£ç¡®ç­”æ¡ˆã€‚&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/015.jpeg&#34; alt=&#34;æ¨ç†ç¤ºä¾‹&#34;&gt; &#34;&#34;&#34;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>justLV/onju-voice</title>
    <updated>2023-08-13T01:32:26Z</updated>
    <id>tag:github.com,2023-08-13:/justLV/onju-voice</id>
    <link href="https://github.com/justLV/onju-voice" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A hackable AI home assistant platform&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Onju Voice ğŸğŸ”ˆ&lt;/h1&gt; &#xA;&lt;p&gt;ğŸ’« &lt;a href=&#34;https://twitter.com/justLV&#34;&gt;DEMO&#39;s&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A hackable AI home assistant platform using the Google Nest Mini (2nd gen) form factor, consisting of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a custom PCB designed to be a drop-in replacement to the original, using the ESP32-S3 for audio processing&lt;/li&gt; &#xA; &lt;li&gt;a server for handling the transcription, response generation and Text-to-Speech from multiple devices on the same network&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/images/header_white.jpg&#34; width=&#34;960&#34;&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains firmware, server code and some example applications, intended to be as accessible as possible for getting up and running i.e.:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/#-firmware&#34;&gt;Firmware&lt;/a&gt; for the custom PCB can be programmed using the Arduino IDE and a USB cable (installation of ESP-IDF not required)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/#%EF%B8%8F-server&#34;&gt;Server code&lt;/a&gt; has minimal requirements besides running Whisper locally, and should be able to run on most devices that you can leave plugged in whether MacOS / Linux / Win etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/images/rich.png&#34;&gt; &#xA;&lt;h2&gt;Example applications&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ“© Querying and replying to messages (using a &lt;a href=&#34;https://github.com/justLV/onju-voice-maubot&#34;&gt;custom Maubot plugin&lt;/a&gt; &amp;amp; Beeper)&lt;/li&gt; &#xA; &lt;li&gt;ğŸ’¡ Light control with &lt;a href=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/#-home-assistant&#34;&gt;Home Assistant&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ğŸ“ Adding and retrieving notes/memos for the LLM to craft a response with&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Not included:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ‘¥ Multiple voice characters. Iâ€™ll leave it to the user to clone voices as they deem fair use. Also from experience LLMâ€™s &amp;lt; GPT4 donâ€™t consistently enough follow instructions to reliably respond in different characters AND perform multiple function calling with complicated prompts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Current features of the device &amp;lt;&amp;gt; server platform&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Auto-discovery of devices using multicast announcements&lt;/li&gt; &#xA; &lt;li&gt;Remembering conversation history and voice settings for each device&lt;/li&gt; &#xA; &lt;li&gt;Sending &amp;amp; receiving audio data from the device, packed as 16-bit, 16kHz (UDP sending, TCP receiving partially buffered into PSRAM)&lt;/li&gt; &#xA; &lt;li&gt;Speaker and microphone visualization with the LEDâ€™s, and custom LED control via the server&lt;/li&gt; &#xA; &lt;li&gt;Mute switch functionality, tap-to-wake for enabling the microphone, and setting mic timeout via the server&lt;/li&gt; &#xA; &lt;li&gt;Device-level logging to individual files and console output using &lt;code&gt;rich&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;[coming soon] SoftAP WiFi provisioning to prevent need for programming WiFi credentials&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Limitations of this release:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The Arduino IDE doesnâ€™t (yet) support the Espressifâ€™s Audio SDKâ€™s, such as &lt;a href=&#34;https://github.com/espressif/esp-adf&#34;&gt;ESP-ADF&lt;/a&gt;, &lt;a href=&#34;https://github.com/espressif/esp-skainet&#34;&gt;ESP-Skainet&lt;/a&gt; etc. For these demo&#39;s it&#39;s not absolutely required, but if you use Espressifâ€™s ESP-IDF with these SDK&#39;s you&#39;d unlock features such as: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;VAD (Voice Activity Detection) - in this example VAD is offloaded to the server using webrtcvad, and the listening period is extended by either tapping the device or by the server sending mic keep alive timeouts (network traffic is really minimal at 16-bit, 16kHz)&lt;/li&gt; &#xA;   &lt;li&gt;AEC (Acoustic Echo Cancellation) - to allow you to effectively talk over the assistant by removing the speaker output from audio input&lt;/li&gt; &#xA;   &lt;li&gt;BSS (Blind Source Separation) - letâ€™s you use both micâ€™s for isolating speakers based on location, and other noise suppression&lt;/li&gt; &#xA;   &lt;li&gt;Wakewords and other on-device commands - Iâ€™m not a believer in this given how finicky these can be and donâ€™t think these are and think all command logic should be handled by layers of language models on the server.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The server currently only does transcription locally and uses: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OpenAI for generating responses &amp;amp; functions calls, but if you have the hardware you could run a local LLM, using something like ToolLLM for calling APIâ€™s to add almost any capabilities youâ€™d wish.&lt;/li&gt; &#xA;   &lt;li&gt;Text-to-speech from Elevenlabs - this is fair to say the easiest to get running, fastest and most expressive option out there but FWIR data policy is a little dubious so careful about sending anything too sensitive. Iâ€™d really like to see comparable performing open source options that you can run locally&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Conversation flow is highly serialized, i.e. recording &amp;gt; transcription &amp;gt; LLM &amp;gt; TTS needs to finish each step before moving onto the next. Not included here is feeding incomplete transcriptions to a smaller model, and streaming slower LLM&#39;s like GPT4 to Elevenlabs and sending streaming responses back, it&#39;s currently a little too hacky to include in this release.&lt;/li&gt; &#xA; &lt;li&gt;No wakeword usage, mostly done intentionally as I feel uttering a wake-word before every response is a terrible experience. This currently uses a combo of VAD, mic-timeouts sent from server, tap-to-wake, mute switch usage etc. Not included here is experiments running a smaller, faster LLM for classification with a running transcription before handing off to a larger LLM with specific prompt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Other areas for improvement&lt;/h2&gt; &#xA;&lt;p&gt;These are things I didn&#39;t get time to implement but I believe would be invaluable and pretty achievable&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker diarization - know who is saying what, and have the LLM enage in multi-user conversations or infer when it isn&#39;t being spoken to&lt;/li&gt; &#xA; &lt;li&gt;Interruptions - requires AEC for simultaneous listening and playback&lt;/li&gt; &#xA; &lt;li&gt;Smaller local models/LLM&#39;s for running classification, detecting intent and routing to larger LLM&#39;s&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;ğŸ–¥ï¸ Server&lt;/h2&gt; &#xA;&lt;p&gt;Ensure you can install &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt; and run at least the base model, following any debugging steps they have if not. If you can get past that, it should be as simple as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd server&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Adjust settings in the &lt;code&gt;config.yaml&lt;/code&gt;, and tweak aspects such as how much silence is needed to start processing to trade-off snappiness vs avoiding cutting off the user.&lt;/p&gt; &#xA;&lt;p&gt;Add your Elevenlabs token to &lt;code&gt;credentials.json&lt;/code&gt; and ensure you have a cloned voice in your account that you set in the &lt;code&gt;config.yaml&lt;/code&gt; under &lt;code&gt;elevenlabs_default_voice&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ll also need a greeting WAV set in &lt;code&gt;config.yaml&lt;/code&gt; under &lt;code&gt;greeting_wav&lt;/code&gt;, that will be sent to devices on connecting to the WiFi. This is up to you to record or procure (&lt;a href=&#34;https://github.com/ytdl-org/youtube-dl&#34;&gt;e.g.&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;A small subset of the config parameters can be set as optional arguments when running the script. For e.g. the following will run the server with note-taking, Home Assistant, Maubot, real sending of messages enabled (a safe guard disabled by default), and a smaller English only Whisper model for transcription.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python server.py --n --ha --mb --send --whisper base.en&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ğŸ¡ Home Assistant&lt;/h3&gt; &#xA;&lt;p&gt;I recommend setting this up on the same server or one that is always plugged in on your network, following the &lt;a href=&#34;https://www.home-assistant.io/installation/linux#docker-compose&#34;&gt;Docker Compose instructions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then go through the onboarding, setup a user, name your devices and get a Long Lived token to add to &lt;code&gt;credentials.json&lt;/code&gt; together with the URL e.g. &lt;code&gt;http://my-local-server:8123/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ğŸ¤– Maubot&lt;/h3&gt; &#xA;&lt;p&gt;Follow instructions &lt;a href=&#34;https://github.com/justLV/onju-home-maubot&#34;&gt;here&lt;/a&gt; to setup Maubot with your Beeper account. Ensure the correct URL is setup in &lt;code&gt;config.yaml&lt;/code&gt;, set &lt;code&gt;send_replies&lt;/code&gt; to True if your friends are forgiving of the odd mistakes, and set a &lt;code&gt;footer&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Donâ€™t have Beeper yet and canâ€™t wait? &lt;a href=&#34;https://docs.mau.fi/bridges/go/imessage/mac/setup.html&#34;&gt;Try setup a Matrix bridge yourself&lt;/a&gt; and a custom function definition for OpenAI function calling (and share how you did it!)&lt;/p&gt; &#xA;&lt;p&gt;Following this example you can also integrate e-mail.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“Ÿ Firmware&lt;/h2&gt; &#xA;&lt;p&gt;Irrespective of what you use for development, the quickest &amp;amp; least error prone setup for building &amp;amp; flashing firmware is probably installing the Arduino IDE &lt;a href=&#34;https://www.arduino.cc/en/software&#34;&gt;Software&lt;/a&gt;, and then using this IDE or your preference i.e. VSCode for development (Copilot)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add the ESP32 boards as detailed &lt;a href=&#34;https://docs.espressif.com/projects/arduino-esp32/en/latest/installing.html&#34;&gt;here&lt;/a&gt; (TL;DR add &lt;code&gt;https://espressif.github.io/arduino-esp32/package_esp32_index.json&lt;/code&gt; to &lt;code&gt;Preferences &amp;gt; Additional Boards Manager URLâ€™s&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Under Boards Manager, install â€œesp32â€ by Espressif Systems&lt;/li&gt; &#xA; &lt;li&gt;Under Library Manager, install â€œAdafruit NeoPixel Libraryâ€&lt;/li&gt; &#xA; &lt;li&gt;Clone this repo to &lt;code&gt;Documents/Arduino&lt;/code&gt; for simplicity.&lt;/li&gt; &#xA; &lt;li&gt;Add your WiFi credentials to &lt;code&gt;credentials.h&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;bash setup-git-hash.sh&lt;/code&gt; to add a header with the git-hash (optional). This will then automatically update after commits, and help track the firmware that your devices are running from the server side.&lt;/li&gt; &#xA; &lt;li&gt;Open File &amp;gt; Sketchbook &amp;gt; onju-home &amp;gt; onjuino&lt;/li&gt; &#xA; &lt;li&gt;Select Tools &amp;gt; Board &amp;gt; esp32 &amp;gt; ESP32S3 Dev Module&lt;/li&gt; &#xA; &lt;li&gt;Under Tools ensure: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;USB CDC on Boot set to Enabled&lt;/li&gt; &#xA;   &lt;li&gt;PSRAM set to OPI PSRAM&lt;/li&gt; &#xA;   &lt;li&gt;Board is plugged in and Port is selected (you may need to install USB bridge drivers as detailed by Espressif, donâ€™t worry if name is incorrect)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Build and upload&lt;/li&gt; &#xA; &lt;li&gt;If not reset, press the reset button. In Serial Monitor you can also send &lt;code&gt;r&lt;/code&gt; to reset the device (assuming it is already booted)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ§© Hardware&lt;/h2&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/images/copper.png&#34; width=&#34;48%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/images/render.png&#34; width=&#34;48%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://365.altium.com/files/C44B8519-69BA-464B-A221-24D527B89E2C&#34;&gt;Schematics &amp;amp; PCB preview&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PCB&#39;s will be made available from Crowd Supply (&lt;a href=&#34;https://www.crowdsupply.com/onju/onju-voice&#34;&gt;sign up link&lt;/a&gt;), to leverage their fulfillment expertize and bulk ordering.&lt;/p&gt; &#xA;&lt;p&gt;I will be sharing more detailed instructions for replacement.&lt;/p&gt; &#xA;&lt;p&gt;Replacement gaskets for the microphone &amp;amp; LED&#39;s can be made using &lt;a href=&#34;https://www.amazon.com/gp/product/B07KCJ31J9&#34;&gt;adhesive foam&lt;/a&gt; and a &lt;a href=&#34;https://www.amazon.com/gp/product/B087D2Z43F&#34;&gt;punch set&lt;/a&gt;) for example&lt;/p&gt; &#xA;&lt;h2&gt;â“Questions&lt;/h2&gt; &#xA;&lt;h3&gt;Does this replace my Google Nest Mini?&lt;/h3&gt; &#xA;&lt;p&gt;While this replicates the interfaces of the Google Nest Mini, donâ€™t expect this to be a 1:1 replacement, for e.g. it is not intended to be a music playback device (although there is probably no reason it couldnâ€™t be developed to be used as such). Itâ€™s also worth re-iterating that like the Google Nest Mini, this requires a separate server, although this can be in your home running local models instead of in a Google datacenter. &lt;strong&gt;The original is well tested, maintained, certified and works out the box, while this is essentially a dev board with some neat examples for you to build on top of&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;What if I donâ€™t have a Google Nest Mini but still want to use this?&lt;/h3&gt; &#xA;&lt;p&gt;Fortunately theyâ€™re still being sold, you may find deals for &amp;lt;$40 which is pretty good for the quality of speaker and form factor. I picked up quite a few from eBay, just make sure you get the 2nd gen.&lt;/p&gt; &#xA;&lt;p&gt;The adventurous can get try replacement shells from &lt;a href=&#34;https://www.aliexpress.us/item/3256803723188315.html&#34;&gt;AliExpress&lt;/a&gt; for e.g., but youâ€™ll still need a base, power input, mute switch, speaker &amp;amp; mount, capacitive touch panels, and replacement gaskets etc. A hero out there could design a custom enclosure that fits an off-the-shelf speaker.&lt;/p&gt; &#xA;&lt;h3&gt;But Iâ€™m really impatient and want to get hacking away! What can I do?&lt;/h3&gt; &#xA;&lt;p&gt;a) if you can commit to making significant contributions to the codebase and/or major contributions to the board design or RF review, we may be able to make early samples available&lt;/p&gt; &#xA;&lt;p&gt;b) if you donâ€™t need the form factor, donâ€™t mind rolling up my sleeves, and have some HW experience, you can breadboard it out with readily available components until you can get your hands on an order. Here are the components that should be able to get a demo running (ğŸŒ¸ Adafruit link for convenience but shop around wherever youâ€™d like)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ESP32-S3 devboard, ideally w/ PSRAM (e.g. &lt;a href=&#34;https://www.adafruit.com/product/5700&#34;&gt;QT Py S3&lt;/a&gt; or &lt;a href=&#34;https://www.adafruit.com/product/5364&#34;&gt;ESP32-S3&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/3421&#34;&gt;Microphone&lt;/a&gt; (only need 1 for the Arduino implementation, ensure it&#39;s a SPH0645 to limit debugging)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/3006&#34;&gt;Amplifier&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/1313&#34;&gt;Speaker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/1426&#34;&gt;Neopixel LED strip&lt;/a&gt; - just set the firmware to the correct #&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/3314&#34;&gt;Breadboard &amp;amp; wire kit&lt;/a&gt; (you can use protruding pieces of wire for cap touch)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You&#39;ll need to update the &lt;code&gt;custom_boards.h&lt;/code&gt; with your pin mapping&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;ğŸ PR&#39;s, issues, suggestions &amp;amp; general feedback welcome!ğŸ¡&lt;/strong&gt;&lt;/h2&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/PUG</title>
    <updated>2023-08-13T01:32:26Z</updated>
    <id>tag:github.com,2023-08-13:/facebookresearch/PUG</id>
    <link href="https://github.com/facebookresearch/PUG" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the repository for the Photorealistic Unreal Graphics (PUG) datasets for representation learning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;font size=&#34;9&#34;&gt;&lt;/font&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;font size=&#34;9&#34;&gt;&lt;b&gt;PUG&lt;/b&gt;: &lt;b&gt;P&lt;/b&gt;hotorealistic &lt;b&gt;U&lt;/b&gt;nreal &lt;b&gt;G&lt;/b&gt;raphics&lt;/font&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;font size=&#34;9&#34;&gt;&lt;/font&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;font size=&#34;9&#34;&gt; &lt;a href=&#34;https://pug.metademolab.com&#34;&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2308.03977&#34;&gt;&lt;strong&gt;Research Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://pug.metademolab.com/faq.html&#34;&gt;&lt;strong&gt;Datasheet&lt;/strong&gt;&lt;/a&gt; &lt;/font&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/PUG/assets/5903040/5fd73746-a45b-4056-ae99-3726dadb51a8&#34;&gt;https://github.com/facebookresearch/PUG/assets/5903040/5fd73746-a45b-4056-ae99-3726dadb51a8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This codebase contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;download links for the PUG-datasets&lt;/li&gt; &#xA; &lt;li&gt;dataloaders&lt;/li&gt; &#xA; &lt;li&gt;scripts that are needed to samples images from a running interactive environment made with the Unreal Engine.&lt;/li&gt; &#xA; &lt;li&gt;script to evaluate VLMs models with PUG: SPAR&lt;/li&gt; &#xA; &lt;li&gt;list of the assets used to create the PUG datasets (which are listed in each PUG folders)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Downloading the PUG datasets&lt;/h2&gt; &#xA;&lt;p&gt;Here are the links to download the PUG datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/large_objects/pug/PUG_ANIMAL.tar.gz&#34;&gt;PUG: Animals (78GB)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/large_objects/pug/PUG_IMAGENET.tar.gz&#34;&gt;PUG: ImageNet (27GB)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/large_objects/pug/PUG_SPAR.tar.gz&#34;&gt;PUG: SPAR (16GB)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/large_objects/pug/PUG_AR4T.tar.gz&#34;&gt;PUG: AR4T (97GB)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dataset loaders&lt;/h2&gt; &#xA;&lt;p&gt;Please look at each PUG subfolder to get information on how to load the datasets.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/PUG/main/PUG_Animals&#34;&gt;PUG Animals&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/PUG/main/PUG_ImageNet&#34;&gt;PUG ImageNet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/PUG/main/PUG_SPAR&#34;&gt;PUG SPAR&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/PUG/main/PUG_AR4T&#34;&gt;PUG AR4T&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to create a PUG environment ?&lt;/h2&gt; &#xA;&lt;p&gt;The instruction are availables in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/PUG/main/torchmultiverse&#34;&gt;torchmultiverse&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;The datasets are distributed under the CC-BY-NC, with the addenda that they should not be used to train Generative AI models, as found in the LICENSE file.&lt;/p&gt; &#xA;&lt;h2&gt;Citing PUG&lt;/h2&gt; &#xA;&lt;p&gt;If you use the PUG datasets, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{bordes2023pug,&#xA;      title={PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning}, &#xA;      author={Florian Bordes and Shashank Shekhar and Mark Ibrahim and Diane Bouchacourt and Pascal Vincent and Ari S. Morcos},&#xA;      year={2023},&#xA;      eprint={2308.03977},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>