<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-17T01:29:44Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>zjunlp/EasyEdit</title>
    <updated>2023-08-17T01:29:44Z</updated>
    <id>tag:github.com,2023-08-17:/zjunlp/EasyEdit</id>
    <link href="https://github.com/zjunlp/EasyEdit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An Easy-to-use Knowledge Editing Framework for LLMs.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/figs/logo.png&#34; width=&#34;180px&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;An Easy-to-use Knowledge Editing Framework for Large Language Models.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/version-v0.0.1-blue&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-green.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/last_commit-July-blue&#34; alt=&#34;Static Badge&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/PRs-Welcome-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#overview&#34;&gt;Overview&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#requirements&#34;&gt;Installation&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#use-easyedit&#34;&gt;How To Use&lt;/a&gt; • &lt;a href=&#34;https://zjunlp.gitbook.io/easyedit&#34;&gt;Docs&lt;/a&gt; • &lt;a href=&#34;https://colab.research.google.com/drive/1zcj8YgeqttwkpfoHXz9O9_rWxFFufXSO?usp=sharing&#34;&gt;Online Tutorial&lt;/a&gt; • &lt;a href=&#34;https://arxiv.org/abs/2308.07269&#34;&gt;Paper&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#citation&#34;&gt;Citation&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#contributors&#34;&gt;Contributors&lt;/a&gt; • &lt;a href=&#34;https://github.com/zjunlp/EasyEdit/raw/main/tutorial.pdf&#34;&gt;Slides&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#news&#34;&gt;What&#39;s New&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#editing-demo&#34;&gt;Editing Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#knowledge-editing&#34;&gt;Knowledge Editing&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#task-definition&#34;&gt;Task Definition&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#overview&#34;&gt;Overview&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#overview&#34;&gt;Module Framework&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#current-implementation&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#tutorial-notebook&#34;&gt;Tutorial Notebook&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#editing-performance&#34;&gt;Editing Performance&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#requirements&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#pip-installation&#34;&gt;Use Pip Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#docker-installation&#34;&gt;Use Docker Installation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#use-easyedit&#34;&gt;Usage Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#citation&#34;&gt;How To Cite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#other-related-projects&#34;&gt;Other Knowledge Editing Projects&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🔔News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023-8-15 We release the paper &#34;&lt;a href=&#34;https://arxiv.org/abs/2308.07269&#34;&gt;EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models&lt;/a&gt;.&#34;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023-7-12 We release version 0.0.1, supporting several knowledge editing techniques for LLMs. EasyEdit helps to better align LLMs with changing needs and values of users.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023-5-22 We release the paper &#34;&lt;a href=&#34;https://arxiv.org/abs/2305.13172&#34;&gt;Editing Large Language Models: Problems, Methods, and Opportunities&lt;/a&gt;&#34; and provide a paper list at &lt;a href=&#34;https://github.com/zjunlp/KnowledgeEditingPapers&#34;&gt;PaperList&lt;/a&gt;.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023-3-25 The EasyEdit project has been launched and is under development.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This repository is a subproject of &lt;a href=&#34;https://github.com/zjunlp/KnowLM&#34;&gt;KnowLM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EasyEdit&lt;/strong&gt; is now publicly open-sourced, with a &lt;a href=&#34;https://www.youtube.com/watch?v=NaQRvSYuQMo&#34;&gt;demo video&lt;/a&gt; and long-term maintenance.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Editing Demo&lt;/h2&gt; &#xA;&lt;p&gt;There is a demonstration of editing. The GIF file is created by &lt;a href=&#34;https://github.com/faressoft/terminalizer&#34;&gt;Terminalizer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/figs/demo_usage.gif&#34; width=&#34;550&#34; height=&#34;470&#34; align=&#34;center&#34;&gt; &#xA;&lt;h2&gt;Knowledge Editing&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/figs/demo.gif&#34; width=&#34;70%&#34; height=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Task Definition&lt;/h3&gt; &#xA;&lt;p&gt;Deployed models may still make unpredictable errors. For example, Large Language Models (LLMs) notoriously &lt;em&gt;hallucinate&lt;/em&gt;, &lt;em&gt;perpetuate bias&lt;/em&gt;, and &lt;em&gt;factually decay&lt;/em&gt;, so we should be able to adjust specific behaviors of pre-trained models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Knowledge editing&lt;/strong&gt; aims to adjust an initial base model&#39;s $(f_\theta)$ behavior on the particular edit descriptor $[x_e, y_e]$ efficiently, such as(The president of USA: Donald Trump -&amp;gt; Joe Biden):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;$x_e$: &#34;Who is the president of the US?&lt;/li&gt; &#xA; &lt;li&gt;$y_e$: &#34;Joe Biden.&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;without influencing the model behavior on unrelated samples. The ultimate goal is to create an edited model $(f_\theta&#39;)$.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/figs/Illustration.png&#34; width=&#34;400px&#34;&gt; &#xA;&lt;p&gt;The knowledge editing process generally impacts the predictions for a broad set of inputs &lt;strong&gt;that are closely&lt;/strong&gt; associated with the edit example, called the &lt;strong&gt;editing scope&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A successful edit should adjust the model’s behavior within the editing scope while remaining unrelated inputs(as below formula).&lt;/p&gt; &#xA;&lt;p&gt;$$ f_{\theta_{e}}(x) = \begin{cases} y_e &amp;amp; \text{if } x \in I(x_e,y_e) \ f_{\theta}(x) &amp;amp; \text{if } x \in O(x_e, y_e) \end{cases} $$&lt;/p&gt; &#xA;&lt;p&gt;In addition to this, the performance of knowledge editing should be measured from multiple dimensions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Reliability&lt;/code&gt;: the success rate of editing with a given editing description&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Generalization&lt;/code&gt;: the success rate of editing &lt;strong&gt;within&lt;/strong&gt; the editing scope&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Locality&lt;/code&gt;: whether the model&#39;s output changes after editing for unrelated inputs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Portability&lt;/code&gt;: the success rate of editing for factual reasoning(one hop, synonym, one-to-one relation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Efficiency&lt;/code&gt;: time and memory consumption required during the editing process&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🌟Overview&lt;/h2&gt; &#xA;&lt;p&gt;EasyEdit is a Python package for edit Large Language Models (LLM) like &lt;code&gt;GPT-J&lt;/code&gt;, &lt;code&gt;Llama&lt;/code&gt;, &lt;code&gt;GPT-NEO&lt;/code&gt;, &lt;code&gt;GPT2&lt;/code&gt;, &lt;code&gt;T5&lt;/code&gt;(support models from &lt;strong&gt;1B&lt;/strong&gt; to &lt;strong&gt;65B&lt;/strong&gt;), the objective of which is to alter the behavior of LLMs efficiently within a specific domain without negatively impacting performance across other inputs. It is designed to be easy to use and easy to extend.&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/figs/FrameWork.png&#34;&gt; &lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;EasyEdit contains a unified framework for &lt;strong&gt;Editor&lt;/strong&gt;, &lt;strong&gt;Method&lt;/strong&gt; and &lt;strong&gt;Evaluate&lt;/strong&gt;, respectively representing the editing scenario, editing technique, and evaluation method.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Each Knowledge Editing scenario comprises of three components:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Editor&lt;/code&gt;: such as BaseEditor(&lt;strong&gt;Factual Knowledge&lt;/strong&gt; and &lt;strong&gt;Generation&lt;/strong&gt; Editor) for LM, MultiModalEditor(&lt;strong&gt;MultiModal Knowledge&lt;/strong&gt;).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Method&lt;/code&gt;: the specific knowledge editing technique used(such as &lt;strong&gt;ROME&lt;/strong&gt;, &lt;strong&gt;MEND&lt;/strong&gt;, ..).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Evaluate&lt;/code&gt;: &lt;strong&gt;Metrics&lt;/strong&gt; for evaluating knowledge editing performance. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;Reliability&lt;/code&gt;, &lt;code&gt;Generalization&lt;/code&gt;, &lt;code&gt;Locality&lt;/code&gt;, &lt;code&gt;Portability&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The current supported knowledge editing techniques are as follows:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/kmeng01/rome&#34;&gt;FT-L&lt;/a&gt;: Fine-Tuning with $L_\infty$ constraint&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/eric-mitchell/serac&#34;&gt;SERAC&lt;/a&gt;: Mitchell et al. Memory-based&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Zce1112zslx/IKE&#34;&gt;IKE&lt;/a&gt;: Ce Zheng et al. In-Context Editing&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;!-- - [KE](https://github.com/nicola-decao/KnowledgeEditor): De Cao et al. Knowledge Editor --&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/eric-mitchell/mend&#34;&gt;MEND&lt;/a&gt;: Mitchell et al. Hypernetwork&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Hunter-DDM/knowledge-neurons&#34;&gt;KN&lt;/a&gt;: Damai Dai et al. Locate then Edit&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/kmeng01/rome&#34;&gt;ROME&lt;/a&gt;: Kevin Meng et al. Locate and Edit&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/kmeng01/memit&#34;&gt;MEMIT&lt;/a&gt;: Kevin Meng et al. Locate and Edit &#xA;    &lt;blockquote&gt; &#xA;     &lt;p&gt;Due to the limited compatibility of this toolkit and limited by the transformer version, some knowledge editing methods are not supported. You can find relevant editing methods in the following links&lt;/p&gt; &#xA;    &lt;/blockquote&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/ZeroYuHuang/Transformer-Patcher&#34;&gt;T-Patcher&lt;/a&gt; | &lt;a href=&#34;https://github.com/nicola-decao/KnowledgeEditor&#34;&gt;KE&lt;/a&gt; | &lt;a href=&#34;https://github.com/dqxiu/CaliNet&#34;&gt;CaliNet&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Current Implementation&lt;/h4&gt; &#xA;&lt;p&gt;You can choose different editing methods according to your specific needs.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;T5&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GPT-2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GPT-J&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GPT-NEO&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;LlaMA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;LlaMA-2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Baichuan&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FT-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SERAC&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;IKE&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MEND&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;KN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ROME&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MEMIT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- |     KE       |  ✅  |  ✅  |  ✅  |  |  | --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;dataset&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Google Drive&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BaiduNetDisk&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;ZsRE&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1IVcf5ikpfKuuuYeedUGomH01i1zaWuI6/view?usp=sharing&#34;&gt;[Google Drive]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1kFJxzY3X5UwDlGoe8rs-UQ?pwd=ygqc&#34;&gt;[BaiduNetDisk]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Question Answering dataset using question rephrasings&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;Counterfact&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1IVcf5ikpfKuuuYeedUGomH01i1zaWuI6/view?usp=sharing&#34;&gt;[Google Drive]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1kFJxzY3X5UwDlGoe8rs-UQ?pwd=ygqc&#34;&gt;[BaiduNetDisk]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Counterfact dataset using Entity replacement&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We provide zsre and counterfact datasets to verify the effectiveness of knowledge editing. You can download them here. &lt;a href=&#34;https://drive.google.com/file/d/1IVcf5ikpfKuuuYeedUGomH01i1zaWuI6/view?usp=sharing&#34;&gt;[Google Drive]&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/1kFJxzY3X5UwDlGoe8rs-UQ?pwd=ygqc&#34;&gt;[BaiduNetDisk]&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for &lt;strong&gt;locality&lt;/strong&gt;, in addition to testing unrelated instances, we also provide tests on distracting (&lt;a href=&#34;https://arxiv.org/abs/2305.17553&#34;&gt;reference: Detecting Edit Failures...&lt;/a&gt;), other attribution, and other downstream tasks (such as commonsense reasoning).&lt;/li&gt; &#xA; &lt;li&gt;for &lt;strong&gt;portability&lt;/strong&gt;, it tests whether the model can apply edited instances for inference. We provide evaluations for one-hop reasoning, subject alias, and inverse relation (eg, a one-to-one relationship between spouses should be bidirectionally edited).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Tutorial notebook&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GPT-2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;LlaMA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;IKE&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;In-Context Learning (ICL) Edit&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1m6Xg05XCs_WZKH0D9KJQqg9z0ZiDhEkL&#34;&gt;[Colab-gpt2]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1m6Xg05XCs_WZKH0D9KJQqg9z0ZiDhEkL&#34;&gt;[Colab-llama]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;ROME&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Locate-Then-Edit Neurons&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1KkyWqyV3BjXCWfdrrgbR-QS3AAokVZbr?usp=sharing&#34;&gt;[Colab-gpt2]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1W18GPlBCV9K6lDy7eX8V5W0knTLr5r0A&#34;&gt;[Colab-llama]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;MEMIT&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Locate-Then-Edit Neurons&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1P1lVklP8bTyh8uxxSuHnHwB91i-1LW6Z&#34;&gt;[Colab-gpt2]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/19fKCKtVBU2fqj6eTvDokGoTrxvXkEPPq&#34;&gt;[Colab-llama]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Editing Performance&lt;/h4&gt; &#xA;&lt;p&gt;We present editing results of the four metrics on &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-hf&#34;&gt;LlaMA-2-7B&lt;/a&gt; using EasyEdit. We adopt &lt;a href=&#34;https://drive.google.com/file/d/1IVcf5ikpfKuuuYeedUGomH01i1zaWuI6/view?usp=sharing&#34;&gt;ZsRE&lt;/a&gt; as the test dataset.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reliability&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Generalization&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Locality&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Portability&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FT-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.94&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.07&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SERAC&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;99.49&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;99.13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;100.00&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.13&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;IKE&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;100.00&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;99.98&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;67.56&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MEND&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;94.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90.27&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.14&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;KN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.43&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.43&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.07&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ROME&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92.45&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;99.63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.46&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MEMIT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92.94&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.97&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;99.49&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.03&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;h4&gt;🔧Pip Installation&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: Please use Python 3.9+ for EasyEdit&lt;/strong&gt; To get started, simply install conda and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/zjunlp/EasyEdit.git&#xA;conda create -n EasyEdit python=3.9.7&#xA;...&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;🐳Docker Installation&lt;/h4&gt; &#xA;&lt;p&gt;We packaged the environment, you can download Docker from &lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Pull the Docker image from Docker Hub or Aliyun:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull zjunlp/easyedit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull registry.cn-hangzhou.aliyuncs.com/zjunlp/easyedit:v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to build the Docker image locally, you can clone the project to your local machine and build the Docker image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/zjunlp/EasyEdit.git&#xA;cd EasyEdit&#xA;docker build -t your-image-name .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the Docker image as a container:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -p 8080:80 your-image-name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;📌Use EasyEdit&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;edit large language models(LLMs) around &lt;strong&gt;&lt;em&gt;5 seconds&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;BaseEditor&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;BaseEditor&lt;/code&gt;is the class for Language Modality Knowledge Editing. You can choose the appropriate editing method based on your specific needs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Due to different transformer versions and different GPU models, the editing results may fluctuate &lt;strong&gt;slightly&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Introduction by a Simple Example&lt;/h4&gt; &#xA;&lt;p&gt;With the modularity and flexibility of &lt;code&gt;EasyEdit&lt;/code&gt;, you can easily use it to edit model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step1: Define a PLM as the object to be edited.&lt;/strong&gt; Choose the PLM to be edited. &lt;code&gt;EasyEdit&lt;/code&gt; supports partial models(&lt;code&gt;T5&lt;/code&gt;, &lt;code&gt;GPTJ&lt;/code&gt;, &lt;code&gt;GPT-NEO&lt;/code&gt;, &lt;code&gt;LlaMA&lt;/code&gt; so far) retrievable on &lt;a href=&#34;https://huggingface.co/&#34;&gt;HuggingFace&lt;/a&gt;. The corresponding configuration file directory is &lt;code&gt;hparams/YUOR_METHOD/YOUR_MODEL.YAML&lt;/code&gt;, such as &lt;code&gt;hparams/MEND/gpt2-xl&lt;/code&gt;, set the corresponding &lt;code&gt;model_name&lt;/code&gt; to select the object for knowledge editing.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model_name: gpt2-xl&#xA;model_class: GPT2LMHeadModel&#xA;tokenizer_class: GPT2Tokenizer&#xA;tokenizer_name: gpt2-xl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step2: Choose the appropriate Knowledge Editing Method&lt;/strong&gt; The selection of editing methods is a &lt;strong&gt;crucial&lt;/strong&gt; step, as different methods have their own strengths and weaknesses. Users need to consider the trade-off between editing success rate, generalization, and maintaining unrelated performance. For specific performance details of each method, please refer to the paper: &lt;a href=&#34;https://arxiv.org/abs/2305.13172&#34;&gt;Editing Large Language Models: Problems, Methods, and Opportunities&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## In this case, we use MEND method, so you should import `MENDHyperParams`&#xA;from easyeditor import MENDHyperParams&#xA;## Loading config from hparams/MEMIT/gpt2-xl.yaml&#xA;hparams = MENDHyperParams.from_hparams(&#39;./hparams/MEND/gpt2-xl&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step3: Provide the edit descriptor and edit target&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## edit descriptor: prompt that you want to edit&#xA;prompts = [&#xA;    &#39;What university did Watts Humphrey attend?&#39;,&#xA;    &#39;Which family does Ramalinaceae belong to&#39;,&#xA;    &#39;What role does Denny Herzig play in football?&#39;&#xA;]&#xA;## You can set `ground_truth` to None !!!(or set to original output)&#xA;ground_truth = [&#39;Illinois Institute of Technology&#39;, &#39;Lecanorales&#39;, &#39;defender&#39;]&#xA;## edit target: expected output&#xA;target_new = [&#39;University of Michigan&#39;, &#39;Lamiinae&#39;, &#39;winger&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step4: Combine them into a &lt;code&gt;BaseEditor&lt;/code&gt;&lt;/strong&gt; &lt;code&gt;EasyEdit&lt;/code&gt; provides a simple and unified way to init Editor, like huggingface: &lt;strong&gt;from_hparams&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## Construct Language Model Editor&#xA;editor = BaseEditor.from_hparams(hparams)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step5: Provide the data for evaluation&lt;/strong&gt; Note that the data for portability and locality are both &lt;strong&gt;optional&lt;/strong&gt;(set to None for basic editing success rate evaluation only). The data format for both is a &lt;strong&gt;dict&lt;/strong&gt;, for each measurement dimension, you need to provide the corresponding prompt and its corresponding ground truth. Here is an example of the data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;locality_inputs = {&#xA;    &#39;neighborhood&#39;:{&#xA;        &#39;prompt&#39;: [&#39;Joseph Fischhof, the&#39;, &#39;Larry Bird is a professional&#39;, &#39;In Forssa, they understand&#39;],&#xA;        &#39;ground_truth&#39;: [&#39;piano&#39;, &#39;basketball&#39;, &#39;Finnish&#39;]&#xA;    },&#xA;    &#39;distracting&#39;: {&#xA;        &#39;prompt&#39;: [&#39;Ray Charles, the violin Hauschka plays the instrument&#39;, &#39;Grant Hill is a professional soccer Magic Johnson is a professional&#39;, &#39;The law in Ikaalinen declares the language Swedish In Loviisa, the language spoken is&#39;],&#xA;        &#39;ground_truth&#39;: [&#39;piano&#39;, &#39;basketball&#39;, &#39;Finnish&#39;]&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the above example, we evaluate the performance of the editing methods about &#34;neighborhood&#34; and &#34;distracting&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step6: Edit and Evaluation&lt;/strong&gt; Done! We can conduct Edit and Evaluation for your model to be edited. The &lt;code&gt;edit&lt;/code&gt; function will return a series of metrics related to the editing process as well as the modified model weights.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;metrics, edited_model, _ = editor.edit(&#xA;    prompts=prompts,&#xA;    ground_truth=ground_truth,&#xA;    target_new=target_new,&#xA;    locality_inputs=locality_inputs,&#xA;    keep_original_weight=True&#xA;)&#xA;## metrics: edit success, rephrase success, locality e.g.&#xA;## edited_model: post-edit model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;We specify the return metrics as &lt;code&gt;dict&lt;/code&gt; format, including model prediction evaluations before and after editing. For each edit, it will include the following metrics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rewrite_acc&lt;/code&gt; $\rightarrow$ &lt;strong&gt;Reliablilty&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rephrase_acc&lt;/code&gt; $\rightarrow$ &lt;strong&gt;Generalization&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;locality&lt;/code&gt; $\rightarrow$ &lt;strong&gt;Locality&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;portablility&lt;/code&gt; $\rightarrow$ &lt;strong&gt;Portablility&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;post&#34;: {&#xA;        &#34;rewrite_acc&#34;: ,&#xA;        &#34;rephrase_acc&#34;: ,&#xA;        &#34;locality&#34;: {&#xA;            &#34;YOUR_LOCALITY_KEY&#34;: ,&#xA;            //...&#xA;        },&#xA;        &#34;portablility&#34;: {&#xA;            &#34;YOUR_PORTABILITY_KEY&#34;: ,&#xA;            //...&#xA;        },&#xA;    },&#xA;    &#34;pre&#34;: {&#xA;        &#34;rewrite_acc&#34;: ,&#xA;        &#34;rephrase_acc&#34;: ,&#xA;        &#34;portablility&#34;: {&#xA;            &#34;YOUR_PORTABILITY_KEY&#34;: ,&#xA;            //...&#xA;        },&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For evaluation for Reliablilty, you only need to provide the corresponding editing &lt;code&gt;prompts&lt;/code&gt; and editing &lt;code&gt;target_new&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For evaluation for Generalization, &lt;code&gt;rephrase_prompts&lt;/code&gt; are required.&lt;/li&gt; &#xA; &lt;li&gt;For evaluation for Locality and Portablility, you need to define the name of the corresponding metric, as well as &lt;code&gt;prompts&lt;/code&gt; and &lt;code&gt;ground_truth&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;blockquote&gt; &#xA;     &lt;p&gt;Note: the length needs to be equal to the edit prompts&lt;/p&gt; &#xA;    &lt;/blockquote&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Trainer&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;meta-learning based: &lt;code&gt;MEND&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;memory-based routing: &lt;code&gt;SERAC&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For above editing methods, pre-training of corresponding meta-networks or classifiers is required. Therefore, in EasyEdit, we provide a unified framework for pretraining the relevant network structures. Take the training MEND for example:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt; and &lt;strong&gt;Step 2&lt;/strong&gt; are the same as the example above, which involves selecting the appropriate editing model and editing method.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step3: Provide the edit training set&lt;/strong&gt; The currently supported and available datasets are: &lt;code&gt;zsre&lt;/code&gt; and &lt;code&gt;counterfact&lt;/code&gt;(&lt;a href=&#34;https://drive.google.com/file/d/1v99zkInGC5g4G1uBwXuajE65kjEuppT5/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;). Please place them in the &#34;data&#34; directory and initialize the dataset_class (&lt;code&gt;ZsreDataset&lt;/code&gt; for zsre and &lt;code&gt;CounterFactDataset&lt;/code&gt; for counterfact) to load the corresponding training set.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_ds = ZsreDataset(&#39;./data/zsre_mend_train.json&#39;, config=training_hparams)&#xA;eval_ds = ZsreDataset(&#39;./data/zsre_mend_eval.json&#39;, config=training_hparams)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step4: Combine them into a &lt;code&gt;Trainer&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;trainer = EditTrainer(&#xA;    config=training_hparams,&#xA;    train_set=train_ds,&#xA;    val_set=eval_ds&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step6: Run and Edit&lt;/strong&gt; Done! We can conduct Run and Evaluation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;trainer.run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run: The &lt;code&gt;CHECKPOINT&lt;/code&gt; will be saved to the path &lt;code&gt;RESULTS_DIR&lt;/code&gt;(in &lt;code&gt;global.yml&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Edit: Set the &lt;code&gt;archive&lt;/code&gt; field in the &lt;strong&gt;hparams file&lt;/strong&gt; to &lt;code&gt;CHECKPOINT&lt;/code&gt;. EasyEdit will automatically load the corresponding pre-trained weights during the editing process(&lt;a href=&#34;https://raw.githubusercontent.com/zjunlp/EasyEdit/main/#use-easyedit&#34;&gt;Go to edit&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- ## Overall Results&#xA;&gt; Note that the following experimental results are from this [paper](https://arxiv.org/abs/2305.13172).The actual editing performance of this tool is still under testing and will be announced **as soon as possible**.&#xA;*  We tested the editing performance of different knowledge editing methods on various model, the test results are shown in the table below(`-` refers to the results that the methods empirically fail to edit LLMs). --&gt; &#xA;&lt;!--&#xA;- For `zsre` dataset:&#xA;&#xA;&lt;div style=&#34;text-align: center&#34;&gt;&#xA;&lt;table style=&#34;text-align: center&#34;&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;th&gt;&lt;/th&gt;&lt;th colspan=&#34;3&#34; style=&#34;text-align: center;&#34;&gt;T5-3B&lt;/th&gt;&lt;th colspan=&#34;3&#34; style=&#34;text-align: center;&#34;&gt;GPT-J&lt;/th&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;&lt;b&gt;Method&lt;/b&gt;&lt;/td&gt;&lt;td&gt;Reliability&lt;/td&gt;&lt;td&gt;Generalization&lt;/td&gt;&lt;td&gt;Locality&lt;/td&gt;&lt;td&gt;Reliability&lt;/td&gt;&lt;td&gt;Generalization&lt;/td&gt;&lt;td&gt;Locality&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;FT-L&lt;/td&gt;&lt;td&gt;20.71&lt;/td&gt;&lt;td&gt;19.68&lt;/td&gt;&lt;td&gt;89.01&lt;/td&gt;&lt;td&gt;54.70&lt;/td&gt;&lt;td&gt;49.20&lt;/td&gt;&lt;td&gt;37.24&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;SERAC&lt;/td&gt;&lt;td&gt;99.80&lt;/td&gt;&lt;td&gt;99.66&lt;/td&gt;&lt;td&gt;98.13&lt;/td&gt;&lt;td&gt;90.16&lt;/td&gt;&lt;td&gt;89.96&lt;/td&gt;&lt;td&gt;99.90&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;IKE&lt;/td&gt;&lt;td&gt;67.00&lt;/td&gt;&lt;td&gt;67.11&lt;/td&gt;&lt;td&gt;63.60&lt;/td&gt;&lt;td&gt;99.96&lt;/td&gt;&lt;td&gt;99.87&lt;/td&gt;&lt;td&gt;59.21&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;KE&lt;/td&gt;&lt;td&gt;3.00&lt;/td&gt;&lt;td&gt;5.40&lt;/td&gt;&lt;td&gt;96.43&lt;/td&gt;&lt;td&gt;6.60&lt;/td&gt;&lt;td&gt;7.80&lt;/td&gt;&lt;td&gt;94.18&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;MEND&lt;/td&gt;&lt;td&gt;78.80&lt;/td&gt;&lt;td&gt;89.80&lt;/td&gt;&lt;td&gt;98.45&lt;/td&gt;&lt;td&gt;45.60&lt;/td&gt;&lt;td&gt;48.00&lt;/td&gt;&lt;td&gt;88.21&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;KN&lt;/td&gt;&lt;td&gt;22.51&lt;/td&gt;&lt;td&gt;22.70&lt;/td&gt;&lt;td&gt;16.43&lt;/td&gt;&lt;td&gt;11.34&lt;/td&gt;&lt;td&gt;9.40&lt;/td&gt;&lt;td&gt;90.03&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;ROME&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;99.18&lt;/td&gt;&lt;td&gt;94.90&lt;/td&gt;&lt;td&gt;99.19&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;MEMIT&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;99.23&lt;/td&gt;&lt;td&gt;87.16&lt;/td&gt;&lt;td&gt;99.62&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;&lt;/table&gt;&#xA;&lt;/div&gt;&#xA;&#xA;- For `counterfact` dataset:&#xA;&#xA;&lt;div style=&#34;text-align: center&#34;&gt;&#xA;&lt;table style=&#34;text-align: center&#34;&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;th&gt;&lt;/th&gt;&lt;th colspan=&#34;3&#34; style=&#34;text-align: center;&#34;&gt;T5-3B&lt;/th&gt;&lt;th colspan=&#34;3&#34; style=&#34;text-align: center;&#34;&gt;GPT-J&lt;/th&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;&lt;b&gt;Method&lt;/b&gt;&lt;/td&gt;&lt;td&gt;Reliability&lt;/td&gt;&lt;td&gt;Generalization&lt;/td&gt;&lt;td&gt;Locality&lt;/td&gt;&lt;td&gt;Reliability&lt;/td&gt;&lt;td&gt;Generalization&lt;/td&gt;&lt;td&gt;Locality&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;FT-L&lt;/td&gt;&lt;td&gt;33.57&lt;/td&gt;&lt;td&gt;23.54&lt;/td&gt;&lt;td&gt;72.72&lt;/td&gt;&lt;td&gt;99.90&lt;/td&gt;&lt;td&gt;97.53&lt;/td&gt;&lt;td&gt;1.02&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;SERAC&lt;/td&gt;&lt;td&gt;99.89&lt;/td&gt;&lt;td&gt;98.71&lt;/td&gt;&lt;td&gt;99.93&lt;/td&gt;&lt;td&gt;99.78&lt;/td&gt;&lt;td&gt;99.41&lt;/td&gt;&lt;td&gt;98.89&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;IKE&lt;/td&gt;&lt;td&gt;97.77&lt;/td&gt;&lt;td&gt;82.99&lt;/td&gt;&lt;td&gt;37.76&lt;/td&gt;&lt;td&gt;99.61&lt;/td&gt;&lt;td&gt;72.67&lt;/td&gt;&lt;td&gt;35.57&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;KE&lt;/td&gt;&lt;td&gt;1.00&lt;/td&gt;&lt;td&gt;1.40&lt;/td&gt;&lt;td&gt;96.28&lt;/td&gt;&lt;td&gt;13.40&lt;/td&gt;&lt;td&gt;11.00&lt;/td&gt;&lt;td&gt;94.38&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;MEND&lt;/td&gt;&lt;td&gt;81.40&lt;/td&gt;&lt;td&gt;93.40&lt;/td&gt;&lt;td&gt;91.58&lt;/td&gt;&lt;td&gt;73.80&lt;/td&gt;&lt;td&gt;74.20&lt;/td&gt;&lt;td&gt;93.75&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;KN&lt;/td&gt;&lt;td&gt;47.86&lt;/td&gt;&lt;td&gt;46.78&lt;/td&gt;&lt;td&gt;57.10&lt;/td&gt;&lt;td&gt;1.66&lt;/td&gt;&lt;td&gt;1.38&lt;/td&gt;&lt;td&gt;58.28&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;ROME&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;99.80&lt;/td&gt;&lt;td&gt;86.63&lt;/td&gt;&lt;td&gt;93.61&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;    &lt;tr&gt;&#xA;        &lt;td&gt;MEMIT&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;td&gt;99.90&lt;/td&gt;&lt;td&gt;73.13&lt;/td&gt;&lt;td&gt;97.17&lt;/td&gt;&#xA;    &lt;/tr&gt;&#xA;&lt;/table&gt;&#xA;&lt;/div&gt; --&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt; &lt;b&gt; TO DO &lt;/b&gt; &lt;/summary&gt; In next version, we plan to: &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;release a multimodal Editor for LLMs.&lt;/li&gt; &#xA;  &lt;li&gt;support more editing methods for &lt;code&gt;BaiChuan&lt;/code&gt;, &lt;code&gt;FALCON&lt;/code&gt;, etc.&lt;/li&gt; &#xA;  &lt;li&gt;knowledge editing for other tasks(except factual editing), like &lt;code&gt;textual knowledge editing&lt;/code&gt;, &lt;code&gt;personality editing&lt;/code&gt;, etc.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Meanwhile, we will offer long-term maintenance to fix bugs, solve issues and meet new requests. So if you have any problems, please put issues to us.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite our paper if you use EasyEdit in your work.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{yao2023editing,&#xA;      title={Editing Large Language Models: Problems, Methods, and Opportunities},&#xA;      author={Yunzhi Yao and Peng Wang and Bozhong Tian and Siyuan Cheng and Zhoubo Li and Shumin Deng and Huajun Chen and Ningyu Zhang},&#xA;      year={2023},&#xA;      eprint={2305.13172},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🎉Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/zjunlp/EasyEdit/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=zjunlp/EasyEdit&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;We thank all the contributors to this project, more contributors are welcome!&lt;/p&gt; &#xA;&lt;h4&gt;Other Related Projects&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kmeng01/rome&#34;&gt;ROME&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hiyouga/FastEdit&#34;&gt;FastEdit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;🙌 We would like to express our heartfelt gratitude for the contribution of &lt;a href=&#34;https://github.com/kmeng01/rome&#34;&gt;ROME&lt;/a&gt; to our project, as we have utilized portions of their source code in our project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>alaamaalouf/FollowAnything</title>
    <updated>2023-08-17T01:29:44Z</updated>
    <id>tag:github.com,2023-08-17:/alaamaalouf/FollowAnything</id>
    <link href="https://github.com/alaamaalouf/FollowAnything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Follow Anything&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.05737&#34;&gt;📰 Paper&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=6Mgt3EPytrw&#34;&gt;📹 Explainer Video&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Follow Anything:&lt;/em&gt; Open-set detection, tracking, and following in real-time.&lt;/h4&gt; &#xA;&lt;p&gt;🏛️ &lt;strong&gt;&lt;a href=&#34;https://www.csail.mit.edu/&#34;&gt;1 CSAIL, MIT&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href=&#34;https://www.csail.mit.edu/%5D(https://seas.harvard.edu/)https://seas.harvard.edu/&#34;&gt;2 SEAS, Harvard&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;✍️ &lt;em&gt;&lt;a href=&#34;https://www.csail.mit.edu/person/alaa-maalouf&#34;&gt;Alaa Maalouf&lt;/a&gt;, &lt;a href=&#34;https://react.seas.harvard.edu/people/ninad-jadhav&#34;&gt;Ninad Jadhav&lt;/a&gt;, &lt;a href=&#34;https://krrish94.github.io/&#34;&gt;Krishna Murthy Jatavallabhula&lt;/a&gt;, &lt;a href=&#34;https://www.mit.edu/~chahine/&#34;&gt;Makram Chahine&lt;/a&gt;, &lt;a href=&#34;https://www.danielmvogt.com/&#34;&gt;Daniel M.Vogt&lt;/a&gt;, &lt;a href=&#34;https://wyss.harvard.edu/team/associate-faculty/robert-wood/&#34;&gt;Robert J. Wood&lt;/a&gt;, &lt;a href=&#34;https://groups.csail.mit.edu/vision/torralbalab/&#34;&gt;Antonio Torralba&lt;/a&gt;, and &lt;a href=&#34;https://danielarus.csail.mit.edu/&#34;&gt;Daniela Rus&lt;/a&gt;&lt;/em&gt; ✍️&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alaamaalouf/FollowAnything/main/Images_and_videos_for_Github_visualizations/teaser.png?raw=true&#34; alt=&#34;FAn design&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;FAn&lt;/em&gt; - Follow Anything is a robotic system to detect, track, and follow any object in real-time while accounting for occlusion and object re-emergence. &lt;em&gt;FAn&lt;/em&gt; is an open-vocabulary and multimodal system -- it is not restricted to concepts seen at training time and can be initialized/queried using text, images, or clicks.&lt;/p&gt; &#xA;&lt;h2&gt;📹 Demo Videos&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Car following and re-detecting:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/alaamaalouf/FollowAnything/main/Images_and_videos_for_Github_visualizations/Car_following.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/alaamaalouf/FollowAnything/main/Images_and_videos_for_Github_visualizations/Car_tracking.gif&#34; width=&#34;200&#34;&gt; &lt;/p&gt;&#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Drone following:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/alaamaalouf/FollowAnything/main/Images_and_videos_for_Github_visualizations/drone_following.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/alaamaalouf/FollowAnything/main/Images_and_videos_for_Github_visualizations/drone_tracking.gif&#34; width=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Manually moving brick following and re-detecting:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/alaamaalouf/FollowAnything/main/Images_and_videos_for_Github_visualizations/Brick_following.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/alaamaalouf/FollowAnything/main/Images_and_videos_for_Github_visualizations/Brick_tracking.gif&#34; width=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;🤔 FAn approach&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alaamaalouf/FollowAnything/main/Images_and_videos_for_Github_visualizations/example_visualize.png?raw=true&#34; alt=&#34;FAn approach&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;An input frame of $4$ whales with a click query on a whale and click query on water.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;In the first step, SAM extracts multiple masks (segmentations), then,&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;based on DINO features (for text queries we use CLIP, and for audio queries, we could use AudioCLIP), FAn classifies each mask to what object it refers to from the given queries (water/whales).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Finally, whales are detected by assigning the masks whose DINO feature descriptor is closest to the whales&#39; query feature descriptor.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;NOTE: The heat maps are shown in the click (query figures).&lt;/p&gt; &#xA;&lt;h2&gt;Automatic re-detection&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alaamaalouf/FollowAnything/main/Images_and_videos_for_Github_visualizations/redetection_example.png?raw=true&#34; alt=&#34;FAn approach&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;FAn supports Automatic re-detection via cross-trajectory stored ViT features.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;At every frame, FAn stores the DINO features representing the tracked object.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once the object is lost, FAn either applies a segmentation model or gets suggested masks from the tracker.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For every mask, FAn computes the DINO/CLIP descriptors, and&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Compares it to the pre-computed ones.&lt;/p&gt; &lt;p&gt;a. If a high similarity is obtained: keep tracking the object,&lt;/p&gt; &lt;p&gt;b. else&#34; GOTO step (3) again on the next frame.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The code was tested with &lt;code&gt;python=3.9.12&lt;/code&gt;, as well as &lt;code&gt;pytorch=1.9.0+cu102&lt;/code&gt; and &lt;code&gt;torchvision=0.10.0+cu102&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository locally:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA; git clone https://github.com/alaamaalouf/FollowAnything.git&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the directory Segment-and-Track-Anything as explained in: &lt;a href=&#34;https://github.com/z-x-yang/Segment-and-Track-Anything&#34;&gt;https://github.com/z-x-yang/Segment-and-Track-Anything&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Then, download:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; a. SAM model to Segment-and-Track-Anything/ckpt directory, the default model is SAM-VIT-B (sam_vit_b_01ec64.pth).&#xA;&#xA; b. DeAOT/AOT model to Segment-and-Track-Anything/ckpt directory, the default model is R50-DeAOT-L (R50_DeAOTL_PRE_YTB_DAV.pth).&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note - some files are slightly modified in the directory Segment-and-Track-Anything, hence, use the version provided in this directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you wish to use SiamMask as a tracker (default is AOT from step &#34;2&#34;) install SiamMask as detailed in: &lt;a href=&#34;https://github.com/foolwood/SiamMask&#34;&gt;https://github.com/foolwood/SiamMask&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;pip install mavsdk (you may need to do more simple pip installs for other libraries).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Example Commands&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;pre&gt;&lt;code&gt;python follow_anything.py --desired_height 240 --desired_width 320 --path_to_video example_videos/car_following.avi --save_images_to outputs/ --detect dino --redetect_by tracker --use_sam --tracker aot --queries_dir queries/toy_car_following --desired_feature 0 --plot_visualization&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;pre&gt;&lt;code&gt;python follow_anything.py --desired_height 240 --desired_width 320 --path_to_video example_videos/brick_following.avi --save_images_to outputs/ --detect dino --use_sam --tracker aot --queries_dir queries/brick_following/ --desired_feature 7 --plot_visualizations&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;pre&gt;&lt;code&gt;python follow_anything.py --desired_height 240 --desired_width 320 --path_to_video example_videos/car_following.avi --save_images_to outputs/ --detect box --redetect_by box --use_sam --tracker aot  -plot_visualization&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage on offline video&lt;/h2&gt; &#xA;&lt;p&gt;First, we show how to run the system, without the drone or online stream, i.e., we show how to detect and track a desired object from a video.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;To manually detect and track a desired object by a bounding box:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python follow_anything.py  --desired_height 240 --desired_width 320 --path_to_video &amp;lt;PATH TO VIDEO&amp;gt; --save_images_to outputs/ --detect box --redetect_by box --tracker aot --plot_visualizations&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;    a. --desired_height and desired_height: the desired image width and height to work with.&#xA;&#xA;    b. --path_to_video: full or relative path to the desired video.&#xA;&#xA;    c. --save_images_to: will store all outputs of &#34;segmentations/tracking/detection&#34; to the provided directory.&#xA;&#xA;    d. --detect: either by box, click, dino, or clip -- dino and clip require additional flags (see next section).&#xA;&#xA;    e. --tracker: either aot or SiamMask&#xA;&#xA;    f. --plot_visualizations: plots all stages visualizations. &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;To automatically detect and track a desired object we apply the following two stages:&lt;/p&gt; &lt;p&gt;2.1 annotation phase - run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python annotate_features.py --desired_height 240 --desired_width 320 --queries_dir &amp;lt;directory where to store the queries features&amp;gt; --path_to_images &amp;lt;path to a directory containing the images we wish to annotate&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;2.1. Run the system with --detect dino -redetect_by dino&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python follow_anything.py  --desired_height 240 --desired_width 320 --path_to_video &amp;lt;PATH TO VIDEO&amp;gt; --save_images_to outputs/  --detect dino --redetect_by dino --use_sam --tracker aot --queries_dir &amp;lt;directory where you stored the queries features in step a&amp;gt;  --desired_feature &amp;lt;desired_label&amp;gt;  --plot_visualizations&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code&gt;  a. queries_dir: the directory where you stored the queries features in step a&#xA;  b. desired_feature:  the label of the desired annotated object.&#xA;  c. use_sam: use sam before detection to provide segmentation (slower but better) ---&amp;gt; You can remove this flag to get faster detection.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For faster Dino detection performance add: &lt;code&gt; --use_16bit --use_traced_model&lt;/code&gt; and remove &lt;code&gt;--use_sam&lt;/code&gt; (this mode is less accurate but more efficient).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To use text for detection add:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;--detect clip --desired_feature &amp;lt;text explaining the desired feature as well as possible&amp;gt;  --use_sam  --text_query &amp;lt;text explaining the desired feature as well as possible, text explaining object two, ..., text explaining the last object in the scene&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage on an online video stream and a drone&lt;/h2&gt; &#xA;&lt;p&gt;All you need is to pick the relevant command as explained in the section above and add the flags &lt;code&gt;--path_to_video rtsp://192.168.144.10:8554/H264Video --fly_drone --port ttyUSB0 --baud 57600&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--path_to_video rtsp://192.168.144.10:8554/H264Video&lt;/code&gt;: Path to the stream.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--fly_drone&lt;/code&gt;: Indication to fly the drone.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--port ttyUSB0&lt;/code&gt;: The used port for connecting to the drone.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--baud 57600&lt;/code&gt;: Baud rate.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;♥&lt;/span&gt; Acknowledgment&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on the following awesome works &lt;a href=&#34;https://github.com/z-x-yang/Segment-and-Track-Anything&#34;&gt;Segment-and-Track-Anything&lt;/a&gt;, &lt;a href=&#34;https://github.com/ShirAmir/dino-vit-features&#34;&gt;dino-vit-features&lt;/a&gt;, &lt;a href=&#34;https://github.com/concept-fusion/concept-fusion&#34;&gt;ConceptFusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/foolwood/SiamMask&#34;&gt;SiamMask&lt;/a&gt;, and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thanks for these amazing contributions!&lt;/p&gt; &#xA;&lt;h2&gt;🖋️ Citations&lt;/h2&gt; &#xA;&lt;p&gt;Please consider citing our paper and the related paper(s) (see links) in your publications if it helps your research.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{maalouf2023follow,&#xA;  title={Follow Anything: Open-set detection, tracking, and following in real-time},&#xA;  author={Maalouf, Alaa and Jadhav, Ninad and Jatavallabhula, Krishna Murthy and Chahine, Makram and Vogt, Daniel M and Wood, Robert J and Torralba, Antonio and Rus, Daniela},&#xA;  journal={arXiv preprint arXiv:2308.05737},&#xA;  year={2023}&#xA;}&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>