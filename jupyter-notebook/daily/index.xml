<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-13T01:32:26Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>RonaldJEN/FinanceChatGLM</title>
    <updated>2023-08-13T01:32:26Z</updated>
    <id>tag:github.com,2023-08-13:/RonaldJEN/FinanceChatGLM</id>
    <link href="https://github.com/RonaldJEN/FinanceChatGLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SMP 2023 ChatGLM金融大模型挑战赛 60 分baseline思路介绍&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SMP 2023 ChatGLM 金融大模型挑战赛 60 分 Baseline 思路&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;队伍:&lt;/strong&gt; 小打小闹&lt;/p&gt; &#xA;&lt;h2&gt;数据提取整体过程&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/001.png&#34; alt=&#34;数据提取整体过程&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;PDF 表格识别结果对比&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/002.jpeg&#34; alt=&#34;识别结果1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/003.jpeg&#34; alt=&#34;识别结果2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;自研PDF表格识别逻辑&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/004.png&#34; alt=&#34;逻辑1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/005.png&#34; alt=&#34;逻辑2&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/006.png&#34; alt=&#34;逻辑3&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;主要步骤:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;定位表格区域&lt;/li&gt; &#xA; &lt;li&gt;识别单元格&lt;/li&gt; &#xA; &lt;li&gt;跨页表格合并&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;识别单元格并生成表格算法伪代码&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/007.jpeg&#34; alt=&#34;伪代码1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/008.jpeg&#34; alt=&#34;伪代码2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;基于有限状态机的数据提取&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/009.png&#34; alt=&#34;有限状态机&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;资产负债表示意图 (三大表之一) 数据入库&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/010.jpeg&#34; alt=&#34;数据入库&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;主要内容:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;公司基本信息&lt;/li&gt; &#xA; &lt;li&gt;资产负债表&lt;/li&gt; &#xA; &lt;li&gt;现金流量表&lt;/li&gt; &#xA; &lt;li&gt;利润表&lt;/li&gt; &#xA; &lt;li&gt;公司员工信息&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;公司全称与简称及代码对照&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/011.jpeg&#34; alt=&#34;对照表&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;信息从表格转为文本描述&lt;/h2&gt; &#xA;&lt;h3&gt;公司员工信息&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/012.png&#34; alt=&#34;员工信息&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;文本描述示例: 安靠智电&lt;/h3&gt; &#xA;&lt;p&gt;安靠智电（股票代码：300617）在2019年共有642名职工，其中74人是研发人员，研发人员占比11.53%。该公司有10名硕士学历以上学历的员工，但没有博士学历的员工。&lt;/p&gt; &#xA;&lt;h2&gt;整体推理流程&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/013.jpeg&#34; alt=&#34;推理流程&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;⚠️ 不要相信大模型的数学能力&lt;/h2&gt; &#xA;&lt;h3&gt;改进前&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/014.png&#34; alt=&#34;改进前&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;改进后&lt;/h3&gt; &#xA;&lt;p&gt;直接帮他算好。&lt;/p&gt; &#xA;&lt;p&gt;安记食品2019年营业利润为49072627.15元, 2019年营业收入为421296738.60元。根据公式:&lt;/p&gt; &#xA;&lt;p&gt;[ \text{营业利润率} = \frac{\text{营业利润}}{\text{营业收入}} \times 100 ]&lt;/p&gt; &#xA;&lt;p&gt;得出结果安记食品2019年营业利润率为11.65%。&lt;/p&gt; &#xA;&lt;h2&gt;⚠️ 不要相信大模型的推理能力&lt;/h2&gt; &#xA;&lt;h3&gt;建议 剔除冗余信息，否则可能无法得到正确答案。&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RonaldJEN/FinanceChatGLM/main/img/015.jpeg&#34; alt=&#34;推理示例&#34;&gt; &#34;&#34;&#34;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>justLV/onju-voice</title>
    <updated>2023-08-13T01:32:26Z</updated>
    <id>tag:github.com,2023-08-13:/justLV/onju-voice</id>
    <link href="https://github.com/justLV/onju-voice" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A hackable AI home assistant platform&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Onju Voice 🍐🔈&lt;/h1&gt; &#xA;&lt;p&gt;💫 &lt;a href=&#34;https://twitter.com/justLV&#34;&gt;DEMO&#39;s&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A hackable AI home assistant platform using the Google Nest Mini (2nd gen) form factor, consisting of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a custom PCB designed to be a drop-in replacement to the original, using the ESP32-S3 for audio processing&lt;/li&gt; &#xA; &lt;li&gt;a server for handling the transcription, response generation and Text-to-Speech from multiple devices on the same network&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/images/header_white.jpg&#34; width=&#34;960&#34;&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains firmware, server code and some example applications, intended to be as accessible as possible for getting up and running i.e.:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/#-firmware&#34;&gt;Firmware&lt;/a&gt; for the custom PCB can be programmed using the Arduino IDE and a USB cable (installation of ESP-IDF not required)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/#%EF%B8%8F-server&#34;&gt;Server code&lt;/a&gt; has minimal requirements besides running Whisper locally, and should be able to run on most devices that you can leave plugged in whether MacOS / Linux / Win etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/images/rich.png&#34;&gt; &#xA;&lt;h2&gt;Example applications&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📩 Querying and replying to messages (using a &lt;a href=&#34;https://github.com/justLV/onju-voice-maubot&#34;&gt;custom Maubot plugin&lt;/a&gt; &amp;amp; Beeper)&lt;/li&gt; &#xA; &lt;li&gt;💡 Light control with &lt;a href=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/#-home-assistant&#34;&gt;Home Assistant&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;📝 Adding and retrieving notes/memos for the LLM to craft a response with&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Not included:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;👥 Multiple voice characters. I’ll leave it to the user to clone voices as they deem fair use. Also from experience LLM’s &amp;lt; GPT4 don’t consistently enough follow instructions to reliably respond in different characters AND perform multiple function calling with complicated prompts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Current features of the device &amp;lt;&amp;gt; server platform&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Auto-discovery of devices using multicast announcements&lt;/li&gt; &#xA; &lt;li&gt;Remembering conversation history and voice settings for each device&lt;/li&gt; &#xA; &lt;li&gt;Sending &amp;amp; receiving audio data from the device, packed as 16-bit, 16kHz (UDP sending, TCP receiving partially buffered into PSRAM)&lt;/li&gt; &#xA; &lt;li&gt;Speaker and microphone visualization with the LED’s, and custom LED control via the server&lt;/li&gt; &#xA; &lt;li&gt;Mute switch functionality, tap-to-wake for enabling the microphone, and setting mic timeout via the server&lt;/li&gt; &#xA; &lt;li&gt;Device-level logging to individual files and console output using &lt;code&gt;rich&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;[coming soon] SoftAP WiFi provisioning to prevent need for programming WiFi credentials&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Limitations of this release:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The Arduino IDE doesn’t (yet) support the Espressif’s Audio SDK’s, such as &lt;a href=&#34;https://github.com/espressif/esp-adf&#34;&gt;ESP-ADF&lt;/a&gt;, &lt;a href=&#34;https://github.com/espressif/esp-skainet&#34;&gt;ESP-Skainet&lt;/a&gt; etc. For these demo&#39;s it&#39;s not absolutely required, but if you use Espressif’s ESP-IDF with these SDK&#39;s you&#39;d unlock features such as: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;VAD (Voice Activity Detection) - in this example VAD is offloaded to the server using webrtcvad, and the listening period is extended by either tapping the device or by the server sending mic keep alive timeouts (network traffic is really minimal at 16-bit, 16kHz)&lt;/li&gt; &#xA;   &lt;li&gt;AEC (Acoustic Echo Cancellation) - to allow you to effectively talk over the assistant by removing the speaker output from audio input&lt;/li&gt; &#xA;   &lt;li&gt;BSS (Blind Source Separation) - let’s you use both mic’s for isolating speakers based on location, and other noise suppression&lt;/li&gt; &#xA;   &lt;li&gt;Wakewords and other on-device commands - I’m not a believer in this given how finicky these can be and don’t think these are and think all command logic should be handled by layers of language models on the server.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The server currently only does transcription locally and uses: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OpenAI for generating responses &amp;amp; functions calls, but if you have the hardware you could run a local LLM, using something like ToolLLM for calling API’s to add almost any capabilities you’d wish.&lt;/li&gt; &#xA;   &lt;li&gt;Text-to-speech from Elevenlabs - this is fair to say the easiest to get running, fastest and most expressive option out there but FWIR data policy is a little dubious so careful about sending anything too sensitive. I’d really like to see comparable performing open source options that you can run locally&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Conversation flow is highly serialized, i.e. recording &amp;gt; transcription &amp;gt; LLM &amp;gt; TTS needs to finish each step before moving onto the next. Not included here is feeding incomplete transcriptions to a smaller model, and streaming slower LLM&#39;s like GPT4 to Elevenlabs and sending streaming responses back, it&#39;s currently a little too hacky to include in this release.&lt;/li&gt; &#xA; &lt;li&gt;No wakeword usage, mostly done intentionally as I feel uttering a wake-word before every response is a terrible experience. This currently uses a combo of VAD, mic-timeouts sent from server, tap-to-wake, mute switch usage etc. Not included here is experiments running a smaller, faster LLM for classification with a running transcription before handing off to a larger LLM with specific prompt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Other areas for improvement&lt;/h2&gt; &#xA;&lt;p&gt;These are things I didn&#39;t get time to implement but I believe would be invaluable and pretty achievable&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaker diarization - know who is saying what, and have the LLM enage in multi-user conversations or infer when it isn&#39;t being spoken to&lt;/li&gt; &#xA; &lt;li&gt;Interruptions - requires AEC for simultaneous listening and playback&lt;/li&gt; &#xA; &lt;li&gt;Smaller local models/LLM&#39;s for running classification, detecting intent and routing to larger LLM&#39;s&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;🖥️ Server&lt;/h2&gt; &#xA;&lt;p&gt;Ensure you can install &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt; and run at least the base model, following any debugging steps they have if not. If you can get past that, it should be as simple as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd server&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Adjust settings in the &lt;code&gt;config.yaml&lt;/code&gt;, and tweak aspects such as how much silence is needed to start processing to trade-off snappiness vs avoiding cutting off the user.&lt;/p&gt; &#xA;&lt;p&gt;Add your Elevenlabs token to &lt;code&gt;credentials.json&lt;/code&gt; and ensure you have a cloned voice in your account that you set in the &lt;code&gt;config.yaml&lt;/code&gt; under &lt;code&gt;elevenlabs_default_voice&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ll also need a greeting WAV set in &lt;code&gt;config.yaml&lt;/code&gt; under &lt;code&gt;greeting_wav&lt;/code&gt;, that will be sent to devices on connecting to the WiFi. This is up to you to record or procure (&lt;a href=&#34;https://github.com/ytdl-org/youtube-dl&#34;&gt;e.g.&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;A small subset of the config parameters can be set as optional arguments when running the script. For e.g. the following will run the server with note-taking, Home Assistant, Maubot, real sending of messages enabled (a safe guard disabled by default), and a smaller English only Whisper model for transcription.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python server.py --n --ha --mb --send --whisper base.en&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;🏡 Home Assistant&lt;/h3&gt; &#xA;&lt;p&gt;I recommend setting this up on the same server or one that is always plugged in on your network, following the &lt;a href=&#34;https://www.home-assistant.io/installation/linux#docker-compose&#34;&gt;Docker Compose instructions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then go through the onboarding, setup a user, name your devices and get a Long Lived token to add to &lt;code&gt;credentials.json&lt;/code&gt; together with the URL e.g. &lt;code&gt;http://my-local-server:8123/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;🤖 Maubot&lt;/h3&gt; &#xA;&lt;p&gt;Follow instructions &lt;a href=&#34;https://github.com/justLV/onju-home-maubot&#34;&gt;here&lt;/a&gt; to setup Maubot with your Beeper account. Ensure the correct URL is setup in &lt;code&gt;config.yaml&lt;/code&gt;, set &lt;code&gt;send_replies&lt;/code&gt; to True if your friends are forgiving of the odd mistakes, and set a &lt;code&gt;footer&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Don’t have Beeper yet and can’t wait? &lt;a href=&#34;https://docs.mau.fi/bridges/go/imessage/mac/setup.html&#34;&gt;Try setup a Matrix bridge yourself&lt;/a&gt; and a custom function definition for OpenAI function calling (and share how you did it!)&lt;/p&gt; &#xA;&lt;p&gt;Following this example you can also integrate e-mail.&lt;/p&gt; &#xA;&lt;h2&gt;📟 Firmware&lt;/h2&gt; &#xA;&lt;p&gt;Irrespective of what you use for development, the quickest &amp;amp; least error prone setup for building &amp;amp; flashing firmware is probably installing the Arduino IDE &lt;a href=&#34;https://www.arduino.cc/en/software&#34;&gt;Software&lt;/a&gt;, and then using this IDE or your preference i.e. VSCode for development (Copilot)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add the ESP32 boards as detailed &lt;a href=&#34;https://docs.espressif.com/projects/arduino-esp32/en/latest/installing.html&#34;&gt;here&lt;/a&gt; (TL;DR add &lt;code&gt;https://espressif.github.io/arduino-esp32/package_esp32_index.json&lt;/code&gt; to &lt;code&gt;Preferences &amp;gt; Additional Boards Manager URL’s&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Under Boards Manager, install “esp32” by Espressif Systems&lt;/li&gt; &#xA; &lt;li&gt;Under Library Manager, install “Adafruit NeoPixel Library”&lt;/li&gt; &#xA; &lt;li&gt;Clone this repo to &lt;code&gt;Documents/Arduino&lt;/code&gt; for simplicity.&lt;/li&gt; &#xA; &lt;li&gt;Add your WiFi credentials to &lt;code&gt;credentials.h&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;bash setup-git-hash.sh&lt;/code&gt; to add a header with the git-hash (optional). This will then automatically update after commits, and help track the firmware that your devices are running from the server side.&lt;/li&gt; &#xA; &lt;li&gt;Open File &amp;gt; Sketchbook &amp;gt; onju-home &amp;gt; onjuino&lt;/li&gt; &#xA; &lt;li&gt;Select Tools &amp;gt; Board &amp;gt; esp32 &amp;gt; ESP32S3 Dev Module&lt;/li&gt; &#xA; &lt;li&gt;Under Tools ensure: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;USB CDC on Boot set to Enabled&lt;/li&gt; &#xA;   &lt;li&gt;PSRAM set to OPI PSRAM&lt;/li&gt; &#xA;   &lt;li&gt;Board is plugged in and Port is selected (you may need to install USB bridge drivers as detailed by Espressif, don’t worry if name is incorrect)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Build and upload&lt;/li&gt; &#xA; &lt;li&gt;If not reset, press the reset button. In Serial Monitor you can also send &lt;code&gt;r&lt;/code&gt; to reset the device (assuming it is already booted)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🧩 Hardware&lt;/h2&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/images/copper.png&#34; width=&#34;48%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/justLV/onju-voice/master/images/render.png&#34; width=&#34;48%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://365.altium.com/files/C44B8519-69BA-464B-A221-24D527B89E2C&#34;&gt;Schematics &amp;amp; PCB preview&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PCB&#39;s will be made available from Crowd Supply (&lt;a href=&#34;https://www.crowdsupply.com/onju/onju-voice&#34;&gt;sign up link&lt;/a&gt;), to leverage their fulfillment expertize and bulk ordering.&lt;/p&gt; &#xA;&lt;p&gt;I will be sharing more detailed instructions for replacement.&lt;/p&gt; &#xA;&lt;p&gt;Replacement gaskets for the microphone &amp;amp; LED&#39;s can be made using &lt;a href=&#34;https://www.amazon.com/gp/product/B07KCJ31J9&#34;&gt;adhesive foam&lt;/a&gt; and a &lt;a href=&#34;https://www.amazon.com/gp/product/B087D2Z43F&#34;&gt;punch set&lt;/a&gt;) for example&lt;/p&gt; &#xA;&lt;h2&gt;❓Questions&lt;/h2&gt; &#xA;&lt;h3&gt;Does this replace my Google Nest Mini?&lt;/h3&gt; &#xA;&lt;p&gt;While this replicates the interfaces of the Google Nest Mini, don’t expect this to be a 1:1 replacement, for e.g. it is not intended to be a music playback device (although there is probably no reason it couldn’t be developed to be used as such). It’s also worth re-iterating that like the Google Nest Mini, this requires a separate server, although this can be in your home running local models instead of in a Google datacenter. &lt;strong&gt;The original is well tested, maintained, certified and works out the box, while this is essentially a dev board with some neat examples for you to build on top of&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;What if I don’t have a Google Nest Mini but still want to use this?&lt;/h3&gt; &#xA;&lt;p&gt;Fortunately they’re still being sold, you may find deals for &amp;lt;$40 which is pretty good for the quality of speaker and form factor. I picked up quite a few from eBay, just make sure you get the 2nd gen.&lt;/p&gt; &#xA;&lt;p&gt;The adventurous can get try replacement shells from &lt;a href=&#34;https://www.aliexpress.us/item/3256803723188315.html&#34;&gt;AliExpress&lt;/a&gt; for e.g., but you’ll still need a base, power input, mute switch, speaker &amp;amp; mount, capacitive touch panels, and replacement gaskets etc. A hero out there could design a custom enclosure that fits an off-the-shelf speaker.&lt;/p&gt; &#xA;&lt;h3&gt;But I’m really impatient and want to get hacking away! What can I do?&lt;/h3&gt; &#xA;&lt;p&gt;a) if you can commit to making significant contributions to the codebase and/or major contributions to the board design or RF review, we may be able to make early samples available&lt;/p&gt; &#xA;&lt;p&gt;b) if you don’t need the form factor, don’t mind rolling up my sleeves, and have some HW experience, you can breadboard it out with readily available components until you can get your hands on an order. Here are the components that should be able to get a demo running (🌸 Adafruit link for convenience but shop around wherever you’d like)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ESP32-S3 devboard, ideally w/ PSRAM (e.g. &lt;a href=&#34;https://www.adafruit.com/product/5700&#34;&gt;QT Py S3&lt;/a&gt; or &lt;a href=&#34;https://www.adafruit.com/product/5364&#34;&gt;ESP32-S3&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/3421&#34;&gt;Microphone&lt;/a&gt; (only need 1 for the Arduino implementation, ensure it&#39;s a SPH0645 to limit debugging)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/3006&#34;&gt;Amplifier&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/1313&#34;&gt;Speaker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/1426&#34;&gt;Neopixel LED strip&lt;/a&gt; - just set the firmware to the correct #&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adafruit.com/product/3314&#34;&gt;Breadboard &amp;amp; wire kit&lt;/a&gt; (you can use protruding pieces of wire for cap touch)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You&#39;ll need to update the &lt;code&gt;custom_boards.h&lt;/code&gt; with your pin mapping&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;🍐 PR&#39;s, issues, suggestions &amp;amp; general feedback welcome!🏡&lt;/strong&gt;&lt;/h2&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/PUG</title>
    <updated>2023-08-13T01:32:26Z</updated>
    <id>tag:github.com,2023-08-13:/facebookresearch/PUG</id>
    <link href="https://github.com/facebookresearch/PUG" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the repository for the Photorealistic Unreal Graphics (PUG) datasets for representation learning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;font size=&#34;9&#34;&gt;&lt;/font&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;font size=&#34;9&#34;&gt;&lt;b&gt;PUG&lt;/b&gt;: &lt;b&gt;P&lt;/b&gt;hotorealistic &lt;b&gt;U&lt;/b&gt;nreal &lt;b&gt;G&lt;/b&gt;raphics&lt;/font&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;font size=&#34;9&#34;&gt;&lt;/font&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;font size=&#34;9&#34;&gt; &lt;a href=&#34;https://pug.metademolab.com&#34;&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2308.03977&#34;&gt;&lt;strong&gt;Research Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://pug.metademolab.com/faq.html&#34;&gt;&lt;strong&gt;Datasheet&lt;/strong&gt;&lt;/a&gt; &lt;/font&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/PUG/assets/5903040/5fd73746-a45b-4056-ae99-3726dadb51a8&#34;&gt;https://github.com/facebookresearch/PUG/assets/5903040/5fd73746-a45b-4056-ae99-3726dadb51a8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This codebase contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;download links for the PUG-datasets&lt;/li&gt; &#xA; &lt;li&gt;dataloaders&lt;/li&gt; &#xA; &lt;li&gt;scripts that are needed to samples images from a running interactive environment made with the Unreal Engine.&lt;/li&gt; &#xA; &lt;li&gt;script to evaluate VLMs models with PUG: SPAR&lt;/li&gt; &#xA; &lt;li&gt;list of the assets used to create the PUG datasets (which are listed in each PUG folders)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Downloading the PUG datasets&lt;/h2&gt; &#xA;&lt;p&gt;Here are the links to download the PUG datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/large_objects/pug/PUG_ANIMAL.tar.gz&#34;&gt;PUG: Animals (78GB)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/large_objects/pug/PUG_IMAGENET.tar.gz&#34;&gt;PUG: ImageNet (27GB)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/large_objects/pug/PUG_SPAR.tar.gz&#34;&gt;PUG: SPAR (16GB)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/large_objects/pug/PUG_AR4T.tar.gz&#34;&gt;PUG: AR4T (97GB)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dataset loaders&lt;/h2&gt; &#xA;&lt;p&gt;Please look at each PUG subfolder to get information on how to load the datasets.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/PUG/main/PUG_Animals&#34;&gt;PUG Animals&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/PUG/main/PUG_ImageNet&#34;&gt;PUG ImageNet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/PUG/main/PUG_SPAR&#34;&gt;PUG SPAR&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/PUG/main/PUG_AR4T&#34;&gt;PUG AR4T&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to create a PUG environment ?&lt;/h2&gt; &#xA;&lt;p&gt;The instruction are availables in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/PUG/main/torchmultiverse&#34;&gt;torchmultiverse&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;The datasets are distributed under the CC-BY-NC, with the addenda that they should not be used to train Generative AI models, as found in the LICENSE file.&lt;/p&gt; &#xA;&lt;h2&gt;Citing PUG&lt;/h2&gt; &#xA;&lt;p&gt;If you use the PUG datasets, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{bordes2023pug,&#xA;      title={PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning}, &#xA;      author={Florian Bordes and Shashank Shekhar and Mark Ibrahim and Diane Bouchacourt and Pascal Vincent and Ari S. Morcos},&#xA;      year={2023},&#xA;      eprint={2308.03977},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>