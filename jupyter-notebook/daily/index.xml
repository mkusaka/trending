<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-24T01:35:48Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>alex-sokolov2011/100_Numpy_exercises_Rus_ver</title>
    <updated>2022-09-24T01:35:48Z</updated>
    <id>tag:github.com,2022-09-24:/alex-sokolov2011/100_Numpy_exercises_Rus_ver</id>
    <link href="https://github.com/alex-sokolov2011/100_Numpy_exercises_Rus_ver" rel="alternate"></link>
    <summary type="html">&lt;p&gt;100 —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π –ø–æ numpy –≤–µ—Ä—Å–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;100 numpy exercises (100 —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π –ø–æ numpy)&lt;/h1&gt; &#xA;&lt;p&gt;–Ø –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–ª –∑–Ω–∞–º–µ–Ω–∏—Ç—ã–µ 100 —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ numpy. –ö–∞–∂–¥–æ–µ –∑–∞–¥–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏—Ç:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;–ø–µ—Ä–µ–≤–æ–¥ —É—Å–ª–æ–≤–∏–π –∑–∞–¥–∞–Ω–∏–π –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ (–≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–∞—Ö —Å—Ç–∞—Ä–∞–ª—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ)&lt;/li&gt; &#xA; &lt;li&gt;—Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –∫–ª–∞—Å—Å–æ–≤ (–º–µ—Ç–æ–¥–æ–≤, —Ñ—É–Ω–∫—Ü–∏–π) –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ —Ä–µ—à–µ–Ω–∏–∏ –ø–æ–¥ –∫–∞–∂–¥—ã–º –∑–∞–¥–∞–Ω–∏–µ–º&lt;/li&gt; &#xA; &lt;li&gt;–∫—Ä–∞—Ç–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ (–º–µ—Ç–æ–¥–æ–≤, —Ñ—É–Ω–∫—Ü–∏–π) –∏ –∞—Ç—Ä–∏–±—É—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤ —Ä–µ—à–µ–Ω–∏–∏ —Ä—è–¥–æ–º —Å–æ —Å—Å—ã–ª–∫–∞–º–∏&lt;/li&gt; &#xA; &lt;li&gt;—É–¥–∞–ª—ë–Ω —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫–æ–¥ –Ω–∞ —Å–µ–Ω—Ç—è–±—Ä—å 2022 –≥–æ–¥–∞ (–≤–µ—Ä—Å–∏—è 1.23.2)&lt;/li&gt; &#xA; &lt;li&gt;–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã –æ—à–∏–±–∫–∏&lt;/li&gt; &#xA; &lt;li&gt;–≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Å–¥–µ–ª–∞–Ω —Ä–∞–∑–±–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;–ó–≤–µ–∑–¥–æ—á–∫–∞–º–∏ –≤ —Å–∫–æ–±–æ—á–∫–∞—Ö –æ—Ç–º–µ—á–µ–Ω–∞ —É—Å–ª–æ–≤–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π. –ù–∞ –º–æ–π –≤–∑–≥–ª—è–¥ –º–∞—Ç–µ—Ä–∏–∞–ª –±—É–¥–µ—Ç –ø–æ–ª–µ–∑–µ–Ω:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;–Ω–æ–≤–∏—á–∫–∞–º, —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å –∞–Ω–≥–ª–∏–π—Å–∫–∏–º –∏ –∫–æ—Ç–æ—Ä—ã–º –≤–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å —É—Å–ª–æ–≤–∏–µ –∑–∞–¥–∞–Ω–∏—è –ø–µ—Ä–µ–¥ —Ç–µ–º –∫–∞–∫ –µ–≥–æ –¥–µ–ª–∞—Ç—å&lt;/li&gt; &#xA; &lt;li&gt;—Ç–µ–º –∫—Ç–æ —É–∂–µ —É–≤–µ—Ä–µ–Ω–Ω–æ –ø–æ—Å—Ç–∏–≥–∞–µ—Ç pandas –∏ –Ω–∞—á–∞–ª –ø–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ –∏–Ω–æ–≥–¥–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–µ–µ –∑–∞–ø—Ä–æ–≥–∞—Ç—å –Ω–∞ numpy&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;–ñ–µ–ª–∞—é –ø–ª–æ–¥–æ—Ç–≤–æ—Ä–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ numpy –≤ Python! &lt;a href=&#34;https://github.com/alex-sokolov2011/100_Numpy_exercises_Rus_ver/raw/main/100_Numpy_exercises_Rus_ver.ipynb&#34;&gt;—Å–º–æ—Ç—Ä–µ—Ç—å jupyter notebook —Ç—É—Ç&lt;/a&gt;. –î–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫ —Å –∫–æ–¥–æ–º –µ—Å—Ç—å &lt;a href=&#34;https://www.kaggle.com/code/sokolovaleks/100-numpy-exercises-rus-version&#34;&gt;–≤–µ—Ä—Å–∏—è –Ω–∞ kaggle&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –ø—Ä–∏–º–µ—Ä—ã —Ö–æ—Ä–æ—à–∏—Ö –∏ –ø–æ–ª–µ–∑–Ω—ã—Ö –∑–∞–¥–∞–Ω–∏–π –ø–æ numpy, –ø–∏—à–∏—Ç–µ –º–Ω–µ &lt;a href=&#34;mailto:sokaa2011@gmail.com&#34;&gt;sokaa2011@gmail.com&lt;/a&gt; - —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ–º –æ–ø—É–±–ª–∏–∫—É—é —Ç—É—Ç –∏–ª–∏ –¥–æ–±–∞–≤–ª—è–π—Ç–µ —Å–∞–º–∏ –≤ contribution.&lt;/p&gt; &#xA;&lt;p&gt;–û—Ä–∏–≥–∏–Ω–∞–ª –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —Ç—É—Ç &lt;a href=&#34;https://github.com/rougier/numpy-100&#34;&gt;https://github.com/rougier/numpy-100&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>scikit-learn-contrib/MAPIE</title>
    <updated>2022-09-24T01:35:48Z</updated>
    <id>tag:github.com,2022-09-24:/scikit-learn-contrib/MAPIE</id>
    <link href="https://github.com/scikit-learn-contrib/MAPIE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A scikit-learn-compatible module for estimating prediction intervals.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;.. -&lt;em&gt;- mode: rst -&lt;/em&gt;-&lt;/p&gt; &#xA;&lt;p&gt;|GitHubActions|_ |Codecov|_ |ReadTheDocs|_ |License|_ |PythonVersion|_ |PyPi|_ |Conda|_ |Release|_ |Commits|_ |DOI|_&lt;/p&gt; &#xA;&lt;p&gt;.. |GitHubActions| image:: &lt;a href=&#34;https://github.com/scikit-learn-contrib/MAPIE/actions/workflows/test.yml/badge.svg&#34;&gt;https://github.com/scikit-learn-contrib/MAPIE/actions/workflows/test.yml/badge.svg&lt;/a&gt; .. _GitHubActions: &lt;a href=&#34;https://github.com/scikit-learn-contrib/MAPIE/actions&#34;&gt;https://github.com/scikit-learn-contrib/MAPIE/actions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |Codecov| image:: &lt;a href=&#34;https://codecov.io/gh/scikit-learn-contrib/MAPIE/branch/master/graph/badge.svg?token=F2S6KYH4V1&#34;&gt;https://codecov.io/gh/scikit-learn-contrib/MAPIE/branch/master/graph/badge.svg?token=F2S6KYH4V1&lt;/a&gt; .. _Codecov: &lt;a href=&#34;https://codecov.io/gh/scikit-learn-contrib/MAPIE&#34;&gt;https://codecov.io/gh/scikit-learn-contrib/MAPIE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |ReadTheDocs| image:: &lt;a href=&#34;https://readthedocs.org/projects/mapie/badge&#34;&gt;https://readthedocs.org/projects/mapie/badge&lt;/a&gt; .. _ReadTheDocs: &lt;a href=&#34;https://mapie.readthedocs.io/en/latest&#34;&gt;https://mapie.readthedocs.io/en/latest&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |License| image:: &lt;a href=&#34;https://img.shields.io/github/license/simai-ml/MAPIE&#34;&gt;https://img.shields.io/github/license/simai-ml/MAPIE&lt;/a&gt; .. _License: &lt;a href=&#34;https://github.com/scikit-learn-contrib/MAPIE/raw/master/LICENSE&#34;&gt;https://github.com/scikit-learn-contrib/MAPIE/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |PythonVersion| image:: &lt;a href=&#34;https://img.shields.io/pypi/pyversions/mapie&#34;&gt;https://img.shields.io/pypi/pyversions/mapie&lt;/a&gt; .. _PythonVersion: &lt;a href=&#34;https://pypi.org/project/mapie/&#34;&gt;https://pypi.org/project/mapie/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |PyPi| image:: &lt;a href=&#34;https://img.shields.io/pypi/v/mapie&#34;&gt;https://img.shields.io/pypi/v/mapie&lt;/a&gt; .. _PyPi: &lt;a href=&#34;https://pypi.org/project/mapie/&#34;&gt;https://pypi.org/project/mapie/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |Conda| image:: &lt;a href=&#34;https://img.shields.io/conda/vn/conda-forge/mapie&#34;&gt;https://img.shields.io/conda/vn/conda-forge/mapie&lt;/a&gt; .. _Conda: &lt;a href=&#34;https://anaconda.org/conda-forge/mapie&#34;&gt;https://anaconda.org/conda-forge/mapie&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |Release| image:: &lt;a href=&#34;https://img.shields.io/github/v/release/scikit-learn-contrib/mapie&#34;&gt;https://img.shields.io/github/v/release/scikit-learn-contrib/mapie&lt;/a&gt; .. _Release: &lt;a href=&#34;https://github.com/scikit-learn-contrib/MAPIE/releases&#34;&gt;https://github.com/scikit-learn-contrib/MAPIE/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |Commits| image:: &lt;a href=&#34;https://img.shields.io/github/commits-since/scikit-learn-contrib/mapie/latest/master&#34;&gt;https://img.shields.io/github/commits-since/scikit-learn-contrib/mapie/latest/master&lt;/a&gt; .. _Commits: &lt;a href=&#34;https://github.com/scikit-learn-contrib/MAPIE/commits/master&#34;&gt;https://github.com/scikit-learn-contrib/MAPIE/commits/master&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |DOI| image:: &lt;a href=&#34;https://img.shields.io/badge/10.48550/arXiv.2207.12274-B31B1B.svg&#34;&gt;https://img.shields.io/badge/10.48550/arXiv.2207.12274-B31B1B.svg&lt;/a&gt; .. _DOI: &lt;a href=&#34;https://arxiv.org/abs/2207.12274&#34;&gt;https://arxiv.org/abs/2207.12274&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://github.com/simai-ml/MAPIE/raw/master/doc/images/mapie_logo_nobg_cut.png&#34;&gt;https://github.com/simai-ml/MAPIE/raw/master/doc/images/mapie_logo_nobg_cut.png&lt;/a&gt; :width: 400 :align: center&lt;/p&gt; &#xA;&lt;h1&gt;MAPIE - Model Agnostic Prediction Interval Estimator&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;MAPIE&lt;/strong&gt; allows you to easily estimate prediction intervals (or prediction sets) using your favourite scikit-learn-compatible model for single-output regression or multi-class classification settings.&lt;/p&gt; &#xA;&lt;p&gt;Prediction intervals output by &lt;strong&gt;MAPIE&lt;/strong&gt; encompass both aleatoric and epistemic uncertainties and are backed by strong theoretical guarantees thanks to conformal prediction methods [1-7].&lt;/p&gt; &#xA;&lt;h1&gt;üîó Requirements&lt;/h1&gt; &#xA;&lt;p&gt;Python 3.7+&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MAPIE&lt;/strong&gt; stands on the shoulders of giants.&lt;/p&gt; &#xA;&lt;p&gt;Its only internal dependencies are &lt;code&gt;scikit-learn &amp;lt;https://scikit-learn.org/stable/&amp;gt;&lt;/code&gt;_ and &lt;code&gt;numpy=&amp;gt;1.21 &amp;lt;https://numpy.org/&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h1&gt;üõ† Installation&lt;/h1&gt; &#xA;&lt;p&gt;Install via &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: sh&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install mapie&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or via &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;.. code:: sh&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda install -c conda-forge mapie&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install directly from the github repository :&lt;/p&gt; &#xA;&lt;p&gt;.. code:: sh&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install git+https://github.com/scikit-learn-contrib/MAPIE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;‚ö°Ô∏è Quickstart&lt;/h1&gt; &#xA;&lt;p&gt;Let us start with a basic regression problem. Here, we generate one-dimensional noisy data that we fit with a linear model.&lt;/p&gt; &#xA;&lt;p&gt;.. code:: python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;from sklearn.linear_model import LinearRegression&#xA;from sklearn.datasets import make_regression&#xA;&#xA;regressor = LinearRegression()&#xA;X, y = make_regression(n_samples=500, n_features=1, noise=20, random_state=59)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since MAPIE is compliant with the standard scikit-learn API, we follow the standard sequential &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt; process like any scikit-learn regressor. We set two values for alpha to estimate prediction intervals at approximately one and two standard deviations from the mean.&lt;/p&gt; &#xA;&lt;p&gt;.. code:: python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from mapie.regression import MapieRegressor&#xA;alpha = [0.05, 0.32]&#xA;mapie = MapieRegressor(regressor)&#xA;mapie.fit(X, y)&#xA;y_pred, y_pis = mapie.predict(X, alpha=alpha)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MAPIE returns a &lt;code&gt;np.ndarray&lt;/code&gt; of shape &lt;code&gt;(n_samples, 3, len(alpha))&lt;/code&gt; giving the predictions, as well as the lower and upper bounds of the prediction intervals for the target quantile for each desired alpha value.&lt;/p&gt; &#xA;&lt;p&gt;You can compute the coverage of your prediction intervals.&lt;/p&gt; &#xA;&lt;p&gt;.. code:: python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from mapie.metrics import regression_coverage_score&#xA;coverage_scores = [&#xA;    regression_coverage_score(y, y_pis[:, 0, i], y_pis[:, 1, i])&#xA;    for i, _ in enumerate(alpha)&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The estimated prediction intervals can then be plotted as follows.&lt;/p&gt; &#xA;&lt;p&gt;.. code:: python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from matplotlib import pyplot as plt&#xA;plt.xlabel(&#34;x&#34;)&#xA;plt.ylabel(&#34;y&#34;)&#xA;plt.scatter(X, y, alpha=0.3)&#xA;plt.plot(X, y_pred, color=&#34;C1&#34;)&#xA;order = np.argsort(X[:, 0])&#xA;plt.plot(X[order], y_pis[order][:, 0, 1], color=&#34;C1&#34;, ls=&#34;--&#34;)&#xA;plt.plot(X[order], y_pis[order][:, 1, 1], color=&#34;C1&#34;, ls=&#34;--&#34;)&#xA;plt.fill_between(&#xA;    X[order].ravel(),&#xA;    y_pis[order][:, 0, 0].ravel(),&#xA;    y_pis[order][:, 1, 0].ravel(),&#xA;    alpha=0.2&#xA;)&#xA;plt.title(&#xA;    f&#34;Target and effective coverages for &#34;&#xA;    f&#34;alpha={alpha[0]:.2f}: ({1-alpha[0]:.3f}, {coverage_scores[0]:.3f})\n&#34;&#xA;    f&#34;Target and effective coverages for &#34;&#xA;    f&#34;alpha={alpha[1]:.2f}: ({1-alpha[1]:.3f}, {coverage_scores[1]:.3f})&#34;&#xA;)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The title of the plot compares the target coverages with the effective coverages. The target coverage, or the confidence interval, is the fraction of true labels lying in the prediction intervals that we aim to obtain for a given dataset. It is given by the alpha parameter defined in &lt;code&gt;MapieRegressor&lt;/code&gt;, here equal to 0.05 and 0.32, thus giving target coverages of 0.95 and 0.68. The effective coverage is the actual fraction of true labels lying in the prediction intervals.&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://github.com/simai-ml/MAPIE/raw/master/doc/images/quickstart_1.png&#34;&gt;https://github.com/simai-ml/MAPIE/raw/master/doc/images/quickstart_1.png&lt;/a&gt; :width: 400 :align: center&lt;/p&gt; &#xA;&lt;h1&gt;üìò Documentation&lt;/h1&gt; &#xA;&lt;p&gt;The full documentation can be found &lt;code&gt;on this link &amp;lt;https://mapie.readthedocs.io/en/latest/&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How does MAPIE work on regression ?&lt;/strong&gt; It is basically based on cross-validation and relies on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Conformity scores on the whole training set obtained by cross-validation,&lt;/li&gt; &#xA; &lt;li&gt;Perturbed models generated during the cross-validation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;MAPIE&lt;/strong&gt; then combines all these elements in a way that provides prediction intervals on new data with strong theoretical guarantees [1-2].&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://github.com/simai-ml/MAPIE/raw/master/doc/images/mapie_internals_regression.png&#34;&gt;https://github.com/simai-ml/MAPIE/raw/master/doc/images/mapie_internals_regression.png&lt;/a&gt; :width: 300 :align: center&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How does MAPIE work on classification ?&lt;/strong&gt; It is based on the construction of calibrated conformity scores to estimate prediction sets and relies on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Construction of a conformity score&lt;/li&gt; &#xA; &lt;li&gt;Calibration of the conformity score on a calibration set not seen by the model during training&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;MAPIE&lt;/strong&gt; then uses the calibrated conformity scores to estimate sets of labels associated with the desired coverage on new data with strong theoretical guarantees [3-4-5].&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://github.com/simai-ml/MAPIE/raw/master/doc/images/mapie_internals_classification.png&#34;&gt;https://github.com/simai-ml/MAPIE/raw/master/doc/images/mapie_internals_classification.png&lt;/a&gt; :width: 300 :align: center&lt;/p&gt; &#xA;&lt;h1&gt;üìù Contributing&lt;/h1&gt; &#xA;&lt;p&gt;You are welcome to propose and contribute new ideas. We encourage you to &lt;code&gt;open an issue &amp;lt;https://github.com/simai-ml/MAPIE/issues&amp;gt;&lt;/code&gt;_ so that we can align on the work to be done. It is generally a good idea to have a quick discussion before opening a pull request that is potentially out-of-scope. For more information on the contribution process, please go &lt;code&gt;here &amp;lt;CONTRIBUTING.rst&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h1&gt;ü§ù Affiliations&lt;/h1&gt; &#xA;&lt;p&gt;MAPIE has been developed through a collaboration between Quantmetry, Michelin, and ENS Paris-Saclay with the financial support from R√©gion Ile de France.&lt;/p&gt; &#xA;&lt;p&gt;|Quantmetry|_ |Michelin|_ |ENS|_ |IledeFrance|_&lt;/p&gt; &#xA;&lt;p&gt;.. |Quantmetry| image:: &lt;a href=&#34;https://www.quantmetry.com/wp-content/uploads/2020/08/08-Logo-quant-Texte-noir.svg&#34;&gt;https://www.quantmetry.com/wp-content/uploads/2020/08/08-Logo-quant-Texte-noir.svg&lt;/a&gt; :width: 150 .. _Quantmetry: &lt;a href=&#34;https://www.quantmetry.com/&#34;&gt;https://www.quantmetry.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |Michelin| image:: &lt;a href=&#34;https://www.michelin.com/wp-content/themes/michelin/public/img/michelin-logo-en.svg&#34;&gt;https://www.michelin.com/wp-content/themes/michelin/public/img/michelin-logo-en.svg&lt;/a&gt; :width: 100 .. _Michelin: &lt;a href=&#34;https://www.michelin.com/en/&#34;&gt;https://www.michelin.com/en/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |ENS| image:: &lt;a href=&#34;https://file.diplomeo-static.com/file/00/00/01/34/13434.svg&#34;&gt;https://file.diplomeo-static.com/file/00/00/01/34/13434.svg&lt;/a&gt; :width: 100 .. _ENS: &lt;a href=&#34;https://ens-paris-saclay.fr/en&#34;&gt;https://ens-paris-saclay.fr/en&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |IledeFrance| image:: &lt;a href=&#34;https://www.iledefrance.fr/themes/custom/portail_idf/logo.svg&#34;&gt;https://www.iledefrance.fr/themes/custom/portail_idf/logo.svg&lt;/a&gt; :width: 100 .. _IledeFrance: &lt;a href=&#34;https://www.iledefrance.fr/&#34;&gt;https://www.iledefrance.fr/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üîç References&lt;/h1&gt; &#xA;&lt;p&gt;MAPIE methods belong to the field of conformal inference.&lt;/p&gt; &#xA;&lt;p&gt;[1]&amp;nbsp;Rina Foygel Barber, Emmanuel J. Cand√®s, Aaditya Ramdas, and Ryan J. Tibshirani. &#34;Predictive inference with the jackknife+.&#34; Ann. Statist., 49(1):486‚Äì507, February 2021.&lt;/p&gt; &#xA;&lt;p&gt;[2] Byol Kim, Chen Xu, and Rina Foygel Barber. &#34;Predictive Inference Is Free with the Jackknife+-after-Bootstrap.&#34; 34th Conference on Neural Information Processing Systems (NeurIPS 2020).&lt;/p&gt; &#xA;&lt;p&gt;[3] Mauricio Sadinle, Jing Lei, and Larry Wasserman. &#34;Least Ambiguous Set-Valued Classifiers With Bounded Error Levels.&#34; Journal of the American Statistical Association, 114:525, 223-234, 2019.&lt;/p&gt; &#xA;&lt;p&gt;[4] Yaniv Romano, Matteo Sesia and Emmanuel J. Cand√®s. &#34;Classification with Valid and Adaptive Coverage.&#34; NeurIPS 202 (spotlight).&lt;/p&gt; &#xA;&lt;p&gt;[5] Anastasios Nikolas Angelopoulos, Stephen Bates, Michael Jordan and Jitendra Malik. &#34;Uncertainty Sets for Image Classifiers using Conformal Prediction.&#34; International Conference on Learning Representations 2021.&lt;/p&gt; &#xA;&lt;p&gt;[6] Yaniv Romano, Evan Patterson, Emmanuel J. Cand√®s. &#34;Conformalized Quantile Regression.&#34; Advances in neural information processing systems 32 (2019).&lt;/p&gt; &#xA;&lt;p&gt;[7] Chen Xu and Yao Xie. &#34;Conformal Prediction Interval for Dynamic Time-Series.&#34; International Conference on Machine Learning (ICML, 2021).&lt;/p&gt; &#xA;&lt;h1&gt;üìù License&lt;/h1&gt; &#xA;&lt;p&gt;MAPIE is free and open-source software licensed under the &lt;code&gt;3-clause BSD license &amp;lt;https://github.com/simai-ml/MAPIE/blob/master/LICENSE&amp;gt;&lt;/code&gt;_.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>FrozenBurning/Text2Light</title>
    <updated>2022-09-24T01:35:48Z</updated>
    <id>tag:github.com,2022-09-24:/FrozenBurning/Text2Light</id>
    <link href="https://github.com/FrozenBurning/Text2Light" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[SIGGRAPH Asia 2022] Text2Light: Zero-Shot Text-Driven HDR Panorama Generation&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;Text2Light: Zero-Shot Text-Driven HDR Panorama Generation&lt;/h1&gt; &#xA; &lt;div&gt; &#xA;  &lt;a href=&#34;https://frozenburning.github.io/&#34; target=&#34;_blank&#34;&gt;Zhaoxi Chen&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://wanggcong.github.io/&#34; target=&#34;_blank&#34;&gt;Guangcong Wang&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://liuziwei7.github.io/&#34; target=&#34;_blank&#34;&gt;Ziwei Liu&lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;   S-Lab, Nanyang Technological University &#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://sa2022.siggraph.org/&#34; target=&#34;_blank&#34;&gt;TOG 2022 (Proc. SIGGRAPH Asia)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;h3&gt;TL;DR&lt;/h3&gt; &#xA; &lt;h4&gt;Text2Light can generate HDR panoramas in 4K+ resolution using free-form texts solely. &lt;br&gt; Our high-quality results can be directly applied to downstream tasks, e.g., light 3D scenes and immersive VR.&lt;/h4&gt; &#xA; &lt;h3&gt;&lt;a href=&#34;https://frozenburning.github.io/projects/text2light&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/XDx6tOHigPE&#34;&gt;Video&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2209.09898&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/FrozenBurning/Text2Light/blob/master/text2light.ipynb&#34;&gt;Colab&lt;/a&gt;&lt;/h3&gt;  &#xA; &lt;img src=&#34;https://github.com/FrozenBurning/FrozenBurning.github.io/raw/master/projects/text2light/img/teaser.gif&#34; width=&#34;100%&#34;&gt;  &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;[09/2022] Our online demo in Colab is released! &lt;a href=&#34;https://colab.research.google.com/github/FrozenBurning/Text2Light/blob/master/text2light.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[09/2022] Paper uploaded to arXiv. &lt;a href=&#34;https://arxiv.org/abs/2209.09898&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2209.09898-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[09/2022] Model weights released. &lt;a href=&#34;https://drive.google.com/drive/folders/1HKBjC7oQOzrkGFKMQmSh6PySv6AycDS3?usp=sharing&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Google%20Drive-4285F4?style=for-the-badge&amp;amp;logo=googledrive&amp;amp;logoColor=yellow&#34; alt=&#34;Google Drive&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[09/2022] Code released.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful for your research, please consider citing this paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{chen2022text2light,&#xA;    title={Text2Light: Zero-Shot Text-Driven HDR Panorama Generation},&#xA;    author={Chen, Zhaoxi and Wang, Guangcong and Liu, Ziwei},&#xA;    journal={ACM Transactions on Graphics (TOG)},&#xA;    volume={41},&#xA;    number={6},&#xA;    articleno={195},&#xA;    pages={1--16},&#xA;    year={2022},&#xA;    publisher={ACM New York, NY, USA}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We highly recommend using &lt;a href=&#34;https://www.anaconda.com/&#34;&gt;Anaconda&lt;/a&gt; to manage your python environment. You can setup the required environment by the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment.yml&#xA;conda activate text2light&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Text-driven HDRI Generation&lt;/h2&gt; &#xA;&lt;p&gt;You may do the following steps to generate HDR panoramas from free-form texts with our models.&lt;/p&gt; &#xA;&lt;h3&gt;Download Pretrained Models&lt;/h3&gt; &#xA;&lt;p&gt;Please download our checkpoints from &lt;a href=&#34;https://drive.google.com/drive/folders/1HKBjC7oQOzrkGFKMQmSh6PySv6AycDS3?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; to run the following inference scripts. We use the model trained on our full dataset by default (&lt;code&gt;local_sampler&lt;/code&gt;). Note that we also release models that trained on outdoor (&lt;code&gt;local_sampler_outdoor&lt;/code&gt;) and indoor (&lt;code&gt;local_sampler_indoor&lt;/code&gt;) scenes respectively.&lt;/p&gt; &#xA;&lt;h3&gt;All-in-one Inference Script&lt;/h3&gt; &#xA;&lt;p&gt;All inference codes are in &lt;a href=&#34;https://raw.githubusercontent.com/FrozenBurning/Text2Light/master/text2light.py&#34;&gt;text2light.py&lt;/a&gt;, you can learn to use it by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python text2light.py -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here are some examples, the output will be saved in &lt;code&gt;./generated_panorama&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate a HDR panorama from a single sentence:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python text2light.py -rg logs/global_sampler_clip -rl logs/local_sampler_outdoor --outdir ./generated_panorama --text &#34;YOUR SCENE DESCRIPTION&#34; --clip clip_emb.npy --sritmo ./logs/sritmo.pth --sr_factor 4&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate HDR panoramas from a list of texts:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# assume your texts is stored in alt.txt&#xA;python text2light.py -rg logs/global_sampler_clip -rl logs/local_sampler_outdoor --outdir ./generated_panorama --text ./alt.txt --clip clip_emb.npy --sritmo ./logs/sritmo.pth --sr_factor 4&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate low-resolution (512x1024) LDR panoramas only:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# assume your texts is stored in alt.txt&#xA;python text2light.py -rg logs/global_sampler_clip -rl logs/local_sampler_outdoor --outdir ./generated_panorama --text ./alt.txt --clip clip_emb.npy&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here are some examples of Text2Light in generating HDRIs. The generated results can be directly used to render 3D scenes like &lt;a href=&#34;https://download.blender.org/demo/test/pabellon_barcelona_v1.scene_.zip&#34;&gt;Barcelona Pavillion&lt;/a&gt; from &lt;a href=&#34;https://www.blender.org/download/demo-files/&#34;&gt;Blender Demo Files&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; width=&#34;50%&#34;&gt;&lt;img src=&#34;https://github.com/FrozenBurning/FrozenBurning.github.io/raw/master/projects/text2light/img/github_demo_house.jpg&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; width=&#34;50%&#34;&gt;&lt;img src=&#34;https://github.com/FrozenBurning/FrozenBurning.github.io/raw/master/projects/text2light/img/github_demo_ball.jpg&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Rendering&lt;/h2&gt; &#xA;&lt;p&gt;Our generated HDR panoramas can be directly used in any modern graphics pipeline as the environment texture and light source. Here we take &lt;a href=&#34;https://www.blender.org/&#34;&gt;Blender&lt;/a&gt; as an example.&lt;/p&gt; &#xA;&lt;h3&gt;From GUI&lt;/h3&gt; &#xA;&lt;p&gt;Open Blender -&amp;gt; Select &lt;code&gt;Shading&lt;/code&gt; Panel -&amp;gt; Select &lt;code&gt;Shader Type&lt;/code&gt; as &lt;code&gt;World&lt;/code&gt; -&amp;gt; Add an &lt;code&gt;Environment Texture&lt;/code&gt; node -&amp;gt; Browse and select our generated panoramas -&amp;gt; Render&lt;/p&gt; &#xA;&lt;p&gt;You can also refer to this &lt;a href=&#34;https://www.youtube.com/watch?v=gC4Uqr4E78U&#34;&gt;tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of rendering a landscape in San Francisco using the HDRI with input texts as &lt;code&gt;landscape photography of mountain ranges under purple and pink skies&lt;/code&gt;.&lt;/p&gt;  &#xA;&lt;img src=&#34;https://github.com/FrozenBurning/FrozenBurning.github.io/raw/master/projects/text2light/img/github_blender.jpg&#34; width=&#34;100%&#34;&gt;  &#xA;&lt;h3&gt;From Command line&lt;/h3&gt; &#xA;&lt;p&gt;For the ease of batch processing, e.g. rendering with multiple HDRIs, we offer scripts in command line for rendering your 3D assets.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the Linux version of Blender from &lt;a href=&#34;https://www.blender.org/download/&#34;&gt;Blender Download Page&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Unpack it and check the usage of Blender: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# assume your downloaded version is 3.1.2&#xA;tar -xzvf blender-3.1.2-linux-x64.tar.xz&#xA;cd blender-3.1.2-linux-x64&#xA;./blender --help&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Add an alias to your .bashrc or .zshrc: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# PATH_TO_DOWNLOADED_BLENDER indicates the parent directory where you save the downloaded blender&#xA;alias blender=&#34;/PATH_TO_DOWNLOADED_BLENDER/blender-3.1.2-linux-x64/blender&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Back to the codebase of Text2Light, and run the following commands for different rendering setup: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Render four shader balls given all HDRIs stored at &lt;code&gt;PATH_TO_HDRI&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;blender --background --python rendering_shader_ball.py -- ./rendered_balls 100 1000 PATH_TO_HDRI&#xA;&lt;/code&gt;&lt;/pre&gt; The results will be saved in &lt;code&gt;./rendered_balls&lt;/code&gt; which looks like:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; width=&#34;50%&#34;&gt;&lt;img src=&#34;https://github.com/FrozenBurning/FrozenBurning.github.io/blob/master/projects/text2light/img/rendered_ball/full_[green grass field with trees and mountains in the distance]_balls.png&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; width=&#34;50%&#34;&gt;&lt;img src=&#34;https://github.com/FrozenBurning/FrozenBurning.github.io/blob/master/projects/text2light/img/rendered_ball/full_[landscape photography of mountain ranges under purple and pink skies]_balls.png&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; width=&#34;50%&#34;&gt;&lt;img src=&#34;https://github.com/FrozenBurning/FrozenBurning.github.io/blob/master/projects/text2light/img/rendered_ball/full_[Audience, Auditorium, Conference]_balls.png&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; width=&#34;50%&#34;&gt;&lt;img src=&#34;https://github.com/FrozenBurning/FrozenBurning.github.io/blob/master/projects/text2light/img/rendered_ball/full_[white bed linen with white pillow]_balls.png&#34; width=&#34;100%&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;Our training is stage-wise with multiple steps. The details are listed as follows.&lt;/p&gt; &#xA;&lt;h3&gt;Data Preparation&lt;/h3&gt; &#xA;&lt;p&gt;Assume all your HDRIs for training are stored at &lt;code&gt;PATH_TO_HDR_DATA&lt;/code&gt;, please run &lt;a href=&#34;https://raw.githubusercontent.com/FrozenBurning/Text2Light/master/process_hdri.py&#34;&gt;process_hdri.py&lt;/a&gt; to process the data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python process_hdri.py --src PATH_TO_HDR_DATA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The processed data will be saved to &lt;code&gt;./data&lt;/code&gt; by default and organized as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ ...&#xA;‚îî‚îÄ‚îÄ Text2Light/&#xA;    ‚îú‚îÄ‚îÄ data/&#xA;        ‚îú‚îÄ‚îÄ train/&#xA;            ‚îú‚îÄ‚îÄ calib_hdr&#xA;            ‚îú‚îÄ‚îÄ ldr&#xA;            ‚îî‚îÄ‚îÄ raw_hdr&#xA;        ‚îú‚îÄ‚îÄ val/&#xA;            ‚îú‚îÄ‚îÄ calib_hdr&#xA;            ‚îú‚îÄ‚îÄ ldr&#xA;            ‚îî‚îÄ‚îÄ raw_hdr&#xA;        ‚îî‚îÄ‚îÄ meta/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Stage I - Text-driven LDR Panorama Generation&lt;/h3&gt; &#xA;&lt;p&gt;The training stage1 is launched by &lt;a href=&#34;https://raw.githubusercontent.com/FrozenBurning/Text2Light/master/train_stage1.py&#34;&gt;train_stage1.py&lt;/a&gt;, you can check the usage by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_stage1.py -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Train the global codebook &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_stage1.py --base configs/global_codebook.yaml -t True --gpu 0,1,2,3,4,5,6,7&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Train the local codebook &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_stage1.py --base configs/local_codebook.yaml -t True --gpu 0,1,2,3,4,5,6,7&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Train the text-conditioned global sampler. Please specify the path to global codebook in the config YAML. &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_stage1.py --base configs/global_sampler_clip.yaml -t True --gpu 0,1,2,3,4,5,6,7 &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Train the structure-aware local sampler. Please specify the path to global and local codebooks in the config YAML, respectively. &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_stage1.py --base configs/local_sampler_spe.yaml -t True --gpu 0,1,2,3,4,5,6,7 &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Stage II - Super-resolution Inverse Tonemapping&lt;/h3&gt; &#xA;&lt;p&gt;The training stage2 is launched by &lt;a href=&#34;https://raw.githubusercontent.com/FrozenBurning/Text2Light/master/train_stage2.py&#34;&gt;train_stage2.py&lt;/a&gt;, you can check the usage by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_stage2.py -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default setting can be trained on a single A100 GPU without DDP:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# assume you use the default --dst_dir in process_hdri.py, thus the hdr dataset would be stored in ./data&#xA;python train_stage2.py --dir ./data --save_dir ./output/bs32_7e-5 --workers 16 --val_ep 5 --gpu 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To enable distributed training, for example, over 8 GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_stage2.py --dir ./data --save_dir ./output/bs32_7e-5 --workers 8 --val_ep 5 --ddp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This work is supported by the National Research Foundation, Singapore under its AI Singapore Programme, NTU NAP, MOE AcRF Tier 2 (T2EP20221-0033), and under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).&lt;/p&gt; &#xA;&lt;p&gt;Text2Light is implemented on top of the &lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;VQGAN&lt;/a&gt; codebase. We also thanks &lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;CLIP&lt;/a&gt; and &lt;a href=&#34;https://github.com/yinboc/liif&#34;&gt;LIIF&lt;/a&gt; for their released models and codes. Thanks this &lt;a href=&#34;https://github.com/yuki-koyama/blender-cli-rendering&#34;&gt;repo&lt;/a&gt; for its amazing command line rendering toolbox.&lt;/p&gt;</summary>
  </entry>
</feed>