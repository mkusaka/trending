<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-28T01:41:59Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>youssefHosni/Practical-Time-Series-In-Python</title>
    <updated>2023-06-28T01:41:59Z</updated>
    <id>tag:github.com,2023-06-28:/youssefHosni/Practical-Time-Series-In-Python</id>
    <link href="https://github.com/youssefHosni/Practical-Time-Series-In-Python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Practical guidance for time series analysis in Python&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Practical Time Series In Python&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/youssefHosni/Practical-Time-Series-In-Python/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/youssefHosni/Practical-Time-Series-In-Python.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/youssefHosni/Practical-Time-Series-In-Python/graphs/contributors/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/youssefHosni/Practical-Time-Series-In-Python.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/youssefHosni/Practical-Time-Series-In-Python/issues/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/youssefHosni/Practical-Time-Series-In-Python.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/youssefHosni/Practical-Time-Series-In-Python/pulls/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/youssefHosni/Practical-Time-Series-In-Python.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/youssefHosni/Practical-Time-Series-In-Python/watchers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/youssefHosni/Practical-Time-Series-In-Python.svg?style=social&amp;amp;label=Watch&#34; alt=&#34;GitHub watchers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/youssefHosni/Practical-Time-Series-In-Python/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/youssefHosni/Practical-Time-Series-In-Python.svg?style=social&amp;amp;label=Fork&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/youssefHosni/Practical-Time-Series-In-Python/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/youssefHosni/Practical-Time-Series-In-Python.svg?style=social&amp;amp;label=Star&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/youssefHosni/Time-Series-With-Python/raw/main/Time-Series-Analysis.jpg&#34; alt=&#34;alt_text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Time series data is one of the most common data types in the industry and you will probably be working with it in your career. Therefore understanding how to work with it and how to apply analytical and forecasting techniques are critical for every aspiring data scientist. In this series of articles, I will go through the basic techniques to work with time-series data, starting from data manipulation, analysis, and visualization to understand your data and prepare it and then using the statistical, machine, and deep learning techniques for forecasting and classification. It will be more of a practical guide in which I will be applying each discussed and explained concept to real data.&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the codes and the data used in the &lt;a href=&#34;https://medium.com/@youssefraafat57/list/time-series-in-python-a152db6b5b2c&#34;&gt;Time Series In Python series of articles on medium&lt;/a&gt;, this series will contain ten articles as the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Manipulating Time Series Data In Python Pandas [A Practical Guide] &lt;a href=&#34;https://github.com/youssefHosni/Time-Series-With-Python/tree/main/Time%20Series%20Manipulation&#34;&gt;Codes &amp;amp; Data&lt;/a&gt; | &lt;a href=&#34;https://pub.towardsai.net/manipulating-time-series-data-in-python-49aed42685a0&#34;&gt;Article&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Time Series Analysis in Python Pandas [A Practical Guide]&amp;nbsp;&lt;a href=&#34;https://github.com/youssefHosni/Time-Series-With-Python/tree/main/Time%20Series%20Analysis&#34;&gt;Codes &amp;amp; Data&lt;/a&gt; | &lt;a href=&#34;https://pub.towardsai.net/time-series-data-analysis-in-python-1492ee4ca974&#34;&gt;Article&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Visualizing Time Series Data in Python [A practical Guide] &lt;a href=&#34;https://github.com/youssefHosni/Time-Series-With-Python/tree/main/Time%20Series%20Data%20Visualization&#34;&gt;Codes &amp;amp; Data&lt;/a&gt; | &lt;a href=&#34;https://pub.towardsai.net/time-series-data-visualization-in-python-2b1959726312&#34;&gt;Article&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Arima Models in Python [A practical Guide] [Part1] &lt;a href=&#34;https://pub.towardsai.net/time-series-forecasting-with-arima-models-in-python-part-1-c2940a7dbc48&#34;&gt;Codes &amp;amp; Data&lt;/a&gt; | &lt;a href=&#34;https://pub.towardsai.net/time-series-forecasting-with-arima-models-in-python-part-1-c2940a7dbc48&#34;&gt;Article&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Arima Models in Python [A practical Guide] [Part2] &lt;a href=&#34;https://pub.towardsai.net/time-series-forecasting-with-arima-models-in-python-part-1-c2940a7dbc48&#34;&gt;Codes &amp;amp; Data&lt;/a&gt; | &lt;a href=&#34;https://pub.towardsai.net/time-series-forecasting-with-arima-models-in-python-part-2-91a30d10efb0&#34;&gt;Article&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Machine Learning for Time Series Data [A practical Guide] [Regression] &lt;a href=&#34;https://github.com/youssefHosni/Time-Series-With-Python/tree/main/Machine%20Learning%20For%20Time%20Series&#34;&gt;Codes &amp;amp; Data&lt;/a&gt; | &lt;a href=&#34;https://pub.towardsai.net/machine-learning-for-time-series-data-in-python-regression-5e19fa2e7471&#34;&gt;Article&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Machine Learning for Time Series Data [A practical Guide] [Classifcation] &lt;a href=&#34;&#34;&gt;Codes &amp;amp; Data&lt;/a&gt; | &lt;a href=&#34;&#34;&gt;Article&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep Learning for Time Series Data [A practical Guide] &lt;a href=&#34;&#34;&gt;Codes &amp;amp; Data&lt;/a&gt; | &lt;a href=&#34;&#34;&gt;Article&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Time Series Forecasting project using statistical analysis, machine learning &amp;amp; deep learning &lt;a href=&#34;&#34;&gt;Codes &amp;amp; Data&lt;/a&gt; | &lt;a href=&#34;&#34;&gt;Article&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Time Series Classification using statistical analysis, machine learning &amp;amp; deep learning &lt;a href=&#34;&#34;&gt;Codes &amp;amp; Data&lt;/a&gt; | &lt;a href=&#34;&#34;&gt;Article&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>danielroich/PTI</title>
    <updated>2023-06-28T01:41:59Z</updated>
    <id>tag:github.com,2023-06-28:/danielroich/PTI</id>
    <link href="https://github.com/danielroich/PTI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Implementation for &#34;Pivotal Tuning for Latent-based editing of Real Images&#34; (ACM TOG 2022) https://arxiv.org/abs/2106.05744&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PTI: Pivotal Tuning for Latent-based editing of Real Images (ACM TOG 2022)&lt;/h1&gt; &#xA;&lt;!-- &gt; Recently, a surge of advanced facial editing techniques have been proposed&#xA;that leverage the generative power of a pre-trained StyleGAN. To successfully&#xA;edit an image this way, one must first project (or invert) the image into&#xA;the pre-trained generator’s domain. As it turns out, however, StyleGAN’s&#xA;latent space induces an inherent tradeoff between distortion and editability,&#xA;i.e. between maintaining the original appearance and convincingly altering&#xA;some of its attributes. Practically, this means it is still challenging to&#xA;apply ID-preserving facial latent-space editing to faces which are out of the&#xA;generator’s domain. In this paper, we present an approach to bridge this&#xA;gap. Our technique slightly alters the generator, so that an out-of-domain&#xA;image is faithfully mapped into an in-domain latent code. The key idea is&#xA;pivotal tuning — a brief training process that preserves the editing quality&#xA;of an in-domain latent region, while changing its portrayed identity and&#xA;appearance. In Pivotal Tuning Inversion (PTI), an initial inverted latent code&#xA;serves as a pivot, around which the generator is fined-tuned. At the same&#xA;time, a regularization term keeps nearby identities intact, to locally contain&#xA;the effect. This surgical training process ends up altering appearance features&#xA;that represent mostly identity, without affecting editing capabilities.&#xA;To supplement this, we further show that pivotal tuning can also adjust the&#xA;generator to accommodate a multitude of faces, while introducing negligible&#xA;distortion on the rest of the domain. We validate our technique through&#xA;inversion and editing metrics, and show preferable scores to state-of-the-art&#xA;methods. We further qualitatively demonstrate our technique by applying&#xA;advanced edits (such as pose, age, or expression) to numerous images of&#xA;well-known and recognizable identities. Finally, we demonstrate resilience&#xA;to harder cases, including heavy make-up, elaborate hairstyles and/or headwear,&#xA;which otherwise could not have been successfully inverted and edited&#xA;by state-of-the-art methods. --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.05744&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2008.00951-b31b1b.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;br&gt; Inference Notebook: &lt;a href=&#34;https://colab.research.google.com/github/danielroich/PTI/blob/main/notebooks/inference_playground.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; height=&#34;20&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielroich/PTI/main/docs/teaser.jpg&#34;&gt; &lt;br&gt; Pivotal Tuning Inversion (PTI) enables employing off-the-shelf latent based semantic editing techniques on real images using StyleGAN. PTI excels in identity preserving edits, portrayed through recognizable figures — Serena Williams and Robert Downey Jr. (top), and in handling faces which are clearly out-of-domain, e.g., due to heavy makeup (bottom). &lt;br&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;Official Implementation of our PTI paper + code for evaluation metrics. PTI introduces an optimization mechanizem for solving the StyleGAN inversion task. Providing near-perfect reconstruction results while maintaining the high editing abilitis of the native StyleGAN latent space W. For more details, see &lt;a href=&#34;https://arxiv.org/abs/2106.05744&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2008.00951-b31b1b.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Recent Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;2021.07.01&lt;/strong&gt;: Fixed files download phase in the inference notebook. Which might caused the notebook not to run smoothly.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2021.06.29&lt;/strong&gt;: Added support for CPU. In order to run PTI on CPU please change &lt;code&gt;device&lt;/code&gt; parameter under &lt;code&gt;configs/global_config.py&lt;/code&gt; to &#34;cpu&#34; instead of &#34;cuda&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2021.06.25&lt;/strong&gt; : Adding mohawk edit using StyleCLIP+PTI in inference notebook. Updating documentation in inference notebook due to Google Drive rate limit reached. Currently, Google Drive does not allow to download the pretrined models using Colab automatically. Manual intervention might be needed.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux or macOS&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU + CUDA CuDNN (Not mandatory bur recommended)&lt;/li&gt; &#xA; &lt;li&gt;Python 3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dependencies: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;lpips&lt;/li&gt; &#xA;   &lt;li&gt;wandb&lt;/li&gt; &#xA;   &lt;li&gt;pytorch&lt;/li&gt; &#xA;   &lt;li&gt;torchvision&lt;/li&gt; &#xA;   &lt;li&gt;matplotlib&lt;/li&gt; &#xA;   &lt;li&gt;dlib&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;All dependencies can be installed using &lt;em&gt;pip install&lt;/em&gt; and the package name&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pretrained Models&lt;/h2&gt; &#xA;&lt;p&gt;Please download the pretrained models from the following links.&lt;/p&gt; &#xA;&lt;h3&gt;Auxiliary Models&lt;/h3&gt; &#xA;&lt;p&gt;We provide various auxiliary models needed for PTI inversion task.&lt;br&gt; This includes the StyleGAN generator and pre-trained models used for loss computation.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Path&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl&#34;&gt;FFHQ StyleGAN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;StyleGAN2-ada model trained on FFHQ with 1024x1024 output resolution.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1HKmjg6iXsWr4aFPuU0gBXPGR83wqMzq7/view?usp=sharing&#34;&gt;Dlib alignment&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Dlib alignment used for images preproccessing.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1ALC5CLA89Ouw40TwvxcwebhzWXM5YSCm/view?usp=sharing&#34;&gt;FFHQ e4e encoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pretrained e4e encoder. Used for StyleCLIP editing.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note: The StyleGAN model is used directly from the official &lt;a href=&#34;https://github.com/NVlabs/stylegan2-ada-pytorch&#34;&gt;stylegan2-ada-pytorch implementation&lt;/a&gt;. For StyleCLIP pretrained mappers, please see &lt;a href=&#34;https://github.com/orpatashnik/StyleCLIP/raw/main/utils.py&#34;&gt;StyleCLIP&#39;s official routes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;By default, we assume that all auxiliary models are downloaded and saved to the directory &lt;code&gt;pretrained_models&lt;/code&gt;. However, you may use your own paths by changing the necessary values in &lt;code&gt;configs/path_configs.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Inversion&lt;/h2&gt; &#xA;&lt;h3&gt;Preparing your Data&lt;/h3&gt; &#xA;&lt;p&gt;In order to invert a real image and edit it you should first align and crop it to the correct size. To do so you should perform &lt;em&gt;One&lt;/em&gt; of the following steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Run &lt;code&gt;notebooks/align_data.ipynb&lt;/code&gt; and change the &#34;images_path&#34; variable to the raw images path&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;utils/align_data.py&lt;/code&gt; and change the &#34;images_path&#34; variable to the raw images path&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Weights And Biases&lt;/h3&gt; &#xA;&lt;p&gt;The project supports &lt;a href=&#34;https://wandb.ai/home&#34;&gt;Weights And Biases&lt;/a&gt; framework for experiment tracking. For the inversion task it enables visualization of the losses progression and the generator intermediate results during the initial inversion and the &lt;em&gt;Pivotal Tuning&lt;/em&gt;(PT) procedure.&lt;/p&gt; &#xA;&lt;p&gt;The log frequency can be adjusted using the parameters defined at &lt;code&gt;configs/global_config.py&lt;/code&gt; under the &#34;Logs&#34; subsection.&lt;/p&gt; &#xA;&lt;p&gt;There is no no need to have an account. However, in order to use the features provided by Weights and Biases you first have to register on their site.&lt;/p&gt; &#xA;&lt;h3&gt;Running PTI&lt;/h3&gt; &#xA;&lt;p&gt;The main training script is &lt;code&gt;scripts/run_pti.py&lt;/code&gt;. The script receives aligned and cropped images from paths configured in the &#34;Input info&#34; subscetion in &lt;code&gt;configs/paths_config.py&lt;/code&gt;. Results are saved to directories found at &#34;Dirs for output files&#34; under &lt;code&gt;configs/paths_config.py&lt;/code&gt;. This includes inversion latent codes and tuned generators. The hyperparametrs for the inversion task can be found at &lt;code&gt;configs/hyperparameters.py&lt;/code&gt;. They are intilized to the default values used in the paper.&lt;/p&gt; &#xA;&lt;h2&gt;Editing&lt;/h2&gt; &#xA;&lt;p&gt;By default, we assume that all auxiliary edit directions are downloaded and saved to the directory &lt;code&gt;editings&lt;/code&gt;. However, you may use your own paths by changing the necessary values in &lt;code&gt;configs/path_configs.py&lt;/code&gt; under &#34;Edit directions&#34; subsection.&lt;/p&gt; &#xA;&lt;p&gt;Example of editing code can be found at &lt;code&gt;scripts/latent_editor_wrapper.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inference Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;To help visualize the results of PTI we provide a Jupyter notebook found in &lt;code&gt;notebooks/inference_playground.ipynb&lt;/code&gt;.&lt;br&gt; The notebook will download the pretrained models and run inference on a sample image found online or on images of your choosing. It is recommended to run this in &lt;a href=&#34;https://colab.research.google.com/github/danielroich/PTI/blob/main/notebooks/inference_playground.ipynb&#34;&gt;Google Colab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The notebook demonstrates how to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Invert an image using PTI&lt;/li&gt; &#xA; &lt;li&gt;Visualise the inversion and use the PTI output&lt;/li&gt; &#xA; &lt;li&gt;Edit the image after PTI using InterfaceGAN and StyleCLIP&lt;/li&gt; &#xA; &lt;li&gt;Compare to other inversion methods&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Currently the repository supports qualitative evaluation for reconstruction of: PTI, SG2 (&lt;em&gt;W Space&lt;/em&gt;), e4e, SG2Plus (&lt;em&gt;W+ Space&lt;/em&gt;). As well as editing using InterfaceGAN and GANSpace for the same inversion methods. To run the evaluation please see &lt;code&gt;evaluation/qualitative_edit_comparison.py&lt;/code&gt;. Examples of the evaluation scripts are:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielroich/PTI/main/docs/model_rec.jpg&#34;&gt; &lt;br&gt; Reconsturction comparison between different methods. The images order is: Original image, W+ inversion, e4e inversion, W inversion, PTI inversion &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielroich/PTI/main/docs/stern_rotation.jpg&#34;&gt; &lt;br&gt; InterfaceGAN pose edit comparison between different methods. The images order is: Original, W+, e4e, W, PTI &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielroich/PTI/main/docs/tyron_original.jpg&#34; width=&#34;220&#34; height=&#34;220&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/danielroich/PTI/main/docs/tyron_edit.jpg&#34; width=&#34;220&#34; height=&#34;220&#34;&gt; &lt;br&gt; Image per edit or several edits without comparison &lt;br&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Coming Soon - Quantitative evaluation and StyleCLIP qualitative evaluation&lt;/h3&gt; &#xA;&lt;h2&gt;Repository structure&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Path&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description &lt;img width=&#34;200&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;├&amp;nbsp; configs&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containing configs defining Hyperparameters, paths and logging&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;├&amp;nbsp; criteria&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containing various loss and regularization criterias for the optimization&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;├&amp;nbsp; dnnlib&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containing internal utils for StyleGAN2-ada&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;├&amp;nbsp; docs&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containing the latent space edit directions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;├&amp;nbsp; editings&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containing images displayed in the README&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;├&amp;nbsp; environment&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containing Anaconda environment used in our experiments&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;├&amp;nbsp; licenses&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containing licenses of the open source projects used in this repository&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;├&amp;nbsp; models&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containing models used in different editing techniques and first phase inversion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;├&amp;nbsp; notebooks&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder with jupyter notebooks to demonstrate the usage of PTI end-to-end&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;├&amp;nbsp; scripts&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder with running scripts for inversion, editing and metric computations&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;├&amp;nbsp; torch_utils&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containing internal utils for StyleGAN2-ada&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;├&amp;nbsp; training&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder containing the core training logic of PTI&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;├&amp;nbsp; utils&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Folder with various utility functions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN2-ada model and implementation:&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/NVlabs/stylegan2-ada-pytorch&#34;&gt;https://github.com/NVlabs/stylegan2-ada-pytorch&lt;/a&gt; Copyright © 2021, NVIDIA Corporation.&lt;br&gt; Nvidia Source Code License &lt;a href=&#34;https://nvlabs.github.io/stylegan2-ada-pytorch/license.html&#34;&gt;https://nvlabs.github.io/stylegan2-ada-pytorch/license.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LPIPS model and implementation:&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/richzhang/PerceptualSimilarity&#34;&gt;https://github.com/richzhang/PerceptualSimilarity&lt;/a&gt;&lt;br&gt; Copyright (c) 2020, Sou Uchida&lt;br&gt; License (BSD 2-Clause) &lt;a href=&#34;https://github.com/richzhang/PerceptualSimilarity/raw/master/LICENSE&#34;&gt;https://github.com/richzhang/PerceptualSimilarity/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;e4e model and implementation:&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/omertov/encoder4editing&#34;&gt;https://github.com/omertov/encoder4editing&lt;/a&gt; Copyright (c) 2021 omertov&lt;br&gt; License (MIT) &lt;a href=&#34;https://github.com/omertov/encoder4editing/raw/main/LICENSE&#34;&gt;https://github.com/omertov/encoder4editing/blob/main/LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleCLIP model and implementation:&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/orpatashnik/StyleCLIP&#34;&gt;https://github.com/orpatashnik/StyleCLIP&lt;/a&gt; Copyright (c) 2021 orpatashnik&lt;br&gt; License (MIT) &lt;a href=&#34;https://github.com/orpatashnik/StyleCLIP/raw/main/LICENSE&#34;&gt;https://github.com/orpatashnik/StyleCLIP/blob/main/LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InterfaceGAN implementation:&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/genforce/interfacegan&#34;&gt;https://github.com/genforce/interfacegan&lt;/a&gt; Copyright (c) 2020 genforce&lt;br&gt; License (MIT) &lt;a href=&#34;https://github.com/genforce/interfacegan/raw/master/LICENSE&#34;&gt;https://github.com/genforce/interfacegan/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GANSpace implementation:&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/harskish/ganspace&#34;&gt;https://github.com/harskish/ganspace&lt;/a&gt; Copyright (c) 2020 harkish&lt;br&gt; License (Apache License 2.0) &lt;a href=&#34;https://github.com/harskish/ganspace/raw/master/LICENSE&#34;&gt;https://github.com/harskish/ganspace/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;This repository structure is based on &lt;a href=&#34;https://github.com/omertov/encoder4editing&#34;&gt;encoder4editing&lt;/a&gt; and &lt;a href=&#34;https://github.com/yuval-alaluf/restyle-encoder&#34;&gt;ReStyle&lt;/a&gt; repositories&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;For any inquiry please contact us at our email addresses: &lt;a href=&#34;mailto:danielroich@gmail.com&#34;&gt;danielroich@gmail.com&lt;/a&gt; or &lt;a href=&#34;mailto:ron.mokady@gmail.com&#34;&gt;ron.mokady@gmail.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code for your research, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{roich2021pivotal,&#xA;  title={Pivotal Tuning for Latent-based Editing of Real Images},&#xA;  author={Roich, Daniel and Mokady, Ron and Bermano, Amit H and Cohen-Or, Daniel},&#xA;  publisher = {Association for Computing Machinery},&#xA;  journal={ACM Trans. Graph.},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/OrienterNet</title>
    <updated>2023-06-28T01:41:59Z</updated>
    <id>tag:github.com,2023-06-28:/facebookresearch/OrienterNet</id>
    <link href="https://github.com/facebookresearch/OrienterNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Source Code for Paper &#34;OrienterNet Visual Localization in 2D Public Maps with Neural Matching&#34;&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;&lt;ins&gt;OrienterNet&lt;/ins&gt;&lt;br&gt;Visual Localization in 2D Public Maps&lt;br&gt;with Neural Matching&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://psarlin.com/&#34;&gt;Paul-Edouard&amp;nbsp;Sarlin&lt;/a&gt; · &lt;a href=&#34;https://danieldetone.com/&#34;&gt;Daniel&amp;nbsp;DeTone&lt;/a&gt; · &lt;a href=&#34;https://scholar.google.com/citations?user=WhISCE4AAAAJ&amp;amp;hl=en&#34;&gt;Tsun-Yi&amp;nbsp;Yang&lt;/a&gt; · &lt;a href=&#34;https://scholar.google.com/citations?user=Ta4TDJoAAAAJ&amp;amp;hl=en&#34;&gt;Armen&amp;nbsp;Avetisyan&lt;/a&gt; · &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=49_cCT8AAAAJ&#34;&gt;Julian&amp;nbsp;Straub&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://tom.ai/&#34;&gt;Tomasz&amp;nbsp;Malisiewicz&lt;/a&gt; · &lt;a href=&#34;https://scholar.google.com/citations?user=484sccEAAAAJ&amp;amp;hl=en&#34;&gt;Samuel&amp;nbsp;Rota&amp;nbsp;Bulo&lt;/a&gt; · &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=MhowvPkAAAAJ&#34;&gt;Richard&amp;nbsp;Newcombe&lt;/a&gt; · &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=CxbDDRMAAAAJ&#34;&gt;Peter&amp;nbsp;Kontschieder&lt;/a&gt; · &lt;a href=&#34;https://scholar.google.com/citations?user=AGoNHcsAAAAJ&amp;amp;hl=en&#34;&gt;Vasileios&amp;nbsp;Balntas&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt;CVPR 2023&lt;/h2&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1zH_2mzdB18BnJVq48ZvJhMorcRjrWAXI?usp=sharing&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/pdf/2304.02009.pdf&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://psarlin.com/orienternet&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/wglW8jnupSs&#34;&gt;Video&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://psarlin.com/orienternet&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/OrienterNet/main/assets/teaser.svg?sanitize=true&#34; alt=&#34;teaser&#34; width=&#34;60%&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;em&gt;OrienterNet is a deep neural network that can accurately localize an image&lt;br&gt;using the same 2D semantic maps that humans use to orient themselves.&lt;/em&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This repository hosts the source code for OrienterNet, a research project by Meta Reality Labs. OrienterNet leverages the power of deep learning to provide accurate positioning of images using free and globally-available maps from OpenStreetMap. As opposed to complex existing algorithms that rely on 3D point clouds, OrienterNet estimates a position and orientation by matching a neural Bird&#39;s-Eye-View with 2D maps.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;OrienterNet requires Python &amp;gt;= 3.8 and &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;. To run the demo, clone this repo and install the minimal requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/facebookresearch/OrienterNet&#xA;python -m pip install -r requirements/demo.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the evaluation and training, install the full requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install -r requirements/full.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demo ➡️ &lt;a href=&#34;https://colab.research.google.com/drive/1zH_2mzdB18BnJVq48ZvJhMorcRjrWAXI?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Check out the Jupyter notebook &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/OrienterNet/main/demo.ipynb&#34;&gt;&lt;code&gt;demo.ipynb&lt;/code&gt;&lt;/a&gt; (&lt;a href=&#34;https://colab.research.google.com/drive/1zH_2mzdB18BnJVq48ZvJhMorcRjrWAXI?usp=sharing&#34;&gt;run it on Colab!&lt;/a&gt;) for a minimal demo - take a picture with your phone in any city and find its exact location in a few seconds!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/OrienterNet/main/demo.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/OrienterNet/main/assets/demo.jpg&#34; alt=&#34;demo&#34; width=&#34;60%&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;em&gt;OrienterNet positions any image within a large area - try it with your own images!&lt;/em&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;h4&gt;Mapillary Geo-Localization dataset&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Click to expand]&lt;/summary&gt; &#xA; &lt;p&gt;To obtain the dataset:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Create a developper account at &lt;a href=&#34;https://www.mapillary.com/dashboard/developers&#34;&gt;mapillary.com&lt;/a&gt; and obtain a free access token.&lt;/li&gt; &#xA;  &lt;li&gt;Run the following script to download the data from Mapillary and prepare it:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m maploc.data.mapillary.prepare --token $YOUR_ACCESS_TOKEN&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;By default the data is written to the directory &lt;code&gt;./datasets/MGL/&lt;/code&gt;. Then run the evaluation with the pre-trained model:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m maploc.evaluation.mapillary --experiment OrienterNet_MGL model.num_rotations=256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;This downloads the pre-trained models if necessary. The results should be close to the following:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;Recall xy_max_error: [14.37, 48.69, 61.7] at (1, 3, 5) m/°&#xA;Recall yaw_max_error: [20.95, 54.96, 70.17] at (1, 3, 5) m/°&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;This requires a GPU with 11GB of memory. If you run into OOM issues, consider reducing the number of rotations (the default is 256):&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m maploc.evaluation.mapillary --experiment OrienterNet_MGL \&#xA;    model.num_rotations=128&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To export visualizations for the first 100 examples:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m maploc.evaluation.mapillary --experiment OrienterNet_MGL \&#xA;    --output_dir ./viz_MGL/ --num 100 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To run the evaluation in sequential mode (by default with 10 frames):&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m maploc.evaluation.mapillary --experiment OrienterNet_MGL --sequential&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;KITTI dataset&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Click to expand]&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Download and prepare the dataset to &lt;code&gt;./datasets/kitti/&lt;/code&gt;:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m maploc.data.kitti.prepare&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Run the evaluation with the model trained on MGL:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m maploc.evaluation.kitti --experiment OrienterNet_MGL&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You should expect the following results:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;Recall directional_error: [[50.33, 85.18, 92.73], [24.38, 56.13, 67.98]] at (1, 3, 5) m/°&#xA;Recall yaw_max_error: [29.22, 68.2, 84.49] at (1, 3, 5) m/°&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You can similarly export some visual examples:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m maploc.evaluation.kitti --experiment OrienterNet_MGL \&#xA;    --output_dir ./viz_KITTI/ --num 100 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Aria Detroit &amp;amp; Seattle&lt;/h4&gt; &#xA;&lt;p&gt;We are currently unable to release the dataset used to evaluate OrienterNet in the CVPR 2023 paper.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h4&gt;MGL dataset&lt;/h4&gt; &#xA;&lt;p&gt;We trained the model on the MGL dataset using 3x 3090 GPUs (24GB VRAM each) and a total batch size of 12 for 340k iterations (about 3-4 days) with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m maploc.train experiment.name=OrienterNet_MGL_reproduce&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Feel free to use any other experiment name. Configurations are managed by &lt;a href=&#34;https://hydra.cc/&#34;&gt;Hydra&lt;/a&gt; and &lt;a href=&#34;https://omegaconf.readthedocs.io&#34;&gt;OmegaConf&lt;/a&gt; so any entry can be overridden from the command line. You may thus reduce the number of GPUs and the batch size via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m maploc.train experiment.name=OrienterNet_MGL_reproduce&#xA;  experiment.gpus=1 data.loading.train.batch_size=4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Be aware that this can reduce the overall performance. The checkpoints are written to &lt;code&gt;./experiments/experiment_name/&lt;/code&gt;. Then run the evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# the best checkpoint:&#xA;python -m maploc.evaluation.mapillary --experiment OrienterNet_MGL_reproduce&#xA;# a specific checkpoint:&#xA;python -m maploc.evaluation.mapillary \&#xA;    --experiment OrienterNet_MGL_reproduce/checkpoint-step=340000.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;KITTI&lt;/h4&gt; &#xA;&lt;p&gt;To fine-tune a trained model on the KITTI dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m maploc.train experiment.name=OrienterNet_MGL_kitti data=kitti \&#xA;    training.finetune_from_checkpoint=&#39;&#34;experiments/OrienterNet_MGL_reproduce/checkpoint-step=340000.ckpt&#34;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interactive development&lt;/h2&gt; &#xA;&lt;p&gt;We provide several visualization notebooks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/OrienterNet/main/notebooks/visualize_predictions_mgl.ipynb&#34;&gt;Visualize predictions on the MGL dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/OrienterNet/main/notebooks/visualize_predictions_kitti.ipynb&#34;&gt;Visualize predictions on the KITTI dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/OrienterNet/main/notebooks/visualize_predictions_sequences.ipynb&#34;&gt;Visualize sequential predictions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;OpenStreetMap data&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Click to expand]&lt;/summary&gt; &#xA; &lt;p&gt;To make sure that the results are consistent over time, we used OSM data downloaded from &lt;a href=&#34;https://download.geofabrik.de/&#34;&gt;Geofabrik&lt;/a&gt; in November 2021. By default, the dataset scripts &lt;code&gt;maploc.data.[mapillary,kitti].prepare&lt;/code&gt; download pre-generated raster tiles. If you wish to use different OSM classes, you can pass &lt;code&gt;--generate_tiles&lt;/code&gt;, which will download and use our prepared raw &lt;code&gt;.osm&lt;/code&gt; XML files. You may alternatively download more recent files.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The MGL dataset is made available under the &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;CC-BY-SA&lt;/a&gt; license following the data available on the Mapillary platform. The model implementation and the pre-trained weights follow a &lt;a href=&#34;https://creativecommons.org/licenses/by-nc/2.0/&#34;&gt;CC-BY-NC&lt;/a&gt; license. Keep in mind that OpenStreetMap &lt;a href=&#34;https://www.openstreetmap.org/copyright&#34;&gt;follows a different license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;BibTex citation&lt;/h2&gt; &#xA;&lt;p&gt;Please consider citing our work if you use any code from this repo or ideas presented in the paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{sarlin2023orienternet,&#xA;  author    = {Paul-Edouard Sarlin and&#xA;               Daniel DeTone and&#xA;               Tsun-Yi Yang and&#xA;               Armen Avetisyan and&#xA;               Julian Straub and&#xA;               Tomasz Malisiewicz and&#xA;               Samuel Rota Bulo and&#xA;               Richard Newcombe and&#xA;               Peter Kontschieder and&#xA;               Vasileios Balntas},&#xA;  title     = {{OrienterNet: Visual Localization in 2D Public Maps with Neural Matching}},&#xA;  booktitle = {CVPR},&#xA;  year      = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>