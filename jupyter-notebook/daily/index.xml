<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-06T01:39:38Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>maziarraissi/PetGPT</title>
    <updated>2023-04-06T01:39:38Z</updated>
    <id>tag:github.com,2023-04-06:/maziarraissi/PetGPT</id>
    <link href="https://github.com/maziarraissi/PetGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Train your own PetGPT at home!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PetGPT&lt;/h1&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt; and &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;Alpaca-LoRA&lt;/a&gt;, you can train your own PetGPT at home. This GitHub repository offers a step by step guide in its README.md file.&lt;/p&gt; &#xA;&lt;h2&gt;LLaMA (Large Language Model Meta AI)&lt;/h2&gt; &#xA;&lt;p&gt;This repository assumes that you have already found a way to download the checkpoints and tokenizer for &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt; (e.g., by filling out this &lt;a href=&#34;https://forms.gle/jk851eBVbX1m5TAv5&#34;&gt;google form&lt;/a&gt;). You should create a subdirectory (named &lt;code&gt;LLaMA&lt;/code&gt;) within this repository having a structure similar to the following tree.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;LLaMA&#xA;├── 13B&#xA;│   ├── checklist.chk&#xA;│   ├── consolidated.00.pth&#xA;│   ├── consolidated.01.pth&#xA;│   └── params.json&#xA;├── 30B&#xA;│   ├── checklist.chk&#xA;│   ├── consolidated.00.pth&#xA;│   ├── consolidated.01.pth&#xA;│   ├── consolidated.02.pth&#xA;│   ├── consolidated.03.pth&#xA;│   └── params.json&#xA;├── 65B&#xA;│   ├── checklist.chk&#xA;│   ├── consolidated.00.pth&#xA;│   ├── consolidated.01.pth&#xA;│   ├── consolidated.02.pth&#xA;│   ├── consolidated.03.pth&#xA;│   ├── consolidated.04.pth&#xA;│   ├── consolidated.05.pth&#xA;│   ├── consolidated.06.pth&#xA;│   ├── consolidated.07.pth&#xA;│   └── params.json&#xA;├── 7B&#xA;│   ├── checklist.chk&#xA;│   ├── consolidated.00.pth&#xA;│   └── params.json&#xA;├── llama.sh&#xA;├── tokenizer_checklist.chk&#xA;└── tokenizer.model&#xA;&#xA;4 directories, 26 files&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Clone the &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt; repository to make sure that everything works as expected.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/facebookresearch/llama.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The LLaMA repository is already included here for reproducibility purposes in the folder named &lt;code&gt;llama&lt;/code&gt;. You can now run the following commands.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd llama&#xA;&#xA;torchrun --nproc_per_node 1 example.py --ckpt_dir ../LLaMA/7B --tokenizer_path ../LLaMA/tokenizer.model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Converting LLaMA to Hugging Face&lt;/h2&gt; &#xA;&lt;p&gt;Create an empty directory within this repository called &lt;code&gt;LLaMA_HF&lt;/code&gt;. The following two scripts will then help you convert the LLaMA checkpoints and tokenizer to the Hugging Face format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;convert_llama_tokenizer_to_hf.ipynb&#xA;convert_llama_weights_to_hf.ipynb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These two scripts are simplied versions of &lt;a href=&#34;https://github.com/huggingface/transformers/raw/main/src/transformers/models/llama/convert_llama_weights_to_hf.py&#34;&gt;convert_llama_weights_to_hf.py&lt;/a&gt; for pedagogical purposes.&lt;/p&gt; &#xA;&lt;p&gt;This should result in a subdirectory (named &lt;code&gt;LLaMA_HF&lt;/code&gt;) within this repository having a structure similar to the following tree.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;LaMA_HF&#xA;├── config.json&#xA;├── generation_config.json&#xA;├── pytorch_model-00001-of-00002.bin&#xA;├── pytorch_model-00002-of-00002.bin&#xA;├── pytorch_model.bin.index.json&#xA;├── special_tokens_map.json&#xA;├── tokenizer_config.json&#xA;└── tokenizer.model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Exploratory Data Analysis&lt;/h2&gt; &#xA;&lt;p&gt;Clone the &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;Alpaca-LoRA&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/tloen/alpaca-lora.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The Alpaca-LoRA repository is already included here for reproducibility purposes in the folder named &lt;code&gt;alpaca-lora&lt;/code&gt;. Within this folder, there is a file called &lt;code&gt;alpaca_data_cleaned.json&lt;/code&gt;. This file contains a cleaned and curated version of the dataset used to train the original &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Alpaca&lt;/a&gt;. The following script will help you explore this dataset and build some intuition.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;exploratory_data_analysis.ipynb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fine Tuning&lt;/h2&gt; &#xA;&lt;p&gt;You can now use the following commands to finetune the LLaMA model on the &lt;code&gt;alpaca_data_cleaned.json&lt;/code&gt; dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd alpaca-lora&#xA;&#xA;mkdir output&#xA;&#xA;WORLD_SIZE=4 CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 --master_port=1234 finetune.py --base_model ../LLaMA_HF --data_path alpaca_data_cleaned.json --output_dir output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The command given above works for a machine with 4 GPUs. You should adjust it according to your own setup.&lt;/p&gt; &#xA;&lt;p&gt;Once the finetuning process is finished, it will write the model artifacts in a folder called &lt;code&gt;output&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;alpaca-lora/output/&#xA;├── adapter_config.json&#xA;├── adapter_model.bin&#xA;├── checkpoint-1000&#xA;│   ├── optimizer.pt&#xA;│   ├── pytorch_model.bin&#xA;│   ├── rng_state_0.pth&#xA;│   ├── rng_state_1.pth&#xA;│   ├── rng_state_2.pth&#xA;│   ├── rng_state_3.pth&#xA;│   ├── scaler.pt&#xA;│   ├── scheduler.pt&#xA;│   ├── trainer_state.json&#xA;│   └── training_args.bin&#xA;├── checkpoint-600&#xA;│   ├── optimizer.pt&#xA;│   ├── pytorch_model.bin&#xA;│   ├── rng_state_0.pth&#xA;│   ├── rng_state_1.pth&#xA;│   ├── rng_state_2.pth&#xA;│   ├── rng_state_3.pth&#xA;│   ├── scaler.pt&#xA;│   ├── scheduler.pt&#xA;│   ├── trainer_state.json&#xA;│   └── training_args.bin&#xA;└── checkpoint-800&#xA;    ├── optimizer.pt&#xA;    ├── pytorch_model.bin&#xA;    ├── rng_state_0.pth&#xA;    ├── rng_state_1.pth&#xA;    ├── rng_state_2.pth&#xA;    ├── rng_state_3.pth&#xA;    ├── scaler.pt&#xA;    ├── scheduler.pt&#xA;    ├── trainer_state.json&#xA;    └── training_args.bin&#xA;&#xA;3 directories, 32 files&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generating Text&lt;/h2&gt; &#xA;&lt;p&gt;Here is how you can interact with the finetuned model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --load_8bit --base_model ../LLaMA_HF --lora_weights output --share_gradio False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The final screen hosted on &lt;a href=&#34;http://0.0.0.0:7860&#34;&gt;http://0.0.0.0:7860&lt;/a&gt; will look like the following image.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/maziarraissi/PetGPT/main/petgpt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hemansnation/God-Level-Data-Science-ML-Full-Stack</title>
    <updated>2023-04-06T01:39:38Z</updated>
    <id>tag:github.com,2023-04-06:/hemansnation/God-Level-Data-Science-ML-Full-Stack</id>
    <link href="https://github.com/hemansnation/God-Level-Data-Science-ML-Full-Stack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This roadmap contains 16 Chapters that can be completed in 8 months, whether you are a fresher in the field or an experienced professional who wants to transition into Data Science.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;God-Level Data Science ML Full Stack&lt;/h1&gt; &#xA;&lt;img src=&#34;https://github.com/hemansnation/Data-Science-ML-Full-Stack/raw/master/images/components.png&#34;&gt; &#xA;&lt;h3&gt;The‌ ‌Roadmap‌ ‌is‌ ‌divided‌ ‌into‌ ‌16 ‌Sections‌&lt;/h3&gt; &#xA;&lt;p&gt;Duration:‌ ‌256‌ ‌Hours‌ of Learning ‌(8 ‌Months)‌ ‌and many more hours for practice and project building. ‌&lt;/p&gt; &#xA;&lt;h2&gt;Month 1 - May&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#1--python-programming-and-logic-building&#34;&gt;Python‌ ‌Programming‌ ‌and‌ ‌Logic‌ ‌Building‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#2--data-structure--algorithms&#34;&gt;Data‌ ‌Structure‌ ‌&amp;amp;‌ ‌Algorithms‌&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Month 2 - June&lt;/h2&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#3--pandas-numpy-matplotlib&#34;&gt;Pandas‌ ‌Numpy‌ ‌Matplotlib‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#4--statistics&#34;&gt;Statistics‌&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Month 3 - July&lt;/h2&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#5--machine-learning&#34;&gt;Machine‌ ‌Learning‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#6--mlops&#34;&gt;ML Operations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Month 4 - August&lt;/h2&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#7--natural-language-processing&#34;&gt;Natural‌ ‌Language‌ ‌Processing‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#8--computer-vision&#34;&gt;Computer‌ ‌Vision‌‌&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Month 5 - September&lt;/h2&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#9--data-visualization-with-tableau&#34;&gt;Data‌ ‌Visualization‌ ‌with‌ ‌Tableau‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#10--structure-query-language-sql&#34;&gt;Structure‌d ‌Query‌ ‌Language‌ ‌(SQL)‌&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Month 6 - October&lt;/h2&gt; &#xA;&lt;ol start=&#34;11&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#11--data-engineering&#34;&gt;Data Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#12--data-system-design&#34;&gt;Data System Design&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Month 7 - November&lt;/h2&gt; &#xA;&lt;ol start=&#34;13&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#13--five-major-projects-and-git&#34;&gt;Five‌ ‌Major‌ Capstone ‌Projects‌&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#14--interview-preperation&#34;&gt;Interview Preparations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Month 8 - December&lt;/h2&gt; &#xA;&lt;ol start=&#34;15&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#15--git--github&#34;&gt;Git &amp;amp; GitHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#16--personal-profile--portfolio&#34;&gt;Personal Branding and portfolio&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/master/#resources&#34;&gt;Resources&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;Dataset Collection&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Technology‌ ‌Stack‌&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python‌&lt;/li&gt; &#xA; &lt;li&gt;Data‌ ‌Structures‌&lt;/li&gt; &#xA; &lt;li&gt;NumPy‌&lt;/li&gt; &#xA; &lt;li&gt;Pandas‌&lt;/li&gt; &#xA; &lt;li&gt;Matplotlib‌&lt;/li&gt; &#xA; &lt;li&gt;Seaborn‌&lt;/li&gt; &#xA; &lt;li&gt;Scikit-Learn‌&lt;/li&gt; &#xA; &lt;li&gt;Statsmodels‌&lt;/li&gt; &#xA; &lt;li&gt;Natural‌ ‌Language‌ ‌Toolkit‌ ‌(‌ ‌NLTK‌ ‌)‌&lt;/li&gt; &#xA; &lt;li&gt;PyTorch‌&lt;/li&gt; &#xA; &lt;li&gt;OpenCV‌&lt;/li&gt; &#xA; &lt;li&gt;Tableau‌&lt;/li&gt; &#xA; &lt;li&gt;Structure‌ ‌Query‌ ‌Language‌ ‌(‌ ‌SQL‌ ‌)‌&lt;/li&gt; &#xA; &lt;li&gt;PySpark‌&lt;/li&gt; &#xA; &lt;li&gt;Azure‌ ‌Fundamentals‌&lt;/li&gt; &#xA; &lt;li&gt;Azure‌ ‌Data‌ ‌Factory‌&lt;/li&gt; &#xA; &lt;li&gt;Databricks‌&lt;/li&gt; &#xA; &lt;li&gt;5‌ ‌Major‌ ‌Projects‌&lt;/li&gt; &#xA; &lt;li&gt;Git‌ ‌and‌ ‌GitHub‌ ‌&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;1 | Python Programming and Logic Building&lt;/h1&gt; &#xA;&lt;p&gt;I will prefer Python Programming Language. Python is the best for starting your programming journey. Here is the roadmap of python for logic building.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python basics, Variables, Operators, Conditional Statements&lt;/li&gt; &#xA; &lt;li&gt;List and Strings&lt;/li&gt; &#xA; &lt;li&gt;While Loop, Nested Loops, Loop Else&lt;/li&gt; &#xA; &lt;li&gt;For Loop, Break, and Continue statements&lt;/li&gt; &#xA; &lt;li&gt;Functions, Return Statement, Recursion&lt;/li&gt; &#xA; &lt;li&gt;Dictionary, Tuple, Set&lt;/li&gt; &#xA; &lt;li&gt;File Handling, Exception Handling&lt;/li&gt; &#xA; &lt;li&gt;Object-Oriented Programming&lt;/li&gt; &#xA; &lt;li&gt;Modules and Packages&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hemansnation/Python-Roadmap-2022&#34;&gt;In-Depth Roadmap of Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;2 | Data Structure &amp;amp; Algorithms&lt;/h1&gt; &#xA;&lt;p&gt;Data Structure is the most important thing to learn not only for data scientists but for all the people working in computer science. With data structure, you get an internal understanding of the working of everything in software.&lt;/p&gt; &#xA;&lt;p&gt;Understand these topics&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Types of Algorithm Analysis&lt;/li&gt; &#xA; &lt;li&gt;Asymptotic Notation, Big-O, Omega, Theta&lt;/li&gt; &#xA; &lt;li&gt;Stacks&lt;/li&gt; &#xA; &lt;li&gt;Queues&lt;/li&gt; &#xA; &lt;li&gt;Linked List&lt;/li&gt; &#xA; &lt;li&gt;Trees&lt;/li&gt; &#xA; &lt;li&gt;Graphs&lt;/li&gt; &#xA; &lt;li&gt;Sorting&lt;/li&gt; &#xA; &lt;li&gt;Searching&lt;/li&gt; &#xA; &lt;li&gt;Hashing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;3 | Pandas Numpy Matplotlib&lt;/h1&gt; &#xA;&lt;p&gt;Python supports n-dimensional arrays with Numpy. For data in 2-dimensions, Pandas is the best library for analysis. You can use other tools but tools have drag-and-drop features and have limitations. Pandas can be customized as per the need as we can code depending upon the real-life problem.&lt;/p&gt; &#xA;&lt;h3&gt;Numpy&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vectors, Matrix&lt;/li&gt; &#xA; &lt;li&gt;Operations on Matrix&lt;/li&gt; &#xA; &lt;li&gt;Mean, Variance, and Standard Deviation&lt;/li&gt; &#xA; &lt;li&gt;Reshaping Arrays&lt;/li&gt; &#xA; &lt;li&gt;Transpose and Determinant of Matrix&lt;/li&gt; &#xA; &lt;li&gt;Diagonal Operations, Trace&lt;/li&gt; &#xA; &lt;li&gt;Add, Subtract, Multiply, Dot, and Cross Product.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pandas&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Series and DataFrames&lt;/li&gt; &#xA; &lt;li&gt;Slicing, Rows, and Columns&lt;/li&gt; &#xA; &lt;li&gt;Operations on DataFrame&lt;/li&gt; &#xA; &lt;li&gt;Different ways to create DataFrame&lt;/li&gt; &#xA; &lt;li&gt;Read, Write Operations with CSV files&lt;/li&gt; &#xA; &lt;li&gt;Handling Missing values, replace values, and Regular Expression&lt;/li&gt; &#xA; &lt;li&gt;GroupBy and Concatenation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Matplotlib&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Graph Basics&lt;/li&gt; &#xA; &lt;li&gt;Format Strings in Plots&lt;/li&gt; &#xA; &lt;li&gt;Label Parameters, Legend&lt;/li&gt; &#xA; &lt;li&gt;Bar Chart, Pie Chart, Histogram, Scatter Plot&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;4 | Statistics&lt;/h1&gt; &#xA;&lt;h3&gt;Descriptive Statistics&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Measure of Frequency and Central Tendency&lt;/li&gt; &#xA; &lt;li&gt;Measure of Dispersion&lt;/li&gt; &#xA; &lt;li&gt;Probability Distribution&lt;/li&gt; &#xA; &lt;li&gt;Gaussian Normal Distribution&lt;/li&gt; &#xA; &lt;li&gt;Skewness and Kurtosis&lt;/li&gt; &#xA; &lt;li&gt;Regression Analysis&lt;/li&gt; &#xA; &lt;li&gt;Continuous and Discrete Functions&lt;/li&gt; &#xA; &lt;li&gt;Goodness of Fit&lt;/li&gt; &#xA; &lt;li&gt;Normality Test&lt;/li&gt; &#xA; &lt;li&gt;ANOVA&lt;/li&gt; &#xA; &lt;li&gt;Homoscedasticity&lt;/li&gt; &#xA; &lt;li&gt;Linear and Non-Linear Relationship with Regression&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Inferential Statistics&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;t-Test&lt;/li&gt; &#xA; &lt;li&gt;z-Test&lt;/li&gt; &#xA; &lt;li&gt;Hypothesis Testing&lt;/li&gt; &#xA; &lt;li&gt;Type I and Type II errors&lt;/li&gt; &#xA; &lt;li&gt;t-Test and its types&lt;/li&gt; &#xA; &lt;li&gt;One way ANOVA&lt;/li&gt; &#xA; &lt;li&gt;Two way ANOVA&lt;/li&gt; &#xA; &lt;li&gt;Chi-Square Test&lt;/li&gt; &#xA; &lt;li&gt;Implementation of continuous and categorical data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;5 | Machine Learning&lt;/h1&gt; &#xA;&lt;p&gt;The best way to master machine learning algorithms is to work with the Scikit-Learn framework. Scikit-Learn contains predefined algorithms and you can work with them just by generating the object of the class. These are the algorithm you must know including the types of Supervised and Unsupervised Machine Learning:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linear Regression&lt;/li&gt; &#xA; &lt;li&gt;Logistic Regression&lt;/li&gt; &#xA; &lt;li&gt;Decision Tree&lt;/li&gt; &#xA; &lt;li&gt;Gradient Descent&lt;/li&gt; &#xA; &lt;li&gt;Random Forest&lt;/li&gt; &#xA; &lt;li&gt;Ridge and Lasso Regression&lt;/li&gt; &#xA; &lt;li&gt;Naive Bayes&lt;/li&gt; &#xA; &lt;li&gt;Support Vector Machine&lt;/li&gt; &#xA; &lt;li&gt;KMeans Clustering&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Other Concepts and Topics for ML&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Measuring Accuracy&lt;/li&gt; &#xA; &lt;li&gt;Bias-Variance Trade-off&lt;/li&gt; &#xA; &lt;li&gt;Applying Regularization&lt;/li&gt; &#xA; &lt;li&gt;Elastic Net Regression&lt;/li&gt; &#xA; &lt;li&gt;Predictive Analytics&lt;/li&gt; &#xA; &lt;li&gt;Exploratory Data Analysis&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;6 | MLOps&lt;/h1&gt; &#xA;&lt;p&gt;You can master any one of the cloud services provider from AWS, GCP and Azure. You can switch easily once you understand one of them.&lt;/p&gt; &#xA;&lt;p&gt;We will focus on AWS - Amazon Web Services first&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deploy ML models using Flask&lt;/li&gt; &#xA; &lt;li&gt;Amazon Lex - Natural Language Understanding&lt;/li&gt; &#xA; &lt;li&gt;AWS Polly - Voice Analysis&lt;/li&gt; &#xA; &lt;li&gt;Amazon Transcribe - Speech to Text&lt;/li&gt; &#xA; &lt;li&gt;Amazon Textract - Extract Text&lt;/li&gt; &#xA; &lt;li&gt;Amazon Rekognition - Image Applications&lt;/li&gt; &#xA; &lt;li&gt;Amazon SageMaker - Building and deploying models&lt;/li&gt; &#xA; &lt;li&gt;Working with Deep Learning on AWS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;7 | Natural Language Processing&lt;/h1&gt; &#xA;&lt;p&gt;If you are interested in working with Text, you should do some of the work an NLP Engineer do and understand the working of Language models.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sentiment analysis&lt;/li&gt; &#xA; &lt;li&gt;POS Tagging, Parsing,&lt;/li&gt; &#xA; &lt;li&gt;Text preprocessing&lt;/li&gt; &#xA; &lt;li&gt;Stemming and Lemmatization&lt;/li&gt; &#xA; &lt;li&gt;Sentiment classification using Naive Bayes&lt;/li&gt; &#xA; &lt;li&gt;TF-IDF, N-gram,&lt;/li&gt; &#xA; &lt;li&gt;Machine Translation, BLEU Score&lt;/li&gt; &#xA; &lt;li&gt;Text Generation, Summarization, ROUGE Score&lt;/li&gt; &#xA; &lt;li&gt;Language Modeling, Perplexity&lt;/li&gt; &#xA; &lt;li&gt;Building a text classifier&lt;/li&gt; &#xA; &lt;li&gt;Identifying the gender&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;8 | Computer Vision&lt;/h1&gt; &#xA;&lt;p&gt;To work on image and video analytics we can master computer vision. To work on computer vision we have to understand images.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch Tensors&lt;/li&gt; &#xA; &lt;li&gt;Understanding Pretrained models like AlexNet, ImageNet, ResNet.&lt;/li&gt; &#xA; &lt;li&gt;Neural Networks&lt;/li&gt; &#xA; &lt;li&gt;Building a perceptron&lt;/li&gt; &#xA; &lt;li&gt;Building a single layer neural network&lt;/li&gt; &#xA; &lt;li&gt;Building a deep neural network&lt;/li&gt; &#xA; &lt;li&gt;Recurrent neural network for sequential data analysis&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Convolutional Neural Networks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understanding the ConvNet topology&lt;/li&gt; &#xA; &lt;li&gt;Convolution layers&lt;/li&gt; &#xA; &lt;li&gt;Pooling layers&lt;/li&gt; &#xA; &lt;li&gt;Image Content Analysis&lt;/li&gt; &#xA; &lt;li&gt;Operating on images using OpenCV-Python&lt;/li&gt; &#xA; &lt;li&gt;Detecting edges&lt;/li&gt; &#xA; &lt;li&gt;Histogram equalization&lt;/li&gt; &#xA; &lt;li&gt;Detecting corners&lt;/li&gt; &#xA; &lt;li&gt;Detecting SIFT feature points&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;9 | Data Visualization with Tableau&lt;/h1&gt; &#xA;&lt;p&gt;How to use it Visual Perception&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What is it, How it works, Why Tableau&lt;/li&gt; &#xA; &lt;li&gt;Connecting to Data&lt;/li&gt; &#xA; &lt;li&gt;Building charts&lt;/li&gt; &#xA; &lt;li&gt;Calculations&lt;/li&gt; &#xA; &lt;li&gt;Dashboards&lt;/li&gt; &#xA; &lt;li&gt;Sharing our work&lt;/li&gt; &#xA; &lt;li&gt;Advanced Charts, Calculated Fields, Calculated Aggregations&lt;/li&gt; &#xA; &lt;li&gt;Conditional Calculation, Parameterized Calculation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;10 | Structured Query Language (SQL)&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fundamental to SQL syntax and Installation&lt;/li&gt; &#xA; &lt;li&gt;Creating Tables, Modifiers&lt;/li&gt; &#xA; &lt;li&gt;Inserting and Retrieving Data, SELECT INSERT UPDATE DELETE&lt;/li&gt; &#xA; &lt;li&gt;Aggregating Data using Functions, Filtering and RegEX&lt;/li&gt; &#xA; &lt;li&gt;Subqueries, retrieve data based on conditions, grouping of Data.&lt;/li&gt; &#xA; &lt;li&gt;Practice Questions&lt;/li&gt; &#xA; &lt;li&gt;JOINs&lt;/li&gt; &#xA; &lt;li&gt;Advanced SQL concepts such as transactions, views, stored procedures, and functions.&lt;/li&gt; &#xA; &lt;li&gt;Database Design principles, normalization, and ER diagrams.&lt;/li&gt; &#xA; &lt;li&gt;Practice, Practice, Practice: Practice writing SQL queries on real-world datasets, and work on projects to apply your knowledge.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;11 | Data Engineering&lt;/h1&gt; &#xA;&lt;h3&gt;BigData&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What is BigData?&lt;/li&gt; &#xA; &lt;li&gt;How is BigData applied within Business?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;PySpark&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Resilient Distributed Datasets&lt;/li&gt; &#xA; &lt;li&gt;Schema&lt;/li&gt; &#xA; &lt;li&gt;Lambda Expressions&lt;/li&gt; &#xA; &lt;li&gt;Transformations&lt;/li&gt; &#xA; &lt;li&gt;Actions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data Modeling&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Duplicate Data&lt;/li&gt; &#xA; &lt;li&gt;Descriptive Analysis on Data&lt;/li&gt; &#xA; &lt;li&gt;Visualizations&lt;/li&gt; &#xA; &lt;li&gt;ML lib&lt;/li&gt; &#xA; &lt;li&gt;ML Packages&lt;/li&gt; &#xA; &lt;li&gt;Pipelines&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Streaming&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Packaging Spark Applications&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;12 | Data System Design&lt;/h1&gt; &#xA;&lt;h3&gt;What is system design?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;IP and OSI Model&lt;/li&gt; &#xA; &lt;li&gt;Domain Name System (DNS)&lt;/li&gt; &#xA; &lt;li&gt;Load Balancing&lt;/li&gt; &#xA; &lt;li&gt;Clustering&lt;/li&gt; &#xA; &lt;li&gt;Caching&lt;/li&gt; &#xA; &lt;li&gt;Availability, Scalability, Storage&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Databases and DBMS&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SQL databases&lt;/li&gt; &#xA; &lt;li&gt;NoSQL databases&lt;/li&gt; &#xA; &lt;li&gt;SQL vs NoSQL databases&lt;/li&gt; &#xA; &lt;li&gt;Database Replication&lt;/li&gt; &#xA; &lt;li&gt;Indexes&lt;/li&gt; &#xA; &lt;li&gt;Normalization and Denormalization&lt;/li&gt; &#xA; &lt;li&gt;CAP theorem&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;System Design Interview&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;URL Shortener&lt;/li&gt; &#xA; &lt;li&gt;Whatsapp, Twitter, Netflix, Uber&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;13 | Five Major Projects and Git&lt;/h1&gt; &#xA;&lt;p&gt;We follow project-based learning and we will work on all the projects in parallel.&lt;/p&gt; &#xA;&lt;h1&gt;14 | Interview Preperation&lt;/h1&gt; &#xA;&lt;h1&gt;15 | Git &amp;amp; GitHub&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://god-level-python.notion.site/Git-GitHub-Course-Make-Recruiters-reach-You-Build-your-stunning-profile-First-open-source-cont-1d4d70450aa94dd7ad2c062c0fec3cb8&#34;&gt;Git &amp;amp; GitHub Course&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understanding Git&lt;/li&gt; &#xA; &lt;li&gt;Commands and How to commit your first code?&lt;/li&gt; &#xA; &lt;li&gt;How to use GitHub?&lt;/li&gt; &#xA; &lt;li&gt;How to make your first open-source contribution?&lt;/li&gt; &#xA; &lt;li&gt;How to work with a team? - Part 1&lt;/li&gt; &#xA; &lt;li&gt;How to create your stunning GitHub profile?&lt;/li&gt; &#xA; &lt;li&gt;How to build your own viral repository?&lt;/li&gt; &#xA; &lt;li&gt;Building a personal landing page for your Portfolio for FREE&lt;/li&gt; &#xA; &lt;li&gt;How to grow followers on GitHub?&lt;/li&gt; &#xA; &lt;li&gt;How to work with a team? Part 2 - issues, milestone and projects&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;16 | Personal Profile &amp;amp; Portfolio&lt;/h1&gt; &#xA;&lt;h1&gt;Resources&lt;/h1&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;1️⃣ &lt;a href=&#34;https://github.com/awesomedata/awesome-public-datasets&#34;&gt;Awesome Public Datasets&lt;/a&gt; This list of a topic-centric public data sources in high quality.&lt;/p&gt; &#xA;&lt;p&gt;2️⃣&lt;a href=&#34;https://github.com/niderhoff/nlp-datasets&#34;&gt;NLP Datasets&lt;/a&gt; Alphabetical list of free/public domain datasets with text data for use in NLP.&lt;/p&gt; &#xA;&lt;p&gt;3️⃣&lt;a href=&#34;https://github.com/jsbroks/awesome-dataset-tools&#34;&gt;Awesome Dataset Tools&lt;/a&gt; A curated list of awesome dataset tools.&lt;/p&gt; &#xA;&lt;p&gt;4️⃣&lt;a href=&#34;https://github.com/xephonhq/awesome-time-series-database&#34;&gt;Awesome time series database&lt;/a&gt; A curated list of time series databases.&lt;/p&gt; &#xA;&lt;p&gt;5️⃣&lt;a href=&#34;https://github.com/shramos/Awesome-Cybersecurity-Datasets&#34;&gt;Awesome-Cybersecurity-Datasets&lt;/a&gt; A curated list of amazingly awesome Cybersecurity datasets.&lt;/p&gt; &#xA;&lt;p&gt;6️⃣&lt;a href=&#34;https://github.com/mint-lab/awesome-robotics-datasets&#34;&gt;Awesome Robotics Datasets&lt;/a&gt; Robotics Dataset Collections.&lt;/p&gt; &#xA;&lt;h1&gt;Research Starting Point&lt;/h1&gt; &#xA;&lt;h3&gt;Machine Learning&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ime.unicamp.br/~dias/Intoduction%20to%20Statistical%20Learning.pdf&#34;&gt;Introduction to Statistical Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Deep Learning&lt;/h3&gt; &#xA;&lt;h3&gt;Reinforcement Learning&lt;/h3&gt; &#xA;&lt;h1&gt;Projects&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hemansnation/God-Level-Data-Science-ML-Full-Stack/tree/master/projects&#34;&gt;Here is the list of project ideas&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Data Science ML Full Stack Live Batch -&amp;gt; &lt;a href=&#34;https://god-level-python.notion.site/Data-Science-ML-Full-Stack-Roadmap-05e6c16389c845d1a438ed4cff2b9952&#34;&gt;Register Here&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h3&gt;Want to join the Community Group for Live Batch?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chat.whatsapp.com/BSUPbYhzzM1BcJplcTTIxb&#34;&gt;https://chat.whatsapp.com/BSUPbYhzzM1BcJplcTTIxb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Socials&lt;/h1&gt; &#xA;&lt;h3&gt;Join Telegram for Data Science ML AI Resources:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://t.me/+sREuRiFssMo4YWJl&#34;&gt;https://t.me/+sREuRiFssMo4YWJl&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Connect with me on these platforms:&lt;/h3&gt; &#xA;&lt;p&gt;LinkedIn: &lt;a href=&#34;https://www.linkedin.com/in/hemansnation/&#34;&gt;https://www.linkedin.com/in/hemansnation/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;YouTube: &lt;a href=&#34;https://www.youtube.com/@Himanshu-Ramchandani&#34;&gt;https://www.youtube.com/@Himanshu-Ramchandani&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Twitter: &lt;a href=&#34;https://twitter.com/hemansnation&#34;&gt;https://twitter.com/hemansnation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;GitHub: &lt;a href=&#34;https://github.com/hemansnation&#34;&gt;https://github.com/hemansnation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Instagram: &lt;a href=&#34;https://www.instagram.com/masterdexter.ai/&#34;&gt;https://www.instagram.com/masterdexter.ai/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;AI Jobs LinkedIn Group:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/groups/12540639/&#34;&gt;https://www.linkedin.com/groups/12540639/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Medium Blog:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://medium.com/@hemansnation&#34;&gt;https://medium.com/@hemansnation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Notes on Data, Product, and AI - Newsletter:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/build-relation/newsletter-follow?entityUrn=7014799989251956736&#34;&gt;https://www.linkedin.com/build-relation/newsletter-follow?entityUrn=7014799989251956736&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Any Query?&lt;/h3&gt; &#xA;&lt;p&gt;Email Me Here: &lt;a href=&#34;mailto:connect@himanshuramchandani.co&#34;&gt;connect@himanshuramchandani.co&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ai-forever/Kandinsky-2</title>
    <updated>2023-04-06T01:39:38Z</updated>
    <id>tag:github.com,2023-04-06:/ai-forever/Kandinsky-2</id>
    <link href="https://github.com/ai-forever/Kandinsky-2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kandinsky 2 — multilingual text2image latent diffusion model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kandinsky 2.1&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Framework-PyTorch-orange.svg?sanitize=true&#34; alt=&#34;Framework: PyTorch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/sberbank-ai/Kandinsky_2.1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Huggingface-yello.svg?sanitize=true&#34; alt=&#34;Huggingface space&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1xSbu-b-EwYd6GdaFPRVgvXBX_mciZ41e?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://habr.com/ru/company/sberbank/blog/725282/&#34;&gt;Habr post&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://fusionbrain.ai/diffusion&#34;&gt;Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install &#34;git+https://github.com/ai-forever/Kandinsky-2.git&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model architecture:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/Kandinsky-2/main/content/kandinsky21.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas.&lt;/p&gt; &#xA;&lt;p&gt;As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.&lt;/p&gt; &#xA;&lt;p&gt;For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Other architecture parts:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Text encoder (XLM-Roberta-Large-Vit-L-14) - 560M&lt;/li&gt; &#xA; &lt;li&gt;Diffusion Image Prior — 1B&lt;/li&gt; &#xA; &lt;li&gt;CLIP image encoder (ViT-L/14) - 427M&lt;/li&gt; &#xA; &lt;li&gt;Latent Diffusion U-Net - 1.22B&lt;/li&gt; &#xA; &lt;li&gt;MoVQ encoder/decoder - 67M&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets.&lt;/p&gt; &#xA;&lt;h2&gt;How to use:&lt;/h2&gt; &#xA;&lt;p&gt;Check our jupyter notebooks with examples in &lt;code&gt;./notebooks&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;h3&gt;1. text2image&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from kandinsky2 import get_kandinsky2&#xA;model = get_kandinsky2(&#39;cuda&#39;, task_type=&#39;text2img&#39;, model_version=&#39;2.1&#39;, use_flash_attention=False)&#xA;images = model.generate_text2img(&#xA;    &#34;red cat, 4k photo&#34;, &#xA;    num_steps=100,&#xA;    batch_size=1, &#xA;    guidance_scale=4,&#xA;    h=768, w=768,&#xA;    sampler=&#39;p_sampler&#39;, &#xA;    prior_cf_scale=4,&#xA;    prior_steps=&#34;5&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/Kandinsky-2/main/content/einstein.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;prompt: &#34;Einstein in space around the logarithm scheme&#34;&lt;/p&gt; &#xA;&lt;h3&gt;2. image fuse&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from kandinsky2 import get_kandinsky2&#xA;from PIL import Image&#xA;model = get_kandinsky2(&#39;cuda&#39;, task_type=&#39;text2img&#39;, model_version=&#39;2.1&#39;, use_flash_attention=False)&#xA;images_texts = [&#39;red cat&#39;, Image.open(&#39;img1.jpg&#39;), Image.open(&#39;img2.jpg&#39;), &#39;a man&#39;]&#xA;weights = [0.25, 0.25, 0.25, 0.25]&#xA;images = model.mix_images(&#xA;    images_texts, &#xA;    weights, &#xA;    num_steps=150,&#xA;    batch_size=1, &#xA;    guidance_scale=5,&#xA;    h=768, w=768,&#xA;    sampler=&#39;p_sampler&#39;, &#xA;    prior_cf_scale=4,&#xA;    prior_steps=&#34;5&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/Kandinsky-2/main/content/fuse.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;3. inpainting&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from kandinsky2 import get_kandinsky2&#xA;from PIL import Image&#xA;import numpy as np&#xA;&#xA;model = get_kandinsky2(&#39;cuda&#39;, task_type=&#39;inpainting&#39;, model_version=&#39;2.1&#39;, use_flash_attention=False)&#xA;init_image = Image.open(&#39;img.jpg&#39;)&#xA;mask = np.ones((768, 768), dtype=np.float32)&#xA;mask[:,:550] =  0&#xA;images = model.generate_inpainting(&#xA;    &#39;man 4k photo&#39;, &#xA;    init_image, &#xA;    mask, &#xA;    num_steps=150,&#xA;    batch_size=1, &#xA;    guidance_scale=5,&#xA;    h=768, w=768,&#xA;    sampler=&#39;p_sampler&#39;, &#xA;    prior_cf_scale=4,&#xA;    prior_steps=&#34;5&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Kandinsky 2.0&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Framework-PyTorch-orange.svg?sanitize=true&#34; alt=&#34;Framework: PyTorch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/sberbank-ai/Kandinsky_2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Huggingface-yello.svg?sanitize=true&#34; alt=&#34;Huggingface space&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1uPg9KwGZ2hJBl9taGA_3kyKGw12Rh3ij?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://habr.com/ru/company/sberbank/blog/701162/&#34;&gt;Habr post&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://fusionbrain.ai/diffusion&#34;&gt;Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install &#34;git+https://github.com/ai-forever/Kandinsky-2.git&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model architecture:&lt;/h2&gt; &#xA;&lt;p&gt;It is a latent diffusion model with two multilingual text encoders:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;mCLIP-XLMR 560M parameters&lt;/li&gt; &#xA; &lt;li&gt;mT5-encoder-small 146M parameters&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Kandinsky 2.0&lt;/strong&gt; was trained on a large 1B multilingual set, including samples that we used to train Kandinsky.&lt;/p&gt; &#xA;&lt;p&gt;In terms of diffusion architecture Kandinsky 2.0 implements UNet with 1.2B parameters.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Kandinsky 2.0&lt;/strong&gt; architecture overview:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/Kandinsky-2/main/content/NatallE.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to use:&lt;/h2&gt; &#xA;&lt;p&gt;Check our jupyter notebooks with examples in &lt;code&gt;./notebooks&lt;/code&gt; folder&lt;/p&gt; &#xA;&lt;h3&gt;1. text2img&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from kandinsky2 import get_kandinsky2&#xA;&#xA;model = get_kandinsky2(&#39;cuda&#39;, task_type=&#39;text2img&#39;)&#xA;images = model.generate_text2img(&#39;A teddy bear на красной площади&#39;, batch_size=4, h=512, w=512, num_steps=75, denoised_type=&#39;dynamic_threshold&#39;, dynamic_threshold_v=99.5, sampler=&#39;ddim_sampler&#39;, ddim_eta=0.05, guidance_scale=10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/Kandinsky-2/main/content/bear.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;prompt: &#34;A teddy bear на красной площади&#34;&lt;/p&gt; &#xA;&lt;h3&gt;2. inpainting&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from kandinsky2 import get_kandinsky2&#xA;from PIL import Image&#xA;import numpy as np&#xA;&#xA;model = get_kandinsky2(&#39;cuda&#39;, task_type=&#39;inpainting&#39;)&#xA;init_image = Image.open(&#39;image.jpg&#39;)&#xA;mask = np.ones((512, 512), dtype=np.float32)&#xA;mask[100:] =  0&#xA;images = model.generate_inpainting(&#39;Девушка в красном платье&#39;, init_image, mask, num_steps=50, denoised_type=&#39;dynamic_threshold&#39;, dynamic_threshold_v=99.5, sampler=&#39;ddim_sampler&#39;, ddim_eta=0.05, guidance_scale=10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/Kandinsky-2/main/content/inpainting.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;prompt: &#34;Девушка в красном платье&#34;&lt;/p&gt; &#xA;&lt;h3&gt;3. img2img&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from kandinsky2 import get_kandinsky2&#xA;from PIL import Image&#xA;&#xA;model = get_kandinsky2(&#39;cuda&#39;, task_type=&#39;img2img&#39;)&#xA;init_image = Image.open(&#39;image.jpg&#39;)&#xA;images = model.generate_img2img(&#39;кошка&#39;, init_image, strength=0.8, num_steps=50, denoised_type=&#39;dynamic_threshold&#39;, dynamic_threshold_v=99.5, sampler=&#39;ddim_sampler&#39;, ddim_eta=0.05, guidance_scale=10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Authors&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Arseniy Shakhmatov: &lt;a href=&#34;https://github.com/cene555&#34;&gt;Github&lt;/a&gt;, &lt;a href=&#34;https://t.me/gradientdip&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Anton Razzhigaev: &lt;a href=&#34;https://github.com/razzant&#34;&gt;Github&lt;/a&gt;, &lt;a href=&#34;https://t.me/abstractDL&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Aleksandr Nikolich: &lt;a href=&#34;https://github.com/AlexWortega&#34;&gt;Github&lt;/a&gt;, &lt;a href=&#34;https://t.me/lovedeathtransformers&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vladimir Arkhipkin: &lt;a href=&#34;https://github.com/oriBetelgeuse&#34;&gt;Github&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Igor Pavlov: &lt;a href=&#34;https://github.com/boomb0om&#34;&gt;Github&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Andrey Kuznetsov: &lt;a href=&#34;https://github.com/kuznetsoffandrey&#34;&gt;Github&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Denis Dimitrov: &lt;a href=&#34;https://github.com/denndimitrov&#34;&gt;Github&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>