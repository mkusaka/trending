<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-07T01:36:38Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lxe/simple-llm-finetuner</title>
    <updated>2023-04-07T01:36:38Z</updated>
    <id>tag:github.com,2023-04-07:/lxe/simple-llm-finetuner</id>
    <link href="https://github.com/lxe/simple-llm-finetuner" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Simple UI for LLM Model Finetuning&lt;/p&gt;&lt;hr&gt;&lt;hr&gt; &#xA;&lt;h2&gt;title: Simple LLM Finetuner emoji: ðŸ¦™ colorFrom: yellow colorTo: orange sdk: gradio app_file: app.py pinned: false&lt;/h2&gt; &#xA;&lt;h1&gt;ðŸ¦™ Simple LLM Finetuner&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/lxe/simple-llama-finetuner/blob/master/Simple_LLaMA_FineTuner.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Open%20in%20Colab&amp;amp;message=Select%20HIGH%20RAM&amp;amp;color=yellow&amp;amp;logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/lxe/simple-llama-finetuner&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20In%20Spaces-blue.svg?sanitize=true&#34; alt=&#34;Open In Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lxe/no-bugs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/no-bugs-brightgreen.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lxe/onehundred/tree/master&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/coverage-%F0%9F%92%AF-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Simple LLM Finetuner is a beginner-friendly interface designed to facilitate fine-tuning various language models using &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; method via the &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT library&lt;/a&gt; on commodity NVIDIA GPUs. With small dataset and sample lengths of 256, you can even run this on a regular Colab Tesla T4 instance.&lt;/p&gt; &#xA;&lt;p&gt;With this intuitive UI, you can easily manage your dataset, customize parameters, train, and evaluate the model&#39;s inference capabilities.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zphang/minimal-llama/&#34;&gt;https://github.com/zphang/minimal-llama/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;https://github.com/tloen/alpaca-lora&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;https://github.com/huggingface/peft&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simply paste datasets in the UI, separated by double blank lines&lt;/li&gt; &#xA; &lt;li&gt;Adjustable parameters for fine-tuning and inference&lt;/li&gt; &#xA; &lt;li&gt;Beginner-friendly UI with explanations for each parameter&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux or WSL&lt;/li&gt; &#xA; &lt;li&gt;Modern NVIDIA GPU with &amp;gt;= 16 GB of VRAM (but it might be possible to run with less for smaller sample lengths)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;I recommend using a virtual environment to install the required packages. Conda preferred.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n simple-llm-finetuner python=3.10&#xA;conda activate simple-llm-finetuner&#xA;conda install -y cuda -c nvidia/label/cuda-11.7.0&#xA;conda install -y pytorch=2 pytorch-cuda=11.7 -c pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On WSL, you might need to install CUDA manually by following &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;Distribution=WSL-Ubuntu&amp;amp;target_version=2.0&amp;amp;target_type=deb_local&#34;&gt;these steps&lt;/a&gt;, then running the following before you launch:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export LD_LIBRARY_PATH=/usr/lib/wsl/lib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Clone the repository and install the required packages.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/lxe/simple-llm-finetuner.git&#xA;cd simple-llm-finetuner&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Launch it&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open &lt;a href=&#34;http://127.0.0.1:7860/&#34;&gt;http://127.0.0.1:7860/&lt;/a&gt; in your browser. Prepare your training data by separating each sample with 2 blank lines. Paste the whole training dataset into the textbox. Specify the new LoRA adapter name in the &#34;New PEFT Adapter Name&#34; textbox, then click train. You might need to adjust the max sequence length and batch size to fit your GPU memory. The model will be saved in the &lt;code&gt;lora/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;After training is done, navigate to &#34;Inference&#34; tab, select your LoRA, and play with it.&lt;/p&gt; &#xA;&lt;p&gt;Have fun!&lt;/p&gt; &#xA;&lt;h2&gt;YouTube Walkthough&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=yM1wanDkNz8&#34;&gt;https://www.youtube.com/watch?v=yM1wanDkNz8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT License&lt;/p&gt;</summary>
  </entry>
</feed>