<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-21T01:48:06Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dair-ai/ML-Notebooks</title>
    <updated>2022-07-21T01:48:06Z</updated>
    <id>tag:github.com,2022-07-21:/dair-ai/ML-Notebooks</id>
    <link href="https://github.com/dair-ai/ML-Notebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ğŸ”¥ A series of code examples for all sorts of machine learning tasks and applications.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ™ ML Notebooks&lt;/h1&gt; &#xA;&lt;p&gt;Contains code examples for all sorts of machine learning tasks and applications. The notebooks are meant to be minimal and easily reusable and extendable. You are free to use them for educational and research purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;table class=&#34;tg&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Name&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Description&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Notebook&lt;/b&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Introduction to Computational Graphs&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;A basic tutorial to learn about computational graphs&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1eG1AF36Wa0EaANandAhrsbC3j04487SH?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;PyTorch Hello World!&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Build a simple neural network and train it&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1ac0K9_aa46c77XEeYtaMAfSOfmH1Bl9L?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;A Gentle Introduction to PyTorch&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;A detailed explanation introducing PyTorch concepts&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1K7Ks1ERaS-w4rzW_ukeBag8mYRZME1es?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Counterfactual Explanations&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;A basic tutorial to learn about counterfactual explanations for explainable AI&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1mTSRjqki3VsH9MVPfNtJ5nJxcCHvL8B6?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Logistic Regression from Scratch&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;An implementation of logistic regression from scratch&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1iBoJ0kngkOthy7SgVaVQA1aHEROt5mra?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Concise Logistic Regression&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Concise implementation of logistic regression model for binary image classification.&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/14hnFJvHDq9w7FGb8P6pd6-I7F3djTRG9?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;First Neural Network - Image Classifier&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Build a minimal image classifier using MNIST&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1i94k-n97Z5r1KWV9Vly9IiKnYxf3Tfvu?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Neural Network from Scratch&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;An implementation of simple neural network from scratch&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1YBcEZMUHhJUiwOIwQbqwmAAGrRznpP_E?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Introduction to GNNs&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Introduction to Graph Neural Networks. Applies basic GCN to Cora dataset for node classification.&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1d0jLDwgNBtjBVQOFe8lO_1WrqTVeVZx9?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;NLP&lt;/h2&gt; &#xA;&lt;table class=&#34;tg&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Name&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Description&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Notebook&lt;/b&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Bag of Words Text Classifier&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Build a simple bag of words text classifier.&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/19suDts9MNIhx0TeGO26_BIY2Xc0n6DBC?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Continuous Bag of Words (CBOW) Text Classifier&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Build a continuous bag of words text classifier.&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1lqS67-mbCspIKzx6y9wn7CuP96utWzP2?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Deep Continuous Bag of Words (Deep CBOW) Text Classifier&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Build a deep continuous bag of words text classifier.&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/18yz-qvMQYIYZt1BLihSJrKQZXh8zjH8x?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Text Data Augmentation&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;An introduction to the most commonly used data augmentation techniques for text and their implementation&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1kyLaRevVf7VVy9BxJBJaL_ET4wyKSi-S?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Emotion Classification with Fine-tuned BERT&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Emotion classification using fine-tuned BERT model&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1nwCE6b9PXIKhv2hvbqf1oZKIGkXMTi1X?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Transformers&lt;/h2&gt; &#xA;&lt;table class=&#34;tg&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Name&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Description&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Notebook&lt;/b&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Text Classification using Transformer&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;An implementation of Attention Mechanism and Positional Embeddings on a text classification task&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Jc-_kcO3xYHDMFYSsIcUimcc38_WFlyh?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.kaggle.com/code/ritvik1909/text-classification-attention&#34; target=&#34;_blank&#34;&gt;&lt;img align=&#34;left&#34; alt=&#34;Kaggle&#34; title=&#34;Open in Kaggle&#34; src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Neural Machine Translation using Transformer&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;An implementation of Transformer to translate human readabke dates in any format to YYYY-MM-DD format.&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Y6JhWMmgU52MU9vyrOnHf6iK760Qx35h?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.kaggle.com/code/ritvik1909/neural-machine-translation-attention&#34; target=&#34;_blank&#34;&gt;&lt;img align=&#34;left&#34; alt=&#34;Kaggle&#34; title=&#34;Open in Kaggle&#34; src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Feature Tokenizer Transformer&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;An implementation of Feature Tokenizer Transformer on a classification task&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1tdPifaZCTVpjzCh1FOyGPywMEfrbBZLh?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.kaggle.com/code/ritvik1909/feature-tokenizer-transformer/&#34; target=&#34;_blank&#34;&gt;&lt;img align=&#34;left&#34; alt=&#34;Kaggle&#34; title=&#34;Open in Kaggle&#34; src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Named Entity Recognition using Transformer&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;An implementation of Transformer to perform token classification and identify species in PubMed abstracts&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/12AdzQuOvFMrhlVI92dfaVpttkgBcgWcf?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.kaggle.com/code/ritvik1909/named-entity-recognition-attention&#34; target=&#34;_blank&#34;&gt;&lt;img align=&#34;left&#34; alt=&#34;Kaggle&#34; title=&#34;Open in Kaggle&#34; src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Extractive Question Answering using Transformer&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;An implementation of Transformer to perform extractive question answering&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Eq3PkrItGTZMTww-wlzume2j4nkQYsdP?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.kaggle.com/code/ritvik1909/question-answering-attention/&#34; target=&#34;_blank&#34;&gt;&lt;img align=&#34;left&#34; alt=&#34;Kaggle&#34; title=&#34;Open in Kaggle&#34; src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Computer Vision&lt;/h2&gt; &#xA;&lt;table class=&#34;tg&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Name&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Description&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Notebook&lt;/b&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Siamese Network&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;An implementation of Siamese Network for finding Image Similarity&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1sn7BDKVvi8-Ng37gvfyNw8OCf8kZY91o?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://kaggle.com/code/ritvik1909/siamese-network&#34; target=&#34;_blank&#34;&gt;&lt;img align=&#34;left&#34; alt=&#34;Kaggle&#34; title=&#34;Open in Kaggle&#34; src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Variational Auto Encoder&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;An implementation of Variational Auto Encoder to generate Augmentations for MNIST Handwritten Digits&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/13na-qaRSMwD2jplXjg_oapPDmVozIzLL?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.kaggle.com/code/ritvik1909/variational-auto-encoder&#34; target=&#34;_blank&#34;&gt;&lt;img align=&#34;left&#34; alt=&#34;Kaggle&#34; title=&#34;Open in Kaggle&#34; src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Object Detection using Sliding Window and Image Pyramid&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;A basic object detection implementation using sliding window and image pyramid on top of an image classifer&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1CiLN6g7puBheHq-tfk4YB4X-Lgypur8M?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.kaggle.com/code/ritvik1909/object-detection-sliding-window&#34; target=&#34;_blank&#34;&gt;&lt;img align=&#34;left&#34; alt=&#34;Kaggle&#34; title=&#34;Open in Kaggle&#34; src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Object Detection using Selective Search&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;A basic object detection implementation using selective search on top of an image classifer&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1C7Zo5JRMrcETMxn-8jRuszrEXXjRbN37?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.kaggle.com/code/ritvik1909/object-detection-selective-search&#34; target=&#34;_blank&#34;&gt;&lt;img align=&#34;left&#34; alt=&#34;Kaggle&#34; title=&#34;Open in Kaggle&#34; src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Generative Adversarial Network&lt;/h2&gt; &#xA;&lt;table class=&#34;tg&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Name&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Description&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th class=&#34;tg-yw4l&#34;&gt;&lt;b&gt;Notebook&lt;/b&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Deep Convolutional GAN&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;An Implementation of Deep Convolutional GAN to generate MNIST digits&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1ss9FCHvI0ZAuS1PJL6RxlgEhM_glZR6S?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.kaggle.com/code/ritvik1909/deep-convolutional-gan/&#34; target=&#34;_blank&#34;&gt;&lt;img align=&#34;left&#34; alt=&#34;Kaggle&#34; title=&#34;Open in Kaggle&#34; src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Wasserstein GAN with Gradient Penalty&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;An Implementation of Wasserstein GAN with Gradient Penalty to generate MNIST digits&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1x7aDlxkv3PYev2AsmZRl8pII2zsY_DVs?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.kaggle.com/code/ritvik1909/wasserstein-gan-with-gradient-penalty/&#34; target=&#34;_blank&#34;&gt;&lt;img align=&#34;left&#34; alt=&#34;Kaggle&#34; title=&#34;Open in Kaggle&#34; src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;Conditional GAN&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;An Implementation of Conditional GAN to generate MNIST digits&lt;/td&gt; &#xA;   &lt;td class=&#34;tg-yw4l&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Z74CpEu76e6A62gxKXIJ8u8xyVjF21_k?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; width=&#34;&#34;&gt; &lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.kaggle.com/code/ritvik1909/conditional-gan/&#34; target=&#34;_blank&#34;&gt;&lt;img align=&#34;left&#34; alt=&#34;Kaggle&#34; title=&#34;Open in Kaggle&#34; src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If you find any bugs or have any questions regarding these notebooks, please open an issue. We will address it as soon as we can.&lt;/p&gt; &#xA;&lt;p&gt;Reach out on &lt;a href=&#34;https://twitter.com/omarsar0&#34;&gt;Twitter&lt;/a&gt; if you have any questions.&lt;/p&gt; &#xA;&lt;p&gt;Please cite the following if you use the code examples in your research:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{saravia2022ml,&#xA;  title={ML Notebooks},&#xA;  author={Saravia, Elvis and Rastogi, Ritvik},&#xA;  journal={https://github.com/dair-ai/ML-Notebooks},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>icoxfog417/mlnote-note</title>
    <updated>2022-07-21T01:48:06Z</updated>
    <id>tag:github.com,2022-07-21:/icoxfog417/mlnote-note</id>
    <link href="https://github.com/icoxfog417/mlnote-note" rel="alternate"></link>
    <summary type="html">&lt;p&gt;æ©Ÿæ¢°å­¦ç¿’å¸³ã‚’å­¦ã¶ãƒãƒ¼ãƒˆ&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/icoxfog417/mlnote-note/raw/main/images/top.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;mlnote-note&lt;/strong&gt;ã¯ã€å²¡å´ ç›´è¦³å…ˆç”ŸãŒä½œæˆã•ã‚ŒãŸ&lt;a href=&#34;https://chokkan.github.io/mlnote/index.html&#34;&gt;æ©Ÿæ¢°å­¦ç¿’å¸³&lt;/a&gt;ã‚’40æ—¥é–“ã€ã¡ã‚‡ã£ã¨ãšã¤æ¯æ—¥å­¦ã¶ãŸã‚ã®æ•™æã§ã™ã€‚&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1æ—¥ã”ã¨ã®å­¦ç¿’å†…å®¹ã‚’1æšã®ã‚¹ãƒ©ã‚¤ãƒ‰ã§è¦ç´„ã—ã¦è§£èª¬ã—ã¦ã„ã¾ã™ã€‚&lt;/li&gt; &#xA; &lt;li&gt;ç¢ºèªå•é¡Œã‚’è§£ãã®ã«é›†ä¸­ã§ãã‚‹ã‚ˆã†ã€ç’°å¢ƒæ§‹ç¯‰ãŒä¸è¦ãª&lt;a href=&#34;https://aws.amazon.com/jp/builders-flash/202205/awsgeek-sagemaker-studio-lab/&#34;&gt;Amazon SageMaker Studio Lab&lt;/a&gt;ã§ç¢ºèªå•é¡Œã‚’è§£ã„ã¦ã„ã¾ã™(â€»)ã€‚&lt;/li&gt; &#xA; &lt;li&gt;ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã«ã‚ˆã‚‹å­¦ç¿’ã®å®šç€ã‚’ä¿ƒã™ãŸã‚ã«ã€ã‚¹ãƒ©ã‚¤ãƒ‰ã®ä½œæˆã«ä½¿ã£ãŸPowerPointãƒ•ã‚¡ã‚¤ãƒ«&lt;a href=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/mlnote-slides.pptx&#34;&gt;&lt;code&gt;mlnote-slides.pptx&lt;/code&gt;&lt;/a&gt;ã‚’ãƒªãƒã‚¸ãƒˆãƒªã«å«ã‚ã¦ã„ã¾ã™ã€‚è‡ªåˆ†ãªã‚Šã«æ©Ÿæ¢°å­¦ç¿’å¸³ã‚’è¦ç´„ã—ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã™ã‚‹ãŸã‚ã®ç´ æã¨ã—ã¦ãŠä½¿ã„ãã ã•ã„ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;â€»æ¼”ç¿’ç”¨ã®Notebookã¨ã€å›ç­”ä¾‹ã‚’è¨˜å…¥ã—ãŸNotebookã¯ãã‚Œãã‚Œã€ŒOpen Studio Labã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨é–‹ã‘ã¾ã™ã€‚é–‹ãã®ã«Studio Labã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¯ä¸è¦ã§ã™ã€‚å‹•ã‹ã™ã®ã«å¿…è¦ã§ã™ã€‚ä½¿ã„æ–¹ã¯ã€Œå­¦ç¿’ã‚’å§‹ã‚ã‚‹ã€ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ã‚ªãƒªã‚¸ãƒŠãƒ«ã®è¦ç´„ã‚’ä½œã‚‰ã‚ŒãŸæ–¹ã¯ã€ãœã²&lt;code&gt;40daysmlnote&lt;/code&gt;ã§å‘Ÿã„ã¦ã„ãŸã ã‘ã‚‹ã¨å¬‰ã—ã„ã§ã™ï¼æœ¬ãƒªãƒã‚¸ãƒˆãƒªå†…ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã«èª¤ã‚ŠãŒã‚ã£ãŸã‚Šã—ãŸå ´åˆã¯&lt;a href=&#34;https://github.com/icoxfog417/mlnote-note/issues&#34;&gt;Issues&lt;/a&gt;ã«ã¦ã”é€£çµ¡ãã ã•ã„ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/search?q=%2340daysmlnote&amp;amp;src=typed_query&amp;amp;f=live&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Twitter-%231DA1F2.svg?style=for-the-badge&amp;amp;logo=Twitter&amp;amp;logoColor=white&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/hashtag/30daymlnote?src=hashtag_click&#34;&gt;è‘—è€…ãŒæ¯æ—¥ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã—ãŸæ§˜å­ã¯ã“ã¡ã‚‰ã‹ã‚‰å‚ç…§ã§ãã¾ã™ã€‚&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ç›®æ¬¡&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/#%E5%9B%9E%E5%B8%B0&#34;&gt;å›å¸°&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/#%E5%88%86%E9%A1%9E&#34;&gt;åˆ†é¡&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/#%E6%95%99%E5%B8%AB%E7%84%A1%E3%81%97%E5%AD%A6%E7%BF%92&#34;&gt;æ•™å¸«ç„¡ã—å­¦ç¿’&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/#%E5%AD%A6%E7%BF%92%E3%82%92%E5%A7%8B%E3%82%81%E3%82%8B&#34;&gt;å­¦ç¿’ã‚’å§‹ã‚ã‚‹&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å›å¸°&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Day&lt;/th&gt; &#xA;   &lt;th&gt;Lecture&lt;/th&gt; &#xA;   &lt;th&gt;Summary&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/regression/01sra.html&#34;&gt;1.å˜å›å¸° 1.1~1.4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter1/chapter1-1.svg?sanitize=true&#34; alt=&#34;chapter1-1.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/regression/01sra.html#a-b&#34;&gt;1.å˜å›å¸° 1.4~1.7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter1/chapter1-2.svg?sanitize=true&#34; alt=&#34;chapter1-2.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/regression/01sra.html#id13&#34;&gt;1.å˜å›å¸° ç¢ºèªå•é¡Œ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¼”ç¿’ç”¨: &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter1.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt; å›ç­”ä¾‹: &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter1_answer.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/regression/02mra.html&#34;&gt;2.é‡å›å¸° 2.1~2.8&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter2/chapter2-1.svg?sanitize=true&#34; alt=&#34;chapter2-1.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/regression/02mra.html#id17&#34;&gt;2.é‡å›å¸° ç¢ºèªå•é¡Œ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¼”ç¿’ç”¨:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter2.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt; å›ç­”ä¾‹ &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter2_answer.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/regression/03regularization.html&#34;&gt;3.ãƒ¢ãƒ‡ãƒ«é¸æŠã¨æ­£å‰‡åŒ– 3.1~3.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter3/chapter3-1.svg?sanitize=true&#34; alt=&#34;chapter3-1.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/regression/03regularization.html#id4&#34;&gt;3.ãƒ¢ãƒ‡ãƒ«é¸æŠã¨æ­£å‰‡åŒ– ç¢ºèªå•é¡Œ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¼”ç¿’ç”¨:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter3.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt; å›ç­”ä¾‹:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter3_answer.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/regression/04sgd.html&#34;&gt;4.å‹¾é…æ³•ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š 4.1~4.4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter4/chapter4-1.svg?sanitize=true&#34; alt=&#34;chapter4-1.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/regression/04sgd.html#id10&#34;&gt;4.å‹¾é…æ³•ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š 4.5~4.8&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter4/chapter4-2.svg?sanitize=true&#34; alt=&#34;chapter4-2.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/regression/04sgd.html#id20&#34;&gt;4.å‹¾é…æ³•ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š ç¢ºèªå•é¡Œ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¼”ç¿’ç”¨: &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter4.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt; å›ç­”ä¾‹:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter4_answer.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;åˆ†é¡&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Day&lt;/th&gt; &#xA;   &lt;th&gt;Lecture&lt;/th&gt; &#xA;   &lt;th&gt;Summary&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/01binary.html&#34;&gt;5.ç·šå½¢äºŒå€¤åˆ†é¡ 5.1~5.4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter5/chapter5-1.svg?sanitize=true&#34; alt=&#34;chapter5-1.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/01binary.html#id7&#34;&gt;5.ç·šå½¢äºŒå€¤åˆ†é¡ 5.5~5.7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter5/chapter5-2.svg?sanitize=true&#34; alt=&#34;chapter5-2.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/01binary.html#id11&#34;&gt;5.ç·šå½¢äºŒå€¤åˆ†é¡ 5.8&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter5/chapter5-3.svg?sanitize=true&#34; alt=&#34;chapter5-3.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/01binary.html#id19&#34;&gt;5.ç·šå½¢äºŒå€¤åˆ†é¡ ç¢ºèªå•é¡Œ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¼”ç¿’ç”¨:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter5.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt; å›ç­”ä¾‹:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter5_answer.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;15&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/02multi.html&#34;&gt;6.ç·šå½¢å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ 6.1~6.6&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter6/chapter6-1.svg?sanitize=true&#34; alt=&#34;chapter6-1.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/02multi.html#id10&#34;&gt;6.ç·šå½¢å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ 6.7~6.8&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter6/chapter6-2.svg?sanitize=true&#34; alt=&#34;chapter6-2.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;17&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/02multi.html#id13&#34;&gt;6.ç·šå½¢å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ 6.9&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter6/chapter6-3.svg?sanitize=true&#34; alt=&#34;chapter6-3.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;18&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/02multi.html#id16&#34;&gt;6.ç·šå½¢å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ ç¢ºèªå•é¡Œ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¼”ç¿’ç”¨:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter6.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt; å›ç­”ä¾‹:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter6_answer.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;19&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/03nn.html&#34;&gt;7.ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (1) 7.1~7.2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter7/chapter7-1.svg?sanitize=true&#34; alt=&#34;chapter7-1.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/03nn.html#id18&#34;&gt;7.ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (1) 7.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter7/chapter7-2.svg?sanitize=true&#34; alt=&#34;chapter7-2.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;21&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/03nn.html#id21&#34;&gt;7.ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (1) ç¢ºèªå•é¡Œ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¼”ç¿’ç”¨:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter7.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt; å›ç­”ä¾‹:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter7_answer.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;22&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/04nntrain.html&#34;&gt;8.ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (2) 8.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter8/chapter8-1.svg?sanitize=true&#34; alt=&#34;chapter8-1.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;23&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/04nntrain.html#nn&#34;&gt;8.ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (2) 8.2~8.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter8/chapter8-2.svg?sanitize=true&#34; alt=&#34;chapter8-2.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/04nntrain.html#id13&#34;&gt;8.ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (2) ç¢ºèªå•é¡Œ(8.4)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¼”ç¿’ç”¨:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter8.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt; å›ç­”ä¾‹:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter8_answer.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/04nntrain.html#id13&#34;&gt;8.ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (2) ç¢ºèªå•é¡Œ(8.5)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¼”ç¿’ç”¨:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter8.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt; å›ç­”ä¾‹:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter8_answer.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;26&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/05svm.html&#34;&gt;9.ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ãƒˆãƒ«ãƒã‚·ãƒ³ 9.1~9.2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter9/chapter9-1.svg?sanitize=true&#34; alt=&#34;chapter9-1.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;27&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/05svm.html#id6&#34;&gt;9.ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ãƒˆãƒ«ãƒã‚·ãƒ³ 9.4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter9/chapter9-2.svg?sanitize=true&#34; alt=&#34;chapter9-2.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;28&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/05svm.html#svm&#34;&gt;9.ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ãƒˆãƒ«ãƒã‚·ãƒ³ 9.3,9.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter9_explain.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;æ•™å¸«ç„¡ã—å­¦ç¿’&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Day&lt;/th&gt; &#xA;   &lt;th&gt;Lecture&lt;/th&gt; &#xA;   &lt;th&gt;Summary&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;29&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/unsupervised/01kmeans.html&#34;&gt;10.ééšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° 10.1~10.2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter10/chapter10-1.svg?sanitize=true&#34; alt=&#34;chapter10-1.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/unsupervised/01kmeans.html#lloyd&#34;&gt;10.ééšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° 10.3&lt;del&gt;10.4,10.6&lt;/del&gt;10.8&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter10/chapter10-2.svg?sanitize=true&#34; alt=&#34;chapter10-2.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;31&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/unsupervised/01kmeans.html#id20&#34;&gt;10.ééšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ç¢ºèªå•é¡Œ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¼”ç¿’ç”¨:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter10.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt; å›ç­”ä¾‹:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter10_answer.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/unsupervised/02hac.html&#34;&gt;11.éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° 11.1~11.2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter11/chapter11-1.svg?sanitize=true&#34; alt=&#34;chapter11-1.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;33&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/unsupervised/02hac.html#id10&#34;&gt;11.éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° 11.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter11/chapter11-2.svg?sanitize=true&#34; alt=&#34;chapter11-2.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;34&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/unsupervised/02hac.html#id19&#34;&gt;11.éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ç¢ºèªå•é¡Œ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¼”ç¿’ç”¨:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter11.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt; å›ç­”ä¾‹:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter11_answer.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;35&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/unsupervised/03pca.html&#34;&gt;12.ä¸»æˆåˆ†åˆ†æ (1) 12.1~12.3, 12.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter12/chapter12-1.svg?sanitize=true&#34; alt=&#34;chapter12-1.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;36&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/unsupervised/03pca.html#id5&#34;&gt;12.ä¸»æˆåˆ†åˆ†æ (1) 12.4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter12/chapter12-2.svg?sanitize=true&#34; alt=&#34;chapter12-2.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;37&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/unsupervised/03pca.html#id8&#34;&gt;12.ä¸»æˆåˆ†åˆ†æ (1) ç¢ºèªå•é¡Œ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¼”ç¿’ç”¨:&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter12.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt; å›ç­”ä¾‹:&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/unsupervised/04pca2.html&#34;&gt;13.ä¸»æˆåˆ†åˆ†æ (2) 13.1~13.2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter13/chapter13-1.svg?sanitize=true&#34; alt=&#34;chapter13-1.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;39&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/unsupervised/04pca2.html#id7&#34;&gt;13.ä¸»æˆåˆ†åˆ†æ (2) 13.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/icoxfog417/mlnote-note/main/notebooks/images/chapter13/chapter13-2.svg?sanitize=true&#34; alt=&#34;chapter13-2.svg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;40&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chokkan.github.io/mlnote/classification/05svm.html#svm&#34;&gt;13.ä¸»æˆåˆ†åˆ†æ (2) 13.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/icoxfog417/mlnote-note/blob/main/notebooks/chapter13.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open in SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;å­¦ç¿’ã‚’å§‹ã‚ã‚‹&lt;/h2&gt; &#xA;&lt;h3&gt;Studio Labã‚’ä½¿ã†&lt;/h3&gt; &#xA;&lt;p&gt;Studio Labã¯ã€ãƒ–ãƒ©ã‚¦ã‚¶ä¸Šã§JupyterLabã®ç’°å¢ƒãŒåˆ©ç”¨ã§ãã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®ã¿ã€ç„¡æ–™ã§åˆ©ç”¨ãŒå¯èƒ½ã§ã™ã€‚è©³ç´°ã¯&lt;a href=&#34;https://github.com/aws-sagemaker-jp/awesome-studio-lab-jp/raw/main/README_usage.md&#34;&gt;Amazon SageMaker Studio Lab ã®ä½¿ã„æ–¹&lt;/a&gt;ã€ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯&lt;a href=&#34;https://bit.ly/3sB7nC3&#34;&gt;Request Account&lt;/a&gt;ã‚ˆã‚Šã§ãã¾ã™ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã‚’ä½¿ã†&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸå¾Œã€æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§ç’°å¢ƒã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;conda env create --file environment.yml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Windowsã®å ´åˆã¯&lt;code&gt;environment-windows.yml&lt;/code&gt;ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>onnx/tensorflow-onnx</title>
    <updated>2022-07-21T01:48:06Z</updated>
    <id>tag:github.com,2022-07-21:/onnx/tensorflow-onnx</id>
    <link href="https://github.com/onnx/tensorflow-onnx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Convert TensorFlow, Keras, Tensorflow.js and Tflite models to ONNX&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tf2onnx - Convert TensorFlow, Keras, Tensorflow.js and Tflite models to ONNX.&lt;/h1&gt; &#xA;&lt;p&gt;tf2onnx converts TensorFlow (tf-1.x or tf-2.x), keras, tensorflow.js and tflite models to ONNX via command line or python api.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: tensorflow.js support was just added. While we tested it with many tfjs models from tfhub, it should be considered experimental.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;TensorFlow has many more ops than ONNX and occasionally mapping a model to ONNX creates issues.&lt;/p&gt; &#xA;&lt;p&gt;You find a list of supported TensorFlow ops and their mapping to ONNX &lt;a href=&#34;https://raw.githubusercontent.com/onnx/tensorflow-onnx/main/support_status.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The common issues we run into we try to document here &lt;a href=&#34;https://raw.githubusercontent.com/onnx/tensorflow-onnx/main/Troubleshooting.md&#34;&gt;Troubleshooting Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Build Type&lt;/th&gt; &#xA;   &lt;th&gt;OS&lt;/th&gt; &#xA;   &lt;th&gt;Python&lt;/th&gt; &#xA;   &lt;th&gt;TensorFlow&lt;/th&gt; &#xA;   &lt;th&gt;ONNX opset&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unit Test - Basic&lt;/td&gt; &#xA;   &lt;td&gt;Linux, MacOS&lt;sup&gt;*&lt;/sup&gt;, Windows&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.7-3.9&lt;/td&gt; &#xA;   &lt;td&gt;1.13-1.15, 2.1-2.8&lt;/td&gt; &#xA;   &lt;td&gt;9-16&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/tensorflow-onnx/tensorflow-onnx/_build/latest?definitionId=16&amp;amp;branchName=main&#34;&gt;&lt;img src=&#34;https://dev.azure.com/tensorflow-onnx/tensorflow-onnx/_apis/build/status/unit_test?branchName=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unit Test - Full&lt;/td&gt; &#xA;   &lt;td&gt;Linux, MacOS, Windows&lt;/td&gt; &#xA;   &lt;td&gt;3.7-3.9&lt;/td&gt; &#xA;   &lt;td&gt;1.13-1.15, 2.1-2.8&lt;/td&gt; &#xA;   &lt;td&gt;9-16&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/tensorflow-onnx/tensorflow-onnx/_build/latest?definitionId=18&amp;amp;branchName=main&#34;&gt;&lt;img src=&#34;https://dev.azure.com/tensorflow-onnx/tensorflow-onnx/_apis/build/status/unit_test-matrix?branchName=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Supported Versions&lt;/h2&gt; &#xA;&lt;h3&gt;ONNX&lt;/h3&gt; &#xA;&lt;p&gt;tf2onnx will use the ONNX version installed on your system and installs the latest ONNX version if none is found.&lt;/p&gt; &#xA;&lt;p&gt;We support and test ONNX opset-9 to opset-16. opset-6 to opset-8 should work but we don&#39;t test them. By default we use &lt;code&gt;opset-13&lt;/code&gt; for the resulting ONNX graph.&lt;/p&gt; &#xA;&lt;p&gt;If you want the graph to be generated with a specific opset, use &lt;code&gt;--opset&lt;/code&gt; in the command line, for example &lt;code&gt;--opset 13&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;TensorFlow&lt;/h3&gt; &#xA;&lt;p&gt;We support &lt;code&gt;tf-1.x graphs&lt;/code&gt; and &lt;code&gt;tf-2.x&lt;/code&gt;. To keep our test matrix manageable we test tf2onnx running on top of &lt;code&gt;tf-1.13 or better&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When running under tf-2.x tf2onnx will use the tensorflow V2 controlflow.&lt;/p&gt; &#xA;&lt;p&gt;You can install tf2onnx on top of tf-1.x or tf-2.x.&lt;/p&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;p&gt;We support Python &lt;code&gt;3.7-3.9&lt;/code&gt;. Note that on windows for Python &amp;gt; 3.7 the protobuf package doesn&#39;t use the cpp implementation and is very slow - we recommend to use Python 3.7 for that reason.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;h3&gt;Install TensorFlow&lt;/h3&gt; &#xA;&lt;p&gt;If you don&#39;t have TensorFlow installed already, install the desired TensorFlow build, for example:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install tensorflow&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;(Optional) Install runtime&lt;/h3&gt; &#xA;&lt;p&gt;If you want to run tests, install a runtime that can run ONNX models. For example:&lt;/p&gt; &#xA;&lt;p&gt;ONNX Runtime (available for Linux, Windows, and Mac):&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install onnxruntime&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Install from pypi&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install -U tf2onnx&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Install latest from github&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install git+https://github.com/onnx/tensorflow-onnx&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Build and install latest from source (for development)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;git clone https://github.com/onnx/tensorflow-onnx&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once dependencies are installed, from the tensorflow-onnx folder call:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python setup.py install&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python setup.py develop&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;tensorflow-onnx requires onnx-1.5 or better and will install/upgrade onnx if needed.&lt;/p&gt; &#xA;&lt;p&gt;To create a wheel for distribution:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python setup.py bdist_wheel&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;To get started with &lt;code&gt;tensorflow-onnx&lt;/code&gt;, run the &lt;code&gt;t2onnx.convert&lt;/code&gt; command, providing:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the path to your TensorFlow model (where the model is in &lt;code&gt;saved model&lt;/code&gt; format)&lt;/li&gt; &#xA; &lt;li&gt;a name for the ONNX output file:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;python -m tf2onnx.convert --saved-model tensorflow-model-path --output model.onnx&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The above command uses a default of &lt;code&gt;13&lt;/code&gt; for the ONNX opset. If you need a newer opset, or want to limit your model to use an older opset then you can provide the &lt;code&gt;--opset&lt;/code&gt; argument to the command. If you are unsure about which opset to use, refer to the &lt;a href=&#34;https://github.com/onnx/onnx/releases&#34;&gt;ONNX operator documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python -m tf2onnx.convert --saved-model tensorflow-model-path --opset 16 --output model.onnx&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If your TensorFlow model is in a format other than &lt;code&gt;saved model&lt;/code&gt;, then you need to provide the inputs and outputs of the model graph.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;code&gt;checkpoint&lt;/code&gt; format:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python -m tf2onnx.convert --checkpoint tensorflow-model-meta-file-path --output model.onnx --inputs input0:0,input1:0 --outputs output0:0&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For &lt;code&gt;graphdef&lt;/code&gt; format:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python -m tf2onnx.convert --graphdef tensorflow-model-graphdef-file --output model.onnx --inputs input0:0,input1:0 --outputs output0:0&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If your model is in &lt;code&gt;checkpoint&lt;/code&gt; or &lt;code&gt;graphdef&lt;/code&gt; format and you do not know the input and output nodes of the model, you can use the &lt;a href=&#34;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms&#34;&gt;summarize_graph&lt;/a&gt; TensorFlow utility. The &lt;code&gt;summarize_graph&lt;/code&gt; tool does need to be downloaded and built from source. If you have the option of going to your model provider and obtaining the model in &lt;code&gt;saved model&lt;/code&gt; format, then we recommend doing so.&lt;/p&gt; &#xA;&lt;p&gt;You find an end-to-end tutorial for ssd-mobilenet &lt;a href=&#34;https://raw.githubusercontent.com/onnx/tensorflow-onnx/main/tutorials/ConvertingSSDMobilenetToONNX.ipynb&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We recently added support for tflite. You convert &lt;code&gt;tflite&lt;/code&gt; models via command line, for example:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python -m tf2onnx.convert --opset 16 --tflite tflite--file --output model.onnx&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;CLI reference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m tf2onnx.convert&#xA;    --saved-model SOURCE_SAVED_MODEL_PATH |&#xA;    --checkpoint SOURCE_CHECKPOINT_METAFILE_PATH |&#xA;    --tflite TFLITE_MODEL_PATH |&#xA;    --tfjs TFJS_MODEL_PATH | &#xA;    --input | --graphdef SOURCE_GRAPHDEF_PB&#xA;    --output TARGET_ONNX_MODEL&#xA;    [--inputs GRAPH_INPUTS]&#xA;    [--outputs GRAPH_OUTPUS]&#xA;    [--inputs-as-nchw inputs_provided_as_nchw]&#xA;    [--opset OPSET]&#xA;    [--dequantize]&#xA;    [--tag TAG]&#xA;    [--signature_def SIGNATURE_DEF]&#xA;    [--concrete_function CONCRETE_FUNCTION]&#xA;    [--target TARGET]&#xA;    [--custom-ops list-of-custom-ops]&#xA;    [--load_op_libraries tensorflow_library_path]&#xA;    [--large_model]&#xA;    [--continue_on_error]&#xA;    [--verbose]&#xA;    [--output_frozen_graph]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Parameters&lt;/h3&gt; &#xA;&lt;h4&gt;--saved-model&lt;/h4&gt; &#xA;&lt;p&gt;TensorFlow model as saved_model. We expect the path to the saved_model directory.&lt;/p&gt; &#xA;&lt;h4&gt;--checkpoint&lt;/h4&gt; &#xA;&lt;p&gt;TensorFlow model as checkpoint. We expect the path to the .meta file.&lt;/p&gt; &#xA;&lt;h4&gt;--input or --graphdef&lt;/h4&gt; &#xA;&lt;p&gt;TensorFlow model as graphdef file.&lt;/p&gt; &#xA;&lt;h4&gt;--tfjs&lt;/h4&gt; &#xA;&lt;p&gt;Convert a tensorflow.js model by providing a path to the .tfjs file. Inputs/outputs do not need to be specified.&lt;/p&gt; &#xA;&lt;h4&gt;--tflite&lt;/h4&gt; &#xA;&lt;p&gt;Convert a tflite model by providing a path to the .tflite file. Inputs/outputs do not need to be specified.&lt;/p&gt; &#xA;&lt;h4&gt;--output&lt;/h4&gt; &#xA;&lt;p&gt;The target onnx file path.&lt;/p&gt; &#xA;&lt;h4&gt;--inputs, --outputs&lt;/h4&gt; &#xA;&lt;p&gt;TensorFlow model&#39;s input/output names, which can be found with &lt;a href=&#34;https://raw.githubusercontent.com/onnx/tensorflow-onnx/main/#summarize_graph&#34;&gt;summarize graph tool&lt;/a&gt;. Those names typically end with &lt;code&gt;:0&lt;/code&gt;, for example &lt;code&gt;--inputs input0:0,input1:0&lt;/code&gt;. Inputs and outputs are &lt;em&gt;&lt;strong&gt;not&lt;/strong&gt;&lt;/em&gt; needed for models in saved-model format. Some models specify placeholders with unknown ranks and dims which can not be mapped to onnx. In those cases one can add the shape after the input name inside &lt;code&gt;[]&lt;/code&gt;, for example &lt;code&gt;--inputs X:0[1,28,28,3]&lt;/code&gt;. Use -1 to indicate unknown dimensions.&lt;/p&gt; &#xA;&lt;h4&gt;--inputs-as-nchw&lt;/h4&gt; &#xA;&lt;p&gt;By default we preserve the image format of inputs (&lt;code&gt;nchw&lt;/code&gt; or &lt;code&gt;nhwc&lt;/code&gt;) as given in the TensorFlow model. If your hosts (for example windows) native format nchw and the model is written for nhwc, &lt;code&gt;--inputs-as-nchw&lt;/code&gt; tensorflow-onnx will transpose the input. Doing so is convenient for the application and the converter in many cases can optimize the transpose away. For example &lt;code&gt;--inputs input0:0,input1:0 --inputs-as-nchw input0:0&lt;/code&gt; assumes that images are passed into &lt;code&gt;input0:0&lt;/code&gt; as nchw while the TensorFlow model given uses nhwc.&lt;/p&gt; &#xA;&lt;h4&gt;--ignore_default, --use_default&lt;/h4&gt; &#xA;&lt;p&gt;ONNX requires default values for graph inputs to be constant, while Tensorflow&#39;s PlaceholderWithDefault op accepts computed defaults. To convert such models, pass a comma-separated list of node names to the ignore_default and/or use_default flags. PlaceholderWithDefault nodes with matching names will be replaced with Placeholder or Identity ops, respectively.&lt;/p&gt; &#xA;&lt;h4&gt;--opset&lt;/h4&gt; &#xA;&lt;p&gt;By default we use the opset 13 to generate the graph. By specifying &lt;code&gt;--opset&lt;/code&gt; the user can override the default to generate a graph with the desired opset. For example &lt;code&gt;--opset 16&lt;/code&gt; would create a onnx graph that uses only ops available in opset 16. Because older opsets have in most cases fewer ops, some models might not convert on a older opset.&lt;/p&gt; &#xA;&lt;h4&gt;--dequantize&lt;/h4&gt; &#xA;&lt;p&gt;(This is experimental, only supported for tflite)&lt;/p&gt; &#xA;&lt;p&gt;Produces a float32 model from a quantized tflite model. Detects ReLU and ReLU6 ops from quantization bounds.&lt;/p&gt; &#xA;&lt;h4&gt;--tag&lt;/h4&gt; &#xA;&lt;p&gt;Only valid with parameter &lt;code&gt;--saved_model&lt;/code&gt;. Specifies the tag in the saved_model to be used. Typical value is &#39;serve&#39;.&lt;/p&gt; &#xA;&lt;h4&gt;--signature_def&lt;/h4&gt; &#xA;&lt;p&gt;Only valid with parameter &lt;code&gt;--saved_model&lt;/code&gt;. Specifies which signature to use within the specified --tag value. Typical value is &#39;serving_default&#39;.&lt;/p&gt; &#xA;&lt;h4&gt;--concrete_function&lt;/h4&gt; &#xA;&lt;p&gt;(This is experimental, valid only for TF2.x models)&lt;/p&gt; &#xA;&lt;p&gt;Only valid with parameter &lt;code&gt;--saved_model&lt;/code&gt;. If a model contains a list of concrete functions, under the function name &lt;code&gt;__call__&lt;/code&gt; (as can be viewed using the command &lt;code&gt;saved_model_cli show --all&lt;/code&gt;), this parameter is a 0-based integer specifying which function in that list should be converted. This parameter takes priority over &lt;code&gt;--signature_def&lt;/code&gt;, which will be ignored.&lt;/p&gt; &#xA;&lt;h4&gt;--large_model&lt;/h4&gt; &#xA;&lt;p&gt;(Can be used only for TF2.x models)&lt;/p&gt; &#xA;&lt;p&gt;Only valid with parameter &lt;code&gt;--saved_model&lt;/code&gt;. When set, creates a zip file containing the ONNX protobuf model and large tensor values stored externally. This allows for converting models that exceed the 2 GB protobuf limit.&lt;/p&gt; &#xA;&lt;h4&gt;--output_frozen_graph&lt;/h4&gt; &#xA;&lt;p&gt;Saves the frozen and optimize tensorflow graph to file.&lt;/p&gt; &#xA;&lt;h4&gt;--custom-ops&lt;/h4&gt; &#xA;&lt;p&gt;If a model contains ops not recognized by onnx runtime, you can tag these ops with a custom op domain so that the runtime can still open the model. The format is a comma-separated map of tf op names to domains in the format OpName:domain. If only an op name is provided (no colon), the default domain of &lt;code&gt;ai.onnx.converters.tensorflow&lt;/code&gt; will be used.&lt;/p&gt; &#xA;&lt;h4&gt;--load_op_libraries&lt;/h4&gt; &#xA;&lt;p&gt;Load the comma-separated list of tensorflow plugin/op libraries before conversion.&lt;/p&gt; &#xA;&lt;h4&gt;--target&lt;/h4&gt; &#xA;&lt;p&gt;Some models require special handling to run on some runtimes. In particular, the model may use unsupported data types. Workarounds are activated with &lt;code&gt;--target TARGET&lt;/code&gt;. Currently supported values are listed on this &lt;a href=&#34;https://github.com/onnx/tensorflow-onnx/wiki/target&#34;&gt;wiki&lt;/a&gt;. If your model will be run on Windows ML, you should specify the appropriate target value.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a name=&#34;summarize_graph&#34;&gt;&lt;/a&gt;Tool to get Graph Inputs &amp;amp; Outputs&lt;/h3&gt; &#xA;&lt;p&gt;To find the inputs and outputs for the TensorFlow graph the model developer will know or you can consult TensorFlow&#39;s &lt;a href=&#34;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms&#34;&gt;summarize_graph&lt;/a&gt; tool, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;summarize_graph --in_graph=tests/models/fc-layers/frozen.pb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;There are 2 types of tests.&lt;/p&gt; &#xA;&lt;h3&gt;Unit test&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python setup.py test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Validate pre-trained TensorFlow models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python tests/run_pretrained_models.py&#xA;usage: run_pretrained_models.py [-h] [--cache CACHE] [--tests TESTS] [--backend BACKEND] [--verbose] [--debug] [--config yaml-config]&#xA;&#xA;optional arguments:&#xA;  -h, --help         show this help message and exit&#xA;  --cache CACHE      pre-trained models cache dir&#xA;  --tests TESTS      tests to run&#xA;  --backend BACKEND  backend to use&#xA;  --config           yaml config file&#xA;  --verbose          verbose output, option is additive&#xA;  --opset OPSET      target opset to use&#xA;  --perf csv-file    capture performance numbers for tensorflow and onnx runtime&#xA;  --debug            dump generated graph with shape info&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;run_pretrained_models.py&lt;/code&gt; will run the TensorFlow model, captures the TensorFlow output and runs the same test against the specified ONNX backend after converting the model.&lt;/p&gt; &#xA;&lt;p&gt;If the option &lt;code&gt;--perf csv-file&lt;/code&gt; is specified, we&#39;ll capture the timing for inference of tensorflow and onnx runtime and write the result into the given csv file.&lt;/p&gt; &#xA;&lt;p&gt;You call it for example with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python tests/run_pretrained_models.py --backend onnxruntime --config tests/run_pretrained_models.yaml --perf perf.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a name=&#34;save_pretrained_model&#34;&gt;&lt;/a&gt;Tool to save pre-trained model&lt;/h4&gt; &#xA;&lt;p&gt;We provide an &lt;a href=&#34;https://raw.githubusercontent.com/onnx/tensorflow-onnx/main/tools/save_pretrained_model.py&#34;&gt;utility&lt;/a&gt; to save pre-trained model along with its config. Put &lt;code&gt;save_pretrained_model(sess, outputs, feed_inputs, save_dir, model_name)&lt;/code&gt; in your last testing epoch and the pre-trained model and config will be saved under &lt;code&gt;save_dir/to_onnx&lt;/code&gt;. Please refer to the example in &lt;a href=&#34;https://raw.githubusercontent.com/onnx/tensorflow-onnx/main/tools/save_pretrained_model.py&#34;&gt;tools/save_pretrained_model.py&lt;/a&gt; for more information. Note the minimum required Tensorflow version is r1.6.&lt;/p&gt; &#xA;&lt;h2&gt;Python API Reference&lt;/h2&gt; &#xA;&lt;p&gt;With tf2onnx-1.8.4 we updated our API. Our old API still works - you find the documentation &lt;a href=&#34;https://github.com/onnx/tensorflow-onnx/raw/v1.8.3/README.md#python-api-reference&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;from_keras (tf-2.0 and newer)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;import tf2onnx&#xA;&#xA;model_proto, external_tensor_storage = tf2onnx.convert.from_keras(model,&#xA;                input_signature=None, opset=None, custom_ops=None,&#xA;                custom_op_handlers=None, custom_rewriter=None,&#xA;                inputs_as_nchw=None, outputs_as_nchw=None, extra_opset=None,&#xA;                shape_override=None, target=None, large_model=False, output_path=None)&#xA;&#xA;    Args:&#xA;        model: the tf.keras model we want to convert&#xA;        input_signature: a tf.TensorSpec or a numpy array defining the shape/dtype of the input&#xA;        opset: the opset to be used for the ONNX model, default is the latest&#xA;        custom_ops: if a model contains ops not recognized by onnx runtime,&#xA;            you can tag these ops with a custom op domain so that the&#xA;            runtime can still open the model. Type is a dictionary `{op name: domain}`.&#xA;        target: list of workarounds applied to help certain platforms&#xA;        custom_op_handlers: dictionary of custom ops handlers&#xA;        custom_rewriter: list of custom graph rewriters&#xA;        extra_opset: list of extra opset&#39;s, for example the opset&#39;s used by custom ops&#xA;        shape_override: dict with inputs that override the shapes given by tensorflow&#xA;        inputs_as_nchw: transpose inputs in list from nhwc to nchw&#xA;        outputs_as_nchw: transpose outputs in list from nhwc to nchw&#xA;        large_model: use the ONNX external tensor storage format&#xA;        output_path: save model to output_path&#xA;&#xA;    Returns:&#xA;        An ONNX model_proto and an external_tensor_storage dict.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/onnx/tensorflow-onnx/main/tutorials/keras-resnet50.ipynb&#34;&gt;tutorials/keras-resnet50.ipynb&lt;/a&gt; for an end to end example.&lt;/p&gt; &#xA;&lt;h3&gt;from_function (tf-2.0 and newer)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;import tf2onnx&#xA;&#xA;model_proto, external_tensor_storage = tf2onnx.convert.from_function(function,&#xA;                input_signature=None, opset=None, custom_ops=None,&#xA;                custom_op_handlers=None, custom_rewriter=None, inputs_as_nchw=None,&#xA;                outputs_as_nchw=None, extra_opset=None, shape_override=None,&#xA;                target=None, large_model=False, output_path=None)&#xA;&#xA;    Args:&#xA;        function: the tf.function we want to convert&#xA;        input_signature: a tf.TensorSpec or a numpy array defining the shape/dtype of the input&#xA;        opset: the opset to be used for the ONNX model, default is the latest&#xA;        custom_ops: if a model contains ops not recognized by onnx runtime,&#xA;            you can tag these ops with a custom op domain so that the&#xA;            runtime can still open the model. Type is a dictionary `{op name: domain}`.&#xA;        target: list of workarounds applied to help certain platforms&#xA;        custom_op_handlers: dictionary of custom ops handlers&#xA;        custom_rewriter: list of custom graph rewriters&#xA;        extra_opset: list of extra opset&#39;s, for example the opset&#39;s used by custom ops&#xA;        shape_override: dict with inputs that override the shapes given by tensorflow&#xA;        inputs_as_nchw: transpose inputs in list from nhwc to nchw&#xA;        outputs_as_nchw: transpose outputs in list from nhwc to nchw&#xA;        large_model: use the ONNX external tensor storage format&#xA;        output_path: save model to output_path&#xA;&#xA;    Returns:&#xA;        An ONNX model_proto and an external_tensor_storage dict.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;from_graph_def&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;import tf2onnx&#xA;&#xA;model_proto, external_tensor_storage = tf2onnx.convert.from_graph_def(graph_def,&#xA;                name=None, input_names=None, output_names=None, opset=None,&#xA;                custom_ops=None, custom_op_handlers=None, custom_rewriter=None, &#xA;                inputs_as_nchw=None, outputs_as_nchw=None, extra_opset=None,&#xA;                shape_override=None, target=None, large_model=False,&#xA;                output_path=None)&#xA;&#xA;    Args:&#xA;        graph_def: the graph_def we want to convert&#xA;        input_names: list of input names&#xA;        output_names: list of output names&#xA;        name: A name for the graph&#xA;        opset: the opset to be used for the ONNX model, default is the latest&#xA;        target: list of workarounds applied to help certain platforms&#xA;        custom_op_handlers: dictionary of custom ops handlers&#xA;        custom_rewriter: list of custom graph rewriters&#xA;        extra_opset: list of extra opset&#39;s, for example the opset&#39;s used by custom ops&#xA;        shape_override: dict with inputs that override the shapes given by tensorflow&#xA;        inputs_as_nchw: transpose inputs in list from nhwc to nchw&#xA;        outputs_as_nchw: transpose outputs in list from nhwc to nchw&#xA;        large_model: use the ONNX external tensor storage format&#xA;        output_path: save model to output_path&#xA;&#xA;    Returns:&#xA;        An ONNX model_proto and an external_tensor_storage dict.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;from_tflite&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;import tf2onnx&#xA;&#xA;model_proto, external_tensor_storage = tf2onnx.convert.from_tflite(tflite_path,&#xA;                input_names=None, output_names=None, opset=None, custom_ops=None, custom_op_handlers=None,&#xA;                custom_rewriter=None, inputs_as_nchw=None, outputs_as_nchw=None, extra_opset=None,&#xA;                shape_override=None, target=None, large_model=False, output_path=None):&#xA;&#xA;    Args:&#xA;        tflite_path: the tflite model file full path&#xA;        input_names: list of input names&#xA;        output_names: list of output names&#xA;        opset: the opset to be used for the ONNX model, default is the latest&#xA;        custom_ops: if a model contains ops not recognized by onnx runtime,&#xA;            you can tag these ops with a custom op domain so that the&#xA;            runtime can still open the model. Type is a dictionary `{op name: domain}`.&#xA;        custom_op_handlers: dictionary of custom ops handlers&#xA;        custom_rewriter: list of custom graph rewriters&#xA;        inputs_as_nchw: transpose inputs in list from nhwc to nchw&#xA;        outputs_as_nchw: transpose outputs in list from nhwc to nchw&#xA;        extra_opset: list of extra opset&#39;s, for example the opset&#39;s used by custom ops&#xA;        shape_override: dict with inputs that override the shapes given by tensorflow&#xA;        target: list of workarounds applied to help certain platforms&#xA;        large_model: use the ONNX external tensor storage format&#xA;        output_path: save model to output_path&#xA;&#xA;    Returns:&#xA;        An ONNX model_proto and an external_tensor_storage dict.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Creating custom op mappings from python&lt;/h3&gt; &#xA;&lt;p&gt;For complex custom ops that require graph rewrites or input / attribute rewrites using the python interface to insert a custom op will be the easiest way to accomplish the task. A dictionary of name-&amp;gt;custom_op_handler can be passed to tf2onnx.tfonnx.process_tf_graph. If the op name is found in the graph the handler will have access to all internal structures and can rewrite that is needed. For example &lt;a href=&#34;&#34;&gt;examples/custom_op_via_python.py&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import tensorflow as tf&#xA;import tf2onnx&#xA;from onnx import helper&#xA;&#xA;_TENSORFLOW_DOMAIN = &#34;ai.onnx.converters.tensorflow&#34;&#xA;&#xA;&#xA;def print_handler(ctx, node, name, args):&#xA;    # replace tf.Print() with Identity&#xA;    #   T output = Print(T input, data, @list(type) U, @string message, @int first_n, @int summarize)&#xA;    # becomes:&#xA;    #   T output = Identity(T Input)&#xA;    node.domain = _TENSORFLOW_DOMAIN&#xA;    del node.input[1:]&#xA;    return node&#xA;&#xA;&#xA;with tf.Session() as sess:&#xA;    x = tf.placeholder(tf.float32, [2, 3], name=&#34;input&#34;)&#xA;    x_ = tf.add(x, x)&#xA;    x_ = tf.Print(x, [x], &#34;hello&#34;)&#xA;    _ = tf.identity(x_, name=&#34;output&#34;)&#xA;    onnx_graph = tf2onnx.tfonnx.process_tf_graph(sess.graph,&#xA;                                                 custom_op_handlers={&#34;Print&#34;: (print_handler, [&#34;Identity&#34;, &#34;mode&#34;])},&#xA;                                                 extra_opset=[helper.make_opsetid(_TENSORFLOW_DOMAIN, 1)],&#xA;                                                 input_names=[&#34;input:0&#34;],&#xA;                                                 output_names=[&#34;output:0&#34;])&#xA;    model_proto = onnx_graph.make_model(&#34;test&#34;)&#xA;    with open(&#34;/tmp/model.onnx&#34;, &#34;wb&#34;) as f:&#xA;        f.write(model_proto.SerializeToString())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How tf2onnx works&lt;/h2&gt; &#xA;&lt;p&gt;The converter needs to take care of a few things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Convert the protobuf format. Since the format is similar this step is straight forward.&lt;/li&gt; &#xA; &lt;li&gt;TensorFlow types need to be mapped to their ONNX equivalent.&lt;/li&gt; &#xA; &lt;li&gt;For many ops TensorFlow passes parameters like shapes as inputs where ONNX wants to see them as attributes. Since we use a frozen graph, the converter will fetch the input as constant, converts it to an attribute and remove the original input.&lt;/li&gt; &#xA; &lt;li&gt;TensorFlow in many cases composes ops out of multiple simpler ops. The converter will need to identify the subgraph for such ops, slice the subgraph out and replace it with the ONNX equivalent. This can become fairly complex so we use a graph matching library for it. A good example of this is the tensorflow transpose op.&lt;/li&gt; &#xA; &lt;li&gt;TensorFlow&#39;s default data format is NHWC where ONNX requires NCHW. The converter will insert transpose ops to deal with this.&lt;/li&gt; &#xA; &lt;li&gt;There are some ops like relu6 that are not supported in ONNX but the converter can be composed out of other ONNX ops.&lt;/li&gt; &#xA; &lt;li&gt;ONNX backends are new and their implementations are not complete yet. For some ops the converter generate ops with deal with issues in existing backends.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Step 1 - start with a frozen graph&lt;/h3&gt; &#xA;&lt;p&gt;tf2onnx starts with a frozen graph. This is because of item 3 above.&lt;/p&gt; &#xA;&lt;h3&gt;Step 2 - 1:1 conversion of the protobuf from tensorflow to onnx&lt;/h3&gt; &#xA;&lt;p&gt;tf2onnx first does a simple conversion from the TensorFlow protobuf format to the ONNX protobuf format without looking at individual ops. We do this so we can use the ONNX graph as internal representation and write helper functions around it. The code that does the conversion is in tensorflow_to_onnx(). tensorflow_to_onnx() will return the ONNX graph and a dictionary with shape information from TensorFlow. The shape information is helpful in some cases when processing individual ops. The ONNX graph is wrapped in a Graph object and nodes in the graph are wrapped in a Node object to allow easier graph manipulations on the graph. All code that deals with nodes and graphs is in graph.py.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3 - rewrite subgraphs&lt;/h3&gt; &#xA;&lt;p&gt;In the next step we apply graph matching code on the graph to re-write subgraphs for ops like transpose and lstm. For an example looks at rewrite_transpose().&lt;/p&gt; &#xA;&lt;h3&gt;Step 4 - process individual ops&lt;/h3&gt; &#xA;&lt;p&gt;In the fourth step we look at individual ops that need attention. The dictionary _OPS_MAPPING will map tensorflow op types to a method that is used to process the op. The simplest case is direct_op() where the op can be taken as is. Whenever possible we try to group ops into common processing, for example all ops that require dealing with broadcasting are mapped to broadcast_op(). For an op that composes the tensorflow op from multiple onnx ops, see relu6_op().&lt;/p&gt; &#xA;&lt;h3&gt;Step 5 - optimize the functional ONNX graph&lt;/h3&gt; &#xA;&lt;p&gt;We than try to optimize the functional ONNX graph. For example we remove ops that are not needed, remove transposes as much as possible, de-dupe constants, fuse ops whenever possible, ...&lt;/p&gt; &#xA;&lt;h3&gt;Step 6 - final processing&lt;/h3&gt; &#xA;&lt;p&gt;Once all ops are converted and optimize, we need to do a topological sort since ONNX requires it. process_tf_graph() is the method that takes care of all above steps.&lt;/p&gt; &#xA;&lt;h2&gt;Extending tf2onnx&lt;/h2&gt; &#xA;&lt;p&gt;If you like to contribute and add new conversions to tf2onnx, the process is something like:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;See if the op fits into one of the existing mappings. If so adding it to _OPS_MAPPING is all that is needed.&lt;/li&gt; &#xA; &lt;li&gt;If the new op needs extra processing, start a new mapping function.&lt;/li&gt; &#xA; &lt;li&gt;If the tensorflow op is composed of multiple ops, consider using a graph re-write. While this might be a little harder initially, it works better for complex patterns.&lt;/li&gt; &#xA; &lt;li&gt;Add a unit test in tests/test_backend.py. The unit tests mostly create the tensorflow graph, run it and capture the output, than convert to onnx, run against a onnx backend and compare tensorflow and onnx results.&lt;/li&gt; &#xA; &lt;li&gt;If there are pre-trained models that use the new op, consider adding those to test/run_pretrained_models.py.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/tensorflow-onnx/main/LICENSE&#34;&gt;Apache License v2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>