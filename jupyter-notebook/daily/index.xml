<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-12T01:39:15Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xinyu1205/recognize-anything</title>
    <updated>2023-07-12T01:39:15Z</updated>
    <id>tag:github.com,2023-07-12:/xinyu1205/recognize-anything</id>
    <link href="https://github.com/xinyu1205/recognize-anything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for the Recognize Anything Model (RAM) and Tag2Text Model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;font size=&#34;8&#34;&gt; &lt;span&gt;üè∑&lt;/span&gt; Recognize Anything &amp;amp; Tag2Text &lt;/font&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-HuggingFace%20Space-cyan.svg?sanitize=true&#34; alt=&#34;Web Demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/mhd-medfa/recognize-anything/blob/main/recognize_anything_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Official PyTorch Implementation of &lt;a href=&#34;https://recognize-anything.github.io/&#34;&gt;Recognize Anything: A Strong Image Tagging Model &lt;/a&gt; and &lt;a href=&#34;https://tag2text.github.io/&#34;&gt;Tag2Text: Guiding Vision-Language Model via Image Tagging&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recognize Anything Model(RAM)&lt;/strong&gt; is an image tagging model, which can recognize any common category with high accuracy.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tag2Text&lt;/strong&gt; is a vision-language model guided by tagging, which can support caption, retrieval and tagging.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- Welcome to try our [RAM &amp; Tag2Text web Demo! ü§ó](https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text) --&gt; &#xA;&lt;p&gt;Both Tag2Text and RAM exihibit strong recognition ability. We have combined Tag2Text and RAM with localization models (Grounding-DINO and SAM) and developed a strong visual semantic analysis pipeline in the &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-SAM&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xinyu1205/recognize-anything/main/images/ram_grounded_sam.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üí°&lt;/span&gt; Highlight of RAM&lt;/h2&gt; &#xA;&lt;p&gt;RAM is a strong image tagging model, which can recognize any common category with high accuracy.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Strong and general.&lt;/strong&gt; RAM exhibits exceptional image tagging capabilities with powerful zero-shot generalization; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RAM showcases impressive zero-shot performance, significantly outperforming CLIP and BLIP.&lt;/li&gt; &#xA;   &lt;li&gt;RAM even surpasses the fully supervised manners (ML-Decoder).&lt;/li&gt; &#xA;   &lt;li&gt;RAM exhibits competitive performance with the Google tagging API.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reproducible and affordable.&lt;/strong&gt; RAM requires Low reproduction cost with open-source and annotation-free dataset;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible and versatile.&lt;/strong&gt; RAM offers remarkable flexibility, catering to various application scenarios.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;(Green color means fully supervised learning and Blue color means zero-shot performance.)&lt;/p&gt;&#xA;&lt;table class=&#34;tg&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-c3ow&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xinyu1205/recognize-anything/main/images/experiment_comparison.png&#34; align=&#34;center&#34; width=&#34;800&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt;  &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table class=&#34;tg&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-c3ow&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xinyu1205/recognize-anything/main/images/tagging_results.jpg&#34; align=&#34;center&#34; width=&#34;800&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;RAM significantly improves the tagging ability based on the Tag2text framework.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Accuracy.&lt;/strong&gt; RAM utilizes a &lt;strong&gt;data engine&lt;/strong&gt; to &lt;strong&gt;generate&lt;/strong&gt; additional annotations and &lt;strong&gt;clean&lt;/strong&gt; incorrect ones, &lt;strong&gt;higher accuracy&lt;/strong&gt; compared to Tag2Text.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scope.&lt;/strong&gt; RAM upgrades the number of fixed tags from 3,400+ to &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xinyu1205/recognize-anything/main/ram/data/ram_tag_list.txt&#34;&gt;6,400+&lt;/a&gt;&lt;/strong&gt; (synonymous reduction to 4,500+ different semantic tags), covering &lt;strong&gt;more valuable categories&lt;/strong&gt;. Moreover, RAM is equipped with &lt;strong&gt;open-set capability&lt;/strong&gt;, feasible to recognize tags not seen during training&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üåÖ&lt;/span&gt; Highlight of Tag2text&lt;/h2&gt; &#xA;&lt;p&gt;Tag2Text is an efficient and controllable vision-language model with tagging guidance.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tagging.&lt;/strong&gt; Tag2Text recognizes &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xinyu1205/recognize-anything/main/ram/data/tag_list.txt&#34;&gt;3,400+&lt;/a&gt;&lt;/strong&gt; commonly human-used categories without manual annotations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Captioning.&lt;/strong&gt; Tag2Text integrates &lt;strong&gt;tags information&lt;/strong&gt; into text generation as the &lt;strong&gt;guiding elements&lt;/strong&gt;, resulting in &lt;strong&gt;more controllable and comprehensive descriptions&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Retrieval.&lt;/strong&gt; Tag2Text provides &lt;strong&gt;tags&lt;/strong&gt; as &lt;strong&gt;additional visible alignment indicators&lt;/strong&gt; for image-text retrieval.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table class=&#34;tg&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-c3ow&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xinyu1205/recognize-anything/main/images/tag2text_framework.png&#34; align=&#34;center&#34; width=&#34;800&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt;  &#xA;&lt;!-- ## &lt;span&gt;‚ú®&lt;/span&gt; Highlight Projects with other Models&#xA;- [Tag2Text/RAM with Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything) is trong and general pipeline for visual semantic analysis, which can automatically **recognize**, detect, and segment for an image!&#xA;- [Ask-Anything](https://github.com/OpenGVLab/Ask-Anything) is a multifunctional video question answering tool. Tag2Text provides powerful tagging and captioning capabilities as a fundamental component.&#xA;- [Prompt-can-anything](https://github.com/positive666/Prompt-Can-Anything) is a gradio web library that integrates SOTA multimodal large models, including Tag2text as the core model for graphic understanding --&gt; &#xA;&lt;!-- &#xA;## &lt;span&gt;üî•&lt;/span&gt; News&#xA;&#xA;- **`2023/06/08`**: We release the [Recognize Anything Model (RAM) Tag2Text web demo ü§ó](https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text), checkpoints and inference code!&#xA;- **`2023/06/07`**: We release the [Recognize Anything Model (RAM)](https://recognize-anything.github.io/), a strong image tagging model!&#xA;- **`2023/06/05`**: Tag2Text is combined with [Prompt-can-anything](https://github.com/OpenGVLab/Ask-Anything).&#xA;- **`2023/05/20`**: Tag2Text is combined with [VideoChat](https://github.com/OpenGVLab/Ask-Anything).&#xA;- **`2023/04/20`**: We marry Tag2Text with with [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything).&#xA;- **`2023/04/10`**: Code and checkpoint is available Now!&#xA;- **`2023/03/14`**: [Tag2Text web demo ü§ó](https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text) is available on Hugging Face Space!  --&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚úç&lt;/span&gt; TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release Tag2Text demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release checkpoints.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release inference code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release RAM demo and checkpoints.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release training codes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release training datasets.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üß∞&lt;/span&gt; Checkpoints&lt;/h2&gt; &#xA;&lt;!-- insert a table --&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr style=&#34;text-align: right;&#34;&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Backbone&lt;/th&gt; &#xA;   &lt;th&gt;Data&lt;/th&gt; &#xA;   &lt;th&gt;Illustration&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;1&lt;/th&gt; &#xA;   &lt;td&gt;RAM-14M&lt;/td&gt; &#xA;   &lt;td&gt;Swin-Large&lt;/td&gt; &#xA;   &lt;td&gt;COCO, VG, SBU, CC-3M, CC-12M&lt;/td&gt; &#xA;   &lt;td&gt;Provide strong image tagging ability.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text/blob/main/ram_swin_large_14m.pth&#34;&gt;Download link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;2&lt;/th&gt; &#xA;   &lt;td&gt;Tag2Text-14M&lt;/td&gt; &#xA;   &lt;td&gt;Swin-Base&lt;/td&gt; &#xA;   &lt;td&gt;COCO, VG, SBU, CC-3M, CC-12M&lt;/td&gt; &#xA;   &lt;td&gt;Support comprehensive captioning and tagging.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text/blob/main/tag2text_swin_14m.pth&#34;&gt;Download link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;span&gt;üèÉ&lt;/span&gt; Model Inference&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Setting Up&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the dependencies::&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;/pre&gt;pip install -r requirements.txt&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Download RAM pretrained checkpoints.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Optional) To use RAM and Tag2Text in other projects, better to install recognize-anything as a package:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then the RAM and Tag2Text model can be imported in other projects:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ram.models import ram, tag2text&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;strong&gt;RAM Inference&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Get the English and Chinese outputs of the images: &lt;/p&gt;&#xA;&lt;pre&gt;&lt;/pre&gt; python inference_ram.py --image images/demo/demo1.jpg &#xA;&lt;br&gt; --pretrained pretrained/ram_swin_large_14m.pth &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;RAM Inference on Unseen Categories (Open-Set)&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Firstly, custom recognition categories in &lt;a href=&#34;https://raw.githubusercontent.com/xinyu1205/recognize-anything/main/ram/utils/openset_utils.py&#34;&gt;build_openset_label_embedding&lt;/a&gt;, then get the tags of the images: &lt;/p&gt;&#xA;&lt;pre&gt;&lt;/pre&gt; python inference_ram_openset.py --image images/openset_example.jpg &#xA;&lt;br&gt; --pretrained pretrained/ram_swin_large_14m.pth &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Tag2Text Inference&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Get the tagging and captioning results: &lt;/p&gt;&#xA;&lt;pre&gt;&lt;/pre&gt; python inference_tag2text.py --image images/demo/demo1.jpg &#xA;&lt;br&gt; --pretrained pretrained/tag2text_swin_14m.pth  Or get the tagging and sepcifed captioning results (optional): &#xA;&lt;pre&gt;&lt;/pre&gt;python inference_tag2text.py --image images/demo/demo1.jpg &#xA;&lt;br&gt; --pretrained pretrained/tag2text_swin_14m.pth &#xA;&lt;br&gt; --specified-tags &#34;cloud,sky&#34;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Batch Inference and Evaluation&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We release two datasets &lt;code&gt;OpenImages-common&lt;/code&gt; (214 seen classes) and &lt;code&gt;OpenImages-rare&lt;/code&gt; (200 unseen classes). Copy or sym-link test images of &lt;a href=&#34;https://storage.googleapis.com/openimages/web/download_v6.html&#34;&gt;OpenImages v6&lt;/a&gt; to &lt;code&gt;datasets/openimages_common_214/imgs/&lt;/code&gt; and &lt;code&gt;datasets/openimages_rare_200/imgs&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To evaluate RAM on &lt;code&gt;OpenImages-common&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python batch_inference.py \&#xA;  --model-type ram \&#xA;  --checkpoint pretrained/ram_swin_large_14m.pth \&#xA;  --dataset openimages_common_214 \&#xA;  --output-dir outputs/ram&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To evaluate RAM open-set capability on &lt;code&gt;OpenImages-rare&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python batch_inference.py \&#xA;  --model-type ram \&#xA;  --checkpoint pretrained/ram_swin_large_14m.pth \&#xA;  --open-set \&#xA;  --dataset openimages_rare_200 \&#xA;  --output-dir outputs/ram_openset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To evaluate Tag2Text on &lt;code&gt;OpenImages-common&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python batch_inference.py \&#xA;  --model-type tag2text \&#xA;  --checkpoint pretrained/tag2text_swin_14m.pth \&#xA;  --dataset openimages_common_214 \&#xA;  --output-dir outputs/tag2text&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;code&gt;batch_inference.py&lt;/code&gt; for more options. To get P/R in table 3 of our paper, pass &lt;code&gt;--threshold=0.86&lt;/code&gt; for RAM and &lt;code&gt;--threshold=0.68&lt;/code&gt; for Tag2Text.&lt;/p&gt; &#xA;&lt;p&gt;To batch inference custom images, you can set up you own datasets following the given two datasets.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üèå&lt;/span&gt; Model Training/Finetuning&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Tag2Text&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;At present, we can only open source &lt;a href=&#34;https://raw.githubusercontent.com/xinyu1205/recognize-anything/main/ram/models/tag2text.py#L141&#34;&gt;the forward function of Tag2Text&lt;/a&gt; as much as possible. To train/finetune Tag2Text on a custom dataset, you can refer to the complete training codebase of &lt;a href=&#34;https://github.com/salesforce/BLIP/tree/main&#34;&gt;BLIP&lt;/a&gt; and make the following modifications:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Replace the &#34;models/blip.py&#34; file with the current &#34;&lt;a href=&#34;https://raw.githubusercontent.com/xinyu1205/recognize-anything/main/ram/models/tag2text.py&#34;&gt;tag2text.py&lt;/a&gt;&#34; model file;&lt;/li&gt; &#xA; &lt;li&gt;Load additional tags based on the original dataloader.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;&lt;strong&gt;RAM&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The training code of RAM cannot be open-sourced temporarily as it is in the company&#39;s process.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚úí&lt;/span&gt; Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work to be useful for your research, please consider citing.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zhang2023recognize,&#xA;  title={Recognize Anything: A Strong Image Tagging Model},&#xA;  author={Zhang, Youcai and Huang, Xinyu and Ma, Jinyu and Li, Zhaoyang and Luo, Zhaochuan and Xie, Yanchun and Qin, Yuzhuo and Luo, Tong and Li, Yaqian and Liu, Shilong and others},&#xA;  journal={arXiv preprint arXiv:2306.03514},&#xA;  year={2023}&#xA;}&#xA;&#xA;@article{huang2023tag2text,&#xA;  title={Tag2Text: Guiding Vision-Language Model via Image Tagging},&#xA;  author={Huang, Xinyu and Zhang, Youcai and Ma, Jinyu and Tian, Weiwei and Feng, Rui and Zhang, Yuejie and Li, Yaqian and Guo, Yandong and Zhang, Lei},&#xA;  journal={arXiv preprint arXiv:2303.05657},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚ô•&lt;/span&gt; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This work is done with the help of the amazing code base of &lt;a href=&#34;https://github.com/salesforce/BLIP&#34;&gt;BLIP&lt;/a&gt;, thanks very much!&lt;/p&gt; &#xA;&lt;p&gt;We want to thank @Cheng Rui @Shilong Liu @Ren Tianhe for their help in &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;marrying RAM/Tag2Text with Grounded-SAM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also want to thank &lt;a href=&#34;https://github.com/OpenGVLab/Ask-Anything&#34;&gt;Ask-Anything&lt;/a&gt;, &lt;a href=&#34;https://github.com/positive666/Prompt-Can-Anything&#34;&gt;Prompt-can-anything&lt;/a&gt; for combining RAM/Tag2Text, which greatly expands the application boundaries of RAM/Tag2Text.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AI-Club-IIT-Madras/Summer_School_2023</title>
    <updated>2023-07-12T01:39:15Z</updated>
    <id>tag:github.com,2023-07-12:/AI-Club-IIT-Madras/Summer_School_2023</id>
    <link href="https://github.com/AI-Club-IIT-Madras/Summer_School_2023" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI Club IIT Madras Summer School 2023&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Club Summer School 2023&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains all the slides/notes used during the Summer School conducted at IIT Madras.&lt;/p&gt; &#xA;&lt;p&gt;All the session recordings are present in this YouTube playlist - &lt;a href=&#34;https://youtube.com/playlist?list=PLWkFppvOIj_RK8MBQEgipsDvahLiQJYQH&#34;&gt;Link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tasks&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Link&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Deadline&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Data Visualization Challenge&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI-Club-IIT-Madras/Summer_School_2023/main/Task_1/&#34;&gt;Task 1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;July 15th 2023, 11:59 PM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Sessions&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Session&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Date&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Topic&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Session Notes/Slides&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Recording&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;July 7th 2023&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intro to Python, Numpy, Pandas and Matplotlib&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI-Club-IIT-Madras/Summer_School_2023/main/Session_1/&#34;&gt;Session 1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=A2U0c21IsfA&#34;&gt;Recording&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;July 10th 2023&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Linear, Logistic and Polynomial Regression&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI-Club-IIT-Madras/Summer_School_2023/main/Session_2/&#34;&gt;Session 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.youtube.com/live/eEGmz0Tm1Ck?feature=share&#34;&gt;Recording&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;July 12th 2023&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;KNNs, SVMs, Naive Bayes Classifiers&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Session 3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Recording&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;July 14th 2023&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Decision Trees, Random Forest and Ensemble methods&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Session 4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Recording&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;July 17th 2023&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intro to Deep Learning, Neural Networks, Pytorch basics&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Session 5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Recording&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;July 19th 2023&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Add-ons to improve model performance: Data preprocessing, Feature Engineering, Regularization, Optimizers&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Session 6&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Recording&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;July 21st 2023&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;CNNs (Theory and Implementation)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Session 7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Recording&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;July 24th 2023&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Object Detection - Haar Cascade, HOGs, RCNN, YOLO&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Session 8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Recording&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;July 26th 2023&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intro to Reinforcement Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Session 9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Recording&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>jackievaleri/BioAutoMATED</title>
    <updated>2023-07-12T01:39:15Z</updated>
    <id>tag:github.com,2023-07-12:/jackievaleri/BioAutoMATED</id>
    <link href="https://github.com/jackievaleri/BioAutoMATED" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Automated machine learning for analyzing, interpreting, and designing biological sequences&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/33818756/229146095-6e892838-9a4d-4b24-94a0-c58509e6fe9a.png&#34; alt=&#34;Logo_V3-01&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;BioAutoMATED&lt;/h1&gt; &#xA;&lt;p&gt;an end-to-end automated machine learning tool for explanation and design of biological sequences (&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2405471223001515&#34;&gt;Valeri et al. 2023&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;The design choices underlying machine learning (ML) models present important barriers to entry for many biologists who aim to incorporate ML in their research. Automated machine learning (AutoML) algorithms can address many design challenges that come with applying ML to the life sciences. However, these algorithms are rarely used in systems biology studies because they typically do not explicitly handle biological sequences (e.g., nucleotide, amino acid, or glycan sequences) and cannot be easily compared with other AutoML algorithms. Here, we present BioAutoMATED, an AutoML platform for biological sequence analysis that integrates multiple AutoML methods into a unified framework. Users are automatically provided with relevant techniques for analyzing, interpreting, and designing biological sequences. BioAutoMATED predicts gene regulation, peptide-drug interactions, and glycan annotation with performance comparable to that of manually tuned models, revealing salient sequence characteristics. By automating sequence modeling, BioAutoMATED allows life scientists to more readily incorporate ML into their work.&lt;/p&gt; &#xA;&lt;h1&gt;Installation Instructions&lt;/h1&gt; &#xA;&lt;p&gt;Please find all installation instructions, for both GitHub and &lt;a href=&#34;https://hub.docker.com/r/jackievaleri/bioautomated&#34;&gt;DockerHub&lt;/a&gt; installations, in the provided &lt;a href=&#34;https://github.com/jackievaleri/BioAutoMATED/raw/main/BioAutoMATED_Installation_Guide.md&#34;&gt;Installation Guide&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
</feed>