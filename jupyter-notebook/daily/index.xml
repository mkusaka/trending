<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-30T01:31:28Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>weaviate/recipes</title>
    <updated>2024-03-30T01:31:28Z</updated>
    <id>tag:github.com,2024-03-30:/weaviate/recipes</id>
    <link href="https://github.com/weaviate/recipes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository shares end-to-end notebooks on how to use various features and integrations with Weaviate at the core!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to the Weaviate Recipes repository!&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/weaviate/recipes/main/Weaviate.png&#34; alt=&#34;Weaviate logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo covers end-to-end examples on the various features and integrations with &lt;a href=&#34;https://raw.githubusercontent.com/weaviate/recipes/main/www.weaviate.io&#34;&gt;Weaviate&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Search üîé&lt;/h2&gt; &#xA;&lt;p&gt;This folder shows you how to search. It covers vector search, hybrid search, generative search, multi-modal search, and how to search in Japanese.&lt;/p&gt; &#xA;&lt;h2&gt;Integrations ü§ù&lt;/h2&gt; &#xA;&lt;p&gt;Learn about the various &lt;a href=&#34;https://github.com/weaviate/recipes/tree/main/integrations&#34;&gt;Integrations&lt;/a&gt; with Weaviate! A few demos we have are:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/weaviate/recipes/tree/main/integrations/dspy&#34;&gt;DSPy&lt;/a&gt; - Getting started with DSPy, Query to Blog Post demo, and more&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/weaviate/recipes/tree/main/integrations/llamaindex&#34;&gt;LlamaIndex&lt;/a&gt; - Indexes, Query Engines, Advanced RAG, and more&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/weaviate/recipes/tree/main/integrations/nomic/vector-space-visualization&#34;&gt;Nomic&lt;/a&gt; - Visualize your embeddings&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/weaviate/recipes/tree/main/integrations/ragas&#34;&gt;Ragas&lt;/a&gt; - Evaluate your RAG application&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Reranking ‚òùÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;Learn how to implement pointwise and listwise reranking with Cohere and OpenAI.&lt;/p&gt; &#xA;&lt;h2&gt;Multi-Tenancy üë•&lt;/h2&gt; &#xA;&lt;p&gt;Multi-tenancy is a key feature in Weaviate, allowing for the efficient and secure management of data across multiple users or tenants.&lt;/p&gt; &#xA;&lt;h2&gt;Product Quantization üóúÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;Enabling Product Quantization (PQ) Vector Compression for your Class.&lt;/p&gt; &#xA;&lt;h2&gt;Classification ‚òØÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;Classification using KNN and zero-shot learning.&lt;/p&gt; &#xA;&lt;h2&gt;Spark Connector üí•&lt;/h2&gt; &#xA;&lt;p&gt;Learn how to load data from a Spark Dataframe and feed it into Weaviate.&lt;/p&gt; &#xA;&lt;h2&gt;Feedback ‚ùì&lt;/h2&gt; &#xA;&lt;p&gt;Please note this is an ongoing project, and updates will be made frequently. If you have a feature you would like to see, please drop it in the &lt;a href=&#34;https://forum.weaviate.io/c/general/4&#34;&gt;Weaviate Forum&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenGenerativeAI/llm-colosseum</title>
    <updated>2024-03-30T01:31:28Z</updated>
    <id>tag:github.com,2024-03-30:/OpenGenerativeAI/llm-colosseum</id>
    <link href="https://github.com/OpenGenerativeAI/llm-colosseum" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Benchmark LLMs by fighting in Street Fighter 3! The new way to evaluate the quality of an LLM&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Evaluate LLMs in real time with Street Fighter III&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenGenerativeAI/llm-colosseum/main/logo.png&#34; alt=&#34;colosseum-logo&#34; width=&#34;30%&#34; style=&#34;border-radius: 50%; padding-bottom: 20px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Make LLM fight each other in real time in Street Fighter III.&lt;/p&gt; &#xA;&lt;p&gt;Which LLM will be the best fighter ?&lt;/p&gt; &#xA;&lt;h2&gt;Our criterias üî•&lt;/h2&gt; &#xA;&lt;p&gt;They need to be:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;: It is a real time game, fast decisions are key&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Smart&lt;/strong&gt;: A good fighter thinks 50 moves ahead&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Out of the box thinking&lt;/strong&gt;: Outsmart your opponent with unexpected moves&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adaptable&lt;/strong&gt;: Learn from your mistakes and adapt your strategy&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resilient&lt;/strong&gt;: Keep your RPS high for an entire game&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Let the fight begin ü•∑&lt;/h2&gt; &#xA;&lt;h3&gt;1 VS 1: Mistral 7B vs Mistral 7B&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenGenerativeAI/llm-colosseum/assets/19614572/79b58e26-7902-4687-af5d-0e1e845ecaf8&#34;&gt;https://github.com/OpenGenerativeAI/llm-colosseum/assets/19614572/79b58e26-7902-4687-af5d-0e1e845ecaf8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1 VS 1 X 6 : Mistral 7B vs Mistral 7B&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenGenerativeAI/llm-colosseum/assets/19614572/5d3d386b-150a-48a5-8f68-7e2954ec18db&#34;&gt;https://github.com/OpenGenerativeAI/llm-colosseum/assets/19614572/5d3d386b-150a-48a5-8f68-7e2954ec18db&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;A new kind of benchmark ?&lt;/h2&gt; &#xA;&lt;p&gt;Street Fighter III assesses the ability of LLMs to understand their environment and take actions based on a specific context. As opposed to RL models, which blindly take actions based on the reward function, LLMs are fully aware of the context and act accordingly.&lt;/p&gt; &#xA;&lt;h1&gt;Results&lt;/h1&gt; &#xA;&lt;p&gt;Our experimentations (342 fights so far) led to the following leader board. Each LLM has an ELO score based on its results&lt;/p&gt; &#xA;&lt;h2&gt;Ranking&lt;/h2&gt; &#xA;&lt;h3&gt;ELO ranking&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Rating&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ü•áopenai:gpt-3.5-turbo-0125&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1776.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ü•àmistral:mistral-small-latest&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1586.16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ü•âopenai:gpt-4-1106-preview&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1584.78&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openai:gpt-4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1517.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openai:gpt-4-turbo-preview&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1509.28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openai:gpt-4-0125-preview&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1438.92&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mistral:mistral-medium-latest&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1356.19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mistral:mistral-large-latest&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1231.36&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Win rate matrix&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenGenerativeAI/llm-colosseum/main/notebooks/win_rate_matrix.png&#34; alt=&#34;Win rate matrix&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Explanation&lt;/h1&gt; &#xA;&lt;p&gt;Each player is controlled by an LLM. We send to the LLM a text description of the screen. The LLM decide on the next moves its character will make. The next moves depends on its previous moves, the moves of its opponents, its power and health bars.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Agent based&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Multithreading&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Real time&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/OpenGenerativeAI/llm-colosseum/assets/78322686/3a212601-f54c-490d-aeb9-6f7c2401ebe6&#34; alt=&#34;fight3 drawio&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow instructions in &lt;a href=&#34;https://docs.diambra.ai/#installation&#34;&gt;https://docs.diambra.ai/#installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Download the ROM and put it in &lt;code&gt;~/.diambra/roms&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;(Optional) Create and activate a &lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34;&gt;new python venv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies with &lt;code&gt;make install&lt;/code&gt; or &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;.env&lt;/code&gt; file and fill it with the content like in the &lt;code&gt;.env.example&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;li&gt;Run with &lt;code&gt;make run&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Test mode&lt;/h2&gt; &#xA;&lt;p&gt;To disable the LLM calls, set &lt;code&gt;DISABLE_LLM&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file. It will choose the actions randomly.&lt;/p&gt; &#xA;&lt;h2&gt;Logging&lt;/h2&gt; &#xA;&lt;p&gt;Change the logging level in the &lt;code&gt;script.py&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;Local model&lt;/h2&gt; &#xA;&lt;p&gt;You can run the arena with local models using &lt;a href=&#34;https://ollama.com/&#34;&gt;Ollama&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure you have ollama installed, running, and with a model downloaded (run &lt;code&gt;ollama serve mistral&lt;/code&gt; in the terminal for example)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;make local&lt;/code&gt; to start the fight.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;By default, it runs mistral against mistral. To use other models, you need to change the parameter model in &lt;code&gt;ollama.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from eval.game import Game, Player1, Player2&#xA;&#xA;def main():&#xA;    game = Game(&#xA;        render=True,&#xA;        save_game=True,&#xA;        player_1=Player1(&#xA;            nickname=&#34;Baby&#34;,&#xA;            model=&#34;ollama:mistral&#34;, #&amp;nbsp;change this&#xA;        ),&#xA;        player_2=Player2(&#xA;            nickname=&#34;Daddy&#34;,&#xA;            model=&#34;ollama:mistral&#34;, # change this&#xA;        ),&#xA;    )&#xA;    game.run()&#xA;    return 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The convention we use is &lt;code&gt;model_provider:model_name&lt;/code&gt;. If you want to use another local model than Mistral, you can do &lt;code&gt;ollama:some_other_model&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to make my own LLM model play? Can I improve the prompts?&lt;/h2&gt; &#xA;&lt;p&gt;The LLM is called in &lt;code&gt;Robot.call_llm()&lt;/code&gt; method of the &lt;code&gt;agent/robot.py&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    def call_llm(&#xA;        self,&#xA;        temperature: float = 0.7,&#xA;        max_tokens: int = 50,&#xA;        top_p: float = 1.0,&#xA;    ) -&amp;gt; str:&#xA;        &#34;&#34;&#34;&#xA;        Make an API call to the language model.&#xA;&#xA;        Edit this method to change the behavior of the robot!&#xA;        &#34;&#34;&#34;&#xA;        #&amp;nbsp;self.model is a slug like mistral:mistral-small-latest or ollama:mistral&#xA;        provider_name, model_name = get_provider_and_model(self.model)&#xA;        client = get_sync_client(provider_name) #&amp;nbsp;OpenAI client&#xA;&#xA;        # Generate the prompts&#xA;        move_list = &#34;- &#34; + &#34;\n - &#34;.join([move for move in META_INSTRUCTIONS])&#xA;        system_prompt = f&#34;&#34;&#34;You are the best and most aggressive Street Fighter III 3rd strike player in the world.&#xA;Your character is {self.character}. Your goal is to beat the other opponent. You respond with a bullet point list of moves.&#xA;{self.context_prompt()}&#xA;The moves you can use are:&#xA;{move_list}&#xA;----&#xA;Reply with a bullet point list of moves. The format should be: `- &amp;lt;name of the move&amp;gt;` separated by a new line.&#xA;Example if the opponent is close:&#xA;- Move closer&#xA;- Medium Punch&#xA;&#xA;Example if the opponent is far:&#xA;- Fireball&#xA;- Move closer&#34;&#34;&#34;&#xA;&#xA;        #&amp;nbsp;Call the LLM&#xA;        completion = client.chat.completions.create(&#xA;            model=model_name,&#xA;            messages=[&#xA;                {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: system_prompt},&#xA;                {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Your next moves are:&#34;},&#xA;            ],&#xA;            temperature=temperature,&#xA;            max_tokens=max_tokens,&#xA;            top_p=top_p,&#xA;        )&#xA;&#xA;        #&amp;nbsp;Return the string to be parsed with regex&#xA;        llm_response = completion.choices[0].message.content.strip()&#xA;        return llm_response&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use another model or other prompts, make a call to another client in this function, change the system prompt, or make any fancy stuff.&lt;/p&gt; &#xA;&lt;h3&gt;Submit your model&lt;/h3&gt; &#xA;&lt;p&gt;Create a new class herited from &lt;code&gt;Robot&lt;/code&gt; that has the changes you want to make and open a PR.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ll do our best to add it to the ranking!&lt;/p&gt; &#xA;&lt;h1&gt;Credits&lt;/h1&gt; &#xA;&lt;p&gt;Made with ‚ù§Ô∏è by the OpenGenerativeAI team from &lt;a href=&#34;https://phospho.ai&#34;&gt;phospho&lt;/a&gt; (@oulianov @Pierre-LouisBJT @Platinn) and &lt;a href=&#34;https://www.quivr.app&#34;&gt;Quivr&lt;/a&gt; (@StanGirard) during Mistral Hackathon 2024 in San Francisco&lt;/p&gt;</summary>
  </entry>
</feed>