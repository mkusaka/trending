<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-01T01:37:35Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ChenyangQiQi/FateZero</title>
    <updated>2023-04-01T01:37:35Z</updated>
    <id>tag:github.com,2023-04-01:/ChenyangQiQi/FateZero</id>
    <link href="https://github.com/ChenyangQiQi/FateZero" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Pytorch Implementation for &#34;FateZero: Fusing Attentions for Zero-shot Text-based Video Editing&#34;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;&lt;font color=&#34;red&#34;&gt; FateZero &lt;/font&gt;: Fusing Attentions for Zero-shot Text-based Video Editing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chenyangqiqi.github.io/&#34;&gt;Chenyang Qi&lt;/a&gt;, &lt;a href=&#34;http://vinthony.github.io/&#34;&gt;Xiaodong Cun&lt;/a&gt;, &lt;a href=&#34;https://yzhang2016.github.io&#34;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&#34;https://chenyanglei.github.io/&#34;&gt;Chenyang Lei&lt;/a&gt;, &lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=4oXBp9UAAAAJ&#34;&gt;Ying Shan&lt;/a&gt;, and &lt;a href=&#34;https://cqf.io&#34;&gt;Qifeng Chen&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.09535&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ArXiv-2303.09535-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fate-zero-edit.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/ChenyangQiQi/FateZero/blob/main/colab_fatezero.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/chenyangqi/FateZero&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ChenyangQiQi/FateZero&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ChenyangQiQi/FateZero?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- ![fatezero_demo](./docs/teaser.png) --&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/17_car_posche_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/3_sunflower_vangogh_conat_result.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;silver jeep ‚ûú posche car&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;+ Van Gogh style&#34;&lt;/td&gt; &#xA;   &lt;!-- &lt;td width=25% style=&#34;text-align:center;&#34;&gt;&#34;Wonder Woman, wearing a cowboy hat, is skiing&#34;&lt;/td&gt;&#xA;  &lt;td width=25% style=&#34;text-align:center;&#34;&gt;&#34;A man, wearing pink clothes, is skiing at sunset&#34;&lt;/td&gt; --&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üéè Abstract&lt;/h2&gt; &#xA;&lt;p&gt;&lt;b&gt;TL; DR: &lt;font color=&#34;red&#34;&gt;FateZero&lt;/font&gt; is the first zero-shot framework for text-driven video editing via pretrained diffusion models without training.&lt;/b&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;CLICK for the full abstract&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;The diffusion-based generative models have achieved remarkable success in text-based image generation. However, since it contains enormous randomness in generation progress, it is still challenging to apply such models for real-world visual content editing, especially in videos. In this paper, we propose &lt;font color=&#34;red&#34;&gt;FateZero&lt;/font&gt;, a zero-shot text-based editing method on real-world videos without per-prompt training or use-specific mask. To edit videos consistently, we propose several techniques based on the pre-trained models. Firstly, in contrast to the straightforward DDIM inversion technique, our approach captures intermediate attention maps during inversion, which effectively retain both structural and motion information. These maps are directly fused in the editing process rather than generated during denoising. To further minimize semantic leakage of the source video, we then fuse self-attentions with a blending mask obtained by cross-attention features from the source prompt. Furthermore, we have implemented a reform of the self-attention mechanism in denoising UNet by introducing spatial-temporal attention to ensure frame consistency. Yet succinct, our method is the first one to show the ability of zero-shot text-driven video style and local attribute editing from the trained text-to-image model. We also have a better zero-shot shape-aware editing ability based on the text-to-video model. Extensive experiments demonstrate our superior temporal consistency and editing capability than previous works.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;ü™ß Changelog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2023.03.31 Refine hugging face demo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- - 2023.03.27 Excited to Release [`Hugging face demo`](https://huggingface.co/spaces/chenyangqi/FateZero)! (refinement is in progress) Enjoy the fun of zero-shot video editing freely!&#xA;- 2023.03.27 Release [`attribute editing config`](config/attribute) and &#xA;  [`data`](https://github.com/ChenyangQiQi/FateZero/releases/download/v0.0.1/attribute.zip) used in the paper. --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2023.03.27 Excited to Release &lt;a href=&#34;https://huggingface.co/spaces/chenyangqi/FateZero&#34;&gt;&lt;code&gt;Hugging face demo&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/config/attribute&#34;&gt;&lt;code&gt;attribute editing config&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/ChenyangQiQi/FateZero/releases/download/v0.0.1/attribute.zip&#34;&gt;&lt;code&gt;data&lt;/code&gt;&lt;/a&gt; used in the paper. Enjoy the fun of zero-shot video editing freely!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- - 2023.03.27 Release [`attribute editing config`](config/attribute) and  --&gt; &#xA;&lt;!-- [`data`](https://github.com/ChenyangQiQi/FateZero/releases/download/v0.0.1/attribute.zip) used in the paper. --&gt; &#xA;&lt;!-- - 2023.03.22 Upload a `colab notebook` [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChenyangQiQi/FateZero/blob/main/colab_fatezero.ipynb).&#xA;- 2023.03.22 Release [`style editing config`](config/style) and &#xA;  &lt;!--[`data`](https://hkustconnect-my.sharepoint.com/:u:/g/personal/cqiaa_connect_ust_hk/EaTqRAuW0eJLj0z_JJrURkcBZCC3Zvgsdo6zsXHhpyHhHQ?e=FzuiNG) --&gt; &#xA;&lt;!-- [`data`](https://github.com/ChenyangQiQi/FateZero/releases/download/v0.0.1/style.zip)&#xA;  used in the paper. --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;2023.03.22 Upload &lt;a href=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/config/style&#34;&gt;&lt;code&gt;style editing config&lt;/code&gt;&lt;/a&gt; and &#xA;   &lt;!--[`data`](https://hkustconnect-my.sharepoint.com/:u:/g/personal/cqiaa_connect_ust_hk/EaTqRAuW0eJLj0z_JJrURkcBZCC3Zvgsdo6zsXHhpyHhHQ?e=FzuiNG) --&gt; &lt;a href=&#34;https://github.com/ChenyangQiQi/FateZero/releases/download/v0.0.1/style.zip&#34;&gt;&lt;code&gt;data&lt;/code&gt;&lt;/a&gt; used in the paper and a &lt;code&gt;colab notebook&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/ChenyangQiQi/FateZero/blob/main/colab_fatezero.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2023.03.21 &lt;a href=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/EditingGuidance.md&#34;&gt;Editing guidance&lt;/a&gt; is provided to help users to edit in-the-wild video. Update the &lt;code&gt;codebase and configuration&lt;/code&gt;. Now, it can run with lower resources (16G GPU and less than 16G CPU RAM) with &lt;a href=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/config/low_resource_teaser&#34;&gt;new configuration&lt;/a&gt; in &lt;code&gt;config/low_resource_teaser&lt;/code&gt;. Welcome to play and give feedback!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- A new option store all the attentions in hard disk, which require less ram. --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2023.03.17 Release Code and Paper!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöß Todo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release the edit config and data for all style and attribute results&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Memory and runtime profiling and Editing guidance documents&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Colab and hugging-face&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Tune-a-video optimization and shape editing configs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release more application&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üõ° Setup Environment&lt;/h2&gt; &#xA;&lt;p&gt;Our method is tested using cuda11, fp16 of accelerator and xformers on a single A100 or 3090.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n fatezero38 python=3.8&#xA;conda activate fatezero38&#xA;&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;xformers&lt;/code&gt; is recommended for A100 GPU to save memory and running time.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Click for xformers installation &lt;/summary&gt; &#xA; &lt;p&gt;We find its installation not stable. You may try the following wheel:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/ShivamShrirao/xformers-wheels/releases/download/4c06c79/xformers-0.0.15.dev0+4c06c79.d20221201-cp38-cp38-linux_x86_64.whl&#xA;pip install xformers-0.0.15.dev0+4c06c79.d20221201-cp38-cp38-linux_x86_64.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Validate the installation by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python test_install.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our environment is similar to Tune-A-video (&lt;a href=&#34;https://github.com/showlab/Tune-A-Video&#34;&gt;official&lt;/a&gt;, &lt;a href=&#34;https://github.com/bryandlee/Tune-A-Video&#34;&gt;unofficial&lt;/a&gt;) and &lt;a href=&#34;https://github.com/google/prompt-to-prompt/&#34;&gt;prompt-to-prompt&lt;/a&gt;. You may check them for more details.&lt;/p&gt; &#xA;&lt;h2&gt;‚öîÔ∏è FateZero Editing&lt;/h2&gt; &#xA;&lt;h4&gt;Style and Attribute Editing in Teaser&lt;/h4&gt; &#xA;&lt;p&gt;Download the &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;stable diffusion v1-4&lt;/a&gt; (or other interesting image diffusion model) and put it to &lt;code&gt;./ckpt/stable-diffusion-v1-4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Click for the bash command: &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;mkdir ./ckpt&#xA;# download from huggingface face, takes 20G space&#xA;git lfs install&#xA;git clone https://huggingface.co/CompVis/stable-diffusion-v1-4&#xA;cd ./ckpt&#xA;ln -s ../stable-diffusion-v1-4 .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Then, you could reproduce style and shape editing results in our teaser by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch test_fatezero.py --config config/teaser/jeep_watercolor.yaml&#xA;# or CUDA_VISIBLE_DEVICES=0 python test_fatezero.py --config config/teaser/jeep_watercolor.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;The result is saved at `./result` . (Click for directory structure) &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;result&#xA;‚îú‚îÄ‚îÄ teaser&#xA;‚îÇ   ‚îú‚îÄ‚îÄ jeep_posche&#xA;‚îÇ   ‚îú‚îÄ‚îÄ jeep_watercolor&#xA;‚îÇ           ‚îú‚îÄ‚îÄ cross-attention  # visualization of cross-attention during inversion&#xA;‚îÇ           ‚îú‚îÄ‚îÄ sample           # result&#xA;‚îÇ           ‚îú‚îÄ‚îÄ train_samples    # the input video&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Editing 8 frames on an Nvidia 3090, use &lt;code&gt;100G CPU memory, 12G GPU memory&lt;/code&gt; for editing. We also provide some &lt;a href=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/config/low_resource_teaser&#34;&gt;&lt;code&gt;low-cost setting&lt;/code&gt;&lt;/a&gt; of style editing by different hyper-parameters on a 16GB GPU. You may try these low-cost settings on colab. &lt;a href=&#34;https://colab.research.google.com/github/ChenyangQiQi/FateZero/blob/main/colab_fatezero.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;More speed and hardware benchmarks are &lt;a href=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/EditingGuidance.md#ddim-hyperparameters&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Shape and large motion editing with Tune-A-Video&lt;/h4&gt; &#xA;&lt;p&gt;Besides style and attribution editing above, we also provide a &lt;code&gt;Tune-A-Video&lt;/code&gt; &lt;a href=&#34;https://hkustconnect-my.sharepoint.com/:f:/g/personal/cqiaa_connect_ust_hk/EviSTWoAOs1EmHtqZruq50kBZu1E8gxDknCPigSvsS96uQ?e=492khj&#34;&gt;checkpoint&lt;/a&gt;. You may download and move it to &lt;code&gt;./ckpt/jeep_tuned_200/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;!-- We provide the [Tune-a-Video](https://drive.google.com/file/d/166eNbabM6TeJVy7hxol2gL1kUGKHi3Do/view?usp=share_link), you could download the data, unzip and put it to `data`. : --&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;The directory structure should be like this: (Click for directory structure) &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;ckpt&#xA;‚îú‚îÄ‚îÄ stable-diffusion-v1-4&#xA;‚îú‚îÄ‚îÄ jeep_tuned_200&#xA;...&#xA;data&#xA;‚îú‚îÄ‚îÄ car-turn&#xA;‚îÇ   ‚îú‚îÄ‚îÄ 00000000.png&#xA;‚îÇ   ‚îú‚îÄ‚îÄ 00000001.png&#xA;‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;video_diffusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;You could reproduce the shape editing result in our teaser by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch test_fatezero.py --config config/teaser/jeep_posche.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Reproduce other results in the paper (in progress)&lt;/h3&gt; &#xA;&lt;!-- Download the data of [style editing](https://hkustconnect-my.sharepoint.com/:u:/g/personal/cqiaa_connect_ust_hk/EaTqRAuW0eJLj0z_JJrURkcBZCC3Zvgsdo6zsXHhpyHhHQ?e=FzuiNG) and [attribute editing](https://hkustconnect-my.sharepoint.com/:u:/g/personal/cqiaa_connect_ust_hk/Ee7J2IzZuaVGkefh-ZRp1GwB7RCUYU7MVJCKqeNWmOIpfg?e=dcOwb7)&#xA;--&gt; &#xA;&lt;p&gt;Download the data of style editing and attribute editing from &lt;a href=&#34;https://hkustconnect-my.sharepoint.com/:f:/g/personal/cqiaa_connect_ust_hk/EkIeHj3CQiBNhm6iEEhJQZwBEBJNCGt3FsANmyqeAYbuXQ?e=FxYtJk&#34;&gt;onedrive&lt;/a&gt; or from Github &lt;a href=&#34;https://github.com/ChenyangQiQi/FateZero/releases/tag/v0.0.1&#34;&gt;Release&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Click for wget bash command: &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;wget https://github.com/ChenyangQiQi/FateZero/releases/download/v0.0.1/attribute.zip&#xA;wget https://github.com/ChenyangQiQi/FateZero/releases/download/v0.0.1/style.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Unzip and Place it in &lt;a href=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/data&#34;&gt;&#39;./data&#39;&lt;/a&gt;. Then use the command in &lt;a href=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/config/style&#34;&gt;&#39;config/style&#39;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/config/attribute&#34;&gt;&#39;config/attribute&#39;&lt;/a&gt; to get the results.&lt;/p&gt; &#xA;&lt;p&gt;The config of our tune-a-video ckpts will be updated later.&lt;/p&gt; &#xA;&lt;h2&gt;Tuning guidance to edit YOUR video&lt;/h2&gt; &#xA;&lt;p&gt;We provided a tuning guidance to edit in-the-wild video at &lt;a href=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/EditingGuidance.md&#34;&gt;here&lt;/a&gt;. The work is still in progress. Welcome to give your feedback in issues.&lt;/p&gt; &#xA;&lt;h2&gt;Style Editing Results with Stable Diffusion&lt;/h2&gt; &#xA;&lt;p&gt;We show the difference between the source prompt and the target prompt in the box below each video.&lt;/p&gt; &#xA;&lt;p&gt;Note mp4 and gif files in this GitHub page are compressed. Please check our &lt;a href=&#34;https://fate-zero-edit.github.io/&#34;&gt;Project Page&lt;/a&gt; for mp4 files of original video editing results.&lt;/p&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/style/1_surf_ukiyo_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/style/2_car_watercolor_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/style/6_lily_monet_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;!-- &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/man-skiing/wonder-woman.gif&#34;&gt;&lt;/td&gt;              &#xA;  &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/man-skiing/pink-sunset.gif&#34;&gt;&lt;/td&gt; --&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;+ Ukiyo-e style&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;+ watercolor painting&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;+ Monet style&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/style/4_rabit_pokemon_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/style/5_train_shikai_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/style/7_swan_carton_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;+ Pok√©mon cartoon style&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;+ Makoto Shinkai style&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;+ cartoon style&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Attribute Editing Results with Stable Diffusion&lt;/h2&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/15_rabbit_eat_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/15_rabbit_eat_02_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/15_rabbit_eat_04_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;rabbit, strawberry ‚ûú white rabbit, flower&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;rabbit, strawberry ‚ûú squirrel, carrot&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;rabbit, strawberry ‚ûú white rabbit, leaves&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/16_sq_eat_04_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/16_sq_eat_02_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/16_sq_eat_03_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;squirrel ‚ûú robot squirrel&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;squirrel, Carrot ‚ûú rabbit, eggplant&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;squirrel, Carrot ‚ûú robot mouse, screwdriver&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/13_bear_tiger_leopard_lion_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/13_bear_tiger_leopard_lion_02_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/13_bear_tiger_leopard_lion_03_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;bear ‚ûú a red tiger&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;bear ‚ûú a yellow leopard&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;bear ‚ûú a brown lion&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/14_cat_grass_tiger_corgin_02_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/14_cat_grass_tiger_corgin_03_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/14_cat_grass_tiger_corgin_04_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;cat ‚ûú black cat, grass...&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;cat ‚ûú red tiger&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;cat ‚ûú Shiba-Inu&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/10_bus_gpu_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/11_dog_robotic_corgin_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/11_dog_robotic_corgin_02_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;bus ‚ûú GPU&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;gray dog ‚ûú yellow corgi&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;gray dog ‚ûú robotic dog&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/9_duck_rubber_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/12_fox_snow_wolf_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/attri/12_fox_snow_wolf_02_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;white duck ‚ûú yellow rubber duck&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;grass ‚ûú snow&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;white fox ‚ûú grey wolf&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Shape and large motion editing with Tune-A-Video&lt;/h2&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/shape/17_car_posche_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/shape/18_swan_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/shape/18_swan_02_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;!-- &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/man-skiing/wonder-woman.gif&#34;&gt;&lt;/td&gt;              &#xA;  &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/man-skiing/pink-sunset.gif&#34;&gt;&lt;/td&gt; --&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;silver jeep ‚ûú posche car&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Swan ‚ûú White Duck&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Swan ‚ûú Pink flamingo&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/shape/19_man_wonder_01_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/shape/19_man_wonder_02_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/docs/gif_results/shape/19_man_wonder_03_concat_result.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;A man ‚ûú A Batman&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;A man ‚ûú A Wonder Woman, With cowboy hat&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;A man ‚ûú A Spider-Man&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üïπ Online Demo&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to AK and the team from Hugging Face for providing computing resources to support our Hugging-face Demo, which supports up to 30 steps DDIM steps. &lt;a href=&#34;https://huggingface.co/spaces/chenyangqi/FateZero&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a Colab demo, which supports 10 DDIM steps. &lt;a href=&#34;https://colab.research.google.com/github/ChenyangQiQi/FateZero/blob/main/colab_fatezero.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You may also run them on your local machine. We will refine and optimize the above demos in the following days.&lt;/p&gt; &#xA;&lt;h2&gt;üìÄ Demo Video&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/45789244/225698509-79c14793-3153-4bba-9d6e-ede7d811d7f8.mp4&#34;&gt;https://user-images.githubusercontent.com/45789244/225698509-79c14793-3153-4bba-9d6e-ede7d811d7f8.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The video here is compressed due to the size limit of GitHub. The original full-resolution video is &lt;a href=&#34;https://hkustconnect-my.sharepoint.com/&lt;span&gt;‚úå&lt;/span&gt;/g/personal/cqiaa_connect_ust_hk/EXKDI_nahEhKtiYPvvyU9SkBDTG2W4G1AZ_vkC7ekh3ENw?e=Xhgtmk&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üìç Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{qi2023fatezero,&#xA;      title={FateZero: Fusing Attentions for Zero-shot Text-based Video Editing}, &#xA;      author={Chenyang Qi and Xiaodong Cun and Yong Zhang and Chenyang Lei and Xintao Wang and Ying Shan and Qifeng Chen},&#xA;      year={2023},&#xA;      eprint={2303.09535},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üíó Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This repository borrows heavily from &lt;a href=&#34;https://github.com/showlab/Tune-A-Video&#34;&gt;Tune-A-Video&lt;/a&gt; and &lt;a href=&#34;https://github.com/google/prompt-to-prompt/&#34;&gt;prompt-to-prompt&lt;/a&gt;. Thanks to the authors for sharing their code and models.&lt;/p&gt; &#xA;&lt;h2&gt;üßø Maintenance&lt;/h2&gt; &#xA;&lt;p&gt;This is the codebase for our research work. We are still working hard to update this repo, and more details are coming in days. If you have any questions or ideas to discuss, feel free to contact &lt;a href=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/cqiaa@connect.ust.hk&#34;&gt;Chenyang Qi&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/ChenyangQiQi/FateZero/main/vinthony@gmail.com&#34;&gt;Xiaodong Cun&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cvlab-columbia/viper</title>
    <updated>2023-04-01T01:37:35Z</updated>
    <id>tag:github.com,2023-04-01:/cvlab-columbia/viper</id>
    <link href="https://github.com/cvlab-columbia/viper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for the paper &#34;ViperGPT: Visual Inference via Python Execution for Reasoning&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ViperGPT: Visual Inference via Python Execution for Reasoning&lt;/h1&gt; &#xA;&lt;p&gt;This is the code for the paper &lt;a href=&#34;https://viper.cs.columbia.edu&#34;&gt;ViperGPT: Visual Inference via Python Execution for Reasoning&lt;/a&gt; by &lt;a href=&#34;https://www.didacsuris.com/&#34;&gt;D√≠dac Sur√≠s&lt;/a&gt;*, &lt;a href=&#34;https://sachit-menon.github.io/&#34;&gt;Sachit Menon&lt;/a&gt;* and &lt;a href=&#34;https://www.cs.columbia.edu/~vondrick/&#34;&gt;Carl Vondrick&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cvlab-columbia/viper/main/teaser.gif&#34; alt=&#34;teaser&#34; title=&#34;Teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Clone recursively:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recurse-submodules https://github.com/cvlab-columbia/viper.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After cloning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd viper&#xA;export PATH=/usr/local/cuda/bin:$PATH&#xA;bash setup.sh  # This may take a while. Make sure the vipergpt environment is active&#xA;cd GLIP&#xA;python setup.py clean --all build develop --user&#xA;echo YOUR_OPENAI_API_KEY_HERE &amp;gt; api.key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can start exploring with the &lt;code&gt;main_simple.ipynb&lt;/code&gt; notebook. For running on datasets instead of individual examples, use &lt;code&gt;main_batch.py&lt;/code&gt; as discussed later on.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; WARNING: ViperGPT runs code generated by a large language model. We do not have direct control over this code, so it can be dangerous to run it, especially if modifications to the API are made (the current prompts do not have any dangerous functions like interaction with the filesystem, so it is unlikely that any malicious code can be generated). We cannot guarantee that the code is safe, so use at your own risk, or run in a sandboxed environment. For this reason, the default &lt;code&gt;execute_code&lt;/code&gt; parameter in the config is &lt;code&gt;False&lt;/code&gt;. Set it to &lt;code&gt;True&lt;/code&gt; if you would like the generated code to be executed automatically in &lt;code&gt;main_batch.py&lt;/code&gt;, otherwise you can execute it yourself (as in &lt;code&gt;main_simple.ipynb&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;‚Ñπ&lt;/span&gt; NOTE: OpenAI will discontinue support for the Codex API on March 23rd. This repository implements GPT-3.5 Turbo and GPT-4 as alternatives, but we have not tested them extensively; as they are chat models and not completion, their behavior likely differs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Detailed Installation&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to get started exploring ViperGPT is through &lt;code&gt;main_simple.ipynb&lt;/code&gt;. To run it, you will need to do the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository with its submodules.&lt;/li&gt; &#xA; &lt;li&gt;Install the dependencies. See the see &lt;a href=&#34;https://raw.githubusercontent.com/cvlab-columbia/viper/main/#Dependencies&#34;&gt;Dependencies&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download two pretrained models (the rest are downloaded automatically). See &lt;a href=&#34;https://raw.githubusercontent.com/cvlab-columbia/viper/main/#Pretrained-models&#34;&gt;Pretrained models&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Set up the OpenAI key. See &lt;a href=&#34;https://raw.githubusercontent.com/cvlab-columbia/viper/main/#OpenAI-key&#34;&gt;OpenAI key&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Cloning this Repo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recurse-submodules https://github.com/cvlab-columbia/viper.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;First, create a conda environment using &lt;code&gt;setup_env.sh&lt;/code&gt; and then install our modified version of GLIP. To do so, just &lt;code&gt;cd&lt;/code&gt; into the &lt;code&gt;viper&lt;/code&gt; directory, and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PATH=/usr/local/cuda/bin:$PATH&#xA;bash setup_env.sh&#xA;conda activate vipergpt&#xA;cd GLIP&#xA;python setup.py clean --all build develop --user&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please make sure to install GLIP as described (i.e., from our provided repo) as we have updated the CUDA kernels to be compatible with newer versions of PyTorch, which are required for other models.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained models&lt;/h3&gt; &#xA;&lt;p&gt;Note that ViperGPT may inherit biases from the pretrained models it uses. These biases may be reflected in the outputs generated by our model. It is recommended to consider this potential bias when using ViperGPT and interpreting its outputs.&lt;/p&gt; &#xA;&lt;p&gt;This repository implements more models than the ones described in the paper, which can be useful for further research. Most of the implemented modules automatically download the pretrained models. However, there are four models that need to be downloaded manually, if they are to be used. They have to be stored in the same directory &lt;code&gt;/path/to/pretrained_models&lt;/code&gt;, by default &lt;code&gt;./pretrained_models/&lt;/code&gt;, which has to be specified in the configuration (see &lt;a href=&#34;https://raw.githubusercontent.com/cvlab-columbia/viper/main/#Configuration&#34;&gt;Configuration&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;We provide the convenience script &lt;code&gt;download_models.sh&lt;/code&gt; to perform this download for you; you can set the variable $PRETRAINED_MODEL_PATH match your config&#39;s &lt;code&gt;/path/to/pretrained_models/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Pretrained model system requirements&lt;/h4&gt; &#xA;&lt;p&gt;Many of the models used are very large, and require quite a bit of GPU memory. In particular, GLIP and BLIP2 are especially large. Please use smaller variants of those models if running on hardware that cannot support the larger ones; however, this comes at the expense of performance.&lt;/p&gt; &#xA;&lt;h3&gt;OpenAI key&lt;/h3&gt; &#xA;&lt;p&gt;To run the OpenAI models, you will need to configure an OpenAI key. This can be done by signing up for an account &lt;a href=&#34;https://platform.openai.com/&#34;&gt;e.g. here&lt;/a&gt;, and then creating a key in &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;account/api-keys&lt;/a&gt;. &lt;strong&gt;Create a file &lt;code&gt;api.key&lt;/code&gt; and store the key in it.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running the code&lt;/h2&gt; &#xA;&lt;p&gt;Once the previous steps are done, you can run the Jupyter Notebook &lt;code&gt;main_simple.ipynb&lt;/code&gt;. This notebook contains the code to try ViperGPT on your own images. The notebook is well documented, and it describes how to use the code.&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;You can run ViperGPT on a pre-defined set of query-image/video pairs as well. In order to do that, you will have to create a &lt;code&gt;queries.csv&lt;/code&gt; file, which contains the queries and the filenames for the corresponding images/videos. The format of the file is &lt;code&gt;query,answer,image_name/video_name&lt;/code&gt;. The answer is optional, and only needed for evaluation. See &lt;code&gt;data&lt;/code&gt; for an example.&lt;/p&gt; &#xA;&lt;p&gt;Your dataset directory will contain the &lt;code&gt;queries.csv&lt;/code&gt; file as well as the images/videos in the &lt;code&gt;images&lt;/code&gt;/&lt;code&gt;videos&lt;/code&gt; directory. Add the path to the dataset directory in the configuration (see &lt;a href=&#34;https://raw.githubusercontent.com/cvlab-columbia/viper/main/#Configuration&#34;&gt;Configuration&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;All the configuration parameters are defined in &lt;code&gt;configs/base_config.yaml&lt;/code&gt;. In order to run the code, modify the paths in the parameters &lt;code&gt;path_pretrained_models&lt;/code&gt; and optionally &lt;code&gt;dataset.data_path&lt;/code&gt; to point to the correct directories.&lt;/p&gt; &#xA;&lt;p&gt;For every new configuration you need to run, create a new yaml file in the &lt;code&gt;configs&lt;/code&gt; directory (like &lt;code&gt;my_config.yaml&lt;/code&gt;), and modify the parameters you need to change. The parameters in the new file will overwrite the ones in &lt;code&gt;base_config.yaml&lt;/code&gt;. Any number of configuration files can be specified, they will be merged in the order they are specified in the command line.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;multiprocessing&lt;/code&gt; parameter refers to &lt;em&gt;both&lt;/em&gt; the batch (every sample is run by a different worker) and the models (every model runs in its own process).&lt;/p&gt; &#xA;&lt;h2&gt;Running the code on a dataset, without the Jupyter notebook&lt;/h2&gt; &#xA;&lt;p&gt;The code can be run using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CONFIG_NAMES=your_config_name python main_batch.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;CONFIG_NAMES&lt;/code&gt; is an environment variable that specifies the configuration files to use.&lt;/p&gt; &#xA;&lt;p&gt;If you want to run the code using multiprocessing, set &lt;code&gt;multiprocessing: True&lt;/code&gt; in the config file:&lt;/p&gt; &#xA;&lt;p&gt;It is especially important to consider the risks of executing arbitrary code when running in a batch; in particular, if you modify the API or any inputs to Codex, be mindful to not include potentially damaging abilities such as file modification/deletion.&lt;/p&gt; &#xA;&lt;h2&gt;Code structure&lt;/h2&gt; &#xA;&lt;p&gt;The code is prepared to run in a multiprocessing manner, from two points of view. First, it runs the models in parallel, meaning that each pretrained model runs in its own process. Second, it runs the samples in parallel, meaning that several workers are created to run the samples for a given batch. There is a producer-consumer queuing mechanism where the processes controlling the models are the consumers of inputs coming from the workers that run each sample (producer). Our implementation allows for batching of samples, which means that several workers can send their inputs to the same model process, which will run them as a batch, and return the output to each worker separately.&lt;/p&gt; &#xA;&lt;p&gt;The code has comments and docstrings, but here is a brief overview of the code structure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;vision_models.py&lt;/code&gt;: Contains the code for the pretrained models. Each one of them is a subclass of &lt;code&gt;BaseModel&lt;/code&gt;. Implementing a new model is easy. Just create a new class that inherits from &lt;code&gt;BaseModel&lt;/code&gt; and implement the &lt;code&gt;forward&lt;/code&gt; method, as well as the &lt;code&gt;name&lt;/code&gt; method. The latter will be used to call the model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;vision_processes.py&lt;/code&gt;: Acts as a bridge between the models and the rest of the code. It contains the code for to start all the required processes, whether multiprocessing or not. It automatically detects all the new models implemented in &lt;code&gt;vision_models.py&lt;/code&gt;. It defines a &lt;code&gt;forward&lt;/code&gt; method that takes a name as input (as well as arguments), and calls the appropriate model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;main_batch.py&lt;/code&gt; and &lt;code&gt;main_simple.ipynb&lt;/code&gt;: These are the main files to run the code. The former runs the whole dataset and is suited for parallel processing of samples, while the latter runs a single image/video and is suited for debugging.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;image_patch.py&lt;/code&gt; and &lt;code&gt;video_segment.py&lt;/code&gt;: These are the classes that represent the image patches and video segments. They contain all the methods that call the &lt;code&gt;forward&lt;/code&gt; method of &lt;code&gt;vision_processes.py&lt;/code&gt; and therefore call the models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;configs&lt;/code&gt;: Directory containing the configuration files. The configuration files are in YAML format, and read using OmegaConf.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;datasets&lt;/code&gt;: Directory containing the code for the datasets. The datasets are subclasses of &lt;code&gt;torch.utils.data.Dataset&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;prompts&lt;/code&gt;: Directory containing the prompts for Codex and GPT-3. The Codex ones define the API specifications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;utils.py&lt;/code&gt;, &lt;code&gt;useful_lists&lt;/code&gt; and &lt;code&gt;base_models&lt;/code&gt;: Auxiliary files containing useful functions, lists and pretrained model implementations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code, please consider citing the paper as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{surismenon2023vipergpt,&#xA;    title={ViperGPT: Visual Inference via Python Execution for Reasoning},&#xA;    author={D\&#39;idac Sur\&#39;is and Sachit Menon and Carl Vondrick},&#xA;    journal={arXiv preprint arXiv:2303.08128},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>HarderThenHarder/transformers_tasks</title>
    <updated>2023-04-01T01:37:35Z</updated>
    <id>tag:github.com,2023-04-01:/HarderThenHarder/transformers_tasks</id>
    <link href="https://github.com/HarderThenHarder/transformers_tasks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;‚≠êÔ∏è NLP Algorithms with transformers lib. Supporting Text-Classification, Text-Generation, Information-Extraction, Text-Matching, RLHF etc.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/assets/icon.png&#34; width=&#34;500&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/column/c_1451236880973426688&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Author-Pankeyu-green.svg?sanitize=true&#34; alt=&#34;Author&#34; title=&#34;Author&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OS-Linux/Windows/Mac-red.svg?sanitize=true&#34; alt=&#34;OS&#34; title=&#34;OS&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Based-huggingface_transformers-blue.svg?sanitize=true&#34; alt=&#34;Based&#34; title=&#34;OS&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Status-WIP-darkslateblue.svg?sanitize=true&#34; alt=&#34;Status&#34; title=&#34;Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Stars-370-yellow.svg?sanitize=true&#34; alt=&#34;Stars&#34; title=&#34;Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Fork-77-sandybrown.svg?sanitize=true&#34; alt=&#34;Fork&#34; title=&#34;Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.6+-darkseagreen.svg?sanitize=true&#34; alt=&#34;Python&#34; title=&#34;Python&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://git.io/typing-svg&#34;&gt;&lt;img src=&#34;https://readme-typing-svg.demolab.com?font=Fira+Code&amp;amp;pause=200&amp;amp;color=1AF783&amp;amp;center=true&amp;amp;vCenter=true&amp;amp;width=435&amp;amp;lines=Transformers+%3E%3E%3E%3E%3E%3E%3E%3E%3E%3E+GO!&#34; alt=&#34;Typing SVG&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;ËØ•È°πÁõÆÈõÜÊàê‰∫ÜÂü∫‰∫é &lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;transformers&lt;/a&gt; Â∫ìÂÆûÁé∞ÁöÑÂ§öÁßç NLP ‰ªªÂä°„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;huggingface transformers&lt;/a&gt; ÊòØ‰∏Ä‰∏™ÈùûÂ∏∏Ê£íÁöÑÂºÄÊ∫êÊ°ÜÊû∂ÔºåÊîØÊåÅÈùûÂ∏∏Êñπ‰æøÁöÑÂä†ËΩΩ/ËÆ≠ÁªÉ transformer Ê®°ÂûãÔºå‰Ω†ÂèØ‰ª•Âú®&lt;a href=&#34;https://huggingface.co/docs/transformers/quicktour&#34;&gt;ËøôÈáå&lt;/a&gt;ÁúãÂà∞ËØ•Â∫ìÁöÑÂÆâË£ÖÊñπÊ≥ïÂíåÂÖ•Èó®Á∫ßË∞ÉÁî®ÔºåËØ•Â∫ì‰πüËÉΩÊîØÊåÅÁî®Êà∑ÈùûÂ∏∏‰æøÊç∑ÁöÑ&lt;a href=&#34;https://huggingface.co/docs/transformers/training&#34;&gt;ÂæÆË∞É‰∏Ä‰∏™Â±û‰∫éËá™Â∑±ÁöÑÊ®°Âûã&lt;/a&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Âú®ËØ•È°πÁõÆ‰∏≠Êàë‰ª¨ÈõÜÊàê‰∫Ü‰∏Ä‰∫õ‰∏ªÊµÅÁöÑNLP‰ªªÂä°Ôºå‰Ω†ÂèØ‰ª•ÊâæÂà∞ÂØπÂ∫îÁöÑ‰ªªÂä°ÔºåÂ∞Ü‰ª£Á†Å‰∏≠ÁöÑ&lt;code&gt;ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ&lt;/code&gt;Êõ¥Êç¢Êàê&lt;code&gt;‰Ω†Ëá™Â∑±‰ªªÂä°‰∏ãÁöÑÊï∞ÊçÆÈõÜ&lt;/code&gt;‰ªéËÄåËÆ≠ÁªÉ‰∏Ä‰∏™Á¨¶Âêà‰Ω†Ëá™Â∑±‰ªªÂä°‰∏ãÁöÑÊ®°Âûã„ÄÇ&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;ÁõÆÂâçÂ∑≤ÁªèÂÆûÁé∞ÁöÑNLP‰ªªÂä°Â¶Ç‰∏ãÔºàÊõ¥Êñ∞‰∏≠ÔºâÔºö&lt;/p&gt; &#xA;&lt;h4&gt;1. ÊñáÊú¨ÂåπÈÖçÔºàText MatchingÔºâ&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ËÆ°ÁÆóÊñáÊú¨Èó¥ÁöÑÁõ∏‰ººÂ∫¶ÔºåÂ§öÁî®‰∫éÔºö&lt;code&gt;ÊêúÁ¥¢Âè¨Âõû&lt;/code&gt;„ÄÅ&lt;code&gt;ÊñáÊú¨Ê£ÄÁ¥¢&lt;/code&gt;„ÄÅ&lt;code&gt;Ëï¥Âê´ËØÜÂà´&lt;/code&gt; Á≠â‰ªªÂä°„ÄÇ&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Ê®°Âûã&lt;/th&gt; &#xA;   &lt;th&gt;‰º†ÈÄÅÈó®&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;„ÄêÁõëÁù£„ÄëÊ¶ÇËßà&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/text_matching/supervised/readme.md&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;„ÄêÁõëÁù£„ÄëPointWiseÔºàÂçïÂ°îÔºâ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/text_matching/supervised/train_pointwise.sh&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;„ÄêÁõëÁù£„ÄëDSSMÔºàÂèåÂ°îÔºâ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/text_matching/supervised/train_dssm.sh&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;„ÄêÁõëÁù£„ÄëSentence BertÔºàÂèåÂ°îÔºâ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/text_matching/supervised/train_sentence_transformer.sh&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;„ÄêÊó†ÁõëÁù£„ÄëSimCSE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/text_matching/unsupervised/simcse/readme.md&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h4&gt;2. ‰ø°ÊÅØÊäΩÂèñÔºàInformation ExtractionÔºâ&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Âú®ÁªôÂÆöÁöÑÊñáÊú¨ÊÆµËêΩ‰∏≠ÊäΩÂèñÁõÆÊ†á‰ø°ÊÅØÔºåÂ§öÁî®‰∫éÔºö&lt;code&gt;ÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÔºàNERÔºâ&lt;/code&gt;Ôºå&lt;code&gt;ÂÆû‰ΩìÂÖ≥Á≥ªÊäΩÂèñÔºàREÔºâ&lt;/code&gt; Á≠â‰ªªÂä°„ÄÇ&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Ê®°Âûã&lt;/th&gt; &#xA;   &lt;th&gt;‰º†ÈÄÅÈó®&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ÈÄöÁî®‰ø°ÊÅØÊäΩÂèñÔºàUniverse Information Extraction, UIEÔºâ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/UIE/readme.md&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h4&gt;3. Prompt‰ªªÂä°ÔºàPrompt TasksÔºâ&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ÈÄöËøáËÆæËÆ°ÊèêÁ§∫ÔºàpromptÔºâÊ®°ÊùøÔºåÂÆûÁé∞‰ΩøÁî®Êõ¥Â∞ëÈáèÁöÑÊï∞ÊçÆÂú®È¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºàPretrained ModelÔºâ‰∏äÂæóÂà∞Êõ¥Â•ΩÁöÑÊïàÊûúÔºåÂ§öÁî®‰∫éÔºö&lt;code&gt;Few-Shot&lt;/code&gt;Ôºå&lt;code&gt;Zero-Shot&lt;/code&gt; Á≠â‰ªªÂä°„ÄÇ&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Ê®°Âûã&lt;/th&gt; &#xA;   &lt;th&gt;‰º†ÈÄÅÈó®&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PETÔºàÂü∫‰∫é‰∫∫Â∑•ÂÆö‰πâ prompt pattern ÁöÑÊñπÊ≥ïÔºâ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/prompt_tasks/PET/readme.md&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;p-tuningÔºàÊú∫Âô®Ëá™Âä®Â≠¶‰π† prompt pattern ÁöÑÊñπÊ≥ïÔºâ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/prompt_tasks/p-tuning/readme.md&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h4&gt;4. ÊñáÊú¨ÂàÜÁ±ªÔºàText ClassificationÔºâ&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ÂØπÁªôÂÆöÊñáÊú¨ËøõË°åÂàÜÁ±ªÔºåÂ§öÁî®‰∫éÔºö&lt;code&gt;ÊÉÖÊÑüËØÜÂà´&lt;/code&gt;Ôºå&lt;code&gt;ÊñáÁ´†ÂàÜÁ±ªËØÜÂà´&lt;/code&gt; Á≠â‰ªªÂä°„ÄÇ&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Ê®°Âûã&lt;/th&gt; &#xA;   &lt;th&gt;‰º†ÈÄÅÈó®&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BERT-CLSÔºàÂü∫‰∫é BERT ÁöÑÂàÜÁ±ªÂô®Ôºâ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/text_classification/train.sh&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h4&gt;5. Âº∫ÂåñÂ≠¶‰π† &amp;amp; ËØ≠Ë®ÄÊ®°ÂûãÔºàReinforcement Learning &amp;amp; Language ModelÔºâ&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;RLHFÔºàReinforcement Learning from Human FeedbackÔºâÈÄöËøá‰∫∫Á±ªÁöÑÂèçÈ¶àÔºåÂ∞ÜÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁî®‰∫éÊõ¥Êñ∞ËØ≠Ë®ÄÁîüÊàêÊ®°ÂûãÔºàLMÔºâÔºå‰ªéËÄåËææÂà∞Êõ¥Â•ΩÁöÑÁîüÊàêÊïàÊûúÔºà‰ª£Ë°®‰æãÂ≠êÔºöChatGPTÔºâÔºõÈÄöÂ∏∏ÂåÖÊã¨Ôºö&lt;code&gt;Â•ñÂä±Ê®°ÂûãÔºàReward ModelÔºâ&lt;/code&gt; ËÆ≠ÁªÉÂíå &lt;code&gt;Âº∫ÂåñÂ≠¶‰π†ÔºàReinforcement LearningÔºâ&lt;/code&gt; ËÆ≠ÁªÉ‰∏§‰∏™Èò∂ÊÆµ„ÄÇ&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Ê®°Âûã&lt;/th&gt; &#xA;   &lt;th&gt;‰º†ÈÄÅÈó®&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHFÔºàReward Model ËÆ≠ÁªÉÔºåPPO Êõ¥Êñ∞ GPT2Ôºâ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/RLHF/readme.md&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h4&gt;6. ÊñáÊú¨ÁîüÊàêÔºàText GenerationÔºâ&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ÊñáÊú¨ÁîüÊàêÔºàNLGÔºâÔºåÈÄöÂ∏∏Áî®‰∫éÔºö&lt;code&gt;Â∞èËØ¥Áª≠ÂÜô&lt;/code&gt;Ôºå&lt;code&gt;Êô∫ËÉΩÈóÆÁ≠î&lt;/code&gt;Ôºå&lt;code&gt;ÂØπËØùÊú∫Âô®‰∫∫&lt;/code&gt; Á≠â‰ªªÂä°„ÄÇ&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Ê®°Âûã&lt;/th&gt; &#xA;   &lt;th&gt;‰º†ÈÄÅÈó®&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;‰∏≠ÊñáÈóÆÁ≠îÊ®°ÂûãÔºàT5-BasedÔºâ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/answer_generation/readme.md&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Filling Ê®°ÂûãÔºàT5-BasedÔºâ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/data_augment/filling_model/readme.md&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h4&gt;7. Â§ßÊ®°ÂûãÂ∫îÁî®ÔºàLLM ApplicationÔºâ&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Â∞ùËØïÊûÑÂª∫Â§ßÊ®°ÂûãÔºàLLMÔºâzero-shot Ëß£ÂÜ≥Â§öÁßç‰ªªÂä°ÊâÄÈúÄÁöÑ prompt pattern(s)„ÄÇ&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Ê®°Âûã&lt;/th&gt; &#xA;   &lt;th&gt;‰º†ÈÄÅÈó®&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ÊñáÊú¨ÂàÜÁ±ªÔºàchatglm-6b-BasedÔºâ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/LLM/zero-shot/readme.md&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ÊñáÊú¨ÂåπÈÖçÔºàchatglm-6b-BasedÔºâ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/LLM/zero-shot/readme.md&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;‰ø°ÊÅØÊäΩÂèñÔºàchatglm-6b-BasedÔºâ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/LLM/zero-shot/readme.md&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h4&gt;8. Â§ßÊ®°ÂûãÂæÆË∞ÉÔºàLLM FinetuneÔºâ&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ÂØπÂ§ßÊ®°ÂûãÂú®Â§ö‰∏™ÊåáÂÆö‰ªªÂä°‰∏äËøõË°åÂæÆË∞É„ÄÇ&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Ê®°Âûã&lt;/th&gt; &#xA;   &lt;th&gt;‰º†ÈÄÅÈó®&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM-6B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/HarderThenHarder/transformers_tasks/main/LLM/finetune/readme.md&#34;&gt;[ËøôÈáå]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>