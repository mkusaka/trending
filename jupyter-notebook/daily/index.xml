<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-22T01:39:00Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ajcr/100-pandas-puzzles</title>
    <updated>2024-04-22T01:39:00Z</updated>
    <id>tag:github.com,2024-04-22:/ajcr/100-pandas-puzzles</id>
    <link href="https://github.com/ajcr/100-pandas-puzzles" rel="alternate"></link>
    <summary type="html">&lt;p&gt;100 data puzzles for pandas, ranging from short and simple to super tricky (60% complete)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;100 pandas puzzles&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/ajcr/100-pandas-puzzles/raw/master/100-pandas-puzzles.ipynb&#34;&gt;Puzzles notebook&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/ajcr/100-pandas-puzzles/raw/master/100-pandas-puzzles-with-solutions.ipynb&#34;&gt;Solutions notebook&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Inspired by &lt;a href=&#34;https://github.com/rougier/numpy-100&#34;&gt;100 Numpy exerises&lt;/a&gt;, here are 100* short puzzles for testing your knowledge of &lt;a href=&#34;http://pandas.pydata.org/&#34;&gt;pandas&#39;&lt;/a&gt; power.&lt;/p&gt; &#xA;&lt;p&gt;Since pandas is a large library with many different specialist features and functions, these excercises focus mainly on the fundamentals of manipulating data (indexing, grouping, aggregating, cleaning), making use of the core DataFrame and Series objects. Many of the excerises here are straightforward in that the solutions require no more than a few lines of code (in pandas or NumPy - don&#39;t go using pure Python!). Choosing the right methods and following best practices is the underlying goal.&lt;/p&gt; &#xA;&lt;p&gt;The exercises are loosely divided in sections. Each section has a difficulty rating; these ratings are subjective, of course, but should be a seen as a rough guide as to how elaborate the required solution needs to be.&lt;/p&gt; &#xA;&lt;p&gt;Good luck solving the puzzles!&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;* the list of puzzles is not yet complete! Pull requests or suggestions for additional exercises, corrections and improvements are welcomed.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview of puzzles&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Section Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Difficulty&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Importing pandas&lt;/td&gt; &#xA;   &lt;td&gt;Getting started and checking your pandas setup&lt;/td&gt; &#xA;   &lt;td&gt;Easy&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DataFrame basics&lt;/td&gt; &#xA;   &lt;td&gt;A few of the fundamental routines for selecting, sorting, adding and aggregating data in DataFrames&lt;/td&gt; &#xA;   &lt;td&gt;Easy&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DataFrames: beyond the basics&lt;/td&gt; &#xA;   &lt;td&gt;Slightly trickier: you may need to combine two or more methods to get the right answer&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DataFrames: harder problems&lt;/td&gt; &#xA;   &lt;td&gt;These might require a bit of thinking outside the box...&lt;/td&gt; &#xA;   &lt;td&gt;Hard&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Series and DatetimeIndex&lt;/td&gt; &#xA;   &lt;td&gt;Exercises for creating and manipulating Series with datetime data&lt;/td&gt; &#xA;   &lt;td&gt;Easy/Medium&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cleaning Data&lt;/td&gt; &#xA;   &lt;td&gt;Making a DataFrame easier to work with&lt;/td&gt; &#xA;   &lt;td&gt;Easy/Medium&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Using MultiIndexes&lt;/td&gt; &#xA;   &lt;td&gt;Go beyond flat DataFrames with additional index levels&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Minesweeper&lt;/td&gt; &#xA;   &lt;td&gt;Generate the numbers for safe squares in a Minesweeper grid&lt;/td&gt; &#xA;   &lt;td&gt;Hard&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Plotting&lt;/td&gt; &#xA;   &lt;td&gt;Explore pandas&#39; part of plotting functionality to see trends in data&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Setting up&lt;/h2&gt; &#xA;&lt;p&gt;To tackle the puzzles on your own computer, you&#39;ll need a Python 3 environment with the dependencies (namely pandas) installed.&lt;/p&gt; &#xA;&lt;p&gt;One way to do this is as follows. I&#39;m using a bash shell, the procedure with Mac OS should be essentially the same. Windows, I&#39;m not sure about.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Check you have Python 3 installed by printing the version of Python:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -V&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Clone the puzzle repository using Git:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/ajcr/100-pandas-puzzles.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install the dependencies (&lt;strong&gt;caution&lt;/strong&gt;: if you don&#39;t want to modify any Python modules in your active environment, consider using a virtual environment instead):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Launch a jupyter notebook server:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;jupyter notebook --notebook-dir=100-pandas-puzzles&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should be able to see the notebooks and launch them in your web browser.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;This repository has benefitted from numerous contributors, with those who have sent puzzles and fixes listed in &lt;a href=&#34;https://github.com/ajcr/100-pandas-puzzles/raw/master/CONTRIBUTORS.md&#34;&gt;CONTRIBUTORS&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to everyone who has raised an issue too.&lt;/p&gt; &#xA;&lt;h2&gt;Other links&lt;/h2&gt; &#xA;&lt;p&gt;If you feel like reading up on pandas before starting, the official documentation useful and very extensive. Good places get a broader overview of pandas are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://pandas.pydata.org/pandas-docs/version/0.17.0/10min.html&#34;&gt;10 minutes to pandas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://pandas.pydata.org/pandas-docs/version/0.17.0/basics.html&#34;&gt;pandas basics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://pandas.pydata.org/pandas-docs/stable/tutorials.html&#34;&gt;tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://pandas.pydata.org/pandas-docs/version/0.17.0/cookbook.html#cookbook&#34;&gt;cookbook and idioms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/guipsamora/pandas_exercises&#34;&gt;Guilherme Samora&#39;s pandas exercises&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are may other excellent resources and books that are easily searchable and purchaseable.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/RecAI</title>
    <updated>2024-04-22T01:39:00Z</updated>
    <id>tag:github.com,2024-04-22:/microsoft/RecAI</id>
    <link href="https://github.com/microsoft/RecAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bridging LLM and Recommender System.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RecAI/main/assets/logo.png&#34; alt=&#34;Recommender System with AI&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Large Language Models (LLMs) offer significant potential for the development of cutting-edge recommender systems, particularly in terms of enhancing interactivity, explainability, and controllability. These are aspects that have traditionally posed challenges. However, the direct application of a general-purpose LLM for recommendation purposes is not viable due to the absence of specific domain knowledge.&lt;/p&gt; &#xA;&lt;p&gt;The RecAI project aims to bridge this gap by investigating various strategies to integrate LLMs into recommender systems, a concept people usually term as LLM4Rec. Our goal is to reflect the real-world needs of LLM4Rec through holistic views and methodologies.&lt;/p&gt; &#xA;&lt;p&gt;We believe that by adopting a holistic perspective, we can incorporate the majority of practical requirements of LLM4Rec into one or more of the techniques explored in the RecAI project. These techniques include, but are not limited to, Recommender AI agents, the injection of knowledge through personalized prompting, fine-tuning language models as recommenders, evaluation, and LLMs as model explainers. The ultimate objective is to create a more sophisticated, interactive, and user-centric recommender system.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;background-image:url(./assets/background/background.png);background-color:rgba(255, 255, 255, 0.8);background-blend-mode:overlay;background-position:right;background-repeat:no-repeat;background-size:100% 100%;&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RecAI/main/assets/recagent.png&#34; alt=&#34;Recommender AI agent&#34;&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;strong style=&#34;font-size:3vw&#34;&gt; &lt;a href=&#34;https://github.com/microsoft/RecAI/raw/main/InteRecAgent/README.md&#34; style=&#34;color: #0000FF; text-decoration: underline;&#34;&gt; Recommender AI Agent &lt;/a&gt; &lt;/strong&gt; &lt;br&gt; LLMs provide natural interactions and respond smartly to human instructions but lack domain-specific expertise. In contrast, traditional recommender systems excel with in-domain data training yet are constrained to structured data and lack interactivity. InteRecAgent introduces an AI agent that combines the strengths of both: it employs an LLM as the brain and traditional recommender models as tools. Consequently, traditional models like matrix factorization can be transformed into conversational, interactive, and explainable recommender systems.. &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RecAI/main/assets/knowledge-plugin.png&#34; alt=&#34;Selective Knowledge Plugin&#34;&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;strong style=&#34;font-size:3vw&#34;&gt; &lt;a href=&#34;https://github.com/microsoft/RecAI/raw/main/Knowledge_Plugin/README.md&#34; style=&#34;color: #0000FF; text-decoration: underline;&#34;&gt; Selective Knowledge Plugin &lt;/a&gt;&lt;/strong&gt; &lt;br&gt; How can we enhance an LLM&#39;s domain-specific ability without finetuning the model? Then the prompt is the key. In this work, we introduce a method that augments LLMs with selective knowledge, so that large-scale, continuously evolving, and domain-specific data patterns can be injected by prompt. &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RecAI/main/assets/reclm-emb.png&#34; alt=&#34;Embedding RecLM&#34;&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;strong style=&#34;font-size:3vw&#34;&gt; &lt;a href=&#34;https://github.com/microsoft/RecAI/raw/main/RecLM-emb/README.md&#34; style=&#34;color: #0000FF; text-decoration: underline;&#34;&gt; Embedding RecLM &lt;/a&gt;&lt;/strong&gt; &lt;br&gt; Dense retrieval is a crucial component in a range of scenarios, including recommender systems and retrieval-augmented generation (RAG). While generative language models such as GPTs are designed for sequential token generation, they are not optimized for retrieval-oriented embedding. This is where our project, RecLM-emb, comes into play. RecLM-emb aligns with text-embedding models like text-embedding-ada-002, but it is specifically optimized for item retrieval. The goal is to embed everything for item retrieval. Currently it only supports text modality, such as search query, item description, and user instructions. &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RecAI/main/assets/reclm-gen.png&#34; alt=&#34;Generative RecLM &#34;&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;strong style=&#34;font-size:3vw&#34;&gt; &lt;a href=&#34;https://github.com/microsoft/RecAI/raw/main/RecLM-gen/README.md&#34; style=&#34;color: #0000FF; text-decoration: underline;&#34;&gt; Generative RecLM &lt;/a&gt;&lt;/strong&gt; &lt;br&gt; It&#39;s important to note that data patterns vary significantly across domains, meaning a general-purpose LLM may not deliver optimized performance within a specific domain. To adapt to specific domain data patterns, grounding to domain item catelogs, and enhance instruction-following capability, this project discusses the process of fine-tuning a generative Language Model for recommenders, referred to as RecLM-gen. Techniques include supervised finetuning (SFT) and reinforcement learning (RL). Potential applications of this approach include rankers, conversational recommenders, and user simulators. &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RecAI/main/assets/recexplainer.png&#34; alt=&#34;Recommendation model explainer&#34;&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;strong style=&#34;font-size:3vw&#34;&gt; &lt;a href=&#34;https://github.com/microsoft/RecAI/raw/main/RecExplainer/README.md&#34; style=&#34;color: #0000FF; text-decoration: underline;&#34;&gt; Model Explainer &lt;/a&gt;&lt;/strong&gt; &lt;br&gt; Deep learning-based recommender systems are widely used in various online services, thanks to their superiority in effectiveness and efficiency. However, these models often lack interpretability, making them less reliable and transparent for both users and developers. In this work, we propose a new model interpretation approach for recommender systems, call RecExplainer, by using LLMs as surrogate models and learn to mimic and comprehend target recommender models. &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/RecAI/main/assets/eval.png&#34; alt=&#34;Recommendation Evaluator&#34;&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;strong style=&#34;font-size:3vw&#34;&gt; &lt;a href=&#34;https://github.com/microsoft/RecAI/raw/main/RecLM-eval/README.md&#34; style=&#34;color: #0000FF; text-decoration: underline;&#34;&gt; RecLM Evaluator &lt;/a&gt;&lt;/strong&gt; &lt;br&gt; Evaluation is crucial for assessing the true capabilities of models and identifying areas of weakness for further improvement. In the era of using language models as recommenders, which function in a human-like manner, the evaluation method has significantly deviated from traditional styles. This project intends to offer a comprehensive service for the evaluation of LM-based recommender systems. Whether provided with a trained LM or an API (such as Azure OpenAI API), it assesses the model&#39;s performance from various perspectives, including retrieval, ranking, explanation capability, and general AI ability. &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;RecAI uses &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/RecAI/main/LICENSE&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledge&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to the open source codes of the following projects:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/UniRec&#34;&gt;UniRec&lt;/a&gt;   &lt;a href=&#34;https://github.com/microsoft/TaskMatrix/raw/main/visual_chatgpt.py&#34;&gt;VisualChatGPT&lt;/a&gt;   &lt;a href=&#34;https://github.com/microsoft/JARVIS&#34;&gt;JARVIS&lt;/a&gt;   &lt;a href=&#34;https://github.com/langchain-ai/langchain&#34;&gt;LangChain&lt;/a&gt;   &lt;a href=&#34;https://github.com/microsoft/guidance&#34;&gt;guidance&lt;/a&gt;   &lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding&#34;&gt;FlagEmbedding&lt;/a&gt;  &lt;/p&gt; &#xA;&lt;h2&gt;Responsible AI FAQ&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/RecAI/main/RAI_FAQ.md&#34;&gt;RecAI: Responsible AI FAQ&lt;/a&gt; for document on the purposes, capabilities, and limitations of the RecAI systems.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If this project aids your research, please cite our following paper and any related paper in the respective subfolder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{lian2024recai,&#xA;  title={RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems},&#xA;  author={Lian, Jianxun and Lei, Yuxuan and Huang, Xu and Yao, Jing and Xu, Wei and Xie, Xing},&#xA;  journal={arXiv preprint arXiv:2403.06465},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>rasbt/MachineLearning-QandAI-book</title>
    <updated>2024-04-22T01:39:00Z</updated>
    <id>tag:github.com,2024-04-22:/rasbt/MachineLearning-QandAI-book</id>
    <link href="https://github.com/rasbt/MachineLearning-QandAI-book" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Machine Learning Q and AI book&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;em&gt;Machine Learning and AI Beyond the Basics&lt;/em&gt; Book&lt;/h1&gt; &#xA;&lt;p&gt;The Supplementary Materials for the &lt;a href=&#34;https://nostarch.com/machine-learning-q-and-ai&#34;&gt;Machine Learning Q and AI&lt;/a&gt; book by &lt;a href=&#34;http://sebastianraschka.com&#34;&gt;Sebastian Raschka&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please use the &lt;a href=&#34;https://github.com/rasbt/ml-q-and-ai/discussions&#34;&gt;Discussions&lt;/a&gt; for any questions about the book!&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/rasbt/MachineLearning-QandAI-book/main/img/cover.jpg&#34; alt=&#34;2023-ml-qai-cover&#34; width=&#34;200&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;h4&gt;About the Book&lt;/h4&gt; &#xA;&lt;p&gt;If you’ve locked down the basics of machine learning and AI and want a fun way to address lingering knowledge gaps, this book is for you. This rapid-fire series of short chapters addresses 30 essential questions in the field, helping you stay current on the latest technologies you can implement in your own work.&lt;/p&gt; &#xA;&lt;p&gt;Each chapter of &lt;em&gt;Machine Learning Q and AI&lt;/em&gt; asks and answers a central question, with diagrams to explain new concepts and ample references for further reading&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-GPU training paradigms&lt;/li&gt; &#xA; &lt;li&gt;Finetuning transformers&lt;/li&gt; &#xA; &lt;li&gt;Differences between encoder- and decoder-style LLMs&lt;/li&gt; &#xA; &lt;li&gt;Concepts behind vision transformers&lt;/li&gt; &#xA; &lt;li&gt;Confidence intervals for ML&lt;/li&gt; &#xA; &lt;li&gt;And many more!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p style=&#34;font-size: 0.8em;&#34;&gt; This book is a fully edited and revised version of &lt;a href=&#34;https://leanpub.com/machine-learning-q-and-ai&#34;&gt;Machine Learning Q and AI&lt;/a&gt;, which was available on Leanpub. &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h4&gt;Reviews&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;“One could hardly ask for a better guide than Sebastian, who is, without exaggeration, the best machine learning educator currently in the field. On each page, Sebastian not only imparts his extensive knowledge but also shares the passion and curiosity that mark true expertise.”&lt;br&gt; &lt;strong&gt;-- Chris Albon, Director of Machine Learning, The Wikimedia Foundation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;h4&gt;Links&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nostarch.com/machine-learning-q-and-ai&#34;&gt;Preorder directly from No Starch press&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Machine-Learning-AI-Essential-Questions/dp/1718503768&#34;&gt;Preorder directly from Amazon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book&#34;&gt;Supplementary Materias and Discussions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Title&lt;/th&gt; &#xA;   &lt;th&gt;URL Link&lt;/th&gt; &#xA;   &lt;th&gt;Supplementary Code&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;Embeddings, Representations, and Latent Space&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;Self-Supervised Learning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;Few-Shot Learning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;The Lottery Ticket Hypothesis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;Reducing Overfitting with Data&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;Reducing Overfitting with Model Modifications&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;Multi-GPU Training Paradigms&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;The Keys to the Success of Transformers&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;td&gt;Generative AI Models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;Sources of Randomness&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q10-random-sources/data-sampling.ipynb&#34;&gt;data-sampling.ipynb&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q10-random-sources/dropout.ipynb&#34;&gt;dropout.ipynb&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q10-random-sources/random-weights.ipynb&#34;&gt;random-weights.ipynb&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PART II: COMPUTER VISION&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;Calculating the Number of Parameters&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q11-conv-size/q11-conv-size.ipynb&#34;&gt;conv-size.ipynb&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;The Equivalence of Fully Connected and Convolutional Layers&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q12-fc-cnn-equivalence/q12-fc-cnn-equivalence.ipynb&#34;&gt;fc-cnn-equivalence.ipynb&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;Large Training Sets for Vision Transformers&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PART III: NATURAL LANGUAGE PROCESSING&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;The Distributional Hypothesis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;15&lt;/td&gt; &#xA;   &lt;td&gt;Data Augmentation for Text&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q15-text-augment/backtranslation.ipynb&#34;&gt;backtranslation.ipynb&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q15-text-augment/noise-injection.ipynb&#34;&gt;noise-injection.ipynb&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q15-text-augment/sentence-order-shuffling.ipynb&#34;&gt;sentence-order-shuffling.ipynb&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q15-text-augment/synonym-replacement.ipynb&#34;&gt;synonym-replacement.ipynb&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q15-text-augment/synthetic-data.ipynb&#34;&gt;synthetic-data.ipynb&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q15-text-augment/word-deletion.ipynb&#34;&gt;word-deletion.ipynb&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q15-text-augment/word-position-swapping.ipynb&#34;&gt;word-position-swapping.ipynb&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;“Self”-Attention&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;17&lt;/td&gt; &#xA;   &lt;td&gt;Encoder- And Decoder-Style Transformers&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;18&lt;/td&gt; &#xA;   &lt;td&gt;Using and Finetuning Pretrained Transformers&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;19&lt;/td&gt; &#xA;   &lt;td&gt;Evaluating Generative Large Language Models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q19-evaluation-llms/BERTScore.ipynb&#34;&gt;BERTScore.ipynb&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q19-evaluation-llms/bleu.ipynb&#34;&gt;bleu.ipynb&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q19-evaluation-llms/perplexity.ipynb&#34;&gt;perplexity.ipynb&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q19-evaluation-llms/rouge.ipynb&#34;&gt;rouge.ipynb&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PART IV: PRODUCTION AND DEPLOYMENT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;Stateless And Stateful Training&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;21&lt;/td&gt; &#xA;   &lt;td&gt;Data-Centric AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;22&lt;/td&gt; &#xA;   &lt;td&gt;Speeding Up Inference&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;23&lt;/td&gt; &#xA;   &lt;td&gt;Data Distribution Shifts&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PART V: PREDICTIVE PERFORMANCE AND MODEL EVALUATION&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;Poisson and Ordinal Regression&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;Confidence Intervals&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q25_confidence-intervals/1_four-methods.ipynb&#34;&gt;four-methods.ipynb&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q25_confidence-intervals/2_four-methods-vs-true-value.ipynb&#34;&gt;four-methods-vs-true-value.ipynb&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;26&lt;/td&gt; &#xA;   &lt;td&gt;Confidence Intervals Versus Conformal Predictions&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/rasbt/MachineLearning-QandAI-book/raw/main/supplementary/q26_conformal-prediction/conformal_prediction.ipynb&#34;&gt;conformal_prediction.ipynb&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;27&lt;/td&gt; &#xA;   &lt;td&gt;Proper Metrics&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;28&lt;/td&gt; &#xA;   &lt;td&gt;The K in K-Fold Cross-Validation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;29&lt;/td&gt; &#xA;   &lt;td&gt;Training and Test Set Discordance&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;td&gt;Limited Labeled Data&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>