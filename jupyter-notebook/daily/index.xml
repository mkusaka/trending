<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-19T01:38:34Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dauparas/ProteinMPNN</title>
    <updated>2022-09-19T01:38:34Z</updated>
    <id>tag:github.com,2022-09-19:/dauparas/ProteinMPNN</id>
    <link href="https://github.com/dauparas/ProteinMPNN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for the ProteinMPNN paper&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ProteinMPNN&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://docs.google.com/drawings/d/e/2PACX-1vTtnMBDOq8TpHIctUfGN8Vl32x5ISNcPKlxjcQJF2q70PlaH2uFlj2Ac4s3khnZqG1YxppdMr0iTyk-/pub?w=889&amp;amp;h=358&#34; alt=&#34;ProteinMPNN&#34;&gt; Read &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2022.06.03.494563v1&#34;&gt;ProteinMPNN paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To run ProteinMPNN clone this github repo and install Python&amp;gt;=3.0, PyTorch, Numpy.&lt;/p&gt; &#xA;&lt;p&gt;Full protein backbone models: &lt;code&gt;vanilla_model_weights/v_48_002.pt, v_48_010.pt, v_48_020.pt, v_48_030.pt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;CA only models: &lt;code&gt;ca_model_weights/v_48_002.pt, v_48_010.pt, v_48_020.pt&lt;/code&gt;. Enable flag &lt;code&gt;--ca_only&lt;/code&gt; to use these models.&lt;/p&gt; &#xA;&lt;p&gt;Helper scripts: &lt;code&gt;helper_scripts&lt;/code&gt; - helper functions to parse PDBs, assign which chains to design, which residues to fix, adding AA bias, tying residues etc.&lt;/p&gt; &#xA;&lt;p&gt;Code organization:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;protein_mpnn_run.py&lt;/code&gt; - the main script to initialialize and run the model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;protein_mpnn_utils.py&lt;/code&gt; - utility functions for the main script.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;examples/&lt;/code&gt; - simple code examples.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;inputs/&lt;/code&gt; - input PDB files for examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;outputs/&lt;/code&gt; - outputs from examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;colab_notebooks/&lt;/code&gt; - Google Colab examples&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Input flags for &lt;code&gt;protein_mpnn_run.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    argparser.add_argument(&#34;--ca_only&#34;, action=&#34;store_true&#34;, default=False, help=&#34;Parse CA-only structures and use CA-only models (default: false)&#34;)&#xA;    argparser.add_argument(&#34;--path_to_model_weights&#34;, type=str, default=&#34;&#34;, help=&#34;Path to model weights folder;&#34;)&#xA;    argparser.add_argument(&#34;--model_name&#34;, type=str, default=&#34;v_48_020&#34;, help=&#34;ProteinMPNN model name: v_48_002, v_48_010, v_48_020, v_48_030; v_48_010=version with 48 edges 0.10A noise&#34;)&#xA;    argparser.add_argument(&#34;--seed&#34;, type=int, default=0, help=&#34;If set to 0 then a random seed will be picked;&#34;)&#xA;    argparser.add_argument(&#34;--save_score&#34;, type=int, default=0, help=&#34;0 for False, 1 for True; save score=-log_prob to npy files&#34;)&#xA;    argparser.add_argument(&#34;--save_probs&#34;, type=int, default=0, help=&#34;0 for False, 1 for True; save MPNN predicted probabilites per position&#34;)&#xA;    argparser.add_argument(&#34;--score_only&#34;, type=int, default=0, help=&#34;0 for False, 1 for True; score input backbone-sequence pairs&#34;)&#xA;    argparser.add_argument(&#34;--conditional_probs_only&#34;, type=int, default=0, help=&#34;0 for False, 1 for True; output conditional probabilities p(s_i given the rest of the sequence and backbone)&#34;)&#xA;    argparser.add_argument(&#34;--conditional_probs_only_backbone&#34;, type=int, default=0, help=&#34;0 for False, 1 for True; if true output conditional probabilities p(s_i given backbone)&#34;)&#xA;    argparser.add_argument(&#34;--unconditional_probs_only&#34;, type=int, default=0, help=&#34;0 for False, 1 for True; output unconditional probabilities p(s_i given backbone) in one forward pass&#34;)&#xA;    argparser.add_argument(&#34;--backbone_noise&#34;, type=float, default=0.00, help=&#34;Standard deviation of Gaussian noise to add to backbone atoms&#34;)&#xA;    argparser.add_argument(&#34;--num_seq_per_target&#34;, type=int, default=1, help=&#34;Number of sequences to generate per target&#34;)&#xA;    argparser.add_argument(&#34;--batch_size&#34;, type=int, default=1, help=&#34;Batch size; can set higher for titan, quadro GPUs, reduce this if running out of GPU memory&#34;)&#xA;    argparser.add_argument(&#34;--max_length&#34;, type=int, default=200000, help=&#34;Max sequence length&#34;)&#xA;    argparser.add_argument(&#34;--sampling_temp&#34;, type=str, default=&#34;0.1&#34;, help=&#34;A string of temperatures, 0.2 0.25 0.5. Sampling temperature for amino acids. Suggested values 0.1, 0.15, 0.2, 0.25, 0.3. Higher values will lead to more diversity.&#34;)&#xA;    argparser.add_argument(&#34;--out_folder&#34;, type=str, help=&#34;Path to a folder to output sequences, e.g. /home/out/&#34;)&#xA;    argparser.add_argument(&#34;--pdb_path&#34;, type=str, default=&#39;&#39;, help=&#34;Path to a single PDB to be designed&#34;)&#xA;    argparser.add_argument(&#34;--pdb_path_chains&#34;, type=str, default=&#39;&#39;, help=&#34;Define which chains need to be designed for a single PDB &#34;)&#xA;    argparser.add_argument(&#34;--jsonl_path&#34;, type=str, help=&#34;Path to a folder with parsed pdb into jsonl&#34;)&#xA;    argparser.add_argument(&#34;--chain_id_jsonl&#34;,type=str, default=&#39;&#39;, help=&#34;Path to a dictionary specifying which chains need to be designed and which ones are fixed, if not specied all chains will be designed.&#34;)&#xA;    argparser.add_argument(&#34;--fixed_positions_jsonl&#34;, type=str, default=&#39;&#39;, help=&#34;Path to a dictionary with fixed positions&#34;)&#xA;    argparser.add_argument(&#34;--omit_AAs&#34;, type=list, default=&#39;X&#39;, help=&#34;Specify which amino acids should be omitted in the generated sequence, e.g. &#39;AC&#39; would omit alanine and cystine.&#34;)&#xA;    argparser.add_argument(&#34;--bias_AA_jsonl&#34;, type=str, default=&#39;&#39;, help=&#34;Path to a dictionary which specifies AA composion bias if neededi, e.g. {A: -1.1, F: 0.7} would make A less likely and F more likely.&#34;)&#xA;    argparser.add_argument(&#34;--bias_by_res_jsonl&#34;, default=&#39;&#39;, help=&#34;Path to dictionary with per position bias.&#34;)&#xA;    argparser.add_argument(&#34;--omit_AA_jsonl&#34;, type=str, default=&#39;&#39;, help=&#34;Path to a dictionary which specifies which amino acids need to be omited from design at specific chain indices&#34;)&#xA;    argparser.add_argument(&#34;--pssm_jsonl&#34;, type=str, default=&#39;&#39;, help=&#34;Path to a dictionary with pssm&#34;)&#xA;    argparser.add_argument(&#34;--pssm_multi&#34;, type=float, default=0.0, help=&#34;A value between [0.0, 1.0], 0.0 means do not use pssm, 1.0 ignore MPNN predictions&#34;)&#xA;    argparser.add_argument(&#34;--pssm_threshold&#34;, type=float, default=0.0, help=&#34;A value between -inf + inf to restric per position AAs&#34;)&#xA;    argparser.add_argument(&#34;--pssm_log_odds_flag&#34;, type=int, default=0, help=&#34;0 for False, 1 for True&#34;)&#xA;    argparser.add_argument(&#34;--pssm_bias_flag&#34;, type=int, default=0, help=&#34;0 for False, 1 for True&#34;)&#xA;    argparser.add_argument(&#34;--tied_positions_jsonl&#34;, type=str, default=&#39;&#39;, help=&#34;Path to a dictionary with tied positions&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;For example to make a conda environment to run ProteinMPNN:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;conda create --name mlfold&lt;/code&gt; - this creates conda environment called &lt;code&gt;mlfold&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;source activate mlfold&lt;/code&gt; - this activate environment&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch&lt;/code&gt; - install pytorch following steps from &lt;a href=&#34;https://pytorch.org/&#34;&gt;https://pytorch.org/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;These are provided &lt;code&gt;examples/&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;submit_example_1.sh&lt;/code&gt; - simple monomer example&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;submit_example_2.sh&lt;/code&gt; - simple multi-chain example&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;submit_example_3.sh&lt;/code&gt; - directly from the .pdb path&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;submit_example_3_score_only.sh&lt;/code&gt; - return score only (model&#39;s uncertainty)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;submit_example_4.sh&lt;/code&gt; - fix some residue positions&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;submit_example_4_non_fixed.sh&lt;/code&gt; - specify which positions to design&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;submit_example_5.sh&lt;/code&gt; - tie some positions together (symmetry)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;submit_example_6.sh&lt;/code&gt; - homooligomer example&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;submit_example_7.sh&lt;/code&gt; - return sequence unconditional probabilities (PSSM like)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;submit_example_8.sh&lt;/code&gt; - add amino acid bias&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Output example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;3HTN, score=1.1705, global_score=1.2045, fixed_chains=[&#39;B&#39;], designed_chains=[&#39;A&#39;, &#39;C&#39;], model_name=v_48_020, git_hash=015ff820b9b5741ead6ba6795258f35a9c15e94b, seed=37&#xA;NMYSYKKIGNKYIVSINNHTEIVKALNAFCKEKGILSGSINGIGAIGELTLRFFNPKTKAYDDKTFREQMEISNLTGNISSMNEQVYLHLHITVGRSDYSALAGHLLSAIQNGAGEFVVEDYSERISRTYNPDLGLNIYDFER/NMYSYKKIGNKYIVSINNHTEIVKALNAFCKEKGILSGSINGIGAIGELTLRFFNPKTKAYDDKTFREQMEISNLTGNISSMNEQVYLHLHITVGRSDYSALAGHLLSAIQNGAGEFVVEDYSERISRTYNPDLGLNIYDFER&#xA;&amp;gt;T=0.1, sample=1, score=0.7291, global_score=0.9330, seq_recovery=0.5736&#xA;NMYSYKKIGNKYIVSINNHTEIVKALKKFCEEKNIKSGSVNGIGSIGSVTLKFYNLETKEEELKTFNANFEISNLTGFISMHDNKVFLDLHITIGDENFSALAGHLVSAVVNGTCELIVEDFNELVSTKYNEELGLWLLDFEK/NMYSYKKIGNKYIVSINNHTDIVTAIKKFCEDKKIKSGTINGIGQVKEVTLEFRNFETGEKEEKTFKKQFTISNLTGFISTKDGKVFLDLHITFGDENFSALAGHLISAIVDGKCELIIEDYNEEINVKYNEELGLYLLDFNK&#xA;&amp;gt;T=0.1, sample=2, score=0.7414, global_score=0.9355, seq_recovery=0.6075&#xA;NMYKYKKIGNKYIVSINNHTEIVKAIKEFCKEKNIKSGTINGIGQVGKVTLRFYNPETKEYTEKTFNDNFEISNLTGFISTYKNEVFLHLHITFGKSDFSALAGHLLSAIVNGICELIVEDFKENLSMKYDEKTGLYLLDFEK/NMYKYKKIGNKYVVSINNHTEIVEALKAFCEDKKIKSGTVNGIGQVSKVTLKFFNIETKESKEKTFNKNFEISNLTGFISEINGEVFLHLHITIGDENFSALAGHLLSAVVNGEAILIVEDYKEKVNRKYNEELGLNLLDFNL&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;score&lt;/code&gt; - average over residues that were designed negative log probability of sampled amino acids&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;global score&lt;/code&gt; - average over all residues in all chains negative log probability of sampled/fixed amino acids&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;fixed_chains&lt;/code&gt; - chains that were not designed (fixed)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;designed_chains&lt;/code&gt; - chains that were redesigned&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model_name/CA_model_name&lt;/code&gt; - model name that was used to generate results, e.g. &lt;code&gt;v_48_020&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;git_hash&lt;/code&gt; - github version that was used to generate outputs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;seed&lt;/code&gt; - random seed&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;T=0.1&lt;/code&gt; - temperature equal to 0.1 was used to sample sequences&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sample&lt;/code&gt; - sequence sample number 1, 2, 3...etc&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>nicolai256/Stable-textual-inversion_win</title>
    <updated>2022-09-19T01:38:34Z</updated>
    <id>tag:github.com,2022-09-19:/nicolai256/Stable-textual-inversion_win</id>
    <link href="https://github.com/nicolai256/Stable-textual-inversion_win" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;credits for the original script go to &lt;a href=&#34;https://github.com/rinongal/textual_inversion&#34;&gt;https://github.com/rinongal/textual_inversion&lt;/a&gt;, my repo is another implementation&lt;/p&gt; &#xA;&lt;p&gt;please read this tutorial to gain some knowledge on how it works &lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/wvzr7s/tutorial_fine_tuning_stable_diffusion_using_only/&#34;&gt;https://www.reddit.com/r/StableDiffusion/comments/wvzr7s/tutorial_fine_tuning_stable_diffusion_using_only/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;changelog:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; added support for windows&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; added support for img2img + textual inversion&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; added colab notebook that works on free colab for training textual inversion&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; made fork stable-diffusion-dream repo to support textual inversion etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; fixed saving last.ckpt and embeddings.pt every 500 steps&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; fixed merge_embeddings.pt&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; fixed resuming training&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; added squarize outpainting images&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;start with installing stable diffusion dependencies&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;you need to install a couple extra things on top of the ldm env for this to work&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install setuptools==59.5.0&#xA;pip install pillow==9.0.1&#xA;pip install torchmetrics==0.6.0&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;training&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;under 11/12gb vram gpu&#39;s training will not work &lt;em&gt;(for now atleast)&lt;/em&gt; but you can use the colab notebook (you&#39;ll see it when u scroll down)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;to pause training you can double click inside your command prompt&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py \&#xA; --base configs/stable-diffusion/v1-finetune.yaml \&#xA; -t --no-test \&#xA; --actual_resume &#34;SD/checkpoint/path&#34; \&#xA; --gpus 0,  \&#xA; --data_root &#34;C:\path\to\images&#34; \&#xA; --init_word &#34;keyword&#34; \&#xA; -n &#34;projectname&#34; \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;if u get a out of memory error try &lt;code&gt;--base configs/stable-diffusion/v1-finetune_lowmemory.yaml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;you can follow the progress of your training by looking at the images in this folder logs/datasetname model time projectname/images.&lt;/li&gt; &#xA; &lt;li&gt;it trains forever until u stop it so just stop the training whenever ur happy with the result images in logs/randomname/images&lt;/li&gt; &#xA; &lt;li&gt;for small datasets 3000-7000 steps are enough, all of this depends depends on the size of the dataset though. (check in the images folder to see if it&#39;s good)&lt;/li&gt; &#xA; &lt;li&gt;u can stop the training by doing Ctrl+C and it will create a checkpoint.&lt;/li&gt; &#xA; &lt;li&gt;you can resume training from that checkpoint (look under this)&lt;/li&gt; &#xA; &lt;li&gt;results of the resumed checkpoint will be saved in the original checkpoint path but will not export the test images due to there already being test images in there, if you want test images specify a new path with -p logs/newpath&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;resuming&lt;/strong&gt; (make sure your path is specified like this &lt;code&gt;path/path/path&lt;/code&gt; and not like this &lt;code&gt;path\path\path&lt;/code&gt; when resuming)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;!python &#34;main.py&#34; \&#xA; --base &#34;configs/stable-diffusion/v1-finetune.yaml&#34; \&#xA; -t --no-test \&#xA; --actual_resume &#34; models/ldm/stable-diffusion-v1/model.ckpt&#34; \&#xA; --gpus 0 \&#xA; --data_root &#34;C:/path/to/training/images&#34; \&#xA; --init_word &#34;keyword u used when training&#34; \&#xA; --project &#34;logs/training images2022-08-28T07-55-48_myProjectName&#34; \&#xA; --embedding_manager_ckpt &#34;&#34;logs/datasetname model time projectname/checkpoints/embeddings.pt&#34; \&#xA; --resume_from_checkpoint &#34;logs/datasetname model time projectname/checkpoints/last.ckpt&#34; \&#xA; -n &#34;myProjectName2&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;merge trained models together&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;(make sure you use different symbols in placeholder_strings: [&#34;*&#34;] (in the .yaml file while trainig) if u want to use this)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python merge_embeddings.py --manager_ckpts /path/to/first/embedding.pt /path/to/second/embedding.pt [...] --output_path /path/to/output/embedding.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;colab notebook for training if your gpu is not good enough to train. (free colab version works)&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1MggyUS5BWyNdoXpzGkroKgVoKlqJm7vI?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1MggyUS5BWyNdoXpzGkroKgVoKlqJm7vI?usp=sharing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;use this repo for runpod&lt;/strong&gt; &lt;a href=&#34;https://github.com/GamerUntouch/textual_inversion&#34;&gt;https://github.com/GamerUntouch/textual_inversion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;generating&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;for image easy image generation use this repo (text weights + txt2img + img2img + Textual Inversion all supported at once)&lt;/strong&gt; &lt;a href=&#34;https://github.com/lstein/stable-diffusion&#34;&gt;https://github.com/lstein/stable-diffusion&lt;/a&gt; windows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python ./scripts/dream.py --embedding_path /path/to/embedding.pt --full_precision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 ./scripts/dream.py --embedding_path /path/to/embedding.pt --full_precision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.01618&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2208.01618-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;Project Website&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion&lt;/strong&gt;&lt;br&gt; Rinon Gal&lt;sup&gt;1,2&lt;/sup&gt;, Yuval Alaluf&lt;sup&gt;1&lt;/sup&gt;, Yuval Atzmon&lt;sup&gt;2&lt;/sup&gt;, Or Patashnik&lt;sup&gt;1&lt;/sup&gt;, Amit H. Bermano&lt;sup&gt;1&lt;/sup&gt;, Gal Chechik&lt;sup&gt;2&lt;/sup&gt;, Daniel Cohen-Or&lt;sup&gt;1&lt;/sup&gt; &lt;br&gt; &lt;sup&gt;1&lt;/sup&gt;Tel Aviv University, &lt;sup&gt;2&lt;/sup&gt;NVIDIA&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: &lt;br&gt; Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn &lt;i&gt;our&lt;/i&gt; cat into a painting, or imagine a new product based on &lt;i&gt;our&lt;/i&gt; favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new &#34;words&#34; in the embedding space of a frozen text-to-image model. These &#34;words&#34; can be composed into natural language sentences, guiding &lt;i&gt;personalized&lt;/i&gt; creation in an intuitive way. Notably, we find evidence that a &lt;i&gt;single&lt;/i&gt; word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains the official code, data and sample inversions for our Textual Inversion paper.&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;21/08/2022 (C)&lt;/strong&gt; Code released!&lt;/p&gt; &#xA;&lt;h2&gt;TODO:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release code!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Optimize gradient storing / checkpointing. Memory requirements, training times reduced by ~55%&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release data sets&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release pre-trained embeddings&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add Stable Diffusion support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Our code builds on, and shares requirements with &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Latent Diffusion Models (LDM)&lt;/a&gt;. To set up their environment, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will also need the official LDM text-to-image checkpoint, available through the &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;LDM project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Currently, the model can be downloaded by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p models/ldm/text2img-large/&#xA;wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Inversion&lt;/h3&gt; &#xA;&lt;p&gt;To invert an image set, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --base configs/latent-diffusion/txt2img-1p4B-finetune.yaml &#xA;               -t &#xA;               --actual_resume /path/to/pretrained/model.ckpt &#xA;               -n &amp;lt;run_name&amp;gt; &#xA;               --gpus 0, &#xA;               --data_root /path/to/directory/with/images&#xA;               --init_word &amp;lt;initialization_word&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where the initialization word should be a single-token rough description of the object (e.g., &#39;toy&#39;, &#39;painting&#39;, &#39;sculpture&#39;). If the input is comprised of more than a single token, you will be prompted to replace it.&lt;/p&gt; &#xA;&lt;p&gt;In the paper, we use 5k training iterations. However, some concepts (particularly styles) can converge much faster.&lt;/p&gt; &#xA;&lt;p&gt;To run on multiple GPUs, provide a comma-delimited list of GPU indices to the --gpus argument (e.g., &lt;code&gt;--gpus 0,3,7,8&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Embeddings and output images will be saved in the log directory.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;configs/latent-diffusion/txt2img-1p4B-finetune.yaml&lt;/code&gt; for more options, such as changing the placeholder string which denotes the concept (defaults to &#34;*&#34;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt; All training set images should be upright. If you are using phone captured images, check the inputs_gs*.jpg files in the output image directory and make sure they are oriented correctly. Many phones capture images with a 90 degree rotation and denote this in the image metadata. Windows parses these correctly, but PIL does not. Hence you will need to correct them manually (e.g. by pasting them into paint and re-saving) or wait until we add metadata parsing.&lt;/p&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;p&gt;To generate new images of the learned concept, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --ddim_eta 0.0 &#xA;                          --n_samples 8 &#xA;                          --n_iter 2 &#xA;                          --scale 10.0 &#xA;                          --ddim_steps 50 &#xA;                          --embedding_path /path/to/logs/trained_model/checkpoints/embeddings_gs-5049.pt &#xA;                          --ckpt_path /path/to/pretrained/model.ckpt &#xA;                          --prompt &#34;a photo of *&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where * is the placeholder string used during inversion.&lt;/p&gt; &#xA;&lt;h3&gt;Merging Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;LDM embedding checkpoints can be merged into a single file by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python merge_embeddings.py &#xA;--manager_ckpts /path/to/first/embedding.pt /path/to/second/embedding.pt [...]&#xA;--output_path /path/to/output/embedding.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the checkpoints contain conflicting placeholder strings, you will be prompted to select new placeholders. The merged checkpoint can later be used to prompt multiple concepts at once (&#34;A photo of * in the style of @&#34;).&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Models / Data&lt;/h3&gt; &#xA;&lt;p&gt;Coming soon&lt;/p&gt; &#xA;&lt;h2&gt;Stable Diffusion&lt;/h2&gt; &#xA;&lt;p&gt;Stable Diffusion support is a work in progress and will be completed soon™.&lt;/p&gt; &#xA;&lt;h2&gt;Tips and Tricks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adding &#34;a photo of&#34; to the prompt usually results in better target consistency.&lt;/li&gt; &#xA; &lt;li&gt;Results can be seed sensititve. If you&#39;re unsatisfied with the model, try re-inverting with a new seed (by adding &lt;code&gt;--seed &amp;lt;#&amp;gt;&lt;/code&gt; to the prompt).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you make use of our work, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{gal2022textual,&#xA;      doi = {10.48550/ARXIV.2208.01618},&#xA;      url = {https://arxiv.org/abs/2208.01618},&#xA;      author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},&#xA;      title = {An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion},&#xA;      publisher = {arXiv},&#xA;      year = {2022},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;Here are some sample results. Please visit our &lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;project page&lt;/a&gt; or read our paper for more!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nicolai256/Stable-textual-inversion_win/main/img/teaser.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nicolai256/Stable-textual-inversion_win/main/img/samples.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nicolai256/Stable-textual-inversion_win/main/img/style.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rafaballerini/AnaliseDaPlaylistDeRock</title>
    <updated>2022-09-19T01:38:34Z</updated>
    <id>tag:github.com,2022-09-19:/rafaballerini/AnaliseDaPlaylistDeRock</id>
    <link href="https://github.com/rafaballerini/AnaliseDaPlaylistDeRock" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Uma associação de como o rock foi se modificando desde a década de 50 até os dias de hoje e como isso pode ser observado na prática nas músicas presentes na minha playlist de rock do Spotify, chamada &#34;For Those About To Rock&#34;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>