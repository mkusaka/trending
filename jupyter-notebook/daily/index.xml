<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-20T01:30:33Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bmurmann/EE628</title>
    <updated>2024-05-20T01:30:33Z</updated>
    <id>tag:github.com,2024-05-20:/bmurmann/EE628</id>
    <link href="https://github.com/bmurmann/EE628" rel="alternate"></link>
    <summary type="html">&lt;p&gt;EE 628: Analysis and Design of Integrated Circuits (University of Hawaiʻi at Mānoa)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EE 628 (University of Hawaiʻi at Mānoa)&lt;/h1&gt; &#xA;&lt;h3&gt;Analysis and Design of Integrated Circuits&lt;/h3&gt; &#xA;&lt;img align=&#34;right&#34; width=&#34;250&#34; height=&#34;250&#34; src=&#34;https://raw.githubusercontent.com/bmurmann/EE628/main/img/voltmeter.jpeg&#34;&gt; &#xA;&lt;p&gt;Learn mixed-signal circuit design using open-source tools and create your own voltmeter chip!&lt;/p&gt; &#xA;&lt;p&gt;This course is being developed in collaboration with the Microelectronics Commons &lt;a href=&#34;https://microelectronicscommons.org/connect/california-pacific-northwest-ai-hardware-hub-northwest-ai-hub/&#34;&gt;California-Pacific-Northwest AI Hub&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>BaranziniLab/KG_RAG</title>
    <updated>2024-05-20T01:30:33Z</updated>
    <id>tag:github.com,2024-05-20:/BaranziniLab/KG_RAG</id>
    <link href="https://github.com/BaranziniLab/KG_RAG" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Empower Large Language Models (LLM) using Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) for knowledge intensive tasks&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/BaranziniLab/KG_RAG/assets/42702311/0b2f5b42-761e-4d5b-8d6f-77c8b965f017&#34; width=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG#what-is-kg-rag&#34;&gt;What is KG-RAG&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG#example-use-case-of-kg-rag&#34;&gt;Example use case of KG-RAG&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG#without-kg-rag&#34;&gt;Prompting GPT without KG-RAG&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG#with-kg-rag&#34;&gt;Prompting GPT with KG-RAG&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG/raw/main/notebooks/kg_rag_based_gpt_prompts.ipynb&#34;&gt;Example notebook for KG-RAG with GPT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG#how-to-run-kg-rag&#34;&gt;How to run KG-RAG&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG#step-1-clone-the-repo&#34;&gt;Step 1: Clone the repo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG#step-2-create-a-virtual-environment&#34;&gt;Step 2: Create a virtual environment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG#step-3-install-dependencies&#34;&gt;Step 3: Install dependencies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG#step-4-update-configyaml&#34;&gt;Step 4: Update config.yaml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG#step-5-run-the-setup-script&#34;&gt;Step 5: Run the setup script&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG#step-6-run-kg-rag-from-your-terminal&#34;&gt;Step 6: Run KG-RAG from your terminal&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG#using-gpt&#34;&gt;Using GPT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG/raw/main/README.md#using-gpt-interactive-mode&#34;&gt;Using GPT interactive mode&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG#using-llama&#34;&gt;Using Llama&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG/raw/main/README.md#using-llama-interactive-mode&#34;&gt;Using Llama interactive mode&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG?tab=readme-ov-file#command-line-arguments-for-kg-rag&#34;&gt;Command line arguments for KG-RAG&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG/raw/main/README.md#citation&#34;&gt;Citation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is KG-RAG?&lt;/h2&gt; &#xA;&lt;p&gt;KG-RAG stands for Knowledge Graph-based Retrieval Augmented Generation.&lt;/p&gt; &#xA;&lt;h3&gt;Start by watching the video of KG-RAG&lt;/h3&gt; &#xA;&lt;video src=&#34;https://github.com/BaranziniLab/KG_RAG/assets/42702311/86e5b8a3-eb58-4648-95a4-271e9c69b4ed&#34; controls=&#34;controls&#34; style=&#34;max-width: 730px;&#34;&gt; &#xA;&lt;/video&gt; &#xA;&lt;p&gt;It is a task agnostic framework that combines the explicit knowledge of a Knowledge Graph (KG) with the implicit knowledge of a Large Language Model (LLM). Here is the &lt;a href=&#34;https://arxiv.org/abs/2311.17330&#34;&gt;arXiv preprint&lt;/a&gt; of the work.&lt;/p&gt; &#xA;&lt;p&gt;Here, we utilize a massive biomedical KG called &lt;a href=&#34;https://spoke.ucsf.edu/&#34;&gt;SPOKE&lt;/a&gt; as the provider for the biomedical context. SPOKE has incorporated over 40 biomedical knowledge repositories from diverse domains, each focusing on biomedical concept like genes, proteins, drugs, compounds, diseases, and their established connections. SPOKE consists of more than 27 million nodes of 21 different types and 53 million edges of 55 types [&lt;a href=&#34;https://doi.org/10.1093/bioinformatics/btad080&#34;&gt;Ref&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;The main feature of KG-RAG is that it extracts &#34;prompt-aware context&#34; from SPOKE KG, which is defined as:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;the minimal context sufficient enough to respond to the user prompt.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Hence, this framework empowers a general-purpose LLM by incorporating an optimized domain-specific &#39;prompt-aware context&#39; from a biomedical KG.&lt;/p&gt; &#xA;&lt;h2&gt;Example use case of KG-RAG&lt;/h2&gt; &#xA;&lt;p&gt;Following snippet shows the news from FDA &lt;a href=&#34;https://www.fda.gov/drugs/news-events-human-drugs/fda-approves-treatment-weight-management-patients-bardet-biedl-syndrome-aged-6-or-older&#34;&gt;website&lt;/a&gt; about the drug &lt;strong&gt;&#34;setmelanotide&#34;&lt;/strong&gt; approved by FDA for weight management in patients with &lt;em&gt;Bardet-Biedl Syndrome&lt;/em&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/BaranziniLab/KG_RAG/assets/42702311/fc4d0b8d-0edb-461d-86c5-9d0d191bd97d&#34; width=&#34;600&#34; height=&#34;350&#34;&gt; &#xA;&lt;h3&gt;Ask GPT-4 about the above drug:&lt;/h3&gt; &#xA;&lt;h3&gt;WITHOUT KG-RAG&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: This example was run using KG-RAG v0.3.0. We are prompting GPT from the terminal, NOT from the chatGPT browser. Temperature parameter is set to 0 for all the analysis. Refer &lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG/raw/main/config.yaml&#34;&gt;this&lt;/a&gt; yaml file for parameter setting&lt;/em&gt;&lt;/p&gt; &#xA;&lt;video src=&#34;https://github.com/BaranziniLab/KG_RAG/assets/42702311/dbabb812-2a8a-48b6-9785-55b983cb61a4&#34; controls=&#34;controls&#34; style=&#34;max-width: 730px;&#34;&gt; &#xA;&lt;/video&gt; &#xA;&lt;h3&gt;WITH KG-RAG&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: This example was run using KG-RAG v0.3.0. Temperature parameter is set to 0 for all the analysis. Refer &lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG/raw/main/config.yaml&#34;&gt;this&lt;/a&gt; yaml file for parameter setting&lt;/em&gt;&lt;/p&gt; &#xA;&lt;video src=&#34;https://github.com/BaranziniLab/KG_RAG/assets/42702311/acd08954-a496-4a61-a3b1-8fc4e647b2aa&#34; controls=&#34;controls&#34; style=&#34;max-width: 730px;&#34;&gt; &#xA;&lt;/video&gt; &#xA;&lt;p&gt;You can see that, KG-RAG was able to give the correct information about the FDA approved &lt;a href=&#34;https://www.fda.gov/drugs/news-events-human-drugs/fda-approves-treatment-weight-management-patients-bardet-biedl-syndrome-aged-6-or-older&#34;&gt;drug&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to run KG-RAG&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: At the moment, KG-RAG is specifically designed for running prompts related to Diseases. We are actively working on improving its versatility.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Step 1: Clone the repo&lt;/h3&gt; &#xA;&lt;p&gt;Clone this repository. All Biomedical data used in the paper are uploaded to this repository, hence you don&#39;t have to download that separately.&lt;/p&gt; &#xA;&lt;h3&gt;Step 2: Create a virtual environment&lt;/h3&gt; &#xA;&lt;p&gt;Note: Scripts in this repository were run using python 3.10.9&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n kg_rag python=3.10.9&#xA;conda activate kg_rag&#xA;cd KG_RAG&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 3: Install dependencies&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 4: Update config.yaml&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG/raw/main/config.yaml&#34;&gt;config.yaml&lt;/a&gt; holds all the necessary information required to run the scripts in your machine. Make sure to populate &lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG/raw/main/config.yaml&#34;&gt;this&lt;/a&gt; yaml file accordingly.&lt;/p&gt; &#xA;&lt;p&gt;Note: There is another yaml file called &lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG/raw/main/system_prompts.yaml&#34;&gt;system_prompts.yaml&lt;/a&gt;. This is already populated and it holds all the system prompts used in the KG-RAG framework.&lt;/p&gt; &#xA;&lt;h3&gt;Step 5: Run the setup script&lt;/h3&gt; &#xA;&lt;p&gt;Note: Make sure you are in KG_RAG folder&lt;/p&gt; &#xA;&lt;p&gt;Setup script runs in an interactive fashion.&lt;/p&gt; &#xA;&lt;p&gt;Running the setup script will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;create disease vector database for KG-RAG&lt;/li&gt; &#xA; &lt;li&gt;download Llama model in your machine (optional, you can skip this and that is totally fine)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m kg_rag.run_setup&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 6: Run KG-RAG from your terminal&lt;/h3&gt; &#xA;&lt;p&gt;Note: Make sure you are in KG_RAG folder&lt;/p&gt; &#xA;&lt;p&gt;You can run KG-RAG using GPT and Llama model.&lt;/p&gt; &#xA;&lt;h4&gt;Using GPT&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;# GPT_API_TYPE=&#39;azure&#39;&#xA;python -m kg_rag.rag_based_generation.GPT.text_generation -g &amp;lt;your favorite gpt model - &#34;gpt-4&#34; or &#34;gpt-35-turbo&#34;&amp;gt;&#xA;# GPT_API_TYPE=&#39;openai&#39;&#xA;python -m kg_rag.rag_based_generation.GPT.text_generation -g &amp;lt;your favorite gpt model - &#34;gpt-4&#34; or &#34;gpt-3.5-turbo&#34;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;p&gt;Note: The following example was run on AWS p3.8xlarge EC2 instance and using KG-RAG v0.3.0.&lt;/p&gt; &#xA;&lt;video src=&#34;https://github.com/BaranziniLab/KG_RAG/assets/42702311/defcbff7-e777-4db6-b028-10f54c76b234&#34; controls=&#34;controls&#34; style=&#34;max-width: 730px;&#34;&gt; &#xA;&lt;/video&gt; &#xA;&lt;h4&gt;Using GPT interactive mode&lt;/h4&gt; &#xA;&lt;p&gt;This allows the user to go over each step of the process in an interactive fashion&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# GPT_API_TYPE=&#39;azure&#39;&#xA;python -m kg_rag.rag_based_generation.GPT.text_generation -i True -g &amp;lt;your favorite gpt model - &#34;gpt-4&#34; or &#34;gpt-35-turbo&#34;&amp;gt;&#xA;# GPT_API_TYPE=&#39;openai&#39;&#xA;python -m kg_rag.rag_based_generation.GPT.text_generation -i True -g &amp;lt;your favorite gpt model - &#34;gpt-4&#34; or &#34;gpt-3.5-turbo&#34;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using Llama&lt;/h4&gt; &#xA;&lt;p&gt;Note: If you haven&#39;t downloaded Llama during &lt;a href=&#34;https://github.com/BaranziniLab/KG_RAG#step-5-run-the-setup-script&#34;&gt;setup&lt;/a&gt; step, then when you run the following, it may take sometime since it will download the model first.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m kg_rag.rag_based_generation.Llama.text_generation -m &amp;lt;method-1 or method2, if nothing is mentioned it will take &#39;method-1&#39;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;p&gt;Note: The following example was run on AWS p3.8xlarge EC2 instance and using KG-RAG v0.3.0.&lt;/p&gt; &#xA;&lt;video src=&#34;https://github.com/BaranziniLab/KG_RAG/assets/42702311/94bda923-dafb-451a-943a-1d7c65f3ffd4&#34; controls=&#34;controls&#34; style=&#34;max-width: 730px;&#34;&gt; &#xA;&lt;/video&gt; &#xA;&lt;h4&gt;Using Llama interactive mode&lt;/h4&gt; &#xA;&lt;p&gt;This allows the user to go over each step of the process in an interactive fashion&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m kg_rag.rag_based_generation.Llama.text_generation -i True -m &amp;lt;method-1 or method2, if nothing is mentioned it will take &#39;method-1&#39;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Command line arguments for KG-RAG&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Argument&lt;/th&gt; &#xA;   &lt;th&gt;Default Value&lt;/th&gt; &#xA;   &lt;th&gt;Definition&lt;/th&gt; &#xA;   &lt;th&gt;Allowed Options&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-g&lt;/td&gt; &#xA;   &lt;td&gt;gpt-35-turbo&lt;/td&gt; &#xA;   &lt;td&gt;GPT model selection&lt;/td&gt; &#xA;   &lt;td&gt;gpt models provided by OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;Use only for GPT models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-i&lt;/td&gt; &#xA;   &lt;td&gt;False&lt;/td&gt; &#xA;   &lt;td&gt;Flag for interactive mode (shows step-by-step)&lt;/td&gt; &#xA;   &lt;td&gt;True or False&lt;/td&gt; &#xA;   &lt;td&gt;Can be used for both GPT and Llama models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-e&lt;/td&gt; &#xA;   &lt;td&gt;False&lt;/td&gt; &#xA;   &lt;td&gt;Flag for showing evidence of association from the graph&lt;/td&gt; &#xA;   &lt;td&gt;True or False&lt;/td&gt; &#xA;   &lt;td&gt;Can be used for both GPT and Llama models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-m&lt;/td&gt; &#xA;   &lt;td&gt;method-1&lt;/td&gt; &#xA;   &lt;td&gt;Which tokenizer method to use&lt;/td&gt; &#xA;   &lt;td&gt;method-1 or method-2. method-1 uses &#39;AutoTokenizer&#39; and method-2 uses &#39;LlamaTokenizer&#39; and with an additional &#39;legacy&#39; flag set to False while initiating the tokenizer&lt;/td&gt; &#xA;   &lt;td&gt;Use only for Llama models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{soman2023biomedical,&#xA;  title={Biomedical knowledge graph-enhanced prompt generation for large language models},&#xA;  author={Soman, Karthik and Rose, Peter W and Morris, John H and Akbas, Rabia E and Smith, Brett and Peetoom, Braian and Villouta-Reyes, Catalina and Cerono, Gabriel and Shi, Yongmei and Rizk-Jackson, Angela and others},&#xA;  journal={arXiv preprint arXiv:2311.17330},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>UBC-CS/cpsc330-2024s</title>
    <updated>2024-05-20T01:30:33Z</updated>
    <id>tag:github.com,2024-05-20:/UBC-CS/cpsc330-2024s</id>
    <link href="https://github.com/UBC-CS/cpsc330-2024s" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;UBC CPSC 330: Applied Machine Learning (2024s)&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This is the course homepage for CPSC 330: Applied Machine Learning at the University of British Columbia. You are looking at the 2024S1 version, May-June 2024.&lt;/p&gt; &#xA;&lt;h3&gt;Instructor&lt;/h3&gt; &#xA;&lt;p&gt;Mehrdad Oveisi&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;mailto:moveisi@cs.ubc.ca&#34;&gt;moveisi@cs.ubc.ca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/oveisi/&#34;&gt;LinkedIn.com/in/oveisi&lt;/a&gt; (Feel free to connect on LinkedIn)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=6l2ij0IAAAAJ&#34;&gt;Google Scholar&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Office hours&lt;/strong&gt;&lt;/em&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;When:&lt;/strong&gt; Mon, Wed, Fri from 17:00 to 18:00 (as long as there are questions)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Where:&lt;/strong&gt; &lt;a href=&#34;https://ssc.adm.ubc.ca/classroomservices/function/viewlocation?userEvent=ShowLocation&amp;amp;buildingID=MCLD&amp;amp;roomID=2018&#34;&gt;MCLD 2018&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Who:&lt;/strong&gt; Students form both sections are welcome to attend all office hours.&lt;/li&gt; &#xA;   &lt;li&gt;For &lt;strong&gt;more details&lt;/strong&gt; see &lt;a href=&#34;https://github.com/UBC-CS/cpsc330-2024s/raw/main/syllabus.md#class-meetings&#34;&gt;class meetings on syllabus&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Class Schedule&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Section&lt;/th&gt; &#xA;   &lt;th&gt;Location&lt;/th&gt; &#xA;   &lt;th&gt;Day&lt;/th&gt; &#xA;   &lt;th&gt;Lecture&lt;/th&gt; &#xA;   &lt;th&gt;Office Hour&lt;/th&gt; &#xA;   &lt;th&gt;OH Held By&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;911&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ssc.adm.ubc.ca/classroomservices/function/viewlocation?userEvent=ShowLocation&amp;amp;buildingID=DMP&amp;amp;roomID=310&#34;&gt;DMP 310&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mon, Wed, Fri&lt;/td&gt; &#xA;   &lt;td&gt;10:00 - 13:00&lt;/td&gt; &#xA;   &lt;td&gt;13:00 - 14:00&lt;/td&gt; &#xA;   &lt;td&gt;TAs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;912&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ssc.adm.ubc.ca/classroomservices/function/viewlocation?userEvent=ShowLocation&amp;amp;buildingID=MCLD&amp;amp;roomID=2018&#34;&gt;MCLD 2018&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mon, Wed, Fri&lt;/td&gt; &#xA;   &lt;td&gt;14:00 - 17:00&lt;/td&gt; &#xA;   &lt;td&gt;17:00 - 18:00&lt;/td&gt; &#xA;   &lt;td&gt;TAs and Instructor&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For &lt;strong&gt;more details&lt;/strong&gt; see &lt;a href=&#34;https://github.com/UBC-CS/cpsc330-2024s/raw/main/syllabus.md#class-meetings&#34;&gt;class meetings on syllabus&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course Coordinator&lt;/h3&gt; &#xA;&lt;p&gt;Liam Salt&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;mailto:cpsc330-admin@cs.ubc.ca&#34;&gt;cpsc330-admin@cs.ubc.ca&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please email Liam Salt at the above email address for all administrative concerns such as CFA accommodations or exemptions due to sickness or extenuating circumstances.&lt;/p&gt; &#xA;&lt;h3&gt;Teaching Assistants&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please see &lt;a href=&#34;https://github.com/UBC-CS/cpsc330-2023W1/raw/main/syllabus.md#tas&#34;&gt;TAs on syllabus&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;© 2023 Varada Kolhatkar and Mike Gelbart&lt;/p&gt; &#xA;&lt;p&gt;Software licensed under &lt;a href=&#34;https://spdx.org/licenses/MIT.html&#34;&gt;the MIT License&lt;/a&gt;, non-software content licensed under &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) License&lt;/a&gt;. See the &lt;a href=&#34;https://raw.githubusercontent.com/UBC-CS/cpsc330-2024s/main/LICENSE.md&#34;&gt;license file&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Important links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/UBC-CS/cpsc330-2024s/main/syllabus.md&#34;&gt;Syllabus / administrative info&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UBC-CS/cpsc330-2024s/tree/main?tab=readme-ov-file#deliverable-due-dates-tentative&#34;&gt;Calendar and due dates&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLHofvQE1VlGtZoAULxcHb7lOsMved0CuM&#34;&gt;Course videos YouTube channel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ca.prairielearn.com/pl/course_instance/7786&#34;&gt;PrairieLearn&lt;/a&gt; (course assessments)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://canvas.ubc.ca/courses/140666&#34;&gt;Canvas&lt;/a&gt; (homework solutions, etc.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://piazza.com/class/lvo65lmwh53527&#34;&gt;Piazza&lt;/a&gt; (course &lt;strong&gt;announcements&lt;/strong&gt; and discussions)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UBC-CS/cpsc330-2024s/raw/main/docs/setup.md&#34;&gt;&lt;em&gt;Setting up coding environment&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UBC-CS/cpsc330-2024s/tree/main/docs&#34;&gt;Other course documents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UBC-CS/cpsc330-2024s/raw/main/syllabus.md#iclicker&#34;&gt;iClicker Cloud&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compact course schedule (tentative)&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;IMPORTANT NOTE&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;As a general rule, summer terms are quite compact and thus time management is crucial to keep up with the course content and the deadlines. More precisely, based on the &lt;a href=&#34;https://vancouver.calendar.ubc.ca/dates-and-deadlines&#34;&gt;university calendar&lt;/a&gt;, the number of &lt;em&gt;Teaching Days&lt;/em&gt; is 63 in winter terms and it is 28 in summer terms. That means there will be 2.25 (63÷28) times more content to learn per week, and &lt;strong&gt;2.25 times faster pace&lt;/strong&gt; for the homework due dates. In other words, you are expected to learn and deliver the same amount of work compared to winter terms, but do it 2.25 times faster! For this reason, &lt;em&gt;&lt;strong&gt;time management is of utmost importance in order to succeed in the course&lt;/strong&gt;&lt;/em&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;The following chart is a very compact version of the course tentative schedule.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/UBC-CS/cpsc330-2024s/main/docs/img/cpsc330_2024s_tentative_schedule.png&#34; alt=&#34;tentative schedule&#34; width=&#34;600&#34;&gt; &#xA;&lt;p&gt;&amp;nbsp; &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;The following sections provide for more detailed course schedule. &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Deliverable due dates (&lt;em&gt;&lt;strong&gt;tentative&lt;/strong&gt;&lt;/em&gt;)&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Assessment&lt;/th&gt; &#xA;   &lt;th&gt;Due date&lt;/th&gt; &#xA;   &lt;th&gt;Where to find?&lt;/th&gt; &#xA;   &lt;th&gt;Where to submit?&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Syllabus quiz&lt;/td&gt; &#xA;   &lt;td&gt;May 17, 23:00&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ca.prairielearn.com/pl/course_instance/7786&#34;&gt;PrairieLearn&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PrairieLearn&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hw1&lt;/td&gt; &#xA;   &lt;td&gt;May 18, 23:00&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/UBC-CS/cpsc330-2024s/tree/main/hw/&#34;&gt;Github repo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PrairieLearn&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hw2&lt;/td&gt; &#xA;   &lt;td&gt;May 21, 23:00&lt;/td&gt; &#xA;   &lt;td&gt;Github repo&lt;/td&gt; &#xA;   &lt;td&gt;PrairieLearn&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hw3&lt;/td&gt; &#xA;   &lt;td&gt;May 25, 23:00&lt;/td&gt; &#xA;   &lt;td&gt;Github repo&lt;/td&gt; &#xA;   &lt;td&gt;PrairieLearn&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hw4&lt;/td&gt; &#xA;   &lt;td&gt;May 30, 23:00&lt;/td&gt; &#xA;   &lt;td&gt;Github repo&lt;/td&gt; &#xA;   &lt;td&gt;PrairieLearn&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Midterm&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;em&gt;June 4&lt;/em&gt;, 18:00&lt;/strong&gt;&lt;br&gt;Details on Piazza&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ca.prairietest.com&#34;&gt;PrairieTest&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PrairieTest (in-person)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hw5&lt;/td&gt; &#xA;   &lt;td&gt;June 06, 23:00&lt;/td&gt; &#xA;   &lt;td&gt;Github repo&lt;/td&gt; &#xA;   &lt;td&gt;PrairieLearn&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hw6&lt;/td&gt; &#xA;   &lt;td&gt;June 11, 23:00&lt;/td&gt; &#xA;   &lt;td&gt;Github repo&lt;/td&gt; &#xA;   &lt;td&gt;PrairieLearn&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hw7&lt;/td&gt; &#xA;   &lt;td&gt;June 15, 23:00&lt;/td&gt; &#xA;   &lt;td&gt;Github repo&lt;/td&gt; &#xA;   &lt;td&gt;PrairieLearn&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hw8&lt;/td&gt; &#xA;   &lt;td&gt;June 20, 23:00&lt;/td&gt; &#xA;   &lt;td&gt;Github repo&lt;/td&gt; &#xA;   &lt;td&gt;PrairieLearn&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Final exam&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;[TBA]&lt;/strong&gt;&lt;br&gt;&lt;a href=&#34;https://students.ubc.ca/enrolment/exams/exam-schedule&#34;&gt;Final exam schedule&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PrairieTest&lt;/td&gt; &#xA;   &lt;td&gt;PrairieTest (in-person)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Final exam &lt;em&gt;viewing&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;[TBA]&lt;/strong&gt; Perhaps before July 12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://cbtf.ubc.ca/&#34;&gt;CBTF&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;In-person only (online not possible)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Lecture schedule (tentative)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lectures&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The lectures will be in-person (see &lt;em&gt;Class Schedule&lt;/em&gt; above for more details).&lt;/li&gt; &#xA; &lt;li&gt;All lecture files are subject to change without notice up until they are covered in class.&lt;/li&gt; &#xA; &lt;li&gt;You are expected to watch the &#34;Pre-watch&#34; videos before each lecture.&lt;/li&gt; &#xA; &lt;li&gt;You are expected to attend the lectures.&lt;/li&gt; &#xA; &lt;li&gt;You will find the lecture notes under the &lt;a href=&#34;https://raw.githubusercontent.com/UBC-CS/cpsc330-2024s/main/lectures/mehrdad&#34;&gt;lectures&lt;/a&gt; in this repository. Lectures will be posted/updated as they become available.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lectures&lt;/th&gt; &#xA;   &lt;th&gt;Date&lt;/th&gt; &#xA;   &lt;th&gt;Topic&lt;/th&gt; &#xA;   &lt;th&gt;Assigned videos&lt;/th&gt; &#xA;   &lt;th&gt;vs. CPSC 340&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;01&lt;/td&gt; &#xA;   &lt;td&gt;May 13&lt;/td&gt; &#xA;   &lt;td&gt;Course intro&lt;/td&gt; &#xA;   &lt;td&gt;📹 &lt;li&gt;Pre-watch: None&lt;/li&gt;&lt;li&gt;Recap video (after lecture): &lt;a href=&#34;https://youtu.be/-1hTcS5ZE4w&#34;&gt;1.0&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;n/a&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Part I: ML fundamentals and preprocessing&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;02&lt;/td&gt; &#xA;   &lt;td&gt;May 13 &amp;amp; 15&lt;/td&gt; &#xA;   &lt;td&gt;Decision trees&lt;/td&gt; &#xA;   &lt;td&gt;📹 &lt;li&gt;Pre-watch: &lt;a href=&#34;https://youtu.be/YNT8n4cXu4A&#34;&gt;2.1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/6eT5cLL-2Vc&#34;&gt;2.2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;After lecture: &lt;a href=&#34;https://youtu.be/Hcf19Ij35rA&#34;&gt;2.3&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/KEtsfXn4w2E&#34;&gt;2.4&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;less depth&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;03&lt;/td&gt; &#xA;   &lt;td&gt;May 15&lt;/td&gt; &#xA;   &lt;td&gt;ML fundamentals&lt;/td&gt; &#xA;   &lt;td&gt;📹 &lt;li&gt; Pre-watch: &lt;a href=&#34;https://youtu.be/iS2hsRRlc2M&#34;&gt;3.1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/h2AEobwcUQw&#34;&gt;3.2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;After lecture: &lt;a href=&#34;https://youtu.be/4cv8VYonepA&#34;&gt;3.3&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/Ihay8yE5KTI&#34;&gt;3.4&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;similar&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;04&lt;/td&gt; &#xA;   &lt;td&gt;May 17&lt;/td&gt; &#xA;   &lt;td&gt;$k$-NNs and SVM with RBF kernel&lt;/td&gt; &#xA;   &lt;td&gt;📹 &lt;li&gt; Pre-watch: &lt;a href=&#34;https://youtu.be/hCa3EXEUmQk&#34;&gt;4.1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/bENDqXKJLmg&#34;&gt;4.2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;After lecture: &lt;a href=&#34;https://youtu.be/IRGbqi5S9gQ&#34;&gt;4.3&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/ic_zqOhi020&#34;&gt;4.4&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;less depth&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;05&lt;/td&gt; &#xA;   &lt;td&gt;May 17 &amp;amp; 22&lt;/td&gt; &#xA;   &lt;td&gt;Preprocessing, &lt;code&gt;sklearn&lt;/code&gt; pipelines&lt;/td&gt; &#xA;   &lt;td&gt;📹 &lt;li&gt; Pre-watch: &lt;a href=&#34;https://youtu.be/xx9HlmzORRk&#34;&gt;5.1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/G2IXbVzKlt8&#34;&gt;5.2&lt;/a&gt;&lt;/li&gt;&lt;li&gt;After lecture: &lt;a href=&#34;https://youtu.be/nWTce7WJSd4&#34;&gt;5.3&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/2mJ9rAhMMl0&#34;&gt;5.4&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;more depth&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;06&lt;/td&gt; &#xA;   &lt;td&gt;May 22&lt;/td&gt; &#xA;   &lt;td&gt;More preprocessing, &lt;code&gt;sklearn&lt;/code&gt; &lt;code&gt;ColumnTransformer&lt;/code&gt;, text features&lt;/td&gt; &#xA;   &lt;td&gt;📹 &lt;li&gt; Pre-watch: &lt;a href=&#34;https://youtu.be/to2mukSyvLk&#34;&gt;6.1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/hteVvLwrWZ4&#34;&gt;6.2&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;more depth&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;07&lt;/td&gt; &#xA;   &lt;td&gt;May 24&lt;/td&gt; &#xA;   &lt;td&gt;Linear models&lt;/td&gt; &#xA;   &lt;td&gt;📹 &lt;li&gt; Pre-watch: &lt;a href=&#34;https://youtu.be/HXd1U2q4VFA&#34;&gt;7.1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/56L5z_t22qE&#34;&gt;7.2&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/_OAK5KiGLg0&#34;&gt;7.3&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;less depth&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;08&lt;/td&gt; &#xA;   &lt;td&gt;May 24 &amp;amp; 27&lt;/td&gt; &#xA;   &lt;td&gt;Hyperparameter optimization, overfitting the validation set&lt;/td&gt; &#xA;   &lt;td&gt;📹 &lt;li&gt; Pre-watch: &lt;a href=&#34;https://youtu.be/lMWdHZSZMk8&#34;&gt;8.1&lt;/a&gt;,&lt;a href=&#34;https://youtu.be/Z9a9XZ0vQv0&#34;&gt;8.2&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;different&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;09&lt;/td&gt; &#xA;   &lt;td&gt;May 27&lt;/td&gt; &#xA;   &lt;td&gt;Evaluation metrics for classification&lt;/td&gt; &#xA;   &lt;td&gt;📹 &lt;li&gt; Pre-watch: &lt;a href=&#34;https://youtu.be/ZCuCErW5lI8&#34;&gt;9.2&lt;/a&gt;,&lt;a href=&#34;https://youtu.be/XkCTUuoH23c&#34;&gt;9.3&lt;/a&gt;,&lt;a href=&#34;https://youtu.be/jHaKRCFb6Qw&#34;&gt;9.4&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;more depth&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;May 29&lt;/td&gt; &#xA;   &lt;td&gt;Regression metrics&lt;/td&gt; &#xA;   &lt;td&gt;📹 &lt;li&gt;Pre-watch: &lt;a href=&#34;https://youtu.be/lgGTKLwNgkQ&#34;&gt;10.1&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;more depth on metrics less depth on regression&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;May 31&lt;/td&gt; &#xA;   &lt;td&gt;Ensembles&lt;/td&gt; &#xA;   &lt;td&gt;📹 &lt;li&gt;Pre-watch: &lt;a href=&#34;https://youtu.be/8litm1H7DLo&#34;&gt;11.1&lt;/a&gt;,&lt;a href=&#34;https://youtu.be/EkFkY9QB2Hw&#34;&gt;11.2&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;similar&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;May 31 &amp;amp; Jun 3&lt;/td&gt; &#xA;   &lt;td&gt;Feature importances, model interpretation&lt;/td&gt; &#xA;   &lt;td&gt;📹 &lt;li&gt;Pre-watch: &lt;a href=&#34;https://youtu.be/xfICsGL7DXE&#34;&gt;12.1&lt;/a&gt;,&lt;a href=&#34;https://youtu.be/tiSN18OmZOo&#34;&gt;12.2&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;feature importances is new, feature engineering is new&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;Jun 3&lt;/td&gt; &#xA;   &lt;td&gt;Feature engineering and feature selection&lt;/td&gt; &#xA;   &lt;td&gt;None&lt;/td&gt; &#xA;   &lt;td&gt;less depth&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;[TBA]&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Midterm&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Part II: Unsupervised learning, transfer learning, different learning settings&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;Jun 5&lt;/td&gt; &#xA;   &lt;td&gt;Clustering&lt;/td&gt; &#xA;   &lt;td&gt;📹 &lt;li&gt;Pre-watch: &lt;a href=&#34;https://youtu.be/caAuUAXwpb8&#34;&gt;14.1&lt;/a&gt;,&lt;a href=&#34;https://youtu.be/s6AvSZ1_l7I&#34;&gt;14.2&lt;/a&gt;,&lt;a href=&#34;https://youtu.be/M5ilrhcL0oY&#34;&gt;14.3&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;less depth&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15&lt;/td&gt; &#xA;   &lt;td&gt;Jun 5 &amp;amp; 7&lt;/td&gt; &#xA;   &lt;td&gt;More clustering&lt;/td&gt; &#xA;   &lt;td&gt;&lt;li&gt; Post-lecture: &lt;a href=&#34;https://youtu.be/1ZwITQyWpkY&#34;&gt;15.1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/T4NLsrUaRtg&#34;&gt;15.2&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/NM8lFKFZ2IU&#34;&gt;15.3&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;less depth&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;Jun 7&lt;/td&gt; &#xA;   &lt;td&gt;Simple recommender systems&lt;/td&gt; &#xA;   &lt;td&gt;None&lt;/td&gt; &#xA;   &lt;td&gt;less depth&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17&lt;/td&gt; &#xA;   &lt;td&gt;Jun 10&lt;/td&gt; &#xA;   &lt;td&gt;Text data, embeddings, topic modeling&lt;/td&gt; &#xA;   &lt;td&gt;📹 &lt;li&gt;Pre-watch: &lt;a href=&#34;https://youtu.be/GTC_iLPCjdY&#34;&gt;16.1&lt;/a&gt;,&lt;a href=&#34;https://youtu.be/7W5Q8gzNPBc&#34;&gt;16.2&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;new&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18&lt;/td&gt; &#xA;   &lt;td&gt;Jun 10 &amp;amp; 12&lt;/td&gt; &#xA;   &lt;td&gt;Neural networks and computer vision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;less depth&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19&lt;/td&gt; &#xA;   &lt;td&gt;Jun 12&lt;/td&gt; &#xA;   &lt;td&gt;Time series data&lt;/td&gt; &#xA;   &lt;td&gt;(Optional) &lt;a href=&#34;https://www.youtube.com/watch?v=-5wpm-gesOY&#34;&gt;Humour: The Problem with Time &amp;amp; Timezones&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;new&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;Jun 14&lt;/td&gt; &#xA;   &lt;td&gt;Survival analysis&lt;/td&gt; &#xA;   &lt;td&gt;📹 (Optional but highly recommended) &lt;a href=&#34;https://www.youtube.com/watch?v=ITWQ5psx9Sw&#34;&gt;Calling Bullshit 4.1: Right Censoring&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;new&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Part III: Communication, ethics, deployment&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21&lt;/td&gt; &#xA;   &lt;td&gt;Jun 17&lt;/td&gt; &#xA;   &lt;td&gt;Ethics&lt;/td&gt; &#xA;   &lt;td&gt;📹 (Optional but highly recommended) &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLPnZfvKID1Sje5jWxt-4CSZD7bUI4gSPS&#34;&gt;Calling BS videos&lt;/a&gt; Chapter 5 (6 short videos, 50 min total)&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;http://jtleek.com/ads2020/week-15.html&#34;&gt;The ethics of data science&lt;/a&gt;&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;new&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22&lt;/td&gt; &#xA;   &lt;td&gt;Jun 17 &amp;amp; 19&lt;/td&gt; &#xA;   &lt;td&gt;Communication&lt;/td&gt; &#xA;   &lt;td&gt;📹 (Optional but highly recommended) &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLPnZfvKID1Sje5jWxt-4CSZD7bUI4gSPS&#34;&gt;Calling BS videos&lt;/a&gt; Chapter 6 (6 short videos, 47 min total)&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=vbDObzI-CTc&#34;&gt;Can you read graphs? Because I can&#39;t.&lt;/a&gt; by Sabrina (7 min)&lt;/li&gt;&lt;/td&gt; &#xA;   &lt;td&gt;new&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23&lt;/td&gt; &#xA;   &lt;td&gt;Jun 19&lt;/td&gt; &#xA;   &lt;td&gt;Model deployment and Conclusions&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;new&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;(optional reading) Stochastic Gradient Descent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;(optional reading) Combining Multiple Tables&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>