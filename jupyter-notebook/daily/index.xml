<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-04T01:37:47Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>imoneoi/openchat</title>
    <updated>2023-07-04T01:37:47Z</updated>
    <id>tag:github.com,2023-07-04:/imoneoi/openchat</id>
    <link href="https://github.com/imoneoi/openchat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenChat: Less is More for Open-source Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenLLMs: Less is More for Open-source Models&lt;/h1&gt; &#xA;&lt;p&gt;OpenLLMs is a series of open-source language models fine-tuned on a small, yet diverse and high-quality dataset of multi-round conversations. Specifically, we utilize only ~6K GPT-4 conversations directly filtered from the ~90K ShareGPT conversations. Despite the small size of the dataset, OpenLLMs has demostrated remarkable performance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üî• 80.9% win-rate, rank #1 of open-source models on &lt;a href=&#34;https://tatsu-lab.github.io/alpaca_eval/&#34;&gt;AlpacaEval&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üöÄ 105.7% ChatGPT score on &lt;a href=&#34;https://lmsys.org/blog/2023-03-30-vicuna/&#34;&gt;Vicuna GPT-4 eval&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ü§ó Using only &lt;a href=&#34;https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset&#34;&gt;6K data&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zenodo.org/badge/latestdoi/645397533&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/645397533.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/07] We released the &lt;a href=&#34;https://huggingface.co/openchat&#34;&gt;OpenLLMs model series&lt;/a&gt;. Among them, OpenChat obtains &lt;strong&gt;80.9%&lt;/strong&gt; win-rate on AlpacaEval and &lt;strong&gt;105%&lt;/strong&gt; ChatGPT performance on Vicuna GPT-4 evaluation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Models and Dataset&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;‚ö†Ô∏è Note:&lt;/em&gt; The evaluation metrics represent a quantified measure of a subset of the model&#39;s capabilities. A score of 105% does not necessarily indicate that the model is better than ChatGPT in all scenarios or for all use cases. It is important to consider the specific tasks or applications for which the model was evaluated and compare the results accordingly.&lt;/p&gt; &#xA;&lt;h4&gt;Generic Models:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://huggingface.co/openchat/openchat&#34;&gt;OpenChat&lt;/a&gt;&lt;/strong&gt;: based on LLaMA-13B with a context length of 2048. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Achieves &lt;strong&gt;105.7%&lt;/strong&gt; of ChatGPT score on the Vicuna GPT-4 evaluation.&lt;/li&gt; &#xA;   &lt;li&gt;Achieves &lt;strong&gt;80.9%&lt;/strong&gt; win-rate on AlpacaEval.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://huggingface.co/openchat/openchat_8192&#34;&gt;OpenChat-8192&lt;/a&gt;&lt;/strong&gt;: based on LLaMA-13B, with an extended context length of 8192. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Achieves &lt;strong&gt;106.6%&lt;/strong&gt; of ChatGPT score on the Vicuna GPT-4 evaluation.&lt;/li&gt; &#xA;   &lt;li&gt;Achieves &lt;strong&gt;79.5%&lt;/strong&gt; win-rate on AlpacaEval.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Code Models:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://huggingface.co/openchat/opencoderplus&#34;&gt;OpenCoderPlus&lt;/a&gt;&lt;/strong&gt;: based on StarCoderPlus with a native context length of 8192. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Achieves &lt;strong&gt;102.5%&lt;/strong&gt; of ChatGPT score on the Vicuna GPT-4 evaluation.&lt;/li&gt; &#xA;   &lt;li&gt;Achieves a &lt;strong&gt;78.7%&lt;/strong&gt; win-rate on AlpacaEval.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Dataset:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset&#34;&gt;openchat_sharegpt4_dataset&lt;/a&gt;&lt;/strong&gt;: ~6k cleaned and filtered GPT-4 data from ShareGPT.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We have evaluated our models using the two most popular evaluation benchmarks, including Vicuna GPT-4 and AlpacaEval benchmarks. The evaluation results are presented in the following figures.&lt;/p&gt; &#xA;&lt;h3&gt;Vicuna Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Considering that our fine-tuning dataset is produced by GPT-4, we use both GPT-4 and GPT-3.5-Turbo as evaluators, respectively. Note that our evaluation schema slightly differs from Vicuna&#39;s. Following &lt;a href=&#34;https://arxiv.org/pdf/2305.17926.pdf&#34;&gt;Wang et. al, 2023&lt;/a&gt;, we additionally adopted evidence calibration (EC) + balanced position calibration (BPC) to reduce potential bias.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vicuna GPT-4 Evaluation (v.s. gpt-3.5-turbo)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/imoneoi/openchat/master/assets/vicuna_gpt4.svg?sanitize=true&#34; alt=&#34;gpt4eval&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vicuna GPT-3.5-Turbo Evaluation (v.s. gpt-3.5-turbo)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/imoneoi/openchat/master/assets/vicuna_gpt35.svg?sanitize=true&#34; alt=&#34;gpt35eval&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;AlpacaEval&lt;/h3&gt; &#xA;&lt;p&gt;Here we list the minimal version of AlpacaEval with our released models. The full version of AlpacaEval can be found on this &lt;a href=&#34;https://tatsu-lab.github.io/alpaca_eval/&#34;&gt;page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Win Rate&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Std Error&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt4&lt;/td&gt; &#xA;   &lt;td&gt;95.3&lt;/td&gt; &#xA;   &lt;td&gt;0.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;claude&lt;/td&gt; &#xA;   &lt;td&gt;88.4&lt;/td&gt; &#xA;   &lt;td&gt;1.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;chatgpt&lt;/td&gt; &#xA;   &lt;td&gt;86.1&lt;/td&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;openchat-13b&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;80.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;openchat8192-13b&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;79.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;opencoderplus-15b&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;78.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wizardlm-13b&lt;/td&gt; &#xA;   &lt;td&gt;75.3&lt;/td&gt; &#xA;   &lt;td&gt;1.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;guanaco-65b&lt;/td&gt; &#xA;   &lt;td&gt;71.8&lt;/td&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;vicuna-13b&lt;/td&gt; &#xA;   &lt;td&gt;70.4&lt;/td&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;oasst-rlhf-llama-33b&lt;/td&gt; &#xA;   &lt;td&gt;66.5&lt;/td&gt; &#xA;   &lt;td&gt;1.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;text_davinci_003&lt;/td&gt; &#xA;   &lt;td&gt;50.0&lt;/td&gt; &#xA;   &lt;td&gt;0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;falcon-40b-instruct&lt;/td&gt; &#xA;   &lt;td&gt;45.7&lt;/td&gt; &#xA;   &lt;td&gt;1.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;alpaca-farm-ppo-human&lt;/td&gt; &#xA;   &lt;td&gt;41.2&lt;/td&gt; &#xA;   &lt;td&gt;1.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;alpaca-7b&lt;/td&gt; &#xA;   &lt;td&gt;26.5&lt;/td&gt; &#xA;   &lt;td&gt;1.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;text_davinci_001&lt;/td&gt; &#xA;   &lt;td&gt;15.2&lt;/td&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Standard benchmarks (In progress)&lt;/h2&gt; &#xA;&lt;p&gt;Due to the limitations of Vicuna GPT-4 Evaluation and AlpacaEval, we are trying to use extensive standard benchmarks to evaluate the performance of OpenLLMs.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th&gt;LLaMA-13B BFloat16&lt;/th&gt; &#xA;   &lt;th&gt;OpenChat&lt;/th&gt; &#xA;   &lt;th&gt;OpenChat8192&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMLU &lt;a href=&#34;https://github.com/FranxYao/chain-of-thought-hub&#34;&gt;(chain-of-thought hub)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;46.66&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;48.53&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;45.16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To use OpenLLMs, you need to have CUDA and PyTorch installed. You can clone this repository and install the dependencies via pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:imoneoi/OChat.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --no-build-isolation flash-attn&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Weights &amp;amp; Serving&lt;/h2&gt; &#xA;&lt;p&gt;We provide full weights of all models as huggingface repos. You can use the following commands to start a local API server at &lt;code&gt;http://localhost:18888&lt;/code&gt;. Please note that models should be used under their foundation models&#39; license.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Context&lt;/th&gt; &#xA;   &lt;th&gt;Weights&lt;/th&gt; &#xA;   &lt;th&gt;Serve&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenChat&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openchat/openchat&#34;&gt;openchat/openchat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;python -m ochat.serving.openai_api_server --model_type openchat --model_path openchat/openchat&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenChat8192&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;8192&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openchat/openchat_8192&#34;&gt;openchat/openchat_8192&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;python -m ochat.serving.openai_api_server --model_type openchat_8192 --model_path openchat/openchat_8192&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenCoderPlus&lt;/td&gt; &#xA;   &lt;td&gt;15B&lt;/td&gt; &#xA;   &lt;td&gt;8192&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openchat/opencoderplus&#34;&gt;openchat/opencoderplus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;python -m ochat.serving.openai_api_server --model_type opencoder --model_path openchat/opencoderplus&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The server is compatible with the &lt;code&gt;ChatCompletions&lt;/code&gt; protocol (please note that some functionalities are not fully supported) and the &lt;code&gt;openai&lt;/code&gt; package. You can specify the server of &lt;code&gt;openai&lt;/code&gt; package by setting:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;openai.api_base = &#34;http://localhost:18888/v1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The currently supported &lt;code&gt;ChatCompletions&lt;/code&gt; arguments are:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;conversation&lt;/td&gt; &#xA;   &lt;td&gt;The conversation to complete. Example: &lt;code&gt;[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hello&#34;}]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;temperature&lt;/td&gt; &#xA;   &lt;td&gt;Temperature for sampling. Recommended: &lt;code&gt;0.7&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;top_p&lt;/td&gt; &#xA;   &lt;td&gt;Top-P for sampling. Recommended: &lt;code&gt;0.9&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;max_generated_tokens&lt;/td&gt; &#xA;   &lt;td&gt;Maximum number of generated tokens&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;stream&lt;/td&gt; &#xA;   &lt;td&gt;Response in event stream (true / false)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We also provide a &lt;strong&gt;Web UI&lt;/strong&gt; for a better user experience, please refer to the following section for details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; We recommend having a GPU with memory of at least 40GB (1x A100) to run the server.&lt;/p&gt; &#xA;&lt;h2&gt;Web UI&lt;/h2&gt; &#xA;&lt;p&gt;You can interact with the model using &lt;a href=&#34;https://github.com/imoneoi/openchat-ui&#34;&gt;OpenChat-UI&lt;/a&gt;, which is a fork of Chatbot UI with support for OpenChat models.&lt;/p&gt; &#xA;&lt;p&gt;To use OpenChat-UI, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the OpenChat-UI repo:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/imoneoi/openchat-ui.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm i&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Set the API host to the local server (or the address of the OpenChat server)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Create a &lt;code&gt;.env.local&lt;/code&gt; file in the root of the OpenChat-UI repo with the following content:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-conf&#34;&gt;OPENAI_API_HOST=http://localhost:18888&#xA;OPENAI_API_KEY=openchat-dummy-key&#xA;NEXT_PUBLIC_DEFAULT_TEMPERATURE=0.7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Run the App&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model modifications&lt;/h2&gt; &#xA;&lt;p&gt;We added an EOT (end-of-turn) token to every base model. For LLaMA models, the embedding of EOT is initialized as the average of all existing token embeddings. For StarCoder models, the embedding of EOT is randomly initialized with 0.02 standard deviation.&lt;/p&gt; &#xA;&lt;p&gt;For LLaMA-based models with 8192 context, the &lt;code&gt;max_position_embeddings&lt;/code&gt; was set to 8192, and RoPE codes were extrapolated. An attempt to interpolate the RoPE code was made, but it resulted in a significant drop in performance (~101% Vicuna GPT-4 evaluation) without mixing pretraining data.&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;ü§ó Converted dataset available at &lt;a href=&#34;https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset&#34;&gt;openchat_sharegpt4_dataset&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The dataset used in the project is a cleaned and filtered version of ShareGPT, retaining only GPT-4 conversations. The original ShareGPT contained approximately 90K conversations, and only 6K cleaned GPT-4 conversations were retained for fine-tuning.&lt;/p&gt; &#xA;&lt;p&gt;The cleaned GPT-4 conversations were combined with conversation templates and end-of-turn tokens, then cut to the context limit of the model (further content was simply discarded).&lt;/p&gt; &#xA;&lt;p&gt;To run the data pipeline, execute the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./ochat/data/run_data_pipeline.sh INPUT_FOLDER OUTPUT_FOLDER&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The input folder should contain a &lt;code&gt;ShareGPT&lt;/code&gt; folder with &lt;code&gt;.html&lt;/code&gt; files for each ShareGPT conversation page inside.&lt;/p&gt; &#xA;&lt;p&gt;The data pipeline consists of three steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cleaning: HTML cleaning and conversion to Markdown, removing conversations with the wrong format, removing conversations with blocked words, and hash-based exact deduplication&lt;/li&gt; &#xA; &lt;li&gt;Filtering: Preserving only conversations marked as &lt;code&gt;Model: GPT-4&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Converting: Converting and tokenizing all conversations for finetuning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;The final converted dataset follows the format:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;MODEL_TYPE.train.json / .eval.json&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[&#xA;    [token_id_list, supervise_mask_list],&#xA;    [token_id_list, supervise_mask_list],&#xA;    ...&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;MODEL_TYPE.train.text.json / .eval.text.json&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Plain text decoded from &lt;code&gt;token_id_list&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dataset visualization&lt;/h2&gt; &#xA;&lt;p&gt;We provide a tool for visualizing the embeddings of conversations. To use this tool, open &lt;code&gt;ochat/visualization/ui/visualizer.html&lt;/code&gt; using a browser and drag &lt;code&gt;MODEL_TYPE.visualizer.json&lt;/code&gt; into the webpage. Click on 3D plot points to show the corresponding conversation.&lt;/p&gt; &#xA;&lt;p&gt;The embeddings are created using &lt;code&gt;openai_embeddings.py&lt;/code&gt; to calculate embeddings of conversations, then UMAP dimension reduction and K-Means coloring with &lt;code&gt;dim_reduction.ipynb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/imoneoi/openchat/master/assets/embeddings.svg?sanitize=true&#34; alt=&#34;embedding&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;The hyperparameters used in training the models are the same across all models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th&gt;Epochs&lt;/th&gt; &#xA;   &lt;th&gt;Length Grouping&lt;/th&gt; &#xA;   &lt;th&gt;Warmup Ratio&lt;/th&gt; &#xA;   &lt;th&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;2e-5&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;True&lt;/td&gt; &#xA;   &lt;td&gt;0.03&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To train using 8xA100 80GB:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NUM_GPUS=8&#xA;&#xA;deepspeed --num_gpus=$NUM_GPUS --module ochat.training_deepspeed.train \&#xA;    --model_type MODEL_TYPE \&#xA;    --model_path BASE_MODEL_PATH \&#xA;    --save_path TARGET_FOLDER \&#xA;    --length_grouping \&#xA;    --epochs 5 \&#xA;    --data_path DATASET_PATH \&#xA;    --deepspeed \&#xA;    --deepspeed_config ochat/training_deepspeed/deepspeed_config.json&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To run the Vicuna GPT-4 evaluation, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate model answers&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m ochat.evaluation.get_model_answer --model_type MODEL_TYPE --models_path PATH_CONTAINING_ALL_MODELS_SAME_TYPE --data_path ./ochat/evaluation/vicuna --output_path ./eval_results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Generate baseline (GPT-3.5) answers&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_KEY=sk-XXX python -m ochat.evaluation.get_openai_answer --data_path ./ochat/evaluation/vicuna --output_path ./eval_baselines --model_types gpt-3.5-turbo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run GPT-4 evaluation&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_KEY=sk-XXX python -m ochat.evaluation.openai_eval --data_path ./ochat/evaluation/vicuna --baseline_path ./eval_baselines/vicuna_gpt-3.5-turbo.jsonl --input_path ./eval_results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Visualize and plot&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To visualize and plot the evaluation results, use &lt;code&gt;ochat/visualization/eval_result_ui/eval_result_visualizer.html&lt;/code&gt;. Open the file using a browser and select all files inside &lt;code&gt;./eval_results/eval_result_YYYYMMDD&lt;/code&gt; to show the results.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;The same routine as ChatGPT / GPT-4 was used to run other benchmarks or evaluations such as AlpacaEval. Simply run the API server and set the &lt;code&gt;openai.api_base&lt;/code&gt; of the benchmark program.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Foundation Model Limitations&lt;/strong&gt; Despite its advanced capabilities, OpenLLMs is still bound by the limitations inherent in its foundation models. These limitations may impact the model&#39;s performance in areas such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Complex reasoning&lt;/li&gt; &#xA; &lt;li&gt;Mathematical and arithmetic tasks&lt;/li&gt; &#xA; &lt;li&gt;Programming and coding challenges&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hallucination of Non-existent Information&lt;/strong&gt; OpenLLMs may sometimes generate information that does not exist or is not accurate, also known as &#34;hallucination&#34;. Users should be aware of this possibility and verify any critical information obtained from the model.&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improving conversation splitting&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Mixing SFT data with pretraining data (e.g. RedPajama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Extending context by interpolating RoPE (requires mixing with pretraining data)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Trying LIMA dropout (to determine its usefulness)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training larger LLaMA models (needs more computing power)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support inference with 2x consumer GPUs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Our weight license is subject to their corresponding base model. For example, OpenChat and OpenChat-8192 are the same as the model &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&#34;&gt;License&lt;/a&gt; of LLaMA for non-commercial use only, while OpenCoderPlus is under the &lt;a href=&#34;https://huggingface.co/blog/starcoder&#34;&gt;License&lt;/a&gt; of StarCoder. Furthermore, we should follow &lt;a href=&#34;https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb&#34;&gt;Privacy Practices&lt;/a&gt; of ShareGPT. The code is released under Apache License 2.0.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;mailto:imonenext@gmail.com&#34;&gt;imonenext@gmail.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{openllms23,&#xA;  title = {{OpenLLMs: Less is More for Open-source Models}},&#xA;  author = {Wang, Guan and Cheng, Sijie and Yu, Qiying and Liu, Changling},&#xA;  doi = {10.5281/zenodo.8105775},&#xA;  url = {https://github.com/imoneoi/openchat},&#xA;  version = {pre-release},&#xA;  year = {2023},&#xA;  month = {7},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We thank the great work by &lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMA&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;self-instruct&lt;/a&gt;, &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat (Vicuna)&lt;/a&gt;, &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca.git&#34;&gt;Alpaca&lt;/a&gt; and &lt;a href=&#34;https://github.com/bigcode-project/starcoder&#34;&gt;StarCoder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also thank the great support by GPT Desk Pte. Ltd. and Tsinghua Laboratory of Brain and Intelligence (THBI).&lt;/p&gt;</summary>
  </entry>
</feed>