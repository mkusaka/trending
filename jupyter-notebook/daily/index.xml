<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-20T01:32:01Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>liltom-eth/llama2-webui</title>
    <updated>2023-08-20T01:32:01Z</updated>
    <id>tag:github.com,2023-08-20:/liltom-eth/llama2-webui</id>
    <link href="https://github.com/liltom-eth/llama2-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run any Llama 2 locally with gradio UI on GPU or CPU from anywhere (Linux/Windows/Mac). Use `llama2-wrapper` as your local llama2 backend for Generative Agents/Apps.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama2-webui&lt;/h1&gt; &#xA;&lt;p&gt;Running Llama 2 with gradio web UI on GPU or CPU from anywhere (Linux/Windows/Mac).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supporting all Llama 2 models (7B, 13B, 70B, GPTQ, GGML) with 8-bit, 4-bit mode.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://pypi.org/project/llama2-wrapper/&#34;&gt;llama2-wrapper&lt;/a&gt; as your local llama2 backend for Generative Agents/Apps; &lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/colab/Llama_2_7b_Chat_GPTQ.ipynb&#34;&gt;colab example&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/static/screenshot.png&#34; alt=&#34;screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supporting models: &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-chat-hf&#34;&gt;Llama-2-7b&lt;/a&gt;/&lt;a href=&#34;https://huggingface.co/llamaste/Llama-2-13b-chat-hf&#34;&gt;13b&lt;/a&gt;/&lt;a href=&#34;https://huggingface.co/llamaste/Llama-2-70b-chat-hf&#34;&gt;70b&lt;/a&gt;, all &lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ&#34;&gt;Llama-2-GPTQ&lt;/a&gt;, all &lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML&#34;&gt;Llama-2-GGML&lt;/a&gt; ...&lt;/li&gt; &#xA; &lt;li&gt;Supporting model backends: &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;tranformers&lt;/a&gt;, &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes(8-bit inference)&lt;/a&gt;, &lt;a href=&#34;https://github.com/PanQiWei/AutoGPTQ&#34;&gt;AutoGPTQ(4-bit inference)&lt;/a&gt;, &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Demos: &lt;a href=&#34;https://twitter.com/liltom_eth/status/1682791729207070720?s=20&#34;&gt;Run Llama2 on MacBook Air&lt;/a&gt;; &lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/colab/Llama_2_7b_Chat_GPTQ.ipynb&#34;&gt;Run Llama2 on free Colab T4 GPU&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://pypi.org/project/llama2-wrapper/&#34;&gt;llama2-wrapper&lt;/a&gt; as your local llama2 backend for Generative Agents/Apps; &lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/colab/Llama_2_7b_Chat_GPTQ.ipynb&#34;&gt;colab example&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/docs/news.md&#34;&gt;News&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/docs/performance.md&#34;&gt;Benchmark&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/docs/issues.md&#34;&gt;Issue Solutions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#start-web-ui&#34;&gt;Start Web UI&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#env-examples&#34;&gt;Env Examples&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#use-llama2-wrapper-for-your-app&#34;&gt;Use llama2-wrapper for Your App&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#benchmark&#34;&gt;Benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#download-llama-2-models&#34;&gt;Download Llama-2 Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#model-list&#34;&gt;Model List&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#download-script&#34;&gt;Download Script&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#tips&#34;&gt;Tips&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#run-on-nvidia-gpu&#34;&gt;Run on Nvidia GPU&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#run-bitsandbytes-8-bit&#34;&gt;Run bitsandbytes 8 bit&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#run-gptq-4-bit&#34;&gt;Run GPTQ 4 bit&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#run-on-cpu&#34;&gt;Run on CPU&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#mac-metal-acceleration&#34;&gt;Mac Metal Acceleration&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#amdnvidia-gpu-acceleration&#34;&gt;AMD/Nvidia GPU Acceleration&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;h3&gt;Method 1: From &lt;a href=&#34;https://pypi.org/project/llama2-wrapper/&#34;&gt;PyPI&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install llama2-wrapper&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Method 2: From Source:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/liltom-eth/llama2-webui.git&#xA;cd llama2-webui&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install Issues:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;bitsandbytes &amp;gt;= 0.39&lt;/code&gt; may not work on older NVIDIA GPUs. In that case, to use &lt;code&gt;LOAD_IN_8BIT&lt;/code&gt;, you may have to downgrade like this:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install bitsandbytes==0.38.1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;bitsandbytes&lt;/code&gt; also need a special install for Windows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip uninstall bitsandbytes&#xA;pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.0-py3-none-win_amd64.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Start Web UI&lt;/h3&gt; &#xA;&lt;p&gt;Run chatbot simply with web UI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;app.py&lt;/code&gt; will load the default config &lt;code&gt;.env&lt;/code&gt; which uses &lt;code&gt;llama.cpp&lt;/code&gt; as the backend to run &lt;code&gt;llama-2-7b-chat.ggmlv3.q4_0.bin&lt;/code&gt; model for inference. The model &lt;code&gt;llama-2-7b-chat.ggmlv3.q4_0.bin&lt;/code&gt; will be automatically downloaded.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Running on backend llama.cpp.&#xA;Use default model path: ./models/llama-2-7b-chat.ggmlv3.q4_0.bin&#xA;Start downloading model to: ./models/llama-2-7b-chat.ggmlv3.q4_0.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also customize your &lt;code&gt;MODEL_PATH&lt;/code&gt;, &lt;code&gt;BACKEND_TYPE,&lt;/code&gt; and model configs in &lt;code&gt;.env&lt;/code&gt; file to run different llama2 models on different backends (llama.cpp, transformers, gptq).&lt;/p&gt; &#xA;&lt;h4&gt;Env Examples&lt;/h4&gt; &#xA;&lt;p&gt;There are some examples in &lt;code&gt;./env_examples/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Setup&lt;/th&gt; &#xA;   &lt;th&gt;Example .env&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7b-chat-hf 8-bit (transformers backend)&lt;/td&gt; &#xA;   &lt;td&gt;.env.7b_8bit_example&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7b-Chat-GPTQ 4-bit (gptq transformers backend)&lt;/td&gt; &#xA;   &lt;td&gt;.env.7b_gptq_example&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7B-Chat-GGML 4bit (llama.cpp backend)&lt;/td&gt; &#xA;   &lt;td&gt;.env.7b_ggmlv3_q4_0_example&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-13b-chat-hf (transformers backend)&lt;/td&gt; &#xA;   &lt;td&gt;.env.13b_example&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Use llama2-wrapper for Your App&lt;/h3&gt; &#xA;&lt;p&gt;🔥 For developers, we released &lt;code&gt;llama2-wrapper&lt;/code&gt; as a llama2 backend wrapper in &lt;a href=&#34;https://pypi.org/project/llama2-wrapper/&#34;&gt;PYPI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Use &lt;code&gt;llama2-wrapper&lt;/code&gt; as your local llama2 backend to answer questions and more, &lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/colab/ggmlv3_q4_0.ipynb&#34;&gt;colab example&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# pip install llama2-wrapper&#xA;from llama2_wrapper import LLAMA2_WRAPPER, get_prompt &#xA;llama2_wrapper = LLAMA2_WRAPPER()&#xA;# Default running on backend llama.cpp.&#xA;# Automatically downloading model to: ./models/llama-2-7b-chat.ggmlv3.q4_0.bin&#xA;prompt = &#34;Do you know Pytorch&#34;&#xA;answer = llama2_wrapper(get_prompt(prompt), temperature=0.9)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run gptq llama2 model on Nvidia GPU, &lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/colab/Llama_2_7b_Chat_GPTQ.ipynb&#34;&gt;colab example&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama2_wrapper import LLAMA2_WRAPPER &#xA;llama2_wrapper = LLAMA2_WRAPPER(backend_type=&#34;gptq&#34;)&#xA;# Automatically downloading model to: ./models/Llama-2-7b-Chat-GPTQ&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run llama2 7b with bitsandbytes 8 bit with a &lt;code&gt;model_path&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama2_wrapper import LLAMA2_WRAPPER &#xA;llama2_wrapper = LLAMA2_WRAPPER(&#xA;&#x9;model_path = &#34;./models/Llama-2-7b-chat-hf&#34;,&#xA;  backend_type = &#34;transformers&#34;,&#xA;  load_in_8bit = True&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Run benchmark script to compute performance on your device, &lt;code&gt;benchmark.py&lt;/code&gt; will load the same &lt;code&gt;.env&lt;/code&gt; as &lt;code&gt;app.py&lt;/code&gt;.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python benchmark.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also select the &lt;code&gt;iter&lt;/code&gt;, &lt;code&gt;backend_type&lt;/code&gt; and &lt;code&gt;model_path&lt;/code&gt; the benchmark will be run (overwrite .env args) :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python benchmark.py --iter NB_OF_ITERATIONS --backend_type gptq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the number of iterations is 5, but if you want a faster result or a more accurate one you can set it to whatever value you want, but please only report results with at least 5 iterations.&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/colab/Llama_2_7b_Chat_GPTQ.ipynb&#34;&gt;colab example&lt;/a&gt; also show you how to benchmark gptq model on free Google Colab T4 GPU.&lt;/p&gt; &#xA;&lt;p&gt;Some benchmark performance:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;Device&lt;/th&gt; &#xA;   &lt;th&gt;RAM / GPU VRAM&lt;/th&gt; &#xA;   &lt;th&gt;Speed (tokens/sec)&lt;/th&gt; &#xA;   &lt;th&gt;load time (s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7b-chat-hf&lt;/td&gt; &#xA;   &lt;td&gt;8 bit&lt;/td&gt; &#xA;   &lt;td&gt;NVIDIA RTX 2080 Ti&lt;/td&gt; &#xA;   &lt;td&gt;7.7 GB VRAM&lt;/td&gt; &#xA;   &lt;td&gt;3.76&lt;/td&gt; &#xA;   &lt;td&gt;641.36&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7b-Chat-GPTQ&lt;/td&gt; &#xA;   &lt;td&gt;4 bit&lt;/td&gt; &#xA;   &lt;td&gt;NVIDIA RTX 2080 Ti&lt;/td&gt; &#xA;   &lt;td&gt;5.8 GB VRAM&lt;/td&gt; &#xA;   &lt;td&gt;18.85&lt;/td&gt; &#xA;   &lt;td&gt;192.91&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7b-Chat-GPTQ&lt;/td&gt; &#xA;   &lt;td&gt;4 bit&lt;/td&gt; &#xA;   &lt;td&gt;Google Colab T4&lt;/td&gt; &#xA;   &lt;td&gt;5.8 GB VRAM&lt;/td&gt; &#xA;   &lt;td&gt;18.19&lt;/td&gt; &#xA;   &lt;td&gt;37.44&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-2-7b-chat.ggmlv3.q4_0&lt;/td&gt; &#xA;   &lt;td&gt;4 bit&lt;/td&gt; &#xA;   &lt;td&gt;Apple M1 Pro CPU&lt;/td&gt; &#xA;   &lt;td&gt;5.4 GB RAM&lt;/td&gt; &#xA;   &lt;td&gt;17.90&lt;/td&gt; &#xA;   &lt;td&gt;0.18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-2-7b-chat.ggmlv3.q4_0&lt;/td&gt; &#xA;   &lt;td&gt;4 bit&lt;/td&gt; &#xA;   &lt;td&gt;Apple M2 CPU&lt;/td&gt; &#xA;   &lt;td&gt;5.4 GB RAM&lt;/td&gt; &#xA;   &lt;td&gt;13.70&lt;/td&gt; &#xA;   &lt;td&gt;0.13&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-2-7b-chat.ggmlv3.q4_0&lt;/td&gt; &#xA;   &lt;td&gt;4 bit&lt;/td&gt; &#xA;   &lt;td&gt;Apple M2 Metal&lt;/td&gt; &#xA;   &lt;td&gt;5.4 GB RAM&lt;/td&gt; &#xA;   &lt;td&gt;12.60&lt;/td&gt; &#xA;   &lt;td&gt;0.10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-2-7b-chat.ggmlv3.q2_K&lt;/td&gt; &#xA;   &lt;td&gt;2 bit&lt;/td&gt; &#xA;   &lt;td&gt;Intel i7-8700&lt;/td&gt; &#xA;   &lt;td&gt;4.5 GB RAM&lt;/td&gt; &#xA;   &lt;td&gt;7.88&lt;/td&gt; &#xA;   &lt;td&gt;31.90&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Check/contribute the performance of your device in the full &lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/docs/performance.md&#34;&gt;performance doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Download Llama-2 Models&lt;/h2&gt; &#xA;&lt;p&gt;Llama 2 is a collection of pre-trained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters.&lt;/p&gt; &#xA;&lt;p&gt;Llama-2-7b-Chat-GPTQ is the GPTQ model files for &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-chat-hf&#34;&gt;Meta&#39;s Llama 2 7b Chat&lt;/a&gt;. GPTQ 4-bit Llama-2 model require less GPU VRAM to run it.&lt;/p&gt; &#xA;&lt;h3&gt;Model List&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th&gt;set MODEL_PATH in .env&lt;/th&gt; &#xA;   &lt;th&gt;Download URL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;meta-llama/Llama-2-7b-chat-hf&lt;/td&gt; &#xA;   &lt;td&gt;/path-to/Llama-2-7b-chat-hf&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llamaste/Llama-2-7b-chat-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;meta-llama/Llama-2-13b-chat-hf&lt;/td&gt; &#xA;   &lt;td&gt;/path-to/Llama-2-13b-chat-hf&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llamaste/Llama-2-13b-chat-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;meta-llama/Llama-2-70b-chat-hf&lt;/td&gt; &#xA;   &lt;td&gt;/path-to/Llama-2-70b-chat-hf&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llamaste/Llama-2-70b-chat-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;meta-llama/Llama-2-7b-hf&lt;/td&gt; &#xA;   &lt;td&gt;/path-to/Llama-2-7b-hf&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;meta-llama/Llama-2-13b-hf&lt;/td&gt; &#xA;   &lt;td&gt;/path-to/Llama-2-13b-hf&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-13b-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;meta-llama/Llama-2-70b-hf&lt;/td&gt; &#xA;   &lt;td&gt;/path-to/Llama-2-70b-hf&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-70b-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheBloke/Llama-2-7b-Chat-GPTQ&lt;/td&gt; &#xA;   &lt;td&gt;/path-to/Llama-2-7b-Chat-GPTQ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TheBloke/Llama-2-7B-Chat-GGML&lt;/td&gt; &#xA;   &lt;td&gt;/path-to/llama-2-7b-chat.ggmlv3.q4_0.bin&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Running 4-bit model &lt;code&gt;Llama-2-7b-Chat-GPTQ&lt;/code&gt; needs GPU with 6GB VRAM.&lt;/p&gt; &#xA;&lt;p&gt;Running 4-bit model &lt;code&gt;llama-2-7b-chat.ggmlv3.q4_0.bin&lt;/code&gt; needs CPU with 6GB RAM. There is also a list of other 2, 3, 4, 5, 6, 8-bit GGML models that can be used from &lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML&#34;&gt;TheBloke/Llama-2-7B-Chat-GGML&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Download Script&lt;/h3&gt; &#xA;&lt;p&gt;These models can be downloaded from the link using CMD like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Make sure you have git-lfs installed (https://git-lfs.com)&#xA;git lfs install&#xA;git clone git@hf.co:meta-llama/Llama-2-7b-chat-hf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To download Llama 2 models, you need to request access from &lt;a href=&#34;https://ai.meta.com/llama/&#34;&gt;https://ai.meta.com/llama/&lt;/a&gt; and also enable access on repos like &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main&#34;&gt;meta-llama/Llama-2-7b-chat-hf&lt;/a&gt;. Requests will be processed in hours.&lt;/p&gt; &#xA;&lt;p&gt;For GPTQ models like &lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ&#34;&gt;TheBloke/Llama-2-7b-Chat-GPTQ&lt;/a&gt;, you can directly download without requesting access.&lt;/p&gt; &#xA;&lt;p&gt;For GGML models like &lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML&#34;&gt;TheBloke/Llama-2-7B-Chat-GGML&lt;/a&gt;, you can directly download without requesting access.&lt;/p&gt; &#xA;&lt;h2&gt;Tips&lt;/h2&gt; &#xA;&lt;h3&gt;Run on Nvidia GPU&lt;/h3&gt; &#xA;&lt;p&gt;The running requires around 14GB of GPU VRAM for Llama-2-7b and 28GB of GPU VRAM for Llama-2-13b.&lt;/p&gt; &#xA;&lt;p&gt;If you are running on multiple GPUs, the model will be loaded automatically on GPUs and split the VRAM usage. That allows you to run Llama-2-7b (requires 14GB of GPU VRAM) on a setup like 2 GPUs (11GB VRAM each).&lt;/p&gt; &#xA;&lt;h4&gt;Run bitsandbytes 8 bit&lt;/h4&gt; &#xA;&lt;p&gt;If you do not have enough memory, you can set up your &lt;code&gt;LOAD_IN_8BIT&lt;/code&gt; as &lt;code&gt;True&lt;/code&gt; in &lt;code&gt;.env&lt;/code&gt;. This can reduce memory usage by around half with slightly degraded model quality. It is compatible with the CPU, GPU, and Metal backend.&lt;/p&gt; &#xA;&lt;p&gt;Llama-2-7b with 8-bit compression can run on a single GPU with 8 GB of VRAM, like an Nvidia RTX 2080Ti, RTX 4080, T4, V100 (16GB).&lt;/p&gt; &#xA;&lt;h4&gt;Run GPTQ 4 bit&lt;/h4&gt; &#xA;&lt;p&gt;If you want to run 4 bit Llama-2 model like &lt;code&gt;Llama-2-7b-Chat-GPTQ&lt;/code&gt;, you can set up your &lt;code&gt;BACKEND_TYPE&lt;/code&gt; as &lt;code&gt;gptq&lt;/code&gt; in &lt;code&gt;.env&lt;/code&gt; like example &lt;code&gt;.env.7b_gptq_example&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Make sure you have downloaded the 4-bit model from &lt;code&gt;Llama-2-7b-Chat-GPTQ&lt;/code&gt; and set the &lt;code&gt;MODEL_PATH&lt;/code&gt; and arguments in &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Llama-2-7b-Chat-GPTQ&lt;/code&gt; can run on a single GPU with 6 GB of VRAM.&lt;/p&gt; &#xA;&lt;p&gt;If you encounter issue like &lt;code&gt;NameError: name &#39;autogptq_cuda_256&#39; is not defined&lt;/code&gt;, please refer to &lt;a href=&#34;https://huggingface.co/TheBloke/open-llama-13b-open-instruct-GPTQ/discussions/1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;pip install &lt;a href=&#34;https://github.com/PanQiWei/AutoGPTQ/releases/download/v0.3.0/auto_gptq-0.3.0+cu117-cp310-cp310-linux_x86_64.whl&#34;&gt;https://github.com/PanQiWei/AutoGPTQ/releases/download/v0.3.0/auto_gptq-0.3.0+cu117-cp310-cp310-linux_x86_64.whl&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Run on CPU&lt;/h3&gt; &#xA;&lt;p&gt;Run Llama-2 model on CPU requires &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; dependency and &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python&#34;&gt;llama.cpp Python Bindings&lt;/a&gt;, which are already installed.&lt;/p&gt; &#xA;&lt;p&gt;Download GGML models like &lt;code&gt;llama-2-7b-chat.ggmlv3.q4_0.bin&lt;/code&gt; following &lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/#download-llama-2-models&#34;&gt;Download Llama-2 Models&lt;/a&gt; section. &lt;code&gt;llama-2-7b-chat.ggmlv3.q4_0.bin&lt;/code&gt; model requires at least 6 GB RAM to run on CPU.&lt;/p&gt; &#xA;&lt;p&gt;Set up configs like &lt;code&gt;.env.7b_ggmlv3_q4_0_example&lt;/code&gt; from &lt;code&gt;env_examples&lt;/code&gt; as &lt;code&gt;.env&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Run web UI &lt;code&gt;python app.py&lt;/code&gt; .&lt;/p&gt; &#xA;&lt;h4&gt;Mac Metal Acceleration&lt;/h4&gt; &#xA;&lt;p&gt;For Mac users, you can also set up Mac Metal for acceleration, try install this dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip uninstall llama-cpp-python -y&#xA;CMAKE_ARGS=&#34;-DLLAMA_METAL=on&#34; FORCE_CMAKE=1 pip install -U llama-cpp-python --no-cache-dir&#xA;pip install &#39;llama-cpp-python[server]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or check details:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/abetlen/llama-cpp-python/raw/main/docs/install/macos.md&#34;&gt;MacOS Install with Metal GPU&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;AMD/Nvidia GPU Acceleration&lt;/h4&gt; &#xA;&lt;p&gt;If you would like to use AMD/Nvidia GPU for acceleration, check this:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal&#34;&gt;Installation with OpenBLAS / cuBLAS / CLBlast / Metal&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT - see &lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project enables users to adapt it freely for proprietary purposes without any restrictions.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Kindly read our &lt;a href=&#34;https://raw.githubusercontent.com/liltom-eth/llama2-webui/main/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt; to learn and understand our development process.&lt;/p&gt; &#xA;&lt;h3&gt;All Contributors&lt;/h3&gt; &#xA;&lt;a href=&#34;https://github.com/liltom-eth/llama2-webui/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=liltom-eth/llama2-webui&#34;&gt; &lt;/a&gt; &#xA;&lt;h3&gt;Review&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/repo-reviews/repo-reviews.github.io/raw/main/create.md&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Github&#34; src=&#34;https://img.shields.io/badge/review-100000?style=flat&amp;amp;logo=Github&amp;amp;logoColor=white&amp;amp;labelColor=888888&amp;amp;color=555555&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Star History&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#liltom-eth/llama2-webui&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=liltom-eth/llama2-webui&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-chat-hf&#34;&gt;https://huggingface.co/meta-llama/Llama-2-7b-chat-hf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat&#34;&gt;https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ&#34;&gt;https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;https://github.com/ggerganov/llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;https://github.com/TimDettmers/bitsandbytes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PanQiWei/AutoGPTQ&#34;&gt;https://github.com/PanQiWei/AutoGPTQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/lida</title>
    <updated>2023-08-20T01:32:01Z</updated>
    <id>tag:github.com,2023-08-20:/microsoft/lida</id>
    <link href="https://github.com/microsoft/lida" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Automatic Generation of Visualizations and Infographics using Large Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LIDA: Automatic Generation of Visualizations and Infographics using Large Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/lida&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/lida.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.02927&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2303.02927-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/microsoft/lida/blob/main/notebooks/tutorial.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- &lt;img src=&#34;docs/images/lidascreen.png&#34; width=&#34;100%&#34; /&gt; --&gt; &#xA;&lt;p&gt;LIDA is a library for generating data visualizations and data-faithful infographics. LIDA is grammar agnostic (will work with any programming language and visualization libraries e.g. matplotlib, seaborn, altair, d3 etc) and works with multiple large language model providers (OpenAI, PaLM, Cohere, Huggingface). Details on the components of LIDA are described in the &lt;a href=&#34;https://arxiv.org/abs/2303.02927&#34;&gt;paper here&lt;/a&gt; and in this tutorial &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/lida/main/notebooks/tutorial.ipynb&#34;&gt;notebook&lt;/a&gt;. See the project page &lt;a href=&#34;https://microsoft.github.io/lida/&#34;&gt;here&lt;/a&gt; for updates!.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note on Code Execution:&lt;/strong&gt; To create visualizations, LIDA &lt;em&gt;generates&lt;/em&gt; and &lt;em&gt;executes&lt;/em&gt; code. Ensure that you run LIDA in a secure environment.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/lida/main/docs/images/lidamodules.jpg&#34; alt=&#34;lida components&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;LIDA treats &lt;em&gt;&lt;strong&gt;visualizations as code&lt;/strong&gt;&lt;/em&gt; and provides a clean api for generating, executing, editing, explaining, evaluating and repairing visualization code.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Data Summarization&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Goal Generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Visualization Generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Visualization Editing&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Visualization Explanation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Visualization Evaluation and Repair&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Visualization Recommendation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Infographic Generation (beta) # pip install lida[infographics]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from lida import Manager&#xA;&#xA;lida = Manager(text_gen = llm(&#34;openai&#34;)) # palm, cohere ..&#xA;summary = lida.summarize(&#34;data/cars.csv&#34;, enrich=False)&#xA;goals = lida.goals(summary, n=2) # exploratory data analysis&#xA;charts = lida.visualize(summary=summary, n=2) # exploratory data analys&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Setup and verify that your python environment is &lt;strong&gt;&lt;code&gt;python 3.10&lt;/code&gt;&lt;/strong&gt; or higher (preferably, use &lt;a href=&#34;https://docs.conda.io/en/main/miniconda.html#installing&#34;&gt;Conda&lt;/a&gt;). Install the library via pip.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install lida&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once requirements are met, setup your api key. Learn more about setting up keys for other LLM providers &lt;a href=&#34;https://github.com/victordibia/llmx&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&amp;lt;your key&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively you can install the library in dev model by cloning this repo and running &lt;code&gt;pip install -e .&lt;/code&gt; in the repository root.&lt;/p&gt; &#xA;&lt;h2&gt;Web API and UI&lt;/h2&gt; &#xA;&lt;p&gt;LIDA comes with an optional bundled ui and web api that you can explore by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;lida ui  --port=8080 --docs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then navigate to &lt;a href=&#34;http://localhost:8080/&#34;&gt;http://localhost:8080/&lt;/a&gt; in your browser. To view the web api specification, add the &lt;code&gt;--docs&lt;/code&gt; option to the cli command, and navigate to &lt;code&gt;http://localhost:8080/api/docs&lt;/code&gt; in your browser.&lt;/p&gt; &#xA;&lt;p&gt;The fastest and recommended way to get started after installation will be to try out the web ui above or run the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/lida/main/notebooks/tutorial.ipynb&#34;&gt;tutorial notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Data Summarization&lt;/h3&gt; &#xA;&lt;p&gt;Given a dataset, generate a compact summary of the data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lida import Manager&#xA;&#xA;lida = Manager()&#xA;summary = lida.summarize(&#34;data/cars.json&#34;) # generate data summary&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Goal Generation&lt;/h3&gt; &#xA;&lt;p&gt;Generate a set of visualization goals given a data summary.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;goals = lida.goals(summary, n=5) # generate goals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualization Generation&lt;/h3&gt; &#xA;&lt;p&gt;Generate, refine, execute and filter visualization code given a data summary and visualization goal. Note that LIDA represents &lt;strong&gt;visualizations as code&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# generate charts (generate and execute visualization code)&#xA;charts = lida.visualize(summary=summary, goal=goals[0], library=&#34;matplotlib&#34;) # seaborn, ggplot ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualization Editing&lt;/h3&gt; &#xA;&lt;p&gt;Given a visualization, edit the visualization using natural language.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# modify chart using natural language&#xA;instructions = [&#34;convert this to a bar chart&#34;, &#34;change the color to red&#34;, &#34;change y axes label to Fuel Efficiency&#34;, &#34;translate the title to french&#34;]&#xA;edited_charts = lida.edit(code=code,  summary=summary, instructions=instructions, library=library, textgen_config=textgen_config)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualization Explanation&lt;/h3&gt; &#xA;&lt;p&gt;Given a visualization, generate a natural language explanation of the visualization code (accessibility, data transformations applied, visualization code)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# generate explanation for chart&#xA;explanation = lida.explain(code=charts[0].code, summary=summary)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualization Evaluation and Repair&lt;/h3&gt; &#xA;&lt;p&gt;Given a visualization, evaluate to find repair instructions (which may be human authored, or generated), repair the visualization.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;evaluations = lida.evaluate(code=code,  goal=goals[i], library=library)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualization Recommendation&lt;/h3&gt; &#xA;&lt;p&gt;Given a dataset, generate a set of recommended visualizations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;recommendations = lida.recommend(code=code, summary=summary, n=2,  textgen_config=textgen_config)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Infographic Generation [WIP]&lt;/h3&gt; &#xA;&lt;p&gt;Given a visualization, generate a data-faithful infographic. This methods should be considered experimental, and uses stable diffusion models from the &lt;a href=&#34;https://github.com/victordibia/peacasso&#34;&gt;peacasso&lt;/a&gt; library. You will need to run &lt;code&gt;pip install lida[infographics]&lt;/code&gt; to install the required dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;infographics = lida.infographics(visualization = charts[0].raster, n=3, style_prompt=&#34;line art&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Important Notes / Caveats / FAQs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LIDA generates and executes code based on provided input. Ensure that you run LIDA in a secure environment with appropriate permissions.&lt;/li&gt; &#xA; &lt;li&gt;LIDA currently works best with datasets that have a small number of columns (&amp;lt;= 10). This is mainly due to the limited context size for most models. For larger datasets, consider preprocessing your dataset to use a subset of the columns.&lt;/li&gt; &#xA; &lt;li&gt;LIDA assumes the dataset exists and is in a format that can be loaded into a pandas dataframe. For example, a csv file, or a json file with a list of objects. In practices the right dataset may need to be curated and preprocessed to ensure that it is suitable for the task at hand.&lt;/li&gt; &#xA; &lt;li&gt;Smaller LLMs (e.g., OSS LLMs on Huggingface) have limited instruction following capabilities and may not work well with LIDA. LIDA works best with larger LLMs (e.g., OpenAI GPT 3.5, GPT 4).&lt;/li&gt; &#xA; &lt;li&gt;How reliable is the LIDA approach? The LIDA &lt;a href=&#34;https://aclanthology.org/2023.acl-demo.11/&#34;&gt;paper&lt;/a&gt; describes experiments that evaluate the reliability of LIDA using a visualization error rate metric. With the current version of prompts, data summarization techniques, preprocessing/postprocessing logic and LLMs, LIDA has an error rate of &amp;lt; 3.5% on over 2200 visualizations generated (compared to a baseline of over 10% error rate). This area is work in progress.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Naturally, some of these limitations could be addressed by a much welcomed PR.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation and Citation&lt;/h2&gt; &#xA;&lt;p&gt;A short paper describing LIDA (Accepted at ACL 2023 Conference) is available &lt;a href=&#34;https://arxiv.org/abs/2303.02927&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{dibia2023lida,&#xA;    title = &#34;{LIDA}: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models&#34;,&#xA;    author = &#34;Dibia, Victor&#34;,&#xA;    booktitle = &#34;Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)&#34;,&#xA;    month = jul,&#xA;    year = &#34;2023&#34;,&#xA;    address = &#34;Toronto, Canada&#34;,&#xA;    publisher = &#34;Association for Computational Linguistics&#34;,&#xA;    url = &#34;https://aclanthology.org/2023.acl-demo.11&#34;,&#xA;    doi = &#34;10.18653/v1/2023.acl-demo.11&#34;,&#xA;    pages = &#34;113--126&#34;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LIDA builds on insights in automatic generation of visualizaiton from an earlier paper - &lt;a href=&#34;https://arxiv.org/abs/1804.03126&#34;&gt;Data2Vis: Automatic Generation of Data Visualizations Using Sequence to Sequence Recurrent Neural Networks&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>