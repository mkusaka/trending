<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-26T01:33:00Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PaddlePaddle/ERNIE</title>
    <updated>2025-07-26T01:33:00Z</updated>
    <id>tag:github.com,2025-07-26:/PaddlePaddle/ERNIE</id>
    <link href="https://github.com/PaddlePaddle/ERNIE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official repository for ERNIE 4.5 and ERNIEKit ‚Äì its industrial-grade development toolkit based on PaddlePaddle.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/9ad1ffce-2310-4f80-a3cd-7a117bfb4f17&#34; width=&#34;300px&#34;&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://ernie.baidu.com/&#34;&gt;ERNIE Bot&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/baidu&#34;&gt;ü§óHugging Face&lt;/a&gt; | &lt;a href=&#34;https://aistudio.baidu.com/modelsoverview&#34;&gt;AI Studio&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;üìë &lt;a href=&#34;https://yiyan.baidu.com/blog/posts/ernie4.5&#34;&gt;Blog&lt;/a&gt; | üìö &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/&#34;&gt;Cookbook&lt;/a&gt; | üìë &lt;a href=&#34;https://yiyan.baidu.com/blog/publication/&#34;&gt;Paper&lt;/a&gt; | üõ†Ô∏è &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/docs/erniekit.md&#34;&gt;Training&lt;/a&gt; | ‚ö°Ô∏è &lt;a href=&#34;https://github.com/PaddlePaddle/FastDeploy&#34;&gt;Deploy&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/14169&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/14169&#34; alt=&#34;PaddlePaddle%2FERNIE | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction to ERNIE 4.5&lt;/h2&gt; &#xA;&lt;p&gt;We introduce ERNIE 4.5, a new family of large-scale multimodal models comprising 10 distinct variants. The model family consist of Mixture-of-Experts (MoE) models with 47B and 3B active parameters, with the largest model having 424B total parameters, as well as a 0.3B dense model. For the MoE architecture, we propose a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality. This MoE architecture has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks. All of our models are trained with optimal efficiency using the &lt;a href=&#34;https://github.com/PaddlePaddle/Paddle&#34;&gt;PaddlePaddle&lt;/a&gt; deep learning framework, which also enables high-performance inference and streamlined deployment for them. We achieve 47% Model FLOPs Utilization (MFU) in our largest ERNIE 4.5 language model pre-training. Experimental results show that our models achieve state-of-the-art performance across multiple text and multimodal benchmarks, especially in instruction following, world knowledge memorization, visual understanding and multimodal reasoning. All models are publicly accessible under Apache 2.0 to support future research and development in the field. Additionally, we open source the development toolkits for ERNIE 4.5, featuring industrial-grade capabilities, resource-efficient training and inference workflows, and multi-hardware compatibility.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;ERNIE 4.5&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;table style=&#34;table-layout: auto; border-collapse: collapse; border: 1px solid #ddd; text-align: center;&#34;&gt; &#xA;  &lt;thead class=&#34;ant-table-thead&#34;&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th colspan=&#34;2&#34; style=&#34;border: 1px solid #ddd;text-align: center;background: lightgray;vertical-align: middle;color:black&#34;&gt;ERNIE 4.5 Models &lt;/th&gt; &#xA;    &lt;th colspan=&#34;3&#34; style=&#34;border: 1px solid #ddd;text-align: center;background: lightgray;vertical-align: middle;color:black&#34;&gt;Model Information&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th style=&#34;border: 1px solid #ddd;width: 100px;text-align: center;background: lightgray;vertical-align: middle;color:black&#34;&gt;Model Category&lt;/th&gt; &#xA;    &lt;th style=&#34;border: 1px solid #ddd;width: 250px;text-align: center;background: lightgray;vertical-align: middle;color:black&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th style=&#34;border: 1px solid #ddd; width: 100px;text-align: center;background: lightgray;vertical-align: middle;color:black&#34;&gt;Input Modality&lt;/th&gt; &#xA;    &lt;th style=&#34;border: 1px solid #ddd; width: 100px;text-align: center;background: lightgray;vertical-align: middle;color:black&#34;&gt;Output Modality&lt;/th&gt; &#xA;    &lt;th style=&#34;border: 1px solid #ddd; width: 100px;text-align: center;background: lightgray;vertical-align: middle;color:black&#34;&gt;Context Window &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody class=&#34;ant-table-tbody&#34;&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td rowspan=&#34;4&#34; style=&#34;border: 1px solid #ddd;vertical-align: middle;&#34;&gt;Large Language Models (LLMs)&lt;/td&gt; &#xA;    &lt;td style=&#34;border: 1px solid #ddd;&#34;&gt;ERNIE-4.5-300B-A47B-Base&lt;/td&gt; &#xA;    &lt;td rowspan=&#34;4&#34; style=&#34;border: 1px solid #ddd;&#34;&gt;Text&lt;/td&gt; &#xA;    &lt;td rowspan=&#34;4&#34; style=&#34;border: 1px solid #ddd;&#34;&gt;Text&lt;/td&gt; &#xA;    &lt;td rowspan=&#34;10&#34; style=&#34;border: 1px solid #ddd;&#34;&gt;128K&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td style=&#34;border: 1px solid #ddd;&#34;&gt;ERNIE-4.5-300B-A47B&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td style=&#34;border: 1px solid #ddd;&#34;&gt;ERNIE-4.5-21B-A3B-Base&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td style=&#34;border: 1px solid #ddd;&#34;&gt;ERNIE-4.5-21B-A3B&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td rowspan=&#34;4&#34; style=&#34;border: 1px solid #ddd;vertical-align: middle;&#34;&gt; Vision-Language Models (VLMs)&lt;/td&gt; &#xA;    &lt;td style=&#34;border: 1px solid #ddd;&#34;&gt;ERNIE-4.5-VL-424B-A47B-Base&lt;/td&gt; &#xA;    &lt;td rowspan=&#34;4&#34; style=&#34;border: 1px solid #ddd;&#34;&gt;Text/Image/Video&lt;/td&gt; &#xA;    &lt;td rowspan=&#34;4&#34; style=&#34;border: 1px solid #ddd;&#34;&gt;Text&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td style=&#34;border: 1px solid #ddd;&#34;&gt;ERNIE-4.5-VL-424B-A47B&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td style=&#34;border: 1px solid #ddd;&#34;&gt;ERNIE-4.5-VL-28B-A3B-Base&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td style=&#34;border: 1px solid #ddd;&#34;&gt;ERNIE-4.5-VL-28B-A3B&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td rowspan=&#34;2&#34; style=&#34;border: 1px solid #ddd;vertical-align: middle;&#34;&gt;Dense Models&lt;/td&gt; &#xA;    &lt;td style=&#34;border: 1px solid #ddd;&#34;&gt;ERNIE-4.5-0.3B-Base&lt;/td&gt; &#xA;    &lt;td rowspan=&#34;2&#34; style=&#34;border: 1px solid #ddd;&#34;&gt;Text&lt;/td&gt; &#xA;    &lt;td rowspan=&#34;2&#34; style=&#34;border: 1px solid #ddd;&#34;&gt;Text&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td style=&#34;border: 1px solid #ddd;&#34;&gt;ERNIE-4.5-0.3B&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: All models (including pre-trained weights and inference code) have been released on &lt;a href=&#34;https://huggingface.co/baidu&#34;&gt;ü§óHugging Face&lt;/a&gt;, and &lt;a href=&#34;https://aistudio.baidu.com/index&#34;&gt;AI Studio&lt;/a&gt;. Check our &lt;a href=&#34;https://yiyan.baidu.com/blog/posts/ernie4.5&#34;&gt;blog&lt;/a&gt; for more details.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;p&gt;Our model family is characterized by three key innovations:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multimodal Heterogeneous MoE Pre-Training:&lt;/strong&gt; Our models are jointly trained on both textual and visual modalities to better capture the nuances of multimodal information and improve performance on tasks involving text understanding and generation, image understanding, and cross-modal reasoning. To achieve this without one modality hindering the learning of another, we designed a &lt;em&gt;heterogeneous MoE structure&lt;/em&gt;, incorporated &lt;em&gt;modality-isolated routing&lt;/em&gt;, and employed &lt;em&gt;router orthogonal loss&lt;/em&gt; and &lt;em&gt;multimodal token-balanced loss&lt;/em&gt;. These architectural choices ensure that both modalities are effectively represented, allowing for mutual reinforcement during training.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Scaling-Efficient Infrastructure:&lt;/strong&gt; We propose a novel heterogeneous hybrid parallelism and hierarchical load balancing strategy for efficient training of ERNIE 4.5 models. By using intra-node expert parallelism, memory-efficient pipeline scheduling, FP8 mixed-precision training and finegrained recomputation methods, we achieve remarkable pre-training throughput. For inference, we propose &lt;em&gt;multi-expert parallel collaboration&lt;/em&gt; method and &lt;em&gt;convolutional code quantization&lt;/em&gt; algorithm to achieve 4-bit/2-bit lossless quantization. Furthermore, we introduce PD disaggregation with dynamic role switching for effective resource utilization to enhance inference performance for ERNIE 4.5 MoE models. Built on &lt;a href=&#34;https://github.com/PaddlePaddle/Paddle&#34;&gt;PaddlePaddle&lt;/a&gt;, ERNIE 4.5 delivers high-performance inference across a wide range of hardware platforms.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modality-Specific Post-Training:&lt;/strong&gt; To meet the diverse requirements of real-world applications, we fine-tuned variants of the pre-trained model for specific modalities. Our LLMs are optimized for general-purpose language understanding and generation. The VLMs focuses on visuallanguage understanding and supports both thinking and non-thinking modes. Each model employed a combination of &lt;em&gt;Supervised Fine-tuning (SFT)&lt;/em&gt;, &lt;em&gt;Direct Preference Optimization (DPO)&lt;/em&gt; or a modified reinforcement learning method named &lt;em&gt;Unified Preference Optimization (UPO)&lt;/em&gt; for post-training.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Performance and Benchmark Results&lt;/h2&gt; &#xA;&lt;p&gt;ERNIE-4.5-300B-A47B-Base surpasses DeepSeek-V3-671B-A37B-Base on 22 out of 28 benchmarks, demonstrating leading performance across all major capability categories. This underscores the substantial improvements in generalization, reasoning, and knowledge-intensive tasks brought about by scaling up the ERNIE-4.5-Base model relative to other state-of-the-art large models. With a total parameter size of 21B (approximately 70% that of Qwen3-30B), ERNIE-4.5-21B-A3B-Base outperforms Qwen3-30B-A3B-Base on several math and reasoning benchmarks, including BBH and CMATH. ERNIE-4.5-21B-A3B-Base remains highly competitive given its significantly smaller model size, demonstrating notable parameter efficiency and favorable performance trade-offs.&lt;/p&gt; &#xA;&lt;p&gt;ERNIE-4.5-300B-A47B, the post trained model, demonstrates significant strengths in instruction following and knowledge tasks, as evidenced by the state-of-the-art scores on benchmarks such as IFEval, Multi-IF, SimpleQA, and ChineseSimpleQA. The lightweight model ERNIE-4.5-21B-A3B achieves competitive performance compared to Qwen3-30B-A3B, despite having approximately 30% fewer total parameters.&lt;/p&gt; &#xA;&lt;p&gt;In the non-thinking mode, ERNIE-4.5-VL exhibits outstanding proficiency in visual perception, document and chart understanding, and visual knowledge, performing strongly across a range of established benchmarks. Under&amp;nbsp;the&amp;nbsp;thinking&amp;nbsp;mode,&amp;nbsp;ERNIE-4.5-VL&amp;nbsp;not&amp;nbsp;only demonstrates&amp;nbsp;enhanced&amp;nbsp;reasoning&amp;nbsp;abilities&amp;nbsp;compared&amp;nbsp;to&amp;nbsp;the&amp;nbsp;non-thinking&amp;nbsp;mode,&amp;nbsp;but&amp;nbsp;also&amp;nbsp;retains&amp;nbsp;the&amp;nbsp;strong perception&amp;nbsp;capabilities&amp;nbsp;of&amp;nbsp;the&amp;nbsp;latter.&amp;nbsp;ERNIE-4.5-VL-424B-A47B&amp;nbsp;delivers&amp;nbsp;consistently&amp;nbsp;strong&amp;nbsp;results&amp;nbsp;across the&amp;nbsp;full&amp;nbsp;multimodal&amp;nbsp;evaluation&amp;nbsp;suite.&amp;nbsp;Its&amp;nbsp;thinking&amp;nbsp;mode&amp;nbsp;provides&amp;nbsp;a&amp;nbsp;distinct&amp;nbsp;advantage&amp;nbsp;on&amp;nbsp;reasoning-centric tasks,&amp;nbsp;narrowing&amp;nbsp;or&amp;nbsp;even&amp;nbsp;surpassing&amp;nbsp;the&amp;nbsp;gap&amp;nbsp;to&amp;nbsp;OpenAI-o1&amp;nbsp;on&amp;nbsp;challenging&amp;nbsp;benchmarks&amp;nbsp;such&amp;nbsp;as&amp;nbsp;MathVista, MMMU,&amp;nbsp;and&amp;nbsp;VisualPuzzle,&amp;nbsp;while&amp;nbsp;maintaining&amp;nbsp;competitive&amp;nbsp;performance&amp;nbsp;on&amp;nbsp;perception-focused&amp;nbsp;datasets like&amp;nbsp;CV-Bench&amp;nbsp;and&amp;nbsp;RealWorldQA. The lightweight vision-language model ERNIE-4.5-VL-28B-A3B achieves competitive or even superior performance compared to Qwen2.5-VL-7B and Qwen2.5-VL-32B across most benchmarks, despite using significantly fewer activation parameters. Notably, our lightweight model also supports both thinking and non-thinking modes, offering functionalities consistent with ERNIE-4.5-VL-424B-A47B.&lt;/p&gt; &#xA;&lt;h3&gt;Performace of ERNIE-4.5 pre-trained models&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://yiyan.baidu.com/blog/posts/ernie4.5/base_model_benchmark.png&#34; style=&#34;max-width: 80%; height: auto;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Performance of post-trained model ERNIE-4.5-300B-A47B&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://yiyan.baidu.com/blog/posts/ernie4.5/chat_model_benchmark1.png&#34; style=&#34;max-width: 80%; height: auto;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Performance of post-trained model ERNIE-4.5-21B-A3B&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/user-attachments/assets/5bacaae8-ef27-494d-8c65-589ba187a084&#34; style=&#34;max-width: 80%; height: auto;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Performance of post-trained multimodal models in thinking mode&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://yiyan.baidu.com/blog/posts/ernie4.5/vl_model_thinking_benchmark.png&#34; style=&#34;max-width: 80%; height: auto;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Performance of post-trained multimodal models in non-thinking mode&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/user-attachments/assets/3ad69a9d-1233-48be-a7c4-b816d3aa17ca&#34; style=&#34;max-width: 80%; height: auto;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Model Development&lt;/h2&gt; &#xA;&lt;p&gt;ERNIE 4.5 models are trained and deployed for inference using the &lt;a href=&#34;(https://github.com/PaddlePaddle/Paddle)&#34;&gt;PaddlePaddle&lt;/a&gt; framework. The full workflow of training, compression, and inference for ERNIE 4.5 is supported through the &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/docs/erniekit.md&#34;&gt;ERNIEKit&lt;/a&gt; and &lt;a href=&#34;https://github.com/PaddlePaddle/FastDeploy&#34;&gt;FastDeploy&lt;/a&gt; toolkit. The table below details the feature matrix of the ERNIE 4.5 model family for training and inference.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Training&lt;/th&gt; &#xA;    &lt;th&gt;Inference&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ERNIE-4.5-300B-A47B-Base&lt;/td&gt; &#xA;    &lt;td&gt;SFT/SFT-LoRA/DPO/DPO-LoRA&lt;/td&gt; &#xA;    &lt;td&gt;BF16 / W4A16C16 / W8A16C16 / FP8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ERNIE-4.5-300B-A47B&lt;/td&gt; &#xA;    &lt;td&gt;SFT/SFT-LoRA/DPO/DPO-LoRA/QAT&lt;/td&gt; &#xA;    &lt;td&gt;BF16 / W4A16C16 / W8A16C16 / W4A8C8 / FP8 / 2Bits&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ERNIE-4.5-21B-A3B-Base&lt;/td&gt; &#xA;    &lt;td&gt;SFT/SFT-LoRA/DPO/DPO-LoRA&lt;/td&gt; &#xA;    &lt;td&gt;BF16 / W4A16C16 / W8A16C16 / FP8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ERNIE-4.5-21B-A3B&lt;/td&gt; &#xA;    &lt;td&gt;SFT/SFT-LoRA/DPO/DPO-LoRA&lt;/td&gt; &#xA;    &lt;td&gt;BF16 / W4A16C16 / W8A16C16 / FP8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ERNIE-4.5-VL-424B-A47B-Base&lt;/td&gt; &#xA;    &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;    &lt;td&gt;BF16 / W4A16C16 / W8A16C16 / FP8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ERNIE-4.5-VL-424B-A47B&lt;/td&gt; &#xA;    &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;    &lt;td&gt;BF16 / W4A16C16 / W8A16C16 / FP8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ERNIE-4.5-VL-28B-A3B-Base&lt;/td&gt; &#xA;    &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;    &lt;td&gt;BF16 / W4A16C16 / W8A16C16 / FP8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ERNIE-4.5-VL-28B-A3B&lt;/td&gt; &#xA;    &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;    &lt;td&gt;BF16 / W4A16C16 / W8A16C16 / FP8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ERNIE-4.5-0.3B-Base&lt;/td&gt; &#xA;    &lt;td&gt;SFT/SFT-LoRA/DPO/DPO-LoRA&lt;/td&gt; &#xA;    &lt;td&gt;BF16 / W8A16C16 / FP8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ERNIE-4.5-0.3B&lt;/td&gt; &#xA;    &lt;td&gt;SFT/SFT-LoRA/DPO/DPO-LoRA&lt;/td&gt; &#xA;    &lt;td&gt;BF16 / W8A16C16 / FP8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: For different ERNIE 4.5 model, we provide diverse quantization schemes using the notation WxAxCx, where: W indicates weight precision, A indicates activation precision, C indicates KV Cache precision, x represents numerical precision.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ERNIEKit: ERNIE Development Toolkit Based on PaddlePaddle&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;ERNIEKit&lt;/strong&gt; is an industrial-grade training and compression development toolkit for ERNIE models based on PaddlePaddle, offering full-cycle development support for the ERNIE 4.5 model family. Key capabilities include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;High-performance pre-training implementation&lt;/li&gt; &#xA; &lt;li&gt;Full-parameter supervised fine-tuning (SFT)&lt;/li&gt; &#xA; &lt;li&gt;Direct Preference Optimization (DPO)&lt;/li&gt; &#xA; &lt;li&gt;Parameter-efficient fine-tuning and alignment (SFT-LoRA/DPO-LoRA)&lt;/li&gt; &#xA; &lt;li&gt;Quantization-Aware Training (QAT)&lt;/li&gt; &#xA; &lt;li&gt;Post-Training Quantization (PTQ) [WIP]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Minimum hardware requirements for training each model are documented &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/docs/erniekit.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Quick Start&lt;/h4&gt; &#xA;&lt;p&gt;When you install ERNIEKit successfully, you can start training ERNIE 4.5 models with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download model from huggingface&#xA;huggingface-cli download baidu/ERNIE-4.5-0.3B-Paddle --local-dir baidu/ERNIE-4.5-0.3B-Paddle&#xA;# 8K Sequence Length, SFT&#xA;erniekit train examples/configs/ERNIE-4.5-0.3B/sft/run_sft_8k.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For detailed guides on installation, CLI usage, WebUI, multi-node training, and advanced features, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/docs/erniekit.md&#34;&gt;ERNIEKit Training Document&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For detailed guides on High-performance pre-training, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/examples/pre-training/README.md&#34;&gt;Pre-Training Document&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ERNIEKit WebUI demo:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/6d44cb92-0826-42df-aa80-7656445e0f73&#34;&gt;https://github.com/user-attachments/assets/6d44cb92-0826-42df-aa80-7656445e0f73&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;FastDeployÔºöHigh-performance Inference and Deployment Toolkit for LLMs and VLMs Based on PaddlePaddle&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;FastDeploy&lt;/strong&gt; is an inference and deployment toolkit for large language models and visual language models, developed based on PaddlePaddle. It delivers production-ready, easy-to-use multi-hardware deployment solutions with multi-level load-balanced PD disaggregation, comprehensive quantization format support, OpenAI API server and vLLM compatible etc.&lt;/p&gt; &#xA;&lt;p&gt;For installation please refer to &lt;a href=&#34;https://github.com/PaddlePaddle/FastDeploy&#34;&gt;FastDeploy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Offline Inference&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastdeploy import LLM, SamplingParams&#xA;&#xA;prompt = &#34;Write me a poem about large language model.&#34;&#xA;sampling_params = SamplingParams(temperature=0.8, top_p=0.95)&#xA;&#xA;llm = LLM(model=&#34;baidu/ERNIE-4.5-0.3B-Paddle&#34;, max_model_len=32768)&#xA;&#xA;outputs = llm.generate(prompt, sampling_params)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Online Serving&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m fastdeploy.entrypoints.openai.api_server \&#xA;    --model &#34;baidu/ERNIE-4.5-0.3B-Paddle&#34; \&#xA;    --max-model-len 32768 \&#xA;    --port 9904&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more inference and deployment guides, please refer to &lt;a href=&#34;https://github.com/PaddlePaddle/FastDeploy&#34;&gt;FastDeploy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Cookbooks&lt;/h2&gt; &#xA;&lt;p&gt;Discover best-practice guides showcasing ERNIE‚Äôs capabilities across multiple domains:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Cookbook&lt;/th&gt; &#xA;    &lt;th&gt;Description&lt;/th&gt; &#xA;    &lt;th&gt;Gradio Demo&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/notebook/conversation_demo_en.ipynb&#34;&gt;Conversation&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Building conversational applications.&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/conversation_demo.py&#34;&gt;conversation_demo.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/notebook/simple_ernie_bot_demo_en.ipynb&#34;&gt;Simple ERNIE Bot&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Creating a lightweight web-based ERNIE Bot.&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/simple_ernie_bot_demo.py&#34;&gt;simple_ernie_bot_demo.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/notebook/web_search_demo_en.ipynb&#34;&gt;Web-Search-Enhanced Conversation&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Building conversational apps with integrated web search.&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/web_search_demo.py&#34;&gt;web_search_demo.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/notebook/knowledge_retrieval_demo_en.ipynb&#34;&gt;Knowledge Retrieval-based Q&amp;amp;A&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Building intelligent Q&amp;amp;A systems with private knowledge bases.&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/knowledge_retrieval_demo.py&#34;&gt;knowledge_retrieval_demo.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/notebook/advanced_search_demo_en.ipynb&#34;&gt;Advanced Search&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Building article-generation applications using deep information extraction.&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/advanced_search_demo.py&#34;&gt;advanced_search_demo.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/notebook/sft_tutorial_en.ipynb&#34;&gt;SFT tutorial&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Optimizing task performance through supervised fine-tuning with ERNIEKit.&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/notebook/dpo_tutorial_en.ipynb&#34;&gt;DPO tutorial&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Aligning models with human preferences using ERNIEKit.&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/notebook/text_recognition_tutorial_en.ipynb&#34;&gt;Text Recognition&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;A Comprehensive Guide to Developing Text Recognition for Non-Chinese and Non-English Languages Using ERNIE and PaddleOCR.&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/notebook/document_translation_tutorial_en.ipynb&#34;&gt;Document Translation&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Document Translation Practice Based on ERNIE and PaddleOCR.&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/cookbook/notebook/key_information_extraction_tutorial_en.ipynb&#34;&gt;Key Information Extraction&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Key Information Extraction in Contract Scenarios Based on ERNIE and PaddleOCR.&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PaddlePaddle WeChat official account&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Join the tech discussion group&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/864a45ec-0773-44b2-a2f1-c0e21e157792&#34; width=&#34;150&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/52e05674-7143-4207-8b19-67247fe88f55&#34; width=&#34;150&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{ernie2025technicalreport,&#xA;      title={ERNIE 4.5 Technical Report},&#xA;      author={Baidu-ERNIE-Team},&#xA;      year={2025},&#xA;      eprint={},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>