<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-19T01:41:21Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>waymo-research/waymo-open-dataset</title>
    <updated>2022-08-19T01:41:21Z</updated>
    <id>tag:github.com,2022-08-19:/waymo-research/waymo-open-dataset</id>
    <link href="https://github.com/waymo-research/waymo-open-dataset" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Waymo Open Dataset&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Waymo Open Dataset&lt;/h1&gt; &#xA;&lt;p&gt;We have released the Waymo Open Dataset publicly to aid the research community in making advancements in machine perception and autonomous driving technology.&lt;/p&gt; &#xA;&lt;p&gt;The Waymo Open Dataset is composed of two datasets - the Perception dataset with high resolution sensor data and labels for 2,030 scenes, and the Motion dataset with object trajectories and corresponding 3D maps for 103,354 scenes.&lt;/p&gt; &#xA;&lt;h2&gt;June 2022 Update&lt;/h2&gt; &#xA;&lt;p&gt;We released v1.4.0 of the Perception dataset.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added 2D video panoptic segmentation labels and supporting code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;May 2022 Update (part 2)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Released a &lt;a href=&#34;https://raw.githubusercontent.com/waymo-research/waymo-open-dataset/master/waymo-open-dataset/tutorial/tutorial_camera_only.ipynb&#34;&gt;tutorial&lt;/a&gt; for the 3D Camera-Only Detection Challenge.&lt;/li&gt; &#xA; &lt;li&gt;Added support for computing 3D-LET-APL in Python metrics ops. See &lt;code&gt;Compute Metrics&lt;/code&gt; in the &lt;a href=&#34;https://raw.githubusercontent.com/waymo-research/waymo-open-dataset/master/waymo-open-dataset/tutorial/tutorial_camera_only.ipynb&#34;&gt;tutorial&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Fixed a bug in the metrics implementation for the Occupancy and Flow Challenge.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;May 2022 Update&lt;/h2&gt; &#xA;&lt;p&gt;We released v1.3.2 of the Perception dataset to improve the quality and accuracy of the labels.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Updated 3D semantic segmentation labels, for better temporal consistency and to fix mislabeled points.&lt;/li&gt; &#xA; &lt;li&gt;Updated 2D key point labels to fix image cropping issues.&lt;/li&gt; &#xA; &lt;li&gt;Added &lt;code&gt;num_top_lidar_points_in_box&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/waymo-research/waymo-open-dataset/master/waymo_open_dataset/dataset.proto&#34;&gt;dataset.proto&lt;/a&gt; for the 3D Camera-Only Detection Challenge.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;April 2022 Update&lt;/h2&gt; &#xA;&lt;p&gt;We released v1.3.1 of the Perception dataset to support the 2022 Challenges and have updated this repository accordingly.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added metrics (LET-3D-APL and LET-3D-AP) for the 3D Camera-Only Detection Challenge.&lt;/li&gt; &#xA; &lt;li&gt;Added 80 segments of 20-second camera imagery, as a test set for the 3D Camera-Only Detection Challenge.&lt;/li&gt; &#xA; &lt;li&gt;Added z-axis speed and acceleration in &lt;a href=&#34;https://raw.githubusercontent.com/waymo-research/waymo-open-dataset/master/waymo_open_dataset/label.proto#L53-L60&#34;&gt;lidar label metadata&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Fixed some inconsistencies in &lt;code&gt;projected_lidar_labels&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/waymo-research/waymo-open-dataset/master/waymo_open_dataset/dataset.proto&#34;&gt;dataset.proto&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Updated the default configuration for the Occupancy and Flow Challenge, switching from aggregate waypoints to &lt;a href=&#34;https://raw.githubusercontent.com/waymo-research/waymo-open-dataset/master/waymo_open_dataset/protos/occupancy_flow_metrics.proto#L38-L55&#34;&gt;subsampled waypoints&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Updated the &lt;a href=&#34;https://raw.githubusercontent.com/waymo-research/waymo-open-dataset/master/waymo-open-dataset/tutorial/tutorial_3d_semseg.ipynb&#34;&gt;tutorial&lt;/a&gt; for 3D Semantic Segmentation Challenge with more detailed instructions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;March 2022 Update&lt;/h2&gt; &#xA;&lt;p&gt;We released v1.3.0 of the Perception dataset and the 2022 challenges. We have updated this repository to add support for the new labels and the challenges.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added 3D semantic segmentation labels, tutorial, and metrics.&lt;/li&gt; &#xA; &lt;li&gt;Added 2D and 3D keypoint labels, tutorial, and metrics.&lt;/li&gt; &#xA; &lt;li&gt;Added correspondence between 2D (camera) and 3D (lidar) labels (pedestrian only).&lt;/li&gt; &#xA; &lt;li&gt;Added tutorial and utilities for Occupancy Flow Prediction Challenge.&lt;/li&gt; &#xA; &lt;li&gt;Added the soft mAP metric for Motion Prediction Challenge.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;September 2021 Update&lt;/h2&gt; &#xA;&lt;p&gt;We released v1.1 of the Motion dataset to include lane connectivity information. To read more on the technical details, please read &lt;a href=&#34;https://raw.githubusercontent.com/waymo-research/waymo-open-dataset/master/docs/lane_neighbors_and_boundaries.md&#34;&gt;lane_neighbors_and_boundaries.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added lane connections. Each lane has a list of lane IDs that enter or exit the lane.&lt;/li&gt; &#xA; &lt;li&gt;Added lane boundaries. Each lane has a list of left and right boundary features associated with the lane and the segment of the lane where the boundary is active.&lt;/li&gt; &#xA; &lt;li&gt;Added lane neighbors. Each lane has a list of left and right neighboring lanes. These are lanes an agent may make a lane change into.&lt;/li&gt; &#xA; &lt;li&gt;Improved timestamp precision.&lt;/li&gt; &#xA; &lt;li&gt;Improved stop sign Z values.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;March 2021 Update&lt;/h2&gt; &#xA;&lt;p&gt;We expanded the Waymo Open Dataset to also include a Motion dataset comprising object trajectories and corresponding 3D maps for over 100,000 segments. We have updated this repository to add support for this new dataset. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/waymo-research/waymo-open-dataset/master/docs/quick_start.md&#34;&gt;Quick Start&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, we added instructions and examples for the real-time detection challenges. Please follow these &lt;a href=&#34;https://raw.githubusercontent.com/waymo-research/waymo-open-dataset/master/waymo_open_dataset/latency/README.md&#34;&gt;Instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Website&lt;/h2&gt; &#xA;&lt;p&gt;To read more about the dataset and access it, please visit &lt;a href=&#34;https://www.waymo.com/open&#34;&gt;https://www.waymo.com/open&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;p&gt;This code repository contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Definition of the dataset format&lt;/li&gt; &#xA; &lt;li&gt;Evaluation metrics&lt;/li&gt; &#xA; &lt;li&gt;Helper functions in TensorFlow to help with building models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/waymo-research/waymo-open-dataset/master/docs/quick_start.md&#34;&gt;Quick Start&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This code repository (excluding third_party) is licensed under the Apache License, Version 2.0. Code appearing in third_party is licensed under terms appearing therein.&lt;/p&gt; &#xA;&lt;p&gt;The Waymo Open Dataset itself is licensed under separate terms. Please visit &lt;a href=&#34;https://waymo.com/open/terms/&#34;&gt;https://waymo.com/open/terms/&lt;/a&gt; for details. Code located at third_party/camera is licensed under a BSD 3-clause copyright license + an additional limited patent license applicable only when the code is used to process data from the Waymo Open Dataset as authorized by and in compliance with the Waymo Dataset License Agreement for Non-Commercial Use. See third_party/camera for details.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;h3&gt;for Perception dataset&lt;/h3&gt; &#xA;&lt;p&gt;@InProceedings{Sun_2020_CVPR, author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir}, title = {Scalability in Perception for Autonomous Driving: Waymo Open Dataset}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2020} }&lt;/p&gt; &#xA;&lt;h3&gt;for Motion dataset&lt;/h3&gt; &#xA;&lt;p&gt;@InProceedings{Ettinger_2021_ICCV, author={Ettinger, Scott and Cheng, Shuyang and Caine, Benjamin and Liu, Chenxi and Zhao, Hang and Pradhan, Sabeek and Chai, Yuning and Sapp, Ben and Qi, Charles R. and Zhou, Yin and Yang, Zoey and Chouard, Aur&#39;elien and Sun, Pei and Ngiam, Jiquan and Vasudevan, Vijay and McCauley, Alexander and Shlens, Jonathon and Anguelov, Dragomir}, title={Large Scale Interactive Motion Forecasting for Autonomous Driving: The Waymo Open Motion Dataset}, booktitle= Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, month={October}, year={2021}, pages={9710-9719} }&lt;/p&gt; &#xA;&lt;h2&gt;Dataset Metadata&lt;/h2&gt; &#xA;&lt;p&gt;The following table is necessary for this dataset to be indexed by search engines such as &lt;a href=&#34;https://g.co/datasetsearch&#34;&gt;Google Dataset Search&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div itemscope itemtype=&#34;http://schema.org/Dataset&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;property&lt;/th&gt; &#xA;    &lt;th&gt;value&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;name&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code itemprop=&#34;name&#34;&gt;Waymo Open Dataset: An autonomous driving dataset&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;alternateName&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code itemprop=&#34;alternateName&#34;&gt;Waymo Open Dataset&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;url&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code itemprop=&#34;url&#34;&gt;https://github.com/waymo-research/waymo-open-dataset&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;sameAs&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code itemprop=&#34;sameAs&#34;&gt;https://github.com/waymo-research/waymo-open-dataset&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;sameAs&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code itemprop=&#34;sameAs&#34;&gt;https://www.waymo.com/open&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;description&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code itemprop=&#34;description&#34;&gt;The Waymo Open Dataset is comprised of high-resolution sensor data collected by autonomous vehicles operated by the Waymo Driver in a wide variety of conditions. We’re releasing this dataset publicly to aid the research community in making advancements in machine perception and self-driving technology.&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;provider&lt;/td&gt; &#xA;    &lt;td&gt; &#xA;     &lt;div itemscope itemtype=&#34;http://schema.org/Organization&#34; itemprop=&#34;provider&#34;&gt; &#xA;      &lt;table&gt; &#xA;       &lt;tbody&gt;&#xA;        &lt;tr&gt; &#xA;         &lt;th&gt;property&lt;/th&gt; &#xA;         &lt;th&gt;value&lt;/th&gt; &#xA;        &lt;/tr&gt; &#xA;        &lt;tr&gt; &#xA;         &lt;td&gt;name&lt;/td&gt; &#xA;         &lt;td&gt;&lt;code itemprop=&#34;name&#34;&gt;Waymo&lt;/code&gt;&lt;/td&gt; &#xA;        &lt;/tr&gt; &#xA;        &lt;tr&gt; &#xA;         &lt;td&gt;sameAs&lt;/td&gt; &#xA;         &lt;td&gt;&lt;code itemprop=&#34;sameAs&#34;&gt;https://en.wikipedia.org/wiki/Waymo&lt;/code&gt;&lt;/td&gt; &#xA;        &lt;/tr&gt; &#xA;       &lt;/tbody&gt;&#xA;      &lt;/table&gt; &#xA;     &lt;/div&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;license&lt;/td&gt; &#xA;    &lt;td&gt; &#xA;     &lt;div itemscope itemtype=&#34;http://schema.org/CreativeWork&#34; itemprop=&#34;license&#34;&gt; &#xA;      &lt;table&gt; &#xA;       &lt;tbody&gt;&#xA;        &lt;tr&gt; &#xA;         &lt;th&gt;property&lt;/th&gt; &#xA;         &lt;th&gt;value&lt;/th&gt; &#xA;        &lt;/tr&gt; &#xA;        &lt;tr&gt; &#xA;         &lt;td&gt;name&lt;/td&gt; &#xA;         &lt;td&gt;&lt;code itemprop=&#34;name&#34;&gt;Waymo Dataset License Agreement for Non-Commercial Use (August 2019)&lt;/code&gt;&lt;/td&gt; &#xA;        &lt;/tr&gt; &#xA;        &lt;tr&gt; &#xA;         &lt;td&gt;url&lt;/td&gt; &#xA;         &lt;td&gt;&lt;code itemprop=&#34;url&#34;&gt;https://waymo.com/open/terms/&lt;/code&gt;&lt;/td&gt; &#xA;        &lt;/tr&gt; &#xA;       &lt;/tbody&gt;&#xA;      &lt;/table&gt; &#xA;     &lt;/div&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>boyu-ai/Hands-on-RL</title>
    <updated>2022-08-19T01:41:21Z</updated>
    <id>tag:github.com,2022-08-19:/boyu-ai/Hands-on-RL</id>
    <link href="https://github.com/boyu-ai/Hands-on-RL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;https://hrl.boyuai.com/&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;动手学强化学习&lt;/h1&gt; &#xA;&lt;p&gt;欢迎来到《动手学强化学习》（Hands-on Reinforcement Learning）的地带。该系列从强化学习的定义等基础讲起，一步步由浅入深，介绍目前一些主流的强化学习算法。每一章内容都是一个Jupyter Notebook，内含详细的图文介绍和代码讲解。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;由于GitHub上渲染notebook效果有限，我们推荐读者前往&lt;a href=&#34;https://hrl.boyuai.com/&#34;&gt;Hands-on RL主页&lt;/a&gt;进行浏览，我们在此提供了纯代码版本的notebook，供大家下载运行。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;欢迎在&lt;a href=&#34;https://item.jd.com/13129509.html&#34;&gt;京东&lt;/a&gt;和&lt;a href=&#34;http://product.dangdang.com/29391150.html&#34;&gt;当当网&lt;/a&gt;购买《动手学强化学习》。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果你发现了本书的任何问题，或者有任何改善建议的，欢迎填写&lt;a href=&#34;https://wj.qq.com/s2/8824793/9405/&#34;&gt;问卷&lt;/a&gt;反馈给我们！&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;本书配套的强化学习课程已上线到&lt;a href=&#34;https://www.boyuai.com/elites/course/xVqhU42F5IDky94x&#34;&gt;伯禹学习平台&lt;/a&gt;，所有人都可以免费学习和讨论。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://boyuai.oss-cn-shanghai.aliyuncs.com/disk/tmp/hrl-poster.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dglai/Graph-Neural-Networks-in-Life-Sciences</title>
    <updated>2022-08-19T01:41:21Z</updated>
    <id>tag:github.com,2022-08-19:/dglai/Graph-Neural-Networks-in-Life-Sciences</id>
    <link href="https://github.com/dglai/Graph-Neural-Networks-in-Life-Sciences" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;KDD 2022 Hands-on Tutorial: Graph Neural Networks in Life Sciences: Opportunities and Solutions&lt;/h1&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Graphs (or networks) are ubiquitous representations in life sciences and medicine, from molecular interactions maps, signaling transduction pathways, to graphs of scientific knowledge , and patient-disease-intervention relationships derived from population studies and/or real-world evidences. Recent advance in graph machine learning (ML) approaches such as graph neural networks (GNNs) has transformed a diverse set of problems relying on biomedical networks that traditionally depend on descriptive topological data analyses. Small- and macro- molecules that were not modeled as graphs also saw a bloom in GNN-based algorithms improving the state-of-the-art performance for learning their properties. Comparing to graph ML applications from other domains, life sciences offer many unique problems and nuances ranging from graph construction to graph-level, and bi-graph-level supervision tasks.&lt;/p&gt; &#xA;&lt;p&gt;The objective of this tutorial is twofold. First, it will provide a comprehensive overview of the types of biomedical graphs/networks, the underlying biological and medical problems, and the applications of graph ML algorithms for solving those problems. Second, it will showcase four concrete GNN solutions in life sciences with hands-on experience for the attendees. These hands-on sessions will cover: 1) training and fine-tuning GNN models for small-molecule property prediction on atomic graphs, 2) macro-molecule property and function prediction on residue graphs, 3) bi-graph based binding affinity prediction for protein-ligand pairs, and 4) organizing and generating new knowledge for drug discovery and repurposing with knowledge graphs. This tutorial will also instruct the attendees to develop in two extensions of the software library Deep Graph Library (DGL), including DGL-lifesci and DGL-KE, so that they could jumpstart their own graph ML journey to advance life science research and development.&lt;/p&gt; &#xA;&lt;h2&gt;Outline&lt;/h2&gt; &#xA;&lt;p&gt;The tutorial introduces to data science researchers and practitioners graph neural network (GNN) based approaches applied to various problems in biomedical sciences and healthcare. The tutorial first provides an overview of the various opportunities in leveraging GNNs for small molecules, macromolecules and biomedical knowledge graphs. The four hands-on activities will provide the participants a diverse set of biomedical problems and in particular how to deploy a GNN-based library for these applications leading to biological phenotype prediction, interaction prediction, affinity prediction and drug discovery.&lt;/p&gt; &#xA;&lt;p&gt;The tutorial will be broken up into the following five sections:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Section 1: Overview of Graph ML in biomedical science&lt;/em&gt;. This section describes different types of graphs commonly used in biomedical sciences and how graph-based machine learning approaches like GNNs can be leveraged. In particular, we will cover single-entity biomedical networks including gene regulatory network and protein-protein interaction networks, as well as multi-entity networks such as knowledge graphs of proteins, genes, diseases, symptoms, and drugs. This section also introduces graph representations for small and large molecules such as organic compounds and proteins, which can be modeled as independent graphs of atoms and residues, respectively. [section format: slides]&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Section 2: Making sense of small molecules with GNNs&lt;/em&gt;. This section demonstrates how to develop end-to-end graph-based ML pipeline for molecular property prediction. The pipeline first covers how to construct features from atom graphs for small organic compounds. Then, it will cover two use cases using DGL-lifesci command-line interface: 1) training a GNN for molecular property prediction from scratch, and 2) fine-tuning a pre-trained GNN for molecular property prediction. [section format: hands-on with Jupyter Notebook]&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Section 3:&lt;/em&gt; &lt;em&gt;Making sense of macro-molecules with GNNs&lt;/em&gt;. This section demonstrates how to use GNNs to predict properties for macro-molecules including RNAs and proteins. We will cover two hands-on case studies: 1) Prediction of COVID-19 mRNA vaccine degradation with GCN, and 2) protein function prediction using an equivariant GNN on graphs of amino acid residues. [section format: hands-on with Jupyter Notebook]&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Section 4: Going beyond single graph, bi-graph based binding affinity prediction for protein-ligand pairs&lt;/em&gt;. This section demonstrates a case study for making predictions between a pair of graphs. Protein-ligand binding affinity prediction is important for candidate drug screening during the early stage of drug discovery. We demonstrate how PotentialNet can be used for this task, as well as a novel molecular data anonymization procedure for protecting IP of molecular structures. [section format: hands-on with Jupyter Notebook]&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Section 5: Organizing and generating new knowledge for drug discovery and repurposing with knowledge graphs (KGs).&lt;/em&gt; This section showcases another application of graphs in life sciences by employing large-scale KGs to organize the information from diverse medical sources and make prediction on these KGs. KG is a directed heterogeneous multigraph whose node and relation types have domain-specific semantics. We will review three approaches to construct such medical KGs 1) mining medical documents and publications 2) processing and stitching together different KGs coming from various medical databases 3) converting relational databases to KGs. We will show examples detailing how to construct such KGs. The resulting KGs store information efficiently and can be used for KG completion, drug repurposing, and question answering among other tasks. We will review notebooks showcasing how to use the KGs and graph ML to make predictions in these KGs. We also will explain common objectives used for KG completion. [section format: hands-on with Jupyter Notebook]&lt;/p&gt; &#xA;&lt;h2&gt;Instructions for the hands-on sessions:&lt;/h2&gt; &#xA;&lt;p&gt;This workshop requires a Jupter Notebook and related data. For the purposes of this tutorial, we will be using AWS to set up our environment.&lt;/p&gt; &#xA;&lt;p&gt;Within the AWS environment, we will use&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SageMaker notebook instance: An Amazon SageMaker notebook instance is a machine learning (ML) compute instance running the Jupyter Notebook App. SageMaker manages creating the instance and related resources. Use Jupyter notebooks in your notebook instance to prepare and process data, write code to train models, deploy models to SageMaker hosting, and test or validate your models.&lt;/li&gt; &#xA; &lt;li&gt;Neptune database instance: Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications.&lt;/li&gt; &#xA; &lt;li&gt;Neptune ML: Amazon Neptune ML is a new capability of Neptune that uses Graph Neural Networks (GNNs), a machine learning technique purpose-built for graphs, to make easy, fast, and more accurate predictions using graph data. With Neptune ML, you can improve the accuracy of most predictions for graphs by over 50% when compared to making predictions using non-graph methods.&lt;/li&gt; &#xA; &lt;li&gt;IAM execution roles for SageMaker: An IAM role is an IAM identity that you can create in your AWS account that has specific permissions you can use with your SageMaker notebook.&lt;/li&gt; &#xA; &lt;li&gt;S3 Bucket: Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. As part of this workshop, this AWS environment has already been set up for you via AWS Event Engine.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Head to &lt;a href=&#34;https://dashboard.eventengine.run/login&#34;&gt;https://dashboard.eventengine.run/login&lt;/a&gt; and enter the event engine hash &lt;code&gt;7cc6-1b4e149c64-b8&lt;/code&gt;. You will be asked to login with an email to receive the OTP to get an AWS Account. Follow the instructions on the website&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Click on AWS Console&lt;/li&gt; &#xA; &lt;li&gt;On the window popup, select &lt;code&gt;Open Console&lt;/code&gt;. This will open an AWS Console. The AWS Management Console is a browser-based GUI for Amazon Web Services (AWS). Through the console, a customer can manage their cloud computing, cloud storage and other resources running on the Amazon Web Services infrastructure.&lt;/li&gt; &#xA; &lt;li&gt;On the top right, ensure &lt;code&gt;N.Virginia (us-east-1)&lt;/code&gt; is selected. If for any reason, you are logged into the different region, please switch to N.Virginia.&lt;/li&gt; &#xA; &lt;li&gt;On the top of the Console, type in &lt;code&gt;SageMaker&lt;/code&gt; in the search bar&lt;/li&gt; &#xA; &lt;li&gt;On the left sidebar, head to &lt;code&gt;Notebook&lt;/code&gt; &amp;gt; &lt;code&gt;Notebook instances&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;You will see an instance set up. Select &lt;code&gt;Open Jupyter&lt;/code&gt; on the right side of the page under Actions. This will open a Jupyer notebook interface hosted on Amazon SageMaker&lt;/li&gt; &#xA; &lt;li&gt;Time to begin with your workshop!&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>