<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-20T01:25:20Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>opendilab/LMDrive</title>
    <updated>2023-12-20T01:25:20Z</updated>
    <id>tag:github.com,2023-12-20:/opendilab/LMDrive</id>
    <link href="https://github.com/opendilab/LMDrive" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LMDrive: Closed-Loop End-to-End Driving with Large Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LMDrive: Closed-Loop End-to-End Driving with Large Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;An end-to-end, closed-loop, language-based autonomous driving framework, which interacts with the dynamic environment via multi-modal multi-view sensor data and natural language instructions.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://github.com/opendilab/LMDrive&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2312.07488&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/datasets/deepcs233/LMDrive&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#lmdrive-weigths&#34;&gt;Model Zoo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hits.seeyoufarm.com&#34;&gt;&lt;img src=&#34;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fopendilab%2FLMDrive&amp;amp;count_bg=%2379C83D&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=hits&amp;amp;edge_flat=false&#34; alt=&#34;Hits&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;800&#34; src=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/assets/pipeline.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://hao-shao.com/&#34;&gt;Hao Shao&lt;/a&gt;, Yuxuan Hu, &lt;a href=&#34;https://letianwang0.wixsite.com/myhome&#34;&gt;Letian Wang&lt;/a&gt;, &lt;a href=&#34;https://www.trailab.utias.utoronto.ca/stevenwaslander&#34;&gt;Steven L. Waslander&lt;/a&gt;, &lt;a href=&#34;https://liuyu.us/&#34;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&#34;http://www.ee.cuhk.edu.hk/~hsli/&#34;&gt;Hongsheng Li&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This repository contains code for the paper &lt;a href=&#34;https://arxiv.org/abs/2312.07488&#34;&gt;LMDrive: Closed-Loop End-to-End Driving with Large Language Models&lt;/a&gt;. This work proposes a novel language-guided, end-to-end, closed-loop autonomous driving framework.&lt;/p&gt; &#xA;&lt;h2&gt;Demo Video&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;video width=&#34;800&#34; src=&#34;https://github.com/opendilab/LMDrive/assets/17512647/65b2785d-e8bc-4ec1-ac86-e077299a465d&#34;&gt;&lt;/video&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#lmdrive-weigths&#34;&gt;Model Weigths&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#dataset&#34;&gt;Dataset&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#data-generation&#34;&gt;Data Generation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#data-pre-procession&#34;&gt;Data Pre-procession&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#data-parsing&#34;&gt;Data Parsing&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#training&#34;&gt;Training&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#vision-encoder-pre-training&#34;&gt;Vision encoder pre-training&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#instruction-finetuning&#34;&gt;Instruction finetuning&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Our project is built on three parts: (1) vision encoder (corresponding repo: timm); (2) vision LLM (corresponding repo: LAVIS); (3) data collection, agent controller (corresponding repo: InterFuser, Leaderboard, ScenarioRunner).&lt;/p&gt; &#xA;&lt;p&gt;Install anaconda&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;wget https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh&#xA;bash Anaconda3-2020.11-Linux-x86_64.sh&#xA;source ~/.bashrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Clone the repo and build the environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;git clone https://github.com/opendilab/LMDrive.git&#xA;cd LMDrive&#xA;conda create -n lmdrive python=3.8&#xA;conda activate lmdrive&#xA;cd vision_encoder&#xA;python setup.py develop # if you have installed timm before, please uninstall it&#xA;cd ../LAVIS&#xA;python setup.py develop # if you have installed LAVIS before, please uninstall it&#xA;pip3 install -r requirements.txt&#xA;&#xA;pip install flash-attn --no-build-isolation # optional&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download and setup CARLA 0.9.10.1&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;chmod +x setup_carla.sh&#xA;./setup_carla.sh&#xA;pip install carla&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;LMDrive Weigths&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in including any other details in Model Zoo, please open an issue :)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Version&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;VisionEncoder&lt;/th&gt; &#xA;   &lt;th&gt;LLM-base&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DS (LangAuto)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DS (LangAuto-short)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LMDrive-1.0 (LLaVA-v1.5-7B)&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepcs233/LMDrive-llava-v1.5-7b-v1.0&#34;&gt;deepcs233/LMDrive-llava-v1.5-7b-v1.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepcs233/LMDrive-vision-encoder-r50-v1.0&#34;&gt;R50&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/liuhaotian/llava-v1.5-7b&#34;&gt;LLaVA-v1.5-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LMDrive-1.0 (Vicuna-v1.5-7B)&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepcs233/LMDrive-vicuna-v1.5-7b-v1.0&#34;&gt;deepcs233/LMDrive-vicuna-v1.5-7b-v1.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepcs233/LMDrive-vision-encoder-r50-v1.0&#34;&gt;R50&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/lmsys/vicuna-7b-v1.5-16k&#34;&gt;Vicuna-v1.5-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LMDrive-1.0 (LLaMA-7B)&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepcs233/LMDrive-llama-7b-v1.0&#34;&gt;deepcs233/LMDrive-llama-7b-v1.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepcs233/LMDrive-vision-encoder-r50-v1.0&#34;&gt;R50&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/huggyllama/llama-7b&#34;&gt;LLaMA-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;DS denotes the driving score&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;We aim to develop an intelligent driving agent that can generate driving actions based on three sources of input: 1) sensor data (multi-view camera and LiDAR), so that the agent can generate actions that are aware of and compliant with the current scene; 2) navigation instructions (e.g. lane changing, turning), so that the agent can drive to meet the requirement in natural language (instruction from humans or navigation software); and 3) human notice instruction, so that the agent can interact with humans and adapt to human&#39;s suggestions and preferences (e.g. pay attention to adversarial events, deal with long-tail events, etc).&lt;/p&gt; &#xA;&lt;p&gt;We provide a dataset with about 64K data clips, where each clip includes one navigation instruction, several notice instructions, a sequence of multi-modal multi-view sensor data, and control signals. The duration of the clip spans from 2 to 20 seconds. The dataset used in our paper can be downloaded &lt;a href=&#34;https://huggingface.co/datasets/deepcs233/LMDrive&#34;&gt;here&lt;/a&gt;. If you want to create your own dataset, please follow the steps we&#39;ve outlined below.&lt;/p&gt; &#xA;&lt;h3&gt;Overview&lt;/h3&gt; &#xA;&lt;p&gt;The data is generated with &lt;code&gt;leaderboard/team_code/auto_pilot.py&lt;/code&gt; in 8 CARLA towns using the routes and scenarios files provided at &lt;code&gt;leaderboard/data&lt;/code&gt; on CARLA 0.9.10.1 . The dataset is collected at a high frequency (~10Hz).&lt;/p&gt; &#xA;&lt;p&gt;Once you have downloaded our dataset or collected your own dataset, it&#39;s necessary to systematically organize the data as follows. DATASET_ROOT is the root directory where your dataset is stored.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;├── $DATASET_ROOT&#xA;│   └── dataset_index.txt  # for vision encoder pretraining&#xA;│   └── navigation_instruction_list.txt  # for instruction finetuning&#xA;│   └── notice_instruction_list.json  # for instruction finetuning&#xA;│   └── routes_town06_long_w7_11_28_18_28_35  #  data folder&#xA;│   └── routes_town01_short_w2_11_16_08_27_10&#xA;│   └── routes_town02_short_w2_11_16_22_55_25&#xA;│   └── routes_town01_short_w2_11_16_11_44_08 &#xA;      ├── rgb_full&#xA;      ├── lidar&#xA;      └── ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;navigation_instruction_list.txt&lt;/code&gt; and &lt;code&gt;notice_instruction_list.txt&lt;/code&gt; can be generated with our scripts by the data parsing &lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#data-parsing&#34;&gt;scripts&lt;/a&gt;. Each subfolder in the dataset you&#39;ve collected should be structured as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;- routes_town(town_id)_{tiny,short,long}_w(weather_id)_timestamp: corresponding to different towns and routes files&#xA;    - routes_X: contains data for an individual route&#xA;        - rgb_full: a big multi-view camera image at 400x1200 resolution, which can be split into four images (left, center, right, rear)&#xA;        - lidar: 3d point cloud in .npy format. It only includes the LiDAR points captured in 1/20 second, covering 180 degrees of horizontal view. So if you want to utilize 360 degrees of view, you need to merge it with the data from lidar_odd.&#xA;        - lidar_odd: 3d point cloud in .npy format.&#xA;        - birdview: topdown segmentation images, LAV and LBC used this type of data for training&#xA;        - topdown: similar to birdview but it&#39;s captured by the down-facing camera&#xA;        - 3d_bbs: 3d bounding boxes for different agents&#xA;        - affordances: different types of affordances&#xA;        - actors_data: contains the positions, velocities and other metadata of surrounding vehicles and the traffic lights&#xA;        - measurements: contains ego agent&#39;s position, velocity, future waypoints, and other metadata&#xA;        - measurements_full: merges measurement and actors_data&#xA;        - measurements_all.json: merges the files in measurement_full into a single file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;$DATASET_ROOT&lt;/code&gt; directory must contain a file named &lt;code&gt;dataset_index.txt&lt;/code&gt;, which can be generated by our data pre-processing &lt;a href=&#34;https://raw.githubusercontent.com/opendilab/LMDrive/main/#data-pre-procession&#34;&gt;script&lt;/a&gt;. It should list the training and evaluation data in the following format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;relative_route_path_dir&amp;gt; &amp;lt;num_data_frames_in_this_dir&amp;gt;&#xA;routes_town06_long_w7_11_28_18_28_35/ 1062&#xA;routes_town01_short_w2_11_16_08_27_10/ 1785&#xA;routes_town01_short_w2_11_16_09_55_05/ 918&#xA;routes_town02_short_w2_11_16_22_55_25/ 134&#xA;routes_town01_short_w2_11_16_11_44_08/ 569&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, &lt;code&gt;&amp;lt;relative_route_path_dir&amp;gt;&lt;/code&gt; should be a relative path to the &lt;code&gt;$DATASET_ROOT&lt;/code&gt;. The training code will concatenate the &lt;code&gt;$DATASET_ROOT&lt;/code&gt; and &lt;code&gt;&amp;lt;relative_route_path_dir&amp;gt;&lt;/code&gt; to create the full path for loading the data. In this format, 1062 represents the number of frames in the routes_town06_long_w7_11_28_18_28_35/rgb_full directory or routes_town06_long_w7_11_28_18_28_35/lidar, etc.&lt;/p&gt; &#xA;&lt;h3&gt;Data Generation&lt;/h3&gt; &#xA;&lt;h4&gt;Data Generation with multiple CARLA Servers&lt;/h4&gt; &#xA;&lt;p&gt;In addition to the dataset, we have also provided all the scripts used for generating data and these can be modified as required for different CARLA versions. The dataset is collected by a rule-based expert agent in different weathers and towns.&lt;/p&gt; &#xA;&lt;h5&gt;Running CARLA Servers&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Start 4 carla servers: ip [localhost], port [2000, 2002, 2004, 2006]. You can adjust the number of CARLA servers according to your situation and more servers can collect more data. If you use N servers to collect data, it means you have collected data N times on each route, except that the weather and traffic scenarios are random each time.&#xA;&#xA;cd carla&#xA;CUDA_VISIBLE_DEVICES=0 ./CarlaUE4.sh --world-port=2000 -opengl &amp;amp;&#xA;CUDA_VISIBLE_DEVICES=1 ./CarlaUE4.sh --world-port=2002 -opengl &amp;amp;&#xA;CUDA_VISIBLE_DEVICES=2 ./CarlaUE4.sh --world-port=2004 -opengl &amp;amp;&#xA;CUDA_VISIBLE_DEVICES=3 ./CarlaUE4.sh --world-port=2006 -opengl &amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instructions for setting up docker are available &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker&#34;&gt;here&lt;/a&gt;. Pull the docker image of CARLA 0.9.10.1 &lt;code&gt;docker pull carlasim/carla:0.9.10.1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Docker 18:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -it --rm -p 2000-2002:2000-2002 --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=0 carlasim/carla:0.9.10.1 ./CarlaUE4.sh --world-port=2000 -opengl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker 19:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;docker run -it --rm --net=host --gpus &#39;&#34;device=0&#34;&#39; carlasim/carla:0.9.10.1 ./CarlaUE4.sh --world-port=2000 -opengl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the docker container doesn&#39;t start properly then add another environment variable &lt;code&gt;-e SDL_AUDIODRIVER=dsp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Run the Autopilot&lt;/h5&gt; &#xA;&lt;p&gt;Generate scripts for collecting data in batches.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd dataset&#xA;python init_dir.py&#xA;cd ..&#xA;cd data_collection&#xA;python generate_yamls.py # You can modify FPS, waypoints distribution strength ...&#xA;&#xA;# If you do not use 4 servers, the following Python scripts are needed to modify&#xA;python generate_bashs.py&#xA;python generate_batch_collect.py &#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run batch-run scripts of the town and route type that you need to collect.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash data_collection/batch_run/run_route_routes_town01_long.sh&#xA;bash data_collection/batch_run/run_route_routes_town01_short.sh&#xA;...&#xA;bash data_collection/batch_run/run_route_routes_town07_tiny.sh&#xA;...&#xA;bash data_collection/batch_run/run_route_routes_town10_tiny.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Our scripts will use a random weather condition for data collection&lt;/p&gt; &#xA;&lt;h5&gt;Data Generation with a single CARLA Server&lt;/h5&gt; &#xA;&lt;p&gt;With a single CARLA server, roll out the autopilot to start data generation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;carla/CarlaUE4.sh --world-port=2000 -opengl&#xA;./leaderboard/scripts/run_evaluation.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The expert agent used for data generation is defined in &lt;code&gt;leaderboard/team_code/auto_pilot.py&lt;/code&gt;. Different variables which need to be set are specified in &lt;code&gt;leaderboard/scripts/run_evaluation.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Data Pre-procession&lt;/h3&gt; &#xA;&lt;p&gt;We provide some Python scripts for pre-processing the collected data in &lt;code&gt;tools/data_preprocessing&lt;/code&gt;, some of them are optional. Please execute them &lt;strong&gt;in the order&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;python get_list_file.py $DATASET_ROOT&lt;/code&gt;: obtain the dataset_list.txt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python batch_merge_data.py $DATASET_ROOT&lt;/code&gt;: merge several scattered data files into one file to reduce IO time when training. &lt;strong&gt;[Optional]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python batch_rm_rgb_data.py $DATASET_ROOT&lt;/code&gt;: delete redundant files after we have merged them into new files. &lt;strong&gt;[Optional]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python batch_stat_blocked_data.py $DATASET_ROOT&lt;/code&gt;: find the frames that the ego-vehicle is blocked for a long time. By removing them, we can enhance data distribution and decrease the overall data size.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python batch_rm_blocked_data.py $DATASET_ROOT&lt;/code&gt;: delete the blocked frames.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python batch_recollect_data.py $DATASET_ROOT&lt;/code&gt;: since we have removed some frames, we need to reorganize them to ensure that the frame ids are continuous.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python batch_merge_measurements.py $DATASET_ROOT&lt;/code&gt;: merge the measurement files from all frames in one route folder to reduce IO time&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Data Parsing&lt;/h3&gt; &#xA;&lt;p&gt;After collecting and pre-processing the data, we need to parse the navigation instructions and notice instructions data with some Python scripts in &lt;code&gt;tools/data_parsing&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The script for parsing navigation instructions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 parse_instruction.py $DATSET_ROOT &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The parsed navigation clips will be saved in &lt;code&gt;$DATSET_ROOT/navigation_instruction_list.txt&lt;/code&gt;, under the root directory of the dataset.&lt;/p&gt; &#xA;&lt;p&gt;The script for parsing notice instructions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 parse_notice.py $DATSET_ROOT &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The parsed notice clips will be saved in &lt;code&gt;$DATSET_ROOT/notice_instruction_list.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The script for parsing misleading instructions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 parse_misleading.py $DATSET_ROOT &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The parsed misleading clips will be saved in &lt;code&gt;$DATSET_ROOT/misleading_data.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;LMDrive&#39;s training consists of two stages: 1) the vision encoder pre-training stage, to generate visual tokens from sensor inputs; and 2) the instruction-finetuning stage, to align the instruction/vision and control signal.&lt;/p&gt; &#xA;&lt;p&gt;LMDrive is trained on 8 A100 GPUs with 80GB memory (the first stage can be trained on GPUS with 32G memory). To train on fewer GPUs, you can reduce the &lt;code&gt;batch-size&lt;/code&gt; and the &lt;code&gt;learning-rate&lt;/code&gt; while maintaining their proportion. Please download the multi-modal dataset with instructions collected in the CARLA simulator we use in the paper &lt;a href=&#34;https://huggingface.co/datasets/deepcs233/LMDrive&#34;&gt;here&lt;/a&gt;, if you do not collect the dataset by yourself.&lt;/p&gt; &#xA;&lt;h3&gt;Vision encoder pre-training&lt;/h3&gt; &#xA;&lt;p&gt;Pretrain takes around 2~3 days for the visual encoder on 8x A100 (80G). Once the training is completed, you can locate the checkpoint of the vision encoder in the &lt;code&gt;output/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd vision_encoder&#xA;bash scripts/train.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some options to note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;GPU_NUM&lt;/code&gt;: the number of GPUs you want to use. By default, it is set to 8.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DATASET_ROOT&lt;/code&gt;: the root directory for storing the dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model&lt;/code&gt;: the structure of visual model. You can choose memfuser_baseline_e1d3_r26 which replaces ResNet50 with ResNet26. It&#39;s also possible to create new model variants in &lt;code&gt;visual_encoder/timm/models/memfuser.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--train-towns/train-weathers&lt;/code&gt;: the data filter for the training dataset. Similarly, there are corresponding options, &lt;code&gt;val-towns/val-weathers&lt;/code&gt; to filter the validation dataset accordingly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Instruction finetuning&lt;/h3&gt; &#xA;&lt;p&gt;Instruction finetuning takes around 2~3 days for the visual encoder on 8x A100 (80G). Once the training is completed, you can locate the checkpoint of the adapter and qformer in the &lt;code&gt;lavis/output/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd LAVIS&#xA;bash run.sh 8 lavis/projects/lmdrive/notice_llava15_visual_encoder_r50_seq40.yaml # 8 is the GPU number&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some options in the config.yaml to note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;preception_model&lt;/code&gt;: the model architecture of the vision encoder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;preception_model_ckpt&lt;/code&gt;: the checkpoint path of the vision encoder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;llm_model&lt;/code&gt;: the checkpoint path of the llm (Vicuna/LLaVA).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;use_notice_prompt&lt;/code&gt;: whether to use notice instruction data when training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;split_section_num_for_visual_encoder&lt;/code&gt;: the number of sections the frames are divided into during the forward encoding of visual features. Higher values can save more memory, and it needs to be a factor of &lt;code&gt;token_max_length&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;datasets:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;storage&lt;/code&gt;: the root directory for storing the dataset.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;towns/weathers&lt;/code&gt;: the data filter for training/evaluating.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;token_max_length&lt;/code&gt;: the maximum number of frames, if the number of frames exceeds this value, they will be truncated.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;sample_interval&lt;/code&gt;: the interval at which frames are sampled.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Start a CARLA server (described above) and run the required agent. The adequate routes and scenarios files are provided in &lt;code&gt;leaderboard/data&lt;/code&gt; and the required variables need to be set in &lt;code&gt;leaderboard/scripts/run_evaluation.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Some options need to be updated in the &lt;code&gt;leaderboard/team_code/lmdrive_config.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;preception_model&lt;/code&gt;: the model architecture of the vision encoder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;preception_model_ckpt&lt;/code&gt;: the checkpoint path of the vision encoder (obtained in the vision encoder pretraining stage).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;llm_model&lt;/code&gt;: the checkpoint path of the llm (LLaMA/Vicuna/LLaVA).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;lmdrive_ckpt&lt;/code&gt;: the checkpoint path of the lmdrive (obtained in the instruction finetuing stage).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Update &lt;code&gt;leaderboard/scripts/run_evaluation.sh&lt;/code&gt; to include the following code for evaluating the model on Town05 Long Benchmark.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export CARLA_ROOT=/path/to/carla/root&#xA;export TEAM_AGENT=leaderboard/team_code/lmdrive_agent.py&#xA;export TEAM_CONFIG=leaderboard/team_code/lmdrive_config.py&#xA;export CHECKPOINT_ENDPOINT=results/lmdrive_result.json&#xA;export SCENARIOS=leaderboard/data/official/all_towns_traffic_scenarios_public.json&#xA;export ROUTES=leaderboard/data/LangAuto/long.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0 ./leaderboard/scripts/run_evaluation.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, the &lt;code&gt;long.json&lt;/code&gt; and &lt;code&gt;long.xml&lt;/code&gt; files are replaced with &lt;code&gt;short.json&lt;/code&gt; and &lt;code&gt;short.xml&lt;/code&gt; for the evaluation of the agent in the LangAuto-Short benchmark.&lt;/p&gt; &#xA;&lt;p&gt;For LangAuto-Tiny benchmark evaluation, replace the &lt;code&gt;long.json&lt;/code&gt; and &lt;code&gt;long.xml&lt;/code&gt; files with &lt;code&gt;tiny.json&lt;/code&gt; and &lt;code&gt;tiny.xml&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export SCENARIOS=leaderboard/data/LangAuto/tiny.json&#xA;export ROUTES=leaderboard/data/LangAuto/tiny.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LangAuto-Notice&lt;/h3&gt; &#xA;&lt;p&gt;Set the &lt;code&gt;agent_use_notice&lt;/code&gt; as True in the lmdriver_config.py.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our repo, dataset or paper useful, please cite us as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{shao2023lmdrive,&#xA;      title={LMDrive: Closed-Loop End-to-End Driving with Large Language Models}, &#xA;      author={Hao Shao and Yuxuan Hu and Letian Wang and Steven L. Waslander and Yu Liu and Hongsheng Li},&#xA;      year={2023},&#xA;      eprint={2312.07488},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This implementation is based on code from several repositories.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/opendilab/InterFuser&#34;&gt;InterFuser&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/autonomousvision/transfuser&#34;&gt;Transfuser&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bradyz/2020_CARLA_challenge&#34;&gt;2020_CARLA_challenge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/carla-simulator/leaderboard&#34;&gt;CARLA Leaderboard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/carla-simulator/scenario_runner&#34;&gt;Scenario Runner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/salesforce/LAVIS&#34;&gt;LAVIS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/pytorch-image-models&#34;&gt;pytorch-image-models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;All code within this repository is under &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rasbt/LLMs-from-scratch</title>
    <updated>2023-12-20T01:25:20Z</updated>
    <id>tag:github.com,2023-12-20:/rasbt/LLMs-from-scratch</id>
    <link href="https://github.com/rasbt/LLMs-from-scratch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementing a ChatGPT-like LLM from scratch, step by step&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Build a Large Language Model (From Scratch)&lt;/h1&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://mng.bz/orYv&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/images/cover.jpg&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In &lt;a href=&#34;http://mng.bz/orYv&#34;&gt;&lt;em&gt;Build a Large Language Model (from Scratch)&lt;/em&gt;&lt;/a&gt;, you&#39;ll discover how LLMs work from the inside out. In this book, I&#39;ll guide you step by step through creating your own LLM, explaining each stage with clear text, diagrams, and examples.&lt;/p&gt; &#xA;&lt;p&gt;The method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Link to the official &lt;a href=&#34;https://github.com/rasbt/LLMs-from-scratch&#34;&gt;source code repository&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mng.bz/orYv&#34;&gt;Link to the early access version&lt;/a&gt; at Manning&lt;/li&gt; &#xA; &lt;li&gt;ISBN 9781633437166&lt;/li&gt; &#xA; &lt;li&gt;Publication in Early 2025 (estimated)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Chapter&lt;/th&gt; &#xA;   &lt;th&gt;Main code&lt;/th&gt; &#xA;   &lt;th&gt;Code + supplementary&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ch 1: Understanding Large Language Models&lt;/td&gt; &#xA;   &lt;td&gt;No code&lt;/td&gt; &#xA;   &lt;td&gt;No code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ch 2: Working with Text Data&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/ch02.ipynb&#34;&gt;ch02.ipynb&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/dataloader.ipynb&#34;&gt;dataloader.ipynb&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02&#34;&gt;./ch02&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ch 3: Understanding Attention Mechanisms&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/01_main-chapter-code/ch03.ipynb&#34;&gt;ch03.ipynb&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/01_main-chapter-code/multihead-attention.ipynb&#34;&gt;multihead-attention.ipynb&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03&#34;&gt;./ch03&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Appendix A: Introduction to PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/03_main-chapter-code/01_main-chapter-code/code-part1.ipynb&#34;&gt;code-part1.ipynb&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/03_main-chapter-code/01_main-chapter-code/code-part2.ipynb&#34;&gt;code-part2.ipynb&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/03_main-chapter-code/01_main-chapter-code/DDP-script.py&#34;&gt;DDP-script.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A&#34;&gt;./appendix-A&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/images/mental-model.jpg&#34; width=&#34;600px&#34;&gt; &#xA;&lt;p&gt;(A mental model summarizing the contents covered in this book.)&lt;/p&gt;</summary>
  </entry>
</feed>