<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-13T01:37:29Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>caoyunkang/GroundedSAM-zero-shot-anomaly-detection</title>
    <updated>2023-04-13T01:37:29Z</updated>
    <id>tag:github.com,2023-04-13:/caoyunkang/GroundedSAM-zero-shot-anomaly-detection</id>
    <link href="https://github.com/caoyunkang/GroundedSAM-zero-shot-anomaly-detection" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This project addresses zero-shot anomaly detection by combining SAM and Grouding DINO.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/caoyunkang/GroundedSAM-zero-shot-anomaly-detection/master/assets/SegmentAnyAnomaly_logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;GroundedSAM-zero-shot-anomaly-detection&lt;/h1&gt; &#xA;&lt;p&gt;This project aims to segment any anomaly without any training. We develop this interesting demo by combining &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt;! Most of the codes are borrowed from &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded Segment Anything&lt;/a&gt;. Thanks to their excellent work!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why this project?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt; is a strong segmentation model. But it need prompts (like boxes/points) to generate masks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; is a strong zero-shot detector which enable to generate high quality boxes and labels with free-form text.&lt;/li&gt; &#xA; &lt;li&gt;The combination of the two models enable to &lt;strong&gt;detect and segment everything&lt;/strong&gt; with text inputs!&lt;/li&gt; &#xA; &lt;li&gt;In real world industrial inspection applications, models trained with zero or few normal images, is essential in many cases as defects are rare with a wide range of variations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;How we do?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We feed the origin image and anomaly specific description to Grouding DINO, and then filter the bouding boxes using several strategies. Then the filtered bounding boxes are denoted as the prompts in SAM for final anomaly segmentation. &lt;img src=&#34;https://raw.githubusercontent.com/caoyunkang/GroundedSAM-zero-shot-anomaly-detection/master/assets/framework.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Imagine Space&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Some possible avenues for future work ...&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stronger foundation models with segmentation pre-training.&lt;/li&gt; &#xA; &lt;li&gt;Collaboration with (Chat-)GPT.&lt;/li&gt; &#xA; &lt;li&gt;More advanced normality- and abnormality-specific prompts for better zero-shot anomaly detection performance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Examples on the MVTec AD dataset&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/caoyunkang/GroundedSAM-zero-shot-anomaly-detection/master/assets/demo_results.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üî• What&#39;s New&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üÜï Show the way of using anomaly specific prompts to detect anomalies more precise.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üõ† Installation&lt;/h2&gt; &#xA;&lt;p&gt;The code requires &lt;code&gt;python&amp;gt;=3.8&lt;/code&gt;, as well as &lt;code&gt;pytorch&amp;gt;=1.7&lt;/code&gt; and &lt;code&gt;torchvision&amp;gt;=0.8&lt;/code&gt;. Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA(&amp;gt;=11.1) support is strongly recommended.&lt;/p&gt; &#xA;&lt;p&gt;Install Segment Anything:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install -e segment_anything&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install GroundingDINO:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install -e GroundingDINO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following optional dependencies are necessary for mask post-processing, saving masks in COCO format, the example notebooks, and exporting the model in ONNX format. &lt;code&gt;jupyter&lt;/code&gt; is also required to run the example notebooks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install opencv-python pycocotools matplotlib onnxruntime onnx ipykernel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More details can be found in &lt;a href=&#34;https://github.com/facebookresearch/segment-anything#installation&#34;&gt;install segment anything&lt;/a&gt; and &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO#install&#34;&gt;install GroundingDINO&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or you can simply use our script one-click setup environment and download the Model!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üèÉ Run Grounded-Segment-Anything Demo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the checkpoint for segment-anything and grounding-dino:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd $ProjectRoot&#xA;mkdir weights&#xA;cd ./weights&#xA;wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth&#xA;wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run demo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=0&#xA;python zero_shot_ad_demo.py \&#xA;  --config GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;  --grounded_checkpoint weights/groundingdino_swint_ogc.pth \&#xA;  --input_image assets/wood_demo.png \&#xA;  --output_dir &#34;outputs&#34; \&#xA;  --box_threshold 0.15 \&#xA;  --text_threshold 0.15 \&#xA;  --area_threshold 0.70 \&#xA;  --text_prompt &#34;defects&#34; \&#xA;  --device &#34;cuda&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The model prediction visualization will be saved in &lt;code&gt;output_dir&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üèÉ Run Grounded-Segment-Anything + Gradio APP&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gradio_zero_shot_ad_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The gradio app visulization as follow: &lt;img src=&#34;https://raw.githubusercontent.com/caoyunkang/GroundedSAM-zero-shot-anomaly-detection/master/assets/gradio.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üî®&lt;/span&gt;Todolist&lt;/h2&gt; &#xA;&lt;p&gt;We will add following features in the near future...&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Detail the zero-shot anomaly detection framework.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Evaluate on other image anomaly detection datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add video demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add UI for easy evaluation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Colab demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; HuggingFace demo.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üíò Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;segment-anything&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;GroundingDINO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded Segment Anything&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üéÜRelated Work&lt;/h2&gt; &#xA;&lt;p&gt;If you feel good about our work, there are some work you might be interested inÔºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/caoyunkang/IKD&#34;&gt;IKD, image anomaly detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/caoyunkang/CDO&#34;&gt;CDO, image anomaly detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/smiler96/PFM-and-PEFM-for-Image-Anomaly-Detection-and-Segmentation&#34;&gt;PFM, image anomaly detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/smiler96/GCPF&#34;&gt;GCPF, image anomaly detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/caoyunkang/CPMF&#34;&gt;CPMF, point cloud anomaly detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/M-3LAB/awesome-industrial-anomaly-detection&#34;&gt;awesome-industrial-anomaly-detection, survey of anomaly detection&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project helpful for your research, please consider citing the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTex&#34;&gt;@article{kirillov2023segany,&#xA;  title={Segment Anything}, &#xA;  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\&#39;a}r, Piotr and Girshick, Ross},&#xA;  journal={arXiv:2304.02643},&#xA;  year={2023}&#xA;}&#xA;&#xA;@inproceedings{ShilongLiu2023GroundingDM,&#xA;  title={Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection},&#xA;  author={Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>