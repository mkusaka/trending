<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-19T01:38:48Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Nyandwi/machine_learning_complete</title>
    <updated>2023-05-19T01:38:48Z</updated>
    <id>tag:github.com,2023-05-19:/Nyandwi/machine_learning_complete</id>
    <link href="https://github.com/Nyandwi/machine_learning_complete" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A comprehensive machine learning repository containing 30+ notebooks on different concepts, algorithms and techniques.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Complete Machine Learning Package&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;üèÖ Ranked as one of the top data science repositories on GitHub!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p style=&#34;text-align: justify;&#34;&gt; &lt;a href=&#34;https://nbviewer.jupyter.org/github/Nyandwi/machine_learning_complete&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg?sanitize=true&#34; alt=&#34;Render nbviewer&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://deepnote.com/project/machinelearningcomplete-4vpZ-iGjRUeXCQpL5zcIsw/%2Fmachine_learning_complete%2F5_intro_to_machine_learning%2F5_intro_to_machine_learning.ipynb&#34;&gt;&lt;img src=&#34;https://deepnote.com/buttons/launch-in-deepnote-small.svg?sanitize=true&#34; alt=&#34;Launch in Deepnote&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Techniques, tools, best practices, and everything you need to to learn machine learning!&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Nyandwi/machine_learning_complete/main/images/git_cover.png&#34; alt=&#34;tools&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/&#34;&gt;Complete Machine Learning Package&lt;/a&gt; is a comprehensive repository containing 35 notebooks on Python programming, data manipulation, data analysis, data visualization, data cleaning, classical machine learning, Computer Vision and Natural Language Processing(NLP).&lt;/p&gt; &#xA;&lt;p&gt;All notebooks were created with the readers in mind. Every notebook starts with a high-level overview of any specific algorithm/concept being covered. Wherever possible, visuals are used to make things clear.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;What&#39;s New üî•&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;June 23th, 2022: Many people have asked how they can support the package. You can buy us a &lt;a href=&#34;https://www.buymeacoffee.com/jeande&#34;&gt;coffee ‚òïÔ∏è&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;May 18th, 2022: Complete Machine Learning Package is now available on &lt;a href=&#34;https://nyandwi.com/machine_learning_complete/&#34;&gt;web&lt;/a&gt;. It&#39;s now easy to view all notebooks!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;April 9th, 2022: Updated &lt;a href=&#34;https://github.com/Nyandwi/machine_learning_complete/raw/main/8_deep_computer_vision_with_tensorflow/3_transfer_learning_convnets.ipynb&#34;&gt;Transfer Learning with Pretrained Convolutional Neural Networks&lt;/a&gt; with additional things and added further resources.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;November 25th, 2021: Updated &lt;a href=&#34;https://github.com/Nyandwi/machine_learning_complete/raw/main/5_intro_to_machine_learning/ml_fundamentals.md&#34;&gt;Fundamentals of Machine Learning&lt;/a&gt;: Added introductory notes, ML system design workflow, and challenges of learning systems.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Tools Overview&lt;/h2&gt; &#xA;&lt;p&gt;The following are the tools that are covered in Complete Machine Learning Package. They are popular tools that most machine learning engineers and data scientists need in one way or another and day to day.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.python.org&#34;&gt;Python&lt;/a&gt; is a high level programming language that has got a lot of popularity in the data community and with the rapid growth of the libraries and frameworks, this is a right programming language to do ML.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://numpy.org&#34;&gt;NumPy&lt;/a&gt; is a scientific computing tool used for array or matrix operations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://pandas.pydata.org&#34;&gt;Pandas&lt;/a&gt; is a great and simple tool for analyzing and manipulating data from a variety of different sources.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://matplotlib.org&#34;&gt;Matplotlib&lt;/a&gt; is a comprehensive data visualization tool used to create static, animated, and interactive visualizations in Python.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://seaborn.pydata.org&#34;&gt;Seaborn&lt;/a&gt; is another data visualization tool built on top of Matplotlib which is pretty simple to use.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://scikit-learn.org/stable/&#34;&gt;Scikit-Learn&lt;/a&gt;: Instead of building machine learning models from scratch, Scikit-Learn makes it easy to use classical models in a few lines of code. This tool is adapted by almost the whole of the ML community and industries, from the startups to the big techs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org&#34;&gt;TensorFlow&lt;/a&gt; and &lt;a href=&#34;https://keras.io&#34;&gt;Keras&lt;/a&gt; for deep learning: TensorFlow is a popular deep learning framework used for building models suitable for different fields such as Computer Vision and Natural Language Processing. Keras is a high level neural network API that makes it easy to design deep learning models. TensorFlow and Keras have a great &lt;a href=&#34;https://discuss.tensorflow.org&#34;&gt;community&lt;/a&gt; and ecosystem that include tools like &lt;a href=&#34;https://www.tensorflow.org/tensorboard&#34;&gt;TensorBoard&lt;/a&gt;, &lt;a href=&#34;https://www.tensorflow.org/datasets&#34;&gt;TF Datasets&lt;/a&gt;, &lt;a href=&#34;https://www.tensorflow.org/lite&#34;&gt;TensorFlow Lite&lt;/a&gt;, &lt;a href=&#34;https://www.tensorflow.org/tfx/&#34;&gt;TensorFlow Extended&lt;/a&gt;, &lt;a href=&#34;https://www.tensorflow.org/hub&#34;&gt;TensorFlow Hub&lt;/a&gt;, &lt;a href=&#34;https://www.tensorflow.org/js&#34;&gt;TensorFlow.js&lt;/a&gt;, &lt;a href=&#34;https://github.com/tensorflow/gnn&#34;&gt;TensorFlow GNN&lt;/a&gt;, and &lt;a href=&#34;https://www.tensorflow.org/resources/models-datasets&#34;&gt;much&lt;/a&gt; &lt;a href=&#34;https://www.tensorflow.org/resources/tools&#34;&gt;more&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Complete Outline&lt;/h2&gt; &#xA;&lt;h2&gt;Part 1 - Intro to Python and Working with Data&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/Nyandwi/machine_learning_complete/raw/main/0_python_for_ml/intro_to_python.ipynb&#34;&gt;0 - Intro to Python for Machine Learning&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/01_intro_to_Numpy_for_data_computation/&#34;&gt;1 - Data Computation With NumPy&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creating a NumPy Array&lt;/li&gt; &#xA; &lt;li&gt;Selecting Data: Indexing and Slicing An Array&lt;/li&gt; &#xA; &lt;li&gt;Performing Mathematical and other Basic Operations&lt;/li&gt; &#xA; &lt;li&gt;Performing Basic Statistics&lt;/li&gt; &#xA; &lt;li&gt;Manipulating Data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[&lt;em&gt;You can find detailed notes about NumPy &lt;a href=&#34;https://github.com/Nyandwi/machine_learning_complete/raw/main/1_data_computations_with_numpy/detailed_notes_on_numpy.pdf&#34;&gt;here&lt;/a&gt;&lt;/em&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/02_data_manipulation_with_pandas/&#34;&gt;2 - Data Manipulation with Pandas&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basics of Pandas &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Series and DataFrames&lt;/li&gt; &#xA;   &lt;li&gt;Data Indexing and Selection&lt;/li&gt; &#xA;   &lt;li&gt;Dealing with Missing data&lt;/li&gt; &#xA;   &lt;li&gt;Basic operations and Functions&lt;/li&gt; &#xA;   &lt;li&gt;Aggregation Methods&lt;/li&gt; &#xA;   &lt;li&gt;Groupby&lt;/li&gt; &#xA;   &lt;li&gt;Merging, Joining and Concatenate&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Beyond Dataframes: Working with CSV, and Excel&lt;/li&gt; &#xA; &lt;li&gt;Real World Exploratory Data Analysis (EDA)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3 - Data Visualization&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/03_data_visualizations_with_matplotlib/&#34;&gt;Data Visualization with Matplotlib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/04_data_visualization_with_seaborn/&#34;&gt;Data Visualization with Seaborn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/05_data_visualization%20with_pandas/&#34;&gt;Data Visualization with Pandas&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;4 - Real World Data Exploratory Analysis and Data Preparation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/06_exploratory_data_analysis/&#34;&gt;Exploratory Data Analysis&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/07_intro_to_data_preparation/&#34;&gt;Intro to Data Preparation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/08_encoding_categorical_features/&#34;&gt;Handling Categorical Features&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/09_feature_scaling/&#34;&gt;Feature Scaling&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/10_handling_missing_values/&#34;&gt;Handling Missing Values&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Part 2 - Machine Learning&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/11_ml_fundamentals/&#34;&gt;5 - Machine Learning Fundamentals&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What is Machine Learning?&lt;/li&gt; &#xA; &lt;li&gt;The Difference Between Artificial Intelligence, Data Science, Machine Learning, and Deep Learning&lt;/li&gt; &#xA; &lt;li&gt;The Difference Between Machine Learning and Ordinary Programming&lt;/li&gt; &#xA; &lt;li&gt;Applications of Machine Learning&lt;/li&gt; &#xA; &lt;li&gt;When to Use and When not to Use Machine Learning&lt;/li&gt; &#xA; &lt;li&gt;Types of Machine Learning&lt;/li&gt; &#xA; &lt;li&gt;A Typical Machine Learning Project Workflow&lt;/li&gt; &#xA; &lt;li&gt;Evaluation Metrics&lt;/li&gt; &#xA; &lt;li&gt;The Challenges of Machine Learning Systems&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;6 - Classical Machine Learning with Scikit-Learn&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/12_intro_to_sklearn/&#34;&gt;Intro to Scikit-Learn for Machine Learning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/13_linear_models_for_regression/&#34;&gt;Linear Models for Regression&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/14_linear_models_for_classification/&#34;&gt;Linear Models for Classification&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/15_support_vector_machines_for_regression/&#34;&gt;Support Vector Machines: Intro and Regression&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/16_support_vector_machines_for_classification/&#34;&gt;Support Vector Machines for Classification&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/17_decision_trees_for_regression/&#34;&gt;Decision Trees: Intro and Regression&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/18_decision_trees_for_classification/&#34;&gt;Decision Trees for Classification&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/19_random_forests_for_regression/&#34;&gt;Random Forests: Intro and Regression&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/20_random_forests_for_classification/&#34;&gt;Random Forests for Classification&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/21_ensemble_models/&#34;&gt;Beyond Random Forests: More Ensemble Models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/22_intro_to_unsupervised_learning_with_kmeans_clustering/&#34;&gt;Intro to Unsupervised Learning with KMeans Clustering&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/23_a_practical_intro_to_principal_components_analysis/&#34;&gt;A Practical Intro to Principal Component Analysis&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Part 3 - Deep Learning&lt;/h2&gt; &#xA;&lt;h3&gt;7 - Intro to Artificial Neural Networks and TensorFlow&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/24_intro_to_neural_networks/&#34;&gt;Intro to Articial Neural Networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Why Deep Learning&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;A Single Layer Neural Network&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Activation Functions&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Types of Deep Learning Architectures&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Densely Connected Networks&lt;/li&gt; &#xA;     &lt;li&gt;Convolutional Neural Networks&lt;/li&gt; &#xA;     &lt;li&gt;Recurrent Neural Networks&lt;/li&gt; &#xA;     &lt;li&gt;Transformers&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Challenges in Training Deep Neural Networks&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/25_intro_to_tensorflow_for_deeplearning/&#34;&gt;Intro to TensorFlow for Deep Learning&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;What is TensorFlow?&lt;/li&gt; &#xA;   &lt;li&gt;TensorFlow Model APIs&lt;/li&gt; &#xA;   &lt;li&gt;A Quick Tour into TensorFlow Ecosystem&lt;/li&gt; &#xA;   &lt;li&gt;Basics of Tensors&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/26_neural_networks_for_regresion_with_tensorflow/&#34;&gt;Neural Networks for Regression with TensorFlow&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/27_neural_networks_for_classification_with_tensorflow/&#34;&gt;Neural Networks for Classification with TensorFlow&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;8 - Deep Computer Vision with TensorFlow&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/28_intro_to_computer_vision_and_cnn/&#34;&gt;Intro to Computer Vision with Convolutional Neural Networks(CNNs)&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Intro to Computer Vision and CNNs &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;What is Convolutional Neural Networks?&lt;/li&gt; &#xA;     &lt;li&gt;A Typical Architecture of Convolutional Neural Networks&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Coding ConvNets: Image Classification&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/29_cnn_for_real_world_data_and_image_augmentation/#contents&#34;&gt;ConvNets for Real World Data and Image Augmentation&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Intro - Real World Datasets and Data Augmentation&lt;/li&gt; &#xA;   &lt;li&gt;Getting Started: Real World Datasets and Overfitting&lt;/li&gt; &#xA;   &lt;li&gt;Image Augmentation with Keras Image Augmentation Layers&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/30_cnn_architectures_and_transfer_learning/&#34;&gt;Transfer Learning with Pretrained Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Motivation for Transfer Learning&lt;/li&gt; &#xA;   &lt;li&gt;Introduction to Transfer Learning&lt;/li&gt; &#xA;   &lt;li&gt;A Typical Flow of Transfer Learning&lt;/li&gt; &#xA;   &lt;li&gt;Quick Image Classification with Pretrained Models&lt;/li&gt; &#xA;   &lt;li&gt;Transfer Learning and FineTuning in Practice&lt;/li&gt; &#xA;   &lt;li&gt;Quick Image Classification and Transfer Learning with TensorFlow Hub&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;[Updated notebook of Transfer Learning is found &lt;a href=&#34;https://raw.githubusercontent.com/Nyandwi/machine_learning_complete/main/(https://github.com/Nyandwi/machine_learning_complete/raw/main/8_deep_computer_vision_with_tensorflow/3_transfer_learning_convnets.ipynb)&#34;&gt;here&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;9 - Natural Language Processing with TensorFlow&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/31_intro_to_nlp_and_text_preprocessing/&#34;&gt;Intro to NLP and Text Processing with TensorFlow&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Intro to Natural Language Processing&lt;/li&gt; &#xA;   &lt;li&gt;Text Processing with TensorFlow&lt;/li&gt; &#xA;   &lt;li&gt;Using TextVectorization Layer&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/32_using_word_embeddings_to_represent_texts/&#34;&gt;Using Word Embeddings to Represent Texts&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Intro to Word Embeddings&lt;/li&gt; &#xA;   &lt;li&gt;Embedding In Practice&lt;/li&gt; &#xA;   &lt;li&gt;Using Pretrained Embeddings&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/33_recurrent_neural_networks/&#34;&gt;Recurrent Neural Networks (RNNs)&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Intro to Recurrent Neural Networks&lt;/li&gt; &#xA;   &lt;li&gt;Simple RNNs In Practice: Movies Sentiment Analysis&lt;/li&gt; &#xA;   &lt;li&gt;Intro to Long Short Terms Memories&lt;/li&gt; &#xA;   &lt;li&gt;LSTMs in Practice : News Classification&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/34_using_cnns_and_rnns_for_texts_classification/&#34;&gt;Using Convolutional Neural Networks for Texts Classification&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Intro Convolutional Neural Networks for Texts&lt;/li&gt; &#xA;   &lt;li&gt;CNN for Texts in Practice: News Classification&lt;/li&gt; &#xA;   &lt;li&gt;Combining ConvNets and RNNs&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nyandwi.com/machine_learning_complete/35_using_pretrained_bert_for_text_classification/&#34;&gt;Using Pretrained BERT for Text Classification&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Intro to BERT&lt;/li&gt; &#xA;   &lt;li&gt;In Practice: Finetuning a Pretrained BERT&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Used Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Many of the datasets used for this repository are from the following sources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.openml.org&#34;&gt;UC OpenML&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mwaskom/seaborn-data&#34;&gt;Seaborn Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/datasets.html&#34;&gt;Scikit-Learn datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/datasets&#34;&gt;Kaggle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/datasets/catalog/overview&#34;&gt;TensorFlow datasets&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Further Machine Learning Resources&lt;/h2&gt; &#xA;&lt;p&gt;Machine Learning community is very vibrant. Complete Machine Learning Package can get you started, but it&#39;s not enough. Fortunately, there are many faboulous learning resources, some of which are paid or freely available. Here is a list of courses that are often recommended by many people. Note that they are not listed in an order they are to be taken.&lt;/p&gt; &#xA;&lt;h3&gt;Courses&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Machine Learning by Coursera&lt;/strong&gt;: This course was taught by Andrew Ng. It is one of the most popular machine learning courses, it has been taken by over 4M of people. The course focuses more about the fundamentals of machine learning techniques and algorithms. It is free on &lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34;&gt;Coursera&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deep Learning Specialization&lt;/strong&gt;: Also tought by Andrew Ng., Deep Learning Specialization is also a foundations based course. It teaches a decent foundations of major deep learning architectures such as convolutional neural networks and recurrent neural networks. The full course can be &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;audited on Coursera&lt;/a&gt;, or watch freely on &lt;a href=&#34;https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&#34;&gt;Youtube&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MIT Intro to Deep Learning&lt;/strong&gt;: This course provide the foundations of deep learning in resonably short period of time. Each lecture is one hour or less, but the materials are still the best in classs. Check the course page &lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;here&lt;/a&gt;, and lecture videos &lt;a href=&#34;https://www.youtube.com/watch?v=AjtX1N_VT9E&amp;amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;amp;index=4&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MIT Introduction to Data-Centric AI&lt;/strong&gt;: This is the first-ever course on DCAI. This class covers algorithms to find and fix common issues in ML data and to construct better datasets, concentrating on data used in supervised learning tasks like classification. All material taught in this course is highly practical, focused on impactful aspects of real-world ML applications, rather than mathematical details of how particular models work. You can take this course to learn practical techniques not covered in most ML classes, which will help mitigate the ‚Äúgarbage in, garbage out‚Äù problem that plagues many real-world ML applications. Check out the course page &lt;a href=&#34;https://dcai.csail.mit.edu/&#34;&gt;here&lt;/a&gt;, lecture videos &lt;a href=&#34;https://www.youtube.com/watch?v=ayzOzZGHZy4&amp;amp;list=PLnSYPjg2dHQKdig0vVbN-ZnEU0yNJ1mo5&#34;&gt;here&lt;/a&gt;, and lab assignments &lt;a href=&#34;https://github.com/dcai-course/dcai-lab&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;NYU Deep Learning Spring 2021&lt;/strong&gt;: Taught at NYU by Yann LeCun, Alfredo Canziani, this course is one of the most creative courses out there. The materials are presented in amazing way. Check the lecture videos &lt;a href=&#34;https://www.youtube.com/playlist?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&#34;&gt;here&lt;/a&gt;, and the course repo &lt;a href=&#34;https://github.com/Atcold/NYU-DLSP21&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CS231N: Convolutional Neural Networks for Visual Recognition by Stanford&lt;/strong&gt;: CS231N is one of the best deep learning and computer vision courses. The 2017 version was taught by Fei-Fei Li, Justin Johnson and Serena Yeung. The &lt;a href=&#34;http://cs231n.stanford.edu/2016/&#34;&gt;2016 version&lt;/a&gt; was taught by Fei-Fei, Johnson and Andrej Karpathy. See 2017 lecture videos &lt;a href=&#34;https://www.youtube.com/watch?v=vT1JzLTH4G4&amp;amp;list=PLzUTmXVwsnXod6WNdg57Yc3zFx_f-RYsq&amp;amp;index=1&amp;amp;t=457s&#34;&gt;here&lt;/a&gt;, and other materials &lt;a href=&#34;http://cs231n.stanford.edu&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CS224N: Natural Language Processing with Deep Learning by Stanford&lt;/strong&gt;: If you are interested in Natural Language Processing, this is a great course to take. It is taught by Christopher Manning, one of the world class NLP stars. See the lecture videos &lt;a href=&#34;https://www.youtube.com/playlist?list=PLU40WL8Ol94IJzQtileLTqGZuXtGlLMP_&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Practical Deep Learning for Coders by fast.ai&lt;/strong&gt;: This is also an intensive deep learning course that cover the whole spectrum of deep learning architectures and techniques. The &lt;a href=&#34;https://course.fast.ai/videos/?lesson=1&#34;&gt;lecture videos&lt;/a&gt; and other resources such as &lt;a href=&#34;https://github.com/fastai/fastbook&#34;&gt;notebooks&lt;/a&gt; are the &lt;a href=&#34;https://course.fast.ai&#34;&gt;course page&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Machine Learning Engineering for Production (MLOps) Specialization&lt;/strong&gt;: Taught by Andrew Ng., Laurence Moroney, and Robert Crowe, this is one of the best ML engineering course. It teaches how to design end to end machine learning production systems, building efficient data and modelling pipelines, and deploying models in production. You can find the course on &lt;a href=&#34;https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops#about&#34;&gt;Coursera&lt;/a&gt; and other course materials on &lt;a href=&#34;https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Full Stack Deep Learning&lt;/strong&gt;: While the majority of machine learning courses focuses on modelling, this course focuses on shipping machine learning systems. It teaches how to design machine learning projects, data management(storage, access, processing, versioning, and labeling), training, debugging, and deploying machine learning models. See 2021 version &lt;a href=&#34;https://fullstackdeeplearning.com/spring2021/&#34;&gt;here&lt;/a&gt; and 2019 &lt;a href=&#34;https://fall2019.fullstackdeeplearning.com&#34;&gt;here&lt;/a&gt;. You can also skim through the &lt;a href=&#34;https://fullstackdeeplearning.com/spring2021/projects/&#34;&gt;project showcases&lt;/a&gt; to see the kind of the courses outcomes through learners projects.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Books&lt;/h3&gt; &#xA;&lt;p&gt;Below are some few awesome machine learning books.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;The Hundred-Page Machine Learning Book&lt;/strong&gt;: Authored by Andriy Burkov, this is one of the shortest but concise and well written book that you will ever find on the internet. You can read the book for free &lt;a href=&#34;http://themlbook.com/wiki/doku.php&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Machine Learning Engineering&lt;/strong&gt;: Also authored by Andriy Burkov, this is another great machine learning book that uncover each step of machine learning workflow, from data collection, preparation....to model serving and maintenance. The book is also free &lt;a href=&#34;http://www.mlebook.com/wiki/doku.php&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Machine Learning Yearning&lt;/strong&gt;: Authored by Andrew Ng., the book contains various strategies for building effective learning systems. It is partitioned into small parts which makes it easy to read and it is not sorely for machine learning engineers. Anyone working with data science and machine learning teams can find the book helpful. The official book is available &lt;a href=&#34;https://www.deeplearning.ai/programs/&#34;&gt;here&lt;/a&gt; for free, but you can read or download it &lt;a href=&#34;https://github.com/ajaymache/machine-learning-yearning&#34;&gt;here&lt;/a&gt; without requiring to sign up.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow&lt;/strong&gt;: Authored by Aurelion Geron, this is one of the best machine learning books. It is clearly written and full of ideas and best practices. You can get the book &lt;a href=&#34;https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/&#34;&gt;here&lt;/a&gt;, or see its repository &lt;a href=&#34;https://github.com/ageron/handson-ml2&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;: Authored by 3 deep learning legends, Ian Goodfellow and Yoshua Bengio and Aaron Courville, this is one of the great deep learning books that is freely available. You can get it &lt;a href=&#34;https://www.deeplearningbook.org&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deep Learning with Python&lt;/strong&gt;: Authored by Francois Chollet, The Keras designer, this is a very comprehensive deep learning book. You can get the book &lt;a href=&#34;https://www.manning.com/books/deep-learning-with-python-second-edition&#34;&gt;here&lt;/a&gt;, and the book repo &lt;a href=&#34;https://github.com/fchollet/deep-learning-with-python-notebooks&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dive into Deep Learning&lt;/strong&gt;: This is also a great deep learning book that is freely available. The book uses both PyTorch and TensorFlow. You can read the entire book &lt;a href=&#34;https://d2l.ai/index.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Neural Networks and Deep Learning&lt;/strong&gt;: This is also another great deep learning online book by Michael Nielsen. You can read the entire book &lt;a href=&#34;http://neuralnetworksanddeeplearning.com&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you are interested in more machine learning and deep learning resources, you can find them &lt;a href=&#34;https://ludwigstumpp.com/ml-starter-kit&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://deep-learning-drizzle.github.io&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://github.com/dair-ai/ML-YouTube-Courses&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Citing Complete ML Package&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{Nyandwi2021MLPackage,&#xA;    title = &#34;Complete Machine Learning Package&#34;,&#xA;    author = &#34;Nyandwi, Jean de Dieu&#34;,&#xA;    journal = &#34;GitHub&#34;,&#xA;    year = &#34;2021&#34;,&#xA;    url = &#34;https://nyandwi.com/machine_learning_complete&#34;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This repository was created by &lt;a href=&#34;https://nyandwi.com&#34;&gt;Jean de Dieu Nyandwi&lt;/a&gt;. You can find him on &lt;a href=&#34;https://twitter.com/jeande_d&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://linkedin.com/in/nyandwi&#34;&gt;LinkedIn&lt;/a&gt;, &lt;a href=&#34;https://jeande.medium.com&#34;&gt;Medium&lt;/a&gt;, and &lt;a href=&#34;https://www.instagram.com/nyandwi.de&#34;&gt;Instagram&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you enjoy any of this things, you can buy him a &lt;a href=&#34;https://www.buymeacoffee.com/jeande&#34;&gt;coffee&lt;/a&gt; ‚òïÔ∏è&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NoDataFound/hackGPT</title>
    <updated>2023-05-19T01:38:48Z</updated>
    <id>tag:github.com,2023-05-19:/NoDataFound/hackGPT</id>
    <link href="https://github.com/NoDataFound/hackGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;I leverage OpenAI and ChatGPT to do hackerish things&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/hackGPT-v23-purple&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Test the app live here:&lt;/code&gt; &lt;a href=&#34;https://hackgpt.com&#34;&gt;https://hackgpt.com&lt;/a&gt; &lt;img width=&#34;1681&#34; alt=&#34;Screenshot 2023-04-30 at 6 59 28 PM&#34; src=&#34;https://user-images.githubusercontent.com/3261849/235382192-714758c2-5117-4c95-851d-eeeb9301221f.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/NoDataFound/hackGPT/assets/3261849/f8f85831-706f-4398-9da1-365323f656bd&#34; alt=&#34;Screenshot 2023-05-11 at 12 09 48 PM&#34;&gt;&lt;/p&gt; &#xA;&lt;img width=&#34;1681&#34; alt=&#34;Screenshot 2023-04-29 at 2 42 41 PM&#34; src=&#34;https://user-images.githubusercontent.com/3261849/235321459-35eb1ecb-58b6-4439-9fee-dbc63e13f3e1.png&#34;&gt; &#xA;&lt;img width=&#34;1624&#34; alt=&#34;Screenshot 2023-04-24 at 8 41 52 PM&#34; src=&#34;https://user-images.githubusercontent.com/3261849/234341399-f79b9ca3-e829-4831-8e67-e75e8840adfd.png&#34;&gt; &#xA;&lt;p&gt;&lt;code&gt;Hunt for JIRA issues using type=bug, fix issue and commit fix back to ticket as comment &lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/3261849/228703126-adf614ba-d931-4ec0-8d1a-99654063058b.mp4&#34;&gt;https://user-images.githubusercontent.com/3261849/228703126-adf614ba-d931-4ec0-8d1a-99654063058b.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img width=&#34;1678&#34; alt=&#34;Screenshot 2023-03-29 at 8 05 29 PM&#34; src=&#34;https://user-images.githubusercontent.com/3261849/228703250-74cd7ed4-114f-46f5-b4ef-f6644fb5eea4.png&#34;&gt; &#xA;&lt;p&gt;&lt;code&gt;Launch hackGPT with python&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/3261849/222942128-3f75b9b7-5763-4a0c-a4df-aadbb24dcf5d.mp4&#34;&gt;https://user-images.githubusercontent.com/3261849/222942128-3f75b9b7-5763-4a0c-a4df-aadbb24dcf5d.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Added PrettyTable for logging and It will load the chatbot in a new tab of your active broswer&lt;/code&gt; &lt;img width=&#34;1639&#34; alt=&#34;Screenshot 2023-03-05 at 6 49 23 PM&#34; src=&#34;https://user-images.githubusercontent.com/3261849/222996567-4967be7c-68be-4e58-9538-6b5d9bb8649d.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;hackGPT enabled Siri&lt;/code&gt; &lt;img width=&#34;1464&#34; alt=&#34;Screenshot 2023-03-05 at 7 36 26 AM&#34; src=&#34;https://user-images.githubusercontent.com/3261849/222963874-096f2add-ffa9-40be-a7f2-8c604cf5e3b2.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;hackGPT being superduper wrong&lt;/code&gt; &lt;img width=&#34;1663&#34; alt=&#34;Nope&#34; src=&#34;https://user-images.githubusercontent.com/3261849/223754401-e2caea7f-b4ff-452b-851e-2c3ab07c0192.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Launch hackGPT with Google Colab(colab.research.google.com):&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/3261849/218538303-68c06a8e-071b-40a2-b7b2-b9e974b41f09.mp4&#34;&gt;https://user-images.githubusercontent.com/3261849/218538303-68c06a8e-071b-40a2-b7b2-b9e974b41f09.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img width=&#34;1568&#34; alt=&#34;hackGPT&#34; src=&#34;https://user-images.githubusercontent.com/3261849/208184172-b5d79eb2-3dff-49e7-b735-d991c08e6ec8.png&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;Automate the parsing and analysis of json threat data from CyberDefense tools like my SecurityScorecard ASI API tool here: https://github.com/securityscorecard/ssc-asi-tools/tree/master/tools/SSC_APIHunter:&lt;/code&gt;&lt;/p&gt; &#xA;&lt;img width=&#34;1683&#34; alt=&#34;sscplushgpt&#34; src=&#34;https://user-images.githubusercontent.com/3261849/217700643-ab202279-9558-41da-83db-ce64f7e796a1.png&#34;&gt; &#xA;&lt;p&gt;&lt;code&gt;hackGPT chatbot on mobile - https://colab.research.google.com/github/NoDataFound/hackGPT/blob/main/hacklab.hackGPT.ipynb&lt;/code&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/3261849/218890190-e4edceff-ca65-4db0-93ee-82aa055eb576.png&#34; alt=&#34;hackgpt_mobile&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/3261849/222963550-41fc50c5-6c89-45af-a794-31a47fc5a51e.mov&#34;&gt;https://user-images.githubusercontent.com/3261849/222963550-41fc50c5-6c89-45af-a794-31a47fc5a51e.mov&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Automate CVE exploit creation and CyberDefense protections:&lt;/code&gt; (results &lt;a href=&#34;https://github.com/NoDataFound/PwnAI/tree/main/output&#34;&gt;https://github.com/NoDataFound/PwnAI/tree/main/output&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;img width=&#34;1649&#34; alt=&#34;Screenshot 2022-12-14 at 8 08 05 AM&#34; src=&#34;https://user-images.githubusercontent.com/3261849/207626394-cb272c96-c370-4870-9a13-6d43397fb830.png&#34;&gt; &#xA;&lt;p&gt;&lt;code&gt;Ask ChatGPT to print its own source&lt;/code&gt;&lt;/p&gt; &#xA;&lt;img width=&#34;675&#34; alt=&#34;Screenshot 2022-12-16 at 4 46 35 PM&#34; src=&#34;https://user-images.githubusercontent.com/3261849/208202201-db534197-71c8-4f26-8041-72dd25e8d356.png&#34;&gt; &#xA;&lt;img width=&#34;977&#34; alt=&#34;Screenshot 2022-12-04 at 6 27 59 PM&#34; src=&#34;https://user-images.githubusercontent.com/3261849/205525745-fa26c95b-9d86-4c84-8669-be1cde4abaf2.png&#34;&gt; &#xA;&lt;img width=&#34;1269&#34; alt=&#34;Screenshot 2022-12-04 at 6 32 40 PM&#34; src=&#34;https://user-images.githubusercontent.com/3261849/205525669-9eb6e988-4440-43ea-8432-6f07066db7df.png&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/3261849/206036893-b583fad1-6b77-4dfb-8424-639229ffdd19.mov&#34;&gt;https://user-images.githubusercontent.com/3261849/206036893-b583fad1-6b77-4dfb-8424-639229ffdd19.mov&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img align=&#34;center&#34; width=&#34;1800&#34; alt=&#34;hackGPT&#34; src=&#34;https://user-images.githubusercontent.com/3261849/211083766-e987961a-4f0a-427c-bbd8-6479f4449342.png&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ùóúùóªùòÄùòÅùóÆùóπùóπùóÆùòÅùó∂ùóºùóª&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;Clone this repo&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/NoDataFound/PwnAI.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;Clone this repo via SSH (alt. method)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:NoDataFound/hackGPT.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;Setup virtual environment (optional)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;~$ python3 -m venv env&#xA;~$ source env/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;Install dependancies&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;Review Input and Bulk Input samples&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;head -n 10 input/malware/malware_sample &amp;amp;&amp;amp; head -n 10 input/sample_sources&#xA;&#xA;# Exploit Title: TP-Link Tapo c200 1.1.15 - Remote Code Execution (RCE)&#xA;# Date: 02/11/2022&#xA;# Exploit Author: hacefresko&#xA;# Vendor Homepage: https://www.tp-link.com/en/home-networking/cloud-camera/tapo-c200/&#xA;# Version: 1.1.15 and below&#xA;# Tested on: 1.1.11, 1.1.14 and 1.1.15&#xA;# CVE : CVE-2021-4045&#xA;&#xA;# Write up of the vulnerability: https://www.hacefresko.com/posts/tp-link-tapo-c200-unauthenticated-rce&#xA;&#xA;https://github.com/rapid7/metasploit-payloads/blob/master/python/meterpreter/meterpreter.py&#xA;https://github.com/rapid7/metasploit-payloads/blob/master/powershell/MSF.Powershell/Meterpreter/Core.cs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;Open Jupyter Notebook&lt;/code&gt; &lt;em&gt;Install Jupyter Notebook if needed - use pip or download binaries here: &lt;a href=&#34;https://jupyter.org/&#34;&gt;https://jupyter.org/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;install (pictured) https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter-renderers&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ùñ´ùñ∫ùóéùóáùñºùóÅ ùñ≠ùóàùóçùñæùñªùóàùóàùóÑ ùòÑùó∂ùòÅùóµ ùó©ùó¶ùó∞ùóºùó±ùó≤&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://user-images.githubusercontent.com/3261849/205510169-5269cde5-7565-4094-9a07-2d41e65bc717.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Configure .env with your OpenAI API key(notebook will help you)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Use Python&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;set API key on launch&lt;/code&gt; &lt;img width=&#34;959&#34; alt=&#34;Screenshot 2022-12-03 at 1 23 38 PM&#34; src=&#34;https://user-images.githubusercontent.com/3261849/205458244-ed556dd8-c8d8-498d-9a1d-727d139e46d7.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;single searches&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 PwnAI.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;1147&#34; alt=&#34;Screenshot 2022-12-04 at 6 26 38 PM&#34; src=&#34;https://user-images.githubusercontent.com/3261849/205525796-d8d009b5-9d76-4b04-ae24-319e5f1ea924.png&#34;&gt; &#xA;&lt;p&gt;&lt;code&gt;Bulk searches&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 PwnAI_bulk.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;977&#34; alt=&#34;Screenshot 2022-12-04 at 6 27 59 PM&#34; src=&#34;https://user-images.githubusercontent.com/3261849/205525811-8c5eb58b-257e-4412-a462-89f0c3ccb5be.png&#34;&gt;</summary>
  </entry>
  <entry>
    <title>Vaibhavs10/fast-whisper-finetuning</title>
    <updated>2023-05-19T01:38:48Z</updated>
    <id>tag:github.com,2023-05-19:/Vaibhavs10/fast-whisper-finetuning</id>
    <link href="https://github.com/Vaibhavs10/fast-whisper-finetuning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Faster Whisper Finetuning with LoRA powered by ü§ó PEFT&lt;/h1&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://github.com/Vaibhavs10/fast-whisper-finetuning/raw/main/Whisper_w_PEFT.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; - A one size fits all walkthrough, to fine-tune Whisper (large) &lt;strong&gt;5x faster&lt;/strong&gt; on a consumer GPU with &lt;strong&gt;less than 8GB GPU VRAM&lt;/strong&gt;, all with comparable performance to full-finetuning. ‚ö°Ô∏è&lt;/p&gt; &#xA;&lt;p&gt;Not convinced? Here are some benchmarks we ran on a free Google Colab T4 GPU! üëá&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Training type&lt;/th&gt; &#xA;   &lt;th&gt;Trainable params&lt;/th&gt; &#xA;   &lt;th&gt;Memory allocation&lt;/th&gt; &#xA;   &lt;th&gt;Max. batch size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA&lt;/td&gt; &#xA;   &lt;td&gt;&amp;lt;1%&lt;/td&gt; &#xA;   &lt;td&gt;8GB&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;adaLoRA&lt;/td&gt; &#xA;   &lt;td&gt;&amp;lt;0.9%&lt;/td&gt; &#xA;   &lt;td&gt;7.9GB&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Full-fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;100%&lt;/td&gt; &#xA;   &lt;td&gt;OOM on T4&lt;/td&gt; &#xA;   &lt;td&gt;OOM on T4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Vaibhavs10/fast-whisper-finetuning/main/#why-parameter-efficient-fine-tuning-peft&#34;&gt;Why Parameter Efficient Fine Tuning (PEFT)?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Vaibhavs10/fast-whisper-finetuning/main/#fine-tuning-whisper-in-a-google-colab&#34;&gt;Fine-tuning Whisper in a Google Colab&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Vaibhavs10/fast-whisper-finetuning/main/#prepare-environment&#34;&gt;Prepare Environment&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Vaibhavs10/fast-whisper-finetuning/main/#load-dataset&#34;&gt;Load Dataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Vaibhavs10/fast-whisper-finetuning/main/#prepare-feature-extractor-tokenizer-and-data&#34;&gt;Prepare Feature Extractor, Tokenizer and Data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Vaibhavs10/fast-whisper-finetuning/main/#training-and-evaluation&#34;&gt;Training and Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Vaibhavs10/fast-whisper-finetuning/main/#evaluation-and-inference&#34;&gt;Evaluation and Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Vaibhavs10/fast-whisper-finetuning/main/#fin&#34;&gt;Fin!&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We present a step-by-step guide on how to fine-tune Whisper with Common Voice 13.0 dataset using ü§ó Transformers and PEFT. In this Colab, we leverage &lt;code&gt;PEFT&lt;/code&gt; and &lt;code&gt;bitsandbytes&lt;/code&gt; to train a &lt;code&gt;whisper-large-v2&lt;/code&gt; checkpoint seamlessly with a free T4 GPU (16 GB VRAM).&lt;/p&gt; &#xA;&lt;p&gt;For more details on Whisper fine-tuning, datasets and metrics, refer to Sanchit Gandhi&#39;s brilliant blogpost: &lt;a href=&#34;https://huggingface.co/blog/fine-tune-whisper&#34;&gt;Fine-Tune Whisper For Multilingual ASR with ü§ó Transformers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why Parameter Efficient Fine Tuning (&lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt;)?&lt;/h2&gt; &#xA;&lt;p&gt;As model sizes continue to increase, fine-tuning a model has become both computationally expensive and storage heavy. For example, a &lt;code&gt;Whisper-large-v2&lt;/code&gt; model requires ~24GB of GPU VRAM for full fine-tuning and requires ~7 GB of storage for each fine-tuned checkpoint. For low-resource environments this becomes quite a bottleneck and often near impossible to get meaningful results.&lt;/p&gt; &#xA;&lt;p&gt;Cue: PEFT! With PEFT you can tackle this bottleneck head-on. Like Low Rank Adaptation (LoRA), PEFT only fine-tunes a small number of (extra) model parameters while freezing most parameters of the pretrained model, thereby greatly decreasing the computational and storage costs.&lt;/p&gt; &#xA;&lt;h3&gt;Aha! So wait, what&#39;s this LoRA thing?&lt;/h3&gt; &#xA;&lt;p&gt;PEFT comes out-of-the-box with multiple parameter efficient techniques. One such technique is &lt;a href=&#34;https://github.com/microsoft/LoRA&#34;&gt;Low Rank Adaptation or LoRA&lt;/a&gt;. LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture. This greatly reduces the number of trainable parameters for downstream tasks.&lt;/p&gt; &#xA;&lt;p&gt;LoRA performs on-par or better than fine-tuning in model quality despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency.&lt;/p&gt; &#xA;&lt;h3&gt;That&#39;s all cool, but show me the numbers?&lt;/h3&gt; &#xA;&lt;p&gt;Don&#39;t worry, we got ya! We ran multiple experiments to compare a full fine-tuning of the Whisper-large-v2 checkpoint and with PEFT fine-tuning. Here&#39;s what we found:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We were able to fine-tune a 1.6B parameter model with less than 8GB GPU VRAM. ü§Ø&lt;/li&gt; &#xA; &lt;li&gt;With significantly less number of traininable parameters, we were able to fit almost &lt;strong&gt;5x&lt;/strong&gt; more batch size. üìà&lt;/li&gt; &#xA; &lt;li&gt;The resulting checkpoints were less than 1% the size of the original model, ~60MB. üöÄ&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To make things even better, all of this comes with minimal changes to the existing ü§ó transformers Whisper inference codebase.&lt;/p&gt; &#xA;&lt;p&gt;Curious to test this out for yourself? Follow along!&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning Whisper in a Google Colab&lt;/h2&gt; &#xA;&lt;h3&gt;Prepare Environment&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ll employ several popular Python packages to fine-tune the Whisper model. We&#39;ll use &lt;code&gt;datasets&lt;/code&gt; to download and prepare our training data and &lt;code&gt;transformers&lt;/code&gt; to load and train our Whisper model. We&#39;ll also require the &lt;code&gt;librosa&lt;/code&gt; package to pre-process audio files, &lt;code&gt;evaluate&lt;/code&gt; and &lt;code&gt;jiwer&lt;/code&gt; to assess the performance of our model. Finally, we&#39;ll use &lt;code&gt;PEFT&lt;/code&gt;, &lt;code&gt;bitsandbytes&lt;/code&gt;, &lt;code&gt;accelerate&lt;/code&gt; to prepare and fine-tune the model with LoRA.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!pip install -q transformers datasets librosa evaluate jiwer gradio bitsandbytes==0.37 accelerate &#xA;!pip install -q git+https://github.com/huggingface/peft.git@main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We strongly advise you to upload model checkpoints directly the &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face Hub&lt;/a&gt; whilst training. The Hub provides:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integrated version control: you can be sure that no model checkpoint is lost during training.&lt;/li&gt; &#xA; &lt;li&gt;Tensorboard logs: track important metrics over the course of training.&lt;/li&gt; &#xA; &lt;li&gt;Model cards: document what a model does and its intended use cases.&lt;/li&gt; &#xA; &lt;li&gt;Community: an easy way to share and collaborate with the community!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Linking the notebook to the Hub is straightforward - it simply requires entering your Hub authentication token when prompted. Find your Hub authentication token &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;here&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from huggingface_hub import notebook_login&#xA;&#xA;notebook_login()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Load Dataset&lt;/h1&gt; &#xA;&lt;p&gt;Using ü§ó Datasets, downloading and preparing data is extremely simple. We can download and prepare the Common Voice splits in just one line of code.&lt;/p&gt; &#xA;&lt;p&gt;First, ensure you have accepted the terms of use on the Hugging Face Hub: &lt;a href=&#34;https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0&#34;&gt;mozilla-foundation/common_voice_13_0&lt;/a&gt;. Once you have accepted the terms, you will have full access to the dataset and be able to download the data locally.&lt;/p&gt; &#xA;&lt;p&gt;Since Hindi is very low-resource, we&#39;ll combine the &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;validation&lt;/code&gt; splits to give approximately 12 hours of training data. We&#39;ll use the 6 hours of &lt;code&gt;test&lt;/code&gt; data as our held-out test set:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import load_dataset, DatasetDict&#xA;&#xA;common_voice = DatasetDict()&#xA;&#xA;language_abbr = &#34;hi&#34; # Replace with the language ID of your choice here!&#xA;&#xA;common_voice[&#34;train&#34;] = load_dataset(dataset_name, language_abbr, split=&#34;train+validation&#34;, use_auth_token=True)&#xA;common_voice[&#34;test&#34;] = load_dataset(dataset_name, language_abbr, split=&#34;test&#34;, use_auth_token=True)&#xA;&#xA;print(common_voice)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Print output:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;DatasetDict({&#xA;    train: Dataset({&#xA;        features: [&#39;client_id&#39;, &#39;path&#39;, &#39;audio&#39;, &#39;sentence&#39;, &#39;up_votes&#39;, &#39;down_votes&#39;, &#39;age&#39;, &#39;gender&#39;, &#39;accent&#39;, &#39;locale&#39;, &#39;segment&#39;, &#39;variant&#39;],&#xA;        num_rows: 6760&#xA;    })&#xA;    test: Dataset({&#xA;        features: [&#39;client_id&#39;, &#39;path&#39;, &#39;audio&#39;, &#39;sentence&#39;, &#39;up_votes&#39;, &#39;down_votes&#39;, &#39;age&#39;, &#39;gender&#39;, &#39;accent&#39;, &#39;locale&#39;, &#39;segment&#39;, &#39;variant&#39;],&#xA;        num_rows: 2947&#xA;    })&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Most ASR datasets only provide input audio samples (&lt;code&gt;audio&lt;/code&gt;) and the corresponding transcribed text (&lt;code&gt;sentence&lt;/code&gt;). Common Voice contains additional metadata information, such as &lt;code&gt;accent&lt;/code&gt; and &lt;code&gt;locale&lt;/code&gt;, which we can disregard for ASR. Keeping the notebook as general as possible, we only consider the input audio and transcribed text for fine-tuning, discarding the additional metadata information:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;common_voice = common_voice.remove_columns(&#xA;    [&#34;accent&#34;, &#34;age&#34;, &#34;client_id&#34;, &#34;down_votes&#34;, &#34;gender&#34;, &#34;locale&#34;, &#34;path&#34;, &#34;segment&#34;, &#34;up_votes&#34;, &#34;variant&#34;]&#xA;)&#xA;&#xA;print(common_voice)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Print output:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;DatasetDict({&#xA;    train: Dataset({&#xA;        features: [&#39;audio&#39;, &#39;sentence&#39;],&#xA;        num_rows: 6760&#xA;    })&#xA;    test: Dataset({&#xA;        features: [&#39;audio&#39;, &#39;sentence&#39;],&#xA;        num_rows: 2947&#xA;    })&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Prepare Feature Extractor, Tokenizer and Data&lt;/h3&gt; &#xA;&lt;p&gt;The ASR pipeline can be de-composed into three stages:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A feature extractor which pre-processes the raw audio-inputs&lt;/li&gt; &#xA; &lt;li&gt;The model which performs the sequence-to-sequence mapping&lt;/li&gt; &#xA; &lt;li&gt;A tokenizer which post-processes the model outputs to text format&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;In ü§ó Transformers, the Whisper model has an associated feature extractor and tokenizer, called &lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor&#34;&gt;WhisperFeatureExtractor&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer&#34;&gt;WhisperTokenizer&lt;/a&gt; respectively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import WhisperFeatureExtractor&#xA;&#xA;feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import WhisperTokenizer&#xA;&#xA;task = &#34;transcribe&#34;&#xA;&#xA;tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language_abbr, task=task)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To simplify using the feature extractor and tokenizer, we can &lt;em&gt;wrap&lt;/em&gt; both into a single &lt;code&gt;WhisperProcessor&lt;/code&gt; class. This processor object can be used on the audio inputs and model predictions as required. In doing so, we only need to keep track of two objects during training: the &lt;code&gt;processor&lt;/code&gt; and the &lt;code&gt;model&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import WhisperProcessor&#xA;&#xA;processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Prepare Data&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s print the first example of the Common Voice dataset to see what form the data is in:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(common_voice[&#34;train&#34;][0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Print output:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#39;audio&#39;: {&#39;path&#39;: &#39;/root/.cache/huggingface/datasets/downloads/extracted/ff5a2373454f699ff252bdfd5f333826b18a3a91903d16ed05625bbdbabea9c7/common_voice_hi_26008353.mp3&#39;, &#39;array&#39;: array([ 5.81611368e-26, -1.48634016e-25, -9.37040538e-26, ...,&#xA;        1.06425901e-07,  4.46416450e-08,  2.61450239e-09]), &#39;sampling_rate&#39;: 48000}, &#39;sentence&#39;: &#39;‡§π‡§Æ‡§®‡•á ‡§â‡§∏‡§ï‡§æ ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§Æ‡§®‡§æ‡§Ø‡§æ‡•§&#39;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since our input audio is sampled at 48kHz, we need to &lt;em&gt;downsample&lt;/em&gt; it to 16kHz prior to passing it to the Whisper feature extractor, 16kHz being the sampling rate expected by the Whisper model.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ll set the audio inputs to the correct sampling rate using dataset&#39;s &lt;a href=&#34;https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column&#34;&gt;&lt;code&gt;cast_column&lt;/code&gt;&lt;/a&gt; method. This operation does not change the audio in-place, but rather signals to &lt;code&gt;datasets&lt;/code&gt; to resample audio samples &lt;em&gt;on the fly&lt;/em&gt; the first time that they are loaded:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import Audio&#xA;&#xA;common_voice = common_voice.cast_column(&#34;audio&#34;, Audio(sampling_rate=16000))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Re-loading the first audio sample in the Common Voice dataset will resample it to the desired sampling rate:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(common_voice[&#34;train&#34;][0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Re-loading the first audio sample in the Common Voice dataset will resample it to the desired sampling rate:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#39;audio&#39;: {&#39;path&#39;: &#39;/root/.cache/huggingface/datasets/downloads/extracted/ff5a2373454f699ff252bdfd5f333826b18a3a91903d16ed05625bbdbabea9c7/common_voice_hi_26008353.mp3&#39;, &#39;array&#39;: array([ 5.81611368e-26, -1.48634016e-25, -9.37040538e-26, ...,&#xA;        1.06425901e-07,  4.46416450e-08,  2.61450239e-09]), &#39;sampling_rate&#39;: 48000}, &#39;sentence&#39;: &#39;‡§π‡§Æ‡§®‡•á ‡§â‡§∏‡§ï‡§æ ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§Æ‡§®‡§æ‡§Ø‡§æ‡•§&#39;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now we can write a function to prepare our data ready for the model:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We load and resample the audio data by calling &lt;code&gt;batch[&#34;audio&#34;]&lt;/code&gt;. As explained above, ü§ó Datasets performs any necessary resampling operations on the fly.&lt;/li&gt; &#xA; &lt;li&gt;We use the feature extractor to compute the log-Mel spectrogram input features from our 1-dimensional audio array.&lt;/li&gt; &#xA; &lt;li&gt;We encode the transcriptions to label ids through the use of the tokenizer.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def prepare_dataset(batch):&#xA;    # load and resample audio data from 48 to 16kHz&#xA;    audio = batch[&#34;audio&#34;]&#xA;&#xA;    # compute log-Mel input features from input audio array&#xA;    batch[&#34;input_features&#34;] = feature_extractor(audio[&#34;array&#34;], sampling_rate=audio[&#34;sampling_rate&#34;]).input_features[0]&#xA;&#xA;    # encode target text to label ids&#xA;    batch[&#34;labels&#34;] = tokenizer(batch[&#34;sentence&#34;]).input_ids&#xA;    return batch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can apply the data preparation function to all of our training examples using dataset&#39;s &lt;code&gt;.map&lt;/code&gt; method. The argument &lt;code&gt;num_proc&lt;/code&gt; specifies how many CPU cores to use. Setting &lt;code&gt;num_proc&lt;/code&gt; &amp;gt; 1 will enable multiprocessing. If the &lt;code&gt;.map&lt;/code&gt; method hangs with multiprocessing, set &lt;code&gt;num_proc=1&lt;/code&gt; and process the dataset sequentially.&lt;/p&gt; &#xA;&lt;p&gt;Make yourself some tea üçµ, depending on dataset size, this might take 20-30 minutes ‚è∞&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[&#34;train&#34;], num_proc=2)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;common_voice[&#34;train&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Print output:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Dataset({&#xA;    features: [&#39;input_features&#39;, &#39;labels&#39;],&#xA;    num_rows: 6760&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training and Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Now that we&#39;ve prepared our data, we&#39;re ready to dive into the training pipeline. The &lt;a href=&#34;https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer&#34;&gt;ü§ó Trainer&lt;/a&gt; will do much of the heavy lifting for us. All we have to do is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Define a data collator: the data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Evaluation metrics: during evaluation, we want to evaluate the model using the &lt;a href=&#34;https://huggingface.co/metrics/wer&#34;&gt;word error rate (WER)&lt;/a&gt; metric.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Define the training configuration: this will be used by the ü§ó Trainer to define the training schedule.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once we&#39;ve fine-tuned the model, we will evaluate it on the test data to verify that we have correctly trained it to transcribe speech in Hindi.&lt;/p&gt; &#xA;&lt;h3&gt;Define a Data Collator&lt;/h3&gt; &#xA;&lt;p&gt;The data collator for a sequence-to-sequence speech model is unique in the sense that it treats the &lt;code&gt;input_features&lt;/code&gt; and &lt;code&gt;labels&lt;/code&gt; independently: the &lt;code&gt;input_features&lt;/code&gt; must be handled by the feature extractor and the &lt;code&gt;labels&lt;/code&gt; by the tokenizer.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;input_features&lt;/code&gt; are already padded to 30s and converted to a log-Mel spectrogram of fixed dimension by action of the feature extractor, so all we have to do is convert the &lt;code&gt;input_features&lt;/code&gt; to batched PyTorch tensors. We do this using the feature extractor&#39;s &lt;code&gt;.pad&lt;/code&gt; method with &lt;code&gt;return_tensors=pt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;labels&lt;/code&gt; on the other hand are un-padded. We first pad the sequences to the maximum length in the batch using the tokenizer&#39;s &lt;code&gt;.pad&lt;/code&gt; method. The padding tokens are then replaced by &lt;code&gt;-100&lt;/code&gt; so that these tokens are &lt;strong&gt;not&lt;/strong&gt; taken into account when computing the loss. We then cut the BOS token from the start of the label sequence as we append it later during training.&lt;/p&gt; &#xA;&lt;p&gt;We can leverage the &lt;code&gt;WhisperProcessor&lt;/code&gt; we defined earlier to perform both the feature extractor and the tokenizer operations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;from dataclasses import dataclass&#xA;from typing import Any, Dict, List, Union&#xA;&#xA;&#xA;@dataclass&#xA;class DataCollatorSpeechSeq2SeqWithPadding:&#xA;    processor: Any&#xA;&#xA;    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -&amp;gt; Dict[str, torch.Tensor]:&#xA;        # split inputs and labels since they have to be of different lengths and need different padding methods&#xA;        # first treat the audio inputs by simply returning torch tensors&#xA;        input_features = [{&#34;input_features&#34;: feature[&#34;input_features&#34;]} for feature in features]&#xA;        batch = self.processor.feature_extractor.pad(input_features, return_tensors=&#34;pt&#34;)&#xA;&#xA;        # get the tokenized label sequences&#xA;        label_features = [{&#34;input_ids&#34;: feature[&#34;labels&#34;]} for feature in features]&#xA;        # pad the labels to max length&#xA;        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=&#34;pt&#34;)&#xA;&#xA;        # replace padding with -100 to ignore loss correctly&#xA;        labels = labels_batch[&#34;input_ids&#34;].masked_fill(labels_batch.attention_mask.ne(1), -100)&#xA;&#xA;        # if bos token is appended in previous tokenization step,&#xA;        # cut bos token here as it&#39;s append later anyways&#xA;        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():&#xA;            labels = labels[:, 1:]&#xA;&#xA;        batch[&#34;labels&#34;] = labels&#xA;&#xA;        return batch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s initialise the data collator we&#39;ve just defined:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation metrics&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ll use the word error rate (WER) metric, the &#39;de-facto&#39; metric for assessing ASR systems. For more information, refer to the WER &lt;a href=&#34;https://huggingface.co/metrics/wer&#34;&gt;docs&lt;/a&gt;. We&#39;ll load the WER metric from ü§ó Evaluate:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import evaluate&#xA;&#xA;metric = evaluate.load(&#34;wer&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;###&amp;nbsp;Load a Pre-Trained Checkpoint&lt;/p&gt; &#xA;&lt;p&gt;Now let&#39;s load the pre-trained Whisper checkpoint. Again, this is trivial through use of ü§ó Transformers!&lt;/p&gt; &#xA;&lt;p&gt;To reduce our models memory footprint, we load the model in 8bit, this means we quantize the model to use 1/4th precision (when compared to float32) with minimal loss to performance. To read more about how this works, head over &lt;a href=&#34;https://huggingface.co/blog/hf-bitsandbytes-integration&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import WhisperForConditionalGeneration&#xA;&#xA;model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=&#34;auto&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Post-processing on the model&lt;/h3&gt; &#xA;&lt;p&gt;Finally, we need to apply some post-processing steps on the 8-bit model to enable training. We do so by first freezing all the model layers, and then cast the layer-norm and the output layer in &lt;code&gt;float32&lt;/code&gt; for training and model stability.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from peft import prepare_model_for_int8_training&#xA;&#xA;model = prepare_model_for_int8_training(model, output_embedding_layer_name=&#34;proj_out&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since the Whisper model uses Convolutional layers in the Encoder, checkpointing disables grad computation to avoid this we specifically need to make the inputs trainable.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def make_inputs_require_grad(module, input, output):&#xA;    output.requires_grad_(True)&#xA;&#xA;model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Apply Low-rank adapters (LoRA) to the model&lt;/h3&gt; &#xA;&lt;p&gt;Here comes the magic with &lt;code&gt;peft&lt;/code&gt;! Let&#39;s load a &lt;code&gt;PeftModel&lt;/code&gt; and specify that we are going to use low-rank adapters (LoRA) using &lt;code&gt;get_peft_model&lt;/code&gt; utility function from &lt;code&gt;peft&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model&#xA;&#xA;config = LoraConfig(r=32, lora_alpha=64, target_modules=[&#34;q_proj&#34;, &#34;v_proj&#34;], lora_dropout=0.05, bias=&#34;none&#34;)&#xA;&#xA;model = get_peft_model(model, config)&#xA;model.print_trainable_parameters()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Print output:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;trainable params: 15728640 || all params: 1559033600 || trainable%: 1.0088711365810203&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We are ONLY using &lt;strong&gt;1%&lt;/strong&gt; of the total trainable parameters, thereby performing &lt;strong&gt;Parameter-Efficient Fine-Tuning&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Define the Training Configuration&lt;/h3&gt; &#xA;&lt;p&gt;In the final step, we define all the parameters related to training. For more detail on the training arguments, refer to the Seq2SeqTrainingArguments &lt;a href=&#34;https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments&#34;&gt;docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import Seq2SeqTrainingArguments&#xA;&#xA;training_args = Seq2SeqTrainingArguments(&#xA;    output_dir=&#34;reach-vb/test&#34;,  # change to a repo name of your choice&#xA;    per_device_train_batch_size=8,&#xA;    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size&#xA;    learning_rate=1e-3,&#xA;    warmup_steps=50,&#xA;    num_train_epochs=3,&#xA;    evaluation_strategy=&#34;steps&#34;,&#xA;    fp16=True,&#xA;    per_device_eval_batch_size=8,&#xA;    generation_max_length=128,&#xA;    logging_steps=100,&#xA;#    max_steps=100, # only for testing purposes, remove this from your final run :)&#xA;    remove_unused_columns=False,  # required as the PeftModel forward doesn&#39;t have the signature of the wrapped model&#39;s forward&#xA;    label_names=[&#34;labels&#34;],  # same reason as above&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Fine-tuning a model with PEFT comes with a few caveats.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;We need to explicitly set &lt;code&gt;remove_unused_columns=False&lt;/code&gt; and &lt;code&gt;label_names=[&#34;labels&#34;]&lt;/code&gt; as the PeftModel&#39;s forward doesn&#39;t inherit the signature of the base model&#39;s forward.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Since INT8 training requires autocasting, we cannot use the native &lt;code&gt;predict_with_generate&lt;/code&gt; call in Trainer as it doesn&#39;t automatically cast.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Similarly, since we cannot autocast, we cannot pass the &lt;code&gt;compute_metrics&lt;/code&gt; to &lt;code&gt;Seq2SeqTrainer&lt;/code&gt; so we&#39;ll comment it out whilst instantiating the Trainer.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl&#xA;from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR&#xA;&#xA;# This callback helps to save only the adapter weights and remove the base model weights.&#xA;class SavePeftModelCallback(TrainerCallback):&#xA;    def on_save(&#xA;        self,&#xA;        args: TrainingArguments,&#xA;        state: TrainerState,&#xA;        control: TrainerControl,&#xA;        **kwargs,&#xA;    ):&#xA;        checkpoint_folder = os.path.join(args.output_dir, f&#34;{PREFIX_CHECKPOINT_DIR}-{state.global_step}&#34;)&#xA;&#xA;        peft_model_path = os.path.join(checkpoint_folder, &#34;adapter_model&#34;)&#xA;        kwargs[&#34;model&#34;].save_pretrained(peft_model_path)&#xA;&#xA;        pytorch_model_path = os.path.join(checkpoint_folder, &#34;pytorch_model.bin&#34;)&#xA;        if os.path.exists(pytorch_model_path):&#xA;            os.remove(pytorch_model_path)&#xA;        return control&#xA;&#xA;&#xA;trainer = Seq2SeqTrainer(&#xA;    args=training_args,&#xA;    model=model,&#xA;    train_dataset=common_voice[&#34;train&#34;],&#xA;    eval_dataset=common_voice[&#34;test&#34;],&#xA;    data_collator=data_collator,&#xA;    tokenizer=processor.feature_extractor,&#xA;    callbacks=[SavePeftModelCallback],&#xA;)&#xA;model.config.use_cache = False  # silence the warnings. Please re-enable for inference!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and, we are all set! We can now set our model to train! &amp;lt;3&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;trainer.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Depending on your dataset it should take about 6-8 hours to fine-tune the model.&lt;/p&gt; &#xA;&lt;p&gt;Once the model is fine-tuned, we can push the model on to Hugging Face Hub, this will later help us directly infer the model from the model repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;peft_model_id = &#34;reach-vb/whisper-large-v2-hindi-100steps&#34;&#xA;model.push_to_hub(peft_model_id)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation and Inference&lt;/h3&gt; &#xA;&lt;p&gt;On to the fun part, we&#39;ve successfully fine-tuned our model. Now let&#39;s put it to test and calculate the WER on the &lt;code&gt;test&lt;/code&gt; set.&lt;/p&gt; &#xA;&lt;p&gt;As with training, we do have a few caveats to pay attention to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Since we cannot use &lt;code&gt;predict_with_generate&lt;/code&gt; function, we will hand roll our own eval loop with &lt;code&gt;torch.cuda.amp.autocast()&lt;/code&gt; you can check it out below.&lt;/li&gt; &#xA; &lt;li&gt;Since the base model is frozen, PEFT model sometimes fails to recognise the language while decoding. To fix that, we force the starting tokens to mention the language we are transcribing. This is done via &lt;code&gt;forced_decoder_ids = processor.get_decoder_prompt_ids(language=&#34;Marathi&#34;, task=&#34;transcribe&#34;)&lt;/code&gt; and passing that too the &lt;code&gt;model.generate&lt;/code&gt; call.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;That&#39;s it, let&#39;s get transcribing! üî•&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from peft import PeftModel, PeftConfig&#xA;from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer&#xA;&#xA;peft_model_id = &#34;reach-vb/whisper-large-v2-hindi-100steps&#34; # Use the same model ID as before.&#xA;peft_config = PeftConfig.from_pretrained(peft_model_id)&#xA;model = WhisperForConditionalGeneration.from_pretrained(&#xA;    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=&#34;auto&#34;&#xA;)&#xA;model = PeftModel.from_pretrained(model, peft_model_id)&#xA;model.config.use_cache = True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s define our Evaluation loop&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import gc&#xA;import numpy as np&#xA;from tqdm import tqdm&#xA;from torch.utils.data import DataLoader&#xA;from transformers.models.whisper.english_normalizer import BasicTextNormalizer&#xA;&#xA;eval_dataloader = DataLoader(common_voice[&#34;test&#34;], batch_size=8, collate_fn=data_collator)&#xA;forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)&#xA;normalizer = BasicTextNormalizer()&#xA;&#xA;predictions = []&#xA;references = []&#xA;normalized_predictions = []&#xA;normalized_references = []&#xA;&#xA;model.eval()&#xA;for step, batch in enumerate(tqdm(eval_dataloader)):&#xA;    with torch.cuda.amp.autocast():&#xA;        with torch.no_grad():&#xA;            generated_tokens = (&#xA;                model.generate(&#xA;                    input_features=batch[&#34;input_features&#34;].to(&#34;cuda&#34;),&#xA;                    forced_decoder_ids=forced_decoder_ids,&#xA;                    max_new_tokens=255,&#xA;                )&#xA;                .cpu()&#xA;                .numpy()&#xA;            )&#xA;            labels = batch[&#34;labels&#34;].cpu().numpy()&#xA;            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)&#xA;            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)&#xA;            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)&#xA;            predictions.extend(decoded_preds)&#xA;            references.extend(decoded_labels)&#xA;            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])&#xA;            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])&#xA;        del generated_tokens, labels, batch&#xA;    gc.collect()&#xA;wer = 100 * metric.compute(predictions=predictions, references=references)&#xA;normalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)&#xA;eval_metrics = {&#34;eval/wer&#34;: wer, &#34;eval/normalized_wer&#34;: normalized_wer}&#xA;&#xA;print(f&#34;{wer=} and {normalized_wer=}&#34;)&#xA;print(eval_metrics)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference with Pipeline&lt;/h3&gt; &#xA;&lt;p&gt;Wrapping this all up and let&#39;s use the &lt;code&gt;pipeline&lt;/code&gt; API in ü§ó transformers to run inference on our newly-fine-tuned model! To do so, load the LoRA weights from the hub and download the base processor, tokenizer and then pass it all in the pipeline object! That&#39;s it. Happy inferencing!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import (&#xA;    AutomaticSpeechRecognitionPipeline,&#xA;    WhisperForConditionalGeneration,&#xA;    WhisperTokenizer,&#xA;    WhisperProcessor,&#xA;)&#xA;from peft import PeftModel, PeftConfig&#xA;&#xA;&#xA;peft_model_id = &#34;reach-vb/whisper-large-v2-hindi-100steps&#34; # Use the same model ID as before.&#xA;language = &#34;hi&#34;&#xA;task = &#34;transcribe&#34;&#xA;peft_config = PeftConfig.from_pretrained(peft_model_id)&#xA;model = WhisperForConditionalGeneration.from_pretrained(&#xA;    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=&#34;auto&#34;&#xA;)&#xA;&#xA;model = PeftModel.from_pretrained(model, peft_model_id)&#xA;tokenizer = WhisperTokenizer.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)&#xA;processor = WhisperProcessor.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)&#xA;feature_extractor = processor.feature_extractor&#xA;forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)&#xA;pipe = AutomaticSpeechRecognitionPipeline(model=model, tokenizer=tokenizer, feature_extractor=feature_extractor)&#xA;&#xA;&#xA;def transcribe(audio):&#xA;    with torch.cuda.amp.autocast():&#xA;        text = pipe(audio, generate_kwargs={&#34;forced_decoder_ids&#34;: forced_decoder_ids}, max_new_tokens=255)[&#34;text&#34;]&#xA;    return text&#xA;&#xA;transcribe(&#34;test_file.mp3&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fin!&lt;/h2&gt; &#xA;&lt;p&gt;If you made it all the way till the end then pat yourself on the back. Looking back, we learned how to train &lt;em&gt;any&lt;/em&gt; Whisper checkpoint faster, cheaper and with negligible loss in WER.&lt;/p&gt; &#xA;&lt;p&gt;With PEFT, you can also go beyond Speech recognition and apply the same set of techniques to other pretrained models as well. Come check it out here: &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;https://github.com/huggingface/peft&lt;/a&gt; ü§ó&lt;/p&gt; &#xA;&lt;p&gt;Hungry to push this to the limits and test out more SoTA techniques? &lt;a href=&#34;https://github.com/huggingface/peft/raw/main/examples/int8_training/run_adalora_whisper_int8.sh&#34;&gt;Try Whisper with adalora!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Don&#39;t forget to tweet your results and tag us! &lt;a href=&#34;https://twitter.com/huggingface&#34;&gt;@huggingface&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/reach_vb&#34;&gt;@reach_vb&lt;/a&gt; ‚ù§Ô∏è&lt;/p&gt;</summary>
  </entry>
</feed>