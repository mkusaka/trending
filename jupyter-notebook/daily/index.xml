<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-09T01:32:05Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ZhengPeng7/BiRefNet</title>
    <updated>2025-02-09T01:32:05Z</updated>
    <id>tag:github.com,2025-02-09:/ZhengPeng7/BiRefNet</id>
    <link href="https://github.com/ZhengPeng7/BiRefNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CAAI AIR&#39;24] Bilateral Reference for High-Resolution Dichotomous Image Segmentation&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;Bilateral Reference for High-Resolution Dichotomous Image Segmentation&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=TZRzWOsAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Peng Zheng&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 1,4,5,6&lt;/sup&gt;,â€‰ &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=0uPb8MMAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Dehong Gao&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 2&lt;/sup&gt;,â€‰ &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=kakwJ5QAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Deng-Ping Fan&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 1*&lt;/sup&gt;,â€‰ &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=9cMQrVsAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Li Liu&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 3&lt;/sup&gt;,â€‰ &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=qQP6WXIAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Jorma Laaksonen&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 4&lt;/sup&gt;,â€‰ &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=pw_0Z_UAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Wanli Ouyang&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 5&lt;/sup&gt;,â€‰ &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=stFCYOAAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Nicu Sebe&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 6&lt;/sup&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;sup&gt;1 &lt;/sup&gt;Nankai Universityâ€‚ &#xA; &lt;sup&gt;2 &lt;/sup&gt;Northwestern Polytechnical Universityâ€‚ &#xA; &lt;sup&gt;3 &lt;/sup&gt;National University of Defense Technologyâ€‚ &#xA; &lt;br&gt; &#xA; &lt;sup&gt;4 &lt;/sup&gt;Aalto Universityâ€‚ &#xA; &lt;sup&gt;5 &lt;/sup&gt;Shanghai AI Laboratoryâ€‚ &#xA; &lt;sup&gt;6 &lt;/sup&gt;University of Trentoâ€‚ &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;display: flex; justify-content: center; flex-wrap: wrap;&#34;&gt; &#xA; &lt;a href=&#34;https://www.sciopen.com/article/pdf/10.26599/AIR.2024.9150038.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Journal-Paper-red&#34;&gt;&lt;/a&gt;â€‚ &#xA; &lt;a href=&#34;https://arxiv.org/pdf/2401.03407&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-red&#34;&gt;&lt;/a&gt;â€‚ &#xA; &lt;a href=&#34;https://drive.google.com/file/d/1FWvKDWTnK9RsiywfCsIxsnQzqv-dlO5u/view&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E4%B8%AD%E6%96%87%E7%89%88-Paper-red&#34;&gt;&lt;/a&gt;â€‚ &#xA; &lt;a href=&#34;https://www.birefnet.top&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Page-Project-red&#34;&gt;&lt;/a&gt;â€‚ &#xA; &lt;a href=&#34;https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GDrive-Stuff-green&#34;&gt;&lt;/a&gt;â€‚ &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/ZhengPeng7/BiRefNet/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow&#34;&gt;&lt;/a&gt;â€‚ &#xA; &lt;a href=&#34;https://huggingface.co/spaces/ZhengPeng7/BiRefNet_demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HF-Space-blue&#34;&gt;&lt;/a&gt;â€‚ &#xA; &lt;a href=&#34;https://huggingface.co/ZhengPeng7/BiRefNet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HF-Model-blue&#34;&gt;&lt;/a&gt;â€‚ &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;display: flex; justify-content: center; flex-wrap: wrap;&#34;&gt; &#xA; &lt;a href=&#34;https://colab.research.google.com/drive/14Dqg7oeBkFEtchaHLNpig2BcdkZEogba&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Multiple_Images_Inference-F9AB00?style=for-the-badge&amp;amp;logo=googlecolab&amp;amp;color=525252&#34;&gt;&lt;/a&gt;â€‚ &#xA; &lt;a href=&#34;https://colab.research.google.com/drive/1MaEiBfJ4xIaZZn0DqKrhydHB8X97hNXl&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Inference_&amp;amp;_Evaluation-F9AB00?style=for-the-badge&amp;amp;logo=googlecolab&amp;amp;color=525252&#34;&gt;&lt;/a&gt;â€‚ &#xA; &lt;a href=&#34;https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Box_Guided_Segmentation-F9AB00?style=for-the-badge&amp;amp;logo=googlecolab&amp;amp;color=525252&#34;&gt;&lt;/a&gt;â€‚ &#xA;&lt;/div&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;em&gt;DIS-Sample_1&lt;/em&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;em&gt;DIS-Sample_2&lt;/em&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1ItXaA26iYnE8XQ_GgNLy71MOWePoS2-g&amp;amp;sz=w400&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1Z-esCujQF_uEa_YJjkibc3NUrW4aR_d4&amp;amp;sz=w400&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;This repo is the official implementation of &#34;&lt;a href=&#34;https://arxiv.org/pdf/2401.03407&#34;&gt;&lt;strong&gt;Bilateral Reference for High-Resolution Dichotomous Image Segmentation&lt;/strong&gt;&lt;/a&gt;&#34; (&lt;em&gt;&lt;strong&gt;CAAI AIR 2024&lt;/strong&gt;&lt;/em&gt;).&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!note] &lt;strong&gt;We need more GPU resources&lt;/strong&gt; to push forward the performance of BiRefNet, especially on &lt;em&gt;video&lt;/em&gt; tasks and more &lt;em&gt;efficient&lt;/em&gt; model designs on higher-resolution images. If you are happy to cooperate, please contact me at &lt;a href=&#34;mailto:zhengpeng0108@gmail.com&#34;&gt;zhengpeng0108@gmail.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;News &lt;span&gt;ðŸ“°&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Feb 1, 2025&lt;/code&gt;:&lt;/strong&gt; We released the &lt;a href=&#34;https://huggingface.co/ZhengPeng7/BiRefNet_HR&#34;&gt;BiRefNet_HR&lt;/a&gt; for general use, which was trained on images in &lt;code&gt;2048x2048&lt;/code&gt; and shows great performance on higher resolution images! Thanks to &lt;a href=&#34;https://www.freepik.com&#34;&gt;Freepik&lt;/a&gt; for offering H200x4 GPU for this huge training (~3 weeks).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Jan 6, 2025&lt;/code&gt;:&lt;/strong&gt; Validate the success of FP16 inference with ~0 decrease of performance and better efficiency: the standard BiRefNet can run in &lt;code&gt;17 FPS&lt;/code&gt; with &lt;code&gt;resolution==1024x1024&lt;/code&gt; with &lt;code&gt;3.45GB GPU memory&lt;/code&gt; on a single &lt;code&gt;RTX 4090&lt;/code&gt;. Check more details in the model efficiency part below in &lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet?tab=readme-ov-file#model-zoo&#34;&gt;model zoo section&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Dec 5, 2024&lt;/code&gt;:&lt;/strong&gt; Fix the bug of using &lt;code&gt;torch.compile&lt;/code&gt; in latest PyTorch versions (2.5.1) and the slow iteration in FP16 training with accelerate (set as default).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Nov 28, 2024&lt;/code&gt;:&lt;/strong&gt; Congrats to students @Nankai University employed BiRefNet to build their project and won the &lt;a href=&#34;https://drive.google.com/file/d/1WDgcHzzmbPtj3O4tlZyT3HLfNKLBPkje/view?usp=drive_link&#34;&gt;provincial gold medal&lt;/a&gt; and &lt;a href=&#34;https://cy.ncss.cn/information/2c93f4c691983c5b0194264b1880207b&#34;&gt;national bronze medal&lt;/a&gt; on the &lt;a href=&#34;https://cy.ncss.cn/en&#34;&gt;China International College Studentsâ€™ Innovation Competition 2024&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Oct 26, 2024&lt;/code&gt;:&lt;/strong&gt; We added the &lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet?tab=readme-ov-file#pen-fine-tuning-on-custom-data&#34;&gt;guideline of conducting fine-tuning on custom data&lt;/a&gt; with existing weights.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Oct 6, 2024&lt;/code&gt;:&lt;/strong&gt; We uploaded the &lt;a href=&#34;https://huggingface.co/ZhengPeng7/BiRefNet-matting&#34;&gt;BiRefNet-matting&lt;/a&gt; model for general trimap-free matting use.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Sep 24, 2024&lt;/code&gt;:&lt;/strong&gt; We uploaded the &lt;a href=&#34;https://huggingface.co/ZhengPeng7/BiRefNet_lite-2K&#34;&gt;BiRefNet_lite-2K&lt;/a&gt; model, which takes inputs in a much higher resolution (2560x1440). We also added the &lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet/raw/main/tutorials/BiRefNet_inference_video.ipynb&#34;&gt;notebook&lt;/a&gt; for inference on videos.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Sep 7, 2024&lt;/code&gt;:&lt;/strong&gt; Thanks to &lt;a href=&#34;https://www.freepik.com&#34;&gt;Freepik&lt;/a&gt; for supporting me with GPUs for more extensive experiments, especially on BiRefNet for 2K inference!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Aug 30, 2024&lt;/code&gt;:&lt;/strong&gt; We uploaded notebooks in &lt;code&gt;tutorials&lt;/code&gt; to run the inference and ONNX conversion locally.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Aug 23, 2024&lt;/code&gt;:&lt;/strong&gt; Our BiRefNet is now officially released &lt;a href=&#34;https://www.sciopen.com/article/10.26599/AIR.2024.9150038&#34;&gt;online&lt;/a&gt; on CAAI AIR journal. And thanks to the &lt;a href=&#34;https://www.eurekalert.org/news-releases/1055380&#34;&gt;press release&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Aug 19, 2024&lt;/code&gt;:&lt;/strong&gt; We uploaded the ONNX model files of all weights in the &lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet/releases/tag/v1&#34;&gt;GitHub release&lt;/a&gt; and &lt;a href=&#34;https://drive.google.com/drive/u/0/folders/1kZM55bwsRdS__bdnsXpkmH6QPyza-9-N&#34;&gt;GDrive folder&lt;/a&gt;. Check out the &lt;strong&gt;ONNX conversion&lt;/strong&gt; part in &lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet?tab=readme-ov-file#model-zoo&#34;&gt;model zoo&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Jul 30, 2024&lt;/code&gt;:&lt;/strong&gt; Thanks to @not-lain for his kind efforts in adding BiRefNet to the official huggingface.js &lt;a href=&#34;https://github.com/huggingface/huggingface.js/raw/3a8651fbc6508920475564a692bf0e5b601d9343/packages/tasks/src/model-libraries-snippets.ts#L763&#34;&gt;repo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Jul 28, 2024&lt;/code&gt;:&lt;/strong&gt; We released the &lt;a href=&#34;https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK&#34;&gt;Colab demo for box-guided segmentation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Jul 15, 2024&lt;/code&gt;:&lt;/strong&gt; We deployed our BiRefNet on &lt;a href=&#34;https://huggingface.co/ZhengPeng7/BiRefNet&#34;&gt;Hugging Face Models&lt;/a&gt; for users to easily load it in one line code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Jun 21, 2024&lt;/code&gt;:&lt;/strong&gt; We released and uploaded the Chinese version of our original paper to my &lt;a href=&#34;https://drive.google.com/file/d/1aBnJ_R9lbnC2dm8dqD0-pzP2Cu-U1Xpt/view&#34;&gt;GDrive&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;May 28, 2024&lt;/code&gt;:&lt;/strong&gt; We hold a &lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet?tab=readme-ov-file#model-zoo&#34;&gt;model zoo&lt;/a&gt; with well-trained weights of our BiRefNet in different sizes and for different tasks, including general use, matting segmentation, DIS, HRSOD, COD, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;May 7, 2024&lt;/code&gt;:&lt;/strong&gt; We also released the &lt;a href=&#34;https://colab.research.google.com/drive/14Dqg7oeBkFEtchaHLNpig2BcdkZEogba&#34;&gt;Colab demo for multiple images inference&lt;/a&gt;. Many thanks to @rishabh063 for his support on it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Apr 9, 2024&lt;/code&gt;:&lt;/strong&gt; Thanks to &lt;a href=&#34;https://fal.ai/&#34;&gt;Features and Labels Inc.&lt;/a&gt; for deploying a cool online BiRefNet &lt;a href=&#34;https://fal.ai/models/fal-ai/birefnet/playground&#34;&gt;inference API&lt;/a&gt; and providing me with strong GPU resources for 4 months on more extensive experiments!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Mar 7, 2024&lt;/code&gt;:&lt;/strong&gt; We released BiRefNet codes, the well-trained weights for all tasks in the original papers, and all related stuff in my &lt;a href=&#34;https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM&#34;&gt;GDrive folder&lt;/a&gt;. Meanwhile, we also deployed our BiRefNet on &lt;a href=&#34;https://huggingface.co/spaces/ZhengPeng7/BiRefNet_demo&#34;&gt;Hugging Face Spaces&lt;/a&gt; for easier online use and released the &lt;a href=&#34;https://colab.research.google.com/drive/1MaEiBfJ4xIaZZn0DqKrhydHB8X97hNXl&#34;&gt;Colab demo for inference and evaluation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Jan 7, 2024&lt;/code&gt;:&lt;/strong&gt; We released our paper on &lt;a href=&#34;https://arxiv.org/pdf/2401.03407&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸš€&lt;/span&gt; Load BiRefNet in &lt;em&gt;ONE LINE&lt;/em&gt; by HuggingFace, check more: &lt;a href=&#34;https://huggingface.co/ZhengPeng7/birefnet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue&#34; alt=&#34;BiRefNet&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForImageSegmentation&#xA;birefnet = AutoModelForImageSegmentation.from_pretrained(&#39;zhengpeng7/BiRefNet&#39;, trust_remote_code=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸ›¬&lt;/span&gt; Inference Partner:&lt;/h2&gt; &#xA;&lt;p&gt;You can access the &lt;strong&gt;inference API&lt;/strong&gt; service of BiRefNet on &lt;a href=&#34;https://fal.ai&#34;&gt;FAL&lt;/a&gt; or click the &lt;code&gt;Deploy&lt;/code&gt; button on our &lt;a href=&#34;https://huggingface.co/ZhengPeng7/BiRefNet&#34;&gt;HF model page&lt;/a&gt; to set up your own deployment.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fal.ai/models/fal-ai/birefnet/v2&#34;&gt;https://fal.ai/models/fal-ai/birefnet/v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ui.endpoints.huggingface.co/new?repository=ZhengPeng7/BiRefNet&#34;&gt;https://ui.endpoints.huggingface.co/new?repository=ZhengPeng7/BiRefNet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Our BiRefNet has achieved SOTA on many similar HR tasks:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DIS&lt;/strong&gt;: &lt;a href=&#34;https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te1?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te2?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te2&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te3?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te3&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te4?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te4&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-vd?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-vd&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Figure of Comparison on DIS Papers with Codes (by the time of this work):&lt;/summary&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1DLt6CFXdT1QSWDj_6jRkyZINXZ4vmyRp&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1gn5GyKFlJbMIkre1JyEdHDSYcrFmcLD0&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=16CVYYOtafEeZhHqv0am2Daku1n_exMP6&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=10K45xwPXmaTG4Ex-29ss9payA9yBnyLn&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=16EuyqKFJOqwMmagvfnbC9hUurL9pYLLB&amp;amp;sz=w1620&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;COD&lt;/strong&gt;:&lt;a href=&#34;https://paperswithcode.com/sota/camouflaged-object-segmentation-on-cod?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-cod&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/camouflaged-object-segmentation-on-nc4k?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-nc4k&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/camouflaged-object-segmentation-on-camo?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-camo&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/camouflaged-object-segmentation-on-chameleon?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-chameleon&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Figure of Comparison on COD Papers with Codes (by the time of this work):&lt;/summary&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1DLt6CFXdT1QSWDj_6jRkyZINXZ4vmyRp&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1gn5GyKFlJbMIkre1JyEdHDSYcrFmcLD0&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=16CVYYOtafEeZhHqv0am2Daku1n_exMP6&amp;amp;sz=w1620&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;HRSOD&lt;/strong&gt;: &lt;a href=&#34;https://paperswithcode.com/sota/rgb-salient-object-detection-on-davis-s?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/rgb-salient-object-detection-on-davis-s&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/rgb-salient-object-detection-on-hrsod?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/rgb-salient-object-detection-on-hrsod&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/rgb-salient-object-detection-on-uhrsd?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/rgb-salient-object-detection-on-uhrsd&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/salient-object-detection-on-duts-te?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/salient-object-detection-on-duts-te&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/salient-object-detection-on-dut-omron?p=bilateral-reference-for-high-resolution&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/salient-object-detection-on-dut-omron&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Figure of Comparison on HRSOD Papers with Codes (by the time of this work):&lt;/summary&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1hNfQtlTAHT4-AVbk_47852zyRp1NOFLs&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1bcVldUAxYkMI3OMTyaP_jNuOugDfYj-d&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1p1zgyVz27cGEqQMtOKzm_6zoYK3Sw_Zk&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1TubAvcoEbH_mHu3I-AxflnB71nkf35jJ&amp;amp;sz=w1620&#34;&gt; &#xA; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1A3V9HjVtcMQdnGPwuy-DBVhwKuo0q2lT&amp;amp;sz=w1620&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;h4&gt;Try our online demos for inference:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inference and evaluation&lt;/strong&gt; of your given weights: &lt;a href=&#34;https://colab.research.google.com/drive/1MaEiBfJ4xIaZZn0DqKrhydHB8X97hNXl&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Online Inference with GUI&lt;/strong&gt; with adjustable resolutions: &lt;a href=&#34;https://huggingface.co/spaces/ZhengPeng7/BiRefNet_demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Online &lt;strong&gt;Multiple Images Inference&lt;/strong&gt; on Colab: &lt;a href=&#34;https://colab.research.google.com/drive/14Dqg7oeBkFEtchaHLNpig2BcdkZEogba&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://drive.google.com/thumbnail?id=12XmDhKtO1o2fEvBu4OE4ULVB2BK0ecWi&amp;amp;sz=w1620&#34;&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For more general use of our BiRefNet, I extended the original academic one to more general ones for better real-life application.&lt;/p&gt; &#xA; &lt;p&gt;Datasets and datasets are suggested to be downloaded from official pages. But you can also download the packaged ones: &lt;a href=&#34;https://drive.google.com/drive/folders/1hZW6tAGPJwo9mPS7qGGGdpxuvuXiyoMJ&#34;&gt;DIS&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/drive/folders/18_hAE3QM4cwAzEAKXuSNtKjmgFXTQXZN&#34;&gt;HRSOD&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/drive/folders/1EyHmKWsXfaCR9O0BiZEc3roZbRcs4ECO&#34;&gt;COD&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/drive/folders/1cmce_emsS8A5ha5XT2c_CZiJzlLM81ms&#34;&gt;Backbones&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Find performances (almost all metrics) of all models in the &lt;code&gt;exp-TASK_SETTINGS&lt;/code&gt; folders in [&lt;a href=&#34;https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM&#34;&gt;&lt;strong&gt;stuff&lt;/strong&gt;&lt;/a&gt;].&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Models in the original paper, for &lt;b&gt;comparison on benchmarks&lt;/b&gt;:&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Training Sets&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Backbone&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS5K-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1J90LucvDQaS3R_-9E7QUh1mgJ8eQvccb/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;COD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;COD10K-TR, CAMO-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1tM5M72k7a8aKF-dYy-QXaqvfEhbFaWkC/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DUTS-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1f7L0Pb1Y3RkOMbqLCW_zO31dik9AiUFa/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DUTS-TR, HRSOD-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1WJooyTkhoDLllaqwbpur_9Hle0XTHEs_/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DUTS-TR, UHRSD-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1Pu1mv3ORobJatIuUoEuZaWDl2ylP3Gw7/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD-TR, UHRSD-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1xEh7fsgWGaS5c3IffMswasv0_u-aVM9E/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;HRSOD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DUTS-TR, HRSOD-TR, UHRSD-TR&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/13FaxyyOwyCddfZn2vZo1xG1KNZ3cZ-6B/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Models trained with customed data (general, matting), for &lt;b&gt;general use in practical application&lt;/b&gt;:&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Training Sets&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Backbone&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Test Set&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Metric (S, wF[, HCE])&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;general use (2048x2048)&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;AIM-500, DIS-TR, DIS-TEs, HIM2K, PPM-100, TE-HRS10K, TE-Human-2k, TE-P3M-500-P, TR-AM-2k, TR-HRSOD, TR-UHRSD, Distinctions-646_BG-20k, Human-2k_BG-20k, TE-AM-2k, TE-HRSOD, TE-P3M-500-NP, TE-UHRSD, TR-HRS10K, TR-P3M-10k, &lt;a href=&#34;https://huggingface.co/datasets/schirrmacher/humans&#34;&gt;TR-humans&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS-VD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.927, 0.894, 881&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1lWVyimvxQ3bmDlaHeO1IOUbFZqUj9DYg/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;general use&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS5K-TR, DIS-TEs, DUTS-TR_TE, HRSOD-TR_TE, UHRSD-TR_TE, HRS10K-TR_TE, TR-P3M-10k, TE-P3M-500-NP, TE-P3M-500-P, &lt;a href=&#34;https://huggingface.co/datasets/schirrmacher/humans&#34;&gt;TR-humans&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS-VD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.911, 0.875, 1069&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1_IfUnu8Fpfn-nerB89FzdNXQ7zk6FKxc/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;general use&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS5K-TR, DIS-TEs, DUTS-TR_TE, HRSOD-TR_TE, UHRSD-TR_TE, HRS10K-TR_TE, TR-P3M-10k, TE-P3M-500-NP, TE-P3M-500-P, &lt;a href=&#34;https://huggingface.co/datasets/schirrmacher/humans&#34;&gt;TR-humans&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_tiny&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS-VD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.882, 0.830, 1175&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1fzInDWiE2n65tmjaHDSZpqhL0VME6-Yl/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;general use&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS5K-TR, DIS-TEs&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;DIS-VD&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.907, 0.865, 1059&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1P6NJzG3Jf1sl7js2q1CPC3yqvBn_O8UJ/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;general matting&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;P3M-10k (except TE-P3M-500-NP), &lt;a href=&#34;https://huggingface.co/datasets/schirrmacher/humans&#34;&gt;TR-humans&lt;/a&gt;, AM-2k, AIM-500, Human-2k (synthesized with BG-20k), Distinctions-646 (synthesized with BG-20k), HIM2K, PPM-100&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;TE-P3M-500-NP&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.979, 0.988&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1Nlcg58d5bvE-Tbbm8su_eMQba10hdcwQ/view&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;portrait matting&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/JizhiziLi/P3M&#34;&gt;P3M-10k&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/schirrmacher/humans&#34;&gt;humans&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;swin_v1_large&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;P3M-500-P&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.983, 0.989&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1uUeXjEUoD2XF_6YjD_fsct-TJp7TFiqh&#34;&gt;google-drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Segmentation with box &lt;b&gt;guidance&lt;/b&gt;:&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Given box guidance: &lt;a href=&#34;https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Model &lt;b&gt;efficiency&lt;/b&gt;:&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Screenshot from the original paper. All tests here are conducted on a single A100 GPU.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1mTfSD_qt-rFO1t8DRQcyIa5cgWLf1w2-&amp;amp;sz=h300&#34;&gt; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1F_OURIWILVe4u1rSz-aqt6ur__bAef25&amp;amp;sz=h300&#34;&gt;&lt;/p&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;The devices used in the below table differ from those in the original paper (the standard). So, it&#39;s only for reference.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Runtime&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;em&gt;FP32&lt;/em&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;em&gt;FP16&lt;/em&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;A100&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;86.8ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;69.4ms&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4090&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;95.8ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;57.7ms&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;V100&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;384ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;152ms&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;GPU Memory&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;em&gt;FP32&lt;/em&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;em&gt;FP16&lt;/em&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Inference&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4.76GB&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3.45GB&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Training (#GPU=1, batch_size=2, compile=False+PyTorch=2.5.1)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;36.3GB&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;30.4GB&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Training (#GPU=1, batch_size=2, compile=True+PyTorch=2.5.1)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;35.9GB&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;24.9GB&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;ONNX&lt;/b&gt; conversion:&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;We converted from &lt;code&gt;.pth&lt;/code&gt; weights files to &lt;code&gt;.onnx&lt;/code&gt; files.&lt;br&gt; We referred a lot to the &lt;a href=&#34;https://github.com/Kazuhito00/BiRefNet-ONNX-Sample&#34;&gt;Kazuhito00/BiRefNet-ONNX-Sample&lt;/a&gt;, many thanks to @Kazuhito00.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Check our &lt;a href=&#34;https://colab.research.google.com/drive/1z6OruR52LOvDDpnp516F-N4EyPGrp5om&#34;&gt;Colab demo for ONNX conversion&lt;/a&gt; or the &lt;a href=&#34;https://drive.google.com/file/d/1cgL2qyvOO5q3ySfhytypX46swdQwZLrJ&#34;&gt;notebook file for local running&lt;/a&gt;, where you can do the conversion/inference by yourself and find all relevant info.&lt;/li&gt; &#xA;  &lt;li&gt;As tested, BiRefNets with SwinL (default backbone) cost &lt;code&gt;~90%&lt;/code&gt; more time (the inference costs &lt;code&gt;~165ms&lt;/code&gt; on an A100 GPU) using ONNX files. Meanwhile, BiRefNets with SwinT (lightweight) cost &lt;code&gt;~75%&lt;/code&gt; more time (the inference costs &lt;code&gt;~93.8ms&lt;/code&gt; on an A100 GPU) using ONNX files. Input resolution is &lt;code&gt;1024x1024&lt;/code&gt; as default.&lt;/li&gt; &#xA;  &lt;li&gt;The results of the original pth files and the converted onnx files are slightly different, which is acceptable.&lt;/li&gt; &#xA;  &lt;li&gt;Pay attention to the compatibility among &lt;code&gt;onnxruntime-gpu, CUDA, and CUDNN&lt;/code&gt; (we use &lt;code&gt;torch==2.0.1, cuda=11.8&lt;/code&gt; here).&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Third-Party Creations&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We found there&#39;ve been some 3rd party applications based on our BiRefNet. Many thanks for their contribution to the community!&lt;br&gt; Choose the one you like to try with clicks instead of codes:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/tin2tin/2D_Asset_Generator&#34;&gt;&lt;strong&gt;tin2tin/2D_Asset_Generator&lt;/strong&gt;&lt;/a&gt;: this project combined BiRefNet and FLUX as a &lt;strong&gt;Blender add-on&lt;/strong&gt; for &#34;AI generating 2D cutout assets for ex. previz&#34;.&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/6cce7ca7-7817-4406-b6c4-6d4e8c414ed4&#34;&gt;https://github.com/user-attachments/assets/6cce7ca7-7817-4406-b6c4-6d4e8c414ed4&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/camenduru/text-behind-tost&#34;&gt;&lt;strong&gt;camenduru/text-behind-tost&lt;/strong&gt;&lt;/a&gt;: this project employed BiRefNet to extract foreground subjects and &lt;strong&gt;add texts between the subjects and background&lt;/strong&gt;, which looks amazing especially for videos. Check their &lt;a href=&#34;https://x.com/camenduru/status/1856290408294220010&#34;&gt;tweets&lt;/a&gt; for more examples.&lt;/p&gt; &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/9969dd10-38a8-4cf2-a6c7-5b11f074b9b4&#34; height=&#34;300&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://huggingface.co/briaai/RMBG-2.0&#34;&gt;&lt;strong&gt;briaai/RMBG-2.0&lt;/strong&gt;&lt;/a&gt;: this project trained BiRefNet with their &lt;strong&gt;high-quality private data&lt;/strong&gt;, which brings improvement on the DIS task. Note that their weights are for only &lt;strong&gt;non-commercial use&lt;/strong&gt; and are &lt;strong&gt;not aware of transparency&lt;/strong&gt; due to training in the DIS task setting, which focuses only on predicting binary masks.&lt;/p&gt; &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://huggingface.co/briaai/RMBG-2.0/resolve/main/t4.png&#34; height=&#34;300&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/lldacing/ComfyUI_BiRefNet_ll&#34;&gt;&lt;strong&gt;lldacing/ComfyUI_BiRefNet_ll&lt;/strong&gt;&lt;/a&gt;: this project further upgrade the &lt;strong&gt;ComfyUI node&lt;/strong&gt; for BiRefNet with both our &lt;strong&gt;latest weights&lt;/strong&gt; and &lt;strong&gt;the legacy ones&lt;/strong&gt;.&lt;/p&gt; &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/lldacing/ComfyUI_BiRefNet_ll/raw/main/doc/video.gif&#34; height=&#34;300&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/MoonHugo/ComfyUI-BiRefNet-Hugo&#34;&gt;&lt;strong&gt;MoonHugo/ComfyUI-BiRefNet-Hugo&lt;/strong&gt;&lt;/a&gt;: this project further upgrade the &lt;strong&gt;ComfyUI node&lt;/strong&gt; for BiRefNet with our &lt;strong&gt;latest weights&lt;/strong&gt;.&lt;/p&gt; &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/MoonHugo/ComfyUI-BiRefNet-Hugo/raw/main/assets/demo4.gif&#34; height=&#34;300&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/lbq779660843/BiRefNet-Tensorrt&#34;&gt;&lt;strong&gt;lbq779660843/BiRefNet-Tensorrt&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/yuanyang1991/birefnet_tensorrt&#34;&gt;&lt;strong&gt;yuanyang1991/birefnet_tensorrt&lt;/strong&gt;&lt;/a&gt;: they both provided the project to convert BiRefNet to &lt;strong&gt;TensorRT&lt;/strong&gt;, which is faster and better for deployment. Their repos offer solid local establishment (Win and Linux) and &lt;a href=&#34;https://colab.research.google.com/drive/1r8GkFPyMMO0OkMX6ih5FjZnUCQrl2SHV?usp=sharing&#34;&gt;colab demo&lt;/a&gt;, respectively. And @yuanyang1991 kindly offered the comparison among the inference efficiency of naive PyTorch, ONNX, and TensorRT on an RTX 4080S:&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Methods&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1_IfUnu8Fpfn-nerB89FzdNXQ7zk6FKxc/view&#34;&gt;Pytorch&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/u/0/folders/1kZM55bwsRdS__bdnsXpkmH6QPyza-9-N&#34;&gt;ONNX&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TensorRT&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;First Inference Time&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.71s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.32s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.17s&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Methods&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1_IfUnu8Fpfn-nerB89FzdNXQ7zk6FKxc/view&#34;&gt;Pytorch&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/u/0/folders/1kZM55bwsRdS__bdnsXpkmH6QPyza-9-N&#34;&gt;ONNX&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TensorRT&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Avg Inf Time (excluding 1st)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.15s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.43s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.11s&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/dimitribarbot/sd-webui-birefnet&#34;&gt;&lt;strong&gt;dimitribarbot/sd-webui-birefnet&lt;/strong&gt;&lt;/a&gt;: this project allows to add a BiRefNet section to the original &lt;strong&gt;Stable Diffusion WebUI&lt;/strong&gt;&#39;s Extras tab.&lt;/p&gt; &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=159bLXI71FWh4ZsHTvc-wApSN9ytVRmua&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://fal.ai/models/birefnet&#34;&gt;&lt;strong&gt;fal.ai/birefnet&lt;/strong&gt;&lt;/a&gt;: this project on &lt;code&gt;fal.ai&lt;/code&gt; encapsulates BiRefNet &lt;strong&gt;online&lt;/strong&gt; with more useful options in &lt;strong&gt;UI&lt;/strong&gt; and &lt;strong&gt;API&lt;/strong&gt; to call the model.&lt;/p&gt; &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1rNk81YV_Pzb2GykrzfGvX6T7KBXR0wrA&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/ZHO-ZHO-ZHO/ComfyUI-BiRefNet-ZHO&#34;&gt;&lt;strong&gt;ZHO-ZHO-ZHO/ComfyUI-BiRefNet-ZHO&lt;/strong&gt;&lt;/a&gt;: this project further improves the &lt;strong&gt;UI&lt;/strong&gt; for BiRefNet in ComfyUI, especially for &lt;strong&gt;video data&lt;/strong&gt;.&lt;/p&gt; &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1GOqEreyS7ENzTPN0RqxEjaA76RpMlkYM&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet/assets/25921713/3a1c7ab2-9847-4dac-8935-43a2d3cd2671&#34;&gt;https://github.com/ZhengPeng7/BiRefNet/assets/25921713/3a1c7ab2-9847-4dac-8935-43a2d3cd2671&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/viperyl/ComfyUI-BiRefNet&#34;&gt;&lt;strong&gt;viperyl/ComfyUI-BiRefNet&lt;/strong&gt;&lt;/a&gt;: this project packs BiRefNet as &lt;strong&gt;ComfyUI nodes&lt;/strong&gt;, and makes this SOTA model easier use for everyone.&lt;/p&gt; &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1KfxCQUUa2y9T-aysEaeVVjCUt3Z0zSkL&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/rishabh063&#34;&gt;&lt;strong&gt;Rishabh&lt;/strong&gt;&lt;/a&gt; for offering a demo for the &lt;a href=&#34;https://colab.research.google.com/drive/14Dqg7oeBkFEtchaHLNpig2BcdkZEogba&#34;&gt;easier multiple images inference on colab&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;More Visual Comparisons&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://twitter.com/ZHOZHO672070&#34;&gt;&lt;strong&gt;twitter.com/ZHOZHO672070&lt;/strong&gt;&lt;/a&gt; for the comparison with more background-removal methods in images:&lt;/p&gt; &lt;img src=&#34;https://drive.google.com/thumbnail?id=1nvVIFt_Ezs-crPSQxUDqkUBz598fTe63&amp;amp;sz=w1620&#34;&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://twitter.com/toyxyz3&#34;&gt;&lt;strong&gt;twitter.com/toyxyz3&lt;/strong&gt;&lt;/a&gt; for the comparison with more background-removal methods in videos:&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet/assets/25921713/40136198-01cc-4106-81f9-81c985f02e31&#34;&gt;https://github.com/ZhengPeng7/BiRefNet/assets/25921713/40136198-01cc-4106-81f9-81c985f02e31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet/assets/25921713/1a32860c-0893-49dd-b557-c2e35a83c160&#34;&gt;https://github.com/ZhengPeng7/BiRefNet/assets/25921713/1a32860c-0893-49dd-b557-c2e35a83c160&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h4&gt;Environment Setup&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# PyTorch==2.5.1+CUDA12.4 (or 2.0.1+CUDA11.8) is used for faster training (~40%) with compilation.&#xA;conda create -n birefnet python=3.10 -y &amp;amp;&amp;amp; conda activate birefnet&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Dataset Preparation&lt;/h4&gt; &#xA;&lt;p&gt;Download combined training / test sets I have organized well from: &lt;a href=&#34;https://drive.google.com/drive/folders/1hZW6tAGPJwo9mPS7qGGGdpxuvuXiyoMJ&#34;&gt;DIS&lt;/a&gt;--&lt;a href=&#34;https://drive.google.com/drive/folders/1EyHmKWsXfaCR9O0BiZEc3roZbRcs4ECO&#34;&gt;COD&lt;/a&gt;--&lt;a href=&#34;https://drive.google.com/drive/folders/18_hAE3QM4cwAzEAKXuSNtKjmgFXTQXZN&#34;&gt;HRSOD&lt;/a&gt; or the single official ones in the &lt;code&gt;single_ones&lt;/code&gt; folder, or their official pages. You can also find the same ones on my &lt;strong&gt;BaiduDisk&lt;/strong&gt;: &lt;a href=&#34;https://pan.baidu.com/s/1O_pQIGAE4DKqL93xOxHpxw?pwd=PSWD&#34;&gt;DIS&lt;/a&gt;--&lt;a href=&#34;https://pan.baidu.com/s/1RnxAzaHSTGBC1N6r_RfeqQ?pwd=PSWD&#34;&gt;COD&lt;/a&gt;--&lt;a href=&#34;https://pan.baidu.com/s/1_Del53_0lBuG0DKJJAk4UA?pwd=PSWD&#34;&gt;HRSOD&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Weights Preparation&lt;/h4&gt; &#xA;&lt;p&gt;Download backbone weights from &lt;a href=&#34;https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM&#34;&gt;my google-drive folder&lt;/a&gt; or their official pages.&lt;/p&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Train &amp;amp; Test &amp;amp; Evaluation&#xA;./train_test.sh RUN_NAME GPU_NUMBERS_FOR_TRAINING GPU_NUMBERS_FOR_TEST&#xA;# Example: ./train_test.sh tmp-proj 0,1,2,3,4,5,6,7 0&#xA;&#xA;# See train.sh / test.sh for only training / test-evaluation.&#xA;# After the evaluation, run `gen_best_ep.py` to select the best ckpt from a specific metric (you choose it from Sm, wFm, HCE (DIS only)).&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;span&gt;ðŸ–Š&lt;/span&gt; Fine-tuning on Custom Data&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;Guideline&lt;/b&gt;:&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Suppose you have some custom data, fine-tuning on it tends to bring improvement.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Pre-requisites&lt;/strong&gt;: you have put your datasets in the path &lt;code&gt;${data_root_dir}/TASK_NAME/DATASET_NAME&lt;/code&gt;. For example, &lt;code&gt;${data_root_dir}/DIS5K/DIS-TR&lt;/code&gt; and &lt;code&gt;${data_root_dir}/General/TR-HRSOD&lt;/code&gt;, where &lt;code&gt;im&lt;/code&gt; and &lt;code&gt;gt&lt;/code&gt; are both in each dataset folder.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Change an existing task to your custom one&lt;/strong&gt;: replace all &lt;code&gt;&#39;General&#39;&lt;/code&gt; (with single quotes) in the whole project with &lt;code&gt;your custom task name&lt;/code&gt; as the screenshot of vscode given below shows:&lt;img src=&#34;https://drive.google.com/thumbnail?id=1J6gzTmrVnQsmtt3hi6ch3ZrH7Op9PKSB&amp;amp;sz=w400&#34;&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Adapt settings&lt;/strong&gt;: &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;code&gt;sys_home_dir&lt;/code&gt;: path to the root folder, which contains codes / datasets / weights / ... -- project folder / data folder / backbone weights folder are &lt;code&gt;${sys_home_dir}/codes/dis/BiRefNet / ${sys_home_dir}/datasets/dis/General / ${sys_home_dir}/weights/cv/swin_xxx&lt;/code&gt;, respectively.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;code&gt;testsets&lt;/code&gt;: your validation set.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;code&gt;training_set&lt;/code&gt;: your training set.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;code&gt;lambdas_pix_last&lt;/code&gt;: adapt the weights of different losses if you want, especially for the difference between segmentation (classification task) and matting (regression task).&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Use existing weights&lt;/strong&gt;: if you want to use some existing weights to fine-tune that model, please refer to the &lt;code&gt;resume&lt;/code&gt; argument in &lt;code&gt;train.py&lt;/code&gt;. Attention: the epoch of training continues from the epochs the weights file name indicates (e.g., &lt;code&gt;244&lt;/code&gt; in &lt;code&gt;BiRefNet-general-epoch_244.pth&lt;/code&gt;), instead of &lt;code&gt;1&lt;/code&gt;. So, if you want to fine-tune &lt;code&gt;50&lt;/code&gt; more epochs, please specify the epochs as &lt;code&gt;294&lt;/code&gt;. &lt;code&gt;\#Epochs, \#last epochs for validation, and validation step&lt;/code&gt; are set in &lt;code&gt;train.sh&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Good luck to your training :) If you still have questions, feel free to leave issues (recommended way) or contact me.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Well-trained weights:&lt;/h2&gt; &#xA;&lt;p&gt;Download the &lt;code&gt;BiRefNet-{TASK}-{EPOCH}.pth&lt;/code&gt; from [&lt;a href=&#34;https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM&#34;&gt;&lt;strong&gt;stuff&lt;/strong&gt;&lt;/a&gt;] and &lt;a href=&#34;https://github.com/ZhengPeng7/BiRefNet/releases&#34;&gt;the release page&lt;/a&gt; of this repo. Info of the corresponding (predicted_maps/performance/training_log) weights can be also found in folders like &lt;code&gt;exp-BiRefNet-{TASK_SETTINGS}&lt;/code&gt; in the same directory.&lt;/p&gt; &#xA;&lt;p&gt;You can also download the weights from the release of this repo.&lt;/p&gt; &#xA;&lt;p&gt;The results might be a bit different from those in the original paper, you can see them in the &lt;code&gt;eval_results-BiRefNet-{TASK_SETTINGS}&lt;/code&gt; folder in each &lt;code&gt;exp-xx&lt;/code&gt;, we will update them in the following days. Due to the very high cost I used (A100-80G x 8), which many people cannot afford (including myself....), I re-trained BiRefNet on a single A100-40G only and achieved the performance on the same level (even better). It means you can directly train the model on a single GPU with 36.5G+ memory. BTW, 5.5G GPU memory is needed for inference in 1024x1024. (I personally paid a lot for renting an A100-40G to re-train BiRefNet on the three tasks... T_T. Hope it can help you.)&lt;/p&gt; &#xA;&lt;p&gt;But if you have more and more powerful GPUs, you can set GPU IDs and increase the batch size in &lt;code&gt;config.py&lt;/code&gt; to accelerate the training. We have made all these kinds of things adaptive in scripts to seamlessly switch between single-card training and multi-card training. Enjoy it :)&lt;/p&gt; &#xA;&lt;h2&gt;Some of my messages:&lt;/h2&gt; &#xA;&lt;p&gt;This project was originally built for DIS only. But after the updates one by one, I made it larger and larger with many functions embedded together. Finally, you can &lt;strong&gt;use it for any binary image segmentation tasks&lt;/strong&gt;, such as DIS/COD/SOD, medical image segmentation, anomaly segmentation, etc. You can eaily open/close below things (usually in &lt;code&gt;config.py&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-GPU training: open/close with one variable.&lt;/li&gt; &#xA; &lt;li&gt;Backbone choices: Swin_v1, PVT_v2, ConvNets, ...&lt;/li&gt; &#xA; &lt;li&gt;Weighted losses: BCE, IoU, SSIM, MAE, Reg, ...&lt;/li&gt; &#xA; &lt;li&gt;Training tricks: multi-scale supervision, freezing backbone, multi-scale input...&lt;/li&gt; &#xA; &lt;li&gt;Data collator: loading all in memory, smooth combination of different datasets for combined training and test.&lt;/li&gt; &#xA; &lt;li&gt;... I really hope you enjoy this project and use it in more works to achieve new SOTAs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Quantitative Results&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1Ymkh8WN16XMTBOS8dmPTg5eAf-NIl2m5&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1W0mi0ZiYbqsaGuohNXU8Gh7Zj4M3neFg&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Qualitative Results&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1TYZF8pVZc2V0V6g3ik4iAr9iKvJ8BNrf&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://drive.google.com/thumbnail?id=1ZGHC32CAdT9cwRloPzOCKWCrVQZvUAlJ&amp;amp;sz=w1620&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement:&lt;/h2&gt; &#xA;&lt;p&gt;Many of my thanks to the companies / institutes below.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fal.ai&#34;&gt;FAL&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.freepik.com&#34;&gt;Freepik&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://redmond.ai&#34;&gt;Redmond.ai&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.alibaba.com&#34;&gt;Alibaba-ICBU&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zheng2024birefnet,&#xA;  title={Bilateral Reference for High-Resolution Dichotomous Image Segmentation},&#xA;  author={Zheng, Peng and Gao, Dehong and Fan, Deng-Ping and Liu, Li and Laaksonen, Jorma and Ouyang, Wanli and Sebe, Nicu},&#xA;  journal={CAAI Artificial Intelligence Research},&#xA;  volume = {3},&#xA;  pages = {9150038},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Any questions, discussions, or even complaints, feel free to leave issues here (recommended) or send me e-mails (&lt;a href=&#34;mailto:zhengpeng0108@gmail.com&#34;&gt;zhengpeng0108@gmail.com&lt;/a&gt;) or book a meeting with me: &lt;a href=&#34;https://calendly.com/zhengpeng0108/30min&#34;&gt;calendly.com/zhengpeng0108/30min&lt;/a&gt;. You can also join the Discord Group (&lt;a href=&#34;https://discord.gg/d9NN5sgFrq&#34;&gt;https://discord.gg/d9NN5sgFrq&lt;/a&gt;) if you want to talk a lot publicly.&lt;/p&gt;</summary>
  </entry>
</feed>