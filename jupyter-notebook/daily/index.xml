<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-12T01:46:55Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>probml/pyprobml</title>
    <updated>2022-07-12T01:46:55Z</updated>
    <id>tag:github.com,2022-07-12:/probml/pyprobml</id>
    <link href="https://github.com/probml/pyprobml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python code for &#34;Probabilistic Machine learning&#34; book by Kevin Murphy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pyprobml&lt;/h1&gt; &#xA;&lt;img src=&#34;https://img.shields.io/github/stars/probml/pyprobml?style=social&#34;&gt; &#xA;&lt;p&gt;Python 3 code to reproduce the figures in the book series &lt;a href=&#34;https://probml.github.io/pml-book/&#34;&gt;Probabilistic Machine Learning&lt;/a&gt; by Kevin Patrick Murphy. This is work in progress, so expect rough edges. (For the latest status of the code, see &lt;a href=&#34;https://github.com/probml/pyprobml/raw/workflow_testing_indicator/dashboard_figures_book1.md&#34;&gt;Book 1 dashboard&lt;/a&gt; and &lt;a href=&#34;https://github.com/probml/pyprobml/raw/workflow_testing_indicator/dashboard_figures_book2.md&#34;&gt;Book 2 dashboard&lt;/a&gt;.)&lt;/p&gt; &#xA;&lt;p&gt;See also &lt;a href=&#34;https://github.com/probml/probml-utils&#34;&gt;probml-utils&lt;/a&gt; for some utility code.&lt;/p&gt; &#xA;&lt;h2&gt;Running the notebooks&lt;/h2&gt; &#xA;&lt;p&gt;The notebooks needed to make all the figures are available at the following locations.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/probml/pyprobml/raw/master/notebooks.md&#34;&gt;All notebooks (sorted by filename)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/probml/pyprobml/tree/master/notebooks/book1&#34;&gt;Book 1 notebooks (sorted by chaper)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/probml/pyprobml/tree/master/notebooks/book2&#34;&gt;Book 2 notebooks (sorted by chapter)&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Running notebooks in colab&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/notebooks/intro.ipynb&#34;&gt;Colab&lt;/a&gt; has most of the libraries you will need (e.g., scikit-learn, JAX) pre-installed, and gives you access to a free GPU and TPU. We have a created a &lt;a href=&#34;https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/misc/colab_intro.ipynb&#34;&gt;colab intro&lt;/a&gt; notebook with more details. To run the notebooks on colab in any browser, you can go to a particular notebook on GitHub and change the domain from &lt;code&gt;github.com&lt;/code&gt; to &lt;code&gt;githubtocolab.com&lt;/code&gt; as suggested &lt;a href=&#34;https://stackoverflow.com/a/67344477/13330701&#34;&gt;here&lt;/a&gt;. If you are using Google Chrome browser, you can use &lt;a href=&#34;https://chrome.google.com/webstore/detail/open-in-colab/iogfkhleblhcpcekbiedikdehleodpjo&#34;&gt;&#34;Open in Colab&#34; Chrome extension&lt;/a&gt; to do the same with a single click.&lt;/p&gt; &#xA;&lt;h2&gt;Running the notebooks locally&lt;/h2&gt; &#xA;&lt;p&gt;We assume you have already installed &lt;a href=&#34;https://github.com/google/jax#installation&#34;&gt;JAX&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/install&#34;&gt;Tensorflow&lt;/a&gt; and &lt;a href=&#34;https://pytorch.org/&#34;&gt;Torch&lt;/a&gt;, since the details on how to do this depend on whether you have a CPU, GPU, etc.&lt;/p&gt; &#xA;&lt;p&gt;You can use any of the following options to install the other requirements.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Option 1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r https://raw.githubusercontent.com/probml/pyprobml/master/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Option 2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://github.com/probml/pyprobml/raw/master/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; locally to your path and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;GCP, TPUs, and all that&lt;/h2&gt; &#xA;&lt;p&gt;When you want more power or control than colab gives you, you should get a Google Cloud Platform (GCP) account (or you can use some other cloud provider, like Paperspace) to get a virtual machine with GPUs or TPUs. You can then use this as a virtual desktop which you can access via ssh from inside VScode. We have created &lt;a href=&#34;https://github.com/probml/pyprobml/raw/master/notebooks/tutorials/colab_gcp_tpu_tutorial.md&#34;&gt;a short tutorial on Colab, GCP and TPUs&lt;/a&gt; with more information.&lt;/p&gt; &#xA;&lt;h2&gt;How to contribute&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/probml/pyprobml/raw/master/CONTRIBUTING.md&#34;&gt;this guide&lt;/a&gt; for how to contribute code. Please follow &lt;a href=&#34;https://github.com/probml/pyprobml/raw/master/notebooks/README.md&#34;&gt;these guidelines&lt;/a&gt; to contribute new notebooks to the notebooks directory.&lt;/p&gt; &#xA;&lt;h2&gt;Metrics&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://starchart.cc/probml/pyprobml&#34;&gt;&lt;img src=&#34;https://starchart.cc/probml/pyprobml.svg?sanitize=true&#34; alt=&#34;Stargazers over time&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;GSOC&lt;/h2&gt; &#xA;&lt;p&gt;For a summary of some of the contributions to this codebase during Google Summer of Code (GSOC), see these links: &lt;a href=&#34;https://probml.github.io/pml-book/gsoc/gsoc2021.html&#34;&gt;2021&lt;/a&gt; and &lt;a href=&#34;https://probml.github.io/pml-book/gsoc/gsoc2022.html&#34;&gt;2022&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a id=&#34;acknowledgements&#34;&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;I would like to thank the following people for contributing to the code (list autogenerated from &lt;a href=&#34;https://raw.githubusercontent.com/probml/pyprobml/master/internal/contributors/contributors.py&#34;&gt;this script&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/Abdelrahman350.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/alenm10.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/always-newbie161.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/AnandShegde.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/andrewnc.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/animesh-007.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/AnkitaKumariJain14.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/ashishpapanai.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/Drishttii.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/Duane321.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/firatoncel.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/Garvit9000c.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/gerdm.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/jdf22.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/karalleyna.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/karm-patel.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/khanshehjad.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/kzymgch.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/mjsML.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/murphyk.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/nalzok.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/nappaillav.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/Neoanarika.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/Nirzu97.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/nitish1295.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/nouranali.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/patel-zeel.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/patrickmineault.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/raymondyeh07.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/rohit-khoiwal-30.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/shivaditya-meduri.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/shobro.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/susnato.png&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img width=&#34;50&#34; alt=&#34;image&#34; src=&#34;https://github.com/thvasilo.png&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Abdelrahman350&#34;&gt;Abdelrahman350&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/alenm10&#34;&gt;alenm10&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/always-newbie161&#34;&gt;always-newbie161&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/AnandShegde&#34;&gt;AnandShegde&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/andrewnc&#34;&gt;andrewnc&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/animesh-007&#34;&gt;animesh-007&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/AnkitaKumariJain14&#34;&gt;AnkitaKumariJain14&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ashishpapanai&#34;&gt;ashishpapanai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Drishttii&#34;&gt;Drishttii&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Duane321&#34;&gt;Duane321&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/firatoncel&#34;&gt;firatoncel&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Garvit9000c&#34;&gt;Garvit9000c&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/gerdm&#34;&gt;gerdm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jdf22&#34;&gt;jdf22&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/karalleyna&#34;&gt;karalleyna&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/karm-patel&#34;&gt;karm-patel&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/khanshehjad&#34;&gt;khanshehjad&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/kzymgch&#34;&gt;kzymgch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/mjsML&#34;&gt;mjsML&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/murphyk&#34;&gt;murphyk&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/nalzok&#34;&gt;nalzok&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/nappaillav&#34;&gt;nappaillav&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Neoanarika&#34;&gt;Neoanarika&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Nirzu97&#34;&gt;Nirzu97&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/nitish1295&#34;&gt;nitish1295&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/nouranali&#34;&gt;nouranali&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/patel-zeel&#34;&gt;patel-zeel&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/patrickmineault&#34;&gt;patrickmineault&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/raymondyeh07&#34;&gt;raymondyeh07&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/rohit-khoiwal-30&#34;&gt;rohit-khoiwal-30&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/shivaditya-meduri&#34;&gt;shivaditya-meduri&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/shobro&#34;&gt;shobro&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/susnato&#34;&gt;susnato&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/thvasilo&#34;&gt;thvasilo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>Deci-AI/super-gradients</title>
    <updated>2022-07-12T01:46:55Z</updated>
    <id>tag:github.com,2022-07-12:/Deci-AI/super-gradients</id>
    <link href="https://github.com/Deci-AI/super-gradients" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Easily train or fine-tune SOTA computer vision models with one open source training library&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;docs/assets/SG_img/SG - Horizontal.png&#34; width=&#34;600&#34;&gt; &#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;strong&gt;Easily train or fine-tune SOTA computer vision models with one open source training library&lt;/strong&gt; &lt;a href=&#34;https://twitter.com/intent/tweet?text=Easily%20train%20or%20fine-tune%20SOTA%20computer%20vision%20models%20from%20one%20training%20repository&amp;amp;url=https://github.com/Deci-AI/super-gradients&amp;amp;via=deci_ai&amp;amp;hashtags=AI,deeplearning,computervision,training,opensource&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/http/shields.io.svg?style=social&#34; alt=&#34;Tweet&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h4&gt;Fill our 4-question quick survey! We will raffle free SuperGradients swag between those who will participate -&amp;gt; &lt;a href=&#34;https://hz8qtlvwkaw.typeform.com/to/OpKda0Qe&#34;&gt;Fill Survey&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;hr&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.supergradients.com/&#34;&gt;Website&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#why-use-supergradients&#34;&gt;Why Use SG?&lt;/a&gt; • &lt;a href=&#34;https://deci-ai.github.io/super-gradients/user_guide.html#introducing-the-supergradients-library&#34;&gt;User Guide&lt;/a&gt; • &lt;a href=&#34;https://deci-ai.github.io/super-gradients/super_gradients.common.html&#34;&gt;Docs&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#getting-started&#34;&gt;Getting Started Notebooks&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#transfer-learning&#34;&gt;Transfer Learning&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#computer-vision-models---pretrained-checkpoints&#34;&gt;Pretrained Models&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#community&#34;&gt;Community&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#license&#34;&gt;License&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#deci-platform&#34;&gt;Deci Platform&lt;/a&gt; &lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Deci-AI/super-gradients#prerequisites&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9-blue&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients#prerequisites&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pytorch-1.9%20%7C%201.10-blue&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://pypi.org/project/super-gradients/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/super-gradients&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients#computer-vision-models-pretrained-checkpoints&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pre--trained%20models-25-brightgreen&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/Deci-AI/super-gradients&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://join.slack.com/t/supergradients-comm52/shared_invite/zt-10vz6o1ia-b_0W5jEPEnuHXm087K~t8Q&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-community-blueviolet&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202.0-blue&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://deci-ai.github.io/super-gradients/welcome.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-sphinx-brightgreen&#34;&gt; &lt;/a&gt;&lt;/p&gt;&#xA; &lt;a href=&#34;https://deci-ai.github.io/super-gradients/welcome.html&#34;&gt; &lt;/a&gt;&#xA;&lt;/div&gt;&#xA;&lt;a href=&#34;https://deci-ai.github.io/super-gradients/welcome.html&#34;&gt; &lt;h1&gt;SuperGradients&lt;/h1&gt; &lt;h2&gt;Introduction&lt;/h2&gt; &lt;p&gt;Welcome to SuperGradients, a free, open-source training library for PyTorch-based deep learning models. SuperGradients allows you to train or fine-tune SOTA pre-trained models for all the most commonly applied computer vision tasks with just one training library. We currently support object detection, image classification and semantic segmentation for videos and images.&lt;/p&gt; &lt;/a&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://deci-ai.github.io/super-gradients/welcome.html&#34;&gt;Docs and full user guide&lt;/a&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Why use SuperGradients?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Built-in SOTA Models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Easily load and fine-tune production-ready, &lt;a href=&#34;https://github.com/Deci-AI/super-gradients#pretrained-classification-pytorch-checkpoints&#34;&gt;pre-trained SOTA models&lt;/a&gt; that incorporate best practices and validated hyper-parameters for achieving best-in-class accuracy.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Easily Reproduce our Results&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Why do all the grind work, if we already did it for you? leverage tested and proven &lt;a href=&#34;https://github.com/Deci-AI/super-gradients/tree/master/src/super_gradients/recipes&#34;&gt;recipes&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://github.com/Deci-AI/super-gradients/tree/master/src/super_gradients/examples&#34;&gt;code examples&lt;/a&gt; for a wide range of computer vision models generated by our team of deep learning experts. Easily configure your own or use plug &amp;amp; play hyperparameters for training, dataset, and architecture.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Production Readiness and Ease of Integration&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;All SuperGradients models’ are production ready in the sense that they are compatible with deployment tools such as TensorRT (Nvidia) and OpenVINO (Intel) and can be easily taken into production. With a few lines of code you can easily integrate the models into your codebase.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/detection-demo.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;What&#39;s New&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;【07/07/2022】SSD Lite MobileNetV2 - Training &lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/recipes/coco_ssd_lite_mobilenet_v2.yaml&#34;&gt;recipes&lt;/a&gt; and pre-trained &lt;a href=&#34;https://github.com/Deci-AI/super-gradients#pretrained-object-detection-pytorch-checkpoints&#34;&gt;checkpoints&lt;/a&gt; on COCO - Tailored for edge devices! 📱&lt;/li&gt; &#xA; &lt;li&gt;【07/07/2022】 STDC - new pre-trained &lt;a href=&#34;https://github.com/Deci-AI/super-gradients#pretrained-semantic-segmentation-pytorch-checkpoints&#34;&gt;checkpoints&lt;/a&gt; and &lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/recipes&#34;&gt;recipes&lt;/a&gt; for Cityscapes with super SOTA mIoU scores 🎯&lt;/li&gt; &#xA; &lt;li&gt;【16/06/2022】 ResNet50 - new pre-trained checkpoint and &lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/recipes/imagenet_resnet50_kd.yaml&#34;&gt;recipe&lt;/a&gt; for ImageNet top-1 score of 81.9 💪&lt;/li&gt; &#xA; &lt;li&gt;【09/06/2022】 ViT models (Vision Transformer) - Training &lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/recipes&#34;&gt;recipes&lt;/a&gt; and pre-trained &lt;a href=&#34;https://github.com/Deci-AI/super-gradients#pretrained-object-detection-pytorch-checkpoints&#34;&gt;checkpoints&lt;/a&gt; (ViT, BEiT).&lt;/li&gt; &#xA; &lt;li&gt;【09/06/2022】 Knowledge Distillation support - &lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/kd_model/kd_model.py&#34;&gt;training module&lt;/a&gt; and &lt;a href=&#34;https://bit.ly/3HQvbsg&#34;&gt;notebook&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;【06/04/2022】 Integration with professional tools - &lt;a href=&#34;https://deci-ai.github.io/super-gradients/user_guide.html?highlight=wandb#professional-tools-integration&#34;&gt;Weights and Biases&lt;/a&gt; and &lt;a href=&#34;https://dagshub.com/Deci-AI&#34;&gt;DagsHub&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;【09/03/2022】 New &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#quick-start-notebook---semantic-segmentation&#34;&gt;quick start&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#transfer-learning-with-sg-notebook---semantic-segmentation&#34;&gt;transfer learning&lt;/a&gt; example notebooks for Semantic Segmentation.&lt;/li&gt; &#xA; &lt;li&gt;【07/02/2022】 We added RegSeg recipes and pre-trained models to our &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#pretrained-semantic-segmentation-pytorch-checkpoints&#34;&gt;Semantic Segmentation models&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;【01/02/2022】 We added issue templates for feature requests and bug reporting.&lt;/li&gt; &#xA; &lt;li&gt;【20/01/2022】 STDC family - new recipes added with even higher mIoU💪&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Check out SG full &lt;a href=&#34;https://github.com/Deci-AI/super-gradients/releases&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Coming soon&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; YOLOX models (object detection) - recipes and pre-trained checkpoints.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Single class detectors (recipes, pre-trained checkpoints) for edge devices deployment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Single class segmentation (recipes, pre-trained checkpoints) for edge devices deployment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; QAT capabilities (Quantization Aware Training).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Dali implementation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Integration with more professional tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improved pre-trained checkpoints and recipes (DDRNet, ResNet, RegSeg, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Table of Content&lt;/h3&gt; &#xA;&lt;!-- toc --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#quick-start-notebook---classification&#34;&gt;Quick Start Notebook - Classification example&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#quick-start-notebook---object-detection&#34;&gt;Quick Start Notebook - Object detection example&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#quick-start-notebook---semantic-segmentation&#34;&gt;Quick Start Notebook - Semantic segmentation example&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#quick-start-notebook---model-upload-to-deci-lab&#34;&gt;Quick Start Notebook - Upload to Deci Lab example&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#supergradients-complete-walkthrough-notebook&#34;&gt;Walkthrough Notebook&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#transfer-learning&#34;&gt;Transfer Learning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#transfer-learning-with-sg-notebook---object-detection&#34;&gt;Transfer Learning with SG Notebook - Object detection example&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#transfer-learning-with-sg-notebook---semantic-segmentation&#34;&gt;Transfer Learning with SG Notebook - Semantic segmentation example&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#knowledge-distillation-training&#34;&gt;Knowledge Distillation Training&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#knowledge-distillation-training-quick-start-with-sg-notebook---resnet18-example&#34;&gt;Knowledge Distillation Training Quick Start with SG Notebook - ResNet18 example&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#installation-methods&#34;&gt;Installation Methods&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#quick-installation&#34;&gt;Quick Installation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#computer-vision-models---pretrained-checkpoints&#34;&gt;Computer Vision Models - Pretrained Checkpoints&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#pretrained-classification-pytorch-checkpoints&#34;&gt;Pretrained Classification PyTorch Checkpoints&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#pretrained-object-detection-pytorch-checkpoints&#34;&gt;Pretrained Object Detection PyTorch Checkpoints&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#pretrained-semantic-segmentation-pytorch-checkpoints&#34;&gt;Pretrained Semantic Segmentation PyTorch Checkpoints&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#implemented-model-architectures&#34;&gt;Implemented Model Architectures&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#deci-platform&#34;&gt;Deci Platform&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- tocstop --&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Start Training with Just 1 Command Line&lt;/h3&gt; &#xA;&lt;p&gt;The most simple and straightforward way to start training SOTA performance models with SuperGradients reproducible recipes. Just define your dataset path and where you want your checkpoints to be saved and you are good to go from your terminal!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m super_gradients.train_from_recipe --config-name=imagenet_regnetY architecture=regnetY800 dataset_interface.data_dir=&amp;lt;YOUR_Imagenet_LOCAL_PATH&amp;gt; ckpt_root_dir=&amp;lt;CHEKPOINT_DIRECTORY&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quickly Load Pre-Trained Weights for Your Desired Model with SOTA Performance&lt;/h3&gt; &#xA;&lt;p&gt;Want to try our pre-trained models on your machine? Import SuperGradients, initialize your SgModel, and load your desired architecture and pre-trained weights from our &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#computer-vision-models---pretrained-checkpoints&#34;&gt;SOTA model zoo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# The pretrained_weights argument will load a pre-trained architecture on the provided dataset&#xA;# This is an example of loading COCO-2017 pre-trained weights for a YOLOv5 Nano object detection model&#xA;    &#xA;import super_gradients&#xA;from super_gradients.training import SgModel&#xA;&#xA;trainer = SgModel(experiment_name=&#34;yolov5n_coco_experiment&#34;,ckpt_root_dir=&amp;lt;CHECKPOINT_DIRECTORY&amp;gt;)&#xA;trainer.build_model(architecture=&#34;yolo_v5n&#34;, arch_params={&#34;pretrained_weights&#34;: &#34;coco&#34;, num_classes&#34;: 80})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quick Start Notebook - Classification&lt;/h3&gt; &#xA;&lt;p&gt;Get started with our quick start notebook for image classification tasks on Google Colab for a quick and easy start using free GPU hardware.&lt;/p&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3ufnsgT&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt;Classification Quick Start in Google Colab&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/examples/SG_quickstart_classification.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/download_logo.png&#34;&gt;Download notebook&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/Deci-AI/super-gradients/tree/master/src/super_gradients/examples&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt;View source on GitHub&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h3&gt;Quick Start Notebook - Object Detection&lt;/h3&gt; &#xA;&lt;p&gt;Get started with our quick start notebook for object detection tasks on Google Colab for a quick and easy start using free GPU hardware.&lt;/p&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3wqMsEM&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt;Detection Quick Start in Google Colab&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/examples/SG_quickstart_detection.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/download_logo.png&#34;&gt;Download notebook&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/Deci-AI/super-gradients/tree/master/src/super_gradients/examples&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt;View source on GitHub&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h3&gt;Quick Start Notebook - Semantic Segmentation&lt;/h3&gt; &#xA;&lt;p&gt;Get started with our quick start notebook for semantic segmentation tasks on Google Colab for a quick and easy start using free GPU hardware.&lt;/p&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3Jp7w1U&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt;Segmentation Quick Start in Google Colab&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/examples/SG_quickstart_segmentation.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/download_logo.png&#34;&gt;Download notebook&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/Deci-AI/super-gradients/tree/master/src/super_gradients/examples&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt;View source on GitHub&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h3&gt;Quick Start Notebook - Model Upload to Deci Lab&lt;/h3&gt; &#xA;&lt;p&gt;Get Started with an example of how to upload to Deci Lab a freshly trained model&lt;/p&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td vertical-align=&#34;middle&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/drive/1cNvakn8ttLhD9g8IMbe51PBvk-vJ40Oi?usp=sharing&amp;amp;utm_campaign=SG%20github%20repo&amp;amp;utm_source=Google%20Colab&amp;amp;utm_medium=GitHub%20Repo&amp;amp;utm_content=Quickstart%20trainig%20with20model%20upload%20notebook%20-%20README.md&#34;&gt; Classification Quick Start in Google Colab &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td vertical-align=&#34;middle&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/download_logo.png&#34;&gt; &lt;a href=&#34;https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/examples/SG_quickstart_model_upload_deci_lab.ipynb&#34;&gt; Download notebook &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/examples/deci_lab_export_example/deci_lab_export_example.py&#34;&gt; View source on GitHub &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;SuperGradients Complete Walkthrough Notebook&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about SuperGradients training components with our walkthrough notebook on Google Colab for an easy to use tutorial using free GPU hardware&lt;/p&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3JspSPF&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt;SuperGradients Walkthrough in Google Colab&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/examples/SG_Walkthrough.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/download_logo.png&#34;&gt;Download notebook&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/Deci-AI/super-gradients/tree/master/src/super_gradients/examples&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt;View source on GitHub&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h2&gt;Transfer Learning&lt;/h2&gt; &#xA;&lt;h3&gt;Transfer Learning with SG Notebook - Object Detection&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about SuperGradients transfer learning or fine tuning abilities with our COCO pre-trained YoloV5nano fine tuning into a sub-dataset of PASCAL VOC example notebook on Google Colab for an easy to use tutorial using free GPU hardware&lt;/p&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3iGvnP7&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt;Detection Transfer Learning in Google Colab&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/examples/SG_transfer_learning_object_detection.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/download_logo.png&#34;&gt;Download notebook&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/Deci-AI/super-gradients/tree/master/src/super_gradients/examples&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt;View source on GitHub&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h3&gt;Transfer Learning with SG Notebook - Semantic Segmentation&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about SuperGradients transfer learning or fine tuning abilities with our Citiscapes pre-trained RegSeg48 fine tuning into a sub-dataset of Supervisely example notebook on Google Colab for an easy to use tutorial using free GPU hardware&lt;/p&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/37P04PN&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt;Segmentation Transfer Learning in Google Colab&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/examples/SG_transfer_learning_semantic_segmentation.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/download_logo.png&#34;&gt;Download notebook&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/Deci-AI/super-gradients/tree/master/src/super_gradients/examples&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt;View source on GitHub&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h2&gt;Knowledge Distillation Training&lt;/h2&gt; &#xA;&lt;h3&gt;Knowledge Distillation Training Quick Start with SG Notebook - ResNet18 example&lt;/h3&gt; &#xA;&lt;p&gt;Knowledge Distillation is a training technique that uses a large model, teacher model, to improve the performance of a smaller model, the student model. Learn more about SuperGradients knowledge distillation training with our pre-trained BEiT base teacher model and Resnet18 student model on CIFAR10 example notebook on Google Colab for an easy to use tutorial using free GPU hardware&lt;/p&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3HQvbsg&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt;KD Training in Google Colab&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/examples/SG_knowledge_distillation_quickstart.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/download_logo.png&#34;&gt;Download notebook&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://github.com/Deci-AI/super-gradients/tree/master/src/super_gradients/examples&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt;View source on GitHub&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h2&gt;Installation Methods&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;General requirements&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Python 3.7, 3.8 or 3.9 installed.&lt;/li&gt; &#xA;  &lt;li&gt;torch&amp;gt;=1.9.0 &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;The python packages that are specified in requirements.txt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;To train on nvidia GPUs&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-11.2.0-download-archive?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;target_distro=Ubuntu&#34;&gt;Nvidia CUDA Toolkit &amp;gt;= 11.2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;CuDNN &amp;gt;= 8.1.x&lt;/li&gt; &#xA;  &lt;li&gt;Nvidia Driver with CUDA &amp;gt;= 11.2 support (≥460.x)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Quick Installation&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Install stable version using PyPi&lt;/summary&gt; &#xA; &lt;p&gt;See in &lt;a href=&#34;https://pypi.org/project/super-gradients/&#34;&gt;PyPi&lt;/a&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install super-gradients&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;That&#39;s it !&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Install using GitHub&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/Deci-AI/super-gradients.git@stable&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Computer Vision Models - Pretrained Checkpoints&lt;/h2&gt; &#xA;&lt;h3&gt;Pretrained Classification PyTorch Checkpoints&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Resolution&lt;/th&gt; &#xA;   &lt;th&gt;Top-1&lt;/th&gt; &#xA;   &lt;th&gt;Top-5&lt;/th&gt; &#xA;   &lt;th&gt;Latency (HW)*&lt;sub&gt;T4&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Latency (Production)**&lt;sub&gt;T4&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Latency (HW)*&lt;sub&gt;Jetson Xavier NX&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Latency (Production)**&lt;sub&gt;Jetson Xavier NX&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Latency &lt;sub&gt;Cascade Lake&lt;/sub&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT base&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet21K&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;84.15&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;4.46ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;4.60ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;-&lt;/strong&gt; *&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;-&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.22ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT large&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet21K&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;85.64&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;12.81ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;13.19ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;-&lt;/strong&gt; *&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;-&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;187.22ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BEiT&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet21K&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;-ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;-ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;-&lt;/strong&gt; *&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;-&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;-ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientNet B0&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;77.62&lt;/td&gt; &#xA;   &lt;td&gt;93.49&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.93ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.38ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;-&lt;/strong&gt; *&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;-&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;3.44ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RegNet Y200&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;70.88&lt;/td&gt; &#xA;   &lt;td&gt;89.35&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.63ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.08ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.16ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.47ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;2.06ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RegNet Y400&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;74.74&lt;/td&gt; &#xA;   &lt;td&gt;91.46&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.80ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.25ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.62ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.91ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;2.87ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RegNet Y600&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;76.18&lt;/td&gt; &#xA;   &lt;td&gt;92.34&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.77ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.22ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.64ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.93ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;2.39ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RegNet Y800&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;77.07&lt;/td&gt; &#xA;   &lt;td&gt;93.26&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.74ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.19ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.77ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.04ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;2.81ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ResNet 18&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;70.6&lt;/td&gt; &#xA;   &lt;td&gt;89.64&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.52ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.95ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.01ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.30ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;4.56ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ResNet 34&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;74.13&lt;/td&gt; &#xA;   &lt;td&gt;91.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.92ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.34ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.57ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.87ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;7.64ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ResNet 50&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;81.91&lt;/td&gt; &#xA;   &lt;td&gt;93.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.03ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.44ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;4.78ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;5.10ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;9.25ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MobileNet V3_large-150 epochs&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;73.79&lt;/td&gt; &#xA;   &lt;td&gt;91.54&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.67ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.11ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.42ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.71ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.76ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MobileNet V3_large-300 epochs&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;74.52&lt;/td&gt; &#xA;   &lt;td&gt;91.92&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.67ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.11ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.42ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.71ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.76ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MobileNet V3_small&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;67.45&lt;/td&gt; &#xA;   &lt;td&gt;87.47&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.55ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.96ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.01ms&lt;/strong&gt; *&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.35ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.06ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MobileNet V2_w1&lt;/td&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;73.08&lt;/td&gt; &#xA;   &lt;td&gt;91.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.46 ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.89ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.65ms&lt;/strong&gt; *&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.90ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.56ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; &lt;br&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Latency (HW)* - Hardware performance (not including IO)&lt;br&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Latency (Production)** - Production Performance (including IO)&lt;/li&gt; &#xA;  &lt;li&gt;Performance measured for T4 and Jetson Xavier NX with TensorRT, using FP16 precision and batch size 1&lt;/li&gt; &#xA;  &lt;li&gt;Performance measured for Cascade Lake CPU with OpenVINO, using FP16 precision and batch size 1&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Pretrained Object Detection PyTorch Checkpoints&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Resolution&lt;/th&gt; &#xA;   &lt;th&gt;mAP&lt;sup&gt;val&lt;br&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Latency (HW)*&lt;sub&gt;T4&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Latency (Production)**&lt;sub&gt;T4&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Latency (HW)*&lt;sub&gt;Jetson Xavier NX&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Latency (Production)**&lt;sub&gt;Jetson Xavier NX&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Latency &lt;sub&gt;Cascade Lake&lt;/sub&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SSD lite MobileNet v2&lt;/td&gt; &#xA;   &lt;td&gt;COCO&lt;/td&gt; &#xA;   &lt;td&gt;320x320&lt;/td&gt; &#xA;   &lt;td&gt;21.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.77ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.40ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;5.28ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;6.44ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;4.13ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5 nano&lt;/td&gt; &#xA;   &lt;td&gt;COCO&lt;/td&gt; &#xA;   &lt;td&gt;640x640&lt;/td&gt; &#xA;   &lt;td&gt;27.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.48ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;5.43ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;9.28ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;17.44ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;21.71ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5 small&lt;/td&gt; &#xA;   &lt;td&gt;COCO&lt;/td&gt; &#xA;   &lt;td&gt;640x640&lt;/td&gt; &#xA;   &lt;td&gt;37.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.29ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;6.14ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;14.31ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;22.50ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;34.10ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5 medium&lt;/td&gt; &#xA;   &lt;td&gt;COCO&lt;/td&gt; &#xA;   &lt;td&gt;640x640&lt;/td&gt; &#xA;   &lt;td&gt;45.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;4.60ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;8.10ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;26.76ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;34.95ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;65.86ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5 large&lt;/td&gt; &#xA;   &lt;td&gt;COCO&lt;/td&gt; &#xA;   &lt;td&gt;640x640&lt;/td&gt; &#xA;   &lt;td&gt;48.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;7.20ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;10.28ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;43.89ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;51.92ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;122.97ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; &lt;br&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Latency (HW)* - Hardware performance (not including IO)&lt;br&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Latency (Production)** - Production Performance (including IO)&lt;/li&gt; &#xA;  &lt;li&gt;Latency performance measured for T4 and Jetson Xavier NX with TensorRT, using FP16 precision and batch size 1&lt;/li&gt; &#xA;  &lt;li&gt;Latency performance measured for Cascade Lake CPU with OpenVINO, using FP16 precision and batch size 1&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Pretrained Semantic Segmentation PyTorch Checkpoints&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Resolution&lt;/th&gt; &#xA;   &lt;th&gt;mIoU&lt;/th&gt; &#xA;   &lt;th&gt;Latency b1&lt;sub&gt;T4&lt;/sub&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Latency b1&lt;sub&gt;T4&lt;/sub&gt; including IO&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DDRNet 23&lt;/td&gt; &#xA;   &lt;td&gt;Cityscapes&lt;/td&gt; &#xA;   &lt;td&gt;1024x2048&lt;/td&gt; &#xA;   &lt;td&gt;78.65&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;7.62ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;25.94ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DDRNet 23 slim&lt;/td&gt; &#xA;   &lt;td&gt;Cityscapes&lt;/td&gt; &#xA;   &lt;td&gt;1024x2048&lt;/td&gt; &#xA;   &lt;td&gt;76.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.56ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;22.80ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;STDC 1-Seg50&lt;/td&gt; &#xA;   &lt;td&gt;Cityscapes&lt;/td&gt; &#xA;   &lt;td&gt;512x1024&lt;/td&gt; &#xA;   &lt;td&gt;75.07&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.83ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;12.57ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;STDC 1-Seg75&lt;/td&gt; &#xA;   &lt;td&gt;Cityscapes&lt;/td&gt; &#xA;   &lt;td&gt;768x1536&lt;/td&gt; &#xA;   &lt;td&gt;77.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;5.71ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;26.70ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;STDC 2-Seg50&lt;/td&gt; &#xA;   &lt;td&gt;Cityscapes&lt;/td&gt; &#xA;   &lt;td&gt;512x1024&lt;/td&gt; &#xA;   &lt;td&gt;75.79&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.74ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;13.89ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;STDC 2-Seg75&lt;/td&gt; &#xA;   &lt;td&gt;Cityscapes&lt;/td&gt; &#xA;   &lt;td&gt;768x1536&lt;/td&gt; &#xA;   &lt;td&gt;78.93&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;7.35ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;28.18ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RegSeg (exp48)&lt;/td&gt; &#xA;   &lt;td&gt;Cityscapes&lt;/td&gt; &#xA;   &lt;td&gt;1024x2048&lt;/td&gt; &#xA;   &lt;td&gt;78.15&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;13.09ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;41.88ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Larger RegSeg (exp53)&lt;/td&gt; &#xA;   &lt;td&gt;Cityscapes&lt;/td&gt; &#xA;   &lt;td&gt;1024x2048&lt;/td&gt; &#xA;   &lt;td&gt;79.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;24.82ms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;51.87ms&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ShelfNet LW 34&lt;/td&gt; &#xA;   &lt;td&gt;COCO Segmentation (21 classes from PASCAL including background)&lt;/td&gt; &#xA;   &lt;td&gt;512x512&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;-&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;-&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Performance measured on T4 GPU with TensorRT, using FP16 precision and batch size 1 (latency), and not including IO&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Implemented Model Architectures&lt;/h2&gt; &#xA;&lt;h3&gt;Image Classification&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/densenet.py&#34;&gt;DensNet (Densely Connected Convolutional Networks)&lt;/a&gt; - Densely Connected Convolutional Networks &lt;a href=&#34;https://arxiv.org/pdf/1608.06993.pdf&#34;&gt;https://arxiv.org/pdf/1608.06993.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/dpn.py&#34;&gt;DPN&lt;/a&gt; - Dual Path Networks &lt;a href=&#34;https://arxiv.org/pdf/1707.01629&#34;&gt;https://arxiv.org/pdf/1707.01629&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/efficientnet.py&#34;&gt;EfficientNet&lt;/a&gt; - &lt;a href=&#34;https://arxiv.org/abs/1905.11946&#34;&gt;https://arxiv.org/abs/1905.11946&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/googlenet.py&#34;&gt;GoogleNet&lt;/a&gt; - &lt;a href=&#34;https://arxiv.org/pdf/1409.4842&#34;&gt;https://arxiv.org/pdf/1409.4842&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/lenet.py&#34;&gt;LeNet&lt;/a&gt; - &lt;a href=&#34;http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf&#34;&gt;https://yann.lecun.com/exdb/lenet/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/mobilenet.py&#34;&gt;MobileNet&lt;/a&gt; - Efficient Convolutional Neural Networks for Mobile Vision Applications &lt;a href=&#34;https://arxiv.org/pdf/1704.04861&#34;&gt;https://arxiv.org/pdf/1704.04861&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/mobilenetv2.py&#34;&gt;MobileNet v2&lt;/a&gt; - &lt;a href=&#34;https://arxiv.org/pdf/1801.04381&#34;&gt;https://arxiv.org/pdf/1801.04381&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/mobilenetv3.py&#34;&gt;MobileNet v3&lt;/a&gt; - &lt;a href=&#34;https://arxiv.org/pdf/1905.02244&#34;&gt;https://arxiv.org/pdf/1905.02244&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/pnasnet.py&#34;&gt;PNASNet&lt;/a&gt; - Progressive Neural Architecture Search Networks &lt;a href=&#34;https://arxiv.org/pdf/1712.00559&#34;&gt;https://arxiv.org/pdf/1712.00559&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/preact_resnet.py&#34;&gt;Pre-activation ResNet&lt;/a&gt; - &lt;a href=&#34;https://arxiv.org/pdf/1603.05027&#34;&gt;https://arxiv.org/pdf/1603.05027&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/regnet.py&#34;&gt;RegNet&lt;/a&gt; - &lt;a href=&#34;https://arxiv.org/pdf/2003.13678.pdf&#34;&gt;https://arxiv.org/pdf/2003.13678.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/repvgg.py&#34;&gt;RepVGG&lt;/a&gt; - Making VGG-style ConvNets Great Again &lt;a href=&#34;https://arxiv.org/pdf/2101.03697.pdf&#34;&gt;https://arxiv.org/pdf/2101.03697.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/resnet.py&#34;&gt;ResNet&lt;/a&gt; - Deep Residual Learning for Image Recognition &lt;a href=&#34;https://arxiv.org/pdf/1512.03385&#34;&gt;https://arxiv.org/pdf/1512.03385&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/resnext.py&#34;&gt;ResNeXt&lt;/a&gt; - Aggregated Residual Transformations for Deep Neural Networks &lt;a href=&#34;https://arxiv.org/pdf/1611.05431&#34;&gt;https://arxiv.org/pdf/1611.05431&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/senet.py&#34;&gt;SENet &lt;/a&gt; - Squeeze-and-Excitation Networks&lt;a href=&#34;https://arxiv.org/pdf/1709.01507&#34;&gt;https://arxiv.org/pdf/1709.01507&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/shufflenet.py&#34;&gt;ShuffleNet&lt;/a&gt; - &lt;a href=&#34;https://arxiv.org/pdf/1707.01083&#34;&gt;https://arxiv.org/pdf/1707.01083&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/shufflenetv2.py&#34;&gt;ShuffleNet v2&lt;/a&gt; - Efficient Convolutional Neural Network for Mobile Devices&lt;a href=&#34;https://arxiv.org/pdf/1807.11164&#34;&gt;https://arxiv.org/pdf/1807.11164&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/vgg.py&#34;&gt;VGG&lt;/a&gt; - Very Deep Convolutional Networks for Large-scale Image Recognition &lt;a href=&#34;https://arxiv.org/pdf/1409.1556&#34;&gt;https://arxiv.org/pdf/1409.1556&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Object Detection&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/detection_models/csp_darknet53.py&#34;&gt;CSP DarkNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/detection_models/darknet53.py&#34;&gt;DarkNet-53&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/detection_models/ssd.py&#34;&gt;SSD (Single Shot Detector)&lt;/a&gt; - &lt;a href=&#34;https://arxiv.org/pdf/1512.02325&#34;&gt;https://arxiv.org/pdf/1512.02325&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/detection_models/yolov3.py&#34;&gt;YOLO v3&lt;/a&gt; - &lt;a href=&#34;https://arxiv.org/pdf/1804.02767&#34;&gt;https://arxiv.org/pdf/1804.02767&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/detection_models/yolov5.py&#34;&gt;YOLO v5&lt;/a&gt; - &lt;a href=&#34;https://docs.ultralytics.com/&#34;&gt;by Ultralytics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Semantic Segmentation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/segmentation_models/ddrnet.py&#34;&gt;DDRNet (Deep Dual-resolution Networks)&lt;/a&gt; - &lt;a href=&#34;https://arxiv.org/pdf/2101.06085.pdf&#34;&gt;https://arxiv.org/pdf/2101.06085.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/segmentation_models/laddernet.py&#34;&gt;LadderNet&lt;/a&gt; - Multi-path networks based on U-Net for medical image segmentation &lt;a href=&#34;https://arxiv.org/pdf/1810.07810&#34;&gt;https://arxiv.org/pdf/1810.07810&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/segmentation_models/regseg.py&#34;&gt;RegSeg&lt;/a&gt; - Rethink Dilated Convolution for Real-time Semantic Segmentation &lt;a href=&#34;https://arxiv.org/pdf/2111.09957&#34;&gt;https://arxiv.org/pdf/2111.09957&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/segmentation_models/shelfnet.py&#34;&gt;ShelfNet&lt;/a&gt; - &lt;a href=&#34;https://arxiv.org/pdf/1811.11254&#34;&gt;https://arxiv.org/pdf/1811.11254&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/segmentation_models/stdc.py&#34;&gt;STDC&lt;/a&gt; - Rethinking BiSeNet For Real-time Semantic Segmentation &lt;a href=&#34;https://arxiv.org/pdf/2104.13188&#34;&gt;https://arxiv.org/pdf/2104.13188&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;  &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Check SuperGradients &lt;a href=&#34;https://deci-ai.github.io/super-gradients/welcome.html&#34;&gt;Docs&lt;/a&gt; for full documentation, user guide, and examples.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;To learn about making a contribution to SuperGradients, please see our &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/CONTRIBUTING.md&#34;&gt;Contribution page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Our awesome contributors:&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=Deci-AI/super-gradients&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;br&gt;Made with &lt;a href=&#34;https://contrib.rocks&#34;&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you are using SuperGradients library or benchmarks in your research, please cite SuperGradients deep learning training library.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;If you want to be a part of SuperGradients growing community, hear about all the exciting news and updates, need help, request for advanced features, or want to file a bug or issue report, we would love to welcome you aboard!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Slack is the place to be and ask questions about SuperGradients and get support. &lt;a href=&#34;https://join.slack.com/t/supergradients-comm52/shared_invite/zt-10vz6o1ia-b_0W5jEPEnuHXm087K~t8Q&#34;&gt;Click here to join our Slack&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To report a bug, &lt;a href=&#34;https://github.com/Deci-AI/super-gradients/issues&#34;&gt;file an issue&lt;/a&gt; on GitHub.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Join the &lt;a href=&#34;https://www.supergradients.com/#Newsletter&#34;&gt;SG Newsletter&lt;/a&gt; for staying up to date with new features and models, important announcements, and upcoming events.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For a short meeting with us, use this &lt;a href=&#34;https://calendly.com/ofer-baratz-deci/15min&#34;&gt;link&lt;/a&gt; and choose your preferred time.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Deci Platform&lt;/h2&gt; &#xA;&lt;p&gt;Deci Platform is our end to end platform for building, optimizing and deploying deep learning models to production.&lt;/p&gt; &#xA;&lt;p&gt;Sign up for our &lt;a href=&#34;https://console.deci.ai/&#34;&gt;FREE Community Tier&lt;/a&gt; to enjoy immediate improvement in throughput, latency, memory footprint and model size.&lt;/p&gt; &#xA;&lt;p&gt;Features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automatically compile and quantize your models with just a few clicks (TensorRT, OpenVINO).&lt;/li&gt; &#xA; &lt;li&gt;Gain up to 10X improvement in throughput, latency, memory and model size.&lt;/li&gt; &#xA; &lt;li&gt;Easily benchmark your models’ performance on different hardware and batch sizes.&lt;/li&gt; &#xA; &lt;li&gt;Invite co-workers to collaborate on models and communicate your progress.&lt;/li&gt; &#xA; &lt;li&gt;Deci supports all common frameworks and Hardware, from Intel CPUs to Nvidia&#39;s GPUs and Jetsons.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Sign up for Deci Platform for free &lt;a href=&#34;https://console.deci.ai/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NeuromatchAcademy/course-content</title>
    <updated>2022-07-12T01:46:55Z</updated>
    <id>tag:github.com,2022-07-12:/NeuromatchAcademy/course-content</id>
    <link href="https://github.com/NeuromatchAcademy/course-content" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NMA Computational Neuroscience course&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NeuroMatch Academy (NMA) Computational Neuroscience syllabus&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;July 11-29, 2022&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please check out &lt;a href=&#34;https://github.com/NeuromatchAcademy/precourse/raw/main/prereqs/ComputationalNeuroscience.md&#34;&gt;expected prerequisites here&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The content should primarily be accessed from our ebook: &lt;a href=&#34;https://compneuro.neuromatch.io/&#34;&gt;https://compneuro.neuromatch.io/&lt;/a&gt; [under continuous development]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Schedule for 2022: &lt;a href=&#34;https://github.com/NeuromatchAcademy/course-content/raw/main/tutorials/Schedule/daily_schedules.md&#34;&gt;https://github.com/NeuromatchAcademy/course-content/blob/main/tutorials/Schedule/daily_schedules.md&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Licensing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img src=&#34;https://i.creativecommons.org/l/by/4.0/88x31.png&#34; alt=&#34;CC BY 4.0&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg?sanitize=true&#34; alt=&#34;CC BY 4.0&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/BSD-3-Clause&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/9b9ea65d95c9ef878afa1987df65731d47681336/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f736561626f726e2e737667&#34; alt=&#34;BSD-3&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The contents of this repository are shared under under a &lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Software elements are additionally licensed under the &lt;a href=&#34;https://opensource.org/licenses/BSD-3-Clause&#34;&gt;BSD (3-Clause) License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Derivative works may use the license that is more appropriate to the relevant context.&lt;/p&gt;</summary>
  </entry>
</feed>