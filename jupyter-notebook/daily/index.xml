<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-30T01:34:32Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>OML-Team/open-metric-learning</title>
    <updated>2023-12-30T01:34:32Z</updated>
    <id>tag:github.com,2023-12-30:/OML-Team/open-metric-learning</id>
    <link href="https://github.com/OML-Team/open-metric-learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Library for metric learning pipelines and models.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://i.ibb.co/wsmD5r4/photo-2022-06-06-17-40-52.jpg&#34; width=&#34;400px&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://open-metric-learning.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/open-metric-learning/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/open-metric-learning&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/open-metric-learning&#34; alt=&#34;PyPI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/open-metric-learning/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/open-metric-learning.svg?sanitize=true&#34; alt=&#34;Pipi version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/actions/workflows/tests.yaml/badge.svg?&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python_3.8-passing-success&#34; alt=&#34;python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/actions/workflows/tests.yaml/badge.svg?&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python_3.9-passing-success&#34; alt=&#34;python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/actions/workflows/tests.yaml/badge.svg?&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python_3.10-passing-success&#34; alt=&#34;python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/actions/workflows/tests.yaml/badge.svg?&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python_3.11-passing-success&#34; alt=&#34;python&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;OML is a PyTorch-based framework to train and validate the models producing high-quality embeddings.&lt;/p&gt; &#xA; &lt;h3&gt;Trusted by&lt;/h3&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://docs.neptune.ai/integrations/community_developed/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://security.neptune.ai/api/share/b707f1e8-e287-4f01-b590-39a6fa7e9faa/logo.png&#34; width=&#34;100&#34;&gt;&lt;/a&gt;ㅤㅤ &#xA;  &lt;a href=&#34;https://www.newyorker.de/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/d/d8/New_Yorker.svg/1280px-New_Yorker.svg.png&#34; width=&#34;100&#34;&gt;&lt;/a&gt;ㅤㅤ &#xA;  &lt;a href=&#34;https://www.epoch8.co/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://i.ibb.co/GdNVTyt/Screenshot-2023-07-04-at-11-19-24.png&#34; width=&#34;100&#34;&gt;&lt;/a&gt;ㅤㅤ &#xA;  &lt;a href=&#34;https://www.meituan.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/6/61/Meituan_English_Logo.png&#34; width=&#34;100&#34;&gt;&lt;/a&gt;ㅤㅤ &#xA;  &lt;a href=&#34;https://constructor.io/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://rethink.industries/wp-content/uploads/2022/04/constructor.io-logo.png&#34; width=&#34;100&#34;&gt;&lt;/a&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://www.ox.ac.uk/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://i.ibb.co/zhWL6tD/21-05-2019-16-08-10-6922268.png&#34; width=&#34;120&#34;&gt;&lt;/a&gt;ㅤㅤ &lt;a href=&#34;https://www.hse.ru/en/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://www.hse.ru/data/2020/11/16/1367274044/HSE_University_blue.jpg.(230x86x123).jpg&#34; width=&#34;100&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;p&gt;There is a number of people from &lt;a href=&#34;https://www.ox.ac.uk/&#34;&gt;Oxford&lt;/a&gt; and &lt;a href=&#34;https://www.hse.ru/en/&#34;&gt;HSE&lt;/a&gt; universities who have used OML in their theses. &lt;a href=&#34;https://github.com/nilomr/open-metric-learning/tree/great-tit/great-tit-train&#34;&gt;[1]&lt;/a&gt; &lt;a href=&#34;https://github.com/nastygorodi/PROJECT-Deep_Metric_Learning&#34;&gt;[2]&lt;/a&gt; &lt;a href=&#34;https://github.com/nik-fedorov/term_paper_metric_learning&#34;&gt;[3]&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;div align=&#34;left&#34;&gt; &#xA;   &lt;h2&gt;&lt;a href=&#34;https://open-metric-learning.readthedocs.io/en/latest/oml/faq.html&#34;&gt;FAQ&lt;/a&gt;&lt;/h2&gt; &#xA;   &lt;details&gt; &#xA;    &lt;summary&gt;Why do I need OML?&lt;/summary&gt; &#xA;    &lt;p&gt; &lt;/p&gt;&#xA;    &lt;p&gt;You may think &lt;em&gt;&#34;If I need image embeddings I can simply train a vanilla classifier and take its penultimate layer&#34;&lt;/em&gt;. Well, it makes sense as a starting point. But there are several possible drawbacks:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt; &lt;p&gt;If you want to use embeddings to perform searching you need to calculate some distance among them (for example, cosine or L2). Usually, &lt;strong&gt;you don&#39;t directly optimize these distances during the training&lt;/strong&gt; in the classification setup. So, you can only hope that final embeddings will have the desired properties.&lt;/p&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;strong&gt;The second problem is the validation process&lt;/strong&gt;. In the searching setup, you usually care how related your top-N outputs are to the query. The natural way to evaluate the model is to simulate searching requests to the reference set and apply one of the retrieval metrics. So, there is no guarantee that classification accuracy will correlate with these metrics.&lt;/p&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;Finally, you may want to implement a metric learning pipeline by yourself. &lt;strong&gt;There is a lot of work&lt;/strong&gt;: to use triplet loss you need to form batches in a specific way, implement different kinds of triplets mining, tracking distances, etc. For the validation, you also need to implement retrieval metrics, which include effective embeddings accumulation during the epoch, covering corner cases, etc. It&#39;s even harder if you have several gpus and use DDP. You may also want to visualize your search requests by highlighting good and bad search results. Instead of doing it by yourself, you can simply use OML for your purposes.&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &#xA;    &lt;p&gt;&lt;/p&gt; &#xA;   &lt;/details&gt; &#xA;   &lt;details&gt; &#xA;    &lt;summary&gt;What is the difference between Open Metric Learning and PyTorch Metric Learning?&lt;/summary&gt; &#xA;    &lt;p&gt; &lt;/p&gt;&#xA;    &lt;p&gt;&lt;a href=&#34;https://github.com/KevinMusgrave/pytorch-metric-learning&#34;&gt;PML&lt;/a&gt; is the popular library for Metric Learning, and it includes a rich collection of losses, miners, distances, and reducers; that is why we provide straightforward &lt;a href=&#34;https://github.com/OML-Team/open-metric-learning#usage-with-pytorch-metric-learning&#34;&gt;examples&lt;/a&gt; of using them with OML. Initially, we tried to use PML, but in the end, we came up with our library, which is more pipeline / recipes oriented. That is how OML differs from PML:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt; &lt;p&gt;OML has &lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/tree/main/pipelines&#34;&gt;Pipelines&lt;/a&gt; which allows training models by preparing a config and your data in the required format (it&#39;s like converting data into COCO format to train a detector from &lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;mmdetection&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;OML focuses on end-to-end pipelines and practical use cases. It has config based examples on popular benchmarks close to real life (like photos of products of thousands ids). We found some good combinations of hyperparameters on these datasets, trained and published models and their configs. Thus, it makes OML more recipes oriented than PML, and its author &lt;a href=&#34;https://github.com/KevinMusgrave/pytorch-metric-learning/issues/169#issuecomment-670814393&#34;&gt;confirms&lt;/a&gt; this saying that his library is a set of tools rather the recipes, moreover, the examples in PML are mostly for CIFAR and MNIST datasets.&lt;/p&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;OML has the &lt;a href=&#34;https://github.com/OML-Team/open-metric-learning#zoo&#34;&gt;Zoo&lt;/a&gt; of pretrained models that can be easily accessed from the code in the same way as in &lt;code&gt;torchvision&lt;/code&gt; (when you type &lt;code&gt;resnet50(pretrained=True)&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;OML is integrated with &lt;a href=&#34;https://www.pytorchlightning.ai/&#34;&gt;PyTorch Lightning&lt;/a&gt;, so, we can use the power of its &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html&#34;&gt;Trainer&lt;/a&gt;. This is especially helpful when we work with DDP, so, you compare our &lt;a href=&#34;https://open-metric-learning.readthedocs.io/en/latest/examples/python.html&#34;&gt;DDP example&lt;/a&gt; and the &lt;a href=&#34;https://github.com/KevinMusgrave/pytorch-metric-learning/raw/master/examples/notebooks/DistributedTripletMarginLossMNIST.ipynb&#34;&gt;PMLs one&lt;/a&gt;. By the way, PML also has &lt;a href=&#34;https://kevinmusgrave.github.io/pytorch-metric-learning/trainers/&#34;&gt;Trainers&lt;/a&gt;, but it&#39;s not widely used in the examples and custom &lt;code&gt;train&lt;/code&gt; / &lt;code&gt;test&lt;/code&gt; functions are used instead.&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &#xA;    &lt;p&gt;We believe that having Pipelines, laconic examples, and Zoo of pretrained models sets the entry threshold to a really low value.&lt;/p&gt; &#xA;    &lt;p&gt;&lt;/p&gt; &#xA;   &lt;/details&gt; &#xA;   &lt;details&gt; &#xA;    &lt;summary&gt;What is Metric Learning?&lt;/summary&gt; &#xA;    &lt;p&gt; &lt;/p&gt;&#xA;    &lt;p&gt;Metric Learning problem (also known as &lt;em&gt;extreme classification&lt;/em&gt; problem) means a situation in which we have thousands of ids of some entities, but only a few samples for every entity. Often we assume that during the test stage (or production) we will deal with unseen entities which makes it impossible to apply the vanilla classification pipeline directly. In many cases obtained embeddings are used to perform search or matching procedures over them.&lt;/p&gt; &#xA;    &lt;p&gt;Here are a few examples of such tasks from the computer vision sphere:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Person/Animal Re-Identification&lt;/li&gt; &#xA;     &lt;li&gt;Face Recognition&lt;/li&gt; &#xA;     &lt;li&gt;Landmark Recognition&lt;/li&gt; &#xA;     &lt;li&gt;Searching engines for online shops and many others.&lt;/li&gt; &#xA;    &lt;/ul&gt; &#xA;    &lt;p&gt;&lt;/p&gt; &#xA;   &lt;/details&gt; &#xA;   &lt;details&gt; &#xA;    &lt;summary&gt;Glossary (Naming convention) &lt;/summary&gt; &#xA;    &lt;p&gt; &lt;/p&gt;&#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;embedding&lt;/code&gt; - model&#39;s output (also known as &lt;code&gt;features vector&lt;/code&gt; or &lt;code&gt;descriptor&lt;/code&gt;).&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;query&lt;/code&gt; - a sample which is used as a request in the retrieval procedure.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;gallery set&lt;/code&gt; - the set of entities to search items similar to &lt;code&gt;query&lt;/code&gt; (also known as &lt;code&gt;reference&lt;/code&gt; or &lt;code&gt;index&lt;/code&gt;).&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;Sampler&lt;/code&gt; - an argument for &lt;code&gt;DataLoader&lt;/code&gt; which is used to form batches&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;Miner&lt;/code&gt; - the object to form pairs or triplets after the batch was formed by &lt;code&gt;Sampler&lt;/code&gt;. It&#39;s not necessary to form the combinations of samples only inside the current batch, thus, the memory bank may be a part of &lt;code&gt;Miner&lt;/code&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;Samples&lt;/code&gt;/&lt;code&gt;Labels&lt;/code&gt;/&lt;code&gt;Instances&lt;/code&gt; - as an example let&#39;s consider DeepFashion dataset. It includes thousands of fashion item ids (we name them &lt;code&gt;labels&lt;/code&gt;) and several photos for each item id (we name the individual photo as &lt;code&gt;instance&lt;/code&gt; or &lt;code&gt;sample&lt;/code&gt;). All of the fashion item ids have their groups like &#34;skirts&#34;, &#34;jackets&#34;, &#34;shorts&#34; and so on (we name them &lt;code&gt;categories&lt;/code&gt;). Note, we avoid using the term &lt;code&gt;class&lt;/code&gt; to avoid misunderstanding.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;training epoch&lt;/code&gt; - batch samplers which we use for combination-based losses usually have a length equal to &lt;code&gt;[number of labels in training dataset] / [numbers of labels in one batch]&lt;/code&gt;. It means that we don&#39;t observe all of the available training samples in one epoch (as opposed to vanilla classification), instead, we observe all of the available labels.&lt;/li&gt; &#xA;    &lt;/ul&gt; &#xA;    &lt;p&gt;&lt;/p&gt; &#xA;   &lt;/details&gt; &#xA;   &lt;details&gt; &#xA;    &lt;summary&gt;How good may be a model trained with OML? &lt;/summary&gt; &#xA;    &lt;p&gt; &lt;/p&gt;&#xA;    &lt;p&gt;It may be comparable with the current (2022 year) &lt;a href=&#34;https://paperswithcode.com/task/metric-learning&#34;&gt;SotA&lt;/a&gt; methods, for example, &lt;a href=&#34;https://arxiv.org/pdf/2203.10833.pdf&#34;&gt;Hyp-ViT&lt;/a&gt;. &lt;em&gt;(Few words about this approach: it&#39;s a ViT architecture trained with contrastive loss, but the embeddings were projected into some hyperbolic space. As the authors claimed, such a space is able to describe the nested structure of real-world data. So, the paper requires some heavy math to adapt the usual operations for the hyperbolical space.)&lt;/em&gt;&lt;/p&gt; &#xA;    &lt;p&gt;We trained the same architecture with triplet loss, fixing the rest of the parameters: training and test transformations, image size, and optimizer. See configs in &lt;a href=&#34;https://github.com/OML-Team/open-metric-learning#zoo&#34;&gt;Models Zoo&lt;/a&gt;. The trick was in heuristics in our miner and sampler:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://open-metric-learning.readthedocs.io/en/latest/contents/samplers.html#categorybalancesampler&#34;&gt;Category Balance Sampler&lt;/a&gt; forms the batches limiting the number of categories &lt;em&gt;C&lt;/em&gt; in it. For instance, when &lt;em&gt;C = 1&lt;/em&gt; it puts only jackets in one batch and only jeans into another one (just an example). It automatically makes the negative pairs harder: it&#39;s more meaningful for a model to realise why two jackets are different than to understand the same about a jacket and a t-shirt.&lt;/p&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://open-metric-learning.readthedocs.io/en/latest/contents/miners.html#hardtripletsminer&#34;&gt;Hard Triplets Miner&lt;/a&gt; makes the task even harder keeping only the hardest triplets (with maximal positive and minimal negative distances).&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &#xA;    &lt;p&gt;Here are &lt;em&gt;CMC@1&lt;/em&gt; scores for 2 popular benchmarks. SOP dataset: Hyp-ViT — 85.9, ours — 86.6. DeepFashion dataset: Hyp-ViT — 92.5, ours — 92.1. Thus, utilising simple heuristics and avoiding heavy math we are able to perform on SotA level.&lt;/p&gt; &#xA;    &lt;p&gt;&lt;/p&gt; &#xA;   &lt;/details&gt; &#xA;   &lt;details&gt; &#xA;    &lt;summary&gt;What about Self-Supervised Learning?&lt;/summary&gt; &#xA;    &lt;p&gt; &lt;/p&gt;&#xA;    &lt;p&gt;Recent research in SSL definitely obtained great results. The problem is that these approaches required an enormous amount of computing to train the model. But in our framework, we consider the most common case when the average user has no more than a few GPUs.&lt;/p&gt; &#xA;    &lt;p&gt;At the same time, it would be unwise to ignore success in this sphere, so we still exploit it in two ways:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;As a source of checkpoints that would be great to start training with. From publications and our experience, they are much better as initialisation than the default supervised model trained on ImageNet. Thus, we added the possibility to initialise your models using these pretrained checkpoints only by passing an argument in the config or the constructor.&lt;/li&gt; &#xA;     &lt;li&gt;As a source of inspiration. For example, we adapted the idea of a memory bank from &lt;em&gt;MoCo&lt;/em&gt; for the &lt;em&gt;TripletLoss&lt;/em&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &#xA;    &lt;p&gt;&lt;/p&gt; &#xA;   &lt;/details&gt; &#xA;   &lt;details&gt; &#xA;    &lt;summary&gt;Do I need to know other frameworks to use OML?&lt;/summary&gt; &#xA;    &lt;p&gt; &lt;/p&gt;&#xA;    &lt;p&gt;No, you don&#39;t. OML is a framework-agnostic. Despite we use PyTorch Lightning as a loop runner for the experiments, we also keep the possibility to run everything on pure PyTorch. Thus, only the tiny part of OML is Lightning-specific and we keep this logic separately from other code (see &lt;code&gt;oml.lightning&lt;/code&gt;). Even when you use Lightning, you don&#39;t need to know it, since we provide ready to use &lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/raw/main/pipelines/&#34;&gt;Pipelines&lt;/a&gt;.&lt;/p&gt; &#xA;    &lt;p&gt;The possibility of using pure PyTorch and modular structure of the code leaves a room for utilizing OML with your favourite framework after the implementation of the necessary wrappers.&lt;/p&gt; &#xA;    &lt;p&gt;&lt;/p&gt; &#xA;   &lt;/details&gt; &#xA;   &lt;details&gt; &#xA;    &lt;summary&gt;Can I use OML without any knowledge in DataScience?&lt;/summary&gt; &#xA;    &lt;p&gt; &lt;/p&gt;&#xA;    &lt;p&gt;Yes. To run the experiment with &lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/raw/main/pipelines/&#34;&gt;Pipelines&lt;/a&gt; you only need to write a converter to our format (it means preparing the &lt;code&gt;.csv&lt;/code&gt; table with 5 predefined columns). That&#39;s it!&lt;/p&gt; &#xA;    &lt;p&gt;Probably we already have a suitable pre-trained model for your domain in our &lt;em&gt;Models Zoo&lt;/em&gt;. In this case, you don&#39;t even need to train it.&lt;/p&gt; &#xA;    &lt;p&gt;&lt;/p&gt; &#xA;   &lt;/details&gt; &#xA;   &lt;details&gt; &#xA;    &lt;summary&gt;Can OML process texts, sounds and other modalities?&lt;/summary&gt; &#xA;    &lt;p&gt; &lt;/p&gt;&#xA;    &lt;p&gt;You can adapt OML to make it work not only with images. Just open one of the examples and replace &lt;code&gt;Dataset&lt;/code&gt; remaining the rest of the pipeline the same or almost the same. There is several people who successfully used OML for texts in their real-world projects.&lt;/p&gt; &#xA;    &lt;p&gt;Unfortunately, we don&#39;t have ready-to-use tutorials for this kind of usage at the moment.&lt;/p&gt; &#xA;    &lt;p&gt;&lt;/p&gt; &#xA;   &lt;/details&gt; &#xA;   &lt;h2&gt;&lt;a href=&#34;https://open-metric-learning.readthedocs.io/en/latest/index.html&#34;&gt;Documentation&lt;/a&gt;&lt;/h2&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://open-metric-learning.readthedocs.io/en/latest/index.html&#34;&gt;&lt;strong&gt;DOCUMENTATION&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;TUTORIAL TO START WITH:&lt;/strong&gt; &lt;a href=&#34;https://medium.com/@AlekseiShabanov/practical-metric-learning-b0410cda2201&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://habr.com/ru/company/ods/blog/695380/&#34;&gt;Russian&lt;/a&gt; | &lt;a href=&#34;https://blog.csdn.net/fermion0217/article/details/127932087&#34;&gt;Chinese&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;   &lt;hr&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;The &lt;a href=&#34;https://dapladoc-oml-postprocessing-demo-srcappmain-pfh2g0.streamlit.app/&#34;&gt;DEMO&lt;/a&gt; for our paper &lt;a href=&#34;https://arxiv.org/abs/2304.13393&#34;&gt;STIR: Siamese Transformers for Image Retrieval Postprocessing&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;The report for Berlin-based meetup: &#34;Computer Vision in production&#34;. November, 2022. &lt;a href=&#34;https://drive.google.com/drive/folders/1uHmLU8vMrMVMFodt36u0uXAgYjG_3D30?usp=share_link&#34;&gt;Link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;   &lt;h2&gt;&lt;a href=&#34;https://open-metric-learning.readthedocs.io/en/latest/oml/installation.html&#34;&gt;Installation&lt;/a&gt;&lt;/h2&gt; &#xA;   &lt;p&gt;OML is available in PyPI:&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -U open-metric-learning&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;p&gt;You can also pull the prepared image from DockerHub...&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker pull omlteam/oml:gpu&#xA;docker pull omlteam/oml:cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;h2&gt;&lt;a href=&#34;https://open-metric-learning.readthedocs.io/en/latest/feature_extraction/python_examples.html#&#34;&gt;Examples&lt;/a&gt;&lt;/h2&gt; &#xA;   &lt;details&gt; &#xA;    &lt;summary&gt;Training&lt;/summary&gt; &#xA;    &lt;p&gt; &lt;/p&gt;&#xA;    &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from tqdm import tqdm&#xA;&#xA;from oml.datasets.base import DatasetWithLabels&#xA;from oml.losses.triplet import TripletLossWithMiner&#xA;from oml.miners.inbatch_all_tri import AllTripletsMiner&#xA;from oml.models import ViTExtractor&#xA;from oml.samplers.balance import BalanceSampler&#xA;from oml.utils.download_mock_dataset import download_mock_dataset&#xA;&#xA;dataset_root = &#34;mock_dataset/&#34;&#xA;df_train, _ = download_mock_dataset(dataset_root)&#xA;&#xA;extractor = ViTExtractor(&#34;vits16_dino&#34;, arch=&#34;vits16&#34;, normalise_features=False).train()&#xA;optimizer = torch.optim.SGD(extractor.parameters(), lr=1e-6)&#xA;&#xA;train_dataset = DatasetWithLabels(df_train, dataset_root=dataset_root)&#xA;criterion = TripletLossWithMiner(margin=0.1, miner=AllTripletsMiner(), need_logs=True)&#xA;sampler = BalanceSampler(train_dataset.get_labels(), n_labels=2, n_instances=2)&#xA;train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=sampler)&#xA;&#xA;for batch in tqdm(train_loader):&#xA;    embeddings = extractor(batch[&#34;input_tensors&#34;])&#xA;    loss = criterion(embeddings, batch[&#34;labels&#34;])&#xA;    loss.backward()&#xA;    optimizer.step()&#xA;    optimizer.zero_grad()&#xA;&#xA;    # info for logging: positive/negative distances, number of active triplets&#xA;    print(criterion.last_logs)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;p&gt;&lt;/p&gt; &#xA;   &lt;/details&gt; &#xA;   &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1kntDAIdIZ9L40jcndguLAb-XqmCFOgS5?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;details&gt; &#xA;    &lt;summary&gt;Validation&lt;/summary&gt; &#xA;    &lt;p&gt; &lt;/p&gt;&#xA;    &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from tqdm import tqdm&#xA;&#xA;from oml.datasets.base import DatasetQueryGallery&#xA;from oml.metrics.embeddings import EmbeddingMetrics&#xA;from oml.models import ViTExtractor&#xA;from oml.utils.download_mock_dataset import download_mock_dataset&#xA;&#xA;dataset_root = &#34;mock_dataset/&#34;&#xA;_, df_val = download_mock_dataset(dataset_root)&#xA;&#xA;extractor = ViTExtractor(&#34;vits16_dino&#34;, arch=&#34;vits16&#34;, normalise_features=False).eval()&#xA;&#xA;val_dataset = DatasetQueryGallery(df_val, dataset_root=dataset_root)&#xA;&#xA;val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4)&#xA;calculator = EmbeddingMetrics(extra_keys=(&#34;paths&#34;,))&#xA;calculator.setup(num_samples=len(val_dataset))&#xA;&#xA;with torch.no_grad():&#xA;    for batch in tqdm(val_loader):&#xA;        batch[&#34;embeddings&#34;] = extractor(batch[&#34;input_tensors&#34;])&#xA;        calculator.update_data(batch)&#xA;&#xA;metrics = calculator.compute_metrics()&#xA;&#xA;# Logging&#xA;print(calculator.metrics)  # metrics&#xA;print(calculator.metrics_unreduced)  # metrics without averaging over queries&#xA;&#xA;# Visualisation&#xA;calculator.get_plot_for_queries(query_ids=[0, 2], n_instances=5)  # draw predictions on predefined queries&#xA;calculator.get_plot_for_worst_queries(metric_name=&#34;OVERALL/map/5&#34;, n_queries=2, n_instances=5)  # draw mistakes&#xA;calculator.visualize()  # draw mistakes for all the available metrics&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;p&gt;&lt;/p&gt; &#xA;   &lt;/details&gt; &#xA;   &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1O2o3k8I8jN5hRin3dKnAS3WsgG04tmIT?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;details&gt; &#xA;    &lt;summary&gt;Training + Validation [Lightning and logging]&lt;/summary&gt; &#xA;    &lt;p&gt; &lt;/p&gt;&#xA;    &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pytorch_lightning as pl&#xA;import torch&#xA;&#xA;from oml.datasets.base import DatasetQueryGallery, DatasetWithLabels&#xA;from oml.lightning.modules.extractor import ExtractorModule&#xA;from oml.lightning.callbacks.metric import MetricValCallback&#xA;from oml.losses.triplet import TripletLossWithMiner&#xA;from oml.metrics.embeddings import EmbeddingMetrics&#xA;from oml.miners.inbatch_all_tri import AllTripletsMiner&#xA;from oml.models import ViTExtractor&#xA;from oml.samplers.balance import BalanceSampler&#xA;from oml.utils.download_mock_dataset import download_mock_dataset&#xA;from pytorch_lightning.loggers import NeptuneLogger, TensorBoardLogger, WandbLogger&#xA;&#xA;dataset_root = &#34;mock_dataset/&#34;&#xA;df_train, df_val = download_mock_dataset(dataset_root)&#xA;&#xA;# model&#xA;extractor = ViTExtractor(&#34;vits16_dino&#34;, arch=&#34;vits16&#34;, normalise_features=False)&#xA;&#xA;# train&#xA;optimizer = torch.optim.SGD(extractor.parameters(), lr=1e-6)&#xA;train_dataset = DatasetWithLabels(df_train, dataset_root=dataset_root)&#xA;criterion = TripletLossWithMiner(margin=0.1, miner=AllTripletsMiner())&#xA;batch_sampler = BalanceSampler(train_dataset.get_labels(), n_labels=2, n_instances=3)&#xA;train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=batch_sampler)&#xA;&#xA;# val&#xA;val_dataset = DatasetQueryGallery(df_val, dataset_root=dataset_root)&#xA;val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4)&#xA;metric_callback = MetricValCallback(metric=EmbeddingMetrics(extra_keys=[train_dataset.paths_key,]), log_images=True)&#xA;&#xA;# 1) Logging with Tensorboard&#xA;logger = TensorBoardLogger(&#34;.&#34;)&#xA;&#xA;# 2) Logging with Neptune&#xA;# logger = NeptuneLogger(api_key=&#34;&#34;, project=&#34;&#34;, log_model_checkpoints=False)&#xA;&#xA;# 3) Logging with Weights and Biases&#xA;# import os&#xA;# os.environ[&#34;WANDB_API_KEY&#34;] = &#34;&#34;&#xA;# logger = WandbLogger(project=&#34;test_project&#34;, log_model=False)&#xA;&#xA;# run&#xA;pl_model = ExtractorModule(extractor, criterion, optimizer)&#xA;trainer = pl.Trainer(max_epochs=3, callbacks=[metric_callback], num_sanity_val_steps=0, logger=logger)&#xA;trainer.fit(pl_model, train_dataloaders=train_loader, val_dataloaders=val_loader)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;p&gt;&lt;/p&gt; &#xA;   &lt;/details&gt; &#xA;   &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1bVUgdBGWvQgCkba2YtaIRVlUQUz7Q60Z?usp=share_link&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;details&gt; &#xA;    &lt;summary&gt;Using a trained model for retrieval&lt;/summary&gt; &#xA;    &lt;p&gt; &lt;/p&gt;&#xA;    &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;from oml.const import MOCK_DATASET_PATH&#xA;from oml.inference.flat import inference_on_images&#xA;from oml.models import ViTExtractor&#xA;from oml.registry.transforms import get_transforms_for_pretrained&#xA;from oml.utils.download_mock_dataset import download_mock_dataset&#xA;from oml.utils.misc_torch import pairwise_dist&#xA;&#xA;_, df_val = download_mock_dataset(MOCK_DATASET_PATH)&#xA;df_val[&#34;path&#34;] = df_val[&#34;path&#34;].apply(lambda x: MOCK_DATASET_PATH / x)&#xA;queries = df_val[df_val[&#34;is_query&#34;]][&#34;path&#34;].tolist()&#xA;galleries = df_val[df_val[&#34;is_gallery&#34;]][&#34;path&#34;].tolist()&#xA;&#xA;extractor = ViTExtractor.from_pretrained(&#34;vits16_dino&#34;)&#xA;transform, _ = get_transforms_for_pretrained(&#34;vits16_dino&#34;)&#xA;&#xA;args = {&#34;num_workers&#34;: 0, &#34;batch_size&#34;: 8}&#xA;features_queries = inference_on_images(extractor, paths=queries, transform=transform, **args)&#xA;features_galleries = inference_on_images(extractor, paths=galleries, transform=transform, **args)&#xA;&#xA;# Now we can explicitly build pairwise matrix of distances or save you RAM via using kNN&#xA;use_knn = False&#xA;top_k = 3&#xA;&#xA;if use_knn:&#xA;    from sklearn.neighbors import NearestNeighbors&#xA;    knn = NearestNeighbors(algorithm=&#34;auto&#34;, p=2)&#xA;    knn.fit(features_galleries)&#xA;    dists, ii_closest = knn.kneighbors(features_queries, n_neighbors=top_k, return_distance=True)&#xA;&#xA;else:&#xA;    dist_mat = pairwise_dist(x1=features_queries, x2=features_galleries)&#xA;    dists, ii_closest = torch.topk(dist_mat, dim=1, k=top_k, largest=False)&#xA;&#xA;print(f&#34;Top {top_k} items closest to queries are:\n {ii_closest}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;p&gt;&lt;/p&gt; &#xA;   &lt;/details&gt; &#xA;   &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1S2nK6KaReDm-RjjdojdId6CakhhSyvfA?usp=share_link&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;p&gt;&lt;a href=&#34;https://open-metric-learning.readthedocs.io/en/latest/feature_extraction/python_examples.html&#34;&gt;MORE EXAMPLES&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;p&gt;&lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/tree/main/pipelines/features_extraction#training&#34;&gt;&lt;strong&gt;Illustrations, explanations and tips&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;h2&gt;&lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/tree/main/pipelines&#34;&gt;Pipelines&lt;/a&gt;&lt;/h2&gt; &#xA;   &lt;p&gt;Pipelines provide a way to run metric learning experiments via changing only the config file. All you need is to prepare your dataset in a required format.&lt;/p&gt; &#xA;   &lt;p&gt;See &lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/raw/main/pipelines/&#34;&gt;Pipelines&lt;/a&gt; folder for more details:&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Feature extractor &lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/tree/main/pipelines/features_extraction&#34;&gt;pipeline&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;Retrieval re-ranking &lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/tree/main/pipelines/postprocessing&#34;&gt;pipeline&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;   &lt;h2&gt;&lt;a href=&#34;https://open-metric-learning.readthedocs.io/en/latest/feature_extraction/zoo.html&#34;&gt;Zoo&lt;/a&gt;&lt;/h2&gt; &#xA;   &lt;p&gt;Models, trained by us. The metrics below are for &lt;strong&gt;224 x 224&lt;/strong&gt; images:&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;model&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;cmc1&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;dataset&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;weights&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;experiment&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTExtractor.from_pretrained(&#34;vits16_inshop&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.921&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;DeepFashion Inshop&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1niX-TC8cj6j369t7iU2baHQSVN3MVJbW/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/tree/main/pipelines/features_extraction/extractor_inshop&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTExtractor.from_pretrained(&#34;vits16_sop&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.866&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Stanford Online Products&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1zuGRHvF2KHd59aw7i7367OH_tQNOGz7A/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/tree/main/pipelines/features_extraction/extractor_sop&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTExtractor.from_pretrained(&#34;vits16_cars&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.907&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;CARS 196&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/17a4_fg94dox2sfkXmw-KCtiLBlx-ut-1?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/tree/main/pipelines/features_extraction/extractor_cars&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTExtractor.from_pretrained(&#34;vits16_cub&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.837&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;CUB 200 2011&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1TPCN-eZFLqoq4JBgnIfliJoEK48x9ozb?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/tree/main/pipelines/features_extraction/extractor_cub&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &#xA;   &lt;p&gt;Models, trained by other researchers. The metrics below are for &lt;strong&gt;224 x 224&lt;/strong&gt; images:&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;model&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Stanford Online Products&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;DeepFashion InShop&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;CUB 200 2011&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;CARS 196&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTUnicomExtractor.from_pretrained(&#34;vitb16_unicom&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.700&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.734&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.847&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.916&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTUnicomExtractor.from_pretrained(&#34;vitb32_unicom&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.690&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.722&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.796&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.893&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTUnicomExtractor.from_pretrained(&#34;vitl14_unicom&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.726&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.790&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.868&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.922&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTUnicomExtractor.from_pretrained(&#34;vitl14_336px_unicom&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.745&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.810&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.875&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.924&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTCLIPExtractor.from_pretrained(&#34;sber_vitb32_224&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.547&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.514&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.448&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.618&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTCLIPExtractor.from_pretrained(&#34;sber_vitb16_224&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.565&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.565&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.524&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.648&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTCLIPExtractor.from_pretrained(&#34;sber_vitl14_224&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.512&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.555&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.606&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.707&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTCLIPExtractor.from_pretrained(&#34;openai_vitb32_224&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.612&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.491&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.560&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.693&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTCLIPExtractor.from_pretrained(&#34;openai_vitb16_224&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.648&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.606&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.665&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.767&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTCLIPExtractor.from_pretrained(&#34;openai_vitl14_224&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.670&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.675&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.745&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.844&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTExtractor.from_pretrained(&#34;vits16_dino&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.648&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.509&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.627&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.265&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTExtractor.from_pretrained(&#34;vits8_dino&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.651&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.524&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.661&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.315&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTExtractor.from_pretrained(&#34;vitb16_dino&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.658&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.514&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.541&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.288&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ViTExtractor.from_pretrained(&#34;vitb8_dino&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.689&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.599&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.506&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.313&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ResnetExtractor.from_pretrained(&#34;resnet50_moco_v2&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.493&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.267&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.264&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.149&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ResnetExtractor.from_pretrained(&#34;resnet50_imagenet1k_v1&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.515&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.284&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.455&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.247&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &#xA;   &lt;p&gt;*&lt;em&gt;The metrics may be different from the ones reported by papers, because the version of train/val split and usage of bounding boxes may differ.&lt;/em&gt;&lt;/p&gt; &#xA;   &lt;h3&gt;How to use models from Zoo?&lt;/h3&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from oml.const import CKPT_SAVE_ROOT as CKPT_DIR, MOCK_DATASET_PATH as DATA_DIR&#xA;from oml.models import ViTExtractor&#xA;from oml.registry.transforms import get_transforms_for_pretrained&#xA;&#xA;model = ViTExtractor.from_pretrained(&#34;vits16_dino&#34;)&#xA;transforms, im_reader = get_transforms_for_pretrained(&#34;vits16_dino&#34;)&#xA;&#xA;img = im_reader(DATA_DIR / &#34;images&#34; / &#34;circle_1.jpg&#34;)  # put path to your image here&#xA;img_tensor = transforms(img)&#xA;# img_tensor = transforms(image=img)[&#34;image&#34;]  # for transforms from Albumentations&#xA;&#xA;features = model(img_tensor.unsqueeze(0))&#xA;&#xA;# Check other available models:&#xA;print(list(ViTExtractor.pretrained_models.keys()))&#xA;&#xA;# Load checkpoint saved on a disk:&#xA;model_ = ViTExtractor(weights=CKPT_DIR / &#34;vits16_dino.ckpt&#34;, arch=&#34;vits16&#34;, normalise_features=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;h2&gt;&lt;a href=&#34;https://open-metric-learning.readthedocs.io/en/latest/oml/contributing.html&#34;&gt;Contributing guide&lt;/a&gt;&lt;/h2&gt; &#xA;   &lt;p&gt;We welcome new contributors! Please, see our:&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://open-metric-learning.readthedocs.io/en/latest/oml/contributing.html&#34;&gt;Contributing guide&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://github.com/OML-Team/open-metric-learning/projects/1&#34;&gt;Kanban board&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;   &lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;   &lt;p&gt;&lt;a href=&#34;https://github.com/catalyst-team/catalyst&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png&#34; width=&#34;100&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;p&gt;The project was started in 2020 as a module for &lt;a href=&#34;https://github.com/catalyst-team/catalyst&#34;&gt;Catalyst&lt;/a&gt; library. I want to thank people who worked with me on that module: &lt;a href=&#34;https://github.com/julia-shenshina&#34;&gt;Julia Shenshina&lt;/a&gt;, &lt;a href=&#34;https://github.com/elephantmipt&#34;&gt;Nikita Balagansky&lt;/a&gt;, &lt;a href=&#34;https://github.com/Scitator&#34;&gt;Sergey Kolesnikov&lt;/a&gt; and others.&lt;/p&gt; &#xA;   &lt;p&gt;I would like to thank people who continue working on this pipeline when it became a separe project: &lt;a href=&#34;https://github.com/julia-shenshina&#34;&gt;Julia Shenshina&lt;/a&gt;, &lt;a href=&#34;https://github.com/b0nce&#34;&gt;Misha Kindulov&lt;/a&gt;, &lt;a href=&#34;https://github.com/dapladoc&#34;&gt;Aron Dik&lt;/a&gt;, &lt;a href=&#34;https://github.com/DaloroAT&#34;&gt;Aleksei Tarasov&lt;/a&gt; and &lt;a href=&#34;https://github.com/leoromanovich&#34;&gt;Verkhovtsev Leonid&lt;/a&gt;.&lt;/p&gt; &#xA;   &lt;p&gt;&lt;a href=&#34;https://www.newyorker.de/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/d/d8/New_Yorker.svg/1280px-New_Yorker.svg.png&#34; width=&#34;100&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;p&gt;I also want to thank NewYorker, since the part of functionality was developed (and used) by its computer vision team led by me.&lt;/p&gt; &#xA;  &lt;/div&gt;&#xA; &lt;/div&gt;&#xA;&lt;/div&gt;</summary>
  </entry>
</feed>