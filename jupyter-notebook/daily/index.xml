<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-27T01:45:53Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>codebasics/deep-learning-keras-tf-tutorial</title>
    <updated>2022-06-27T01:45:53Z</updated>
    <id>tag:github.com,2022-06-27:/codebasics/deep-learning-keras-tf-tutorial</id>
    <link href="https://github.com/codebasics/deep-learning-keras-tf-tutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn deep learning with tensorflow2.0, keras and python through this comprehensive deep learning tutorial series. Learn deep learning from scratch. Deep learning series for beginners. Tensorflow tutorials, tensorflow 2.0 tutorial. deep learning tutorial python.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLeo1K3hjS3uu7CxAacxVndI4bE_o3BDtO&#34;&gt;Deep Learning using Tensorflow 2.0 and Keras&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Learn deep learning with tensorflow2.0, keras and python through this comprehensive deep learning tutorial series. Learn deep learning from scratch. Deep learning series for beginners. Tensorflow tutorials, tensorflow 2.0 tutorial. deep learning tutorial python.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>graykode/nlp-tutorial</title>
    <updated>2022-06-27T01:45:53Z</updated>
    <id>tag:github.com,2022-06-27:/graykode/nlp-tutorial</id>
    <link href="https://github.com/graykode/nlp-tutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Natural Language Processing Tutorial for Deep Learning Researchers&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;nlp-tutorial&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;100&#34; src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/225px-TensorFlowLogo.svg.png&#34;&gt; &lt;img width=&#34;100&#34; src=&#34;https://media-thumbs.golden.com/OLqzmrmwAzY1P7Sl29k2T9WjJdM=/200x200/smart/golden-storage-production.s3.amazonaws.com/topic_images/e08914afa10a4179893eeb07cb5e4713.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;nlp-tutorial&lt;/code&gt; is a tutorial for who is studying NLP(Natural Language Processing) using &lt;strong&gt;Pytorch&lt;/strong&gt;. Most of the models in NLP were implemented with less than &lt;strong&gt;100 lines&lt;/strong&gt; of code.(except comments or blank lines)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[08-14-2020] Old TensorFlow v1 code is archived in &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/archive&#34;&gt;the archive folder&lt;/a&gt;. For beginner readability, only pytorch version 1.0 or higher is supported.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Curriculum - (Example Purpose)&lt;/h2&gt; &#xA;&lt;h4&gt;1. Basic Embedding Model&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/1-1.NNLM&#34;&gt;NNLM(Neural Network Language Model)&lt;/a&gt; - &lt;strong&gt;Predict Next Word&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&#34;&gt;A Neural Probabilistic Language Model(2003)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-1.NNLM/NNLM.ipynb&#34;&gt;NNLM.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;1-2. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/1-2.Word2Vec&#34;&gt;Word2Vec(Skip-gram)&lt;/a&gt; - &lt;strong&gt;Embedding Words and Show Graph&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&#34;&gt;Distributed Representations of Words and Phrases and their Compositionality(2013)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram(Softmax).ipynb&#34;&gt;Word2Vec.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;1-3. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/1-3.FastText&#34;&gt;FastText(Application Level)&lt;/a&gt; - &lt;strong&gt;Sentence Classification&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/pdf/1607.01759.pdf&#34;&gt;Bag of Tricks for Efficient Text Classification(2016)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-3.FastText/FastText.ipynb&#34;&gt;FastText.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. CNN(Convolutional Neural Network)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/2-1.TextCNN&#34;&gt;TextCNN&lt;/a&gt; - &lt;strong&gt;Binary Sentiment Classification&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;http://www.aclweb.org/anthology/D14-1181&#34;&gt;Convolutional Neural Networks for Sentence Classification(2014)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/2-1.TextCNN/TextCNN.ipynb&#34;&gt;TextCNN.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;3. RNN(Recurrent Neural Network)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;3-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/3-1.TextRNN&#34;&gt;TextRNN&lt;/a&gt; - &lt;strong&gt;Predict Next Step&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;http://psych.colorado.edu/~kimlab/Elman1990.pdf&#34;&gt;Finding Structure in Time(1990)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-1.TextRNN/TextRNN.ipynb&#34;&gt;TextRNN.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;3-2. &lt;a href=&#34;https://github.com/graykode/nlp-tutorial/tree/master/3-2.TextLSTM&#34;&gt;TextLSTM&lt;/a&gt; - &lt;strong&gt;Autocomplete&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://www.bioinf.jku.at/publications/older/2604.pdf&#34;&gt;LONG SHORT-TERM MEMORY(1997)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-2.TextLSTM/TextLSTM.ipynb&#34;&gt;TextLSTM.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;3-3. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/3-3.Bi-LSTM&#34;&gt;Bi-LSTM&lt;/a&gt; - &lt;strong&gt;Predict Next Word in Long Sentence&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-3.Bi-LSTM/Bi_LSTM.ipynb&#34;&gt;Bi_LSTM.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;4. Attention Mechanism&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;4-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/4-1.Seq2Seq&#34;&gt;Seq2Seq&lt;/a&gt; - &lt;strong&gt;Change Word&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/pdf/1406.1078.pdf&#34;&gt;Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation(2014)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-1.Seq2Seq/Seq2Seq.ipynb&#34;&gt;Seq2Seq.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;4-2. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/4-2.Seq2Seq(Attention)&#34;&gt;Seq2Seq with Attention&lt;/a&gt; - &lt;strong&gt;Translate&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34;&gt;Neural Machine Translation by Jointly Learning to Align and Translate(2014)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-2.Seq2Seq(Attention)/Seq2Seq(Attention).ipynb&#34;&gt;Seq2Seq(Attention).ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;4-3. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/4-3.Bi-LSTM(Attention)&#34;&gt;Bi-LSTM with Attention&lt;/a&gt; - &lt;strong&gt;Binary Sentiment Classification&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-3.Bi-LSTM(Attention)/Bi_LSTM(Attention).ipynb&#34;&gt;Bi_LSTM(Attention).ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;5. Model based on Transformer&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;5-1. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/5-1.Transformer&#34;&gt;The Transformer&lt;/a&gt; - &lt;strong&gt;Translate&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention Is All You Need(2017)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer.ipynb&#34;&gt;Transformer.ipynb&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer(Greedy_decoder).ipynb&#34;&gt;Transformer(Greedy_decoder).ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;5-2. &lt;a href=&#34;https://raw.githubusercontent.com/graykode/nlp-tutorial/master/5-2.BERT&#34;&gt;BERT&lt;/a&gt; - &lt;strong&gt;Classification Next Sentence &amp;amp; Predict Masked Tokens&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paper - &lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Colab - &lt;a href=&#34;https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.ipynb&#34;&gt;BERT.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.5+&lt;/li&gt; &#xA; &lt;li&gt;Pytorch 1.0.0+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Author&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tae Hwan Jung(Jeff Jung) @graykode&lt;/li&gt; &#xA; &lt;li&gt;Author Email : &lt;a href=&#34;mailto:nlkey2022@gmail.com&#34;&gt;nlkey2022@gmail.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Acknowledgements to &lt;a href=&#34;http://mojitok.com/&#34;&gt;mojitok&lt;/a&gt; as NLP Research Internship.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>trekhleb/homemade-machine-learning</title>
    <updated>2022-06-27T01:45:53Z</updated>
    <id>tag:github.com,2022-06-27:/trekhleb/homemade-machine-learning</id>
    <link href="https://github.com/trekhleb/homemade-machine-learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🤖 Python examples of popular machine learning algorithms with interactive Jupyter demos and math being explained&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Homemade Machine Learning&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🇺🇦 UKRAINE &lt;a href=&#34;https://twitter.com/MFA_Ukraine&#34;&gt;IS BEING ATTACKED&lt;/a&gt; BY RUSSIAN ARMY. CIVILIANS ARE GETTING KILLED. RESIDENTIAL AREAS ARE GETTING BOMBED.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Help Ukraine via &lt;a href=&#34;https://bank.gov.ua/en/news/all/natsionalniy-bank-vidkriv-spetsrahunok-dlya-zboru-koshtiv-na-potrebi-armiyi&#34;&gt;National Bank of Ukraine&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Help Ukraine via &lt;a href=&#34;https://savelife.in.ua/en/donate/&#34;&gt;SaveLife&lt;/a&gt; fund&lt;/li&gt; &#xA;  &lt;li&gt;More info on &lt;a href=&#34;https://war.ukraine.ua/&#34;&gt;war.ukraine.ua&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/trekhleb/homemade-machine-learning/master?filepath=notebooks&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.org/trekhleb/homemade-machine-learning&#34;&gt;&lt;img src=&#34;https://travis-ci.org/trekhleb/homemade-machine-learning.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;You might be interested in 🤖 &lt;a href=&#34;https://github.com/trekhleb/machine-learning-experiments&#34;&gt;Interactive Machine Learning Experiments&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;em&gt;For Octave/MatLab version of this repository please check &lt;a href=&#34;https://github.com/trekhleb/machine-learning-octave&#34;&gt;machine-learning-octave&lt;/a&gt; project.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This repository contains examples of popular machine learning algorithms implemented in &lt;strong&gt;Python&lt;/strong&gt; with mathematics behind them being explained. Each algorithm has interactive &lt;strong&gt;Jupyter Notebook&lt;/strong&gt; demo that allows you to play with training data, algorithms configurations and immediately see the results, charts and predictions &lt;strong&gt;right in your browser&lt;/strong&gt;. In most cases the explanations are based on &lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34;&gt;this great machine learning course&lt;/a&gt; by Andrew Ng.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The purpose of this repository is &lt;em&gt;not&lt;/em&gt; to implement machine learning algorithms by using 3&lt;sup&gt;rd&lt;/sup&gt; party library one-liners &lt;em&gt;but&lt;/em&gt; rather to practice implementing these algorithms from scratch and get better understanding of the mathematics behind each algorithm. That&#39;s why all algorithms implementations are called &#34;homemade&#34; and not intended to be used for production.&lt;/p&gt; &#xA;&lt;h2&gt;Supervised Learning&lt;/h2&gt; &#xA;&lt;p&gt;In supervised learning we have a set of training data as an input and a set of labels or &#34;correct answers&#34; for each training set as an output. Then we&#39;re training our model (machine learning algorithm parameters) to map the input to the output correctly (to do correct prediction). The ultimate purpose is to find such model parameters that will successfully continue correct &lt;em&gt;input→output&lt;/em&gt; mapping (predictions) even for new input examples.&lt;/p&gt; &#xA;&lt;h3&gt;Regression&lt;/h3&gt; &#xA;&lt;p&gt;In regression problems we do real value predictions. Basically we try to draw a line/plane/n-dimensional plane along the training examples.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Usage examples: stock price forecast, sales analysis, dependency of any number, etc.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;🤖 Linear Regression&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📗 &lt;a href=&#34;https://raw.githubusercontent.com/trekhleb/homemade-machine-learning/master/homemade/linear_regression&#34;&gt;Math | Linear Regression&lt;/a&gt; - theory and links for further readings&lt;/li&gt; &#xA; &lt;li&gt;⚙️ &lt;a href=&#34;https://raw.githubusercontent.com/trekhleb/homemade-machine-learning/master/homemade/linear_regression/linear_regression.py&#34;&gt;Code | Linear Regression&lt;/a&gt; - implementation example&lt;/li&gt; &#xA; &lt;li&gt;▶️ &lt;a href=&#34;https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/univariate_linear_regression_demo.ipynb&#34;&gt;Demo | Univariate Linear Regression&lt;/a&gt; - predict &lt;code&gt;country happiness&lt;/code&gt; score by &lt;code&gt;economy GDP&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;▶️ &lt;a href=&#34;https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/multivariate_linear_regression_demo.ipynb&#34;&gt;Demo | Multivariate Linear Regression&lt;/a&gt; - predict &lt;code&gt;country happiness&lt;/code&gt; score by &lt;code&gt;economy GDP&lt;/code&gt; and &lt;code&gt;freedom index&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;▶️ &lt;a href=&#34;https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/non_linear_regression_demo.ipynb&#34;&gt;Demo | Non-linear Regression&lt;/a&gt; - use linear regression with &lt;em&gt;polynomial&lt;/em&gt; and &lt;em&gt;sinusoid&lt;/em&gt; features to predict non-linear dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Classification&lt;/h3&gt; &#xA;&lt;p&gt;In classification problems we split input examples by certain characteristic.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Usage examples: spam-filters, language detection, finding similar documents, handwritten letters recognition, etc.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;🤖 Logistic Regression&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📗 &lt;a href=&#34;https://raw.githubusercontent.com/trekhleb/homemade-machine-learning/master/homemade/logistic_regression&#34;&gt;Math | Logistic Regression&lt;/a&gt; - theory and links for further readings&lt;/li&gt; &#xA; &lt;li&gt;⚙️ &lt;a href=&#34;https://raw.githubusercontent.com/trekhleb/homemade-machine-learning/master/homemade/logistic_regression/logistic_regression.py&#34;&gt;Code | Logistic Regression&lt;/a&gt; - implementation example&lt;/li&gt; &#xA; &lt;li&gt;▶️ &lt;a href=&#34;https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/logistic_regression_with_linear_boundary_demo.ipynb&#34;&gt;Demo | Logistic Regression (Linear Boundary)&lt;/a&gt; - predict Iris flower &lt;code&gt;class&lt;/code&gt; based on &lt;code&gt;petal_length&lt;/code&gt; and &lt;code&gt;petal_width&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;▶️ &lt;a href=&#34;https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/logistic_regression_with_non_linear_boundary_demo.ipynb&#34;&gt;Demo | Logistic Regression (Non-Linear Boundary)&lt;/a&gt; - predict microchip &lt;code&gt;validity&lt;/code&gt; based on &lt;code&gt;param_1&lt;/code&gt; and &lt;code&gt;param_2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;▶️ &lt;a href=&#34;https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/multivariate_logistic_regression_demo.ipynb&#34;&gt;Demo | Multivariate Logistic Regression | MNIST&lt;/a&gt; - recognize handwritten digits from &lt;code&gt;28x28&lt;/code&gt; pixel images&lt;/li&gt; &#xA; &lt;li&gt;▶️ &lt;a href=&#34;https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/multivariate_logistic_regression_fashion_demo.ipynb&#34;&gt;Demo | Multivariate Logistic Regression | Fashion MNIST&lt;/a&gt; - recognize clothes types from &lt;code&gt;28x28&lt;/code&gt; pixel images&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Unsupervised Learning&lt;/h2&gt; &#xA;&lt;p&gt;Unsupervised learning is a branch of machine learning that learns from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data.&lt;/p&gt; &#xA;&lt;h3&gt;Clustering&lt;/h3&gt; &#xA;&lt;p&gt;In clustering problems we split the training examples by unknown characteristics. The algorithm itself decides what characteristic to use for splitting.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Usage examples: market segmentation, social networks analysis, organize computing clusters, astronomical data analysis, image compression, etc.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;🤖 K-means Algorithm&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📗 &lt;a href=&#34;https://raw.githubusercontent.com/trekhleb/homemade-machine-learning/master/homemade/k_means&#34;&gt;Math | K-means Algorithm&lt;/a&gt; - theory and links for further readings&lt;/li&gt; &#xA; &lt;li&gt;⚙️ &lt;a href=&#34;https://raw.githubusercontent.com/trekhleb/homemade-machine-learning/master/homemade/k_means/k_means.py&#34;&gt;Code | K-means Algorithm&lt;/a&gt; - implementation example&lt;/li&gt; &#xA; &lt;li&gt;▶️ &lt;a href=&#34;https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/k_means/k_means_demo.ipynb&#34;&gt;Demo | K-means Algorithm&lt;/a&gt; - split Iris flowers into clusters based on &lt;code&gt;petal_length&lt;/code&gt; and &lt;code&gt;petal_width&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Anomaly Detection&lt;/h3&gt; &#xA;&lt;p&gt;Anomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Usage examples: intrusion detection, fraud detection, system health monitoring, removing anomalous data from the dataset etc.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;🤖 Anomaly Detection using Gaussian Distribution&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📗 &lt;a href=&#34;https://raw.githubusercontent.com/trekhleb/homemade-machine-learning/master/homemade/anomaly_detection&#34;&gt;Math | Anomaly Detection using Gaussian Distribution&lt;/a&gt; - theory and links for further readings&lt;/li&gt; &#xA; &lt;li&gt;⚙️ &lt;a href=&#34;https://raw.githubusercontent.com/trekhleb/homemade-machine-learning/master/homemade/anomaly_detection/gaussian_anomaly_detection.py&#34;&gt;Code | Anomaly Detection using Gaussian Distribution&lt;/a&gt; - implementation example&lt;/li&gt; &#xA; &lt;li&gt;▶️ &lt;a href=&#34;https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/anomaly_detection/anomaly_detection_gaussian_demo.ipynb&#34;&gt;Demo | Anomaly Detection&lt;/a&gt; - find anomalies in server operational parameters like &lt;code&gt;latency&lt;/code&gt; and &lt;code&gt;threshold&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Neural Network (NN)&lt;/h2&gt; &#xA;&lt;p&gt;The neural network itself isn&#39;t an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Usage examples: as a substitute of all other algorithms in general, image recognition, voice recognition, image processing (applying specific style), language translation, etc.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;🤖 Multilayer Perceptron (MLP)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📗 &lt;a href=&#34;https://raw.githubusercontent.com/trekhleb/homemade-machine-learning/master/homemade/neural_network&#34;&gt;Math | Multilayer Perceptron&lt;/a&gt; - theory and links for further readings&lt;/li&gt; &#xA; &lt;li&gt;⚙️ &lt;a href=&#34;https://raw.githubusercontent.com/trekhleb/homemade-machine-learning/master/homemade/neural_network/multilayer_perceptron.py&#34;&gt;Code | Multilayer Perceptron&lt;/a&gt; - implementation example&lt;/li&gt; &#xA; &lt;li&gt;▶️ &lt;a href=&#34;https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/neural_network/multilayer_perceptron_demo.ipynb&#34;&gt;Demo | Multilayer Perceptron | MNIST&lt;/a&gt; - recognize handwritten digits from &lt;code&gt;28x28&lt;/code&gt; pixel images&lt;/li&gt; &#xA; &lt;li&gt;▶️ &lt;a href=&#34;https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/neural_network/multilayer_perceptron_fashion_demo.ipynb&#34;&gt;Demo | Multilayer Perceptron | Fashion MNIST&lt;/a&gt; - recognize the type of clothes from &lt;code&gt;28x28&lt;/code&gt; pixel images&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Machine Learning Map&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/trekhleb/homemade-machine-learning/master/images/machine-learning-map.png&#34; alt=&#34;Machine Learning Map&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The source of the following machine learning topics map is &lt;a href=&#34;https://vas3k.ru/blog/machine_learning/&#34;&gt;this wonderful blog post&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;h4&gt;Installing Python&lt;/h4&gt; &#xA;&lt;p&gt;Make sure that you have &lt;a href=&#34;https://realpython.com/installing-python/&#34;&gt;Python installed&lt;/a&gt; on your machine.&lt;/p&gt; &#xA;&lt;p&gt;You might want to use &lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34;&gt;venv&lt;/a&gt; standard Python library to create virtual environments and have Python, &lt;code&gt;pip&lt;/code&gt; and all dependent packages to be installed and served from the local project directory to avoid messing with system wide packages and their versions.&lt;/p&gt; &#xA;&lt;h4&gt;Installing Dependencies&lt;/h4&gt; &#xA;&lt;p&gt;Install all dependencies that are required for the project by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launching Jupyter Locally&lt;/h4&gt; &#xA;&lt;p&gt;All demos in the project may be run directly in your browser without installing Jupyter locally. But if you want to launch &lt;a href=&#34;http://jupyter.org/&#34;&gt;Jupyter Notebook&lt;/a&gt; locally you may do it by running the following command from the root folder of the project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After this Jupyter Notebook will be accessible by &lt;code&gt;http://localhost:8888&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Launching Jupyter Remotely&lt;/h4&gt; &#xA;&lt;p&gt;Each algorithm section contains demo links to &lt;a href=&#34;http://nbviewer.jupyter.org/&#34;&gt;Jupyter NBViewer&lt;/a&gt;. This is fast online previewer for Jupyter notebooks where you may see demo code, charts and data right in your browser without installing anything locally. In case if you want to &lt;em&gt;change&lt;/em&gt; the code and &lt;em&gt;experiment&lt;/em&gt; with demo notebook you need to launch the notebook in &lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt;. You may do it by simply clicking the &lt;em&gt;&#34;Execute on Binder&#34;&lt;/em&gt; link in top right corner of the NBViewer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/trekhleb/homemade-machine-learning/master/images/binder-button-place.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;The list of datasets that is being used for Jupyter Notebook demos may be found in &lt;a href=&#34;https://raw.githubusercontent.com/trekhleb/homemade-machine-learning/master/data&#34;&gt;data folder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Supporting the project&lt;/h2&gt; &#xA;&lt;p&gt;You may support this project via ❤️️ &lt;a href=&#34;https://github.com/sponsors/trekhleb&#34;&gt;GitHub&lt;/a&gt; or ❤️️ &lt;a href=&#34;https://www.patreon.com/trekhleb&#34;&gt;Patreon&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>