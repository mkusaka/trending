<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-16T01:33:17Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>weixi-feng/Structured-Diffusion-Guidance</title>
    <updated>2022-12-16T01:33:17Z</updated>
    <id>tag:github.com,2022-12-16:/weixi-feng/Structured-Diffusion-Guidance</id>
    <link href="https://github.com/weixi-feng/Structured-Diffusion-Guidance" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Structured Diffusion Guidance&lt;/h1&gt; &#xA;&lt;h2&gt;We propose a method to fuse language structures into diffusion guidance for compositionality text-to-image generation.&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://weixi-feng.github.io/structure-diffusion-guidance/&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/&#34;&gt;Paper&lt;/a&gt; | [Google Colab](Coming Soon)&lt;/h3&gt; &#xA;&lt;!-- [![][colab]][composable-demo] [![][huggingface]][huggingface-demo] --&gt; &#xA;&lt;p&gt;This is the official codebase for &lt;strong&gt;Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://weixi-feng.github.io/structure-diffusion-guidance/&#34;&gt;Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://weixi-feng.github.io/&#34;&gt;Weixi Feng&lt;/a&gt; &lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=kDzxOzUAAAAJ&amp;amp;&#34;&gt;Xuehai He&lt;/a&gt; &lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://tsujuifu.github.io/&#34;&gt;Tsu-Jui Fu&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://varunjampani.github.io/&#34;&gt;Varun Jampani&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://www.arjunakula.com/&#34;&gt;Arjun Akula&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=BV2dbjEAAAAJ&amp;amp;&#34;&gt;Pradyumna Narayana&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://sites.google.com/site/sugatobasu/&#34;&gt;Sugato Basu&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://eric-xw.github.io/&#34;&gt;Xin Eric Wang&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://sites.cs.ucsb.edu/~william/&#34;&gt;William Yang Wang&lt;/a&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;br&gt; &lt;sup&gt;1&lt;/sup&gt;UCSB, &lt;sup&gt;2&lt;/sup&gt;UCSC, &lt;sup&gt;3&lt;/sup&gt;Google &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Clone this repository and then create a conda environment with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate structure_diffusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you already have a &lt;a href=&#34;https://github.com/CompVis/stable-diffusion/&#34;&gt;stable diffusion&lt;/a&gt; environment, you can run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install stanza nltk scenegraphparser tqdm matplotlib&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;This repository supports stable diffusion 1.4 for now. Please refer to the official &lt;a href=&#34;https://github.com/CompVis/stable-diffusion/#weights&#34;&gt;stable-diffusion&lt;/a&gt; repository to download the pre-trained model and put it under &lt;code&gt;models/ldm/stable-diffusion-v1/&lt;/code&gt;. Our method is training-free and can be applied to the trained stable diffusion checkpoint directly.&lt;/p&gt; &#xA;&lt;p&gt;To generate an image, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img_demo.py --prompt &#34;A red teddy bear in a christmas hat sitting next to a glass&#34; --plms --parser_type constituency&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the guidance scale is set to 7.5 and output image size is 512x512. We only support PLMS sampling and batch size equals to 1 for now. Apart from the default arguments from &lt;a href=&#34;https://github.com/CompVis/stable-diffusion/raw/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/scripts/txt2img.py&#34;&gt;Stable Diffusion&lt;/a&gt;, we add &lt;code&gt;--parser_type&lt;/code&gt; and &lt;code&gt;--conjunction&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: txt2img_demo.py [-h] [--prompt [PROMPT]] ...&#xA;                       [--parser_type {constituency,scene_graph}] [--conjunction] [--save_attn_maps]&#xA;&#xA;optional arguments:&#xA;    ...&#xA;  --parser_type {constituency,scene_graph}&#xA;  --conjunction         If True, the input prompt is a conjunction of two concepts like &#34;A and B&#34;&#xA;  --save_attn_maps      If True, the attention maps will be saved as a .pth file with the name same as the image&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Without specifying the &lt;code&gt;conjunction&lt;/code&gt; argument, the model applies one &lt;code&gt;key&lt;/code&gt; and multiple &lt;code&gt;values&lt;/code&gt; for each cross-attention layer. For concept conjunction prompts, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img_demo.py --prompt &#34;A red car and a white sheep&#34; --plms --parser_type constituency --conjunction&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Overall, compositional prompts remains a challenge for Stable Diffusion v1.4. It may still take several attempts to get a correct image with our method. The improvement is system-level instead of sample-level, and we are still looking for good evaluation metrics for compositional T2I synthesis. We observe less missing objects in &lt;a href=&#34;https://github.com/Stability-AI/stablediffusion&#34;&gt;Stable Diffusion v2&lt;/a&gt;, and we are implementing our method on top of it as well. Please feel free to reach out for a discussion.&lt;/p&gt; &#xA;&lt;h2&gt;Comments&lt;/h2&gt; &#xA;&lt;p&gt;Our codebase builds heavily on &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;. Thanks for open-sourcing!&lt;/p&gt; &#xA;&lt;h2&gt;Citing our Paper&lt;/h2&gt; &#xA;&lt;p&gt;If you find our code or paper useful for your research, please consider citing (Coming soon)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>