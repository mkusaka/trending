<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-03T01:43:20Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nicknochnack/TFODCourse</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/nicknochnack/TFODCourse</id>
    <link href="https://github.com/nicknochnack/TFODCourse" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tensorflow Object Detection Walkthrough&lt;/h1&gt; &#xA;&lt;p&gt;This set of Notebooks provides a complete set of code to be able to train and leverage your own custom object detection model using the Tensorflow Object Detection API. This accompanies the Tensorflow Object Detection course on my &lt;a href=&#34;https://www.youtube.com/c/nicholasrenotte&#34;&gt;YouTube channel&lt;/a&gt;. &lt;img src=&#34;https://i.imgur.com/H3tUyKM.png&#34;&gt; &lt;/p&gt;&#xA;&lt;h2&gt;Steps&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 1.&lt;/b&gt; Clone this repository: https://github.com/nicknochnack/TFODCourse &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 2.&lt;/b&gt; Create a new virtual environment &#xA;&lt;pre&gt;&#xA;python -m venv tfod&#xA;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 3.&lt;/b&gt; Activate your virtual environment &#xA;&lt;pre&gt;&#xA;source tfod/bin/activate # Linux&#xA;.\tfod\Scripts\activate # Windows &#xA;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 4.&lt;/b&gt; Install dependencies and add virtual environment to the Python Kernel &#xA;&lt;pre&gt;&#xA;python -m pip install --upgrade pip&#xA;pip install ipykernel&#xA;python -m ipykernel install --user --name=tfodj&#xA;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 5.&lt;/b&gt; Collect images using the Notebook &#xA;&lt;a href=&#34;https://github.com/nicknochnack/TFODCourse/raw/main/1.%20Image%20Collection.ipynb&#34;&gt;1. Image Collection.ipynb&lt;/a&gt; - ensure you change the kernel to the virtual environment as shown below &#xA;&lt;img src=&#34;https://i.imgur.com/8yac6Xl.png&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 6.&lt;/b&gt; Manually divide collected images into two folders train and test. So now all folders and annotations should be split between the following two folders. &#xA;&lt;br&gt; \TFODCourse\Tensorflow\workspace\images\train&#xA;&lt;br&gt; \TFODCourse\Tensorflow\workspace\images\test &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 7.&lt;/b&gt; Begin training process by opening &#xA;&lt;a href=&#34;https://github.com/nicknochnack/TFODCourse/raw/main/2.%20Training%20and%20Detection.ipynb&#34;&gt;2. Training and Detection.ipynb&lt;/a&gt;, this notebook will walk you through installing Tensorflow Object Detection, making detections, saving and exporting your model. &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 8.&lt;/b&gt; During this process the Notebook will install Tensorflow Object Detection. You should ideally receive a notification indicating that the API has installed successfully at Step 8 with the last line stating OK. &#xA;&lt;img src=&#34;https://i.imgur.com/FSQFo16.png&#34;&gt; If not, resolve installation errors by referring to the &#xA;&lt;a href=&#34;https://github.com/nicknochnack/TFODCourse/raw/main/README.md&#34;&gt;Error Guide.md&lt;/a&gt; in this folder. &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 9.&lt;/b&gt; Once you get to step 6. Train the model, inside of the notebook, you may choose to train the model from within the notebook. I have noticed however that training inside of a separate terminal on a Windows machine you&#39;re able to display live loss metrics. &#xA;&lt;img src=&#34;https://i.imgur.com/K0wLO57.png&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;b&gt;Step 10.&lt;/b&gt; You can optionally evaluate your model inside of Tensorboard. Once the model has been trained and you have run the evaluation command under Step 7. Navigate to the evaluation folder for your trained model e.g. &#xA;&lt;pre&gt; cd Tensorlfow/workspace/models/my_ssd_mobnet/eval&lt;/pre&gt; and open Tensorboard with the following command &#xA;&lt;pre&gt;tensorboard --logdir=. &lt;/pre&gt; Tensorboard will be accessible through your browser and you will be able to see metrics including mAP - mean Average Precision, and Recall. &#xA;&lt;br&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/NeMo</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/NVIDIA/NeMo</id>
    <link href="https://github.com/NVIDIA/NeMo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NeMo: a toolkit for conversational AI&lt;/p&gt;&lt;hr&gt;&lt;p&gt;|status| |documentation| |license| |lgtm_grade| |lgtm_alerts| |black|&lt;/p&gt; &#xA;&lt;p&gt;.. |status| image:: &lt;a href=&#34;http://www.repostatus.org/badges/latest/active.svg&#34;&gt;http://www.repostatus.org/badges/latest/active.svg&lt;/a&gt; :target: &lt;a href=&#34;http://www.repostatus.org/#active&#34;&gt;http://www.repostatus.org/#active&lt;/a&gt; :alt: Project Status: Active – The project has reached a stable, usable state and is being actively developed.&lt;/p&gt; &#xA;&lt;p&gt;.. |documentation| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&lt;/a&gt; :alt: Documentation :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |license| image:: &lt;a href=&#34;https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg&#34;&gt;https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/NVIDIA/NeMo/raw/master/LICENSE&#34;&gt;https://github.com/NVIDIA/NeMo/blob/master/LICENSE&lt;/a&gt; :alt: NeMo core license and license for collections in this repo&lt;/p&gt; &#xA;&lt;p&gt;.. |lgtm_grade| image:: &lt;a href=&#34;https://img.shields.io/lgtm/grade/python/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&#34;&gt;https://img.shields.io/lgtm/grade/python/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&lt;/a&gt; :target: &lt;a href=&#34;https://lgtm.com/projects/g/NVIDIA/NeMo/context:python&#34;&gt;https://lgtm.com/projects/g/NVIDIA/NeMo/context:python&lt;/a&gt; :alt: Language grade: Python&lt;/p&gt; &#xA;&lt;p&gt;.. |lgtm_alerts| image:: &lt;a href=&#34;https://img.shields.io/lgtm/alerts/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&#34;&gt;https://img.shields.io/lgtm/alerts/g/NVIDIA/NeMo.svg?logo=lgtm&amp;amp;logoWidth=18&lt;/a&gt; :target: &lt;a href=&#34;https://lgtm.com/projects/g/NVIDIA/NeMo/alerts/&#34;&gt;https://lgtm.com/projects/g/NVIDIA/NeMo/alerts/&lt;/a&gt; :alt: Total alerts&lt;/p&gt; &#xA;&lt;p&gt;.. |black| image:: &lt;a href=&#34;https://img.shields.io/badge/code%20style-black-000000.svg&#34;&gt;https://img.shields.io/badge/code%20style-black-000000.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/psf/black&#34;&gt;https://github.com/psf/black&lt;/a&gt; :alt: Code style: black&lt;/p&gt; &#xA;&lt;p&gt;.. _main-readme:&lt;/p&gt; &#xA;&lt;h1&gt;&lt;strong&gt;NVIDIA NeMo&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech synthesis (TTS). The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new &lt;code&gt;conversational AI models &amp;lt;https://developer.nvidia.com/conversational-ai#started&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Pre-trained NeMo models. &amp;lt;https://catalog.ngc.nvidia.com/models?query=nemo&amp;amp;orderBy=weightPopularDESC&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Introductory video. &amp;lt;https://www.youtube.com/embed/wBgpMf_KQVw&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speech processing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Automatic Speech Recognition (ASR) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/intro.html&amp;gt;&lt;/code&gt;_ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Supported models: Jasper, QuartzNet, CitriNet, Conformer-CTC, Conformer-Transducer, ContextNet, LSTM-Transducer (RNNT), LSTM-CTC, ...&lt;/li&gt; &#xA;     &lt;li&gt;Supports CTC and Transducer/RNNT losses/decoders&lt;/li&gt; &#xA;     &lt;li&gt;Beam Search decoding&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;Language Modelling for ASR &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/asr_language_modeling.html&amp;gt;&lt;/code&gt;_: N-gram LM in fusion with Beam Search decoding, Neural Rescoring with Transformer&lt;/li&gt; &#xA;     &lt;li&gt;Streaming and Buffered ASR (CTC/Transducer) - &lt;code&gt;Chunked Inference Examples &amp;lt;https://github.com/NVIDIA/NeMo/tree/main/examples/asr/asr_chunked_inference&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speech Classification and Speech Command Recognition &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speech_classification/intro.html&amp;gt;&lt;/code&gt;_: MatchboxNet (Command Recognition)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Voice activity Detection (VAD) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/speech_classification/models.html#marblenet-vad&amp;gt;&lt;/code&gt;_: MarbleNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speaker Recognition &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speaker_recognition/intro.html&amp;gt;&lt;/code&gt;_: TitaNet, ECAPA_TDNN, SpeakerNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speaker Diarization &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/speaker_diarization/intro.html&amp;gt;&lt;/code&gt;_: TitaNet, ECAPA_TDNN, SpeakerNet&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Pretrained models on different languages. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_asr&amp;gt;&lt;/code&gt;_: English, Spanish, German, Russian, Chinese, French, Italian, Polish, ...&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained speech processing models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_asr&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Natural Language Processing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;NeMo Megatron pre-training of Large Language Models &amp;lt;https://developer.nvidia.com/nemo-megatron-early-access&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Neural Machine Translation (NMT) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/machine_translation.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Punctuation and Capitalization &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/punctuation_and_capitalization.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Token classification (named entity recognition) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/token_classification.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Text classification &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/text_classification.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Joint Intent and Slot Classification &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/joint_intent_slot.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Question answering &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/question_answering.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;GLUE benchmark &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/glue_benchmark.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Information retrieval &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/information_retrieval.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Entity Linking &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/entity_linking.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Dialogue State Tracking &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/sgd_qa.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Prompt Tuning &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/prompt_learning.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained NLP models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_nlp&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Speech synthesis (TTS) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tts/intro.html#&amp;gt;&lt;/code&gt;_ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Spectrogram generation: Tacotron2, GlowTTS, TalkNet, FastPitch, FastSpeech2, Mixer-TTS, Mixer-TTS-X&lt;/li&gt; &#xA;   &lt;li&gt;Vocoders: WaveGlow, SqueezeWave, UniGlow, MelGAN, HiFiGAN, UnivNet&lt;/li&gt; &#xA;   &lt;li&gt;End-to-end speech generation: FastPitch_HifiGan_E2E, FastSpeech2_HifiGan_E2E&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;NGC collection of pre-trained TTS models. &amp;lt;https://ngc.nvidia.com/catalog/collections/nvidia:nemo_tts&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Tools &amp;lt;https://github.com/NVIDIA/NeMo/tree/main/tools&amp;gt;&lt;/code&gt;_ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Text Processing (text normalization and inverse text normalization) &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/text_normalization/intro.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;CTC-Segmentation tool &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tools/ctc_segmentation.html&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Speech Data Explorer &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/tools/speech_data_explorer.html&amp;gt;&lt;/code&gt;_: a dash-based tool for interactive exploration of ASR/TTS datasets&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Built for speed, NeMo can utilize NVIDIA&#39;s Tensor Cores and scale out training to multiple GPUs and multiple nodes.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python 3.8 or above&lt;/li&gt; &#xA; &lt;li&gt;Pytorch 1.10.0 or above&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU for training&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;.. |main| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=main&lt;/a&gt; :alt: Documentation Status :scale: 100% :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |stable| image:: &lt;a href=&#34;https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable&#34;&gt;https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable&lt;/a&gt; :alt: Documentation Status :scale: 100% :target: &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&#34;&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;+---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+ | Version | Status | Description | +=========+=============+==========================================================================================================================================+ | Latest | |main| | &lt;code&gt;Documentation of the latest (i.e. main) branch. &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/&amp;gt;&lt;/code&gt;_ | +---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+ | Stable | |stable| | &lt;code&gt;Documentation of the stable (i.e. most recent release) branch. &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/&amp;gt;&lt;/code&gt;_ | +---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------+&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;A great way to start with NeMo is by checking &lt;code&gt;one of our tutorials &amp;lt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h2&gt;Getting help with NeMo&lt;/h2&gt; &#xA;&lt;p&gt;FAQ can be found on NeMo&#39;s &lt;code&gt;Discussions board &amp;lt;https://github.com/NVIDIA/NeMo/discussions&amp;gt;&lt;/code&gt;_. You are welcome to ask questions or start discussions there.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Conda&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;We recommend installing NeMo in a fresh Conda environment.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    conda create --name nemo python==3.8&#xA;    conda activate nemo&#xA;&#xA;Install PyTorch using their `configurator &amp;lt;https://pytorch.org/get-started/locally/&amp;gt;`_. &#xA;&#xA;.. code-block:: bash&#xA;&#xA;    conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch&#xA;&#xA;.. note::&#xA;&#xA;  The command used to install PyTorch may depend on your system.&#xA;&#xA;Pip&#xA;~~~&#xA;Use this installation mode if you want the latest released version.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;    pip install Cython&#xA;    pip install nemo_toolkit[&#39;all&#39;]&#xA;&#xA;.. note::&#xA;&#xA;    Depending on the shell used, you may need to use ``&#34;nemo_toolkit[all]&#34;`` instead in the above command.&#xA;&#xA;Pip from source&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use this installation mode if you want the a version from particular GitHub branch (e.g main).&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;pip install Cython&#xA;python -m pip install git+https://github.com/NVIDIA/NeMo.git@{BRANCH}#egg=nemo_toolkit[all]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From source&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Use this installation mode if you are contributing to NeMo.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    apt-get update &amp;amp;&amp;amp; apt-get install -y libsndfile1 ffmpeg&#xA;    git clone https://github.com/NVIDIA/NeMo&#xA;    cd NeMo&#xA;    ./reinstall.sh&#xA;&#xA;.. note::&#xA;&#xA;    If you only want the toolkit without additional conda-based dependencies, you may replace ``reinstall.sh``&#xA;    with ``pip install -e .`` when your PWD is the root of the NeMo repository.&#xA;&#xA;RNNT&#xA;~~~~&#xA;Note that RNNT requires numba to be installed from conda.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;  conda remove numba&#xA;  pip uninstall numba&#xA;  conda install -c conda-forge numba&#xA;&#xA;Megatron GPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Megatron GPT training requires NVIDIA Apex to be installed.&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: bash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/NVIDIA/apex&#xA;cd apex&#xA;git checkout 9263bc8c6c16555bd55dd759f1a1b8c0cd187d10&#xA;pip install -v --disable-pip-version-check --no-cache-dir --global-option=&#34;--cpp_ext&#34; --global-option=&#34;--cuda_ext&#34; --global-option=&#34;--fast_layer_norm&#34; ./&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker containers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;To build a nemo container with Dockerfile from a branch, please run &#xA;&#xA;.. code-block:: bash&#xA;&#xA;    DOCKER_BUILDKIT=1 docker build -f Dockerfile -t nemo:latest .&#xA;&#xA;&#xA;If you chose to work with main branch, we recommend using NVIDIA&#39;s PyTorch container version 22.04-py3 and then installing from GitHub.&#xA;&#xA;.. code-block:: bash&#xA;&#xA;    docker run --gpus all -it --rm -v &amp;lt;nemo_github_folder&amp;gt;:/NeMo --shm-size=8g \&#xA;    -p 8888:8888 -p 6006:6006 --ulimit memlock=-1 --ulimit \&#xA;    stack=67108864 --device=/dev/snd nvcr.io/nvidia/pytorch:22.04-py3&#xA;&#xA;Examples&#xA;--------&#xA;&#xA;Many examples can be found under `&#34;Examples&#34; &amp;lt;https://github.com/NVIDIA/NeMo/tree/stable/examples&amp;gt;`_ folder.&#xA;&#xA;&#xA;Contributing&#xA;------------&#xA;&#xA;We welcome community contributions! Please refer to the  `CONTRIBUTING.md &amp;lt;https://github.com/NVIDIA/NeMo/blob/stable/CONTRIBUTING.md&amp;gt;`_ CONTRIBUTING.md for the process.&#xA;&#xA;Publications&#xA;------------&#xA;&#xA;We provide an ever growing list of publications that utilize the NeMo framework. Please refer to `PUBLICATIONS.md &amp;lt;https://github.com/NVIDIA/NeMo/blob/main/PUBLICATIONS.md&amp;gt;`_. We welcome the addition of your own articles to this list !&#xA;&#xA;Citation&#xA;--------&#xA;&#xA;.. code-block:: bash&#xA;&#xA;  @article{kuchaiev2019nemo,&#xA;    title={Nemo: a toolkit for building ai applications using neural modules},&#xA;    author={Kuchaiev, Oleksii and Li, Jason and Nguyen, Huyen and Hrinchuk, Oleksii and Leary, Ryan and Ginsburg, Boris and Kriman, Samuel and Beliaev, Stanislav and Lavrukhin, Vitaly and Cook, Jack and others},&#xA;    journal={arXiv preprint arXiv:1909.09577},&#xA;    year={2019}&#xA;  }&#xA;&#xA;License&#xA;-------&#xA;NeMo is under `Apache 2.0 license &amp;lt;https://github.com/NVIDIA/NeMo/blob/stable/LICENSE&amp;gt;`_.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>datawhalechina/thorough-pytorch</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/datawhalechina/thorough-pytorch</id>
    <link href="https://github.com/datawhalechina/thorough-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch入门教程，在线阅读地址：https://datawhalechina.github.io/thorough-pytorch/&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;深入浅出PyTorch&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;在线阅读地址&lt;/strong&gt;：&lt;a href=&#34;https://datawhalechina.github.io/thorough-pytorch/&#34;&gt;https://datawhalechina.github.io/thorough-pytorch/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;配套视频教程&lt;/strong&gt;：&lt;a href=&#34;https://www.bilibili.com/video/BV1L44y1472Z&#34;&gt;https://www.bilibili.com/video/BV1L44y1472Z&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;一、项目初衷&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch是利用深度学习进行数据科学研究的重要工具，在灵活性、可读性和性能上都具备相当的优势，近年来已成为学术界实现深度学习算法最常用的框架。&lt;/p&gt; &#xA;&lt;p&gt;考虑到PyTorch的学习兼具理论储备和动手训练，两手都要抓两手都要硬的特点，我们开发了《深入浅出PyTorch》课程，期望以组队学习的形式，帮助大家从入门到熟练掌握PyTorch工具，进而实现自己的深度学习算法。&lt;/p&gt; &#xA;&lt;p&gt;我们的愿景是：通过组队学习，大家能够掌握由浅入深地PyTorch的基本知识和内容，经过自己的动手实践加深操作的熟练度。同时通过项目实战，充分锻炼编程能力，掌握PyTorch进行深度学习的基本流程，提升解决实际问题的能力。&lt;/p&gt; &#xA;&lt;p&gt;学习的先修要求是，会使用Python编程，了解包括神经网络在内的机器学习算法，勤于动手实践。&lt;/p&gt; &#xA;&lt;p&gt;《深入浅出PyTorch》是一个系列，一共有三个部分。已经上线的是本系列的第一、二部分，后续会不断更新《深入浅出PyTorch》（下），给出更贴合实际应用的实战案例。&lt;/p&gt; &#xA;&lt;h2&gt;二、内容设置（已上线部分）&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;第一章：PyTorch的简介和安装 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;PyTorch简介&lt;/li&gt; &#xA;   &lt;li&gt;PyTorch的安装&lt;/li&gt; &#xA;   &lt;li&gt;PyTorch相关资源简介&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;第二章：PyTorch基础知识 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;张量及其运算&lt;/li&gt; &#xA;   &lt;li&gt;自动求导简介&lt;/li&gt; &#xA;   &lt;li&gt;并行计算、CUDA和cuDNN简介&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;第三章：PyTorch的主要组成模块 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;思考：完成一套深度学习流程需要哪些关键环节&lt;/li&gt; &#xA;   &lt;li&gt;基本配置&lt;/li&gt; &#xA;   &lt;li&gt;数据读入&lt;/li&gt; &#xA;   &lt;li&gt;模型构建&lt;/li&gt; &#xA;   &lt;li&gt;损失函数&lt;/li&gt; &#xA;   &lt;li&gt;优化器&lt;/li&gt; &#xA;   &lt;li&gt;训练和评估&lt;/li&gt; &#xA;   &lt;li&gt;可视化&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;第四章：PyTorch基础实战 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;基础实战——Fashion-MNIST时装分类&lt;/li&gt; &#xA;   &lt;li&gt;基础实战——果蔬分类实战（notebook）&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;第五章：PyTorch模型定义 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;模型定义方式&lt;/li&gt; &#xA;   &lt;li&gt;利用模型块快速搭建复杂网络&lt;/li&gt; &#xA;   &lt;li&gt;模型修改&lt;/li&gt; &#xA;   &lt;li&gt;模型保存与读取&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;第六章：PyTorch进阶训练技巧 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;自定义损失函数&lt;/li&gt; &#xA;   &lt;li&gt;动态调整学习率&lt;/li&gt; &#xA;   &lt;li&gt;模型微调&lt;/li&gt; &#xA;   &lt;li&gt;半精度训练&lt;/li&gt; &#xA;   &lt;li&gt;数据扩充&lt;/li&gt; &#xA;   &lt;li&gt;超参数的修改及保存&lt;/li&gt; &#xA;   &lt;li&gt;PyTorch模型定义与进阶训练技巧&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;第七章：PyTorch可视化 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;可视化网络结构&lt;/li&gt; &#xA;   &lt;li&gt;可视化CNN卷积层&lt;/li&gt; &#xA;   &lt;li&gt;使用TensorBoard可视化训练过程&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;第八章：PyTorch生态简介 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;简介&lt;/li&gt; &#xA;   &lt;li&gt;图像—torchvision&lt;/li&gt; &#xA;   &lt;li&gt;视频—PyTorchVideo&lt;/li&gt; &#xA;   &lt;li&gt;文本—torchtext&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;第九章：常见网络代码的解读 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;计算机视觉 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;图像分类&lt;/li&gt; &#xA;     &lt;li&gt;目标检测&lt;/li&gt; &#xA;     &lt;li&gt;图像分割&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;自然语言处理&lt;/li&gt; &#xA;   &lt;li&gt;音频处理&lt;/li&gt; &#xA;   &lt;li&gt;视频处理&lt;/li&gt; &#xA;   &lt;li&gt;其他&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;三、人员安排&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;成员&amp;nbsp;&lt;/th&gt; &#xA;   &lt;th&gt;个人简介&lt;/th&gt; &#xA;   &lt;th&gt;个人主页&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;李嘉骐&lt;/td&gt; &#xA;   &lt;td&gt;DataWhale成员，清华大学研究生&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.zhihu.com/people/li-jia-qi-16-9/posts&#34;&gt;https://www.zhihu.com/people/li-jia-qi-16-9/posts&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;牛志康&lt;/td&gt; &#xA;   &lt;td&gt;DataWhale成员，西安电子科技大学本科生&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.zhihu.com/people/obeah-82&#34;&gt;https://www.zhihu.com/people/obeah-82&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;刘洋&lt;/td&gt; &#xA;   &lt;td&gt;Datawhale成员，中国科学院数学与系统科学研究所研究生&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.zhihu.com/people/ming-ren-19-34/asks&#34;&gt;https://www.zhihu.com/people/ming-ren-19-34/asks&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;陈安东&lt;/td&gt; &#xA;   &lt;td&gt;DataWhale成员，中央民族大学研究生&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://andongblue.github.io/chenandong.github.io/&#34;&gt;https://andongblue.github.io/chenandong.github.io/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;教程贡献情况（已上线课程内容）：&lt;/p&gt; &#xA;&lt;p&gt;李嘉骐：第三章；第四章；第五章；第六章；第七章；第八章；内容整合&lt;/p&gt; &#xA;&lt;p&gt;牛志康：第一章；第三章；第六章；第七章；第八章，文档部署&lt;/p&gt; &#xA;&lt;p&gt;刘洋：第二章；第三章&lt;/p&gt; &#xA;&lt;p&gt;陈安东：第二章；第三章；第七章&lt;/p&gt; &#xA;&lt;h2&gt;四、 课程编排与使用方法&lt;/h2&gt; &#xA;&lt;p&gt;部分章节直播讲解请观看B站回放（持续更新）：&lt;a href=&#34;https://www.bilibili.com/video/BV1L44y1472Z&#34;&gt;https://www.bilibili.com/video/BV1L44y1472Z&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;课程编排： 深入浅出PyTorch分为三个阶段：PyTorch深度学习基础知识、PyTorch进阶操作、PyTorch案例分析。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;使用方法:&lt;/p&gt; &lt;p&gt;我们的课程内容都以markdown格式或jupyter notebook的形式保存在本仓库内。除了多看加深课程内容的理解外，最重要的还是动手练习、练习、练习&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;组队学习安排:&lt;/p&gt; &lt;p&gt;第一部分：第一章到第四章，学习周期：10天；&lt;/p&gt; &lt;p&gt;第二部分：第五章到第八章，学习周期：11天&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;五、关于贡献&lt;/h2&gt; &#xA;&lt;p&gt;本项目使用&lt;code&gt;Forking&lt;/code&gt;工作流，具体参考&lt;a href=&#34;https://www.atlassian.com/git/tutorials/comparing-workflows/forking-workflow&#34;&gt;atlassian文档&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;大致步骤如下：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;在GitHub上Fork本仓库&lt;/li&gt; &#xA; &lt;li&gt;Clone Fork后的个人仓库&lt;/li&gt; &#xA; &lt;li&gt;设置&lt;code&gt;upstream&lt;/code&gt;仓库地址，并禁用&lt;code&gt;push&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;使用分支开发，课程分支名为&lt;code&gt;lecture{#NO}&lt;/code&gt;，&lt;code&gt;#NO&lt;/code&gt;保持两位，如&lt;code&gt;lecture07&lt;/code&gt;，对应课程目录&lt;/li&gt; &#xA; &lt;li&gt;PR之前保持与原始仓库的同步，之后发起PR请求&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;命令示例：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# fork&#xA;# clone&#xA;git clone git@github.com:USERNAME/thorough-pytorch.git&#xA;# set upstream&#xA;git remote add upstream git@github.com:datawhalechina/thorough-pytorch.git&#xA;# disable upstream push&#xA;git remote set-url --push upstream DISABLE&#xA;# verify&#xA;git remote -v&#xA;# some sample output:&#xA;# origin&#x9;git@github.com:NoFish-528/thorough-pytorch.git (fetch)&#xA;# origin&#x9;git@github.com:NoFish-528/thorough-pytorch.git (push)&#xA;# upstream&#x9;git@github.com:datawhalechina/thorough-pytorch.git (fetch)&#xA;# upstream&#x9;DISABLE (push)&#xA;# do your work&#xA;git checkout -b lecture07&#xA;# edit and commit and push your changes&#xA;git push -u origin lecture07&#xA;# keep your fork up to date&#xA;## fetch upstream main and merge with forked main branch&#xA;git fetch upstream&#xA;git checkout main&#xA;git merge upstream/main&#xA;## rebase brach and force push&#xA;git checkout lecture07&#xA;git rebase main&#xA;git push -f&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Commit Message&lt;/h3&gt; &#xA;&lt;p&gt;提交信息使用如下格式：&lt;code&gt;&amp;lt;type&amp;gt;: &amp;lt;short summary&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;type&amp;gt;: &amp;lt;short summary&amp;gt;&#xA;  │            │&#xA;  │            └─⫸ Summary in present tense. Not capitalized. No period at the end.&#xA;  │&#xA;  └─⫸ Commit Type: lecture{#NO}|others&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;others&lt;/code&gt;包括非课程相关的改动，如本&lt;code&gt;README.md&lt;/code&gt;中的变动，&lt;code&gt;.gitignore&lt;/code&gt;的调整等。&lt;/p&gt; &#xA;&lt;h2&gt;六、更新计划&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;内容&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;更新时间&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;内容&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;模型初始化&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;torch.nn.init&lt;/code&gt;的使用&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;visdom可视化&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.4.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;Visdom&lt;/code&gt;的使用&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;apex&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.5.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;apex的简介和使用&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;超参数的保存&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;使用argparse进行参数的修改&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;onnx&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.5.31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;讲述ONNX格式和实战例子&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;模型部署&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Flask部署PyTorch模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;TorchScript&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TorchScript&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;数据增强&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;imgaug的使用&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;并行训练&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;并行训练&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;模型预训练 - torchhub&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.4.16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;torchhub的简介和使用方法&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;模型预训练 - timm&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;timm预训练模型的使用方法&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;模型预训练 - openmmlab&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.4.27&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;openmmlab系列的使用&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;目标检测 - yolo系列&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yolo系列介绍与trick实现&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;目标检测 - SSD&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SSD的简介和实现&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;目标检测 - RCNN系列&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Fast-RCNN &amp;amp; Mask-RCNN&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;目标检测 - DETR&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DETR的实现&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;图像分类 - GoogLeNet&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.5.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GoogLeNet的介绍与实现&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;图像分类 - Vision transformer&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.5.18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vit介绍与实现&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;图像分类 - MobileNet系列&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.4月&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MobileNet系列介绍与实现&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;图像分类 - GhostNet&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.4月&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GhostNet代码讲解&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;生成式对抗网络 - 生成手写数字实战&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2022.5.25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;生成数字并可视化&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;生成式对抗网络 - DCGAN&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;风格迁移 - StyleGAN&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;生成网络 - VAE&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;图像分割 Deeplab系列&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Deeplab系列代码讲解&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;自然语言处理 LSTM&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LSTM情感分析实战&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;自然语言处理 RNN&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RNN名字分类&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;自然语言处理 Transformer&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;自然语言处理 BERT&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;视频&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;待定&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;音频&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;待定&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;自定义CUDA扩展和算子&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;七、鸣谢与反馈&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;非常感谢DataWhale成员 叶前坤 @&lt;a href=&#34;https://github.com/PureBuckwheat&#34;&gt;PureBuckwheat&lt;/a&gt; 和 胡锐锋 @&lt;a href=&#34;https://github.com/Relph1119&#34;&gt;Relph1119&lt;/a&gt; 对文档的细致校对！&lt;/li&gt; &#xA; &lt;li&gt;如果有任何想法可以联系我们DataWhale也欢迎大家多多提出issue。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;八、关注我们&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/datawhalechina/easy-rl/master/docs/res/qrcode.jpeg&#34; width=&#34;250&#34; height=&#34;270&#34; alt=&#34;Datawhale是一个专注AI领域的开源组织，以“for the learner，和学习者一起成长”为愿景，构建对学习者最有价值的开源学习社区。关注我们，一起学习成长。&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;&lt;img alt=&#34;知识共享许可协议&#34; style=&#34;border-width:0&#34; src=&#34;https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey&#34;&gt;&lt;/a&gt;&lt;br&gt;本作品采用&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议&lt;/a&gt;进行许可。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>aws/amazon-sagemaker-examples</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/aws/amazon-sagemaker-examples</id>
    <link href="https://github.com/aws/amazon-sagemaker-examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Example 📓 Jupyter notebooks that demonstrate how to build, train, and deploy machine learning models using 🧠 Amazon SageMaker.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/aws/amazon-sagemaker-examples/raw/main/_static/sagemaker-banner.png&#34; alt=&#34;SageMaker&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Amazon SageMaker Examples&lt;/h1&gt; &#xA;&lt;p&gt;Example Jupyter notebooks that demonstrate how to build, train, and deploy machine learning models using Amazon SageMaker.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;📚&lt;/span&gt; Background&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/sagemaker/&#34;&gt;Amazon SageMaker&lt;/a&gt; is a fully managed service for data science and machine learning (ML) workflows. You can use Amazon SageMaker to simplify the process of building, training, and deploying ML models.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://sagemaker-examples.readthedocs.io/en/latest/&#34;&gt;SageMaker example notebooks&lt;/a&gt; are Jupyter notebooks that demonstrate the usage of Amazon SageMaker.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;🛠&lt;/span&gt; Setup&lt;/h2&gt; &#xA;&lt;p&gt;The quickest setup to run example notebooks includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An &lt;a href=&#34;http://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html&#34;&gt;AWS account&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Proper &lt;a href=&#34;http://docs.aws.amazon.com/sagemaker/latest/dg/authentication-and-access-control.html&#34;&gt;IAM User and Role&lt;/a&gt; setup&lt;/li&gt; &#xA; &lt;li&gt;An &lt;a href=&#34;http://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html&#34;&gt;Amazon SageMaker Notebook Instance&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;An &lt;a href=&#34;http://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html&#34;&gt;S3 bucket&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;💻&lt;/span&gt; Usage&lt;/h2&gt; &#xA;&lt;p&gt;These example notebooks are automatically loaded into SageMaker Notebook Instances. They can be accessed by clicking on the &lt;code&gt;SageMaker Examples&lt;/code&gt; tab in Jupyter or the SageMaker logo in JupyterLab.&lt;/p&gt; &#xA;&lt;p&gt;Although most examples utilize key Amazon SageMaker functionality like distributed, managed training or real-time hosted endpoints, these notebooks can be run outside of Amazon SageMaker Notebook Instances with minimal modification (updating IAM role definition and installing the necessary libraries).&lt;/p&gt; &#xA;&lt;p&gt;As of February 7, 2022, the default branch is named &#34;main&#34;. See our &lt;a href=&#34;https://github.com/aws/amazon-sagemaker-examples/discussions/3131&#34;&gt;announcement&lt;/a&gt; for details and how to update your existing clone.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;📓&lt;/span&gt; Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Introduction to Ground Truth Labeling Jobs&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide quick walkthroughs to get you up and running with the labeling job workflow for Amazon SageMaker Ground Truth.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/bring_your_own_model_for_sagemaker_labeling_workflows_with_active_learning&#34;&gt;Bring your own model for SageMaker labeling workflows with active learning&lt;/a&gt; is an end-to-end example that shows how to bring your custom training, inference logic and active learning to the Amazon SageMaker ecosystem.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/from_unlabeled_data_to_deployed_machine_learning_model_ground_truth_demo_image_classification&#34;&gt;From Unlabeled Data to a Deployed Machine Learning Model: A SageMaker Ground Truth Demonstration for Image Classification&lt;/a&gt; is an end-to-end example that starts with an unlabeled dataset, labels it using the Ground Truth API, analyzes the results, trains an image classification neural net using the annotated dataset, and finally uses the trained model to perform batch and online inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/ground_truth_object_detection_tutorial&#34;&gt;Ground Truth Object Detection Tutorial&lt;/a&gt; is a similar end-to-end example but for an object detection task.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/data_analysis_of_ground_truth_image_classification_output&#34;&gt;Basic Data Analysis of an Image Classification Output Manifest&lt;/a&gt; presents charts to visualize the number of annotations for each class, differentiating between human annotations and automatic labels (if your job used auto-labeling). It also displays sample images in each class, and creates a pdf which concisely displays the full results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/object_detection_augmented_manifest_training&#34;&gt;Training a Machine Learning Model Using an Output Manifest&lt;/a&gt; introduces the concept of an &#34;augmented manifest&#34; and demonstrates that the output file of a labeling job can be immediately used as the input file to train a SageMaker machine learning model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ground_truth_labeling_jobs/annotation_consolidation&#34;&gt;Annotation Consolidation&lt;/a&gt; demonstrates Amazon SageMaker Ground Truth annotation consolidation techniques for image classification for a completed labeling job.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Introduction to Applying Machine Learning&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide a gentle introduction to machine learning concepts as they are applied in practical use cases across a variety of sectors.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/xgboost_customer_churn&#34;&gt;Predicting Customer Churn&lt;/a&gt; uses customer interaction and service usage data to find those most likely to churn, and then walks through the cost/benefit trade-offs of providing retention incentives. This uses Amazon SageMaker&#39;s implementation of &lt;a href=&#34;https://github.com/dmlc/xgboost&#34;&gt;XGBoost&lt;/a&gt; to create a highly predictive model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/breast_cancer_prediction&#34;&gt;Cancer Prediction&lt;/a&gt; predicts Breast Cancer based on features derived from images, using SageMaker&#39;s Linear Learner.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/ensemble_modeling&#34;&gt;Ensembling&lt;/a&gt; predicts income using two Amazon SageMaker models to show the advantages in ensembling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/video_game_sales&#34;&gt;Video Game Sales&lt;/a&gt; develops a binary prediction model for the success of video games based on review scores.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/gluon_recommender_system&#34;&gt;MXNet Gluon Recommender System&lt;/a&gt; uses neural network embeddings for non-linear matrix factorization to predict user movie ratings on Amazon digital reviews.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/fair_linear_learner&#34;&gt;Fair Linear Learner&lt;/a&gt; is an example of an effective way to create fair linear models with respect to sensitive features.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/US-census_population_segmentation_PCA_Kmeans&#34;&gt;Population Segmentation of US Census Data using PCA and Kmeans&lt;/a&gt; analyzes US census data and reduces dimensionality using PCA then clusters US counties using KMeans to identify segments of similar counties.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/object2vec_document_embedding&#34;&gt;Document Embedding using Object2Vec&lt;/a&gt; is an example to embed a large collection of documents in a common low-dimensional space, so that the semantic distances between these documents are preserved.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/deepar_chicago_traffic_violations&#34;&gt;Traffic violations forecasting using DeepAR&lt;/a&gt; is an example to use daily traffic violation data to predict pattern and seasonality to use Amazon DeepAR alogorithm.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SageMaker Automatic Model Tuning&lt;/h3&gt; &#xA;&lt;p&gt;These examples introduce SageMaker&#39;s hyperparameter tuning functionality which helps deliver the best possible predictions by running a large number of training jobs to determine which hyperparameter values are the most impactful.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/xgboost_direct_marketing&#34;&gt;XGBoost Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning to improve your model fit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/blazingtext_text_classification_20_newsgroups&#34;&gt;BlazingText Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the BlazingText built-in algorithm and 20_newsgroups dataset..&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/tensorflow_mnist&#34;&gt;TensorFlow Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built TensorFlow container and MNIST dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/mxnet_mnist&#34;&gt;MXNet Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built MXNet container and MNIST dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/huggingface_multiclass_text_classification_20_newsgroups&#34;&gt;HuggingFace Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built HuggingFace container and 20_newsgroups dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/keras_bring_your_own&#34;&gt;Keras BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with a custom container running a Keras convolutional network on CIFAR-10 data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/r_bring_your_own&#34;&gt;R BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the custom container from the &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/r_bring_your_own&#34;&gt;Bring Your Own R Algorithm&lt;/a&gt; example.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/hyperparameter_tuning/analyze_results&#34;&gt;Analyzing Results&lt;/a&gt; is a shared notebook that can be used after each of the above notebooks to provide analysis on how training jobs with different hyperparameters performed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SageMaker Autopilot&lt;/h3&gt; &#xA;&lt;p&gt;These examples introduce SageMaker Autopilot. Autopilot automatically performs feature engineering, model selection, model tuning (hyperparameter optimization) and allows you to directly deploy the best model to an endpoint to serve inference requests.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/autopilot/&#34;&gt;Customer Churn AutoML&lt;/a&gt; shows how to use SageMaker Autopilot to automatically train a model for the &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/xgboost_customer_churn&#34;&gt;Predicting Customer Churn&lt;/a&gt; task.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/autopilot/&#34;&gt;Targeted Direct Marketing AutoML&lt;/a&gt; shows how to use SageMaker Autopilot to automatically train a model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-autopilot/housing_prices&#34;&gt;Housing Prices AutoML&lt;/a&gt; shows how to use SageMaker Autopilot for a linear regression problem (predict housing prices).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Introduction to Amazon Algorithms&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide quick walkthroughs to get you up and running with Amazon SageMaker&#39;s custom developed algorithms. Most of these algorithms can train on distributed hardware, scale incredibly well, and are faster and cheaper than popular alternatives.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/1P_kmeans_highlevel&#34;&gt;k-means&lt;/a&gt; is our introductory example for Amazon SageMaker. It walks through the process of clustering MNIST images of handwritten digits using Amazon SageMaker k-means.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/factorization_machines_mnist&#34;&gt;Factorization Machines&lt;/a&gt; showcases Amazon SageMaker&#39;s implementation of the algorithm to predict whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/lda_topic_modeling&#34;&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; introduces topic modeling using Amazon SageMaker Latent Dirichlet Allocation (LDA) on a synthetic dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/linear_learner_mnist&#34;&gt;Linear Learner&lt;/a&gt; predicts whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier from Amazon SageMaker Linear Learner.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/ntm_synthetic&#34;&gt;Neural Topic Model (NTM)&lt;/a&gt; uses Amazon SageMaker Neural Topic Model (NTM) to uncover topics in documents from a synthetic data source, where topic distributions are known.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/pca_mnist&#34;&gt;Principal Components Analysis (PCA)&lt;/a&gt; uses Amazon SageMaker PCA to calculate eigendigits from MNIST.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/seq2seq_translation_en-de&#34;&gt;Seq2Seq&lt;/a&gt; uses the Amazon SageMaker Seq2Seq algorithm that&#39;s built on top of &lt;a href=&#34;https://github.com/awslabs/sockeye&#34;&gt;Sockeye&lt;/a&gt;, which is a sequence-to-sequence framework for Neural Machine Translation based on MXNet. Seq2Seq implements state-of-the-art encoder-decoder architectures which can also be used for tasks like Abstractive Summarization in addition to Machine Translation. This notebook shows translation from English to German text.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/imageclassification_caltech&#34;&gt;Image Classification&lt;/a&gt; includes full training and transfer learning examples of Amazon SageMaker&#39;s Image Classification algorithm. This uses a ResNet deep convolutional neural network to classify images from the caltech dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/xgboost_abalone&#34;&gt;XGBoost for regression&lt;/a&gt; predicts the age of abalone (&lt;a href=&#34;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html&#34;&gt;Abalone dataset&lt;/a&gt;) using regression from Amazon SageMaker&#39;s implementation of &lt;a href=&#34;https://github.com/dmlc/xgboost&#34;&gt;XGBoost&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/xgboost_mnist&#34;&gt;XGBoost for multi-class classification&lt;/a&gt; uses Amazon SageMaker&#39;s implementation of &lt;a href=&#34;https://github.com/dmlc/xgboost&#34;&gt;XGBoost&lt;/a&gt; to classify handwritten digits from the MNIST dataset as one of the ten digits using a multi-class classifier. Both single machine and distributed use-cases are presented.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/deepar_synthetic&#34;&gt;DeepAR for time series forecasting&lt;/a&gt; illustrates how to use the Amazon SageMaker DeepAR algorithm for time series forecasting on a synthetically generated data set.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/blazingtext_word2vec_text8&#34;&gt;BlazingText Word2Vec&lt;/a&gt; generates Word2Vec embeddings from a cleaned text dump of Wikipedia articles using SageMaker&#39;s fast and scalable BlazingText implementation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/object_detection_birds&#34;&gt;Object detection for bird images&lt;/a&gt; demonstrates how to use the Amazon SageMaker Object Detection algorithm with a public dataset of Bird images.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/object2vec_movie_recommendation&#34;&gt;Object2Vec for movie recommendation&lt;/a&gt; demonstrates how Object2Vec can be used to model data consisting of pairs of singleton tokens using movie recommendation as a running example.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/object2vec_multilabel_genre_classification&#34;&gt;Object2Vec for multi-label classification&lt;/a&gt; shows how ObjectToVec algorithm can train on data consisting of pairs of sequences and singleton tokens using the setting of genre prediction of movies based on their plot descriptions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/object2vec_sentence_similarity&#34;&gt;Object2Vec for sentence similarity&lt;/a&gt; explains how to train Object2Vec using sequence pairs as input using sentence similarity analysis as the application.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/ipinsights_login&#34;&gt;IP Insights for suspicious logins&lt;/a&gt; shows how to train IP Insights on a login events for a web server to identify suspicious login attempts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc&#34;&gt;Semantic Segmentation&lt;/a&gt; shows how to train a semantic segmentation algorithm using the Amazon SageMaker Semantic Segmentation algorithm. It also demonstrates how to host the model and produce segmentation masks and probability of segmentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_instance_segmentation&#34;&gt;JumpStart Instance Segmentation&lt;/a&gt; demonstrates how to use a pre-trained Instance Segmentation model available in JumpStart for inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_semantic_segmentation&#34;&gt;JumpStart Semantic Segmentation&lt;/a&gt; demonstrates how to use a pre-trained Semantic Segmentation model available in JumpStart for inference, how to finetune the pre-trained model on a custom dataset using JumpStart transfer learning algorithm, and how to use fine-tuned model for inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_text_generation&#34;&gt;JumpStart Text Generation&lt;/a&gt; shows how to use JumpStart to generate text that appears indistinguishable from the hand-written text.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_text_summarization&#34;&gt;JumpStart Text Summarization&lt;/a&gt; shows how to use JumpStart to summarize the text to contain only the important information.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_image_embedding&#34;&gt;JumpStart Image Embedding&lt;/a&gt; demonstrates how to use a pre-trained model available in JumpStart for image embedding.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_text_embedding&#34;&gt;JumpStart Text Embedding&lt;/a&gt; demonstrates how to use a pre-trained model available in JumpStart for text embedding.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_object_detection&#34;&gt;JumpStart Object Detection&lt;/a&gt; demonstrates how to use a pre-trained Object Detection model available in JumpStart for inference, how to finetune the pre-trained model on a custom dataset using JumpStart transfer learning algorithm, and how to use fine-tuned model for inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_machine_translation&#34;&gt;JumpStart Machine Translation&lt;/a&gt; demonstrates how to translate text from one language to another language in JumpStart.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/jumpstart_named_entity_recognition&#34;&gt;JumpStart Named Entity Recognition&lt;/a&gt; demonstrates how to identify named entities such as names, locations etc. in the text in JumpStart.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker RL&lt;/h3&gt; &#xA;&lt;p&gt;The following provide examples demonstrating different capabilities of Amazon SageMaker RL.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_cartpole_coach&#34;&gt;Cartpole using Coach&lt;/a&gt; demonstrates the simplest usecase of Amazon SageMaker RL using Intel&#39;s RL Coach.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_deepracer_robomaker_coach_gazebo&#34;&gt;AWS DeepRacer&lt;/a&gt; demonstrates AWS DeepRacer trainig using RL Coach in the Gazebo environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_hvac_coach_energyplus&#34;&gt;HVAC using EnergyPlus&lt;/a&gt; demonstrates the training of HVAC systems using the EnergyPlus environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_knapsack_coach_custom&#34;&gt;Knapsack Problem&lt;/a&gt; demonstrates how to solve the knapsack problem using a custom environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_mountain_car_coach_gymEnv&#34;&gt;Mountain Car&lt;/a&gt; Mountain car is a classic RL problem. This notebook explains how to solve this using the OpenAI Gym environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_network_compression_ray_custom&#34;&gt;Distributed Neural Network Compression&lt;/a&gt; This notebook explains how to compress ResNets using RL, using a custom environment and the RLLib toolkit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_portfolio_management_coach_customEnv&#34;&gt;Portfolio Management&lt;/a&gt; This notebook uses a custom Gym environment to manage multiple financial investments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_predictive_autoscaling_coach_customEnv&#34;&gt;Autoscaling&lt;/a&gt; demonstrates how to adjust load depending on demand. This uses RL Coach and a custom environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_roboschool_ray&#34;&gt;Roboschool&lt;/a&gt; is an open source physics simulator that is commonly used to train RL policies for robotic systems. This notebook demonstrates training a few agents using it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_roboschool_stable_baselines&#34;&gt;Stable Baselines&lt;/a&gt; In this notebook example, we will make the HalfCheetah agent learn to walk using the stable-baselines, which are a set of improved implementations of Reinforcement Learning (RL) algorithms based on OpenAI Baselines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_traveling_salesman_vehicle_routing_coach&#34;&gt;Travelling Salesman&lt;/a&gt; is a classic NP hard problem, which this notebook solves with AWS SageMaker RL.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_tic_tac_toe_coach_customEnv&#34;&gt;Tic-tac-toe&lt;/a&gt; is a simple implementation of a custom Gym environment to train and deploy an RL agent in Coach that then plays tic-tac-toe interactively in a Jupyter Notebook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/reinforcement_learning/rl_unity_ray&#34;&gt;Unity Game Agent&lt;/a&gt; shows how to use RL algorithms to train an agent to play Unity3D game.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Scientific Details of Algorithms&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide more thorough mathematical treatment on a select group of algorithms.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/scientific_details_of_algorithms/streaming_median&#34;&gt;Streaming Median&lt;/a&gt; sequentially introduces concepts used in streaming algorithms, which many SageMaker algorithms rely on to deliver speed and scalability.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/scientific_details_of_algorithms/lda_topic_modeling&#34;&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; dives into Amazon SageMaker&#39;s spectral decomposition approach to LDA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/scientific_details_of_algorithms/linear_learner_class_weights_loss_functions&#34;&gt;Linear Learner features&lt;/a&gt; shows how to use the class weights and loss functions features of the SageMaker Linear Learner algorithm to improve performance on a credit card fraud prediction task&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Debugger&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide and introduction to SageMaker Debugger which allows debugging and monitoring capabilities for training of machine learning and deep learning algorithms. Note that although these notebooks focus on a specific framework, the same approach works with all the frameworks that Amazon SageMaker Debugger supports. The notebooks below are listed in the order in which we recommend you review them.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/tensorflow_builtin_rule/&#34;&gt;Using a built-in rule with TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/tensorflow_keras_custom_rule/&#34;&gt;Using a custom rule with TensorFlow Keras&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/mnist_tensor_analysis/&#34;&gt;Interactive tensor analysis in notebook with MXNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/mnist_tensor_plot/&#34;&gt;Visualizing Debugging Tensors of MXNet training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/mxnet_realtime_analysis/&#34;&gt;Real-time analysis in notebook with MXNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/xgboost_builtin_rules/&#34;&gt;Using a built in rule with XGBoost&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/xgboost_realtime_analysis/&#34;&gt;Real-time analysis in notebook with XGBoost&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/mxnet_spot_training/&#34;&gt;Using SageMaker Debugger with Managed Spot Training and MXNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/tensorflow_action_on_rule/&#34;&gt;Reacting to CloudWatch Events from Rules to take an action based on status with TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-debugger/pytorch_custom_container/&#34;&gt;Using SageMaker Debugger with a custom PyTorch container&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Clarify&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide an introduction to SageMaker Clarify which provides machine learning developers with greater visibility into their training data and models so they can identify and limit bias and explain predictions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/fairness_and_explainability&#34;&gt;Fairness and Explainability with SageMaker Clarify&lt;/a&gt; shows how to use SageMaker Clarify Processor API to measure the pre-training bias of a dataset and post-training bias of a model, and explain the importance of the input features on the model&#39;s decision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_model_monitor/fairness_and_explainability&#34;&gt;Amazon SageMaker Clarify Model Monitors&lt;/a&gt; shows how to use SageMaker Clarify Model Monitor API to schedule bias monitor to monitor predictions for bias drift on a regular basis, and schedule explainability monitor to monitor predictions for feature attribution drift on a regular basis.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Publishing content from RStudio on Amazon SageMaker to RStudio Connect&lt;/h3&gt; &#xA;&lt;p&gt;These examples show you how to run R examples, and publish applications in RStudio on Amazon SageMaker to RStudio Connect.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/r_examples/rsconnect_rmarkdown/&#34;&gt;Publishing R Markdown&lt;/a&gt; shows how you can author an R Markdown document (.Rmd, .Rpres) within RStudio on Amazon SageMaker and publish to RStudio Connect for wide consumption.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/r_examples/rsconnect_shiny/&#34;&gt;Publishing R Shiny Apps&lt;/a&gt; shows how you can author an R Shiny application within RStudio on Amazon SageMaker and publish to RStudio Connect for wide consumption.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/r_examples/rsconnect_streamlit/&#34;&gt;Publishing Streamlit Apps&lt;/a&gt; shows how you can author a streamlit application withing Amazon SageMaker Studio and publish to RStudio Connect for wide consumption.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Advanced Amazon SageMaker Functionality&lt;/h3&gt; &#xA;&lt;p&gt;These examples showcase unique functionality available in Amazon SageMaker. They cover a broad range of topics and utilize a variety of methods, but aim to provide the user with sufficient insight or inspiration to develop within Amazon SageMaker.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/data_distribution_types&#34;&gt;Data Distribution Types&lt;/a&gt; showcases the difference between two methods for sending data from S3 to Amazon SageMaker Training instances. This has particular implication for scalability and accuracy of distributed training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/handling_kms_encrypted_data&#34;&gt;Encrypting Your Data&lt;/a&gt; shows how to use Server Side KMS encrypted data with Amazon SageMaker training. The IAM role used for S3 access needs to have permissions to encrypt and decrypt data with the KMS key.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/parquet_to_recordio_protobuf&#34;&gt;Using Parquet Data&lt;/a&gt; shows how to bring &lt;a href=&#34;https://parquet.apache.org/&#34;&gt;Parquet&lt;/a&gt; data sitting in S3 into an Amazon SageMaker Notebook and convert it into the recordIO-protobuf format that many SageMaker algorithms consume.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/working_with_redshift_data&#34;&gt;Connecting to Redshift&lt;/a&gt; demonstrates how to copy data from Redshift to S3 and vice-versa without leaving Amazon SageMaker Notebooks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/xgboost_bring_your_own_model&#34;&gt;Bring Your Own XGBoost Model&lt;/a&gt; shows how to use Amazon SageMaker Algorithms containers to bring a pre-trained model to a realtime hosted endpoint without ever needing to think about REST APIs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/kmeans_bring_your_own_model&#34;&gt;Bring Your Own k-means Model&lt;/a&gt; shows how to take a model that&#39;s been fit elsewhere and use Amazon SageMaker Algorithms containers to host it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/r_bring_your_own&#34;&gt;Bring Your Own R Algorithm&lt;/a&gt; shows how to bring your own algorithm container to Amazon SageMaker using the R language.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/install_r_kernel&#34;&gt;Installing the R Kernel&lt;/a&gt; shows how to install the R kernel into an Amazon SageMaker Notebook Instance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/scikit_bring_your_own&#34;&gt;Bring Your Own scikit Algorithm&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm for training and production-ready hosting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/mxnet_mnist_byom&#34;&gt;Bring Your Own MXNet Model&lt;/a&gt; shows how to bring a model trained anywhere using MXNet into Amazon SageMaker.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/tensorflow_iris_byom&#34;&gt;Bring Your Own TensorFlow Model&lt;/a&gt; shows how to bring a model trained anywhere using TensorFlow into Amazon SageMaker.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/search&#34;&gt;Experiment Management Capabilities with Search&lt;/a&gt; shows how to organize Training Jobs into projects, and track relationships between Models, Endpoints, and Training Jobs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/multi_model_bring_your_own&#34;&gt;Host Multiple Models with Your Own Algorithm&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint with your own custom algorithm.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/multi_model_xgboost_home_value&#34;&gt;Host Multiple Models with XGBoost&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint using a multi-model enabled XGBoost container.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/advanced_functionality/multi_model_sklearn_home_value&#34;&gt;Host Multiple Models with SKLearn&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint using a multi-model enabled SKLearn container.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-script-mode&#34;&gt;SageMaker Training and Inference with Script Mode&lt;/a&gt; shows how to use custom training and inference scripts, similar to those you would use outside of SageMaker, with SageMaker&#39;s prebuilt containers for various frameworks like Scikit-learn, PyTorch, and XGBoost.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-triton&#34;&gt;Host Models with NVidia Triton Server&lt;/a&gt; shows how to deploy models to a realtime hosted endpoint using &lt;a href=&#34;https://developer.nvidia.com/nvidia-triton-inference-server&#34;&gt;Triton&lt;/a&gt; as the model inference server.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Neo Compilation Jobs&lt;/h3&gt; &#xA;&lt;p&gt;These examples provide an introduction to how to use Neo to compile and optimize deep learning models.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/gluoncv_ssd_mobilenet&#34;&gt;GluonCV SSD Mobilenet&lt;/a&gt; shows how to train GluonCV SSD MobileNet and use Amazon SageMaker Neo to compile and optimize the trained model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/imageclassification_caltech&#34;&gt;Image Classification&lt;/a&gt; Adapts from &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/imageclassification_caltech&#34;&gt;image classification&lt;/a&gt; including Neo API and comparison against the uncompiled baseline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/mxnet_mnist&#34;&gt;MNIST with MXNet&lt;/a&gt; Adapts from &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/mxnet_mnist&#34;&gt;MXNet MNIST&lt;/a&gt; including Neo API and comparison against the uncompiled baseline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/pytorch_torchvision&#34;&gt;Deploying pre-trained PyTorch vision models&lt;/a&gt; shows how to use Amazon SageMaker Neo to compile and optimize pre-trained PyTorch models from TorchVision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/tensorflow_distributed_mnist&#34;&gt;Distributed TensorFlow&lt;/a&gt; includes Neo API and comparison against the uncompiled baseline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_neo_compilation_jobs/xgboost_customer_churn&#34;&gt;Predicting Customer Churn&lt;/a&gt; Adapts from &lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_applying_machine_learning/xgboost_customer_churn&#34;&gt;XGBoost customer churn&lt;/a&gt; including Neo API and comparison against the uncompiled baseline.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Processing&lt;/h3&gt; &#xA;&lt;p&gt;These examples show you how to use SageMaker Processing jobs to run data processing workloads.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation&#34;&gt;Scikit-Learn Data Processing and Model Evaluation&lt;/a&gt; shows how to use SageMaker Processing and the Scikit-Learn container to run data preprocessing and model evaluation workloads.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/feature_transformation_with_sagemaker_processing&#34;&gt;Feature transformation with Amazon SageMaker Processing and SparkML&lt;/a&gt; shows how to use SageMaker Processing to run data processing workloads using SparkML prior to training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/feature_transformation_with_sagemaker_processing_dask&#34;&gt;Feature transformation with Amazon SageMaker Processing and Dask&lt;/a&gt; shows how to use SageMaker Processing to transform data using Dask distributed clusters&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker_processing/spark_distributed_data_processing&#34;&gt;Distributed Data Processing using Apache Spark and SageMaker Processing&lt;/a&gt; shows how to use the built-in Spark container on SageMaker Processing using the SageMaker Python SDK.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Pipelines&lt;/h3&gt; &#xA;&lt;p&gt;These examples show you how to use &lt;a href=&#34;https://aws.amazon.com/sagemaker/pipelines&#34;&gt;SageMaker Pipelines&lt;/a&gt; to create, automate and manage end-to-end Machine Learning workflows.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-pipelines/nlp/amazon_comprehend_sagemaker_pipeline&#34;&gt;Amazon Comprehend with SageMaker Pipelines&lt;/a&gt; shows how to deploy a custom text classification using Amazon Comprehend and SageMaker Pipelines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-pipelines/time_series_forecasting/amazon_forecast_pipeline&#34;&gt;Amazon Forecast with SageMaker Pipelines&lt;/a&gt; shows how you can create a dataset, dataset group and predictor with Amazon Forecast and SageMaker Pipelines.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Amazon SageMaker Pre-Built Framework Containers and the Python SDK&lt;/h3&gt; &#xA;&lt;h4&gt;Pre-Built Deep Learning Framework Containers&lt;/h4&gt; &#xA;&lt;p&gt;These examples show you how to train and host in pre-built deep learning framework containers using the SageMaker Python SDK.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/chainer_cifar10&#34;&gt;Chainer CIFAR-10&lt;/a&gt; trains a VGG image classification network on CIFAR-10 using Chainer (both single machine and multi-machine versions are included)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/chainer_mnist&#34;&gt;Chainer MNIST&lt;/a&gt; trains a basic neural network on MNIST using Chainer (shows how to use local mode)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/chainer_sentiment_analysis&#34;&gt;Chainer sentiment analysis&lt;/a&gt; trains a LSTM network with embeddings to predict text sentiment using Chainer&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/scikit_learn_iris&#34;&gt;IRIS with Scikit-learn&lt;/a&gt; trains a Scikit-learn classifier on IRIS data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/scikit_learn_model_registry_batch_transform&#34;&gt;Model Registry and Batch Transform with Scikit-learn&lt;/a&gt; trains a Scikit-learn Random Forest model, registers it in Model Registry, and runs a Batch Transform Job.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/mxnet_gluon_mnist&#34;&gt;MNIST with MXNet Gluon&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit dataset using MXNet Gluon&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/mxnet_mnist&#34;&gt;MNIST with MXNet&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit data using MXNet&#39;s symbolic syntax&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/mxnet_gluon_sentiment&#34;&gt;Sentiment Analysis with MXNet Gluon&lt;/a&gt; trains a text classifier using embeddings with MXNet Gluon&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/tensorflow_script_mode_training_and_serving&#34;&gt;TensorFlow training and serving&lt;/a&gt; trains a basic neural network on MNIST&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/tensorflow_script_mode_horovod&#34;&gt;TensorFlow with Horovod&lt;/a&gt; trains on MNIST using Horovod for distributed training&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/tensorflow_script_mode_using_shell_commands&#34;&gt;TensorFlow using shell commands&lt;/a&gt; shows how to use a shell script for the container&#39;s entry point&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Pre-Built Machine Learning Framework Containers&lt;/h4&gt; &#xA;&lt;p&gt;These examples show you how to build Machine Learning models with frameworks like Apache Spark or Scikit-learn using SageMaker Python SDK.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/sparkml_serving_emr_mleap_abalone&#34;&gt;Inference with SparkML Serving&lt;/a&gt; shows how to build an ML model with Apache Spark using Amazon EMR on Abalone dataset and deploy in SageMaker with SageMaker SparkML Serving.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-python-sdk/scikit_learn_inference_pipeline&#34;&gt;Pipeline Inference with Scikit-learn and LinearLearner&lt;/a&gt; builds a ML pipeline using Scikit-learn preprocessing and LinearLearner algorithm in single endpoint&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using Amazon SageMaker with Apache Spark&lt;/h3&gt; &#xA;&lt;p&gt;These examples show how to use Amazon SageMaker for model training, hosting, and inference through Apache Spark using &lt;a href=&#34;https://github.com/aws/sagemaker-spark&#34;&gt;SageMaker Spark&lt;/a&gt;. SageMaker Spark allows you to interleave Spark Pipeline stages with Pipeline stages that interact with Amazon SageMaker.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/sagemaker-spark/pyspark_mnist&#34;&gt;MNIST with SageMaker PySpark&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using Amazon SageMaker with Amazon Keyspaces (for Apache Cassandra)&lt;/h3&gt; &#xA;&lt;p&gt;These examples show how to use Amazon SageMaker to read data from &lt;a href=&#34;https://docs.aws.amazon.com/keyspaces/&#34;&gt;Amazon Keyspaces&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/ingest_data/sagemaker-keyspaces&#34;&gt;Train Machine Learning Models using Amazon Keyspaces as a Data Source&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;AWS Marketplace&lt;/h3&gt; &#xA;&lt;h4&gt;Create algorithms/model packages for listing in AWS Marketplace for machine learning.&lt;/h4&gt; &#xA;&lt;p&gt;These example notebooks show you how to package a model or algorithm for listing in AWS Marketplace for machine learning.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/creating_marketplace_products&#34;&gt;Creating Marketplace Products&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/creating_marketplace_products/models&#34;&gt;Creating a Model Package - Listing on AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to package a pre-trained model as a SageMaker Model Package that can be listed on AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/creating_marketplace_products/algorithms&#34;&gt;Creating Algorithm and Model Package - Listing on AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm to create SageMaker Algorithm and SageMaker Model Package entities that can be used with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs and listed on AWS Marketplace.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once you have created an algorithm or a model package to be listed in the AWS Marketplace, the next step is to list it in AWS Marketplace, and provide a sample notebook that customers can use to try your algorithm or model package.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/curating_aws_marketplace_listing_and_sample_notebook/ModelPackage&#34;&gt;Curate your AWS Marketplace model package listing and sample notebook&lt;/a&gt; provides instructions on how to craft a sample notebook to be associated with your listing and how to curate a good AWS Marketplace listing that makes it easy for AWS customers to consume your model package.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/curating_aws_marketplace_listing_and_sample_notebook/Algorithm&#34;&gt;Curate your AWS Marketplace algorithm listing and sample notebook&lt;/a&gt; provides instructions on how to craft a sample notebook to be associated with your listing and how to curate a good AWS Marketplace listing that makes it easy for your customers to consume your algorithm.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Use algorithms, data, and model packages from AWS Marketplace.&lt;/h4&gt; &#xA;&lt;p&gt;These examples show you how to use model-packages and algorithms from AWS Marketplace and dataset products from AWS Data Exchange, for machine learning.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_algorithms&#34;&gt;Using Algorithms&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_algorithms/amazon_demo_product&#34;&gt;Using Algorithm From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Algorithm with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_algorithms/automl&#34;&gt;Using AutoML algorithm&lt;/a&gt; provides a detailed walkthrough on how to use AutoML algorithm from AWS Marketplace.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages&#34;&gt;Using Model Packages&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/generic_sample_notebook&#34;&gt;Using Model Packages From AWS Marketplace&lt;/a&gt; is a generic notebook which provides sample code snippets you can modify and use for performing inference on Model Packages from AWS Marketplace, using Amazon SageMaker.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/amazon_demo_product&#34;&gt;Using Amazon Demo product From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Model Package entities with the enhanced SageMaker Transform/Hosting APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/auto_insurance&#34;&gt;Using models for extracting vehicle metadata&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of auto-insurance claim processing.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/improving_industrial_workplace_safety&#34;&gt;Using models for identifying non-compliance at a workplace&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of generating summary reports for identifying non-compliance at a construction/industrial workplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/creative-writing-using-gpt-2-text-generation&#34;&gt;Creative writing using GPT-2 Text Generation&lt;/a&gt; will show you how to use AWS Marketplace GPT-2-XL pre-trained model on Amazon SageMaker to generate text based on your prompt to help you author prose and poetry.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/amazon_augmented_ai_with_aws_marketplace_ml_models&#34;&gt;Amazon Augmented AI with AWS Marketplace ML models&lt;/a&gt; will show you how to use AWS Marketplace pre-trained ML models with Amazon Augmented AI to implement human-in-loop workflow reviews with your ML model predictions.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/data_quality_monitoring&#34;&gt;Monitoring data quality in third-party models from AWS Marketplace&lt;/a&gt; will show you how to perform Data Quality monitoring on a pre-trained third-party model from AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_model_packages/evaluating_aws_marketplace_models_for_person_counting_use_case&#34;&gt;Evaluating ML models from AWS Marketplace for person counting use case&lt;/a&gt; will show you how to use two AWS Marketplace GluonCV pre-trained ML models for person counting use case and evaluate each model for performance in different types of crowd images.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/using_model_packages/preprocessing-audio-data-using-a-machine-learning-model&#34;&gt;Preprocessing audio data using a pre-trained machine learning model&lt;/a&gt; demonstrates the usage of a pre-trained audio track separation model to create synthetic features and improve an acoustic classification model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_data&#34;&gt;Using Dataset Products&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_data/using_data_with_ml_model&#34;&gt;Using Dataset Product from AWS Data Exchange with ML model from AWS Marketplace&lt;/a&gt; is a sample notebook which shows how a dataset from AWS Data Exchange can be used with an ML Model Package from AWS Marketplace.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/aws_marketplace/using_data/image_classification_with_shutterstock_image_datasets&#34;&gt;Using Shutterstock Image Datasets to train Image Classification Models&lt;/a&gt; provides a detailed walkthrough on how to use the &lt;a href=&#34;https://aws.amazon.com/marketplace/pp/prodview-y6xuddt42fmbu?qid=1623195111604&amp;amp;sr=0-1&amp;amp;ref_=srh_res_product_title#offers&#34;&gt;Free Sample: Images &amp;amp; Metadata of “Whole Foods” Shoppers&lt;/a&gt; from Shutterstock&#39;s Image Datasets to train a multi-label image classification model using Shutterstock&#39;s pre-labeled image assets. You can learn more about this implementation &lt;a href=&#34;https://aws.amazon.com/blogs/awsmarketplace/using-shutterstocks-image-datasets-to-train-your-computer-vision-models/&#34;&gt;from this blog post&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;⚖&lt;/span&gt; License&lt;/h2&gt; &#xA;&lt;p&gt;This library is licensed under the &lt;a href=&#34;http://aws.amazon.com/apache2.0/&#34;&gt;Apache 2.0 License&lt;/a&gt;. For more details, please take a look at the &lt;a href=&#34;https://github.com/aws/amazon-sagemaker-examples/raw/master/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;🤝&lt;/span&gt; Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Although we&#39;re extremely excited to receive contributions from the community, we&#39;re still working on the best mechanism to take in examples from external sources. Please bear with us in the short-term if pull requests take longer than expected or are closed. Please read our &lt;a href=&#34;https://github.com/aws/amazon-sagemaker-examples/raw/master/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; if you&#39;d like to open an issue or submit a pull request.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>The-Art-of-Hacking/h4cker</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/The-Art-of-Hacking/h4cker</id>
    <link href="https://github.com/The-Art-of-Hacking/h4cker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository is primarily maintained by Omar Santos and includes thousands of resources related to ethical hacking / penetration testing, digital forensics and incident response (DFIR), vulnerability research, exploit development, reverse engineering, and more.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cyber Security Resources&lt;/h1&gt; &#xA;&lt;center&gt;&#xA; &lt;img src=&#34;https://h4cker.org/img/h4cker2.PNG&#34; width=&#34;200&#34; height=&#34;300&#34;&gt; &#xA;&lt;/center&gt; &#xA;&lt;p&gt;This repository includes thousands of cybersecurity-related references and resources and it is maintained by &lt;a href=&#34;https://omarsantos.io/&#34;&gt;Omar Santos&lt;/a&gt;. This GitHub repository has been created to provide supplemental material to several books, video courses, and live training created by Omar Santos and other co-authors. It provides over 9,000 references, scripts, tools, code, and other resources that help offensive and defensive security professionals learn and develop new skills. This GitHub repository provides guidance on how build your own hacking environment, learn about offensive security (ethical hacking) techniques, vulnerability research, exploit development, reverse engineering, malware analysis, threat intelligence, threat hunting, digital forensics and incident response (DFIR), includes examples of real-life penetration testing reports, and more.&lt;/p&gt; &#xA;&lt;h2&gt;The Art of Hacking Series&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;http://theartofhacking.org&#34;&gt;Art of Hacking Series&lt;/a&gt; is a series of video courses and live training that help you get up and running with your cybersecurity career. The following are the different video courses that are part of the Art of Hacking series:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/library/view/security-penetration-testing/9780134833989&#34;&gt;Security Penetration Testing (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/library/view/wireless-networks-iot/9780134854632/&#34;&gt;Wireless Networks, IoT, and Mobile Devices Hacking (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/videos/enterprise-penetration-testing/9780134854779&#34;&gt;Enterprise Penetration Testing and Continuous Monitoring (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.safaribooksonline.com/videos/hacking-web-applications/9780135261422&#34;&gt;Hacking Web Applications: Security Penetration Testing for Today&#39;s DevOps and Cloud Environments (The Art of Hacking Series)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These courses serve as comprehensive guide for any network and security professional who is starting a career in ethical hacking and penetration testing. It also can help individuals preparing for the &lt;a href=&#34;https://www.offensive-security.com/information-security-certifications/&#34;&gt;Offensive Security Certified Professional (OSCP)&lt;/a&gt;, the &lt;a href=&#34;https://www.eccouncil.org/programs/certified-ethical-hacker-ceh/&#34;&gt;Certified Ethical Hacker (CEH)&lt;/a&gt;, &lt;a href=&#34;https://certification.comptia.org/certifications/pentest&#34;&gt;CompTIA PenTest+&lt;/a&gt; and any other ethical hacking certification. This course helps any cyber security professional that want to learn the skills required to becoming a professional ethical hacker or that want to learn more about general hacking methodologies and concepts.&lt;/p&gt; &#xA;&lt;p&gt;These video courses are published by Pearson, but this GitHub repository is maintained and supported by the lead author of the series &lt;a href=&#34;https://omarsantos.io/&#34;&gt;Omar Santos&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Live Training&lt;/h2&gt; &#xA;&lt;p&gt;Other Live Training and Video Courses: &lt;a href=&#34;https://h4cker.org/training&#34;&gt;https://h4cker.org/training&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>aimacode/aima-python</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/aimacode/aima-python</id>
    <link href="https://github.com/aimacode/aima-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python implementation of algorithms from Russell And Norvig&#39;s &#34;Artificial Intelligence - A Modern Approach&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;code&gt;aima-python&lt;/code&gt; &lt;a href=&#34;https://travis-ci.org/aimacode/aima-python&#34;&gt;&lt;img src=&#34;https://travis-ci.org/aimacode/aima-python.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://mybinder.org/repo/aimacode/aima-python&#34;&gt;&lt;img src=&#34;http://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Python code for the book &lt;em&gt;&lt;a href=&#34;http://aima.cs.berkeley.edu&#34;&gt;Artificial Intelligence: A Modern Approach&lt;/a&gt;.&lt;/em&gt; You can use this in conjunction with a course on AI, or for study on your own. We&#39;re looking for &lt;a href=&#34;https://github.com/aimacode/aima-python/raw/master/CONTRIBUTING.md&#34;&gt;solid contributors&lt;/a&gt; to help.&lt;/p&gt; &#xA;&lt;h1&gt;Updates for 4th Edition&lt;/h1&gt; &#xA;&lt;p&gt;The 4th edition of the book as out now in 2020, and thus we are updating the code. All code here will reflect the 4th edition. Changes include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Move from Python 3.5 to 3.7.&lt;/li&gt; &#xA; &lt;li&gt;More emphasis on Jupyter (Ipython) notebooks.&lt;/li&gt; &#xA; &lt;li&gt;More projects using external packages (tensorflow, etc.).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Structure of the Project&lt;/h1&gt; &#xA;&lt;p&gt;When complete, this project will have Python implementations for all the pseudocode algorithms in the book, as well as tests and examples of use. For each major topic, such as &lt;code&gt;search&lt;/code&gt;, we provide the following files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;search.ipynb&lt;/code&gt; and &lt;code&gt;search.py&lt;/code&gt;: Implementations of all the pseudocode algorithms, and necessary support functions/classes/data. The &lt;code&gt;.py&lt;/code&gt; file is generated automatically from the &lt;code&gt;.ipynb&lt;/code&gt; file; the idea is that it is easier to read the documentation in the &lt;code&gt;.ipynb&lt;/code&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;search_XX.ipynb&lt;/code&gt;: Notebooks that show how to use the code, broken out into various topics (the &lt;code&gt;XX&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tests/test_search.py&lt;/code&gt;: A lightweight test suite, using &lt;code&gt;assert&lt;/code&gt; statements, designed for use with &lt;a href=&#34;http://pytest.org/latest/&#34;&gt;&lt;code&gt;py.test&lt;/code&gt;&lt;/a&gt;, but also usable on their own.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Python 3.7 and up&lt;/h1&gt; &#xA;&lt;p&gt;The code for the 3rd edition was in Python 3.5; the current 4th edition code is in Python 3.7. It should also run in later versions, but does not run in Python 2. You can &lt;a href=&#34;https://www.python.org/downloads&#34;&gt;install Python&lt;/a&gt; or use a browser-based Python interpreter such as &lt;a href=&#34;https://repl.it/languages/python3&#34;&gt;repl.it&lt;/a&gt;. You can run the code in an IDE, or from the command line with &lt;code&gt;python -i filename.py&lt;/code&gt; where the &lt;code&gt;-i&lt;/code&gt; option puts you in an interactive loop where you can run Python functions. All notebooks are available in a &lt;a href=&#34;http://mybinder.org/repo/aimacode/aima-python&#34;&gt;binder environment&lt;/a&gt;. Alternatively, visit &lt;a href=&#34;http://jupyter.org/&#34;&gt;jupyter.org&lt;/a&gt; for instructions on setting up your own Jupyter notebook environment.&lt;/p&gt; &#xA;&lt;p&gt;Features from Python 3.6 and 3.7 that we will be using for this version of the code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.python.org/3.6/whatsnew/3.6.html#whatsnew36-pep498&#34;&gt;f-strings&lt;/a&gt;: all string formatting should be done with &lt;code&gt;f&#39;var = {var}&#39;&lt;/code&gt;, not with &lt;code&gt;&#39;var = {}&#39;.format(var)&lt;/code&gt; nor &lt;code&gt;&#39;var = %s&#39; % var&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.python.org/3.7/library/typing.html&#34;&gt;&lt;code&gt;typing&lt;/code&gt; module&lt;/a&gt;: declare functions with type hints: &lt;code&gt;def successors(state) -&amp;gt; List[State]:&lt;/code&gt;; that is, give type declarations, but omit them when it is obvious. I don&#39;t need to say &lt;code&gt;state: State&lt;/code&gt;, but in another context it would make sense to say &lt;code&gt;s: State&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Underscores in numerics: write a million as &lt;code&gt;1_000_000&lt;/code&gt; not as &lt;code&gt;1000000&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.python.org/3.7/library/dataclasses.html#module-dataclasses&#34;&gt;&lt;code&gt;dataclasses&lt;/code&gt; module&lt;/a&gt;: replace &lt;code&gt;namedtuple&lt;/code&gt; with &lt;code&gt;dataclass&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation Guide&lt;/h2&gt; &#xA;&lt;p&gt;To download the repository:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;git clone https://github.com/aimacode/aima-python.git&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you need to install the basic dependencies to run the project on your system:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd aima-python&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You also need to fetch the datasets from the &lt;a href=&#34;https://github.com/aimacode/aima-data&#34;&gt;&lt;code&gt;aima-data&lt;/code&gt;&lt;/a&gt; repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git submodule init&#xA;git submodule update&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait for the datasets to download, it may take a while. Once they are downloaded, you need to install &lt;code&gt;pytest&lt;/code&gt;, so that you can run the test suite:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install pytest&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then to run the tests:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;py.test&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;And you are good to go!&lt;/p&gt; &#xA;&lt;h1&gt;Index of Algorithms&lt;/h1&gt; &#xA;&lt;p&gt;Here is a table of algorithms, the figure, name of the algorithm in the book and in the repository, and the file where they are implemented in the repository. This chart was made for the third edition of the book and is being updated for the upcoming fourth edition. Empty implementations are a good place for contributors to look for an issue. The &lt;a href=&#34;https://github.com/aimacode/aima-pseudocode&#34;&gt;aima-pseudocode&lt;/a&gt; project describes all the algorithms from the book. An asterisk next to the file name denotes the algorithm is not fully implemented. Another great place for contributors to start is by adding tests and writing on the notebooks. You can see which algorithms have tests and notebook sections below. If the algorithm you want to work on is covered, don&#39;t worry! You can still add more tests and provide some examples of use in the notebook!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Figure&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Name (in 3&lt;sup&gt;rd&lt;/sup&gt; edition)&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Name (in repository)&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;File&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Tests&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Notebook&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Random-Vacuum-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;RandomVacuumAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Model-Based-Vacuum-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;ModelBasedVacuumAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Environment&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Environment&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Agent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Table-Driven-Vacuum-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;TableDrivenVacuumAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Table-Driven-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;TableDrivenAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Reflex-Vacuum-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;ReflexVacuumAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.10&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Simple-Reflex-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;SimpleReflexAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.12&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Model-Based-Reflex-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;ReflexAgentWithState&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/agents.py&#34;&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Problem&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Problem&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Node&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Node&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Queue&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Queue&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/utils.py&#34;&gt;&lt;code&gt;utils.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No Need&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Simple-Problem-Solving-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;SimpleProblemSolvingAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Romania&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;romania&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Tree-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;depth/breadth_first_tree_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Graph-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;depth/breadth_first_graph_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.11&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Breadth-First-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;breadth_first_graph_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Uniform-Cost-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;uniform_cost_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Depth-Limited-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;depth_limited_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Iterative-Deepening-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;iterative_deepening_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Best-First-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;best_first_graph_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.24&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;A*-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;astar_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.26&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Recursive-Best-First-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;recursive_best_first_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Hill-Climbing&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;hill_climbing&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Simulated-Annealing&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;simulated_annealing&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Genetic-Algorithm&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;genetic_algorithm&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.11&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;And-Or-Graph-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;and_or_graph_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.21&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Online-DFS-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;online_dfs_agent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.24&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LRTA*-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;LRTAStarAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Minimax-Decision&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;minimax_decision&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/games.py&#34;&gt;&lt;code&gt;games.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Alpha-Beta-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;alphabeta_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/games.py&#34;&gt;&lt;code&gt;games.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;CSP&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;CSP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/csp.py&#34;&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;AC-3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;AC3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/csp.py&#34;&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Backtracking-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;backtracking_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/csp.py&#34;&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Min-Conflicts&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;min_conflicts&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/csp.py&#34;&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6.11&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Tree-CSP-Solver&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;tree_csp_solver&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/csp.py&#34;&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;KB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;KB&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;KB-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;KB_AgentProgram&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Propositional Logic Sentence&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Expr&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/utils.py&#34;&gt;&lt;code&gt;utils.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.10&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;TT-Entails&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;tt_entails&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.12&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;PL-Resolution&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;pl_resolution&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Convert to CNF&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;to_cnf&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;PL-FC-Entails?&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;pl_fc_entails&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DPLL-Satisfiable?&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;dpll_satisfiable&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.18&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;WalkSAT&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;WalkSAT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.20&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Hybrid-Wumpus-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;HybridWumpusAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.22&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SATPlan&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;SAT_plan&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Subst&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;subst&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Unify&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;unify&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;FOL-FC-Ask&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;fol_fc_ask&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.6&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;FOL-BC-Ask&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;fol_bc_ask&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Air-Cargo-problem&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;air_cargo&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Spare-Tire-Problem&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;spare_tire&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Three-Block-Tower&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;three_block_tower&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Cake-Problem&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;have_cake_and_eat_cake_too&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Graphplan&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;GraphPlan&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.13&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Partial-Order-Planner&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;PartialOrderPlanner&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Job-Shop-Problem-With-Resources&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;job_shop_problem&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Hierarchical-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;hierarchical_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Angelic-Search&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;angelic_search&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.10&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Doubles-tennis&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;double_tennis_problem&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/planning.py&#34;&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Discrete Probability Distribution&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;ProbDist&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;13.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DT-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DTAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Enumeration-Ask&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;enumeration_ask&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.11&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Elimination-Ask&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;elimination_ask&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.13&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Prior-Sample&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;prior_sample&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.14&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Rejection-Sampling&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;rejection_sampling&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.15&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Likelihood-Weighting&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;likelihood_weighting&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;14.16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Gibbs-Ask&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;gibbs_ask&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Forward-Backward&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;forward_backward&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.6&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fixed-Lag-Smoothing&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;fixed_lag_smoothing&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.17&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Particle-Filtering&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;particle_filtering&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Information-Gathering-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;InformationGatheringAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Value-Iteration&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;value_iteration&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/mdp.py&#34;&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Policy-Iteration&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;policy_iteration&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/mdp.py&#34;&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;POMDP-Value-Iteration&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;pomdp_value_iteration&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/mdp.py&#34;&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Decision-Tree-Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DecisionTreeLearner&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/learning.py&#34;&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Cross-Validation&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;cross_validation&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/learning.py&#34;&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;*&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.11&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Decision-List-Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DecisionListLearner&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/learning.py&#34;&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;*&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.24&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Back-Prop-Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;BackPropagationLearner&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/learning.py&#34;&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.34&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;AdaBoost&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;AdaBoost&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/learning.py&#34;&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Current-Best-Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;current_best_learning&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/knowledge.py&#34;&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Version-Space-Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;version_space_learning&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/knowledge.py&#34;&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Minimal-Consistent-Det&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;minimal_consistent_det&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/knowledge.py&#34;&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;19.12&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;FOIL&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;FOIL_container&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/knowledge.py&#34;&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Passive-ADP-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;PassiveADPAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/rl.py&#34;&gt;&lt;code&gt;rl.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Passive-TD-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;PassiveTDAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/rl.py&#34;&gt;&lt;code&gt;rl.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Q-Learning-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;QLearningAgent&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/rl.py&#34;&gt;&lt;code&gt;rl.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;HITS&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;HITS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/nlp.py&#34;&gt;&lt;code&gt;nlp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chart-Parse&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Chart&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/nlp.py&#34;&gt;&lt;code&gt;nlp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;23.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;CYK-Parse&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;CYK_parse&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/nlp.py&#34;&gt;&lt;code&gt;nlp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;25.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Monte-Carlo-Localization&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;monte_carlo_localization&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/probability.py&#34;&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Done&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Included&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Index of data structures&lt;/h1&gt; &#xA;&lt;p&gt;Here is a table of the implemented data structures, the figure, name of the implementation in the repository, and the file where they are implemented.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Figure&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Name (in repository)&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;File&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;romania_map&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;vacumm_world&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.23&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;one_dim_state_space&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;australia_map&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/search.py&#34;&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.13&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;wumpus_world_inference&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.16&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;horn_clauses_KB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/logic.py&#34;&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;17.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;sequential_decision_environment&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/mdp.py&#34;&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;18.2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;waiting_decision_tree&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aimacode/aima-python/master/learning.py&#34;&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Many thanks for contributions over the years. I got bug reports, corrected code, and other support from Darius Bacon, Phil Ruggera, Peng Shao, Amit Patil, Ted Nienstedt, Jim Martin, Ben Catanzariti, and others. Now that the project is on GitHub, you can see the &lt;a href=&#34;https://github.com/aimacode/aima-python/graphs/contributors&#34;&gt;contributors&lt;/a&gt; who are doing a great job of actively improving the project. Many thanks to all contributors, especially &lt;a href=&#34;https://github.com/darius&#34;&gt;@darius&lt;/a&gt;, &lt;a href=&#34;https://github.com/SnShine&#34;&gt;@SnShine&lt;/a&gt;, &lt;a href=&#34;https://github.com/reachtarunhere&#34;&gt;@reachtarunhere&lt;/a&gt;, &lt;a href=&#34;https://github.com/antmarakis&#34;&gt;@antmarakis&lt;/a&gt;, &lt;a href=&#34;https://github.com/Chipe1&#34;&gt;@Chipe1&lt;/a&gt;, &lt;a href=&#34;https://github.com/ad71&#34;&gt;@ad71&lt;/a&gt; and &lt;a href=&#34;https://github.com/MariannaSpyrakou&#34;&gt;@MariannaSpyrakou&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!--Reference Links--&gt;</summary>
  </entry>
  <entry>
    <title>aws-samples/aws-machine-learning-university-accelerated-nlp</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/aws-samples/aws-machine-learning-university-accelerated-nlp</id>
    <link href="https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Machine Learning University: Accelerated Natural Language Processing Class&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aws-samples/aws-machine-learning-university-accelerated-nlp/master/data/MLU_Logo.png&#34; alt=&#34;logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Machine Learning University: Accelerated Natural Language Processing Class&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains &lt;strong&gt;slides&lt;/strong&gt;, &lt;strong&gt;notebooks&lt;/strong&gt; and &lt;strong&gt;datasets&lt;/strong&gt; for the &lt;strong&gt;Machine Learning University (MLU) Accelerated Natural Language Processing&lt;/strong&gt; class. Our mission is to make Machine Learning accessible to everyone. We have courses available across many topics of machine learning and believe knowledge of ML can be a key enabler for success. This class is designed to help you get started with Natural Language Processing (NLP), learn widely used techniques and apply them on real-world problems.&lt;/p&gt; &#xA;&lt;h2&gt;YouTube&lt;/h2&gt; &#xA;&lt;p&gt;Watch all NLP class video recordings in this &lt;a href=&#34;https://www.youtube.com/playlist?list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&#34;&gt;YouTube playlist&lt;/a&gt; from our &lt;a href=&#34;https://www.youtube.com/channel/UC12LqyqTQYbXatYS9AA7Nuw/playlists&#34;&gt;YouTube channel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/0FXKbEgz-uU/0.jpg&#34; alt=&#34;Playlist&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Course Overview&lt;/h2&gt; &#xA;&lt;p&gt;There are three lectures and one final project in this class. Course overview is below.&lt;/p&gt; &#xA;&lt;p&gt;Lecture 1&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;title&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;studio lab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to ML&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Intro to NLP and Text Processing&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture1-Text-Process.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Bag of Words (BoW)&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture1-BOW.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;K Nearest Neighbors (KNN)&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture1-KNN.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Final Project&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture1-Final-Project.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Lecture 2&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;title&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;studio lab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Tree-based Models&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github//aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture2-Tree-Models.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Regression Models&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;Linear Regression &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture2-Linear-Regression.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt; &lt;br&gt; Logistic Regression &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture2-Logistic-Regression.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Optimization-Regularization&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hyperparameter Tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AWS AI/ML Services&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture2-Sagemaker.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Final Project&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture2-Final-Project.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Lecture 3&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;title&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;studio lab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Neural Networks&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture3-Neural-Networks.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Word Embeddings&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture3-Word-Vectors.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Recurrent Neural Networks (RNN)&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github//aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture3-Recurrent-Neural-Networks.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Transformers&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture3-Neural-Networks.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Final Project&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture3-Final-Project.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Final Project:&lt;/strong&gt; Practice working with a &#34;real-world&#34; NLP dataset for the final project. Final project dataset is in the &lt;a href=&#34;https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp/tree/master/data/final_project&#34;&gt;data/final_project folder&lt;/a&gt;. For more details on the final project, check out &lt;a href=&#34;https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp/raw/master/notebooks/MLA-NLP-Lecture1-Final-Project.ipynb&#34;&gt;this notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Interactives/Visuals&lt;/h2&gt; &#xA;&lt;p&gt;Interested in visual, interactive explanations of core machine learning concepts? Check out our &lt;a href=&#34;https://mlu-explain.github.io/&#34;&gt;MLU-Explain articles&lt;/a&gt; to learn at your own pace!&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to contribute to the project, see &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/aws-machine-learning-university-accelerated-nlp/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The license for this repository depends on the section. Data set for the course is being provided to you by permission of Amazon and is subject to the terms of the &lt;a href=&#34;https://www.amazon.com/gp/help/customer/display.html?nodeId=201909000&#34;&gt;Amazon License and Access&lt;/a&gt;. You are expressly prohibited from copying, modifying, selling, exporting or using this data set in any way other than for the purpose of completing this course. The lecture slides are released under the CC-BY-SA-4.0 License. The code examples are released under the MIT-0 License. See each section&#39;s LICENSE file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure/Azure-Sentinel</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/Azure/Azure-Sentinel</id>
    <link href="https://github.com/Azure/Azure-Sentinel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cloud-native SIEM for intelligent security analytics for your entire enterprise.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Microsoft Sentinel and Microsoft 365 Defender&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the unified Microsoft Sentinel and Microsoft 365 Defender repository! This repository contains out of the box detections, exploration queries, hunting queries, workbooks, playbooks and much more to help you get ramped up with Microsoft Sentinel and provide you security content to secure your environment and hunt for threats. The hunting queries also include Microsoft 365 Defender hunting queries for advanced hunting scenarios in both Microsoft 365 Defender and Microsoft Sentinel. You can also submit to &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues&#34;&gt;issues&lt;/a&gt; for any samples or resources you would like to see here as you onboard to Microsoft Sentinel. This repository welcomes contributions and refer to this repository&#39;s &lt;a href=&#34;https://aka.ms/threathunters&#34;&gt;wiki&lt;/a&gt; to get started. For questions and feedback, please contact &lt;a href=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/AzureSentinel@microsoft.com&#34;&gt;AzureSentinel@microsoft.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Resources&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://go.microsoft.com/fwlink/?linkid=2073774&amp;amp;clcid=0x409&#34;&gt;Microsoft Sentinel documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/microsoft-365/security/defender/microsoft-365-defender?view=o365-worldwide&#34;&gt;Microsoft 365 Defender documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/securitywebinars&#34;&gt;Security Community Webinars&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.github.com/en#dotcom&#34;&gt;Getting started with GitHub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We value your feedback. Here are some channels to help surface your questions or feedback:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;General product specific Q&amp;amp;A for SIEM and SOAR - Join in the &lt;a href=&#34;https://techcommunity.microsoft.com/t5/microsoft-sentinel/bd-p/MicrosoftSentinel&#34;&gt;Microsoft Sentinel Tech Community conversations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;General product specific Q&amp;amp;A for XDR - Join in the &lt;a href=&#34;https://techcommunity.microsoft.com/t5/microsoft-365-defender/bd-p/MicrosoftThreatProtection&#34;&gt;Microsoft 365 Defender Tech Community conversations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Product specific feature requests - Upvote or post new on &lt;a href=&#34;https://feedback.azure.com/d365community/forum/37638d17-0625-ec11-b6e6-000d3a4f07b8&#34;&gt;Microsoft Sentinel feedback forums&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Report product or contribution bugs - File a GitHub Issue using &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;amp;labels=&amp;amp;template=bug_report.md&amp;amp;title=&#34;&gt;Bug template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;General feedback on community and contribution process - File a GitHub Issue using &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;amp;labels=&amp;amp;template=feature_request.md&amp;amp;title=&#34;&gt;Feature Request template&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.microsoft.com&#34;&gt;https://cla.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Add in your new or updated contributions to GitHub&lt;/h2&gt; &#xA;&lt;p&gt;Note: If you are a first time contributor to this repository, &lt;a href=&#34;https://docs.github.com/github/getting-started-with-github/fork-a-repo&#34;&gt;General GitHub Fork the repo guidance&lt;/a&gt; before cloning or &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/raw/master/GettingStarted.md&#34;&gt;Specific steps for the Sentinel repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;General Steps&lt;/h2&gt; &#xA;&lt;p&gt;Brand new or update to a contribution via these methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Submit for review directly on GitHub website &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Browse to the folder you want to upload your file to&lt;/li&gt; &#xA;   &lt;li&gt;Choose Upload Files and browse to your file.&lt;/li&gt; &#xA;   &lt;li&gt;You will be required to create your own branch and then submit the Pull Request for review.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://help.github.com/en/desktop/getting-started-with-github-desktop&#34;&gt;GitHub Desktop&lt;/a&gt; or &lt;a href=&#34;https://visualstudio.microsoft.com/vs/&#34;&gt;Visual Studio&lt;/a&gt; or &lt;a href=&#34;https://code.visualstudio.com/?wt.mc_id=DX_841432&#34;&gt;VSCode&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.github.com/github/getting-started-with-github/fork-a-repo&#34;&gt;Fork the repo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository&#34;&gt;Clone the repo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/desktop/contributing-to-projects/creating-a-branch-for-your-work&#34;&gt;Create your own branch&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Do your additions/updates in GitHub Desktop&lt;/li&gt; &#xA;   &lt;li&gt;Be sure to merge master back to your branch before you push.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/using-git/pushing-commits-to-a-remote-repository&#34;&gt;Push your changes to GitHub&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pull Request&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;After you push your changes, you will need to submit the &lt;a href=&#34;https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests&#34;&gt;Pull Request (PR)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Details about the Proposed Changes are required, be sure to include a minimal level of detail so a review can clearly understand the reason for the change and what he change is related to in the code.&lt;/li&gt; &#xA; &lt;li&gt;After submission, check the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/pulls&#34;&gt;Pull Request&lt;/a&gt; for comments&lt;/li&gt; &#xA; &lt;li&gt;Make changes as suggested and update your branch or explain why no change is needed. Resolve the comment when done.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pull Request Detection Template Structure Validation Check&lt;/h3&gt; &#xA;&lt;p&gt;As part of the PR checks we run a structure validation to make sure all required parts of the YAML structure are included. For Detections, there is a new section that must be included. See the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/wiki/Contribute-to-Sentinel-GitHub-Community-of-Queries#now-onto-the-how&#34;&gt;contribution guidelines&lt;/a&gt; for more information. If this section or any other required section is not included, then a validation error will occur similar to the below. The example is specifically if the YAML is missing the entityMappings section:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;A total of 1 test files matched the specified pattern.&#xA;[xUnit.net 00:00:00.95]     Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [FAIL]&#xA;  X Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [104ms]&#xA;  Error Message:&#xA;   Expected object to be &amp;lt;null&amp;gt;, but found System.ComponentModel.DataAnnotations.ValidationException with message &#34;An old mapping for entity &#39;AccountCustomEntity&#39; does not have a matching new mapping entry.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pull Request Kql Validation Check&lt;/h3&gt; &#xA;&lt;p&gt;As part of the PR checks we run a syntax validation of the kql queries defined in the template. If this check fails go to Azure Pipeline (by pressing on the errors link on the checks tab in your PR) &lt;img src=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/.github/Media/Azurepipeline.png&#34; alt=&#34;Azurepipeline&#34;&gt; In the pipeline you can see which test failed and what is the cause: &lt;img src=&#34;https://raw.githubusercontent.com/Azure/Azure-Sentinel/master/.github/Media/PipelineTestsTab.png&#34; alt=&#34;Pipeline Tests Tab&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Example error message:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;A total of 1 test files matched the specified pattern.&#xA;[xUnit.net 00:00:01.81]     Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [FAIL]&#xA;  X Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &#34;ExcessiveBlockedTrafficGeneratedbyUser.yaml&#34;) [21ms]&#xA;  Error Message:&#xA;   Template Id:fa0ab69c-7124-4f62-acdd-61017cf6ce89 is not valid Errors:The name &#39;SymantecEndpointProtection&#39; does not refer to any known table, tabular variable or function., Code: &#39;KS204&#39;, Severity: &#39;Error&#39;, Location: &#39;67..93&#39;,The name &#39;SymantecEndpointProtection&#39; does not refer to any known table, tabular variable or function., Code: &#39;KS204&#39;, Severity: &#39;Error&#39;, Location: &#39;289..315&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using custom logs table (a table which is not defined on all workspaces by default) you should verify your table schema is defined in json file in the folder &lt;em&gt;Azure-Sentinel\.script\tests\KqlvalidationsTests\CustomTables&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example for table tablexyz.json&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;Name&#34;: &#34;tablexyz&#34;,&#xA;  &#34;Properties&#34;: [&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeDateTimeColumn&#34;,&#xA;      &#34;Type&#34;: &#34;DateTime&#34;&#xA;    },&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeStringColumn&#34;,&#xA;      &#34;Type&#34;: &#34;String&#34;&#xA;    },&#xA;    {&#xA;      &#34;Name&#34;: &#34;SomeDynamicColumn&#34;,&#xA;      &#34;Type&#34;: &#34;Dynamic&#34;&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Kql Validation Locally&lt;/h3&gt; &#xA;&lt;p&gt;In order to run the kql validation before submitting Pull Request in you local machine:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need to have &lt;strong&gt;.Net Core 3.1 SDK&lt;/strong&gt; installed &lt;a href=&#34;https://dotnet.microsoft.com/download&#34;&gt;How to download .Net&lt;/a&gt; (Supports all platforms)&lt;/li&gt; &#xA; &lt;li&gt;Open Shell and navigate to &lt;code&gt;Azure-Sentinel\\.script\tests\KqlvalidationsTests\&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;dotnet test&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of output (in Ubuntu):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Welcome to .NET Core 3.1!&#xA;---------------------&#xA;SDK Version: 3.1.403&#xA;&#xA;Telemetry&#xA;---------&#xA;The .NET Core tools collect usage data in order to help us improve your experience. The data is anonymous. It is collected by Microsoft and shared with the community. You can opt-out of telemetry by setting the DOTNET_CLI_TELEMETRY_OPTOUT environment variable to &#39;1&#39; or &#39;true&#39; using your favorite shell.&#xA;&#xA;Read more about .NET Core CLI Tools telemetry: https://aka.ms/dotnet-cli-telemetry&#xA;&#xA;----------------&#xA;Explore documentation: https://aka.ms/dotnet-docs&#xA;Report issues and find source on GitHub: https://github.com/dotnet/core&#xA;Find out what&#39;s new: https://aka.ms/dotnet-whats-new&#xA;Learn about the installed HTTPS developer cert: https://aka.ms/aspnet-core-https&#xA;Use &#39;dotnet --help&#39; to see available commands or visit: https://aka.ms/dotnet-cli-docs&#xA;Write your first app: https://aka.ms/first-net-core-app&#xA;--------------------------------------------------------------------------------------&#xA;Test run for /mnt/c/git/Azure-Sentinel/.script/tests/KqlvalidationsTests/bin/Debug/netcoreapp3.1/Kqlvalidations.Tests.dll(.NETCoreApp,Version=v3.1)&#xA;Microsoft (R) Test Execution Command Line Tool Version 16.7.0&#xA;Copyright (c) Microsoft Corporation.  All rights reserved.&#xA;&#xA;Starting test execution, please wait...&#xA;&#xA;A total of 1 test files matched the specified pattern.&#xA;&#xA;Test Run Successful.&#xA;Total tests: 171&#xA;     Passed: 171&#xA; Total time: 25.7973 Seconds&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Detection schema validation tests&lt;/h3&gt; &#xA;&lt;p&gt;Similarly to KQL Validation, there is an automatic validation of the schema of a detection. The schema validation includes the detection&#39;s frequency and period, the detection&#39;s trigger type and threshold, validity of connectors Ids (&lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/raw/master/.script/tests/detectionTemplateSchemaValidation/ValidConnectorIds.json&#34;&gt;valid connectors Ids list&lt;/a&gt;), etc. A wrong format or missing attributes will result with an informative check failure, which should guide you through the resolution of the issue, but make sure to look into the format of already approved detection.&lt;/p&gt; &#xA;&lt;h3&gt;Run Detection Schema Validation Locally&lt;/h3&gt; &#xA;&lt;p&gt;In order to run the kql validation before submitting Pull Request in you local machine:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need to have &lt;strong&gt;.Net Core 3.1 SDK&lt;/strong&gt; installed &lt;a href=&#34;https://dotnet.microsoft.com/download&#34;&gt;How to download .Net&lt;/a&gt; (Supports all platforms)&lt;/li&gt; &#xA; &lt;li&gt;Open Shell and navigate to &lt;code&gt;Azure-Sentinel\\.script\tests\DetectionTemplateSchemaValidation\&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;dotnet test&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;p&gt;For information on what you can contribute and further details, refer to the &lt;a href=&#34;https://github.com/Azure/Azure-Sentinel/wiki#get-started&#34;&gt;&#34;get started&#34;&lt;/a&gt; section on the project&#39;s &lt;a href=&#34;https://aka.ms/threathunters&#34;&gt;wiki&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>EssayKillerBrain/EssayTopicPredict</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/EssayKillerBrain/EssayTopicPredict</id>
    <link href="https://github.com/EssayKillerBrain/EssayTopicPredict" rel="alternate"></link>
    <summary type="html">&lt;p&gt;高考作文题目预测模型 v1.0&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EssayTopicPredict&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache--2.0-green&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-MIT-orange&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Anti--996-red&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/pypi-v0.0.1a4-yellowgreen&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/stars-%3C%201k-blue&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/issues-1%20open-brightgreen&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;通用型高考作文题目预测模型 v1.0 -人工智能框架，仅限交流与科普。&lt;/p&gt; &#xA;&lt;h2&gt;项目简介&lt;/h2&gt; &#xA;&lt;p&gt;EssayTopicPredict是基于无监督学习、模式识别与NLP领域的最新模型所构建的生成式考试题目AI框架，目前第一版finetune模型针对高考作文，可以有效生成符合人类认知的文章题目。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;项目作者&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;主页1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;主页2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;主页3&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;图灵的猫&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.zhihu.com/people/dong-xi-97-29&#34;&gt;知乎&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://space.bilibili.com/371846699&#34;&gt;B站&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCoEVP6iTw5sfozUGLLWJyDg/featured&#34;&gt;Youtube&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;致谢&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;感谢开源作者&lt;a href=&#34;https://github.com/imcaspar&#34;&gt;@imcaspar&lt;/a&gt; 提供GPT-2中文预训练框架与数据支持。 &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;框架说明&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 基于哈工大RoBerta-WWM-EXT、Bertopic、GAN模型的高考题目预测AI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持bert tokenizer，当前版本基于clue chinese vocab&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 17亿参数多模块异构深度神经网络，超2亿条预训练数据&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 可结合作文生成器一起使用：&lt;a href=&#34;https://colab.research.google.com/github/EssayKillerBrain/EssayKiller_V2/blob/master/colab_online.ipynb&#34;&gt;17亿参数作文杀手&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 端到端生成，从试卷识别到答题卡输出一条龙服务&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;本地环境&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ubuntu 18.04.2/ Windows10 x86&lt;/li&gt; &#xA; &lt;li&gt;Pandas 0.24.2&lt;/li&gt; &#xA; &lt;li&gt;Regex 2019.4.14&lt;/li&gt; &#xA; &lt;li&gt;h5py 2.9.0&lt;/li&gt; &#xA; &lt;li&gt;Numpy 1.16.2&lt;/li&gt; &#xA; &lt;li&gt;Tensorboard 1.15.2&lt;/li&gt; &#xA; &lt;li&gt;Tensorflow-gpu 1.15.2&lt;/li&gt; &#xA; &lt;li&gt;Requests 2.22.0&lt;/li&gt; &#xA; &lt;li&gt;OpenCV 3.4.2&lt;/li&gt; &#xA; &lt;li&gt;CUDA &amp;gt;= 10.0&lt;/li&gt; &#xA; &lt;li&gt;CuDNN &amp;gt;= 7.6.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;开发日志&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2022.04.23 本地Git项目建立&lt;/li&gt; &#xA; &lt;li&gt;2022.05.03 整体模型架构搭建，开始语料收集&lt;/li&gt; &#xA; &lt;li&gt;2022.05.13 数据集清洗、语料处理&lt;/li&gt; &#xA; &lt;li&gt;2022.05.21 Bertopic+DBSCAN聚类算法&lt;/li&gt; &#xA; &lt;li&gt;2022.05.31 RoBerta与摘要模型调整&lt;/li&gt; &#xA; &lt;li&gt;2022.05.30 代码Review与开源发布&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;模型结构&lt;/h2&gt; &#xA;&lt;p&gt;整个框架分为Proprocess、Bert、DNSCAN 3个模块，每个模块的网络单独训练，参数相互独立。&lt;/p&gt; &#xA;&lt;h3&gt;1. 例子&lt;/h3&gt; &#xA;&lt;p&gt;高考语文试卷作文题&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://images.shobserver.com/img/2020/7/7/37b2224ee3de441a8a040cb4f5576c2d.jpg&#34; alt=&#34;浙江卷&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;数据准备&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;人民日报、央视新闻、微博客户端、人民网4个主要爬虫渠道，通过不同API进行爬取（时间为过去12个月内）&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;修改/train/config.py中train_data_root，validation_data_root以及image_path&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;训练&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd train  &#xA;python train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;训练结果&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Epoch 3/100&#xA;25621/25621 [==============================] - 15856s 619ms/step - loss: 0.1035 - acc: 0.9816 - val_loss: 0.1060 - val_acc: 0.9823&#xA;Epoch 4/100&#xA;25621/25621 [==============================] - 15651s 611ms/step - loss: 0.0798 - acc: 0.9879 - val_loss: 0.0848 - val_acc: 0.9878&#xA;Epoch 5/100&#xA;25621/25621 [==============================] - 16510s 644ms/step - loss: 0.0732 - acc: 0.9889 - val_loss: 0.0815 - val_acc: 0.9881&#xA;Epoch 6/100&#xA;25621/25621 [==============================] - 15621s 610ms/step - loss: 0.0691 - acc: 0.9895 - val_loss: 0.0791 - val_acc: 0.9886&#xA;Epoch 7/100&#xA;25621/25621 [==============================] - 15782s 616ms/step - loss: 0.0666 - acc: 0.9899 - val_loss: 0.0787 - val_acc: 0.9887&#xA;Epoch 8/100&#xA;25621/25621 [==============================] - 15560s 607ms/step - loss: 0.0645 - acc: 0.9903 - val_loss: 0.0771 - val_acc: 0.9888&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;2. 网络结构&lt;/h3&gt; &#xA;&lt;h4&gt;2.1 BERT&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Whole Word Masking (wwm)&lt;/strong&gt;，暂翻译为&lt;code&gt;全词Mask&lt;/code&gt;或&lt;code&gt;整词Mask&lt;/code&gt;，是谷歌在2019年5月31日发布的一项BERT的升级版本，主要更改了原预训练阶段的训练样本生成策略。 简单来说，原有基于WordPiece的分词方式会把一个完整的词切分成若干个子词，在生成训练样本时，这些被分开的子词会随机被mask。 在&lt;code&gt;全词Mask&lt;/code&gt;中，如果一个完整的词的部分WordPiece子词被mask，则同属该词的其他部分也会被mask，即&lt;code&gt;全词Mask&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;需要注意的是，这里的mask指的是广义的mask（替换成[MASK]；保持原词汇；随机替换成另外一个词），并非只局限于单词替换成&lt;code&gt;[MASK]&lt;/code&gt;标签的情况。 更详细的说明及样例请参考：&lt;a href=&#34;https://github.com/ymcui/Chinese-BERT-wwm/issues/4&#34;&gt;#4&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;同理，由于谷歌官方发布的&lt;code&gt;BERT-base, Chinese&lt;/code&gt;中，中文是以&lt;strong&gt;字&lt;/strong&gt;为粒度进行切分，没有考虑到传统NLP中的中文分词（CWS）。 我们将全词Mask的方法应用在了中文中，使用了中文维基百科（包括简体和繁体）进行训练，并且使用了&lt;a href=&#34;http://ltp.ai&#34;&gt;哈工大LTP&lt;/a&gt;作为分词工具，即对组成同一个&lt;strong&gt;词&lt;/strong&gt;的汉字全部进行Mask。&lt;/p&gt; &#xA;&lt;p&gt;下述文本展示了&lt;code&gt;全词Mask&lt;/code&gt;的生成样例。 &lt;strong&gt;注意：为了方便理解，下述例子中只考虑替换成[MASK]标签的情况。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;说明&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;样例&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;原始文本&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;使用语言模型来预测下一个词的probability。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;分词文本&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;使用 语言 模型 来 预测 下 一个 词 的 probability 。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;原始Mask输入&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;使 用 语 言 [MASK] 型 来 [MASK] 测 下 一 个 词 的 pro [MASK] ##lity 。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;全词Mask输入&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;使 用 语 言 [MASK] [MASK] 来 [MASK] [MASK] 下 一 个 词 的 [MASK] [MASK] [MASK] 。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;2.2 DBSCAN&lt;/h4&gt; &#xA;&lt;p&gt;基于密度的噪声应用空间聚类(DBSCAN)是一种无监督的ML聚类算法。无监督的意思是它不使用预先标记的目标来聚类数据点。聚类是指试图将相似的数据点分组到人工确定的组或簇中。它可以替代KMeans和层次聚类等流行的聚类算法。&lt;/p&gt; &#xA;&lt;p&gt;KMeans vs DBSCAN： KMeans尤其容易受到异常值的影响。当算法遍历质心时，在达到稳定性和收敛性之前，离群值对质心的移动方式有显著的影响。此外，KMeans在集群大小和密度不同的情况下还存在数据精确聚类的问题。K-Means只能应用球形簇，如果数据不是球形的，它的准确性就会受到影响。最后，KMeans要求我们首先选择希望找到的集群的数量。&lt;/p&gt; &#xA;&lt;p&gt;另一方面，DBSCAN不要求我们指定集群的数量，避免了异常值，并且在任意形状和大小的集群中工作得非常好。它没有质心，聚类簇是通过将相邻的点连接在一起的过程形成的。&lt;/p&gt; &#xA;&lt;h2&gt;中文模型下载&lt;/h2&gt; &#xA;&lt;p&gt;本目录中主要包含base模型，故我们不在模型简称中标注&lt;code&gt;base&lt;/code&gt;字样。对于其他大小的模型会标注对应的标记（例如large）。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;BERT-large模型&lt;/code&gt;&lt;/strong&gt;：24-layer, 1024-hidden, 16-heads, 330M parameters&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;BERT-base模型&lt;/code&gt;&lt;/strong&gt;：12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;注意：开源版本不包含MLM任务的权重；如需做MLM任务，请使用额外数据进行二次预训练（和其他下游任务一样）。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;模型简称&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;语料&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Google下载&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;百度网盘下载&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;RBT6, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EXT数据&lt;sup&gt;[1]&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1_MDAIYIGVgDovWkSs51NDA?pwd=hniy&#34;&gt;TensorFlow（密码hniy）&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;RBT4, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EXT数据&lt;sup&gt;[1]&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1MUrmuTULnMn3L1aw_dXxSA?pwd=sjpt&#34;&gt;TensorFlow（密码sjpt）&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;RBTL3, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EXT数据&lt;sup&gt;[1]&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1Jzn1hYwmv0kXkfTeIvNT61Rn1IbRc-o8&#34;&gt;TensorFlow&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1qs5OasLXXjOnR2XuGUh12NanUl0pkjEv&#34;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1vV9ClBMbsSpt8wUpfQz62Q?pwd=s6cu&#34;&gt;TensorFlow（密码s6cu）&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;RBT3, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EXT数据&lt;sup&gt;[1]&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1-rvV0nBDvRCASbRz8M9Decc3_8Aw-2yi&#34;&gt;TensorFlow&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1_LqmIxm8Nz1Abvlqb8QFZaxYo-TInOed&#34;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1AnapwWj1YBZ_4E6AAtj2lg?pwd=5a57&#34;&gt;TensorFlow（密码5a57）&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;RoBERTa-wwm-ext-large, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EXT数据&lt;sup&gt;[1]&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1dtad0FFzG11CBsawu8hvwwzU2R0FDI94&#34;&gt;TensorFlow&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1-2vEZfIFCdM1-vJ3GD6DlSyKT4eVXMKq&#34;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1F68xzCLWEonTEVP7HQ0Ciw?pwd=dqqe&#34;&gt;TensorFlow（密码dqqe）&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;RoBERTa-wwm-ext, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EXT数据&lt;sup&gt;[1]&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1jMAKIJmPn7kADgD3yQZhpsqM-IRM1qZt&#34;&gt;TensorFlow&lt;/a&gt;&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1eHM3l4fMo6DsQYGmey7UZGiTmQquHw25&#34;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1oR0cgSXE3Nz6dESxr98qVA?pwd=vybq&#34;&gt;TensorFlow（密码vybq）&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;BERT-wwm-ext, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EXT数据&lt;sup&gt;[1]&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1buMLEjdtrXE2c4G1rpsNGWEx7lUQ0RHi&#34;&gt;TensorFlow&lt;/a&gt;&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1iNeYFhCBJWeUsIlnW_2K6SMwXkM4gLb_&#34;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1x-jIw1X2yNYHGak2yiq4RQ?pwd=wgnt&#34;&gt;TensorFlow（密码wgnt）&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;&lt;code&gt;BERT-wwm, Chinese&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;中文维基&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1RoTQsXp2hkQ1gSRVylRIJfQxJUgkfJMW&#34;&gt;TensorFlow&lt;/a&gt;&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;&lt;a href=&#34;https://drive.google.com/open?id=1AQitrjbvCWc51SYiLN-cJq4e0WiNN4KY&#34;&gt;PyTorch&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://pan.baidu.com/s/1HDdDXiYxGT5ub5OeO7qdWw?pwd=qfh8&#34;&gt;TensorFlow（密码qfh8）&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;BERT-base, Chinese&lt;/code&gt;&lt;sup&gt;Google&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;中文维基&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip&#34;&gt;Google Cloud&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;BERT-base, Multilingual Cased&lt;/code&gt;&lt;sup&gt;Google&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;多语种维基&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip&#34;&gt;Google Cloud&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;BERT-base, Multilingual Uncased&lt;/code&gt;&lt;sup&gt;Google&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;多语种维基&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip&#34;&gt;Google Cloud&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[1] EXT数据包括：中文维基百科，其他百科、新闻、问答等数据，总词数达5.4B。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;查看更多哈工大讯飞联合实验室（HFL）发布的资源：&lt;a href=&#34;https://github.com/ymcui/HFL-Anthology&#34;&gt;https://github.com/ymcui/HFL-Anthology&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run.py --model bert&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-16-40-19.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;测试时，需要用正则表达式过滤考试专用词，包括“阅读下面的材料，根据要求写作”，“要求：xxx”，“请完成/请结合/请综合xx”。&lt;/p&gt; &#xA;&lt;p&gt;比如&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-17-17-30.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;人们用眼睛看他人、看世界，却无法直接看到完整的自己。所以，在人生的旅程中，我们需要寻找各种“镜子”、不断绘制“自画像”来审视自我，尝试回答“我是怎样的人”“我想过怎样的生活”“我能做些什么”“如何生活得更有意义”等重要的问题。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{EssayKillerBrain,&#xA;  author = {Turing&#39;s Cat},&#xA;  title = {Autowritting Ai Framework},&#xA;  year = {2020},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/AlanTur1ng/EssayTopicPredict}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;参考资料&lt;/h2&gt; &#xA;&lt;p&gt;[1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;br&gt; [2] ERNIE: Enhanced Representation through Knowledge Integration&lt;br&gt; [3] Fine-tune BERT for Extractive Summarization&lt;br&gt; [4] EAST: An Efficient and Accurate Scene Text Detector&lt;br&gt; [5] An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition&lt;br&gt; [6] Language Models are Unsupervised Multitask Learners&lt;br&gt; [7] &lt;a href=&#34;https://github.com/Morizeyao/GPT2-Chinese&#34;&gt;https://github.com/Morizeyao/GPT2-Chinese&lt;/a&gt;&lt;br&gt; [8] &lt;a href=&#34;https://github.com/argman/EAST&#34;&gt;https://github.com/argman/EAST&lt;/a&gt;&lt;br&gt; [9] &lt;a href=&#34;https://github.com/bgshih/crnn&#34;&gt;https://github.com/bgshih/crnn&lt;/a&gt;&lt;br&gt; [10] &lt;a href=&#34;https://github.com/zhiyou720/chinese_summarizer&#34;&gt;https://github.com/zhiyou720/chinese_summarizer&lt;/a&gt;&lt;br&gt; [11] &lt;a href=&#34;https://zhuanlan.zhihu.com/p/64737915&#34;&gt;https://zhuanlan.zhihu.com/p/64737915&lt;/a&gt;&lt;br&gt; [12] &lt;a href=&#34;https://github.com/ouyanghuiyu/chineseocr_lite&#34;&gt;https://github.com/ouyanghuiyu/chineseocr_lite&lt;/a&gt;&lt;br&gt; [13] &lt;a href=&#34;https://github.com/google-research/bert&#34;&gt;https://github.com/google-research/bert&lt;/a&gt;&lt;br&gt; [14] &lt;a href=&#34;https://github.com/rowanz/grover&#34;&gt;https://github.com/rowanz/grover&lt;/a&gt;&lt;br&gt; [15] &lt;a href=&#34;https://github.com/wind91725/gpt2-ml-finetune-&#34;&gt;https://github.com/wind91725/gpt2-ml-finetune-&lt;/a&gt;&lt;br&gt; [16] &lt;a href=&#34;https://github.com/guodongxiaren/README&#34;&gt;https://github.com/guodongxiaren/README&lt;/a&gt;&lt;br&gt; [17] &lt;a href=&#34;https://www.jianshu.com/p/55560d3e0e8a&#34;&gt;https://www.jianshu.com/p/55560d3e0e8a&lt;/a&gt;&lt;br&gt; [18] &lt;a href=&#34;https://github.com/YCG09/chinese_ocr&#34;&gt;https://github.com/YCG09/chinese_ocr&lt;/a&gt;&lt;br&gt; [19] &lt;a href=&#34;https://github.com/xiaomaxiao/keras_ocr&#34;&gt;https://github.com/xiaomaxiao/keras_ocr&lt;/a&gt;&lt;br&gt; [20] &lt;a href=&#34;https://github.com/nghuyong/ERNIE-Pytorch&#34;&gt;https://github.com/nghuyong/ERNIE-Pytorch&lt;/a&gt;&lt;br&gt; [21] &lt;a href=&#34;https://zhuanlan.zhihu.com/p/43534801&#34;&gt;https://zhuanlan.zhihu.com/p/43534801&lt;/a&gt;&lt;br&gt; [22] &lt;a href=&#34;https://blog.csdn.net/xuxunjie147/article/details/87178774/&#34;&gt;https://blog.csdn.net/xuxunjie147/article/details/87178774/&lt;/a&gt;&lt;br&gt; [23] &lt;a href=&#34;https://github.com/JiangYanting/Pre-modern_Chinese_corpus_dataset&#34;&gt;https://github.com/JiangYanting/Pre-modern_Chinese_corpus_dataset&lt;/a&gt;&lt;br&gt; [24] &lt;a href=&#34;https://github.com/brightmart/nlp_chinese_corpus&#34;&gt;https://github.com/brightmart/nlp_chinese_corpus&lt;/a&gt;&lt;br&gt; [25] &lt;a href=&#34;https://github.com/SophonPlus/ChineseNlpCorpus&#34;&gt;https://github.com/SophonPlus/ChineseNlpCorpus&lt;/a&gt;&lt;br&gt; [26] &lt;a href=&#34;https://github.com/THUNLP-AIPoet/Resources&#34;&gt;https://github.com/THUNLP-AIPoet/Resources&lt;/a&gt;&lt;br&gt; [27] &lt;a href=&#34;https://github.com/OYE93/Chinese-NLP-Corpus&#34;&gt;https://github.com/OYE93/Chinese-NLP-Corpus&lt;/a&gt;&lt;br&gt; [28] &lt;a href=&#34;https://github.com/CLUEbenchmark/CLUECorpus2020&#34;&gt;https://github.com/CLUEbenchmark/CLUECorpus2020&lt;/a&gt;&lt;br&gt; [29] &lt;a href=&#34;https://github.com/zhiyou720/chinese_summarizer&#34;&gt;https://github.com/zhiyou720/chinese_summarizer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;免责声明&lt;/h2&gt; &#xA;&lt;p&gt;该项目中的内容仅供技术研究与科普，不作为任何结论性依据，不提供任何商业化应用授权&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>CompVis/latent-diffusion</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/CompVis/latent-diffusion</id>
    <link href="https://github.com/CompVis/latent-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Latent Diffusion Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#bibtex&#34;&gt;BibTeX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/results.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/rromb&#34;&gt;Robin Rombach&lt;/a&gt;*, &lt;a href=&#34;https://github.com/ablattmann&#34;&gt;Andreas Blattmann&lt;/a&gt;*, &lt;a href=&#34;https://github.com/qp-qp&#34;&gt;Dominik Lorenz&lt;/a&gt;, &lt;a href=&#34;https://github.com/pesser&#34;&gt;Patrick Esser&lt;/a&gt;, &lt;a href=&#34;https://hci.iwr.uni-heidelberg.de/Staff/bommer&#34;&gt;Björn Ommer&lt;/a&gt;&lt;br&gt; * equal contribution&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/modelfigure.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;h3&gt;April 2022&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;Katherine Crowson&lt;/a&gt;, classifier-free guidance received a ~2x speedup and the &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;PLMS sampler&lt;/a&gt; is available. See also &lt;a href=&#34;https://github.com/CompVis/latent-diffusion/pull/51&#34;&gt;this PR&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Our 1.45B &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#text-to-image&#34;&gt;latent diffusion LAION model&lt;/a&gt; was integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces 🤗&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo: &lt;a href=&#34;https://huggingface.co/spaces/multimodalart/latentdiffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;More pre-trained LDMs are available:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A 1.45B &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#text-to-image&#34;&gt;model&lt;/a&gt; trained on the &lt;a href=&#34;https://arxiv.org/abs/2111.02114&#34;&gt;LAION-400M&lt;/a&gt; database.&lt;/li&gt; &#xA;   &lt;li&gt;A class-conditional model on ImageNet, achieving a FID of 3.6 when using &lt;a href=&#34;https://openreview.net/pdf?id=qw8AKxfYbI&#34;&gt;classifier-free guidance&lt;/a&gt; Available via a &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;colab notebook&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;A suitable &lt;a href=&#34;https://conda.io/&#34;&gt;conda&lt;/a&gt; environment named &lt;code&gt;ldm&lt;/code&gt; can be created and activated with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Pretrained Models&lt;/h1&gt; &#xA;&lt;p&gt;A general list of all available checkpoints is available in via our &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#model-zoo&#34;&gt;model zoo&lt;/a&gt;. If you use any of these models in your work, we are always happy to receive a &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/#bibtex&#34;&gt;citation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Text-to-Image&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/txt2img-preview.png&#34; alt=&#34;text2img-figure&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the pre-trained weights (5.7GB)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p models/ldm/text2img-large/&#xA;wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and sample with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a virus monster is playing guitar, oil on canvas&#34; --ddim_eta 0.0 --n_samples 4 --n_iter 4 --scale 5.0  --ddim_steps 50&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will save each sample individually as well as a grid of size &lt;code&gt;n_iter&lt;/code&gt; x &lt;code&gt;n_samples&lt;/code&gt; at the specified output location (default: &lt;code&gt;outputs/txt2img-samples&lt;/code&gt;). Quality, sampling speed and diversity are best controlled via the &lt;code&gt;scale&lt;/code&gt;, &lt;code&gt;ddim_steps&lt;/code&gt; and &lt;code&gt;ddim_eta&lt;/code&gt; arguments. As a rule of thumb, higher values of &lt;code&gt;scale&lt;/code&gt; produce better samples at the cost of a reduced output diversity.&lt;br&gt; Furthermore, increasing &lt;code&gt;ddim_steps&lt;/code&gt; generally also gives higher quality samples, but returns are diminishing for values &amp;gt; 250. Fast sampling (i.e. low values of &lt;code&gt;ddim_steps&lt;/code&gt;) while retaining good quality can be achieved by using &lt;code&gt;--ddim_eta 0.0&lt;/code&gt;.&lt;br&gt; Faster sampling (i.e. even lower values of &lt;code&gt;ddim_steps&lt;/code&gt;) while retaining good quality can be achieved by using &lt;code&gt;--ddim_eta 0.0&lt;/code&gt; and &lt;code&gt;--plms&lt;/code&gt; (see &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;Pseudo Numerical Methods for Diffusion Models on Manifolds&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;Beyond 256²&lt;/h4&gt; &#xA;&lt;p&gt;For certain inputs, simply running the model in a convolutional fashion on larger features than it was trained on can sometimes result in interesting results. To try it out, tune the &lt;code&gt;H&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt; arguments (which will be integer-divided by 8 in order to calculate the corresponding latent size), e.g. run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a sunset behind a mountain range, vector image&#34; --ddim_eta 1.0 --n_samples 1 --n_iter 1 --H 384 --W 1024 --scale 5.0  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to create a sample of size 384x1024. Note, however, that controllability is reduced compared to the 256x256 setting.&lt;/p&gt; &#xA;&lt;p&gt;The example below was generated using the above command. &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/txt2img-convsample.png&#34; alt=&#34;text2img-figure-conv&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inpainting&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/inpainting.png&#34; alt=&#34;inpainting&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the pre-trained weights&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget -O models/ldm/inpainting_big/last.ckpt https://heibox.uni-heidelberg.de/f/4d9ac7ea40c64582b7c9/?dl=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and sample with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/inpaint.py --indir data/inpainting_examples/ --outdir outputs/inpainting_results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;indir&lt;/code&gt; should contain images &lt;code&gt;*.png&lt;/code&gt; and masks &lt;code&gt;&amp;lt;image_fname&amp;gt;_mask.png&lt;/code&gt; like the examples provided in &lt;code&gt;data/inpainting_examples&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Class-Conditional ImageNet&lt;/h2&gt; &#xA;&lt;p&gt;Available via a &lt;a href=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;notebook&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;. &lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/birdhouse.png&#34; alt=&#34;class-conditional&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Unconditional Models&lt;/h2&gt; &#xA;&lt;p&gt;We also provide a script for sampling from unconditional LDMs (e.g. LSUN, FFHQ, ...). Start it via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python scripts/sample_diffusion.py -r models/ldm/&amp;lt;model_spec&amp;gt;/model.ckpt -l &amp;lt;logdir&amp;gt; -n &amp;lt;\#samples&amp;gt; --batch_size &amp;lt;batch_size&amp;gt; -c &amp;lt;\#ddim steps&amp;gt; -e &amp;lt;\#eta&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Train your own LDMs&lt;/h1&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;h3&gt;Faces&lt;/h3&gt; &#xA;&lt;p&gt;For downloading the CelebA-HQ and FFHQ datasets, proceed as described in the &lt;a href=&#34;https://github.com/CompVis/taming-transformers#celeba-hq&#34;&gt;taming-transformers&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h3&gt;LSUN&lt;/h3&gt; &#xA;&lt;p&gt;The LSUN datasets can be conveniently downloaded via the script available &lt;a href=&#34;https://github.com/fyu/lsun&#34;&gt;here&lt;/a&gt;. We performed a custom split into training and validation images, and provide the corresponding filenames at &lt;a href=&#34;https://ommer-lab.com/files/lsun.zip&#34;&gt;https://ommer-lab.com/files/lsun.zip&lt;/a&gt;. After downloading, extract them to &lt;code&gt;./data/lsun&lt;/code&gt;. The beds/cats/churches subsets should also be placed/symlinked at &lt;code&gt;./data/lsun/bedrooms&lt;/code&gt;/&lt;code&gt;./data/lsun/cats&lt;/code&gt;/&lt;code&gt;./data/lsun/churches&lt;/code&gt;, respectively.&lt;/p&gt; &#xA;&lt;h3&gt;ImageNet&lt;/h3&gt; &#xA;&lt;p&gt;The code will try to download (through &lt;a href=&#34;http://academictorrents.com/&#34;&gt;Academic Torrents&lt;/a&gt;) and prepare ImageNet the first time it is used. However, since ImageNet is quite large, this requires a lot of disk space and time. If you already have ImageNet on your disk, you can speed things up by putting the data into &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt; (which defaults to &lt;code&gt;~/.cache/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt;), where &lt;code&gt;{split}&lt;/code&gt; is one of &lt;code&gt;train&lt;/code&gt;/&lt;code&gt;validation&lt;/code&gt;. It should have the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&#xA;├── n01440764&#xA;│   ├── n01440764_10026.JPEG&#xA;│   ├── n01440764_10027.JPEG&#xA;│   ├── ...&#xA;├── n01443537&#xA;│   ├── n01443537_10007.JPEG&#xA;│   ├── n01443537_10014.JPEG&#xA;│   ├── ...&#xA;├── ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you haven&#39;t extracted the data, you can also place &lt;code&gt;ILSVRC2012_img_train.tar&lt;/code&gt;/&lt;code&gt;ILSVRC2012_img_val.tar&lt;/code&gt; (or symlinks to them) into &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_train/&lt;/code&gt; / &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_validation/&lt;/code&gt;, which will then be extracted into above structure without downloading it again. Note that this will only happen if neither a folder &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt; nor a file &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/.ready&lt;/code&gt; exist. Remove them if you want to force running the dataset preparation again.&lt;/p&gt; &#xA;&lt;h2&gt;Model Training&lt;/h2&gt; &#xA;&lt;p&gt;Logs and checkpoints for trained models are saved to &lt;code&gt;logs/&amp;lt;START_DATE_AND_TIME&amp;gt;_&amp;lt;config_spec&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Training autoencoder models&lt;/h3&gt; &#xA;&lt;p&gt;Configs for training a KL-regularized autoencoder on ImageNet are provided at &lt;code&gt;configs/autoencoder&lt;/code&gt;. Training can be started by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python main.py --base configs/autoencoder/&amp;lt;config_spec&amp;gt;.yaml -t --gpus 0,    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;config_spec&lt;/code&gt; is one of {&lt;code&gt;autoencoder_kl_8x8x64&lt;/code&gt;(f=32, d=64), &lt;code&gt;autoencoder_kl_16x16x16&lt;/code&gt;(f=16, d=16), &lt;code&gt;autoencoder_kl_32x32x4&lt;/code&gt;(f=8, d=4), &lt;code&gt;autoencoder_kl_64x64x3&lt;/code&gt;(f=4, d=3)}.&lt;/p&gt; &#xA;&lt;p&gt;For training VQ-regularized models, see the &lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;taming-transformers&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h3&gt;Training LDMs&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;configs/latent-diffusion/&lt;/code&gt; we provide configs for training LDMs on the LSUN-, CelebA-HQ, FFHQ and ImageNet datasets. Training can be started by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python main.py --base configs/latent-diffusion/&amp;lt;config_spec&amp;gt;.yaml -t --gpus 0,&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;&amp;lt;config_spec&amp;gt;&lt;/code&gt; is one of {&lt;code&gt;celebahq-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3),&lt;code&gt;ffhq-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3), &lt;code&gt;lsun_bedrooms-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3), &lt;code&gt;lsun_churches-ldm-vq-4&lt;/code&gt;(f=8, KL-reg. autoencoder, spatial size 32x32x4),&lt;code&gt;cin-ldm-vq-8&lt;/code&gt;(f=8, VQ-reg. autoencoder, spatial size 32x32x4)}.&lt;/p&gt; &#xA;&lt;h1&gt;Model Zoo&lt;/h1&gt; &#xA;&lt;h2&gt;Pretrained Autoencoding Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CompVis/latent-diffusion/main/assets/reconstruction2.png&#34; alt=&#34;rec2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;All models were trained until convergence (no further substantial improvement in rFID).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;rFID vs val&lt;/th&gt; &#xA;   &lt;th&gt;train steps&lt;/th&gt; &#xA;   &lt;th&gt;PSNR&lt;/th&gt; &#xA;   &lt;th&gt;PSIM&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Comments&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, VQ (Z=8192, d=3)&lt;/td&gt; &#xA;   &lt;td&gt;0.58&lt;/td&gt; &#xA;   &lt;td&gt;533066&lt;/td&gt; &#xA;   &lt;td&gt;27.43 +/- 4.26&lt;/td&gt; &#xA;   &lt;td&gt;0.53 +/- 0.21&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f4.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f4.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, VQ (Z=8192, d=3)&lt;/td&gt; &#xA;   &lt;td&gt;1.06&lt;/td&gt; &#xA;   &lt;td&gt;658131&lt;/td&gt; &#xA;   &lt;td&gt;25.21 +/- 4.17&lt;/td&gt; &#xA;   &lt;td&gt;0.72 +/- 0.26&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://heibox.uni-heidelberg.de/f/9c6681f64bb94338a069/?dl=1&#34;&gt;https://heibox.uni-heidelberg.de/f/9c6681f64bb94338a069/?dl=1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no attention&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, VQ (Z=16384, d=4)&lt;/td&gt; &#xA;   &lt;td&gt;1.14&lt;/td&gt; &#xA;   &lt;td&gt;971043&lt;/td&gt; &#xA;   &lt;td&gt;23.07 +/- 3.99&lt;/td&gt; &#xA;   &lt;td&gt;1.17 +/- 0.36&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f8.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f8.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, VQ (Z=256, d=4)&lt;/td&gt; &#xA;   &lt;td&gt;1.49&lt;/td&gt; &#xA;   &lt;td&gt;1608649&lt;/td&gt; &#xA;   &lt;td&gt;22.35 +/- 3.81&lt;/td&gt; &#xA;   &lt;td&gt;1.26 +/- 0.37&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f8-n256.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f8-n256.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=16, VQ (Z=16384, d=8)&lt;/td&gt; &#xA;   &lt;td&gt;5.15&lt;/td&gt; &#xA;   &lt;td&gt;1101166&lt;/td&gt; &#xA;   &lt;td&gt;20.83 +/- 3.61&lt;/td&gt; &#xA;   &lt;td&gt;1.73 +/- 0.43&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://heibox.uni-heidelberg.de/f/0e42b04e2e904890a9b6/?dl=1&#34;&gt;https://heibox.uni-heidelberg.de/f/0e42b04e2e904890a9b6/?dl=1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, KL&lt;/td&gt; &#xA;   &lt;td&gt;0.27&lt;/td&gt; &#xA;   &lt;td&gt;176991&lt;/td&gt; &#xA;   &lt;td&gt;27.53 +/- 4.54&lt;/td&gt; &#xA;   &lt;td&gt;0.55 +/- 0.24&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f4.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f4.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, KL&lt;/td&gt; &#xA;   &lt;td&gt;0.90&lt;/td&gt; &#xA;   &lt;td&gt;246803&lt;/td&gt; &#xA;   &lt;td&gt;24.19 +/- 4.19&lt;/td&gt; &#xA;   &lt;td&gt;1.02 +/- 0.35&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f8.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f8.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=16, KL (d=16)&lt;/td&gt; &#xA;   &lt;td&gt;0.87&lt;/td&gt; &#xA;   &lt;td&gt;442998&lt;/td&gt; &#xA;   &lt;td&gt;24.08 +/- 4.22&lt;/td&gt; &#xA;   &lt;td&gt;1.07 +/- 0.36&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f16.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f16.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=32, KL (d=64)&lt;/td&gt; &#xA;   &lt;td&gt;2.04&lt;/td&gt; &#xA;   &lt;td&gt;406763&lt;/td&gt; &#xA;   &lt;td&gt;22.27 +/- 3.93&lt;/td&gt; &#xA;   &lt;td&gt;1.41 +/- 0.40&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f32.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f32.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Get the models&lt;/h3&gt; &#xA;&lt;p&gt;Running the following script downloads und extracts all available pretrained autoencoding models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/download_first_stages.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first stage models can then be found in &lt;code&gt;models/first_stage_models/&amp;lt;model_spec&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained LDMs&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Datset&lt;/th&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;FID&lt;/th&gt; &#xA;   &lt;th&gt;IS&lt;/th&gt; &#xA;   &lt;th&gt;Prec&lt;/th&gt; &#xA;   &lt;th&gt;Recall&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Comments&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CelebA-HQ&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;5.11 (5.11)&lt;/td&gt; &#xA;   &lt;td&gt;3.29&lt;/td&gt; &#xA;   &lt;td&gt;0.72&lt;/td&gt; &#xA;   &lt;td&gt;0.49&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/celeba.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/celeba.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FFHQ&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;4.98 (4.98)&lt;/td&gt; &#xA;   &lt;td&gt;4.50 (4.50)&lt;/td&gt; &#xA;   &lt;td&gt;0.73&lt;/td&gt; &#xA;   &lt;td&gt;0.50&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/ffhq.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/ffhq.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LSUN-Churches&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-KL-8 (400 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;4.02 (4.02)&lt;/td&gt; &#xA;   &lt;td&gt;2.72&lt;/td&gt; &#xA;   &lt;td&gt;0.64&lt;/td&gt; &#xA;   &lt;td&gt;0.52&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/lsun_churches.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/lsun_churches.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LSUN-Bedrooms&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;2.95 (3.0)&lt;/td&gt; &#xA;   &lt;td&gt;2.22 (2.23)&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;   &lt;td&gt;0.48&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/lsun_bedrooms.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/lsun_bedrooms.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;Class-conditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-8 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;7.77(7.76)* /15.82**&lt;/td&gt; &#xA;   &lt;td&gt;201.56(209.52)* /78.82**&lt;/td&gt; &#xA;   &lt;td&gt;0.84* / 0.65**&lt;/td&gt; &#xA;   &lt;td&gt;0.35* / 0.63**&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/cin.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/cin.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;*: w/ guiding, classifier_scale 10 **: w/o guiding, scores in bracket calculated with script provided by &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;ADM&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Conceptual Captions&lt;/td&gt; &#xA;   &lt;td&gt;Text-conditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-f4 (100 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;16.79&lt;/td&gt; &#xA;   &lt;td&gt;13.89&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/text2img.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/text2img.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;finetuned from LAION&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenImages&lt;/td&gt; &#xA;   &lt;td&gt;Super-resolution&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/sr_bsr.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/sr_bsr.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BSR image degradation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenImages&lt;/td&gt; &#xA;   &lt;td&gt;Layout-to-Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;32.02&lt;/td&gt; &#xA;   &lt;td&gt;15.92&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/layout2img_model.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/layout2img_model.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Landscapes&lt;/td&gt; &#xA;   &lt;td&gt;Semantic Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis256.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis256.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Landscapes&lt;/td&gt; &#xA;   &lt;td&gt;Semantic Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;finetuned on resolution 512x512&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Get the models&lt;/h3&gt; &#xA;&lt;p&gt;The LDMs listed above can jointly be downloaded and extracted via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/download_models.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The models can then be found in &lt;code&gt;models/ldm/&amp;lt;model_spec&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Coming Soon...&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More inference scripts for conditional LDMs.&lt;/li&gt; &#xA; &lt;li&gt;In the meantime, you can play with our colab notebook &lt;a href=&#34;https://colab.research.google.com/drive/1xqzUi2iXQXDqXBHQGP9Mqt2YrYW6cx-J?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1xqzUi2iXQXDqXBHQGP9Mqt2YrYW6cx-J?usp=sharing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Comments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Our codebase for the diffusion models builds heavily on &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;OpenAI&#39;s ADM codebase&lt;/a&gt; and &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;https://github.com/lucidrains/denoising-diffusion-pytorch&lt;/a&gt;. Thanks for open-sourcing!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The implementation of the transformer encoder is from &lt;a href=&#34;https://github.com/lucidrains/x-transformers&#34;&gt;x-transformers&lt;/a&gt; by &lt;a href=&#34;https://github.com/lucidrains?tab=repositories&#34;&gt;lucidrains&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;      title={High-Resolution Image Synthesis with Latent Diffusion Models}, &#xA;      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},&#xA;      year={2021},&#xA;      eprint={2112.10752},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>amueller/introduction_to_ml_with_python</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/amueller/introduction_to_ml_with_python</id>
    <link href="https://github.com/amueller/introduction_to_ml_with_python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notebooks and code for the book &#34;Introduction to Machine Learning with Python&#34;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/amueller/introduction_to_ml_with_python/master&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Introduction to Machine Learning with Python&lt;/h1&gt; &#xA;&lt;p&gt;This repository holds the code for the forthcoming book &#34;Introduction to Machine Learning with Python&#34; by &lt;a href=&#34;http://amueller.io&#34;&gt;Andreas Mueller&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/sarah_guido&#34;&gt;Sarah Guido&lt;/a&gt;. You can find details about the book on the &lt;a href=&#34;http://shop.oreilly.com/product/0636920030515.do&#34;&gt;O&#39;Reilly website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The books requires the current stable version of scikit-learn, that is 0.20.0. Most of the book can also be used with previous versions of scikit-learn, though you need to adjust the import for everything from the &lt;code&gt;model_selection&lt;/code&gt; module, mostly &lt;code&gt;cross_val_score&lt;/code&gt;, &lt;code&gt;train_test_split&lt;/code&gt; and &lt;code&gt;GridSearchCV&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This repository provides the notebooks from which the book is created, together with the &lt;code&gt;mglearn&lt;/code&gt; library of helper functions to create figures and datasets.&lt;/p&gt; &#xA;&lt;p&gt;For the curious ones, the cover depicts a &lt;a href=&#34;https://en.wikipedia.org/wiki/Hellbender&#34;&gt;hellbender&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All datasets are included in the repository, with the exception of the aclImdb dataset, which you can download from the page of &lt;a href=&#34;http://ai.stanford.edu/~amaas/data/sentiment/&#34;&gt;Andrew Maas&lt;/a&gt;. See the book for details.&lt;/p&gt; &#xA;&lt;p&gt;If you get &lt;code&gt;ImportError: No module named mglearn&lt;/code&gt; you can try to install mglearn into your python environment using the command &lt;code&gt;pip install mglearn&lt;/code&gt; in your terminal or &lt;code&gt;!pip install mglearn&lt;/code&gt; in Jupyter Notebook.&lt;/p&gt; &#xA;&lt;h2&gt;Errata&lt;/h2&gt; &#xA;&lt;p&gt;Please note that the first print of the book is missing the following line when listing the assumed imports:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.display import display&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please add this line if you see an error involving &lt;code&gt;display&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The first print of the book used a function called &lt;code&gt;plot_group_kfold&lt;/code&gt;. This has been renamed to &lt;code&gt;plot_label_kfold&lt;/code&gt; because of a rename in scikit-learn.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;To run the code, you need the packages &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;scipy&lt;/code&gt;, &lt;code&gt;scikit-learn&lt;/code&gt;, &lt;code&gt;matplotlib&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;pillow&lt;/code&gt;. Some of the visualizations of decision trees and neural networks structures also require &lt;code&gt;graphviz&lt;/code&gt;. The chapter on text processing also requirs &lt;code&gt;nltk&lt;/code&gt; and &lt;code&gt;spacy&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The easiest way to set up an environment is by installing &lt;a href=&#34;https://www.continuum.io/downloads&#34;&gt;Anaconda&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Installing packages with conda:&lt;/h3&gt; &#xA;&lt;p&gt;If you already have a Python environment set up, and you are using the &lt;code&gt;conda&lt;/code&gt; package manager, you can get all packages by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install numpy scipy scikit-learn matplotlib pandas pillow graphviz python-graphviz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the chapter on text processing you also need to install &lt;code&gt;nltk&lt;/code&gt; and &lt;code&gt;spacy&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install nltk spacy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installing packages with pip&lt;/h3&gt; &#xA;&lt;p&gt;If you already have a Python environment and are using pip to install packages, you need to run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install numpy scipy scikit-learn matplotlib pandas pillow graphviz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You also need to install the graphiz C-library, which is easiest using a package manager. If you are using OS X and homebrew, you can &lt;code&gt;brew install graphviz&lt;/code&gt;. If you are on Ubuntu or debian, you can &lt;code&gt;apt-get install graphviz&lt;/code&gt;. Installing graphviz on Windows can be tricky and using conda / anaconda is recommended. For the chapter on text processing you also need to install &lt;code&gt;nltk&lt;/code&gt; and &lt;code&gt;spacy&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install nltk spacy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Downloading English language model&lt;/h3&gt; &#xA;&lt;p&gt;For the text processing chapter, you need to download the English language model for spacy using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m spacy download en&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Submitting Errata&lt;/h2&gt; &#xA;&lt;p&gt;If you have errata for the (e-)book, please submit them via the &lt;a href=&#34;http://www.oreilly.com/catalog/errata.csp?isbn=0636920030515&#34;&gt;O&#39;Reilly Website&lt;/a&gt;. You can submit fixes to the code as pull-requests here, but I&#39;d appreciate it if you would also submit them there, as this repository doesn&#39;t hold the &#34;master notebooks&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/amueller/introduction_to_ml_with_python/master/cover.jpg&#34; alt=&#34;cover&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>darkprinx/break-the-ice-with-python</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/darkprinx/break-the-ice-with-python</id>
    <link href="https://github.com/darkprinx/break-the-ice-with-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The repository is about 100+ python programming exercise problem discussed, explained, and solved in different ways&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Break The Ice With Python&lt;/h1&gt; &#xA;&lt;h3&gt;A journey of 100+ simple yet interesting problems which are explained, solved, discussed in different pythonic ways&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/darkprinx/100-plus-Python-programming-exercises-extended/master?filepath=notebooks%2F&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fdarkprinx%2F100-plus-Python-programming-exercises-extended%2Fblob%2Fmaster%2Fnotebooks%2FDay_01.ipynb&#34;&gt;&lt;img src=&#34;https://deepnote.com/buttons/try-in-a-jupyter-notebook.svg?sanitize=true&#34; alt=&#34;Deepnote&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;The exercise text contents of this repository was collected from GitHub account of &lt;a href=&#34;https://github.com/zhiwehu/Python-programming-exercises&#34;&gt;zhiwehu&lt;/a&gt;. I collected it to practice and solve all the listed problems with python. Even after these collected problems are all set up, I will try to add more problems in near future. If you are a very beginner with python then I hope this 100+ exercise will help you a lot to get your hands free with python.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;One will find the given problems very simple and easy to understand. A beginner can try 3-5 problems a day which will take a little time to solve but definitely will learn a couple of new stuff (no matter how lazy you are :P ). And after regular practice of only a month, one can find himself solved more than 100++ problems which are obviously not a deniable achievement.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;In this repository, I will be gradually updating the codebase of the given problems with my own solutions. Also, I may try to explain the code and tell my opinion about the problem if needed. Main Authors solutions are in python 2 &amp;amp; my solutions will be in python 3. Every problem is divided into a template format which is discussed below. There is a &lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/issues/3&#34;&gt;discussion&lt;/a&gt; section so don&#39;t forget to share your opinion, ideas and feel free to discuss anything wrong or mistake&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;A Big Thanks to &lt;a href=&#34;https://github.com/apurvmishra99&#34;&gt;apurvmishra99&lt;/a&gt; for contributing the repository by cleaning up the formatting of all Days_.md files. fixing some random errors, fixing some variable naming with PEP8 conventions, and adding a whole new folder of &lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/tree/master/notebooks&#34;&gt;jupyter&lt;/a&gt; notebook of all 24 days.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;100+ Python challenging programming exercises&lt;/h1&gt; &#xA;&lt;h2&gt;1. Problem Template&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Hints&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;2. Practice Status&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%201.md&#34; title=&#34;Day 1 Status&#34;&gt;Day 1&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 1-3&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%202.md&#34; title=&#34;Day 2 Status&#34;&gt;Day 2&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 4-9&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%203.md&#34; title=&#34;Day 3 Status&#34;&gt;Day 3&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 10-13&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%204.md&#34; title=&#34;Day 4 Status&#34;&gt;Day 4&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 14-15&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%205.md&#34; title=&#34;Day 5 Status&#34;&gt;Day 5&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 16-17&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%206.md&#34; title=&#34;Day 6 Status&#34;&gt;Day 6&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 18-19&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%207.md&#34; title=&#34;Day 7 Status&#34;&gt;Day 7&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 20-21&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%208.md&#34; title=&#34;Day 8 Status&#34;&gt;Day 8&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 22-25&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day%209.md&#34; title=&#34;Day 9 Status&#34;&gt;Day 9&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 26-30&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_10.md&#34; title=&#34;Day 10 Status&#34;&gt;Day 10&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 31-37&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_11.md&#34; title=&#34;Day 11 Status&#34;&gt;Day 11&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 38-43&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_12.md&#34; title=&#34;Day 12 Status&#34;&gt;Day 12&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 44-46&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_13.md&#34; title=&#34;Day 13 Status&#34;&gt;Day 13&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 47-50&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_14.md&#34; title=&#34;Day 14 Status&#34;&gt;Day 14&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 51-53&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_15.md&#34; title=&#34;Day 15 Status&#34;&gt;Day 15&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 54-59&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_16.md&#34; title=&#34;Day 16 Status&#34;&gt;Day 16&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 60-64&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_17.md&#34; title=&#34;Day 17 Status&#34;&gt;Day 17&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 65-69&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_18.md&#34; title=&#34;Day 18 Status&#34;&gt;Day 18&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 70-74&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_19.md&#34; title=&#34;Day 19 Status&#34;&gt;Day 19&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 75-79&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_20.md&#34; title=&#34;Day 20 Status&#34;&gt;Day 20&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 80-84&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_21.md&#34; title=&#34;Day 21 Status&#34;&gt;Day 21&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 85-89&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_22.md&#34; title=&#34;Day 22 Status&#34;&gt;Day 22&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 90-94&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_23.md&#34; title=&#34;Day 23 Status&#34;&gt;Day 23&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 95-99&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/darkprinx/100-plus-Python-programming-exercises-extended/raw/master/Status/Day_24.md&#34; title=&#34;Day 24 Status&#34;&gt;Day 24&lt;/a&gt;&lt;/strong&gt;- &lt;em&gt;&lt;strong&gt;Question 100-103&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>yidao620c/python3-cookbook</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/yidao620c/python3-cookbook</id>
    <link href="https://github.com/yidao620c/python3-cookbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;《Python Cookbook》 3rd Edition Translation&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/yidao620c/python3-cookbook/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/yidao620c/python3-cookbook.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/hexpm/l/plug.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yidao620c/python3-cookbook/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/yidao620c/python3-cookbook/total.svg?sanitize=true&#34; alt=&#34;Github downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yidao620c/python3-cookbook/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/yidao620c/python3-cookbook.svg?sanitize=true&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;《Python Cookbook in Chinese》 3rd Edition 翻译&lt;/h1&gt; &#xA;&lt;p&gt;《Python Cookbook》3rd 中文版3.0.0正式发布啦 ^_^！ ——2017/12/07&lt;/p&gt; &#xA;&lt;p&gt;在线阅读地址：&lt;a href=&#34;http://python3-cookbook.readthedocs.org/zh_CN/latest/&#34;&gt;http://python3-cookbook.readthedocs.org/zh_CN/latest/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;最新版(3.0.0)下载&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;中文简体版PDF下载地址：&lt;a href=&#34;https://pan.baidu.com/s/1pL1cI9d&#34;&gt;https://pan.baidu.com/s/1pL1cI9d&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;中文繁体版PDF下载地址：&lt;a href=&#34;https://pan.baidu.com/s/1qX97VJI&#34;&gt;https://pan.baidu.com/s/1qX97VJI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;关于作者David Beazley&lt;/h2&gt; &#xA;&lt;p&gt;本书作者是David Beazley大神，一位独立的计算机科学家、教育家，以及有着35年开发经验的软件开发者。 他在Python社区一直都很活跃，编写了很多的&lt;a href=&#34;http://www.dabeaz.com/software.html&#34;&gt;python包&lt;/a&gt;， 发表了很多的公开&lt;a href=&#34;http://www.dabeaz.com/talks.html&#34;&gt;演讲视频&lt;/a&gt; 以及 &lt;a href=&#34;http://www.dabeaz.com/tutorials.html&#34;&gt;编程教程&lt;/a&gt;。 同时还是&lt;a href=&#34;http://www.dabeaz.com/per.html&#34;&gt;Python Essential Reference&lt;/a&gt; 以及 &lt;a href=&#34;http://www.dabeaz.com/cookbook.html&#34;&gt; Python Cookbook (O&#39;Reilly Media)&lt;/a&gt;的作者。&lt;/p&gt; &#xA;&lt;p&gt;David Beazley大神的博客地址：&lt;a href=&#34;http://www.dabeaz.com/&#34;&gt;http://www.dabeaz.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;译者的话&lt;/h2&gt; &#xA;&lt;p&gt;人生苦短，我用Python！&lt;/p&gt; &#xA;&lt;p&gt;译者一直坚持使用Python3，因为它代表了Python的未来。虽然向后兼容是它的硬伤，但是这个局面迟早会改变的， 而且Python3的未来需要每个人的帮助和支持。 目前市面上的教程书籍，网上的手册大部分基本都是2.x系列的，专门基于3.x系列的书籍少的可怜。&lt;/p&gt; &#xA;&lt;p&gt;最近看到一本《Python Cookbook》3rd Edition，完全基于Python3，写的也很不错。 为了Python3的普及，我也不自量力，想做点什么事情。于是乎，就有了翻译这本书的冲动了！ 这不是一项轻松的工作，却是一件值得做的工作：不仅方便了别人，而且对自己翻译能力也是一种锻炼和提升。&lt;/p&gt; &#xA;&lt;p&gt;译者会坚持对自己每一句的翻译负责，力求高质量。但受能力限制，也难免有疏漏或者表意不当的地方。 如果译文中有什么错漏的地方请大家见谅，也欢迎大家随时指正。&lt;/p&gt; &#xA;&lt;p&gt;目前已经正式完成了整本书的翻译工作，历时2年，不管怎样还是坚持下来了。现在共享给python社区。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;欢迎关注我的个人公众号“飞污熊”，我会定期分享一些自己的Python学习笔记和心得。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/yidao620c/python3-cookbook/raw/master/exts/wuxiong.jpg&#34; alt=&#34;公众号&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;项目说明&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;所有文档均使用reStructuredText编辑，参考 &lt;a href=&#34;http://docutils.sourceforge.net/docs/user/rst/quickref.html&#34;&gt;reStructuredText&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;当前文档生成托管在 &lt;a href=&#34;https://readthedocs.org/&#34;&gt;readthedocs&lt;/a&gt; 上&lt;/li&gt; &#xA; &lt;li&gt;生成的文档预览地址： &lt;a href=&#34;http://python3-cookbook.readthedocs.org/zh_CN/latest/&#34;&gt;python3-cookbook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;使用了python官方文档主题 &lt;a href=&#34;https://github.com/snide/sphinx_rtd_theme&#34;&gt;sphinx-rtd-theme&lt;/a&gt; ，也是默认的主题default.&lt;/li&gt; &#xA; &lt;li&gt;书中所有代码均在python 3.6版本下面运行通过，所有源码放在cookbook包下面&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;# on_rtd is whether we are on readthedocs.org, this line of code grabbed from docs.readthedocs.org&#xA;on_rtd = os.environ.get(&#39;READTHEDOCS&#39;, None) == &#39;True&#39;&#xA;&#xA;if not on_rtd:  # only import and set the theme if we&#39;re building docs locally&#xA;    import sphinx_rtd_theme&#xA;    html_theme = &#39;sphinx_rtd_theme&#39;&#xA;    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]&#xA;&#xA;# otherwise, readthedocs.org uses their theme by default, so no need to specify it&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;其他贡献者&lt;/h2&gt; &#xA;&lt;p&gt;排名不分先后：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Yu Longjun (&lt;a href=&#34;https://github.com/yulongjun&#34;&gt;https://github.com/yulongjun&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;tylinux (&lt;a href=&#34;https://github.com/tylinux&#34;&gt;https://github.com/tylinux&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Kevin Guan (&lt;a href=&#34;https://github.com/K-Guan&#34;&gt;https://github.com/K-Guan&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;littlezz (&lt;a href=&#34;https://github.com/littlezz&#34;&gt;https://github.com/littlezz&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;cclauss (&lt;a href=&#34;https://github.com/cclauss&#34;&gt;https://github.com/cclauss&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Yan Zhang (&lt;a href=&#34;https://github.com/Eskibear&#34;&gt;https://github.com/Eskibear&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;xiuyanduan (&lt;a href=&#34;https://github.com/xiuyanduan&#34;&gt;https://github.com/xiuyanduan&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;FPlust (&lt;a href=&#34;https://github.com/fplust&#34;&gt;https://github.com/fplust&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;lambdaplus (&lt;a href=&#34;https://github.com/lambdaplus&#34;&gt;https://github.com/lambdaplus&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Tony Yang (&lt;a href=&#34;mailto:liuliu036@gmail.com&#34;&gt;liuliu036@gmail.com&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/yidao620c/python3-cookbook/graphs/contributors&#34;&gt;更多贡献者&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;关于源码生成PDF文件&lt;/h2&gt; &#xA;&lt;p&gt;有网友提问怎样通过源码生成PDF文件，由于这个步骤介绍有点长，不适合放在README里面， 我专门写了篇博客专门介绍怎样通过ReadtheDocs托管文档，怎样自己生成PDF文件，大家可以参考一下。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.xncoding.com/2017/01/22/fullstack/readthedoc.html&#34;&gt;https://www.xncoding.com/2017/01/22/fullstack/readthedoc.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;另外关于生成的PDF文件中会自动生成标题编号的问题，有热心网友 &lt;a href=&#34;https://github.com/CarlKing5019&#34;&gt;CarlKing5019&lt;/a&gt;提出了解决方案， 请参考issues108：&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/yidao620c/python3-cookbook/issues/108&#34;&gt;https://github.com/yidao620c/python3-cookbook/issues/108&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;再次感谢每一位贡献者。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;How to Contribute&lt;/h2&gt; &#xA;&lt;p&gt;You are welcome to contribute to the project as follow&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fork project and commit pull requests&lt;/li&gt; &#xA; &lt;li&gt;add/edit wiki&lt;/li&gt; &#xA; &lt;li&gt;report/fix issue&lt;/li&gt; &#xA; &lt;li&gt;code review&lt;/li&gt; &#xA; &lt;li&gt;commit new feature&lt;/li&gt; &#xA; &lt;li&gt;add testcase&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Meanwhile you&#39;d better follow the rules below&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It&#39;s &lt;em&gt;NOT&lt;/em&gt; recommended to submit a pull request directly to &lt;code&gt;master&lt;/code&gt; branch. &lt;code&gt;develop&lt;/code&gt; branch is more appropriate&lt;/li&gt; &#xA; &lt;li&gt;Follow common Python coding conventions&lt;/li&gt; &#xA; &lt;li&gt;Add the following &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;license&lt;/a&gt; in each source file&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;(The Apache License)&lt;/p&gt; &#xA;&lt;p&gt;Copyright (c) 2014-2018 &lt;a href=&#34;https://www.xncoding.com/&#34;&gt;Xiong Neng&lt;/a&gt; and other contributors&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;   http://www.apache.org/licenses/LICENSE-2.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dotnet/csharp-notebooks</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/dotnet/csharp-notebooks</id>
    <link href="https://github.com/dotnet/csharp-notebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Get started learning C# with C# notebooks powered by .NET Interactive and VS Code.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;.NET Interactive Notebooks for C#&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the home of .NET interactive notebooks for C#!&lt;/p&gt; &#xA;&lt;h2&gt;How to Install&lt;/h2&gt; &#xA;&lt;h3&gt;VS Code&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the .NET Coding Pack for VS Code for &lt;a href=&#34;https://aka.ms/dotnet-coding-pack-win&#34;&gt;Windows&lt;/a&gt; or &lt;a href=&#34;https://aka.ms/dotnet-coding-pack-mac&#34;&gt;macOS&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install the &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.dotnet-interactive-vscode&#34;&gt;.NET Interactive Notebooks&lt;/a&gt; extension.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Visual Studio&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download and install &lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34;&gt;Visual Studio 2022&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Download and install &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=MLNET.notebook&#34;&gt;Notebook Editor Extension&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For more information and resources, visit &lt;a href=&#34;https://dotnet.microsoft.com/learntocode&#34;&gt;Learn to code C#&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;C# 101&lt;/h2&gt; &#xA;&lt;p&gt;Download or clone this repo and open the &lt;code&gt;csharp-101&lt;/code&gt; folder in VS Code to get started with the C# 101 notebooks. Or, if you want just tap on one of the Notebook links below and automatically have it open in VS Code!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;#&lt;/th&gt; &#xA;   &lt;th&gt;Topic&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;   &lt;th&gt;Video Link&lt;/th&gt; &#xA;   &lt;th&gt;Documentation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;Hello World&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook01&#34;&gt;01 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=KT2VR7m19So&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=2&#34;&gt;01 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/hello-world?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Intro to C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;The Basics of Strings&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook02&#34;&gt;02 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=JSpC7Cz64h0&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=3&#34;&gt;02 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/hello-world?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Intro to C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;Searching Strings&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook03&#34;&gt;03 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=JL30gSE3WaQ&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=4&#34;&gt;03 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/hello-world?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Intro to C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;Numbers and Integers Math&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook04&#34;&gt;04 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=jEE0pWTq54U&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=5&#34;&gt;04 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/numbers-in-csharp?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Numbers in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;Numbers and Integer Precision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook05&#34;&gt;05 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=31EmPADtv4w&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=6&#34;&gt;05 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/numbers-in-csharp?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Numbers in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;Numbers and Decimals&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook06&#34;&gt;06 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kdKcpF9roeU&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=7&#34;&gt;06 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/numbers-in-csharp?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Numbers in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;Branches (if)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook07&#34;&gt;07 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=y4OTe8LSokg&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=8&#34;&gt;07 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/branches-and-loops-local?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Branches and Loops in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;What Are Loops?&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook08&#34;&gt;08 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=z31m5Up_gSQ&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=10&#34;&gt;08 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/branches-and-loops-local?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Branches and Loops in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;td&gt;Combining Branches and Loops&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook09&#34;&gt;09 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qK7tUpaOXi8&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=11&#34;&gt;09 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/branches-and-loops-local?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Branches and Loops in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;Arrays, Lists, and Collections&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook10&#34;&gt;10 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qLeF_wpnVto&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=12&#34;&gt;10 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/arrays-and-collections?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Arrays, Lists, and Collections in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;Search, Sort, and Index Lists&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook11&#34;&gt;11 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=NJ5ghiutzfY&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=13&#34;&gt;11 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/arrays-and-collections?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Arrays, Lists, and Collections in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;Lists of Other Types&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook12&#34;&gt;12 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oIQdb93xewE&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=14&#34;&gt;12 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/tour-of-csharp/tutorials/arrays-and-collections?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Arrays, Lists, and Collections in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;Objects and Classes&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook13&#34;&gt;13 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=TzgxcAiHCWA&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=16&#34;&gt;13 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/fundamentals/tutorials/classes?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Object Oriented Coding in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;Methods and Members&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook14&#34;&gt;14 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xLhm3bEG__c&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=17&#34;&gt;14 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/fundamentals/tutorials/classes?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Object Oriented Coding in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;15&lt;/td&gt; &#xA;   &lt;td&gt;Methods and Exceptions&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/csharp101-notebook15&#34;&gt;15 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8YsoBBiVVzQ&amp;amp;list=PLdo4fOcmZ0oVxKLQCHpiUWun7vlJJvUiN&amp;amp;index=18&#34;&gt;15 Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/dotnet/csharp/fundamentals/tutorials/classes?WT.mc_id=csharpnotebook-35129-website&#34;&gt;Object Oriented Coding in C#&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Machine Learning&lt;/h2&gt; &#xA;&lt;p&gt;Download or clone this repo and open the &lt;code&gt;machine-learning&lt;/code&gt; folder in Visual Studio 2022 to get started with the machine-learning notebooks. Or, if you want just tap on one of the Notebook links below and automatically have it open in Visual Studio!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Links below require &lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34;&gt;Visual Studio 2022&lt;/a&gt; and &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=MLNET.notebook&#34;&gt;Notebook Editor Extension&lt;/a&gt; 0.3.4 or greater&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Getting Started Series&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;#&lt;/th&gt; &#xA;   &lt;th&gt;Topic&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;Intro to Machine Learning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-01-intro&#34;&gt;01 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;Data Prep and Feature Engineering&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-02-data&#34;&gt;02 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;Training and AutoML&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-03-training&#34;&gt;03 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;Model Evaluation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-04-evaluation&#34;&gt;04 Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;End to End (E2E) Notebooks - examples of the entire ML process.&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;#&lt;/th&gt; &#xA;   &lt;th&gt;Topic&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;E2E&lt;/td&gt; &#xA;   &lt;td&gt;Classification using AutoML (Iris Dataset)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-e2e-iris&#34;&gt;Iris E2E AutoML&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;E2E&lt;/td&gt; &#xA;   &lt;td&gt;Forecasting using Regression (Luna Dataset)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-e2e-luna-regression&#34;&gt;Luna E2E Regression&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;E2E&lt;/td&gt; &#xA;   &lt;td&gt;Forecasting using SSA (Luna Dataset)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-e2e-luna-ssa&#34;&gt;Luna E2E SSA&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;E2E&lt;/td&gt; &#xA;   &lt;td&gt;Regression using AutoML (Taxi Dataset)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-e2e-taxi&#34;&gt;Taxi E2E AutoML&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Reference Notebooks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;#&lt;/th&gt; &#xA;   &lt;th&gt;Topic&lt;/th&gt; &#xA;   &lt;th&gt;Notebook Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;REF&lt;/td&gt; &#xA;   &lt;td&gt;Data Processing with DataFrame&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-ref-data-frame&#34;&gt;Data Frame&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;REF&lt;/td&gt; &#xA;   &lt;td&gt;Graphs and Visualizations&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-ref-visualizations&#34;&gt;Visualizations&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;REF&lt;/td&gt; &#xA;   &lt;td&gt;Kaggle Competitions (Titanic Dataset)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ntbk.io/ml-ref-kaggle-titanic&#34;&gt;Kaggle&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;.NET Foundation&lt;/h2&gt; &#xA;&lt;p&gt;.NET Interative Notebooks for C# is a &lt;a href=&#34;https://www.dotnetfoundation.org/projects&#34;&gt;.NET Foundation&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;There are many .NET related projects on GitHub.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Microsoft/dotnet&#34;&gt;.NET home repo&lt;/a&gt;&amp;nbsp;- links to 100s of .NET projects, from Microsoft and the community.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/aspnet/core/?view=aspnetcore-3.1&#34;&gt;ASP.NET Core home&lt;/a&gt; - the best place to start learning about ASP.NET Core.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This project has adopted the code of conduct defined by the &lt;a href=&#34;http://contributor-covenant.org/&#34;&gt;Contributor Covenant&lt;/a&gt; to clarify expected behavior in our community. For more information, see the &lt;a href=&#34;http://www.dotnetfoundation.org/code-of-conduct&#34;&gt;.NET Foundation Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;.NET (including the csharp-notebooks repo) is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/dotnet/csharp-notebooks/main/LICENSE&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA-AI-IOT/deepstream_python_apps</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/NVIDIA-AI-IOT/deepstream_python_apps</id>
    <link href="https://github.com/NVIDIA-AI-IOT/deepstream_python_apps" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DeepStream SDK Python bindings and sample applications&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepStream Python Apps&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains Python bindings and sample applications for the &lt;a href=&#34;https://developer.nvidia.com/deepstream-sdk&#34;&gt;DeepStream SDK&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;SDK version supported: 6.1&lt;/p&gt; &#xA;&lt;p&gt;&lt;b&gt;The bindings sources along with build instructions are now available under &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/bindings&#34;&gt;bindings&lt;/a&gt;! &lt;/b&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;b&gt;This release comes with Operating System upgrades (from Ubuntu 18.04 to Ubuntu 20.04) for DeepStreamSDK 6.1 support. This translates to upgrade in Python version to 3.8 and &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/3rdparty/gst-python/&#34;&gt;gst-python&lt;/a&gt; version has also been upgraded to 1.16.2 !&lt;/b&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the latest release package complete with bindings and sample applications from the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/releases&#34;&gt;release section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please report any issues or bugs on the &lt;a href=&#34;https://devtalk.nvidia.com/default/board/209&#34;&gt;DeepStream SDK Forums&lt;/a&gt;. This enables the DeepStream community to find help at a central location.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/#deepstream-python-apps&#34;&gt;DeepStream Python Apps&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/#python-bindings&#34;&gt;Python Bindings&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/#sample-applications&#34;&gt;Sample Applications&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a name=&#34;metadata_bindings&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Python Bindings&lt;/h2&gt; &#xA;&lt;p&gt;DeepStream pipelines can be constructed using Gst Python, the GStreamer framework&#39;s Python bindings. For accessing DeepStream MetaData, Python &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/bindings&#34;&gt;bindings&lt;/a&gt; are provided as part of this repository. This module is generated using &lt;a href=&#34;https://github.com/pybind/pybind11&#34;&gt;Pybind11&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/.python-app-pipeline.png&#34; alt=&#34;bindings pipeline&#34; height=&#34;600px&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;These bindings support a Python interface to the MetaData structures and functions. Usage of this interface is documented in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/HOWTO.md&#34;&gt;HOW-TO Guide&lt;/a&gt; and demonstrated in the sample applications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;sample_applications&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sample Applications&lt;/h2&gt; &#xA;&lt;p&gt;Sample applications provided here demonstrate how to work with DeepStream pipelines using Python.&lt;br&gt; The sample applications require &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/#metadata_bindings&#34;&gt;MetaData Bindings&lt;/a&gt; to work.&lt;/p&gt; &#xA;&lt;p&gt;To run the sample applications or write your own, please consult the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/HOWTO.md&#34;&gt;HOW-TO Guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/.test3-app.png&#34; alt=&#34;deepstream python app screenshot&#34; height=&#34;400px&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We currently provide the following sample applications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test1&#34;&gt;deepstream-test1&lt;/a&gt; -- 4-class object detection pipeline&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test2&#34;&gt;deepstream-test2&lt;/a&gt; -- 4-class object detection, tracking and attribute classification pipeline&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;UPDATE&lt;/b&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test3&#34;&gt;deepstream-test3&lt;/a&gt; -- multi-stream pipeline performing 4-class object detection - now also supports triton inference server, no-display mode, file-loop and silent mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test4&#34;&gt;deepstream-test4&lt;/a&gt; -- msgbroker for sending analytics results to the cloud&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-imagedata-multistream&#34;&gt;deepstream-imagedata-multistream&lt;/a&gt; -- multi-stream pipeline with access to image buffers&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-ssd-parser&#34;&gt;deepstream-ssd-parser&lt;/a&gt; -- SSD model inference via Triton server with output parsing in Python&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test1-usbcam&#34;&gt;deepstream-test1-usbcam&lt;/a&gt; -- deepstream-test1 pipelien with USB camera input&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-test1-rtsp-out&#34;&gt;deepstream-test1-rtsp-out&lt;/a&gt; -- deepstream-test1 pipeline with RTSP output&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-opticalflow&#34;&gt;deepstream-opticalflow&lt;/a&gt; -- optical flow and visualization pipeline with flow vectors returned in NumPy array&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-segmentation&#34;&gt;deepstream-segmentation&lt;/a&gt; -- segmentation and visualization pipeline with segmentation mask returned in NumPy array&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-nvdsanalytics&#34;&gt;deepstream-nvdsanalytics&lt;/a&gt; -- multistream pipeline with analytics plugin&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/runtime_source_add_delete&#34;&gt;runtime_source_add_delete&lt;/a&gt; -- add/delete source streams at runtime&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-imagedata-multistream-redaction&#34;&gt;deepstream-imagedata-multistream-redaction&lt;/a&gt; -- multi-stream pipeline with face detection and redaction&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-rtsp-in-rtsp-out&#34;&gt;deepstream-rtsp-in-rtsp-out&lt;/a&gt; -- multi-stream pipeline with RTSP input/output&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;NEW&lt;/b&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps/deepstream-preprocess-test&#34;&gt;deepstream-preprocess-test&lt;/a&gt; -- multi-stream pipeline using nvdspreprocess plugin with custom ROIs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Detailed application information is provided in each application&#39;s subdirectory under &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-IOT/deepstream_python_apps/master/apps&#34;&gt;apps&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>aamini/introtodeeplearning</title>
    <updated>2022-06-03T01:43:20Z</updated>
    <id>tag:github.com,2022-06-03:/aamini/introtodeeplearning</id>
    <link href="https://github.com/aamini/introtodeeplearning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Lab Materials for MIT 6.S191: Introduction to Deep Learning&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aamini/introtodeeplearning/master/assets/banner.png&#34; alt=&#34;banner&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains all of the code and software labs for &lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;MIT 6.S191: Introduction to Deep Learning&lt;/a&gt;! All lecture slides and videos are available on the course website.&lt;/p&gt; &#xA;&lt;h1&gt;Instructions&lt;/h1&gt; &#xA;&lt;p&gt;6.S191 software labs are designed to be completed at your own pace. At the end of each of the labs, there will be instructions on how you can submit your notebook for grade. Additionally, if you would like to submit your lab as part of the 6.S191 lab competitions, instructions regarding what information must be submitted is also provided at the end of each lab.&lt;/p&gt; &#xA;&lt;h2&gt;Opening the labs in Google Colaboratory:&lt;/h2&gt; &#xA;&lt;p&gt;The 2022 6.S191 labs will be run in Google&#39;s Colaboratory, a Jupyter notebook environment that runs entirely in the cloud, you don&#39;t need to download anything. To run these labs, you must have a Google account.&lt;/p&gt; &#xA;&lt;p&gt;On this Github repo, navigate to the lab folder you want to run (&lt;code&gt;lab1&lt;/code&gt;, &lt;code&gt;lab2&lt;/code&gt;, &lt;code&gt;lab3&lt;/code&gt;) and open the appropriate python notebook (*.ipynb). Click the &#34;Run in Colab&#34; link on the top of the lab. That&#39;s it!&lt;/p&gt; &#xA;&lt;h2&gt;Running the labs&lt;/h2&gt; &#xA;&lt;p&gt;Now, to run the labs, open the Jupyter notebook on Colab. Navigate to the &#34;Runtime&#34; tab --&amp;gt; &#34;Change runtime type&#34;. In the pop-up window, under &#34;Runtime type&#34; select &#34;Python 3&#34;, and under &#34;Hardware accelerator&#34; select &#34;GPU&#34;. Go through the notebooks and fill in the &lt;code&gt;#TODO&lt;/code&gt; cells to get the code to compile for yourself!&lt;/p&gt; &#xA;&lt;h3&gt;MIT Deep Learning package&lt;/h3&gt; &#xA;&lt;p&gt;You might notice that inside the labs we install the &lt;code&gt;mitdeeplearning&lt;/code&gt; python package from the Python Package repository:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install mitdeeplearning&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;This package contains convienence functions that we use throughout the course and can be imported like any other Python package.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import mitdeeplearning as mdl&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;We do this for you in each of the labs, but the package is also open source under the same license so you can also use it outside the class.&lt;/p&gt; &#xA;&lt;h2&gt;Lecture Videos&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=njKP3FqW3Sk&amp;amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;amp;index=1&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aamini/introtodeeplearning/master/assets/video_play.png&#34; width=&#34;500&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All lecture videos are available publicly online and linked above! Use and/or modification of lecture slides outside of 6.S191 must reference:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;© MIT 6.S191: Introduction to Deep Learning&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;http://introtodeeplearning.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;All code in this repository is copyright 2022 &lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;MIT 6.S191 Introduction to Deep Learning&lt;/a&gt;. All Rights Reserved.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the MIT License. You may not use this file except in compliance with the License. Use and/or modification of this code outside of 6.S191 must reference:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;© MIT 6.S191: Introduction to Deep Learning&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://introtodeeplearning.com&#34;&gt;http://introtodeeplearning.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
</feed>