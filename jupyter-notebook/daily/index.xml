<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-17T01:35:07Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>teticio/audio-diffusion</title>
    <updated>2022-12-17T01:35:07Z</updated>
    <id>tag:github.com,2022-12-17:/teticio/audio-diffusion</id>
    <link href="https://github.com/teticio/audio-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apply diffusion models using the new Hugging Face diffusers package to synthesize music instead of images.&lt;/p&gt;&lt;hr&gt;&lt;hr&gt; &#xA;&lt;h2&gt;title: Audio Diffusion emoji: ðŸŽµ colorFrom: pink colorTo: blue sdk: gradio sdk_version: 3.1.4 app_file: app.py pinned: false license: gpl-3.0&lt;/h2&gt; &#xA;&lt;h1&gt;audio-diffusion &lt;a href=&#34;https://colab.research.google.com/github/teticio/audio-diffusion/blob/master/notebooks/gradio_app.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;Apply diffusion models to synthesize music instead of images using the new Hugging Face &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; package&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Sample automatically generated loop&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/44233095/204103172-27f25d63-5e77-40ca-91ab-d04a45d4726f.mp4&#34;&gt;https://user-images.githubusercontent.com/44233095/204103172-27f25d63-5e77-40ca-91ab-d04a45d4726f.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;https://soundcloud.com/teticio2/sets/audio-diffusion-loops&#34;&gt;https://soundcloud.com/teticio2/sets/audio-diffusion-loops&lt;/a&gt; for more examples.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Updates&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;5/12/2022&lt;/strong&gt; ðŸ¤— Exciting news! &lt;code&gt;AudioDiffusionPipeline&lt;/code&gt; has been migrated to the Hugging Face &lt;code&gt;diffusers&lt;/code&gt; package so that it is even easier for others to use and contribute.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2/12/2022&lt;/strong&gt;. Added Mel to pipeline and updated the pretrained models to save Mel config (they are now no longer compatible with previous versions of this repo). It is relatively straightforward to migrate previously trained models to the new format (see &lt;a href=&#34;https://huggingface.co/teticio/audio-diffusion-256&#34;&gt;https://huggingface.co/teticio/audio-diffusion-256&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;7/11/2022&lt;/strong&gt;. Added pre-trained latent audio diffusion models &lt;a href=&#34;https://huggingface.co/teticio/latent-audio-diffusion-256&#34;&gt;teticio/latent-audio-diffusion-256&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/teticio/latent-audio-diffusion-ddim-256&#34;&gt;teticio/latent-audio-diffusion-ddim-256&lt;/a&gt;. You can use the pre-trained VAE to train your own latent diffusion models on a different set of audio files.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;22/10/2022&lt;/strong&gt;. Added DDIM encoder and ability to interpolate between audios in latent &#34;noise&#34; space. Mel spectrograms no longer have to be square (thanks to Tristan for this one), so you can set the vertical (frequency) and horizontal (time) resolutions independently.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;15/10/2022&lt;/strong&gt;. Added latent audio diffusion (see below). Also added the possibility to train a DDIM (&lt;a href=&#34;https://arxiv.org/pdf/2010.02502.pdf&#34;&gt;De-noising Diffusion Implicit Models&lt;/a&gt;). These have the benefit that samples can be generated with much fewer steps (~50) than used in training.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;4/10/2022&lt;/strong&gt;. It is now possible to mask parts of the input audio during generation which means you can stitch several samples together (think &#34;out-painting&#34;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;27/9/2022&lt;/strong&gt;. You can now generate an audio based on a previous one. You can use this to generate variations of the same audio or even to &#34;remix&#34; a track (via a sort of &#34;style transfer&#34;). You can find examples of how to do this in the &lt;a href=&#34;https://colab.research.google.com/github/teticio/audio-diffusion/blob/master/notebooks/test_model.ipynb&#34;&gt;&lt;code&gt;test_model.ipynb&lt;/code&gt;&lt;/a&gt; notebook.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/44233095/205305826-8b39c917-26c5-49b4-887c-776f5d69e970.png&#34; alt=&#34;mel spectrogram&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;DDPM (&lt;a href=&#34;https://arxiv.org/abs/2006.11239&#34;&gt;De-noising Diffusion Probabilistic Models&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;Audio can be represented as images by transforming to a &lt;a href=&#34;https://en.wikipedia.org/wiki/Mel-frequency_cepstrum&#34;&gt;mel spectrogram&lt;/a&gt;, such as the one shown above. The class &lt;code&gt;Mel&lt;/code&gt; in &lt;code&gt;mel.py&lt;/code&gt; can convert a slice of audio into a mel spectrogram of &lt;code&gt;x_res&lt;/code&gt; x &lt;code&gt;y_res&lt;/code&gt; and vice versa. The higher the resolution, the less audio information will be lost. You can see how this works in the &lt;a href=&#34;https://github.com/teticio/audio-diffusion/raw/main/notebooks/test_mel.ipynb&#34;&gt;&lt;code&gt;test_mel.ipynb&lt;/code&gt;&lt;/a&gt; notebook.&lt;/p&gt; &#xA;&lt;p&gt;A DDPM is trained on a set of mel spectrograms that have been generated from a directory of audio files. It is then used to synthesize similar mel spectrograms, which are then converted back into audio.&lt;/p&gt; &#xA;&lt;p&gt;You can play around with some pre-trained models on &lt;a href=&#34;https://colab.research.google.com/github/teticio/audio-diffusion/blob/master/notebooks/test_model.ipynb&#34;&gt;Google Colab&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/spaces/teticio/audio-diffusion&#34;&gt;Hugging Face spaces&lt;/a&gt;. Check out some automatically generated loops &lt;a href=&#34;https://soundcloud.com/teticio2/sets/audio-diffusion-loops&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/teticio/audio-diffusion-256&#34;&gt;teticio/audio-diffusion-256&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/teticio/audio-diffusion-256&#34;&gt;teticio/audio-diffusion-256&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;My &#34;liked&#34; Spotify playlist&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/teticio/audio-diffusion-breaks-256&#34;&gt;teticio/audio-diffusion-breaks-256&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/teticio/audio-diffusion-breaks-256&#34;&gt;teticio/audio-diffusion-breaks-256&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Samples that have been used in music, sourced from &lt;a href=&#34;https://whosampled.com&#34;&gt;WhoSampled&lt;/a&gt; and &lt;a href=&#34;https://youtube.com&#34;&gt;YouTube&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/teticio/audio-diffusion-instrumental-hiphop-256&#34;&gt;teticio/audio-diffusion-instrumental-hiphop-256&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/teticio/audio-diffusion-instrumental-hiphop-256&#34;&gt;teticio/audio-diffusion-instrumental-hiphop-256&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Instrumental Hip Hop music&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/teticio/audio-diffusion-ddim-256&#34;&gt;teticio/audio-diffusion-ddim-256&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/teticio/audio-diffusion-256&#34;&gt;teticio/audio-diffusion-256&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;De-noising Diffusion Implicit Model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/teticio/latent-audio-diffusion-256&#34;&gt;teticio/latent-audio-diffusion-256&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/teticio/audio-diffusion-256&#34;&gt;teticio/audio-diffusion-256&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Latent Audio Diffusion model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/teticio/latent-audio-diffusion-ddim-256&#34;&gt;teticio/latent-audio-diffusion-ddim-256&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/teticio/audio-diffusion-256&#34;&gt;teticio/audio-diffusion-256&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Latent Audio Diffusion De-noising Diffusion Implicit Model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Generate Mel spectrogram dataset from directory of audio files&lt;/h2&gt; &#xA;&lt;h4&gt;Install from GitHub (includes training scripts)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/teticio/audio-diffusion.git&#xA;cd audio-diffusion&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install from PyPI&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install audiodiffusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Training can be run with Mel spectrograms of resolution 64x64 on a single commercial grade GPU (e.g. RTX 2080 Ti). The &lt;code&gt;hop_length&lt;/code&gt; should be set to 1024 for better results&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/audio_to_images.py \&#xA;--resolution 64,64 \&#xA;--hop_length 1024 \&#xA;--input_dir path-to-audio-files \&#xA;--output_dir path-to-output-data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Generate dataset of 256x256 Mel spectrograms and push to hub (you will need to be authenticated with &lt;code&gt;huggingface-cli login&lt;/code&gt;)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/audio_to_images.py \&#xA;--resolution 256 \&#xA;--input_dir path-to-audio-files \&#xA;--output_dir data/audio-diffusion-256 \&#xA;--push_to_hub teticio/audio-diffusion-256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the default &lt;code&gt;sample_rate&lt;/code&gt; is 22050 and audios will be resampled if they are at a different rate. If you change this value, you may find that the results in the &lt;code&gt;test_mel.ipynb&lt;/code&gt; notebook are not good (for example, if &lt;code&gt;sample_rate&lt;/code&gt; is 48000) and that it is necessary to adjust &lt;code&gt;n_fft&lt;/code&gt; (for example, to 2000 instead of the default value of 2048; alternatively, you can resample to a &lt;code&gt;sample_rate&lt;/code&gt; of 44100). Make sure you use the same parameters for training and inference. You should also bear in mind that not all resolutions work with the neural network architecture as currently configured - you should be safe if you stick to powers of 2.&lt;/p&gt; &#xA;&lt;h2&gt;Train model&lt;/h2&gt; &#xA;&lt;h4&gt;Run training on local machine&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch --config_file config/accelerate_local.yaml \&#xA;scripts/train_unconditional.py \&#xA;--dataset_name data/audio-diffusion-64 \&#xA;--hop_length 1024 \&#xA;--output_dir models/ddpm-ema-audio-64 \&#xA;--train_batch_size 16 \&#xA;--num_epochs 100 \&#xA;--gradient_accumulation_steps 1 \&#xA;--learning_rate 1e-4 \&#xA;--lr_warmup_steps 500 \&#xA;--mixed_precision no&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Run training on local machine with &lt;code&gt;batch_size&lt;/code&gt; of 2 and &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; 8 to compensate, so that 256x256 resolution model fits on commercial grade GPU and push to hub&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch --config_file config/accelerate_local.yaml \&#xA;scripts/train_unconditional.py \&#xA;--dataset_name teticio/audio-diffusion-256 \&#xA;--output_dir models/audio-diffusion-256 \&#xA;--num_epochs 100 \&#xA;--train_batch_size 2 \&#xA;--eval_batch_size 2 \&#xA;--gradient_accumulation_steps 8 \&#xA;--learning_rate 1e-4 \&#xA;--lr_warmup_steps 500 \&#xA;--mixed_precision no \&#xA;--push_to_hub True \&#xA;--hub_model_id audio-diffusion-256 \&#xA;--hub_token $(cat $HOME/.huggingface/token)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Run training on SageMaker&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch --config_file config/accelerate_sagemaker.yaml \&#xA;scripts/train_unconditional.py \&#xA;--dataset_name teticio/audio-diffusion-256 \&#xA;--output_dir models/ddpm-ema-audio-256 \&#xA;--train_batch_size 16 \&#xA;--num_epochs 100 \&#xA;--gradient_accumulation_steps 1 \&#xA;--learning_rate 1e-4 \&#xA;--lr_warmup_steps 500 \&#xA;--mixed_precision no&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;DDIM (&lt;a href=&#34;https://arxiv.org/pdf/2010.02502.pdf&#34;&gt;De-noising Diffusion Implicit Models&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;h4&gt;A DDIM can be trained by adding the parameter&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--scheduler ddim&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inference can the be run with far fewer steps than the number used for training (e.g., ~50), allowing for much faster generation. Without retraining, the parameter &lt;code&gt;eta&lt;/code&gt; can be used to replicate a DDPM if it is set to 1 or a DDIM if it is set to 0, with all values in between being valid. When &lt;code&gt;eta&lt;/code&gt; is 0 (the default value), the de-noising procedure is deterministic, which means that it can be run in reverse as a kind of encoder that recovers the original noise used in generation. A function &lt;code&gt;encode&lt;/code&gt; has been added to &lt;code&gt;AudioDiffusionPipeline&lt;/code&gt; for this purpose. It is then possible to interpolate between audios in the latent &#34;noise&#34; space using the function &lt;code&gt;slerp&lt;/code&gt; (Spherical Linear intERPolation).&lt;/p&gt; &#xA;&lt;h2&gt;Latent Audio Diffusion&lt;/h2&gt; &#xA;&lt;p&gt;Rather than de-noising images directly, it is interesting to work in the &#34;latent space&#34; after first encoding images using an autoencoder. This has a number of advantages. Firstly, the information in the images is compressed into a latent space of a much lower dimension, so it is much faster to train de-noising diffusion models and run inference with them. Secondly, similar images tend to be clustered together and interpolating between two images in latent space can produce meaningful combinations.&lt;/p&gt; &#xA;&lt;p&gt;At the time of writing, the Hugging Face &lt;code&gt;diffusers&lt;/code&gt; library is geared towards inference and lacking in training functionality (rather like its cousin &lt;code&gt;transformers&lt;/code&gt; in the early days of development). In order to train a VAE (Variational AutoEncoder), I use the &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;stable-diffusion&lt;/a&gt; repo from CompVis and convert the checkpoints to &lt;code&gt;diffusers&lt;/code&gt; format. Note that it uses a perceptual loss function for images; it would be nice to try a perceptual &lt;em&gt;audio&lt;/em&gt; loss function.&lt;/p&gt; &#xA;&lt;h4&gt;Train latent diffusion model using pre-trained VAE&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch ...&#xA;...&#xA;--vae teticio/latent-audio-diffusion-256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install dependencies to train with Stable Diffusion&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install omegaconf pytorch_lightning==1.7.7 torchvision einops&#xA;pip install -e git+https://github.com/CompVis/stable-diffusion.git@main#egg=latent-diffusion&#xA;pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Train an autoencoder&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/train_vae.py \&#xA;--dataset_name teticio/audio-diffusion-256 \&#xA;--batch_size 2 \&#xA;--gradient_accumulation_steps 12&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Train latent diffusion model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch ...&#xA;...&#xA;--vae models/autoencoder-kl&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Alro10/deep-learning-time-series</title>
    <updated>2022-12-17T01:35:07Z</updated>
    <id>tag:github.com,2022-12-17:/Alro10/deep-learning-time-series</id>
    <link href="https://github.com/Alro10/deep-learning-time-series" rel="alternate"></link>
    <summary type="html">&lt;p&gt;List of papers, code and experiments using deep learning for time series forecasting&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deep Learning Time Series Forecasting&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRsWelcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;List of state of the art papers focus on deep learning and resources, code and experiments using deep learning for time series forecasting. Classic methods vs Deep Learning methods, Competitions...&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;&#34;&gt;Table of Contents&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alro10/deep-learning-time-series/master/#Papers&#34;&gt;Papers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alro10/deep-learning-time-series/master/#Conferences&#34;&gt;Conferences&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alro10/deep-learning-time-series/master/#Competitions&#34;&gt;Competitions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alro10/deep-learning-time-series/master/#Code&#34;&gt;Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alro10/deep-learning-time-series/master/#Theory-Resource&#34;&gt;Theory-Resource&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alro10/deep-learning-time-series/master/#Code-Resource&#34;&gt;Code Resource&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alro10/deep-learning-time-series/master/#Datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;h3&gt;2021&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.13008&#34;&gt;Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Haixu Wu, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2111.03394.pdf&#34;&gt;Long Range Probabilistic Forecasting in Time-Series using High Order Statistics&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Prathamesh Deshpande, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/pratham16cse/AggForecaster&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2107.00894.pdf&#34;&gt;Online Multi-Agent Forecasting with Interpretable Collaborative Graph Neural Networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Maosen Li, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://proceedings.mlr.press/v139/rangapuram21a/rangapuram21a.pdf&#34;&gt;End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Syama Sundar Rangapuram, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2104.05522.pdf&#34;&gt;Neural basis expansion analysis with exogenous variables:Forecasting electricity prices with NBEATSx&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Kin G. Olivares, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/cchallu/nbeatsx&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2101.12072.pdf&#34;&gt;Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting&lt;/a&gt; &lt;strong&gt;reference&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Kashif Rasul, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/zalandoresearch/pytorch-ts&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/publication/347133536_An_Experimental_Review_on_Deep_Learning_Architectures_for_Time_Series_Forecasting&#34;&gt;An Experimental Review on Deep Learning Architectures for Time Series Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Pedro Lara-BenÃ­tez, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/pedrolarben/TimeSeriesForecasting-DeepLearning&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2101.02815.pdf&#34;&gt;Long Horizon Forecasting With Temporal Point Processes&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Prathamesh Deshpande, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/pratham16cse/DualTPP&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2012.07436.pdf&#34;&gt;Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting&lt;/a&gt; &lt;code&gt;AAAI 2021&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Haoyi Zhou, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/zhouhaoyi/Informer2020&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2020&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2101.04224.pdf&#34;&gt;CHALLENGES AND APPROACHES TO TIME-SERIES FORECASTING IN DATA CENTER TELEMETRY: A SURVEY&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Shruti Jadon, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S026840122031481X&#34;&gt;Forecasting and Anomaly Detection approaches using LSTM and LSTM Autoencoder techniques with the applications in supply chain management&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;H.D. Nguyen, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.climatechange.ai/papers/neurips2020/41/paper.pdf&#34;&gt;Physics-constrained Deep Recurrent Neural Models of Building Thermal Dynamics&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;JÃ¡n Drgona, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2012.08791.pdf&#34;&gt;MiniRocket: A Very Fast (Almost) Deterministic Transform for Time Series Classification&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Angus Dempster, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/angus924/minirocket&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/file/abc99d6b9938aa86d1f30f8ee0fd169f-Paper.pdf&#34;&gt;Learning to Select the Best Forecasting Tasks for Clinical Outcome Prediction&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yuan Xue, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9179030&#34;&gt;Real-World Anomaly Detection by using Digital Twin Systems and Weakly-Supervised Learning&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Castellani Andrea, et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Honda Research Institute Europe GmbH&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2010.13006.pdf&#34;&gt;Inter-Series Attention Model for COVID-19 Forecasting&lt;/a&gt; &lt;strong&gt;Good reference&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Xiaoyong Jin, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/Gandor26/covid-open&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2010.10742.pdf&#34;&gt;MODEL SELECTION IN RECONCILING HIERARCHICAL TIME SERIES&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;M. ABOLGHASEMI, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/mahdiabolghasemi/Conditional-reconciliation-in-HF&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2010.08158.pdf&#34;&gt;A Strong Baseline for Weekly Time Series Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Rakshitha Godahewa, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/rakshitha123/WeeklyForecasting&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2010.05783.pdf&#34;&gt;Structural Forecasting for Tropical Cyclone Intensity Prediction: Providing Insight with Deep Learning&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Trey McNeely, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dash.harvard.edu/bitstream/handle/1/37365638/KHETARPAL-DOCUMENT-2020.pdf?sequence=1&amp;amp;isAllowed=y&#34;&gt;Modeling Heterogeneous Seasonality With Recurrent Neural Networks Using IoT Time Series Data for Defrost Detection and Anomaly Analysis&lt;/a&gt; &lt;strong&gt;Good Reference&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Khetarpal, Suraj.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Georgiana_Ifrim/publication/344501445_An_Examination_of_the_State-of-the-Art_for_Multivariate_Time_Series_Classification/links/5f7cdfb2458515b7cf6c4efd/An-Examination-of-the-State-of-the-Art-for-Multivariate-Time-Series-Classification.pdf&#34;&gt;An Examination of the State-of-the-Art for Multivariate Time Series Classification&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Bhaskar Dhariyal, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code noy yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2010.01707.pdf&#34;&gt;Rank Position Forecasting in Car Racing&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Bo Peng, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://www.columbia.edu/~jwp2128/Papers/FazelniaIbrahimetal2020.pdf&#34;&gt;Mixed Membership Recurrent Neural Networks for Modeling Customer Purchases&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ghazal Fazelnia, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2009.07943.pdf&#34;&gt;An analysis of deep neural networks for predicting trends in time series data&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Kouame Kouassi and Deshendran Moodley.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2009.08102.pdf&#34;&gt;Automatic Forecasting using Gaussian Processes&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;G. Corani&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3394486.3403362&#34;&gt;Attention based Multi-Modal New Product Sales Time-series Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Vijay Ekambaram&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2009.07052.pdf&#34;&gt;Demand Forecasting of individual Probability Density Functions with Machine Learning&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Felix Wick, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://www.ieomsociety.org/detroit2020/papers/37.pdf&#34;&gt;A Time-Series Forecasting Performance Comparison for Neural Networks with State Space and ARIMA Models&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Milton Soto-Ferrari&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Karthick_Thiyagarajan7/publication/344199272_Short-term_Time_Series_Forecasting_of_Concrete_Sewer_Pipe_Surface_Temperature/links/5f5b0b4492851c07895d48fc/Short-term-Time-Series-Forecasting-of-Concrete-Sewer-Pipe-Surface-Temperature.pdf&#34;&gt;Short-term Time Series Forecasting of Concrete Sewer Pipe Surface Temperature&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Karthick Thiyagarajan, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2009.02040.pdf&#34;&gt;Multivariate Time-series Anomaly Detection via Graph Attention Network&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Hang Zhao, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2009.03474.pdf&#34;&gt;Graph Neural Networks for Model Recommendation using Time Series Data&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Aleksandr Pletnev, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0169207020301114&#34;&gt;Kaggle forecasting competitions: An overlooked learning opportunity&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Casper Solheim Bojer and Jens Peder Meldgaard.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/cbojer/kaggle-project&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2008.12340.pdf&#34;&gt;Forecasting with Multiple Seasonality&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Tianyang Xie and Jie Ding.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2009.00945.pdf&#34;&gt;LAVARNET: Neural network modeling of causal variable relationships for multivariate time series forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Christos Koutlis, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://kdd-milets.github.io/milets2020/papers/MiLeTS2020_paper_13.pdf&#34;&gt;Forecasting Hierarchical Time Series with a Regularized Embedding Space&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Jeffrey L. Gleason.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/jlgleason/hts-constrained-embeddings&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3394486.3403337&#34;&gt;Forecasting the Evolution of Hydropower Generation&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fan Zhou, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/Anewnoob/DeepHydro&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3394486.3403206&#34;&gt;Deep State-Space Generative Model For Correlated Time-to-Event Predictions&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yuan Xue, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://mpra.ub.uni-muenchen.de/102315/1/MPRA_paper_102315.pdf&#34;&gt;Short-term forecasting of the COVID-19 pandemic using Google Trends data: Evidence from 158 countries&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fantazzini, Dean.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2008.03194.pdf&#34;&gt;Scalable Low-Rank Autoregressive Tensor Learning for Spatiotemporal Traffic Data Imputation&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Xinyu Chen, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/xinychen/tensor-learning&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.vanderschaar-lab.com/papers/2020_Clairvoyance.pdf&#34;&gt;clairvoyance: a Unified, End-to-End AutoML Pipeline for Medical Time Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Daniel Jarrett, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://urban.cs.wpi.edu/urbcomp2020/file/08.pdf&#34;&gt;Speed Anomalies and Safe Departure Times from Uber Movement Data&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Nabil Al Nahin Ch, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2008.01848.pdf&#34;&gt;Forecasting AI Progress: A Research Agenda&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ross Gruetzemacher, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Review&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2008.02663&#34;&gt;Improving the Accuracy of Global Forecasting Models using Time Series Data Augmentation&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Kasun Bandara, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2008.00646.pdf&#34;&gt;Interpretable Sequence Learning for COVID-19 Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Sercan O. ArÄ±k, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/reichlab/covid19-forecast-hub&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2008.00181.pdf&#34;&gt;Relation-aware Meta-learning for Market Segment Demand Prediction with Limited Records&lt;/a&gt; &lt;strong&gt;meta-learning&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Jiatu Shi, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.mdpi.com/2079-3197/8/3/70/pdf&#34;&gt;Forecasting Economic Recession through Share Price in the Logistics Industry with Artificial Intelligence (AI)&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;YM Tang, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2008.00444.pdf&#34;&gt;PRINCIPLES AND ALGORITHMS FOR FORECASTING GROUPS OF TIME SERIES: LOCALITY AND GLOBALITY&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Pablo Montero-Manso and Rob J Hyndman&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2008.01670.pdf&#34;&gt;Multi-stream RNN for Merchant Transaction Prediction&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Zhongfang Zhuang, et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;KDD 2020 Workshop on Machine Learning in Finance&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2007.15159.pdf&#34;&gt;Prediction of hierarchical time series using structured regularization and its application to artificial neural networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Tomokaze Shiratori, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9149573&#34;&gt;Cold-Start Promotional Sales Forecasting through Gradient Boosted-based Contrastive Explanations&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Carlos Aguilar-Palacios, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/CarlitosDev/contrastiveExplanation/tree/master/contrastiveRegressor&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2007.15541.pdf&#34;&gt;Anomaly Detection at Scale: The Case for Deep Distributional Time Series Models&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fadhel Ayed, et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Amazon Research&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/awslabs/gluon-ts/tree/distribution_anomaly_detection/distribution_anomaly_detection&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/ariannezhad-2020-demand.pdf&#34;&gt;Demand Forecasting in the Presence of Privileged Information&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Mozhdeh Ariannezhad, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/mzhariann/PIANN&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/publication/342976923_Seasonal_Self-evolving_Neural_Networks_Based_Short-term_Wind_Farm_Generation_Forecast&#34;&gt;Seasonal Self-evolving Neural Networks Based Short-term Wind Farm Generation Forecast&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yunchuan Liu, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2007.09577.pdf&#34;&gt;Distributed ARIMA Models for Ultra-long Time Series&lt;/a&gt; &lt;strong&gt;Spark&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Xiaoqian Wang, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/xqnwang/darima&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.03778&#34;&gt;Adversarial Attacks on Probabilistic Autoregressive Forecasting Models&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RaphaÃ«l Dang-Nhu, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/eth-sri/probabilistic-forecasts-attacks&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2007.03568.pdf&#34;&gt;Superiority of Simplicity: A Lightweight Model for Network Device Workload Prediction&lt;/a&gt; &lt;strong&gt;LSTM application&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Alexander Acker, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/citlab/fed_challenge&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2007.02842.pdf&#34;&gt;Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Lei Bai, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/LeiBAI/AGCRN&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9115645&#34;&gt;Dynamic Multi-Scale Convolutional Neural Network for Time Series Classification&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;BIN QIAN, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://germain-forestier.info/publis/ijcnn2020.pdf&#34;&gt;Neural Architecture Search for Time Series Classification&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Hojjat Rakhshani, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/ML-MHs/IJCNN2020&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2006.13707.pdf&#34;&gt;Frequentist Uncertainty in Recurrent Neural Networks via Blockwise Influence Functions&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ahmed M. Alaa and Mihaela van der Schaar.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2006.12672.pdf&#34;&gt;Time Series Regression&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Chang Wei Tan, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/ChangWeiTan/TSRegression&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://odr.chalmers.se/bitstream/20.500.12380/300824/1/Master_s_Thesis_Johan_Ramne_.pdf&#34;&gt;Forecasting Supplier Delivery Performance with Recurrent Neural Networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Johan Ramne&lt;/li&gt; &#xA;   &lt;li&gt;Master Thesis.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2006.10119.pdf&#34;&gt;Markovian RNN: An Adaptive Time Series Prediction Network with HMM-based Switching for Nonstationary Environments&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fatih Ilhan, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3399579.3399869&#34;&gt;Resilient Neural Forecasting Systems&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Michael Bohlke-Schneider, et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Amazon Research&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPRW_2020/papers/w66/Graber_Dynamic_Neural_Relational_Inference_for_Forecasting_Trajectories_CVPRW_2020_paper.pdf&#34;&gt;Dynamic Neural Relational Inference for Forecasting Trajectories&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Colin Graber and Alexander Schwing&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;CVPR 2020&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/cgraber/cvpr_dNRI&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/tgis.12644&#34;&gt;Traffic transformer: Capturing the continuity and periodicity of time series for traffic forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ling Cai, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2006.06553.pdf&#34;&gt;Stanza: A Nonlinear State Space Model for Probabilistic Inference in Non-Stationary Time Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Anna K. Yanchenko and Sayan Mukherjee.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.scirp.org/journal/paperinformation.aspx?paperid=100727&#34;&gt;Neuroevolution Strategy for Time Series Prediction&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;George Naskos, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.mdpi.com/2076-3417/10/11/3880&#34;&gt;COVID-19: A Comparison of Time Series Methods to Forecast Percentage of Active Cases per Population&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Vasilis Papastefanopoulos, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/ML-Upatras/COVID-19-A-comparison-of-time-series-methods-foractive-cases-forecasting&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2006.00630.pdf&#34;&gt;A machine learning approach for forecasting hierarchical time series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Paolo Mancuso, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://www.jethrobrowell.com/uploads/4/5/4/0/45405281/probcast___pmaps2020.pdf&#34;&gt;ProbCast: Open-source Production, Evaluation and Visualisation of Probabilistic Forecasts&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Jethro Browell and Ciaran Gilbert.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/jbrowell/ProbCast&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ecole-itn.eu/wp-content/uploads/2020/05/IJCNN2020_Sibghat_Final.pdf&#34;&gt;Exploring Clinical Time Series Forecasting with Meta-Features in Variational Recurrent Models&lt;/a&gt;&lt;strong&gt;meta-learning&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Sibghat Ullah, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/SibghatUllah13/VRNNs-for-Clinical-Time-Series-Forecasting&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://spj.sciencemag.org/plantphenomics/2020/4261965/&#34;&gt;Semisupervised Deep State-Space Model for Plant Growth Modeling&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;S. Shibata, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2005.11633.pdf&#34;&gt;EFFECTIVE AND EFFICIENT COMPUTATION WITH MULTIPLE-TIMESCALE SPIKING RECURRENT NEURAL NETWORKS&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Bojian Yin, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0925231220300606&#34;&gt;Multivariate time series forecasting via attention-based encoderâ€“decoder framework&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Shengdong Du, et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Neurocomputing&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.mdpi.com/1424-8220/20/10/2832&#34;&gt;A Novel LSTM for Multivariate Time Series with Massive Missingness&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Nazanin Fouladgar and Kary FrÃ¤mling.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.10437.pdf&#34;&gt;N-BEATS: NEURAL BASIS EXPANSION ANALYSIS FOR INTERPRETABLE TIME SERIES FORECASTING&lt;/a&gt;&lt;code&gt; ICLR 2020&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Boris N. Oreshkin, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2005.10698.pdf&#34;&gt;How to Learn from Others: Transfer Machine Learning with Additive Regression Models to Improve Sales Forecasting&lt;/a&gt;&lt;strong&gt;good new approach&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Robin Hirt, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.medrxiv.org/content/medrxiv/early/2020/05/22/2020.05.20.20103200.full.pdf&#34;&gt;The Hybrid Forecasting Method SVR-ESAR for Covid-19&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Juan Frausto Solis, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9094173&#34;&gt;Forecasting the Short-Term Metro Ridership With Seasonal and Trend Decomposition Using Loess and LSTM Neural Networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;DEWANG CHEN, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2005.10111.pdf&#34;&gt;The Effectiveness of Discretization in Forecasting: An Empirical Study on Neural Time Series Models&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Stephan Rabanser, et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;AWS AI Labs&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2005.08067.pdf&#34;&gt;FORECASTING WITH SKTIME: DESIGNING SKTIMEâ€™S NEW FORECASTING API AND APPLYING IT TO REPLICATE AND EXTEND THE M4 STUDY&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Markus LÃ¶ning and Franz J. KirÃ¡ly.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/sktime/sktime-dl&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1909.04293.pdf&#34;&gt;LSTM-MSNet: Leveraging Forecasts on Sets of Related Time Series with Multiple Seasonal Patterns&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Kasun Bandara, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/kasungayan/LSTMMSNet&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2005.06978.pdf&#34;&gt;A NETWORK-BASED TRANSFER LEARNING APPROACH TO IMPROVE SALES FORECASTING OF NEW PRODUCTS&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Karb, Tristan, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3357384.3358132&#34;&gt;DSANet: Dual Self-Attention Network for Multivariate Time Series Forecasting&lt;/a&gt; &lt;strong&gt;Good new approach&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Siteng Huang, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/bighuang624/DSANet&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://fruct.org/publications/fruct26/files/Moro.pdf&#34;&gt;An Approach for Complex Event Streams Processing and Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Viktor Morozov, Mikhail Petrovskiy.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2005.03297.pdf&#34;&gt;Knowledge Enhanced Neural Fashion Trend Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yunshan Ma, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-47358-7_30&#34;&gt;Augmented Out-of-Sample Comparison Method for Time Series Forecasting Techniques&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Igor Ilic, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9081393&#34;&gt;Enhancing High Frequency Technical Indicators Forecasting Using Shrinking Deep Neural Networks&lt;/a&gt; &lt;code&gt;ICIM 2020&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Xiaoyu Tan, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.13408.pdf&#34;&gt;Time Series Forecasting With Deep Learning: A Survey&lt;/a&gt; &lt;strong&gt;Good summary&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Bryan Lim and Stefan Zohren&lt;/li&gt; &#xA;   &lt;li&gt;Survey&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.10240.pdf&#34;&gt;Neural forecasting: Introduction and literature overview&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Konstantinos Benidis, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Not is a overview.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.09760.pdf&#34;&gt;Take a NAP: Non-Autoregressive Prediction for Pedestrian Trajectories&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Hao Xue, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.08492.pdf&#34;&gt;Orbit: Probabilistic Forecast with Exponential Smoothing&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Edwin Ng, et a.&lt;/li&gt; &#xA;   &lt;li&gt;Code is available upon request.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0169207020300224&#34;&gt;Daily retail demand forecasting using machine learning with emphasis on calendric special days&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Jakob Huber and Heiner Stuckenschmidt.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.03398.pdf&#34;&gt;FORECASTING IN MULTIVARIATE IRREGULARLY SAMPLED TIME SERIES WITH MISSING VALUES&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Shivam Srivastava, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;IBM Almaden Research Center&lt;/strong&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2001.10098.pdf&#34;&gt;Multi-label Prediction in Time Series Data using Deep Neural Networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Wenyu Zhang, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.02441.pdf&#34;&gt;TraDE: Transformers for Density Estimation&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Rasool Fakoor, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.01498.pdf&#34;&gt;Deep Probabilistic Modelling of Price Movements for High-Frequency Trading&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ye-Sheen Lim and Denise Gorse.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2003.14162.pdf&#34;&gt;Deep State Space Models for Nonlinear System Identification&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Daniel Gedon, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2003.12162.pdf&#34;&gt;Zero-shot and few-shot time series forecasting with ordinal regression recurrent neural networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Bernardo Perez Orozco and Stephen J. Roberts.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/bperezorozco/ordinal_tsf&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2003.12194.pdf&#34;&gt;Financial Time Series Representation Learning&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Philippe Chatigny, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2003.10551.pdf&#34;&gt;G-Net: A Deep Learning Approach to G-computation for Counterfactual Outcome Prediction Under Dynamic Treatment Regimes&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Rui Li, et al.&lt;/li&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;IBM research and MIT&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2003.09779.pdf&#34;&gt;Deep Markov Spatio-Temporal Factorization&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Amirreza Farnoosh, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/ecai20hr.pdf&#34;&gt;Harmonic Recurrent Process for Time Series Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Shao-Qun Zhang and Zhi-Hua Zhou.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ssc.io/pdf/modin711s.pdf&#34;&gt;Elastic Machine Learning Algorithms in Amazon SageMaker&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Edo Liberty, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2002.12478.pdf&#34;&gt;Time Series Data Augmentation for Deep Learning: A Survey&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Qingsong Wen, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2002.12135.pdf&#34;&gt;Block Hankel Tensor ARIMA for Multiple Short Time Series Forecasting&lt;/a&gt;&lt;code&gt;AAAI 2020&lt;/code&gt;&lt;strong&gt;meta-learning&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;QIQUAN SHI, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/huawei-noah/BHT-ARIMA&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/publication/339362837_Learnings_from_Kaggle&#39;s_Forecasting_Competitions&#34;&gt;Learnings from Kaggle&#39;s Forecasting Competitions&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Casper Solheim Bojer, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/8999262&#34;&gt;An Industry Case of Large-Scale Demand Forecasting of Hierarchical Components&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Rodrigo Rivera-Castro, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2002.06103.pdf&#34;&gt;Multi-variate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Kashif Rasul, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/zalandoresearch/pytorch-ts&#34;&gt;Code&lt;/a&gt;].&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2002.04155.pdf&#34;&gt;ForecastNet: A Time-Variant Deep Feed-Forward Neural Network Architecture for Multi-Step-Ahead Time-Series Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Joel Janek Dabrowski, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://pdfs.semanticscholar.org/810b/dfa0f63f03473be79556b90dc79a88a1f769.pdf&#34;&gt;Anomaly detection for Cybersecurity: time series forecasting and deep learning&lt;/a&gt;&lt;code&gt;Good review about forecasting&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Giordano ColÃ².&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://krvarshney.github.io/pubs/BhattacharjyaSGMVS_aaai2020.pdf&#34;&gt;Event-Driven Continuous Time Bayesian Networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Debarun Bhattacharjya, et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Research AI, IBM&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1911.10273.pdf&#34;&gt;Joint Modeling of Local and Global Temporal Dynamics for Multivariate Time Series Forecasting with Missing Values&lt;/a&gt;&lt;code&gt;AAAI 2020&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Xianfeng Tang, et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;IBM Research, NY&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/8964133&#34;&gt;Topology-Based Clusterwise Regression for User Segmentation and Demand Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Rodrigo Rivera-Castro, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S2210650219301270&#34;&gt;Evolutionary LSTM-FCN networks for pattern classification in industrial processes&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Patxi Ortego, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://rd.springer.com/chapter/10.1007/978-3-030-37309-2_10&#34;&gt;Forecasting Multivariate Time-Series Data Using LSTM and Mini-Batches&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Athar Khodabakhsh, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://faculty.ist.psu.edu/xzz89/publications/AAAI20.pdf&#34;&gt;Tensorized LSTM with Adaptive Shared Memory for Learning Trends in Multivariate Time Series&lt;/a&gt;&lt;code&gt;AAAI 2020&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Dongkuan Xu, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/DerronXu/DeepTrends/tree/master&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/pdf/2001.04050.pdf&#34;&gt;RELATIONAL STATE-SPACE MODEL FOR STOCHASTIC MULTI-OBJECT SYSTEMS&lt;/a&gt;&lt;/strong&gt;&lt;code&gt;ICLR 2020&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fan Yang, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2001.04601.pdf&#34;&gt;For2For: Learning to forecast from forecasts&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Zhao, Shi, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1909.08181.pdf&#34;&gt;Self-boosted Time-series Forecasting with Multi-task and Multi-view Learning&lt;/a&gt; &lt;code&gt;AAAI 2020&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Long H. Nguyen, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2019&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.aminer.cn/pub/5db9297a47c8f766461f7974/enhancing-the-locality-and-breaking-the-memory-bottleneck-of-transformer-on-time?anchor=conclusion&#34;&gt;Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting&lt;/a&gt; &lt;strong&gt;Reference&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Shiyang Li, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/mlpotter/Transformer_Time_Series&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3292500.3332289&#34;&gt;Forecasting Big Time Series: Theory and Practice&lt;/a&gt;&lt;code&gt;KDD 2019&lt;/code&gt; &lt;strong&gt;Relevant tutorial&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Christos Faloutsos, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://lovvge.github.io/Forecasting-Tutorial-KDD-2019/&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://urban-computing.com/pdf/kdd19-BinWang.pdf&#34;&gt;Deep Uncertainty Quantification: A Machine Learning Approach for Weather Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Bin Wang, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/BruceBinBoxing/Deep_Learning_Weather_Forecasting&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/publication/334556784_A_hybrid_method_of_exponential_smoothing_and_recurrent_neural_networks_for_time_series_forecasting&#34;&gt;A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Slawek Smyl&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Winning submission of the M4 forecasting competition&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/Mcompetitions/M4-methods/tree/slaweks_ES-RNN/118%20-%20slaweks17&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.03806.pdf&#34;&gt;Think Globally, Act Locally: A Deep Neural Network Approach to High-Dimensional Time Series Forecasting&lt;/a&gt;&lt;code&gt;NeurIPS 2019&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Rajat Sen, et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Amazon&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/rajatsen91/deepglo&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.03028&#34;&gt;Deep Landscape Forecasting for Real-time Bidding Advertising&lt;/a&gt; &lt;code&gt;KDD 2019&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Kan Ren, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/rk2900/DLF&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1702.03584.pdf&#34;&gt;Similarity Preserving Representation Learning for Time Series Clustering&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Qi Lei, et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;IBM research&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/cecilialeiqi/SPIRAL&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3357384.3358132&#34;&gt;DSANet: Dual Self-Attention Network for Multivariate Time Series Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Siteng Huang, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1904.04912.pdf&#34;&gt;Enhancing Time Series Momentum Strategies Using Deep Neural Networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Bryan Lim, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://hal.inria.fr/hal-02422148/document&#34;&gt;DYNAMIC TIME LAG REGRESSION: PREDICTING WHAT &amp;amp; WHEN&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Mandar Chandorkar, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks.pdf&#34;&gt;Time-series Generative Adversarial Networks&lt;/a&gt;&lt;code&gt;NeurIPS 2019&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Jinsung Yoon. et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1912.09363.pdf&#34;&gt;Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Bryan Lim, et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Google Research&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/tft&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://openreview.net/pdf?id=H1xXYy3VKr&#34;&gt;Deep Amortized Variational Inference for Multivariate Time Series Imputation with Latent Gaussian Process Models&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Vincent Fortuin, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1912.01762.pdf&#34;&gt;Deep Physiological State Space Model for Clinical Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yuan Xue, et al.&lt;/li&gt; &#xA;   &lt;li&gt;not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.12436&#34;&gt;AR-Net: A simple Auto-Regressive Neural Network for time-series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Oskar Triebe, et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Facebook Research&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ecole-itn.eu/wp-content/uploads/2019/11/LMID_Sneha_finalversion.pdf&#34;&gt;Learning Time-series Data of Industrial Design Optimization using Recurrent Neural Networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Sneha Saha, et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Honda Research Institute Europe GmbH&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.01767&#34;&gt;RobustSTL: A Robust Seasonal-Trend Decomposition Algorithm for Long Time Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Qingsong Wen, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/LeeDoYup/RobustSTL&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1911.05035.pdf&#34;&gt;Constructing Gradient Controllable Recurrent Neural Networks Using Hamiltonian Dynamics&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Konstantin Rusch, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://openreview.net/pdf?id=rygjcsR9Y7&#34;&gt;SOM-VAE: Interpretable Discrete Representation Learning on Time Series&lt;/a&gt;&lt;code&gt;ICLR 2019&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Vincent Fortuin, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/ratschlab/SOM-VAE&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1901.10738&#34;&gt;Unsupervised Scalable Representation Learning for Multivariate Time Series&lt;/a&gt;&lt;code&gt;NeurIPS 2019&lt;/code&gt; &lt;a href=&#34;https://nips.cc/Conferences/2019/Schedule?showParentSession=15627&#34;&gt;In Applications -- Time Series Analysis &lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Jean-Yves Franceschi, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/White-Link/UnsupervisedScalableRepresentationLearningTimeSeries&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.13570v1.pdf&#34;&gt;Factorized Inference in Deep Markov Models for Incomplete Multimodal Time Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Zhi-Xuan Tan, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.09620.pdf&#34;&gt;You May Not Need Order in Time Series Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yunkai Zhang, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Alro10/deep-learning-time-series/raw/master/YMN_order.png&#34; alt=&#34;alt text&#34; width=&#34;80%&#34; height=&#34;60%&#34;&gt; &lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1909.09020.pdf&#34;&gt;Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models&lt;/a&gt;&lt;code&gt;NeurIPS2019&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Vincent Le Guen and Nicolas Thome.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/vincent-leguen/STDL&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.07927.pdf&#34;&gt;Dynamic Local Regret for Non-convex Online Forecasting&lt;/a&gt;&lt;code&gt;NeurIPS 2019&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Sergul Aydore, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/Timbasa/Dynamic_Local_Regret_for_Non-convex_Online_Forecasting_NeurIPS2019&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.06366.pdf&#34;&gt;Bayesian Temporal Factorization for Multidimensional Time Series Prediction&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Xinyu Chen, and Lijun Sun&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/xinychen/transdim&#34;&gt;Code and data&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.03906.pdf&#34;&gt;Probabilistic sequential matrix factorization&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ã–mer Deniz Akyildiz, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.03818.pdf&#34;&gt;Sequential VAE-LSTM for Anomaly Detection on Time Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Run-Qing Chen, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.03002.pdf&#34;&gt;High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes&lt;/a&gt;&lt;code&gt;NeurIPS 2019&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;David Salinas, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1901.08096.pdf&#34;&gt;Recurrent Neural Filters: Learning Independent Bayesian Filtering Steps for Time Series Prediction&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Bryan Lim, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/8855402&#34;&gt;LHCnn: A Novel Efficient Multivariate Time Series Prediction Framework Utilizing Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Chengxi Liu, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1909.07872.pdf&#34;&gt;SKTIME: A UNIFIED INTERFACE FOR MACHINE LEARNING WITH TIME SERIE&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/alan-turing-institute/sktime&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/pdf/1909.00590.pdf&#34;&gt;Recurrent Neural Networks for Time Series Forecasting: Current Status and Future Directions&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/HansikaPH/time-series-forecasting&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Antonio_Parmezan/publication/330742498_Evaluation_of_statistical_and_machine_learning_models_for_time_series_prediction_Identifying_the_state-of-the-art_and_the_best_conditions_for_the_use_of_each_model/links/5c558145a6fdccd6b5dc3e2e/Evaluation-of-statistical-and-machine-learning-models-for-time-series-prediction-Identifying-the-state-of-the-art-and-the-best-conditions-for-the-use-of-each-model.pdf&#34;&gt;Evaluation of statistical and machine learning models for time series prediction: Identifying the state-of-the-art and the best conditions for the use of each model&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Antonio Rafael Sabino Parmezan, Vinicius M. A. Souza and Gustavo E. A. P. A. Batista. USP&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.ijcai.org/proceedings/2019/0932.pdf&#34;&gt;Explainable Deep Neural Networks for Multivariate Time Series Predictions&lt;/a&gt; &lt;code&gt;IJCAI 2019&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Roy Assaf and Anika Schumann.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;IBM Research, Zurich&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.ijcai.org/proceedings/2019/0378.pdf&#34;&gt;Outlier Detection for Time Series with Recurrent Autoencoder Ensembles&lt;/a&gt; &lt;code&gt;IJCAI 2019&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/tungk/OED&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.ijcai.org/proceedings/2019/0402.pdf&#34;&gt;Learning Interpretable Deep State Space Model for Probabilistic Time Series Forecasting&lt;/a&gt; &lt;code&gt;IJCAI 2019&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.12417.pdf&#34;&gt;Deep Factors for Forecasting&lt;/a&gt; &lt;code&gt;ICML 2019&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yuyang Wang, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://proceedings.mlr.press/v89/gasthaus19a/gasthaus19a.pdf&#34;&gt;Probabilistic Forecasting with Spline Quantile Function RNNs&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1809.04356&#34;&gt;Deep learning for time series classification: a review&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.04503&#34;&gt;Multivariate LSTM-FCNs for Time Series Classification&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0169207019301529&#34;&gt;Criteria for classifying forecasting methods&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1906.05264&#34;&gt;GluonTS: Probabilistic Time Series Models in Python&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://gluon-ts.mxnet.io&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1704.04110&#34;&gt;DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;David Salinas, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2018&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1705.04378.pdf&#34;&gt;An overview and comparative analysis of Recurrent Neural Networks for Short Term Load Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Filippo Maria Bianchi, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194889&#34;&gt;Statistical and Machine Learning forecasting methods: Concerns and ways forward&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Spyros Makridakis, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16325/16790&#34;&gt;Attend and Diagnose: Clinical Time Series Analysis Using Attention Models&lt;/a&gt; &lt;code&gt;AAAI 2018&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Huan Song, Deepta Rajan, et al.&lt;/li&gt; &#xA;   &lt;li&gt;not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://papers.nips.cc/paper/7462-precision-and-recall-for-time-series&#34;&gt;Precision and Recall for Time Series&lt;/a&gt; &lt;code&gt;NeurIPS2018&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Nesime Tatbul, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/8004-deep-state-space-models-for-time-series-forecasting.pdf&#34;&gt;Deep State Space Models for Time Series Forecasting&lt;/a&gt; &lt;code&gt;NeurIPS2018&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Code not yet&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.00098&#34;&gt;Deep Factors with Gaussian Processes for Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Third workshop on Bayesian Deep Learning (NeurIPS 2018)&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://aws.amazon.com/blogs/machine-learning/now-available-in-amazon-sagemaker-deepar-algorithm-for-more-accurate-time-series-forecasting/&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1707.01926.pdf&#34;&gt;DIFFUSION CONVOLUTIONAL RECURRENT NEURAL NETWORK: DATA-DRIVEN TRAFFIC FORECASTING&lt;/a&gt;&lt;code&gt;ICLR 2018&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yaguang Li, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/liyaguang/DCRNN&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1802.01059.pdf&#34;&gt;DEEP TEMPORAL CLUSTERING: FULLY UNSUPERVISED LEARNING OF TIME-DOMAIN FEATURES&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Naveen Sai Madiraju, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/FlorentF9/DeepTemporalClustering&#34;&gt;Code-unofficial implementation &lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.07015.pdf&#34;&gt;Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Guokun Lai, Wei-Cheng Chang, Yiming Yang, Hanxiao Liu&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/laiguokun/LSTNet&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://papers.nips.cc/paper/7977-forecasting-treatment-responses-over-time-using-recurrent-marginal-structural-networks.pdf&#34;&gt;Forecasting Treatment Responses Over Time Using Recurrent Marginal Structural Networks&lt;/a&gt; &lt;code&gt;NeurIPS 2018&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Bryan Lim. et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/sjblim/rmsn_nips_2018&#34;&gt;Code&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1809.02105.pdf&#34;&gt;A Memory-Network Based Solution for Multivariate Time-Series Forecasting&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yen-Yu Chang, et al.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Maple728/MTNet&#34;&gt;Code-unofficial implementation&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2017&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.econstor.eu/bitstream/10419/157808/1/886576210.pdf&#34;&gt;Deep learning with long short-term memory networks for financial market predictions&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fischer, Thomas and Krauss, Christopher.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/7150-discriminative-state-space-models.pdf&#34;&gt;Discriminative State-Space Models&lt;/a&gt;&lt;code&gt;NIPS 2017&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Vitaly Kuznetsov and Mehryar Mohri.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.ijcai.org/Proceedings/2017/0316.pdf&#34;&gt;Hybrid Neural Networks for Learning the Trend in Time Series&lt;/a&gt;&lt;strong&gt;review&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Tao Lin, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2016&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/publication/309385800_Data_Preprocessing_and_Augmentation_for_Multiple_Short_Time_Series_Forecasting_with_Recurrent_Neural_Networks&#34;&gt;Data Preprocessing and Augmentation for Multiple Short Time Series Forecasting with Recurrent Neural Networks&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Slawek Smyl and Karthik Kuber&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/6160-temporal-regularized-matrix-factorization-for-high-dimensional-time-series-prediction&#34;&gt;Temporal Regularized Matrix Factorization for High-dimensional Time Series Prediction&lt;/a&gt;&lt;code&gt;NIPS 2016&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Hsiang-Fu Yu, et al.&lt;/li&gt; &#xA;   &lt;li&gt;[&lt;a href=&#34;https://github.com/rofuyu/exp-trmf-nips16&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://proceedings.mlr.press/v49/kuznetsov16.pdf&#34;&gt;Time Series Prediction and Online Learning&lt;/a&gt;&lt;code&gt;JMLR 2016&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Vitaly Kuznetsov and Mehryar Mohri.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ideas.repec.org/p/zbw/iwqwdp/032016.html&#34;&gt;Deep neural networks, gradient-boosted trees, random forests: Statistical arbitrage on the S&amp;amp;P 500&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Krauss, Christopher, et al.&lt;/li&gt; &#xA;   &lt;li&gt;Code not yet.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Comparative: Classical methods vs Deep Learning methods&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1803.06386.pdf&#34;&gt;Forecasting economic and financial time series: ARIMA VS. LSTM&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://pdfs.semanticscholar.org/e58c/7343ea25d05f6d859d66d6bb7fb91ecf9c2f.pdf&#34;&gt;A comparative study between LSTM and ARIMA for sales forecasting in retail&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/arima-sarima-vs-lstm-with-ensemble-learning-insights-for-time-series-data-509a5d87f20a&#34;&gt;ARIMA/SARIMA vs LSTM with Ensemble learning Insights for Time Series Data&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Conferences&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Machine learning&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nips.cc/&#34;&gt;NeurIPS&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://icml.cc/&#34;&gt;ICML&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://iclr.cc/&#34;&gt;ICLR&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Artificial intelligence&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.aaai.org/&#34;&gt;AAAI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.aistats.org/&#34;&gt;AISTATS&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://e-nns.org/icann2019/&#34;&gt;ICANN&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.ijcai.org/&#34;&gt;IJCAI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;http://www.auai.org/&#34;&gt;UAI&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Competitions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mofc.unic.ac.cy/m5-competition/&#34;&gt;M5 Competition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mcompetitions/M4-methods&#34;&gt;M4 Competition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Code&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Alro10/deep-learning-time-series/tree/master/notebooks&#34;&gt;Notebooks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;Code&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Theory-Resource&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/forecasting&#34;&gt;Time Series Forecasting Best Practices &amp;amp; Examples from Microsoft&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/attention-for-time-series-classification-and-forecasting-261723e0006d&#34;&gt;Attention-for-time-series-classification-and-forecasting&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/deep-learning-for-high-dimensional-time-series-7a72b033a7e0&#34;&gt;Deep learning for high dimensional time series-blog&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://deeplearning.ai/ai-notes/optimization/&#34;&gt;Deep Learning AI-Optimization&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/back-to-basics-deriving-back-propagation-on-simple-rnn-lstm-feat-aidan-gomez-c7f286ba973d&#34;&gt;Backpropagation for LSTM&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://blog.usejournal.com/stock-market-prediction-by-recurrent-neural-network-on-lstm-model-56de700bff68&#34;&gt;Stock Market Prediction by Recurrent Neural Network on LSTM Model&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.05245.pdf&#34;&gt;Decoupling Hierarchical Recurrent Neural Networks With Locally Computable Losses&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/time-series-analysis-with-deep-learning-simplified-5c444315d773&#34;&gt;Time Series Analysis with Deep Learning : Simplified&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/machine-learning-techniques-applied-to-stock-price-prediction-6c1994da8001&#34;&gt;ML techniques applied to stock prices&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/robjhyndman/ETC3550Slides&#34;&gt;Forecasting: Principles and Practice: Slides&lt;/a&gt;&lt;strong&gt;Good material&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Code-Resource&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/oliverguhr/transformer-time-series-prediction&#34;&gt;Transformer Time Series Prediction&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/EvilPsyCHo/Deep-Time-Series-Prediction&#34;&gt;DeepSeries: Deep Learning Models for time series prediction.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2005.10361.pdf&#34;&gt;varstan: An R package for Bayesian analysis of structured time series models with Stan&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/firmai/tsgan&#34;&gt;Time-series Generative Adversarial Networks: tsgan&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/MSRDL/Deep4Cast&#34;&gt;Deep4cast: Forecasting for Decision Making under Uncertainty&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/jxx123/fireTS&#34;&gt;fireTS: sklean style package for multi-variate time-series prediction.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/epiforecasts/EpiSoon&#34;&gt;EpiSoon: Forecasting the effective reproduction number over short timescales&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/pyaf/load_forecasting&#34;&gt;Electric Load Forecasting&lt;/a&gt;: Load forecasting on Delhi area electric power load using ARIMA, RNN, LSTM and GRU models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/rstudio-conf-2020/time-series-forecasting&#34;&gt;Time Series and Forecasting in R&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/timeseriesAI/timeseriesAI&#34;&gt;TimeseriesAI&lt;/a&gt;: Practical Deep Learning for Time Series / Sequential Data using fastai/ Pytorch.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/timescale/timescaledb&#34;&gt;TimescaleDB&lt;/a&gt;: An open-source time-series SQL database optimized for fast ingest and complex queries. Packaged as a PostgreSQL extension.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/RamiKrispin/TSstudio&#34;&gt;TSstudio: Tools for time series analysis and forecasting &lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/facebook/prophet&#34;&gt;Prophet: Automatic Forecasting Procedure&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/johannfaouzi/pyts&#34;&gt;pyts: a Python package for time series classification&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/wassname/attentive-neural-processes&#34;&gt;Using attentive neural processes for forecasting power usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://robjhyndman.com/hyndsight/fable2/&#34;&gt;Non-Gaussian forecasting using fable - R&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/alan-turing-institute/sktime&#34;&gt;SKTIME&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/task/multivariate-time-series-forecasting&#34;&gt;Papers with code - Multivariate time series forecasting&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html&#34;&gt;DeepAR by Amazon&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/blogs/machine-learning/now-available-in-amazon-sagemaker-deepar-algorithm-for-more-accurate-time-series-forecasting/&#34;&gt;DFGP by Amazon&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/c/demand-forecasting-kernels-only&#34;&gt;https://www.kaggle.com/c/demand-forecasting-kernels-only&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/c/favorita-grocery-sales-forecasting&#34;&gt;https://www.kaggle.com/c/favorita-grocery-sales-forecasting&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/c/grupo-bimbo-inventory-demand&#34;&gt;https://www.kaggle.com/c/grupo-bimbo-inventory-demand&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting&#34;&gt;https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0169207019301098&#34;&gt;Predicting/hypothesizing the findings of the M4 Competition&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/jdb78/pytorch-forecasting&#34;&gt;pytorch-forecasting&lt;/a&gt;: A Python package for time series forecasting with PyTorch. It includes state-of-the-art network architectures&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/xephonhq/awesome-time-series-database&#34;&gt;A curated list of awesome time series databases&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014&#34;&gt;Electricity dataset from UCI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/PEMS-SF&#34;&gt;Traffic dataset from UCI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://archive.ics.uci.edu/ml/datasets/Air+Quality&#34;&gt;Air quality from UCI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/zhiyongc/Seattle-Loop-Data&#34;&gt;Seattle freeway traffic speed&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/c/web-traffic-time-series-forecasting&#34;&gt;Kaggle-Web Traffic Time Series Forecasting&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>AISIN-TRC/E2Pose</title>
    <updated>2022-12-17T01:35:07Z</updated>
    <id>tag:github.com,2022-12-17:/AISIN-TRC/E2Pose</id>
    <link href="https://github.com/AISIN-TRC/E2Pose" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fully Convolutional Networks for End-to-End Multi-Person Pose Estimation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;E2Pose: Fully Convolutional Networks for End-to-End Multi-Person Pose Estimation&lt;/h1&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;h2&gt;Demo on Google Colab&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://colab.research.google.com/github/AISIN-TRC/E2Pose/blob/main/demo_inference.ipynb&#34;&gt;demo_inference.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Download pre-train models&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./pretrains/download.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference E2Pose&#39;s demo on localhost&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#inference video&#xA;./inference.sh --src &#39;./sample/$YOUR_MOVIE.mp4&#39;&#xA;#inference image&#xA;./inference.sh --src &#39;./sample/$YOUR_IMAGE.jpg&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Convert to TensorRT model (Running on Jetson devices takes a very long time)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./generate_trtmodel.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(Reference) Time required for conversion @ Jetson AGX Xavier&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ResNet101/512x512 : tf2onnx = 108 minutes, onnx2trt = 20 minutes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Benchmark code for comparing framerates&lt;/h1&gt; &#xA;&lt;h2&gt;OpenPifPaf: Composite Fields for Semantic Keypoint Detection and Spatio-Temporal Association [&lt;a href=&#34;https://arxiv.org/abs/2103.02440&#34;&gt;arxiv&lt;/a&gt;][&lt;a href=&#34;https://github.com/openpifpaf/openpifpaf&#34;&gt;github&lt;/a&gt;]&lt;/h2&gt; &#xA;&lt;h3&gt;Inference openPifPaf&#39;s demo on localhost&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#inference video&#xA;./inference_pifpaf.sh --src &#39;./sample/$YOUR_MOVIE.mp4&#39;&#xA;#inference image&#xA;./inference_pifpaf.sh --src &#39;./sample/$YOUR_IMAGE.jpg&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;OpenMMLab: Pose Estimation Toolbox and Benchmark [&lt;a href=&#34;https://github.com/open-mmlab/mmpose&#34;&gt;github&lt;/a&gt;]&lt;/h2&gt; &#xA;&lt;h3&gt;inference mmpose&#39;s demo on localhsot&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#inference video&#xA;./inference_mmpose.sh --src &#39;./smaple/$YOUR_MOVIE.mp4&#39;&#xA;#inference image&#xA;./inference_mmpose.sh --src &#39;./sample/$YOUR_IMAGE.jpg&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Commercial License&lt;/h1&gt; &#xA;&lt;p&gt;The open source license is in the &lt;a href=&#34;https://raw.githubusercontent.com/AISIN-TRC/E2Pose/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file. This software is also available for licensing via the AISIN Corp. (&lt;a href=&#34;https://www.aisin.com/&#34;&gt;https://www.aisin.com/&lt;/a&gt;).&lt;/p&gt;</summary>
  </entry>
</feed>