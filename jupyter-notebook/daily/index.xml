<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-16T01:36:41Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>williamyang1991/StyleGANEX</title>
    <updated>2023-03-16T01:36:41Z</updated>
    <id>tag:github.com,2023-03-16:/williamyang1991/StyleGANEX</id>
    <link href="https://github.com/williamyang1991/StyleGANEX" rel="alternate"></link>
    <summary type="html">&lt;p&gt;StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;StyleGANEX - Official PyTorch Implementation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/18130694/224256980-03fb15e7-9858-4300-9d35-7604d03c69f9.mp4&#34;&gt;https://user-images.githubusercontent.com/18130694/224256980-03fb15e7-9858-4300-9d35-7604d03c69f9.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository provides the official PyTorch implementation for the following paper:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://williamyang1991.github.io/&#34;&gt;Shuai Yang&lt;/a&gt;, &lt;a href=&#34;https://liming-jiang.com/&#34;&gt;Liming Jiang&lt;/a&gt;, &lt;a href=&#34;https://liuziwei7.github.io/&#34;&gt;Ziwei Liu&lt;/a&gt; and &lt;a href=&#34;https://www.mmlab-ntu.com/person/ccloy/&#34;&gt;Chen Change Loy&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.mmlab-ntu.com/project/styleganex/&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2303.06146&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/8oK0TXQmxg8&#34;&gt;&lt;strong&gt;Supplementary Video&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://colab.research.google.com/github/williamyang1991/StyleGANEX/blob/master/inference_playground.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;google colab logo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/PKUWilliamYang/StyleGANEX&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://visitor-badge.laobi.icu/badge?page_id=williamyang1991/styleganex&#34; alt=&#34;visitors&#34;&gt;&lt;/p&gt; &#xA;&lt;!--![visitors](https://visitor-badge.glitch.me/badge?page_id=williamyang1991/styleganex)--&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; &lt;em&gt;Recent advances in face manipulation using StyleGAN have produced impressive results. However, StyleGAN is inherently limited to cropped aligned faces at a fixed image resolution it is pre-trained on. In this paper, we propose a simple and effective solution to this limitation by using dilated convolutions to rescale the receptive fields of shallow layers in StyleGAN, without altering any model parameters. This allows fixed-size small features at shallow layers to be extended into larger ones that can accommodate variable resolutions, making them more robust in characterizing unaligned faces. To enable real face inversion and manipulation, we introduce a corresponding encoder that provides the first-layer feature of the extended StyleGAN in addition to the latent style code. We validate the effectiveness of our method using unaligned face inputs of various resolutions in a diverse set of face manipulation tasks, including facial attribute editing, super-resolution, sketch/mask-to-face translation, and face toonification.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;br&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Support for Unaligned Faces&lt;/strong&gt;: StyleGANEX can manipulate normal field-of-view face images and videos.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Compatibility&lt;/strong&gt;: StyleGANEX can directly load pre-trained StyleGAN parameters without retraining.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible Manipulation&lt;/strong&gt;: StyleGANEX retains the style representation and editing ability of StyleGAN.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18130694/224257328-b6d9bac1-d467-468f-9dba-c89dfed8b931.jpg&#34; alt=&#34;overview&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[03/2023] Integrated to ðŸ¤— &lt;a href=&#34;https://huggingface.co/spaces/PKUWilliamYang/StyleGANEX&#34;&gt;Hugging Face&lt;/a&gt;. Enjoy the web demo!&lt;/li&gt; &#xA; &lt;li&gt;[03/2023] Inference code is released.&lt;/li&gt; &#xA; &lt;li&gt;[03/2023] This website is created.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Clone this repo:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/williamyang1991/StyleGANEX.git&#xA;cd StyleGANEX&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dependencies:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We have tested on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA 10.1&lt;/li&gt; &#xA; &lt;li&gt;PyTorch 1.7.1&lt;/li&gt; &#xA; &lt;li&gt;Pillow 8.3.1; Matplotlib 3.4.2; opencv-python 4.5.3; tqdm 4.61.2; Ninja 1.10.2; dlib 19.24.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;(1) Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Inference Notebook&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://colab.research.google.com/github/williamyang1991/StyleGANEX/blob/master/inference_playground.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;google colab logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To help users get started, we provide a Jupyter notebook found in &lt;code&gt;./inference_playground.ipynb&lt;/code&gt; that allows one to visualize the performance of StyleGANEX. The notebook will download the necessary pretrained models and run inference on the images found in &lt;code&gt;./data/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Gradio demo&lt;/h3&gt; &#xA;&lt;p&gt;We also provide a UI for testing StyleGANEX that is built with gradio. Running the following command in a terminal will launch the demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python app_gradio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This demo is also hosted on &lt;a href=&#34;https://huggingface.co/spaces/PKUWilliamYang/StyleGANEX&#34;&gt;Hugging Face&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-trained Models&lt;/h3&gt; &#xA;&lt;p&gt;Pre-trained models can be downloaded from &lt;a href=&#34;https://drive.google.com/drive/folders/1zGssOxjdklMd_5kdBKV9VkENnS5EXZlx?usp=share_link&#34;&gt;Google Drive&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/15SZrkvgduUaMI33LJGnEJg?pwd=luck&#34;&gt;Baidu Cloud&lt;/a&gt; (access code: luck) or &lt;a href=&#34;https://huggingface.co/PKUWilliamYang/StyleGANEX/tree/main/pretrained_models&#34;&gt;Hugging Face&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt;&#xA;   &lt;th&gt;Model&lt;/th&gt;&#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Inversion&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/157twOAYihuy_b6l_XrmP7QUOMhkNC2Ii/view?usp=share_link&#34;&gt;styleganex_inversion.pt&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained model for StyleGANEX inversion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Image translation&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1ewbdY_0fRZS6GIboFcvx6QDBbqHXprvR/view?usp=share_link&#34;&gt;styleganex_sr32.pt&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained model specially for 32x face super resolution&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1XQ4vp8DB2dSrvQVj3xifSl4sUGMxr4zK/view?usp=share_link&#34;&gt;styleganex_sr.pt&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained model for 4x-48x face super resolution&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1L3iRp3UE-_Or0NqzUtqZPLQ9hQ_9yax5/view?usp=share_link&#34;&gt;styleganex_sketch2face.pt&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained model for skech-to-face translation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1rHC63z1tUX63-56RUc0thhHSVAQr8Cf2/view?usp=share_link&#34;&gt;styleganex_mask2face.pt&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained model for parsing map-to-face translation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;5&#34;&gt;Video editing&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/164eD7pafO74xiCCFLzhb56ofOBwIUnpL/view?usp=share_link&#34;&gt;styleganex_edit_hair.pt&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained model for hair color editing on videos&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1tH-vlpn5THyD-HoOQrYGuUENH49daAFX/view?usp=share_link&#34;&gt;styleganex_edit_age.pt&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained model for age editing on videos&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/16Kth67C1AX3SjZS3030DChB_eHpqGOsn/view?usp=share_link&#34;&gt;styleganex_toonify_cartoon.pt&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained Cartoon model for video face toonification&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1OkCw4mrrCvTnEfPOeUxjv7yFpzzQ5trf/view?usp=share_link&#34;&gt;styleganex_toonify_arcane.pt&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained Arcane model for video face toonification&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1_XZjvj-rQvT2q3hiqPZ8gIClMo3ZGVH-/view?usp=share_link&#34;&gt;styleganex_toonify_pixar.pt&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;pre-trained Pixar model for video face toonification&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Supporting model&lt;/th&gt;&#xA;   &lt;th&gt; &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1jY0mTjVB8njDh6e0LP_2UxuRK3MnjoIR/view&#34;&gt;faceparsing.pth&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;BiSeNet for face parsing from &lt;a href=&#34;https://github.com/zllrunning/face-parsing.PyTorch&#34;&gt;face-parsing.PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;The downloaded models are suggested to be put into &lt;code&gt;./pretrained_models/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;StyleGANEX Inversion&lt;/h3&gt; &#xA;&lt;p&gt;We can embed a face image into the latent space of StyleGANEX to obtain its w+ latent code and the first-layer feature f with &lt;code&gt;inversion.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python inversion.py --ckpt STYLEGANEX_MODEL_PATH --data_path FACE_IMAGE_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results are saved in the folder &lt;code&gt;./output/&lt;/code&gt;. The results contain a reconstructed image &lt;code&gt;FILE_NAME_inversion.jpg&lt;/code&gt; and a &lt;code&gt;FILE_NAME_inversion.pt&lt;/code&gt; file. You can obtain w+ latent code and the first-layer feature f by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;latents = torch.load(&#39;./output/FILE_NAME_inversion.pt&#39;)&#xA;wplus_hat = latents[&#39;wplus&#39;].to(device) # w+&#xA;f_hat = [latents[&#39;f&#39;][0].to(device)]    # f&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;./inference_playground.ipynb&lt;/code&gt; provides some face editing examples based on &lt;code&gt;wplus_hat&lt;/code&gt; and &lt;code&gt;f_hat&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Image Translation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;image_translation.py&lt;/code&gt; supports face super-resolution, sketch-to-face translation and parsing map-to-face translation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python image_translation.py --ckpt STYLEGANEX_MODEL_PATH --data_path FACE_INPUT_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results are saved in the folder &lt;code&gt;./output/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Additional notes to consider:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--parsing_model_ckpt&lt;/code&gt; (default: &lt;code&gt;pretrained_models/faceparsing.pth&lt;/code&gt;): path to the pre-trained parsing model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--resize_factor&lt;/code&gt; (default: 32): super resolution resize factor &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For &lt;a href=&#34;&#34;&gt;styleganex_sr.pt&lt;/a&gt;, should be in [4, 48]&lt;/li&gt; &#xA;   &lt;li&gt;For &lt;a href=&#34;&#34;&gt;styleganex_sr32.pt&lt;/a&gt;, should be 32&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--number&lt;/code&gt; (default: 4): output number of multi-modal translation (for sketch/mask-to-face translation task)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--use_raw_data&lt;/code&gt; (default: False): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;if not specified, apply possible pre-processing to the input data &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;For &lt;a href=&#34;&#34;&gt;styleganex_sr/sr32.pt&lt;/a&gt;, the input face image, e.g., &lt;code&gt;./data/ILip77SbmOE.png&lt;/code&gt; will be downsampled based on &lt;code&gt;--resize_factor&lt;/code&gt;. The downsampled image will be also saved in &lt;code&gt;./output/&lt;/code&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;For &lt;a href=&#34;&#34;&gt;styleganex_sketch2face.pt&lt;/a&gt;, no pre-processing will be applied.&lt;/li&gt; &#xA;     &lt;li&gt;For &lt;a href=&#34;&#34;&gt;styleganex_mask2face.pt&lt;/a&gt;, the input face image, e.g., &lt;code&gt;./data/ILip77SbmOE.png&lt;/code&gt; will be transformed into a parsing map. The parsing map and its visualization version will be also saved in &lt;code&gt;./output/&lt;/code&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;if specified, directly load input data without pre-processing &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;For &lt;a href=&#34;&#34;&gt;styleganex_sr/sr32.pt&lt;/a&gt;, the input should be downsampled face images, e.g., &lt;code&gt;./data/ILip77SbmOE_45x45.png&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;For &lt;a href=&#34;&#34;&gt;styleganex_sketch2face.pt&lt;/a&gt;, the input should be a one-channel sketch image e.g., &lt;code&gt;./data/234_sketch.jpg&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;For &lt;a href=&#34;&#34;&gt;styleganex_mask2face.pt&lt;/a&gt;, the input should be a one-channel parsing map e.g., &lt;code&gt;./data/ILip77SbmOE_mask.png&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Video Editing&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;video_editing.py&lt;/code&gt; supports video facial attribute editing and video face toonification.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python video_editing.py --ckpt STYLEGANEX_MODEL_PATH --data_path FACE_INPUT_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results are saved in the folder &lt;code&gt;./output/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Additional notes to consider:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--data_path&lt;/code&gt;: the input can be either an image or a video.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--scale_factor&lt;/code&gt;: for attribute editing task (&lt;a href=&#34;&#34;&gt;styleganex_edit_hair/age&lt;/a&gt;), control the editing degree.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;(2) Training&lt;/h2&gt; &#xA;&lt;p&gt;The training code will be released upon the publication of the paper.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;(3) Results&lt;/h2&gt; &#xA;&lt;p&gt;Overview of StyleGANEX inversion and facial attribute/style editing on unaligned faces:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18130694/224259844-c9b37f4f-c786-48cd-a92f-121606b14b36.jpg&#34; alt=&#34;result&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Video facial attribute editing:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/18130694/224287063-7465a301-4d11-4322-819a-59d548308ce1.mp4&#34;&gt;https://user-images.githubusercontent.com/18130694/224287063-7465a301-4d11-4322-819a-59d548308ce1.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Video face toonification:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/18130694/224287136-7e5ce82d-664f-4a23-8ed3-e7005efb3b24.mp4&#34;&gt;https://user-images.githubusercontent.com/18130694/224287136-7e5ce82d-664f-4a23-8ed3-e7005efb3b24.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this work useful for your research, please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{yang2023styleganex,&#xA;â€ƒtitle = {StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces},&#xA;â€ƒauthor = {Yang, Shuai and Jiang, Liming and Liu, Ziwei and and Loy, Chen Change},&#xA; journal = {arXiv preprint arXiv:2303.06146},&#xA;â€ƒyear = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;The code is mainly developed based on &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;stylegan2-pytorch&lt;/a&gt;, &lt;a href=&#34;https://github.com/eladrich/pixel2style2pixel&#34;&gt;pixel2style2pixel&lt;/a&gt; and &lt;a href=&#34;https://github.com/williamyang1991/VToonify&#34;&gt;VToonify&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure/openai-samples</title>
    <updated>2023-03-16T01:36:41Z</updated>
    <id>tag:github.com,2023-03-16:/Azure/openai-samples</id>
    <link href="https://github.com/Azure/openai-samples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Samples for working with Azure OpenAI Service&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;What is the Azure OpenAI Service?&lt;/h1&gt; &#xA;&lt;p&gt;Azure OpenAI Service provides REST API access to OpenAI&#39;s powerful language models including the GPT-3, Codex and Embeddings model series. These models can be easily adapted to your specific task including but not limited to content generation, summarization, semantic search, and natural language to code translation. Users can access the service through REST APIs, Python SDK, or our web-based interface in the Azure OpenAI Studio.&lt;/p&gt; &#xA;&lt;h1&gt;How do I get access to Azure OpenAI?&lt;/h1&gt; &#xA;&lt;p&gt;Access is currently limited as we navigate high demand, upcoming product improvements, and &lt;a href=&#34;https://www.microsoft.com/ai/responsible-ai?activetab=pivot1:primaryr6&#34;&gt;Microsoftâ€™s commitment to responsible AI&lt;/a&gt;. For now, we&#39;re working with customers with an existing partnership with Microsoft, lower risk use cases, and those committed to incorporating mitigations. In addition to applying for initial access, all solutions using Azure OpenAI are required to go through a use case review before they can be released for production use.&lt;/p&gt; &#xA;&lt;p&gt;More specific information is included in the application form. We appreciate your patience as we work to responsibly enable broader access to Azure OpenAI.&lt;/p&gt; &#xA;&lt;p&gt;Apply here for initial access or for a production review: &lt;a href=&#34;https://aka.ms/oaiapply&#34;&gt;Apply now&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All solutions using Azure OpenAI are also required to go through a use case review before they can be released for production use, and are evaluated on a case-by-case basis. In general, the more sensitive the scenario the more important risk mitigation measures will be for approval.&lt;/p&gt; &#xA;&lt;h1&gt;Important concepts and terminology&lt;/h1&gt; &#xA;&lt;p&gt;Refer to the following documents for a better understanding of Azure OpenAI concepts and the related terminology:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/completions&#34;&gt;How to generate or manipulate text&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/understand-embeddings&#34;&gt;What are embeddings?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/embeddings?tabs=console&#34;&gt;How to generate embeddings?&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Responsible AI with Azure OpenAI&lt;/h1&gt; &#xA;&lt;p&gt;At Microsoft, we&#39;re committed to the advancement of AI driven by principles that put people first. Generative models such as the ones available in Azure OpenAI have significant potential benefits, but without careful design and thoughtful mitigations, such models have the potential to generate incorrect or even harmful content. Microsoft has made significant investments to help guard against abuse and unintended harm, which includes requiring applicants to show well-defined use cases, incorporating Microsoftâ€™s &lt;a href=&#34;https://www.microsoft.com/ai/responsible-ai?activetab=pivot1:primaryr6&#34;&gt;principles for responsible AI use&lt;/a&gt;, building content filters to support customers, and providing responsible AI implementation guidance to onboarded customers.&lt;/p&gt; &#xA;&lt;p&gt;More details on the RAI guidelines for the Azure OpenAI service can be found &lt;a href=&#34;https://learn.microsoft.com/en-us/legal/cognitive-services/openai/transparency-note?context=/azure/cognitive-services/openai/context/context&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;OpenAI Samples&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains samples demonstrating how to use GPT and ChatGPT via Python SDK or REST API.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install all Python modules and packages listed in the requirements.txt file using the below command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Microsoft Azure Endpoints&lt;/h3&gt; &#xA;&lt;p&gt;In order to use the Open AI library or REST API with Microsoft Azure endpoints, you need to set COMPLETIONS_MODEL, EMBEDDINGS_MODEL, OPENAI_API_BASE &amp;amp; OPENAI_API_VERSION in &lt;em&gt;config.json&lt;/em&gt; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;{&#xA;    &#34;COMPLETIONS_MODEL&#34;:&#34;&amp;lt;Completions Model Name&amp;gt;&#34;,&#xA;    &#34;EMBEDDINGS_MODEL&#34;:&#34;&amp;lt;Embeddings Model Name&amp;gt;&#34;,&#xA;    &#34;OPENAI_API_BASE&#34;:&#34;https://&amp;lt;Your Azure Resource Name&amp;gt;.openai.azure.com&#34;,&#xA;    &#34;OPENAI_API_VERSION&#34;:&#34;&amp;lt;OpenAI API Version&amp;gt;&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;For getting started:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &#34;OPENAI_API_KEY&#34; as variable name and &amp;lt;Your API Key Value&amp;gt; as variable value in the environment variables.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; One can get the OPENAI_API_KEY value from the Azure Portal. Go to https://portal.azure.com, find your resource and then under &#34;Resource Management&#34; -&amp;gt; &#34;Keys and Endpoints&#34; look for one of the &#34;Keys&#34; values. &#xA;&lt;br&gt; **STEPS** - &#xA;&lt;pre&gt;&lt;code&gt;  WINDOWS Users: &#xA;     setx OPENAI_API_KEY &#34;REPLACE_WITH_YOUR_KEY_VALUE_HERE&#34;&#xA;&#xA;  MACOS/LINUX Users: &#xA;     export OPENAI_API_KEY=&#34;REPLACE_WITH_YOUR_KEY_VALUE_HERE&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For &lt;em&gt;Completions&lt;/em&gt; scenario, one can start with using your model name (&#34;COMPLETIONS_MODEL&#34; in &lt;em&gt;config.json&lt;/em&gt; file) as &#34;text_davinci_003&#34;. &lt;br&gt; And for &lt;em&gt;Embeddings&lt;/em&gt; scenario, one can use &#34;text-embedding-ada-002&#34; as model name (&#34;EMBEDDINGS_MODEL&#34;in &lt;em&gt;config.json&lt;/em&gt; file).&lt;/li&gt; &#xA; &lt;li&gt;To find your &#34;OPENAI_API_BASE&#34; go to &lt;a href=&#34;https://portal.azure.com&#34;&gt;https://portal.azure.com&lt;/a&gt;, find your resource and then under &#34;Resource Management&#34; -&amp;gt; &#34;Keys and Endpoints&#34; look for the &#34;Endpoint&#34; value.&lt;/li&gt; &#xA; &lt;li&gt;Current OpenAI api version is &#34;2022-12-01&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Learn more about Azure OpenAI Service REST API &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Python 3.8+ &lt;br&gt; Jupyter Notebook 6.5.2&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
</feed>