<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-24T01:37:29Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>philschmid/deep-learning-pytorch-huggingface</title>
    <updated>2023-03-24T01:37:29Z</updated>
    <id>tag:github.com,2023-03-24:/philschmid/deep-learning-pytorch-huggingface</id>
    <link href="https://github.com/philschmid/deep-learning-pytorch-huggingface" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Getting Started with Deep Learning with PyTorch and Hugging Face&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains instructions/examples/tutorials for getting started with Deep Learning Pytorch and Hugging Face libraries like &lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;transformers&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/datasets/index&#34;&gt;datasets&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/philschmid/deep-learning-pytorch-huggingface/main/training/pytorch-2-0-bert-text-classification.ipynb&#34;&gt;Getting started with Pytorch 2.0 and Hugging Face Transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/philschmid/deep-learning-pytorch-huggingface/main/training/deepseed-flan-t5-summarization.ipynb&#34;&gt;Fine-tune FLAN-T5 XL/XXL using DeepSpeed &amp;amp; Hugging Face Transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/philschmid/deep-learning-pytorch-huggingface/main/training/flan-t5-samsum-summarization.ipynb&#34;&gt;Fine-tune FLAN-T5 for chat &amp;amp; dialogue summarization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/philschmid/deep-learning-pytorch-huggingface/main/training/accelerate-tpu-bert-text-classification.ipynb&#34;&gt;Getting started with Transformers and TPU using PyTorch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Before we can start make sure you have met the following requirements&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AWS Account with quota&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html&#34;&gt;AWS CLI&lt;/a&gt; installed&lt;/li&gt; &#xA; &lt;li&gt;AWS IAM user &lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html&#34;&gt;configured in CLI&lt;/a&gt; with permission to create and manage ec2 instances&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Commands&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &#39;export PATH=&#34;${HOME}/.local/bin:$PATH&#34;&#39; &amp;gt;&amp;gt; ${HOME}/.bashrc &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;watch -n0.1 nvidia-smi&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>lich99/ChatGLM-finetune-LoRA</title>
    <updated>2023-03-24T01:37:29Z</updated>
    <id>tag:github.com,2023-03-24:/lich99/ChatGLM-finetune-LoRA</id>
    <link href="https://github.com/lich99/ChatGLM-finetune-LoRA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for fintune ChatGLM-6b using low-rank adaptation (LoRA)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGLM-finetune-LoRA&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains code for fintune &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6b&lt;/a&gt; using &lt;a href=&#34;https://arxiv.org/pdf/2106.09685.pdf&#34;&gt;low-rank adaptation (LoRA)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a &lt;a href=&#34;https://github.com/lich99/ChatGLM-finetune-LoRA/raw/main/saved/chatglm-6b_alpaca_5.pt&#34;&gt;finetuned weight&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2022/3/24: Support Multi-GPU training, DeepSpeed, Batch collate. Using accelerate to launch &lt;code&gt;train.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Stanford Alpaca&#39;s Dataset&lt;/h3&gt; &#xA;&lt;p&gt;Here we use the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;&#39;s Dataset as an example for fine-tuning. We also provide a &lt;a href=&#34;https://github.com/lich99/ChatGLM-finetune-LoRA/raw/main/saved/chatglm-6b_alpaca_5.pt&#34;&gt;finetuned weight&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;example line:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;{&#39;prompt&#39;: &#39;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nClassify the movie genres from the given context.\n\n### Input:\nThis movie tells the story of two brothers who were both born with magical powers.\n\n### Response:&#39;, &#39;completion&#39;: &#39;Fantasy&#39;}&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;LoRA&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;config = LoraConfig(&#xA;              peft_type=&#34;LORA&#34;, &#xA;              task_type=&#34;SEQ_2_SEQ_LM&#34;, &#xA;              r=8, &#xA;              lora_alpha=8, &#xA;              target_modules=[&#34;q&#34;, &#34;v&#34;],&#xA;              lora_dropout=0.1, &#xA;              )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using above LoRA config, we have &lt;code&gt;trainable_params:3670016 (0.06%), non_trainable_params:6255206400&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Save &amp;amp; Load&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;torch.save(lora.lora_state_dict(model), &#39;path to file you saved&#39;)&#xA;model.load_state_dict(torch.load(&#39;path to file you saved&#39;), strict=False)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>camenduru/text-to-video-synthesis-colab</title>
    <updated>2023-03-24T01:37:29Z</updated>
    <id>tag:github.com,2023-03-24:/camenduru/text-to-video-synthesis-colab</id>
    <link href="https://github.com/camenduru/text-to-video-synthesis-colab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Text To Video Synthesis Colab&lt;/p&gt;&lt;hr&gt;&lt;p&gt;üê£ Please follow me for new updates &lt;a href=&#34;https://twitter.com/camenduru&#34;&gt;https://twitter.com/camenduru&lt;/a&gt; &lt;br&gt; üî• Please join our discord server &lt;a href=&#34;https://discord.gg/k5BwmmvJJU&#34;&gt;https://discord.gg/k5BwmmvJJU&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üö¶ WIP üö¶&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Colab&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/text-to-video-synthesis-colab/blob/main/text_to_video_synthesis.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Original&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/text-to-video-synthesis-colab/blob/main/text_to_video_synthesis_diffusers.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Diffusers&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Tutorial Video&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=b8D4am73e6I&#34;&gt;https://www.youtube.com/watch?v=b8D4am73e6I&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;maybe we can edit &lt;code&gt;max_frames&lt;/code&gt; in &lt;code&gt;/content/models/configuration.json&lt;/code&gt; for video duration&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{   &#34;framework&#34;: &#34;pytorch&#34;,&#xA;    &#34;task&#34;: &#34;text-to-video-synthesis&#34;,&#xA;    &#34;model&#34;: {&#xA;        &#34;type&#34;: &#34;latent-text-to-video-synthesis&#34;,&#xA;        &#34;model_args&#34;: {&#xA;            &#34;ckpt_clip&#34;: &#34;open_clip_pytorch_model.bin&#34;,&#xA;            &#34;ckpt_unet&#34;: &#34;text2video_pytorch_model.pth&#34;,&#xA;            &#34;ckpt_autoencoder&#34;: &#34;VQGAN_autoencoder.pth&#34;,&#xA;            &#34;max_frames&#34;: 16,&#xA;            &#34;tiny_gpu&#34;: 1&#xA;        },&#xA;        &#34;model_cfg&#34;: {&#xA;            &#34;unet_in_dim&#34;: 4,&#xA;            &#34;unet_dim&#34;: 320,&#xA;            &#34;unet_y_dim&#34;: 768,&#xA;            &#34;unet_context_dim&#34;: 1024,&#xA;            &#34;unet_out_dim&#34;: 4,&#xA;            &#34;unet_dim_mult&#34;: [1, 2, 4, 4],&#xA;            &#34;unet_num_heads&#34;: 8,&#xA;            &#34;unet_head_dim&#34;: 64,&#xA;            &#34;unet_res_blocks&#34;: 2,&#xA;            &#34;unet_attn_scales&#34;: [1, 0.5, 0.25],&#xA;            &#34;unet_dropout&#34;: 0.1,&#xA;            &#34;temporal_attention&#34;: &#34;True&#34;,&#xA;            &#34;num_timesteps&#34;: 1000,&#xA;            &#34;mean_type&#34;: &#34;eps&#34;,&#xA;            &#34;var_type&#34;: &#34;fixed_small&#34;,&#xA;            &#34;loss_type&#34;: &#34;mse&#34;&#xA;        }&#xA;    },&#xA;    &#34;pipeline&#34;: {&#xA;        &#34;type&#34;: &#34;latent-text-to-video-synthesis&#34;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Main Repo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.modelscope.cn/models/damo/text-to-video-synthesis/summary&#34;&gt;https://www.modelscope.cn/models/damo/text-to-video-synthesis/summary&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/modelscope/modelscope&#34;&gt;https://github.com/modelscope/modelscope&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;https://github.com/huggingface/diffusers&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Models License&lt;/h2&gt; &#xA;&lt;p&gt;Apache License 2.0&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;      A giraffe underneath a microwave. &#xA;     &lt;br&gt; &#xA;     &lt;img src=&#34;https://user-images.githubusercontent.com/54370274/226195676-edd1b5da-906c-445e-b6a5-612a4dbfb1fe.gif&#34; alt=&#34;A giraffe underneath a microwave.&#34; style=&#34;width: 300px;&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;      A goldendoodle playing in a park by a lake. &#xA;     &lt;br&gt; &#xA;     &lt;img src=&#34;https://user-images.githubusercontent.com/54370274/226195681-f54e38c2-1936-4153-b145-f238853a4df0.gif&#34; alt=&#34;A goldendoodle playing in a park by a lake.&#34; style=&#34;width: 300px;&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;      A panda bear driving a car. &#xA;     &lt;br&gt; &#xA;     &lt;img src=&#34;https://user-images.githubusercontent.com/54370274/226195685-e188e342-5c6d-4e68-ab3f-32e2d7d30e34.gif&#34; alt=&#34;A panda bear driving a car.&#34; style=&#34;width: 300px;&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;      A teddy bear running in New York City. &#xA;     &lt;br&gt; &#xA;     &lt;img src=&#34;https://user-images.githubusercontent.com/54370274/226195689-318e0e5e-ee14-4443-84a0-a3c8b07b8aed.gif&#34; alt=&#34;A teddy bear running in New York City.&#34; style=&#34;width: 300px;&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;      Drone flythrough of a fast food restaurant &#xA;     &lt;br&gt;on a dystopian alien planet. &#xA;     &lt;br&gt; &#xA;     &lt;img src=&#34;https://user-images.githubusercontent.com/54370274/226195692-0853b49a-9cd5-4f9b-84e6-2288632ca2f7.gif&#34; alt=&#34;Drone flythrough of a fast food restaurant on a dystopian alien planet.&#34; style=&#34;width: 300px;&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;      A dog wearing a Superhero outfit with red cape &#xA;     &lt;br&gt;flying through the sky. &#xA;     &lt;br&gt; &#xA;     &lt;img src=&#34;https://user-images.githubusercontent.com/54370274/226195699-14b16290-15e7-4577-aaae-ea16c15f44c3.gif&#34; alt=&#34;A dog wearing a Superhero outfit with red cape flying through the sky.&#34; style=&#34;width: 300px;&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;      Monkey learning to play the piano. &#xA;     &lt;br&gt; &#xA;     &lt;img src=&#34;https://user-images.githubusercontent.com/54370274/226195867-f6b079ff-ee1a-4dea-928c-dbf28d4a656e.gif&#34; alt=&#34;Monkey learning to play the piano.&#34; style=&#34;width: 300px;&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;      A litter of puppies running through the yard. &#xA;     &lt;br&gt; &#xA;     &lt;img src=&#34;https://user-images.githubusercontent.com/54370274/226195930-1fd957df-f403-4ae3-9a85-f4f954c82f5a.gif&#34; alt=&#34;A litter of puppies running through the yard.&#34; style=&#34;width: 300px;&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&#xA;    &lt;center&gt;&#xA;      Robot dancing in times square. &#xA;     &lt;br&gt; &#xA;     &lt;img src=&#34;https://user-images.githubusercontent.com/54370274/226209983-eae320fc-078e-4e62-9989-d97beb9477eb.gif&#34; alt=&#34;Robot dancing in times square.&#34; style=&#34;width: 300px;&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt;</summary>
  </entry>
</feed>