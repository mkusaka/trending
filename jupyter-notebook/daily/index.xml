<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-09T01:37:53Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>FeijiangHan/CTISPED</title>
    <updated>2023-11-09T01:37:53Z</updated>
    <id>tag:github.com,2023-11-09:/FeijiangHan/CTISPED</id>
    <link href="https://github.com/FeijiangHan/CTISPED" rel="alternate"></link>
    <summary type="html">&lt;p&gt;UNet series network architectures (UNet, R2UNet, Attention UNet, Nested UNet, Tiny UNet etc.), combined with joint training of YOLO and other networks&lt;/p&gt;&lt;hr&gt;&lt;div class=&#34;column&#34; align=&#34;middle&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;/p&gt;  &#xA; &lt;a href=&#34;https://github.com/matrixorigin/matrixone/raw/main/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-red.svg?sanitize=true&#34; alt=&#34;license&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://golang.org/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Language-Python-blue.svg?sanitize=true&#34; alt=&#34;language&#34;&gt; &lt;/a&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/platform-MacOS-white.svg?sanitize=true&#34; alt=&#34;macos&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/platform-Linux-9cf.svg?sanitize=true&#34; alt=&#34;linux&#34;&gt; &#xA; &lt;a href=&#34;https://www.codefactor.io/repository/github/matrixorigin/matrixone&#34;&gt; &lt;img src=&#34;https://www.codefactor.io/repository/github/matrixorigin/matrixone/badge?s=7280f4312fca2f2e6938fb8de5b726c5252541f0&#34; alt=&#34;codefactor&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://docs.matrixorigin.cn/en/0.7.0/MatrixOne/Release-Notes/v0.7.0/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Release-v0.4.0-green.svg?sanitize=true&#34; alt=&#34;release&#34;&gt; &lt;/a&gt; &#xA; &lt;h5 align=&#34;center&#34;&gt;If you are interested in This project, please kindly give Me a triple `Star`, `Fork` and `Watch`, Thanks!&lt;/h5&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;CTISPED: CT Image Segmentation for Pulmonary Embolism DiagnosisCTISPED&lt;/h1&gt; &#xA;&lt;p&gt;Pulmonary embolism is a lung condition most commonly caused by blood clots, also known as thromboembolism. By analyzing CT scans of the lungs, doctors can identify clots in the pulmonary vessels to facilitate timely treatment. The goal of this project is to assist physicians in detecting areas of pulmonary embolism using deep learning, reducing their workload and improving detection accuracy.&lt;/p&gt; &#xA;&lt;p&gt;Both 2D CT slices (.dcm) and 3D CT volumes (.nii) can be used to identify pulmonary embolism. Segmenting the nii volumes slice-by-slice generates the dcm images, so these two formats can be interconverted. Alternatively, detecting embolism directly from 3D vascular models extracts the key information without needing full volume segmentation.&lt;/p&gt; &#xA;&lt;p&gt;For image processing, convolutional neural networks were chosen for their efficiency and performance with medical images. For segmentation, UNet architectures are widely used, and various 2D UNet models have been implemented in the Unet Family folder. The 3D AANet model in the AAnet folder focuses on the pulmonary arteries. Additionally, YOLOv4 in the YOLO folder identifies lung region ROIs to reduce the segmentation search space.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;To address class imbalance, the following strategies were employed:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Dice Loss and Focal Loss as objective functions to improve model robustness.&lt;/li&gt; &#xA; &lt;li&gt;Data augmentation via strategic oversampling and synthesis based on target ratio analysis, such as concatenating low ratio samples and replicating targets. This alleviates distribution skew.&lt;/li&gt; &#xA; &lt;li&gt;Lung segmentation and cropping to reduce background and indirectly increase target ratios.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Testing showed these approaches improved small target detection.&lt;/p&gt; &#xA;&lt;p&gt;Model training incorporates an ensemble with Tiny-Attention-Residual-Unet and Attention UNet, expected to improve accuracy by 8-10%. Convergence is ensured through techniques like little sample, sliding averages, and cosine annealing.&lt;/p&gt; &#xA;&lt;p&gt;Finally, a segmentation-detection-fusion pipeline was explored by using YOLO for ROI extraction, UNet for segmentation, and merging the outputs. This cascade could synergistically boost performance.&lt;/p&gt; &#xA;&lt;p&gt;Overall, the techniques aim to showcase skills in medical deep learning, data augmentation, model integration, and pulmonary embolism diagnosis. Please let me know if you would like me to modify or expand any part of the description.&lt;/p&gt; &#xA;&lt;p&gt;For more information please see the Unets-2D REAMDE&lt;/p&gt; &#xA;&lt;h1&gt;UNETS-2D&lt;/h1&gt; &#xA;&lt;p&gt;This code implements training and evaluation of various semantic segmentation models on medical image datasets using PyTorch.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supports various semantic segmentation network architectures: UNet, R2UNet, Attention UNet, Nested UNet etc.&lt;/li&gt; &#xA; &lt;li&gt;Provides multiple loss functions like cross entropy loss, Dice loss and combined loss.&lt;/li&gt; &#xA; &lt;li&gt;Implements lung CT dataset for training and validation.&lt;/li&gt; &#xA; &lt;li&gt;Contains complete training and validation loop with metrics visualization.&lt;/li&gt; &#xA; &lt;li&gt;Logs training process using TensorBoard.&lt;/li&gt; &#xA; &lt;li&gt;Supports model checkpointing and resuming training.&lt;/li&gt; &#xA; &lt;li&gt;Calculates common evaluation metrics for semantic segmentation and visualizes them.&lt;/li&gt; &#xA; &lt;li&gt;Enables multi-GPU training using PyTorch DataParallel.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;python train.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Main arguments:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model&lt;/code&gt;: Model architecture, UNet by default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--loss&lt;/code&gt;: Loss function, Dice loss by default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--dataset&lt;/code&gt;: Dataset, lung CT by default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--epoch&lt;/code&gt;: Number of training epochs, 300 by default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--batch_size&lt;/code&gt;: Batch size, 2 by default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--lr&lt;/code&gt;: Learning rate, 1e-4 by default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--checkpoint&lt;/code&gt;: Path to pretrained model checkpoint&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--gpu&lt;/code&gt;: GPU device id&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--parallel&lt;/code&gt;: Enable multi-GPU training&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The code will automatically log TensorBoard events, save model checkpoints.&lt;/p&gt; &#xA;&lt;h2&gt;Code Structure&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;models/&lt;/code&gt;: Different model architectures&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;datasets/&lt;/code&gt;: Dataset loaders&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;utils/&lt;/code&gt;: Utility functions like metrics calculation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Main workflow:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Parse arguments, load data&lt;/li&gt; &#xA; &lt;li&gt;Build model, define optimizer and loss&lt;/li&gt; &#xA; &lt;li&gt;Training loop: forward pass, backprop, optimize&lt;/li&gt; &#xA; &lt;li&gt;Calculate metrics, log TensorBoard&lt;/li&gt; &#xA; &lt;li&gt;Save model checkpoints&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;AANET-3D&lt;/h1&gt; &#xA;&lt;h2&gt;Model Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The model architecture is AANet, which contains encoding and decoding blocks with skip connections. Batch normalization is synchronized across GPUs.&lt;/p&gt; &#xA;&lt;h2&gt;Training Pipeline&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Loads lung CT scans and segmentation masks as training data&lt;/li&gt; &#xA; &lt;li&gt;Applies data augmentation like random cropping, flipping, rotation&lt;/li&gt; &#xA; &lt;li&gt;Defines AANet model, Dice + Tversky loss, AdamW optimizer&lt;/li&gt; &#xA; &lt;li&gt;Training loop with forward/backward passes, optimization, learning rate scheduling&lt;/li&gt; &#xA; &lt;li&gt;Evaluates validation metrics like F1 score&lt;/li&gt; &#xA; &lt;li&gt;Logs training metrics, validation metrics using TensorBoard&lt;/li&gt; &#xA; &lt;li&gt;Saves model checkpoints during training&lt;/li&gt; &#xA; &lt;li&gt;Tests trained model on test set and generates lung segmentation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Train model:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python train.py --epochs 500 --batch_size 8&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Main arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--epochs&lt;/code&gt;: Number of training epochs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--batch_size&lt;/code&gt;: Batch size&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--learn_rate&lt;/code&gt;: Learning rate&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--log_path&lt;/code&gt;: TensorBoard log directory&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The code supports multi-GPU training via PyTorch DataParallel.&lt;/p&gt; &#xA;&lt;h2&gt;Output&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Trained model checkpoints saved to &lt;code&gt;save_models/&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;TensorBoard logs written to &lt;code&gt;log_path&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Segmentation results on test set stored as PNGs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;YOLO Object Detection Python Wrapper&lt;/h1&gt; &#xA;&lt;p&gt;This code provides a Python wrapper for performing object detection using YOLO models. It interfaces with the YOLO DLL to run detection on images.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Loads YOLO network config, weights and metadata&lt;/li&gt; &#xA; &lt;li&gt;Runs detection on images, returns bounding boxes and labels&lt;/li&gt; &#xA; &lt;li&gt;Supports batch detection on multiple images&lt;/li&gt; &#xA; &lt;li&gt;Displays detection results on images&lt;/li&gt; &#xA; &lt;li&gt;Handles CPU or GPU mode automatically&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Run detection on an image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from darknet_python import performDetect&#xA;&#xA;boxes = performDetect(imagePath=&#34;dog.jpg&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Perform batch detection on multiple images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from darknet_python import performBatchDetect &#xA;&#xA;batch_boxes = performBatchDetect(img_list=[&#39;img1.jpg&#39;, &#39;img2.jpg&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Main parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;imagePath&lt;/code&gt; - Path to input image&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;configPath&lt;/code&gt;, &lt;code&gt;weightPath&lt;/code&gt;, &lt;code&gt;metaPath&lt;/code&gt; - Paths to YOLO files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;thresh&lt;/code&gt; - Detection threshold&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;showImage&lt;/code&gt; - Show output image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Implementation&lt;/h2&gt; &#xA;&lt;p&gt;The Python wrapper calls into the YOLO DLL to run detection. Main steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Load network, metadata and image&lt;/li&gt; &#xA; &lt;li&gt;Pass image through network for predictions&lt;/li&gt; &#xA; &lt;li&gt;Parse predictions into boxes, scores and labels&lt;/li&gt; &#xA; &lt;li&gt;Apply NMS thresholding to boxes&lt;/li&gt; &#xA; &lt;li&gt;Draw boxes on image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;👏 All contributors&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/feijianghan&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/88610657?s=96&amp;amp;v=4&#34; width=&#34;30;&#34; alt=&#34;nnsgmsone&#34;&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Feijiang Han&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;  # License This project is licensed under the [Apache License, Version 2.0](LICENSE). &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt;</summary>
  </entry>
</feed>