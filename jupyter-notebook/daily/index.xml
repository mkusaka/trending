<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-21T01:37:16Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>facebookresearch/co-tracker</title>
    <updated>2023-07-21T01:37:16Z</updated>
    <id>tag:github.com,2023-07-21:/facebookresearch/co-tracker</id>
    <link href="https://github.com/facebookresearch/co-tracker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CoTracker is a model for tracking any point (pixel) on a video.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CoTracker: It is Better to Track Together&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ai.facebook.com/research/&#34;&gt;Meta AI Research, GenAI&lt;/a&gt;&lt;/strong&gt;; &lt;strong&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/&#34;&gt;University of Oxford, VGG&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nikitakaraevv.github.io/&#34;&gt;Nikita Karaev&lt;/a&gt;, &lt;a href=&#34;https://www.irocco.info/&#34;&gt;Ignacio Rocco&lt;/a&gt;, &lt;a href=&#34;https://ai.facebook.com/people/benjamin-graham/&#34;&gt;Benjamin Graham&lt;/a&gt;, &lt;a href=&#34;https://nneverova.github.io/&#34;&gt;Natalia Neverova&lt;/a&gt;, &lt;a href=&#34;https://www.robots.ox.ac.uk/~vedaldi/&#34;&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href=&#34;https://chrirupp.github.io/&#34;&gt;Christian Rupprecht&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2307.07635&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://co-tracker.github.io/&#34;&gt;&lt;code&gt;Project&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/#citing-cotracker&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/facebookresearch/co-tracker/blob/main/notebooks/demo.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/assets/bmx-bumps.gif&#34; alt=&#34;bmx-bumps&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoTracker&lt;/strong&gt; is a fast transformer-based model that can track any point in a video. It brings to tracking some of the benefits of Optical Flow.&lt;/p&gt; &#xA;&lt;p&gt;CoTracker can track:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Every pixel&lt;/strong&gt; in a video&lt;/li&gt; &#xA; &lt;li&gt;Points sampled on a regular grid on any video frame&lt;/li&gt; &#xA; &lt;li&gt;Manually selected points&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Try these tracking modes for yourself with our &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/co-tracker/blob/master/notebooks/demo.ipynb&#34;&gt;Colab demo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation Instructions&lt;/h2&gt; &#xA;&lt;p&gt;Ensure you have both PyTorch and TorchVision installed on your system. Follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; for the installation. We strongly recommend installing both PyTorch and TorchVision with CUDA support.&lt;/p&gt; &#xA;&lt;h2&gt;Steps to Install CoTracker and its dependencies:&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/facebookresearch/co-tracker&#xA;cd co-tracker&#xA;pip install -e .&#xA;pip install opencv-python einops timm matplotlib moviepy flow_vis&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download Model Weights:&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir checkpoints&#xA;cd checkpoints&#xA;wget https://dl.fbaipublicfiles.com/cotracker/cotracker_stride_4_wind_8.pth&#xA;wget https://dl.fbaipublicfiles.com/cotracker/cotracker_stride_4_wind_12.pth&#xA;wget https://dl.fbaipublicfiles.com/cotracker/cotracker_stride_8_wind_16.pth&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running the Demo:&lt;/h2&gt; &#xA;&lt;p&gt;Try our &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/co-tracker/blob/master/notebooks/demo.ipynb&#34;&gt;Colab demo&lt;/a&gt; or run a local demo with 10*10 points sampled on a grid on the first frame of a video:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo.py --grid_size 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To reproduce the results presented in the paper, download the following datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepmind/tapnet&#34;&gt;TAP-Vid&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/benjiebob/BADJA&#34;&gt;BADJA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.11898&#34;&gt;ZJU-Mocap (FastCapture)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And install the necessary dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install hydra-core==1.1.0 mediapy tensorboard &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, execute the following command to evaluate on BADJA:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python ./cotracker/evaluation/evaluate.py --config-name eval_badja exp_dir=./eval_outputs dataset_root=your/badja/path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To train the CoTracker as described in our paper, you first need to generate annotations for &lt;a href=&#34;https://github.com/google-research/kubric&#34;&gt;Google Kubric&lt;/a&gt; MOVI-f dataset. Instructions for annotation generation can be found &lt;a href=&#34;https://github.com/deepmind/tapnet&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once you have the annotated dataset, you need to make sure you followed the steps for evaluation setup and install the training dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install pytorch_lightning==1.6.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can launch training on Kubric. Our model was trained for 50000 iterations on 32 GPUs (4 nodes with 8 GPUs).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train.py --batch_size 1 --num_workers 28 \&#xA;--num_steps 50000 --ckpt_path ./ --model_name cotracker \&#xA;--save_freq 200 --sequence_len 24 --eval_datasets tapvid_davis_first badja \&#xA;--traj_per_sample 256 --sliding_window_len 8 --updateformer_space_depth 6 --updateformer_time_depth 6 \&#xA;--save_every_n_epoch 10 --evaluate_every_n_epoch 10 --model_stride 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The majority of CoTracker is licensed under CC-BY-NC, however portions of the project are available under separate license terms: Particle Video Revisited is licensed under the MIT license, TAP-Vid is licensed under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank &lt;a href=&#34;https://github.com/aharley/pips&#34;&gt;PIPs&lt;/a&gt; and &lt;a href=&#34;https://github.com/deepmind/tapnet&#34;&gt;TAP-Vid&lt;/a&gt; for publicly releasing their code and data. We also want to thank &lt;a href=&#34;https://lukemelas.github.io/&#34;&gt;Luke Melas-Kyriazi&lt;/a&gt; for proofreading the paper, &lt;a href=&#34;https://jytime.github.io/&#34;&gt;Jianyuan Wang&lt;/a&gt;, &lt;a href=&#34;https://shapovalov.ro/&#34;&gt;Roman Shapovalov&lt;/a&gt; and &lt;a href=&#34;https://adamharley.com/&#34;&gt;Adam W. Harley&lt;/a&gt; for the insightful discussions.&lt;/p&gt; &#xA;&lt;h2&gt;Citing CoTracker&lt;/h2&gt; &#xA;&lt;p&gt;If you find our repository useful, please consider giving it a star ‚≠ê and citing our paper in your work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{karaev2023cotracker,&#xA;  title={CoTracker: It is Better to Track Together},&#xA;  author={Nikita Karaev and Ignacio Rocco and Benjamin Graham and Natalia Neverova and Andrea Vedaldi and Christian Rupprecht},&#xA;  journal={arXiv:2307.07635},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>