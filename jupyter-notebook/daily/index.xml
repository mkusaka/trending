<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-31T01:33:40Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>GeostatsGuy/DataScience_Interactive_Python</title>
    <updated>2024-03-31T01:33:40Z</updated>
    <id>tag:github.com,2024-03-31:/GeostatsGuy/DataScience_Interactive_Python</id>
    <link href="https://github.com/GeostatsGuy/DataScience_Interactive_Python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python interactive dashboards for learning data science&lt;/p&gt;&lt;hr&gt;&lt;p&gt; &lt;img src=&#34;https://github.com/GeostatsGuy/GeostatsPy/raw/master/TCG_color_logo.png&#34; width=&#34;220&#34; height=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;DataScience_Interactive_Python&lt;/h1&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;Interactive dashboards to help you over the intellectual hurdles of data science!&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;To support my students in my &lt;strong&gt;Data Analytics and Geostatistics&lt;/strong&gt;, &lt;strong&gt;Spatial Data Analytics&lt;/strong&gt; and &lt;strong&gt;Machine Learning&lt;/strong&gt; courses and anyone else learning data analytics and machine learning, I have developed a set of Python interactive dashboards. When students struggle with a concept I make a new interactive dashboard so they can learn by playing with the statistics, models or theoretical concepts!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Michael Pyrcz, Professor, The University of Texas at Austin, Data Analytics, Geostatistics and Machine Learning&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://twitter.com/geostatsguy&#34;&gt;Twitter&lt;/a&gt; | &lt;a href=&#34;https://github.com/GeostatsGuy&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;http://michaelpyrcz.com&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;amp;hl=en&amp;amp;oi=ao&#34;&gt;GoogleScholar&lt;/a&gt; | &lt;a href=&#34;https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446&#34;&gt;Book&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig&#34;&gt;YouTube&lt;/a&gt; | &lt;a href=&#34;https://www.linkedin.com/in/michael-pyrcz-61a648a1&#34;&gt;LinkedIn&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/GeostatsGuy/DataScience_Interactive_Python/HEAD&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To further support my students, I&#39;m using &lt;a href=&#34;https://mybinder.readthedocs.io/en/latest/index.html&#34;&gt;Binder&lt;/a&gt; to host some of my &lt;strong&gt;interactive Python spatial data analytics, geostatistics and machine learning demonstration workflows&lt;/strong&gt; online. Some of my students are having issues with setting up their local computing environments and instantiating the interactive workflows.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;I hope this will assist these students and remove barriers for these educational tools to invite a wider audience that may benefit from experiential learning - playing with the systems and machines in real-time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Click on the link above to launch binder with container to run the included workflow.&lt;/p&gt; &#xA;&lt;p&gt;A minimum environment is set set up with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.7.10 - due to the depdendency of GeostatsPy on the Numba package for code acceleration&lt;/li&gt; &#xA; &lt;li&gt;MatPlotLib - plotting&lt;/li&gt; &#xA; &lt;li&gt;NumPy - gridded data and array math&lt;/li&gt; &#xA; &lt;li&gt;Pandas - tabulated data&lt;/li&gt; &#xA; &lt;li&gt;SciPy - statistics module&lt;/li&gt; &#xA; &lt;li&gt;ipywidgets - for plot interactivity&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/geostatspy/&#34;&gt;GeostatsPy&lt;/a&gt; - geostatistical algorithms and functions (Pyrcz et al., 2021)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The required datasets are available in the &lt;a href=&#34;https://github.com/GeostatsGuy/GeoDataSets&#34;&gt;GeoDataSets&lt;/a&gt; repository and linked in the workflows&lt;/p&gt; &#xA;&lt;p&gt;The interative Python examples include a variety of topics like:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bayesian and frequentist statistics&lt;/li&gt; &#xA; &lt;li&gt;univariate and bivariate statistics&lt;/li&gt; &#xA; &lt;li&gt;confidence intervals and hypothesis testing&lt;/li&gt; &#xA; &lt;li&gt;Monte Carlo methods and bootstrap&lt;/li&gt; &#xA; &lt;li&gt;inferential machine learning, principal component and cluster analysis&lt;/li&gt; &#xA; &lt;li&gt;predictive machine learning, norms, model parameter training and hyperparameter tuning, overfit models&lt;/li&gt; &#xA; &lt;li&gt;uncertainty modeling checking&lt;/li&gt; &#xA; &lt;li&gt;spatial data debiasing&lt;/li&gt; &#xA; &lt;li&gt;variogram calculation and modeling&lt;/li&gt; &#xA; &lt;li&gt;spatial estimation, issues and trend modeling&lt;/li&gt; &#xA; &lt;li&gt;spatial simulation and summarization over realizations&lt;/li&gt; &#xA; &lt;li&gt;decision making in the presence of uncertainty&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to see all my shared educational content check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/GeostatsGuy/Resources&#34;&gt;&lt;strong&gt;Resources Inventory&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GeostatsGuy/DataScience_Interactive_Python/main/www.youtube.com/GeostatsGuyLectures&#34;&gt;&lt;strong&gt;GeostatsGuy Lectures&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I hope this is helpful to anyone interested to learn about spatial data analytics, geostatistics and machine learning. I&#39;m all about remoing barriers to education and encouraging folks to learn coding and data-driven modeling!&lt;/p&gt; &#xA;&lt;p&gt;Sincerely,&lt;/p&gt; &#xA;&lt;p&gt;Michael&lt;/p&gt; &#xA;&lt;h4&gt;The Author:&lt;/h4&gt; &#xA;&lt;h3&gt;Michael Pyrcz, Professor, The University of Texas at Austin&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Novel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;With over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers&#39; and geoscientists&#39; impact in subsurface resource development.&lt;/p&gt; &#xA;&lt;p&gt;For more about Michael check out these links:&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://twitter.com/geostatsguy&#34;&gt;Twitter&lt;/a&gt; | &lt;a href=&#34;https://github.com/GeostatsGuy&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;http://michaelpyrcz.com&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;amp;hl=en&amp;amp;oi=ao&#34;&gt;GoogleScholar&lt;/a&gt; | &lt;a href=&#34;https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446&#34;&gt;Book&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig&#34;&gt;YouTube&lt;/a&gt; | &lt;a href=&#34;https://www.linkedin.com/in/michael-pyrcz-61a648a1&#34;&gt;LinkedIn&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;Want to Work Together?&lt;/h4&gt; &#xA;&lt;p&gt;I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I&#39;d be happy to drop by and work with you!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;I can be reached at &lt;a href=&#34;mailto:mpyrcz@austin.utexas.edu&#34;&gt;mpyrcz@austin.utexas.edu&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I&#39;m always happy to discuss,&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Michael&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin&lt;/p&gt; &#xA;&lt;h4&gt;More Resources Available at: &lt;a href=&#34;https://twitter.com/geostatsguy&#34;&gt;Twitter&lt;/a&gt; | &lt;a href=&#34;https://github.com/GeostatsGuy&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;http://michaelpyrcz.com&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;amp;hl=en&amp;amp;oi=ao&#34;&gt;GoogleScholar&lt;/a&gt; | &lt;a href=&#34;https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446&#34;&gt;Book&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig&#34;&gt;YouTube&lt;/a&gt; | &lt;a href=&#34;https://www.linkedin.com/in/michael-pyrcz-61a648a1&#34;&gt;LinkedIn&lt;/a&gt;&lt;/h4&gt;</summary>
  </entry>
  <entry>
    <title>jasonppy/VoiceCraft</title>
    <updated>2024-03-31T01:33:40Z</updated>
    <id>tag:github.com,2024-03-31:/jasonppy/VoiceCraft</id>
    <link href="https://github.com/jasonppy/VoiceCraft" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Zero-Shot Speech Editing and Text-to-Speech in the Wild&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://jasonppy.github.io/VoiceCraft_web&#34;&gt;Demo&lt;/a&gt; &lt;a href=&#34;https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;TL;DR&lt;/h3&gt; &#xA;&lt;p&gt;VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both &lt;strong&gt;speech editing&lt;/strong&gt; and &lt;strong&gt;zero-shot text-to-speech (TTS)&lt;/strong&gt; on in-the-wild data including audiobooks, internet videos, and podcasts.&lt;/p&gt; &#xA;&lt;p&gt;To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; 03/28/2024: Model weights are up on HuggingFaceü§ó &lt;a href=&#34;https://huggingface.co/pyp1/VoiceCraft/tree/main&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Codebase upload&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Environment setup&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference demo for speech editing and TTS&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Training guidance&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; RealEdit dataset and training manifest&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Model weights (both 330M and 830M, the former seems to be just as good)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Write colab notebooks for better hands-on experience&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; HuggingFace Spaces demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Better guidance on training&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to run TTS inference&lt;/h2&gt; &#xA;&lt;p&gt;There are two ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;with docker. see &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/#quickstart&#34;&gt;quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;without docker. see &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/#environment-setup&#34;&gt;envrionment setup&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;When you are inside the docker image or you have installed all dependencies, Checkout &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/inference_tts.ipynb&#34;&gt;&lt;code&gt;inference_tts.ipynb&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;QuickStart&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; To try out TTS inference with VoiceCraft, the best way is using docker. Thank &lt;a href=&#34;https://github.com/ubergarm&#34;&gt;@ubergarm&lt;/a&gt; and &lt;a href=&#34;https://github.com/jay-c88&#34;&gt;@jayc88&lt;/a&gt; for making this happen.&lt;/p&gt; &#xA;&lt;p&gt;Tested on Linux and Windows and should work with any host with docker installed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1. clone the repo on in a directory on a drive with plenty of free space&#xA;git clone git@github.com:jasonppy/VoiceCraft.git&#xA;cd VoiceCraft&#xA;&#xA;# 2. assumes you have docker installed with nvidia container container-toolkit (windows has this built into the driver)&#xA;# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/1.13.5/install-guide.html&#xA;# sudo apt-get install -y nvidia-container-toolkit-base || yay -Syu nvidia-container-toolkit || echo etc...&#xA;&#xA;# 3. Try to start an existing container otherwise create a new one passing in all GPUs&#xA;./start-jupyter.sh  # linux&#xA;start-jupyter.bat   # windows&#xA;&#xA;# 4. now open a webpage on the host box to the URL shown at the bottom of:&#xA;docker logs jupyter&#xA;&#xA;# 5. optionally look inside from another terminal&#xA;docker exec -it jupyter /bin/bash&#xA;export USER=(your_linux_username_used_above)&#xA;export HOME=/home/$USER&#xA;sudo apt-get update&#xA;&#xA;# 6. confirm video card(s) are visible inside container&#xA;nvidia-smi&#xA;&#xA;# 7. Now in browser, open inference_tts.ipynb and work through one cell at a time&#xA;echo GOOD LUCK&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Environment setup&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n voicecraft python=3.9.16&#xA;conda activate voicecraft&#xA;&#xA;pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201&#xA;apt-get install ffmpeg # if you don&#39;t already have ffmpeg installed&#xA;pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft&#xA;apt-get install espeak-ng # backend for the phonemizer installed below&#xA;pip install tensorboard==2.16.2&#xA;pip install phonemizer==3.2.1&#xA;pip install torchaudio==2.0.2&#xA;pip install datasets==2.16.0&#xA;pip install torchmetrics==0.11.1&#xA;# install MFA for getting forced-alignment, this could take a few minutes&#xA;conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068&#xA;# conda install pocl # above gives an warning for installing pocl, not sure if really need this&#xA;&#xA;# to run ipynb&#xA;conda install -n voicecraft ipykernel --update-deps --force-reinstall&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have encountered version issues when running things, checkout &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/environment.yml&#34;&gt;environment.yml&lt;/a&gt; for exact matching.&lt;/p&gt; &#xA;&lt;h2&gt;Inference Examples&lt;/h2&gt; &#xA;&lt;p&gt;Checkout &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/inference_speech_editing.ipynb&#34;&gt;&lt;code&gt;inference_speech_editing.ipynb&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/inference_tts.ipynb&#34;&gt;&lt;code&gt;inference_tts.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To train an VoiceCraft model, you need to prepare the following parts:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;utterances and their transcripts&lt;/li&gt; &#xA; &lt;li&gt;encode the utterances into codes using e.g. Encodec&lt;/li&gt; &#xA; &lt;li&gt;convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)&lt;/li&gt; &#xA; &lt;li&gt;manifest (i.e. metadata)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Step 1,2,3 are handled in &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/data/phonemize_encodec_encode_hf.py&#34;&gt;./data/phonemize_encodec_encode_hf.py&lt;/a&gt;, where&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)&lt;/li&gt; &#xA; &lt;li&gt;phoneme sequence and encodec codes are also extracted using the script.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;An example run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate voicecraft&#xA;export CUDA_VISIBLE_DEVICES=0&#xA;cd ./data&#xA;python phonemize_encodec_encode_hf.py \&#xA;--dataset_size xs \&#xA;--download_to path/to/store_huggingface_downloads \&#xA;--save_dir path/to/store_extracted_codes_and_phonemes \&#xA;--encodec_model_path path/to/encodec_model \&#xA;--mega_batch_size 120 \&#xA;--batch_size 32 \&#xA;--max_len 30000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where encodec_model_path is avaliable &lt;a href=&#34;https://huggingface.co/pyp1/VoiceCraft&#34;&gt;here&lt;/a&gt;. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our &lt;a href=&#34;https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf&#34;&gt;paper&lt;/a&gt;. If you encounter OOM during extraction, try decrease the batch_size and/or max_len. The extracted codes, phonemes, and vocab.txt will be stored at &lt;code&gt;path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;As for manifest, please download train.txt and validation.txt from &lt;a href=&#34;https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main&#34;&gt;here&lt;/a&gt;, and put them under &lt;code&gt;path/to/store_extracted_codes_and_phonemes/manifest/&lt;/code&gt;. Please also download vocab.txt from &lt;a href=&#34;https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main&#34;&gt;here&lt;/a&gt; if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same).&lt;/p&gt; &#xA;&lt;p&gt;Now, you are good to start training!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate voicecraft&#xA;cd ./z_scripts&#xA;bash e830M.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The codebase is under CC BY-NC-SA 4.0 (&lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/LICENSE-CODE&#34;&gt;LICENSE-CODE&lt;/a&gt;), and the model weights are under Coqui Public Model License 1.0.0 (&lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/LICENSE-MODEL&#34;&gt;LICENSE-MODEL&lt;/a&gt;). Note that we use some of the code from other repository that are under different licenses: &lt;code&gt;./models/codebooks_patterns.py&lt;/code&gt; is under MIT license; &lt;code&gt;./models/modules&lt;/code&gt;, &lt;code&gt;./steps/optim.py&lt;/code&gt;, &lt;code&gt;data/tokenizer.py&lt;/code&gt; are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try &lt;a href=&#34;https://github.com/roedoejet/g2p&#34;&gt;g2p&lt;/a&gt; (MIT License) or &lt;a href=&#34;https://github.com/NeuralVox/OpenPhonemizer&#34;&gt;OpenPhonemizer&lt;/a&gt; (BSD-3-Clause Clear), although these are not tested.&lt;/p&gt; &#xA;&lt;!-- How to use g2p to convert english text into IPA phoneme sequence&#xA;first install it with `pip install g2p`&#xA;```python&#xA;from g2p import make_g2p&#xA;transducer = make_g2p(&#39;eng&#39;, &#39;eng-ipa&#39;)&#xA;transducer(&#34;hello&#34;).output_string &#xA;# it will output: &#39;h ålo ä&#39;&#xA;``` --&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We thank Feiteng for his &lt;a href=&#34;https://github.com/lifeiteng/vall-e&#34;&gt;VALL-E reproduction&lt;/a&gt;, and we thank audiocraft team for open-sourcing &lt;a href=&#34;https://github.com/facebookresearch/audiocraft&#34;&gt;encodec&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{peng2024voicecraft,&#xA;  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},&#xA;  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},&#xA;  journal   = {arXiv},&#xA;  year      = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone&#39;s speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>aswintechguy/Deep-Learning-Projects</title>
    <updated>2024-03-31T01:33:40Z</updated>
    <id>tag:github.com,2024-03-31:/aswintechguy/Deep-Learning-Projects</id>
    <link href="https://github.com/aswintechguy/Deep-Learning-Projects" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains all the deep learning projects done as tutorial&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deep Learning Projects&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains mini projects in deep learning with jupyter notebook files. Go to the projects folder and see the readme for detailed instructions about the projects.&lt;/p&gt; &#xA;&lt;h1&gt;Complete video tutorial for the projects:-&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://bit.ly/mlprojectsplaylist&#34;&gt;http://bit.ly/mlprojectsplaylist&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>