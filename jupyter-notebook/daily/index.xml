<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-25T01:33:56Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>afizs/python-notes</title>
    <updated>2022-11-25T01:33:56Z</updated>
    <id>tag:github.com,2022-11-25:/afizs/python-notes</id>
    <link href="https://github.com/afizs/python-notes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;📒 Python Notes&lt;/h1&gt; &#xA;&lt;h3&gt;This repo contains all the Python projects, tips and notes I share on twitter and YouTube.&lt;/h3&gt; &#xA;&lt;h3&gt;⭐️ Star this repo, if you don&#39;t want to miss any update.&lt;/h3&gt;</summary>
  </entry>
  <entry>
    <title>SKTBrain/KoBERT</title>
    <updated>2022-11-25T01:33:56Z</updated>
    <id>tag:github.com,2022-11-25:/SKTBrain/KoBERT</id>
    <link href="https://github.com/SKTBrain/KoBERT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Korean BERT pre-trained cased (KoBERT)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;KoBERT&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#kobert&#34;&gt;KoBERT&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#korean-bert-pre-trained-cased-kobert&#34;&gt;Korean BERT pre-trained cased (KoBERT)&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#why&#34;&gt;Why&#39;?&#39;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#training-environment&#34;&gt;Training Environment&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#how-to-install&#34;&gt;How to install&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#how-to-use&#34;&gt;How to use&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#using-with-pytorch&#34;&gt;Using with PyTorch&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#using-with-onnx&#34;&gt;Using with ONNX&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#using-with-mxnet-gluon&#34;&gt;Using with MXNet-Gluon&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#tokenizer&#34;&gt;Tokenizer&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#subtasks&#34;&gt;Subtasks&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#naver-sentiment-analysis&#34;&gt;Naver Sentiment Analysis&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#kobert%EC%99%80-crf%EB%A1%9C-%EB%A7%8C%EB%93%A0-%ED%95%9C%EA%B5%AD%EC%96%B4-%EA%B0%9D%EC%B2%B4%EB%AA%85%EC%9D%B8%EC%8B%9D%EA%B8%B0&#34;&gt;KoBERT와 CRF로 만든 한국어 객체명인식기&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#korean-sentence-bert&#34;&gt;Korean Sentence BERT&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#release&#34;&gt;Release&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#contacts&#34;&gt;Contacts&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Korean BERT pre-trained cased (KoBERT)&lt;/h2&gt; &#xA;&lt;h3&gt;Why&#39;?&#39;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;구글 &lt;a href=&#34;https://github.com/google-research/bert/raw/master/multilingual.md&#34;&gt;BERT base multilingual cased&lt;/a&gt;의 한국어 성능 한계&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training Environment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Architecture&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predefined_args = {&#xA;        &#39;attention_cell&#39;: &#39;multi_head&#39;,&#xA;        &#39;num_layers&#39;: 12,&#xA;        &#39;units&#39;: 768,&#xA;        &#39;hidden_size&#39;: 3072,&#xA;        &#39;max_length&#39;: 512,&#xA;        &#39;num_heads&#39;: 12,&#xA;        &#39;scaled&#39;: True,&#xA;        &#39;dropout&#39;: 0.1,&#xA;        &#39;use_residual&#39;: True,&#xA;        &#39;embed_size&#39;: 768,&#xA;        &#39;embed_dropout&#39;: 0.1,&#xA;        &#39;token_type_vocab_size&#39;: 2,&#xA;        &#39;word_embed&#39;: None,&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;학습셋&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;데이터&lt;/th&gt; &#xA;   &lt;th&gt;문장&lt;/th&gt; &#xA;   &lt;th&gt;단어&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;한국어 위키&lt;/td&gt; &#xA;   &lt;td&gt;5M&lt;/td&gt; &#xA;   &lt;td&gt;54M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;학습 환경 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;V100 GPU x 32, Horovod(with InfiniBand)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/imgs/2019-04-29_TensorBoard.png&#34; alt=&#34;2019-04-29 텐서보드 로그&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;사전(Vocabulary) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;크기 : 8,002&lt;/li&gt; &#xA;   &lt;li&gt;한글 위키 기반으로 학습한 토크나이저(SentencePiece)&lt;/li&gt; &#xA;   &lt;li&gt;Less number of parameters(92M &amp;lt; 110M )&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;see &lt;a href=&#34;https://github.com/SKTBrain/KoBERT/raw/master/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How to install&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Install KoBERT as a python package&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install git+https://git@github.com/SKTBrain/KoBERT.git@master&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you want to modify source codes, please clone this repository&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/SKTBrain/KoBERT.git&#xA;cd KoBERT&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;h3&gt;Using with PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Huggingface transformers API가 편하신 분은 &lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/kobert_hf&#34;&gt;여기&lt;/a&gt;를 참고하세요.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; from kobert import get_pytorch_kobert_model&#xA;&amp;gt;&amp;gt;&amp;gt; input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])&#xA;&amp;gt;&amp;gt;&amp;gt; input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])&#xA;&amp;gt;&amp;gt;&amp;gt; token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])&#xA;&amp;gt;&amp;gt;&amp;gt; model, vocab  = get_pytorch_kobert_model()&#xA;&amp;gt;&amp;gt;&amp;gt; sequence_output, pooled_output = model(input_ids, input_mask, token_type_ids)&#xA;&amp;gt;&amp;gt;&amp;gt; pooled_output.shape&#xA;torch.Size([2, 768])&#xA;&amp;gt;&amp;gt;&amp;gt; vocab&#xA;Vocab(size=8002, unk=&#34;[UNK]&#34;, reserved=&#34;[&#39;[MASK]&#39;, &#39;[SEP]&#39;, &#39;[CLS]&#39;]&#34;)&#xA;&amp;gt;&amp;gt;&amp;gt; # Last Encoding Layer&#xA;&amp;gt;&amp;gt;&amp;gt; sequence_output[0]&#xA;tensor([[-0.2461,  0.2428,  0.2590,  ..., -0.4861, -0.0731,  0.0756],&#xA;        [-0.2478,  0.2420,  0.2552,  ..., -0.4877, -0.0727,  0.0754],&#xA;        [-0.2472,  0.2420,  0.2561,  ..., -0.4874, -0.0733,  0.0765]],&#xA;       grad_fn=&amp;lt;SelectBackward&amp;gt;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;model&lt;/code&gt;은 디폴트로 &lt;code&gt;eval()&lt;/code&gt;모드로 리턴됨, 따라서 학습 용도로 사용시 &lt;code&gt;model.train()&lt;/code&gt;명령을 통해 학습 모드로 변경할 필요가 있다.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Naver Sentiment Analysis Fine-Tuning with pytorch &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Colab에서 [런타임] - [런타임 유형 변경] - 하드웨어 가속기(GPU) 사용을 권장합니다.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using with ONNX&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import onnxruntime&#xA;&amp;gt;&amp;gt;&amp;gt; import numpy as np&#xA;&amp;gt;&amp;gt;&amp;gt; from kobert import get_onnx_kobert_model&#xA;&amp;gt;&amp;gt;&amp;gt; onnx_path = get_onnx_kobert_model()&#xA;&amp;gt;&amp;gt;&amp;gt; sess = onnxruntime.InferenceSession(onnx_path)&#xA;&amp;gt;&amp;gt;&amp;gt; input_ids = [[31, 51, 99], [15, 5, 0]]&#xA;&amp;gt;&amp;gt;&amp;gt; input_mask = [[1, 1, 1], [1, 1, 0]]&#xA;&amp;gt;&amp;gt;&amp;gt; token_type_ids = [[0, 0, 1], [0, 1, 0]]&#xA;&amp;gt;&amp;gt;&amp;gt; len_seq = len(input_ids[0])&#xA;&amp;gt;&amp;gt;&amp;gt; pred_onnx = sess.run(None, {&#39;input_ids&#39;:np.array(input_ids),&#xA;&amp;gt;&amp;gt;&amp;gt;                             &#39;token_type_ids&#39;:np.array(token_type_ids),&#xA;&amp;gt;&amp;gt;&amp;gt;                             &#39;input_mask&#39;:np.array(input_mask),&#xA;&amp;gt;&amp;gt;&amp;gt;                             &#39;position_ids&#39;:np.array(range(len_seq))})&#xA;&amp;gt;&amp;gt;&amp;gt; # Last Encoding Layer&#xA;&amp;gt;&amp;gt;&amp;gt; pred_onnx[-2][0]&#xA;array([[-0.24610452,  0.24282141,  0.25895312, ..., -0.48613444,&#xA;        -0.07305173,  0.07560554],&#xA;       [-0.24783179,  0.24200465,  0.25520486, ..., -0.4877185 ,&#xA;        -0.0727044 ,  0.07536091],&#xA;       [-0.24721591,  0.24196623,  0.2560626 , ..., -0.48743123,&#xA;        -0.07326943,  0.07650235]], dtype=float32)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;ONNX 컨버팅은 &lt;a href=&#34;https://github.com/soeque1&#34;&gt;soeque1&lt;/a&gt;께서 도움을 주셨습니다.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Using with MXNet-Gluon&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import mxnet as mx&#xA;&amp;gt;&amp;gt;&amp;gt; from kobert import get_mxnet_kobert_model&#xA;&amp;gt;&amp;gt;&amp;gt; input_id = mx.nd.array([[31, 51, 99], [15, 5, 0]])&#xA;&amp;gt;&amp;gt;&amp;gt; input_mask = mx.nd.array([[1, 1, 1], [1, 1, 0]])&#xA;&amp;gt;&amp;gt;&amp;gt; token_type_ids = mx.nd.array([[0, 0, 1], [0, 1, 0]])&#xA;&amp;gt;&amp;gt;&amp;gt; model, vocab = get_mxnet_kobert_model(use_decoder=False, use_classifier=False)&#xA;&amp;gt;&amp;gt;&amp;gt; encoder_layer, pooled_output = model(input_id, token_type_ids)&#xA;&amp;gt;&amp;gt;&amp;gt; pooled_output.shape&#xA;(2, 768)&#xA;&amp;gt;&amp;gt;&amp;gt; vocab&#xA;Vocab(size=8002, unk=&#34;[UNK]&#34;, reserved=&#34;[&#39;[MASK]&#39;, &#39;[SEP]&#39;, &#39;[CLS]&#39;]&#34;)&#xA;&amp;gt;&amp;gt;&amp;gt; # Last Encoding Layer&#xA;&amp;gt;&amp;gt;&amp;gt; encoder_layer[0]&#xA;[[-0.24610372  0.24282135  0.2589539  ... -0.48613444 -0.07305248&#xA;   0.07560539]&#xA; [-0.24783105  0.242005    0.25520545 ... -0.48771808 -0.07270523&#xA;   0.07536077]&#xA; [-0.24721491  0.241966    0.25606337 ... -0.48743105 -0.07327032&#xA;   0.07650219]]&#xA;&amp;lt;NDArray 3x768 @cpu(0)&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Naver Sentiment Analysis Fine-Tuning with MXNet &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_gluon_kobert.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Tokenizer&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pretrained &lt;a href=&#34;https://github.com/google/sentencepiece&#34;&gt;Sentencepiece&lt;/a&gt; tokenizer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from gluonnlp.data import SentencepieceTokenizer&#xA;&amp;gt;&amp;gt;&amp;gt; from kobert import get_tokenizer&#xA;&amp;gt;&amp;gt;&amp;gt; tok_path = get_tokenizer()&#xA;&amp;gt;&amp;gt;&amp;gt; sp  = SentencepieceTokenizer(tok_path)&#xA;&amp;gt;&amp;gt;&amp;gt; sp(&#39;한국어 모델을 공유합니다.&#39;)&#xA;[&#39;▁한국&#39;, &#39;어&#39;, &#39;▁모델&#39;, &#39;을&#39;, &#39;▁공유&#39;, &#39;합니다&#39;, &#39;.&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Subtasks&lt;/h2&gt; &#xA;&lt;h3&gt;Naver Sentiment Analysis&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dataset : &lt;a href=&#34;https://github.com/e9t/nsmc&#34;&gt;https://github.com/e9t/nsmc&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Accuracy&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/bert/raw/master/multilingual.md&#34;&gt;BERT base multilingual cased&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.875&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KoBERT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/logs/bert_naver_small_512_news_simple_20190624.txt&#34;&gt;0.901&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SKT-AI/KoGPT2&#34;&gt;KoGPT2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.899&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;KoBERT와 CRF로 만든 한국어 객체명인식기&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/eagle705/pytorch-bert-crf-ner&#34;&gt;https://github.com/eagle705/pytorch-bert-crf-ner&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;문장을 입력하세요:  SKTBrain에서 KoBERT 모델을 공개해준 덕분에 BERT-CRF 기반 객체명인식기를 쉽게 개발할 수 있었다.&#xA;len: 40, input_token:[&#39;[CLS]&#39;, &#39;▁SK&#39;, &#39;T&#39;, &#39;B&#39;, &#39;ra&#39;, &#39;in&#39;, &#39;에서&#39;, &#39;▁K&#39;, &#39;o&#39;, &#39;B&#39;, &#39;ER&#39;, &#39;T&#39;, &#39;▁모델&#39;, &#39;을&#39;, &#39;▁공개&#39;, &#39;해&#39;, &#39;준&#39;, &#39;▁덕분에&#39;, &#39;▁B&#39;, &#39;ER&#39;, &#39;T&#39;, &#39;-&#39;, &#39;C&#39;, &#39;R&#39;, &#39;F&#39;, &#39;▁기반&#39;, &#39;▁&#39;, &#39;객&#39;, &#39;체&#39;, &#39;명&#39;, &#39;인&#39;, &#39;식&#39;, &#39;기를&#39;, &#39;▁쉽게&#39;, &#39;▁개발&#39;, &#39;할&#39;, &#39;▁수&#39;, &#39;▁있었다&#39;, &#39;.&#39;, &#39;[SEP]&#39;]&#xA;len: 40, pred_ner_tag:[&#39;[CLS]&#39;, &#39;B-ORG&#39;, &#39;I-ORG&#39;, &#39;I-ORG&#39;, &#39;I-ORG&#39;, &#39;I-ORG&#39;, &#39;O&#39;, &#39;B-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;[SEP]&#39;]&#xA;decoding_ner_sentence: [CLS] &amp;lt;SKTBrain:ORG&amp;gt;에서 &amp;lt;KoBERT:POH&amp;gt; 모델을 공개해준 덕분에 &amp;lt;BERT-CRF:POH&amp;gt; 기반 객체명인식기를 쉽게 개발할 수 있었다.[SEP]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Korean Sentence BERT&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BM-K/KoSentenceBERT-SKT&#34;&gt;https://github.com/BM-K/KoSentenceBERT-SKT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cosine Pearson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cosine Spearman&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Euclidean Pearson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Euclidean Spearman&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Manhattan Pearson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Manhattan Spearman&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dot Pearson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dot Spearman&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NLl&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.48&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.81&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.90&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.22&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.81&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;STS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;80.42&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;79.64&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;77.93&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.43&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;77.92&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.44&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;76.56&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;75.83&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;STS + NLI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.81&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.47&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.68&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;77.78&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.71&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;77.83&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.75&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.22&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;v0.2.3 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;support &lt;code&gt;onnx 1.8.0&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;v0.2.2 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fix &lt;code&gt;No module named &#39;kobert.utils&#39;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;v0.2.1 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;guide default &#39;import statements&#39;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;v0.2 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;download large files from &lt;code&gt;aws s3&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;rename functions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;v0.1.2 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Guaranteed compatibility with higher versions of transformers&lt;/li&gt; &#xA;   &lt;li&gt;fix pad token index id&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;v0.1.1 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;사전(vocabulary)과 토크나이저 통합&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;v0.1 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;초기 모델 릴리즈&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contacts&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;KoBERT&lt;/code&gt; 관련 이슈는 &lt;a href=&#34;https://github.com/SKTBrain/KoBERT/issues&#34;&gt;이곳&lt;/a&gt;에 등록해 주시기 바랍니다.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;KoBERT&lt;/code&gt;는 &lt;code&gt;Apache-2.0&lt;/code&gt; 라이선스 하에 공개되어 있습니다. 모델 및 코드를 사용할 경우 라이선스 내용을 준수해주세요. 라이선스 전문은 &lt;code&gt;LICENSE&lt;/code&gt; 파일에서 확인하실 수 있습니다.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>matheusfacure/python-causality-handbook</title>
    <updated>2022-11-25T01:33:56Z</updated>
    <id>tag:github.com,2022-11-25:/matheusfacure/python-causality-handbook</id>
    <link href="https://github.com/matheusfacure/python-causality-handbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Causal Inference for the Brave and True. A light-hearted yet rigorous approach to learning about impact estimation and causality.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Causal Inference for The Brave and True&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/matheusfacure/python-causality-handbook/master/causal-inference-for-the-brave-and-true/data/img/brave-and-true.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zenodo.org/badge/latestdoi/255903310&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/255903310.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A light-hearted yet rigorous approach to learning impact estimation and sensitivity analysis. Everything in Python and with as many memes as I could find.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/landing-page.html&#34;&gt;Check out the book here!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to read the book in Chinese, @xieliaing was very kind to make a translation:&lt;br&gt; &lt;a href=&#34;https://github.com/xieliaing/CausalInferenceIntro&#34;&gt;因果推断：从概念到实践&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to read the book in Spanish, @donelianc was very kind to make a translation:&lt;br&gt; &lt;a href=&#34;https://github.com/donelianc/introduccion-inferencia-causal&#34;&gt;Inferencia Causal para los Valientes y Verdaderos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to read it in Korean, @jsshin2019 has put up a team to make the that translation possible:&lt;br&gt; &lt;a href=&#34;https://github.com/TeamCausality/Causal-Inference-with-Python&#34;&gt;Python으로 하는 인과추론 : 개념부터 실습까지&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Also, some really kind folks (@vietecon, @dinhtrang24 and @anhpham52) also translated this content into Vietnamese:&lt;br&gt; &lt;a href=&#34;https://github.com/vietecon/NhanQuaPython&#34;&gt;Nhân quả Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;I like to think of this entire series as a tribute to Joshua Angrist, Alberto Abadie and Christopher Walters for their amazing Econometrics class. Most of the ideas here are taken from their classes at the American Economic Association. Watching them is what is keeping me sane during this tough year of 2020.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/cont-ed/2017-webcasts&#34;&gt;Cross-Section Econometrics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/cont-ed/2020-webcasts&#34;&gt;Mastering Mostly Harmless Econometrics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I&#39;ll also like to reference the amazing books from Angrist. They have shown me that Econometrics, or &#39;Metrics as they call it, is not only extremely useful but also profoundly fun.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34;&gt;Mostly Harmless Econometrics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.masteringmetrics.com/&#34;&gt;Mastering &#39;Metrics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;My final reference is Miguel Hernan and Jamie Robins&#39; book. It has been my trustworthy companion in the most thorny causal questions I had to answer.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/&#34;&gt;Causal Inference Book&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Support This Work&lt;/h2&gt; &#xA;&lt;p&gt;Causal Inference for the Brave and True is an open-source material on mostly econometrics and the statistics of science. It uses only free software, based in Python. Its goal is to be accessible, not only financially, but intellectual. I&#39;ve tried my best to keep the writing entertaining while maintaining the necessary scientific rigor.&lt;br&gt; Recently, the book has been translated into Vietnamese by some very nice folks from the London School of Economics. Although I was thrilled by it, the translation process also revealed the insufiencies of my english. For this reason, I&#39;m looking for funds to hire professional proofreading services and sort that problem once and for all. To help me with that, go to &lt;a href=&#34;https://www.patreon.com/causal_inference_for_the_brave_and_true&#34;&gt;https://www.patreon.com/causal_inference_for_the_brave_and_true&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>