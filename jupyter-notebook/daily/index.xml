<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-01T01:39:18Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>elmoallistair/google-it-automation</title>
    <updated>2023-03-01T01:39:18Z</updated>
    <id>tag:github.com,2023-03-01:/elmoallistair/google-it-automation</id>
    <link href="https://github.com/elmoallistair/google-it-automation" rel="alternate"></link>
    <summary type="html">&lt;p&gt;google it automation with python professional certificate&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;&lt;a href=&#34;https://www.coursera.org/professional-certificates/google-it-automation&#34;&gt;Google IT Automation with Python Professional Certificate&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/elmoallistair/google-it-automation/master/google-it-automation.jpg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This repository written for reference and self-documentation purpose&lt;/li&gt; &#xA; &lt;li&gt;Feel free to contribute!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;About this Professional Certificate&lt;/h3&gt; &#xA;&lt;p&gt;This new beginner-level, six-course certificate, developed by Google, is designed to provide IT professionals with in-demand skills -- including Python, Git, and IT automation -- that can help you advance your career.&lt;/p&gt; &#xA;&lt;p&gt;Knowing how to write code to solve problems and automate solutions is a crucial skill for anybody in IT. Python, in particular, is now the &lt;a href=&#34;https://insights.dice.com/2019/10/08/python-java-top-languages-employers/&#34;&gt;most in-demand programming language by employers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This program builds on your IT foundations to help you take your career to the next level. It’s designed to teach you how to program with Python and how to use Python to automate common system administration tasks. You&#39;ll also learn to use Git and GitHub, troubleshoot and debug complex problems, and apply automation at scale by using configuration management and the Cloud.&lt;/p&gt; &#xA;&lt;p&gt;This certificate can be completed in about 6 months and is designed to prepare you for a variety of roles in IT, like more advanced IT Support Specialist or Junior Systems Administrator positions. Upon completing the program, you’ll have the option to share your information with potential employers, like Walmart, Sprint, Hulu, Bank of America, Google (of course!), and more.&lt;/p&gt; &#xA;&lt;p&gt;We recommend that you have Python installed on your machine. For some courses, you’ll need a computer where you can install Git or ask your administrator to install it for you.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>luxonis/depthai-experiments</title>
    <updated>2023-03-01T01:39:18Z</updated>
    <id>tag:github.com,2023-03-01:/luxonis/depthai-experiments</id>
    <link href="https://github.com/luxonis/depthai-experiments" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Experimental projects we&#39;ve done with DepthAI.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/luxonis/depthai-experiments/master/README.zh-CN.md&#34;&gt;depthai_experiments中文文档&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;depthai-experiments&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/luxonis&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/790680891252932659?label=Discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discuss.luxonis.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Forum-discuss-orange&#34; alt=&#34;Forum&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.luxonis.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Docs-DepthAI-yellow&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Projects we&#39;ve done with DepthAI. These can be anything from &#34;here&#39;s some code and it works most of the time&#34; to &#34;this is almost a tutorial&#34;.&lt;/p&gt; &#xA;&lt;p&gt;The following list isn&#39;t exhaustive (as we randomly add experiments and we may forget to update this list):&lt;/p&gt; &#xA;&lt;h2&gt;Gaze Estimation (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-gaze-estimation#gen2-gaze-estimation&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/5244214/106155520-0f483d00-6181-11eb-8b95-a2cb73cc4bac.mp4&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5244214/106155937-4fa7bb00-6181-11eb-8c23-21abe12f7fe4.gif&#34; alt=&#34;Gaze Example Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Age and Gender Recognition (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-age-gender#gen2-age--gender-recognition&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=PwnVrPaF-vs&#34; title=&#34;Age/Gender recognition on DepthAI&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5244214/106005496-954a8200-60b4-11eb-923e-b84df9de9fff.gif&#34; alt=&#34;Gen2 Age &amp;amp; Gender recognition&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Automated Face-Blurring (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-blur-faces#gen2-blur-faces-in-real-time&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/139135932-b907f037-9336-4c42-a479-5715d9693c9c.gif&#34; alt=&#34;Blur Face&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Spatial Calculation - On Host to Show/Explain Math That Happens in OAK-D for the Spatial Location Calculator (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-calc-spatials-on-host#calculate-spatial-coordinates-on-the-host&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/146296930-9e7071f5-33b9-45f9-af21-cace7ffffc0f.gif&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Multi-camera spatial-detection-fusion (&lt;a href=&#34;https://raw.githubusercontent.com/luxonis/depthai-experiments/master/gen2-multiple-devices/spatial-detection-fusion&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/luxonis/depthai-experiments/raw/master/gen2-multiple-devices/spatial-detection-fusion/img/demo.gif?raw=true&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Stereo Depth from Camera and From Host (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-camera-demo#gen2-camera-demo&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/32992551/99454609-e59eaa00-28e3-11eb-8858-e82fd8e6eaac.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Automatic JPEG Encoding and Saving Based on AI Results (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-class-saver-jpeg#gen2-class-saver-jpeg&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5244214/107018096-47163c80-67a0-11eb-88f6-c67fb3c2f421.jpg&#34; alt=&#34;raw_frame example&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;overlay_frame&lt;/code&gt; represents a path to RGB frame with detection overlays (bounding box and label)&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5244214/107018179-63b27480-67a0-11eb-8423-4fd311a6d860.jpg&#34; alt=&#34;raw_frame example&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;cropped_frame&lt;/code&gt; represents a path to cropped RGB frame containing only ROI of the detected object&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5244214/107018256-7dec5280-67a0-11eb-964e-2cc08b6b75fd.jpg&#34; alt=&#34;raw_frame example&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;An example entries in &lt;code&gt;dataset.csv&lt;/code&gt; are shown below&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;timestamp,label,left,top,right,bottom,raw_frame,overlay_frame,cropped_frame&#xA;16125187249289,bottle,0,126,79,300,data/raw/16125187249289.jpg,data/bottle/16125187249289_overlay.jpg,data/bottle/16125187249289_cropped.jpg&#xA;16125187249289,person,71,37,300,297,data/raw/16125187249289.jpg,data/person/16125187249289_overlay.jpg,data/person/16125187249289_cropped.jpg&#xA;16125187249653,bottle,0,126,79,300,data/raw/16125187249653.jpg,data/bottle/16125187249653_overlay.jpg,data/bottle/16125187249653_cropped.jpg&#xA;16125187249653,person,71,36,300,297,data/raw/16125187249653.jpg,data/person/16125187249653_overlay.jpg,data/person/16125187249653_cropped.jpg&#xA;16125187249992,bottle,0,126,80,300,data/raw/16125187249992.jpg,data/bottle/16125187249992_overlay.jpg,data/bottle/16125187249992_cropped.jpg&#xA;16125187249992,person,71,37,300,297,data/raw/16125187249992.jpg,data/person/16125187249992_overlay.jpg,data/person/16125187249992_cropped.jpg&#xA;16125187250374,person,37,38,300,299,data/raw/16125187250374.jpg,data/person/16125187250374_overlay.jpg,data/person/16125187250374_cropped.jpg&#xA;16125187250769,bottle,0,126,79,300,data/raw/16125187250769.jpg,data/bottle/16125187250769_overlay.jpg,data/bottle/16125187250769_cropped.jpg&#xA;16125187250769,person,71,36,299,297,data/raw/16125187250769.jpg,data/person/16125187250769_overlay.jpg,data/person/16125187250769_cropped.jpg&#xA;16125187251120,bottle,0,126,80,300,data/raw/16125187251120.jpg,data/bottle/16125187251120_overlay.jpg,data/bottle/16125187251120_cropped.jpg&#xA;16125187251120,person,77,37,300,298,data/raw/16125187251120.jpg,data/person/16125187251120_overlay.jpg,data/person/16125187251120_cropped.jpg&#xA;16125187251492,bottle,0,126,79,300,data/raw/16125187251492.jpg,data/bottle/16125187251492_overlay.jpg,data/bottle/16125187251492_cropped.jpg&#xA;16125187251492,person,74,38,300,297,data/raw/16125187251492.jpg,data/person/16125187251492_overlay.jpg,data/person/16125187251492_cropped.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Face Mask Detection (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-coronamask&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/c4KEFG2eR3M&#34; title=&#34;COVID-19 mask detection&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5244214/112673778-6a3a9f80-8e65-11eb-9b7b-e352beffe67a.gif&#34; alt=&#34;COVID-19 mask-no-mask megaAI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Crowd Counting (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-crowdcounting#gen2-crowd-counting-with-density-maps-on-depthai&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/luxonis/depthai-experiments/master/gen2-crowdcounting/imgs/example.gif&#34; alt=&#34;Image example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Cumulative Object Counting (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-cumulative-object-counting#cumulative-object-counting&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TannerGilbert/Tensorflow-2-Object-Counting/master/doc/cumulative_object_counting.PNG&#34; alt=&#34;cumulative object counting&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to Run Customer CV Models On-Device (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-custom-models#demos&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Concatenate frames&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/134209980-09c6e2f9-8a26-45d5-a6ad-c31d9e2816e1.png&#34; alt=&#34;Concat frames&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blur frames&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://docs.luxonis.com/en/latest/_images/blur.jpeg&#34; alt=&#34;Blur frames&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Corner detection&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/134209951-4e1c7343-a333-4fb6-bdc9-bc86f6dc36b2.jpeg&#34; alt=&#34;Laplacian corner detection&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Semantic Segmentation of Depth (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-deeplabv3_depth#gen2-deeplabv3-on-depthai---depth-cropping&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/59799831/132396685-c494f21b-8101-4be4-a787-dd382ae6b470.gif&#34; alt=&#34;Deeplabv3 Depth GIF&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Multi-Class Semantic Segmentation (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-deeplabv3_multiclass#gen2-deeplabv3-multiclass-on-depthai&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/luxonis/depthai-experiments/master/gen2-deeplabv3_multiclass/imgs/example.gif&#34; alt=&#34;Multi-class Semantic Segmentation&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Depth-Driven Focus (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-depth-driven-focus#depth-driven-focus&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/144228694-68344fce-8932-4c23-b2f0-601be59184b6.gif&#34; alt=&#34;Depth driven focus&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Monocular Depth Estimation - Neural Network Based (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-depth-mbnv2&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/140496170-6e3ad321-7314-40cb-8cc0-f622464aa4bd.gif&#34; alt=&#34;Image example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tutorial on How To Display High-Res Object Detections (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-display-detections&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/141347853-00a1c5ac-d473-4cf9-a9f5-bdf6271e8ebe.png&#34; alt=&#34;edit_bb&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running EfficientDet Object Detector On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-efficientDet&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=UHXWj9TNGrM&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/117892266-4c5bb980-b2b0-11eb-9c0c-68f5da6c2759.gif&#34; alt=&#34;Watch the demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running EfficientNet Image Classifier On-Camera (&lt;a href=&#34;https://raw.githubusercontent.com/luxonis/depthai-experiments/master/%5Burl%5D(https://github.com/luxonis/depthai-experiments/tree/master/gen2-efficientnet-classification#efficientnet-b0)&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/67831664/119170640-2b9a1d80-ba81-11eb-8a3f-a3837af38a73.jpg&#34; alt=&#34;result&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Facial Expression (Emotion) Recognition On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-emotion-recognition#gen2-emotion-recognition&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/140508779-f9b1465a-8bc1-48e0-8747-80cdb7f2e4fc.png&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Face Detection On-Camera (libfacedetection) (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-face-detection#gen2-face-detection-on-depthai&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/luxonis/depthai-experiments/raw/master/gen2-face-detection/imgs/example.gif?raw=true&#34; alt=&#34;libfacedetection&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Face Recognition On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-face-recognition#face-recognition&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=HNAeBwNCRek&#34; title=&#34;Face recognition&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/134054837-eed40899-7c1d-4160-aaf0-1d7c405bb7f4.gif&#34; alt=&#34;Face recognition&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Facial Landmarks On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-facemesh&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/luxonis/depthai-experiments/raw/master/gen2-facemesh/imgs/example.gif?raw=true&#34; alt=&#34;Facial Landmarks&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Fire Detection On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-fire-detection#fire-detection&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/luxonis/depthai-experiments/raw/master/gen2-fire-detection/images/fire_demo.gif?raw=true&#34; alt=&#34;Fire Detection&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Head Posture Detection On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-head-posture-detection#gen2-head-posture-detection&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/172148301-45adb7ce-3aab-478f-8cad-0c05f349ce50.gif&#34; alt=&#34;Head Pose Detection&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Human-Machine Safety Example On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-human-machine-safety#gen2-human-machine-safety&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=BcjZLaCYGi4&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/121198687-a1202f00-c872-11eb-949a-df9f1167494f.gif&#34; alt=&#34;Watch the demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Human Skeletal Pose Estimation (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-human-pose#gen2-pose-estimation-example&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Py3-dHQymko&#34; title=&#34;Human pose estimation on DepthAI&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5244214/107493701-35f97100-6b8e-11eb-8b13-02a7a8dbec21.gif&#34; alt=&#34;Gen2 Age &amp;amp; Gender recognition&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;LaneNet Lane Segmentation On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-lanenet&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/luxonis/depthai-experiments/raw/master/gen2-lanenet/imgs/example.gif?raw=true&#34; alt=&#34;LaneNet Lane Segmentation&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License Plate Recognition On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-license-plate-recognition#gen2-license-plates-recognition&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=tB_-mVVNIro&#34; title=&#34;License Plates recognition on DepthAI&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5244214/111202991-c62f3980-85c4-11eb-8bce-a3c517abeca1.gif&#34; alt=&#34;Gen2 License Plates recognition&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Lossless Zooming (4K to 1080p Zoom/Crop) On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-lossless-zooming#lossless-zooming&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/8X0IcnkeIf8&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/144095838-d082040a-9716-4f8e-90e5-15bcb23115f9.gif&#34; alt=&#34;Lossless Zooming&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running Mask-RCNN On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-maskrcnn-resnet50#mask-r-cnn-on-depthai&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/56075061/145182204-af540962-f233-480c-82a0-56b2587e5072.gif&#34; alt=&#34;Example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;MegaDepth Neural Depth Running On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-mega-depth#gen2-megadepth-on-depthai&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/raw/master/gen2-mega-depth/imgs/example.gif?raw=true&#34;&gt;MegaDepth&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;MJPEG Streaming From On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-mjpeg-streaming#mjpeg-streaming-server&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=695o0EO1Daw&#34; title=&#34;DepthAI on Mac&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5244214/90745571-92ab5b80-e2d0-11ea-9052-3f0c8afa9b0b.gif&#34; alt=&#34;MJPEG Streaming DepthAI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Class Agnostic Object Detector Running On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-mobile-object-localizer#gen2-mobile-object-localizer-on-depthai&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/140496684-e886fc00-612d-44dd-a6fe-c0d47988246f.gif&#34; alt=&#34;Image example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to Use Multiple Cameras Simultaneously (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-multiple-devices#gen2-multiple-devices-per-host&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=N1IY2CfhmEc&#34; title=&#34;Multiple devices per host&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/113307040-01d83c00-9305-11eb-9a42-c69c72a5dba5.gif&#34; alt=&#34;Multiple devices per host&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to Sync NN Data with Image Data for Custom Neural Networks (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-nn-sync#gen2-nn-frame-sync&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5244214/104956823-36f31480-59cd-11eb-9568-64c0f0003dd0.gif&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Optical Character Recognition in the Wild On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-ocr#how-to-run&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Bv-p76A3YMk&#34; title=&#34;Gen2 OCR Pipeline&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/32992551/105749743-13febe00-5f01-11eb-8b5f-dca801f5d125.png&#34; alt=&#34;Text Detection + OCR on DepthAI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Palm Detection On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-palm-detection#palm-detection&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/luxonis/depthai-experiments/raw/master/gen2-palm-detection/images/palm_detection.gif?raw=true&#34; alt=&#34;Palm Detection&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pedestrian Re-Identification (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-pedestrian-reidentification#pedestrian-reidentification&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QlXGtMWVV18&#34; title=&#34;Person Re-ID on DepthAI&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/32992551/108567421-71e6b180-72c5-11eb-8af0-c6e5c3382874.png&#34; alt=&#34;Pedestrian Re-Identification&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;People Counting On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-people-counter#gen2-people-counting&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=_cAP-yHhUN4&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/119807472-11c26580-bedb-11eb-907a-196b8bb92f28.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;People Direction-Tracker and Counter (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-people-tracker#gen2-people-tracker&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/145656510-94e12444-7524-47f9-a036-7ed8ee78fd7a.gif&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Playing an On-Camera Encoded Stream on the Host (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-play-encoded-stream#gen2-play-encoded-stream&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/59799831/132475640-6e9f8b7f-52f4-4f75-af81-86c7f6e45b94.gif&#34; alt=&#34;Encoding demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Recording and Playing Back Depth in RealSense -Compatible Format (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-record-replay#record-and-replay&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18037362/141661982-f206ed61-b505-4b17-8673-211a4029754b.gif&#34; alt=&#34;depth gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Road Segmentation On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-road-segmentation#gen2-road-segmentation-on-depthai&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5244214/130064359-b9534b08-0783-4c86-979b-08cbcaff9341.gif&#34; alt=&#34;Road Segmentation on DepthAI&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Roboflow Integration (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-roboflow-integration#oak--roboflow-demo&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/26127866/147658296-23be4621-d37a-4fd6-a169-3ea414ffa636.mp4&#34;&gt;https://user-images.githubusercontent.com/26127866/147658296-23be4621-d37a-4fd6-a169-3ea414ffa636.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Social Distancing Example (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-social-distancing#gen2-social-distancing&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-Ut9TemGZ8I&#34; title=&#34;DepthAI Social Distancing Proof of Concept&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5244214/90741333-73f89500-e2cf-11ea-919b-b1f47dc55c4a.gif&#34; alt=&#34;COVID-19 Social Distancing with DepthAI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Text Blurring On-Device (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-text-blur#gen2-text-blurring-on-depthai&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/luxonis/depthai-experiments/raw/master/gen2-text-blur/imgs/example.gif?raw=true&#34; alt=&#34;Text Blurring&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Image Classification On-Device (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-tf-image-classification#gen2-tensorflow-image-classification-example&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5244214/109003919-522a0180-76a8-11eb-948c-a74432c22be1.gif&#34; alt=&#34;Pedestrian Re-Identification&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Facial Key-point Triangulation On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-triangulation#gen2-triangulation---stereo-neural-inference-demo&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/59799831/132098832-70a2d0b9-1a30-4994-8dad-dc880a803fb3.gif&#34; alt=&#34;Stereo Inference GIF&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;WebRTC Streaming Example (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-webrtc-streaming#gen2-webrtc-streaming-example&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/8aeqGgO8LjY&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5244214/121884542-58a1bf00-cd13-11eb-851d-dc45d541e385.gif&#34; alt=&#34;Gen2 WebRTC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;YOLO V3 V4 V5 X and P On-Camera (&lt;a href=&#34;https://github.com/luxonis/depthai-experiments/tree/master/gen2-yolo&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/56075061/144863247-fa819d1d-28d6-498a-89a8-c3f94d9e9357.gif&#34; alt=&#34;yolo-logo&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>advimman/lama</title>
    <updated>2023-03-01T01:39:18Z</updated>
    <id>tag:github.com,2023-03-01:/advimman/lama</id>
    <link href="https://github.com/advimman/lama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🦙 LaMa Image Inpainting, Resolution-robust Large Mask Inpainting with Fourier Convolutions, WACV 2022&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🦙 LaMa: Resolution-robust Large Mask Inpainting with Fourier Convolutions&lt;/h1&gt; &#xA;&lt;p&gt;by Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, Victor Lempitsky.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; &#34;font-size:30px;&#34;&gt; 🔥🔥🔥 &lt;br&gt; &lt;b&gt; LaMa generalizes surprisingly well to much higher resolutions (~2k❗️) than it saw during training (256x256), and achieves the excellent performance even in challenging scenarios, e.g. completion of periodic structures.&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://advimman.github.io/lama-project/&#34;&gt;Project page&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2109.07161&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://ashukha.com/projects/lama_21/lama_supmat_2021.pdf&#34;&gt;Supplementary&lt;/a&gt;] [&lt;a href=&#34;https://senya-ashukha.github.io/projects/lama_21/paper.txt&#34;&gt;BibTeX&lt;/a&gt;] [&lt;a href=&#34;https://www.casualganpapers.com/large-masks-fourier-convolutions-inpainting/LaMa-explained.html&#34;&gt;Casual GAN Papers Summary&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://colab.research.google.com/github/advimman/lama/blob/master//colab/LaMa_inpainting.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;br&gt; Try out in Google Colab &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/projects/lama_21/ezgif-4-0db51df695a8.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/projects/lama_21/gif_for_lightning_v1_white.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;LaMa development&lt;/h1&gt; &#xA;&lt;p&gt;(Feel free to share your paper by creating an issue)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amazing results &lt;a href=&#34;https://arxiv.org/abs/2206.13644&#34;&gt;paper&lt;/a&gt; / &lt;a href=&#34;https://www.youtube.com/watch?v=gEukhOheWgE&#34;&gt;video&lt;/a&gt; / code &lt;a href=&#34;https://github.com/advimman/lama/pull/112&#34;&gt;https://github.com/advimman/lama/pull/112&lt;/a&gt; / by Geomagical Labs (&lt;a href=&#34;https://raw.githubusercontent.com/advimman/lama/main/geomagical.com&#34;&gt;geomagical.com&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/images/FeatureRefinement.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Non-official 3rd party apps:&lt;/h1&gt; &#xA;&lt;p&gt;(Feel free to share your app/implementation/demo by creating an issue)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cleanup.pictures/&#34;&gt;https://cleanup.pictures&lt;/a&gt; - a simple interactive object removal tool by &lt;a href=&#34;https://twitter.com/cyrildiagne&#34;&gt;@cyrildiagne&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/lama-cleaner&#34;&gt;lama-cleaner&lt;/a&gt; by &lt;a href=&#34;https://github.com/Sanster/lama-cleaner&#34;&gt;@Sanster&lt;/a&gt; is a self-host version of &lt;a href=&#34;https://cleanup.pictures/&#34;&gt;https://cleanup.pictures&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See demo: &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/lama&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; by &lt;a href=&#34;https://github.com/AK391&#34;&gt;@AK391&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Telegram bot &lt;a href=&#34;https://t.me/MagicEraserBot&#34;&gt;@MagicEraserBot&lt;/a&gt; by &lt;a href=&#34;https://github.com/Moldoteck&#34;&gt;@Moldoteck&lt;/a&gt;, &lt;a href=&#34;https://github.com/Moldoteck/MagicEraser&#34;&gt;code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andy971022/auto-lama&#34;&gt;Auto-LaMa&lt;/a&gt; = DE:TR object detection + LaMa inpainting by &lt;a href=&#34;https://github.com/andy971022&#34;&gt;@andy971022&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhaoyun0071/LAMA-Magic-Eraser-Local&#34;&gt;LAMA-Magic-Eraser-Local&lt;/a&gt; = a standalone inpainting application built with PyQt5 by &lt;a href=&#34;https://github.com/zhaoyun0071&#34;&gt;@zhaoyun0071&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.hama.app/&#34;&gt;Hama&lt;/a&gt; - object removal with a smart brush which simplifies mask drawing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.modelscope.cn/models/damo/cv_fft_inpainting_lama/summary&#34;&gt;ModelScope&lt;/a&gt; = the largest Model Community in Chinese by &lt;a href=&#34;https://github.com/chenbinghui1&#34;&gt;@chenbinghui1&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/qwopqwop200/lama-with-maskdino&#34;&gt;LaMa with MaskDINO&lt;/a&gt; = MaskDINO object detection + LaMa inpainting with refinement by &lt;a href=&#34;https://github.com/qwopqwop200&#34;&gt;@qwopqwop200&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Environment setup&lt;/h1&gt; &#xA;&lt;p&gt;Clone the repo: &lt;code&gt;git clone https://github.com/advimman/lama.git&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are three options of an environment:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Python virtualenv:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;virtualenv inpenv --python=/usr/bin/python3&#xA;source inpenv/bin/activate&#xA;pip install torch==1.8.0 torchvision==0.9.0&#xA;&#xA;cd lama&#xA;pip install -r requirements.txt &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Conda&lt;/p&gt; &lt;pre&gt;&lt;code&gt;% Install conda for Linux, for other OS download miniconda at https://docs.conda.io/en/latest/miniconda.html&#xA;wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh&#xA;bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda&#xA;$HOME/miniconda/bin/conda init bash&#xA;&#xA;cd lama&#xA;conda env create -f conda_env.yml&#xA;conda activate lama&#xA;conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch -y&#xA;pip install pytorch-lightning==1.2.9&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Docker: No actions are needed 🎉.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Inference &lt;a name=&#34;prediction&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Download pre-trained models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install tool for yandex disk link extraction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install wldhx.yadisk-direct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The best model (Places2, Places Challenge):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -L $(yadisk-direct https://disk.yandex.ru/d/ouP6l8VJ0HpMZg) -o big-lama.zip&#xA;unzip big-lama.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All models (Places &amp;amp; CelebA-HQ):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -L $(yadisk-direct https://disk.yandex.ru/d/EgqaSnLohjuzAg) -o lama-models.zip&#xA;unzip lama-models.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Prepare images and masks&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download test images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -L $(yadisk-direct https://disk.yandex.ru/d/xKQJZeVRk5vLlQ) -o LaMa_test_images.zip&#xA;unzip LaMa_test_images.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;OR prepare your data:&lt;/summary&gt; 1) Create masks named as `[images_name]_maskXXX[image_suffix]`, put images and masks in the same folder. &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;You can use the &lt;a href=&#34;https://github.com/advimman/lama/raw/main/bin/gen_mask_dataset.py&#34;&gt;script&lt;/a&gt; for random masks generation.&lt;/li&gt; &#xA;  &lt;li&gt;Check the format of the files: &lt;pre&gt;&lt;code&gt;image1_mask001.png&#xA;image1.png&#xA;image2_mask001.png&#xA;image2.png&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Specify &lt;code&gt;image_suffix&lt;/code&gt;, e.g. &lt;code&gt;.png&lt;/code&gt; or &lt;code&gt;.jpg&lt;/code&gt; or &lt;code&gt;_input.jpg&lt;/code&gt; in &lt;code&gt;configs/prediction/default.yaml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Predict&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/LaMa_test_images outdir=$(pwd)/output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;OR&lt;/strong&gt; in the docker&lt;/p&gt; &#xA;&lt;p&gt;The following command will pull the docker image from Docker Hub and execute the prediction script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash docker/2_predict.sh $(pwd)/big-lama $(pwd)/LaMa_test_images $(pwd)/output device=cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker cuda: TODO&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;4. Predict with Refinement&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 bin/predict.py refine=True model.path=$(pwd)/big-lama indir=$(pwd)/LaMa_test_images outdir=$(pwd)/output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Train and Eval&lt;/h1&gt; &#xA;&lt;p&gt;Make sure you run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then download models for &lt;em&gt;perceptual loss&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p ade20k/ade20k-resnet50dilated-ppm_deepsup/&#xA;wget -P ade20k/ade20k-resnet50dilated-ppm_deepsup/ http://sceneparsing.csail.mit.edu/model/pytorch/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Places&lt;/h2&gt; &#xA;&lt;p&gt;⚠️ NB: FID/SSIM/LPIPS metric values for Places that we see in LaMa paper are computed on 30000 images that we produce in evaluation section below. For more details on evaluation data check [&lt;a href=&#34;https://ashukha.com/projects/lama_21/lama_supmat_2021.pdf#subsection.3.1&#34;&gt;Section 3. Dataset splits in Supplementary&lt;/a&gt;] ⚠️&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Download data from http://places2.csail.mit.edu/download.html&#xA;# Places365-Standard: Train(105GB)/Test(19GB)/Val(2.1GB) from High-resolution images section&#xA;wget http://data.csail.mit.edu/places/places365/train_large_places365standard.tar&#xA;wget http://data.csail.mit.edu/places/places365/val_large.tar&#xA;wget http://data.csail.mit.edu/places/places365/test_large.tar&#xA;&#xA;# Unpack train/test/val data and create .yaml config for it&#xA;bash fetch_data/places_standard_train_prepare.sh&#xA;bash fetch_data/places_standard_test_val_prepare.sh&#xA;&#xA;# Sample images for test and viz at the end of epoch&#xA;bash fetch_data/places_standard_test_val_sample.sh&#xA;bash fetch_data/places_standard_test_val_gen_masks.sh&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier location=places_standard&#xA;&#xA;# To evaluate trained model and report metrics as in our paper&#xA;# we need to sample previously unseen 30k images and generate masks for them&#xA;bash fetch_data/places_standard_evaluation_prepare_data.sh&#xA;&#xA;# Infer model on thick/thin/medium masks in 256 and 512 and run evaluation &#xA;# like this:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier_/ \&#xA;indir=$(pwd)/places_standard_dataset/evaluation/random_thick_512/ \&#xA;outdir=$(pwd)/inference/random_thick_512 model.checkpoint=last.ckpt&#xA;&#xA;python3 bin/evaluate_predicts.py \&#xA;$(pwd)/configs/eval2_gpu.yaml \&#xA;$(pwd)/places_standard_dataset/evaluation/random_thick_512/ \&#xA;$(pwd)/inference/random_thick_512 \&#xA;$(pwd)/inference/random_thick_512_metrics.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;CelebA&lt;/h2&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Make shure you are in lama folder&#xA;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&#xA;# Download CelebA-HQ dataset&#xA;# Download data256x256.zip from https://drive.google.com/drive/folders/11Vz0fqHS2rXDb5pprgTjpD7S2BAJhi1P&#xA;&#xA;# unzip &amp;amp; split into train/test/visualization &amp;amp; create config for it&#xA;bash fetch_data/celebahq_dataset_prepare.sh&#xA;&#xA;# generate masks for test and visual_test at the end of epoch&#xA;bash fetch_data/celebahq_gen_masks.sh&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier-celeba data.batch_size=10&#xA;&#xA;# Infer model on thick/thin/medium masks in 256 and run evaluation &#xA;# like this:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier-celeba_/ \&#xA;indir=$(pwd)/celeba-hq-dataset/visual_test_256/random_thick_256/ \&#xA;outdir=$(pwd)/inference/celeba_random_thick_256 model.checkpoint=last.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;Places Challenge&lt;/h2&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# This script downloads multiple .tar files in parallel and unpacks them&#xA;# Places365-Challenge: Train(476GB) from High-resolution images (to train Big-Lama) &#xA;bash places_challenge_train_download.sh&#xA;&#xA;TODO: prepare&#xA;TODO: train &#xA;TODO: eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;Create your data&lt;/h2&gt; &#xA;&lt;p&gt;Please check bash scripts for data preparation and mask generation from CelebaHQ section, if you stuck at one of the following steps.&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Make shure you are in lama folder&#xA;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&#xA;# You need to prepare following image folders:&#xA;$ ls my_dataset&#xA;train&#xA;val_source # 2000 or more images&#xA;visual_test_source # 100 or more images&#xA;eval_source # 2000 or more images&#xA;&#xA;# LaMa generates random masks for the train data on the flight,&#xA;# but needs fixed masks for test and visual_test for consistency of evaluation.&#xA;&#xA;# Suppose, we want to evaluate and pick best models &#xA;# on 512x512 val dataset  with thick/thin/medium masks &#xA;# And your images have .jpg extention:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \ # thick, thin, medium&#xA;my_dataset/val_source/ \&#xA;my_dataset/val/random_&amp;lt;size&amp;gt;_512.yaml \# thick, thin, medium&#xA;--ext jpg&#xA;&#xA;# So the mask generator will: &#xA;# 1. resize and crop val images and save them as .png&#xA;# 2. generate masks&#xA;&#xA;ls my_dataset/val/random_medium_512/&#xA;image1_crop000_mask000.png&#xA;image1_crop000.png&#xA;image2_crop000_mask000.png&#xA;image2_crop000.png&#xA;...&#xA;&#xA;# Generate thick, thin, medium masks for visual_test folder:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \  #thick, thin, medium&#xA;my_dataset/visual_test_source/ \&#xA;my_dataset/visual_test/random_&amp;lt;size&amp;gt;_512/ \ #thick, thin, medium&#xA;--ext jpg&#xA;&#xA;&#xA;ls my_dataset/visual_test/random_thick_512/&#xA;image1_crop000_mask000.png&#xA;image1_crop000.png&#xA;image2_crop000_mask000.png&#xA;image2_crop000.png&#xA;...&#xA;&#xA;# Same process for eval_source image folder:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \  #thick, thin, medium&#xA;my_dataset/eval_source/ \&#xA;my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \ #thick, thin, medium&#xA;--ext jpg&#xA;&#xA;&#xA;&#xA;# Generate location config file which locate these folders:&#xA;&#xA;touch my_dataset.yaml&#xA;echo &#34;data_root_dir: $(pwd)/my_dataset/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;echo &#34;out_root_dir: $(pwd)/experiments/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;echo &#34;tb_dir: $(pwd)/tb_logs/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;mv my_dataset.yaml ${PWD}/configs/training/location/&#xA;&#xA;&#xA;# Check data config for consistency with my_dataset folder structure:&#xA;$ cat ${PWD}/configs/training/data/abl-04-256-mh-dist&#xA;...&#xA;train:&#xA;  indir: ${location.data_root_dir}/train&#xA;  ...&#xA;val:&#xA;  indir: ${location.data_root_dir}/val&#xA;  img_suffix: .png&#xA;visual_test:&#xA;  indir: ${location.data_root_dir}/visual_test&#xA;  img_suffix: .png&#xA;&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier location=my_dataset data.batch_size=10&#xA;&#xA;# Evaluation: LaMa training procedure picks best few models according to &#xA;# scores on my_dataset/val/ &#xA;&#xA;# To evaluate one of your best models (i.e. at epoch=32) &#xA;# on previously unseen my_dataset/eval do the following &#xA;# for thin, thick and medium:&#xA;&#xA;# infer:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier_/ \&#xA;indir=$(pwd)/my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \&#xA;outdir=$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512 \&#xA;model.checkpoint=epoch32.ckpt&#xA;&#xA;# metrics calculation:&#xA;python3 bin/evaluate_predicts.py \&#xA;$(pwd)/configs/eval2_gpu.yaml \&#xA;$(pwd)/my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \&#xA;$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512 \&#xA;$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512_metrics.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;OR&lt;/strong&gt; in the docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;TODO: train&#xA;TODO: eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Hints&lt;/h1&gt; &#xA;&lt;h3&gt;Generate different kinds of masks&lt;/h3&gt; &#xA;&lt;p&gt;The following command will execute a script that generates random masks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash docker/1_generate_masks_from_raw_images.sh \&#xA;    configs/data_gen/random_medium_512.yaml \&#xA;    /directory_with_input_images \&#xA;    /directory_where_to_store_images_and_masks \&#xA;    --ext png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The test data generation command stores images in the format, which is suitable for &lt;a href=&#34;https://raw.githubusercontent.com/advimman/lama/main/#prediction&#34;&gt;prediction&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The table below describes which configs we used to generate different test sets from the paper. Note that we &lt;em&gt;do not fix a random seed&lt;/em&gt;, so the results will be slightly different each time.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Places 512x512&lt;/th&gt; &#xA;   &lt;th&gt;CelebA 256x256&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Narrow&lt;/td&gt; &#xA;   &lt;td&gt;random_thin_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_thin_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;random_medium_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_medium_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wide&lt;/td&gt; &#xA;   &lt;td&gt;random_thick_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_thick_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Feel free to change the config path (argument #1) to any other config in &lt;code&gt;configs/data_gen&lt;/code&gt; or adjust config files themselves.&lt;/p&gt; &#xA;&lt;h3&gt;Override parameters in configs&lt;/h3&gt; &#xA;&lt;p&gt;Also you can override parameters in config like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 bin/train.py -cn &amp;lt;config&amp;gt; data.batch_size=10 run_title=my-title&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where .yaml file extension is omitted&lt;/p&gt; &#xA;&lt;h3&gt;Models options&lt;/h3&gt; &#xA;&lt;p&gt;Config names for models from paper (substitude into the training command):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;* big-lama&#xA;* big-lama-regular&#xA;* lama-fourier&#xA;* lama-regular&#xA;* lama_small_train_masks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which are seated in configs/training/folder&lt;/p&gt; &#xA;&lt;h3&gt;Links&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All the data (models, test images, etc.) &lt;a href=&#34;https://disk.yandex.ru/d/AmdeG-bIjmvSug&#34;&gt;https://disk.yandex.ru/d/AmdeG-bIjmvSug&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Test images from the paper &lt;a href=&#34;https://disk.yandex.ru/d/xKQJZeVRk5vLlQ&#34;&gt;https://disk.yandex.ru/d/xKQJZeVRk5vLlQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The pre-trained models &lt;a href=&#34;https://disk.yandex.ru/d/EgqaSnLohjuzAg&#34;&gt;https://disk.yandex.ru/d/EgqaSnLohjuzAg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The models for perceptual loss &lt;a href=&#34;https://disk.yandex.ru/d/ncVmQlmT_kTemQ&#34;&gt;https://disk.yandex.ru/d/ncVmQlmT_kTemQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Our training logs are available at &lt;a href=&#34;https://disk.yandex.ru/d/9Bt1wNSDS4jDkQ&#34;&gt;https://disk.yandex.ru/d/9Bt1wNSDS4jDkQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training time &amp;amp; resources&lt;/h3&gt; &#xA;&lt;p&gt;TODO&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Segmentation code and models if form &lt;a href=&#34;https://github.com/CSAILVision/semantic-segmentation-pytorch&#34;&gt;CSAILVision&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;LPIPS metric is from &lt;a href=&#34;https://github.com/richzhang/PerceptualSimilarity&#34;&gt;richzhang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SSIM is from &lt;a href=&#34;https://github.com/Po-Hsun-Su/pytorch-ssim&#34;&gt;Po-Hsun-Su&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FID is from &lt;a href=&#34;https://github.com/mseitzer/pytorch-fid&#34;&gt;mseitzer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found this code helpful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{suvorov2021resolution,&#xA;  title={Resolution-robust Large Mask Inpainting with Fourier Convolutions},&#xA;  author={Suvorov, Roman and Logacheva, Elizaveta and Mashikhin, Anton and Remizova, Anastasia and Ashukha, Arsenii and Silvestrov, Aleksei and Kong, Naejin and Goka, Harshith and Park, Kiwoong and Lempitsky, Victor},&#xA;  journal={arXiv preprint arXiv:2109.07161},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>