<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-29T01:44:05Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pesser/stable-diffusion</title>
    <updated>2022-07-29T01:44:05Z</updated>
    <id>tag:github.com,2022-07-29:/pesser/stable-diffusion</id>
    <link href="https://github.com/pesser/stable-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Latent Diffusion Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/#bibtex&#34;&gt;BibTeX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/assets/results.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/rromb&#34;&gt;Robin Rombach&lt;/a&gt;*, &lt;a href=&#34;https://github.com/ablattmann&#34;&gt;Andreas Blattmann&lt;/a&gt;*, &lt;a href=&#34;https://github.com/qp-qp&#34;&gt;Dominik Lorenz&lt;/a&gt;, &lt;a href=&#34;https://github.com/pesser&#34;&gt;Patrick Esser&lt;/a&gt;, &lt;a href=&#34;https://hci.iwr.uni-heidelberg.de/Staff/bommer&#34;&gt;BjÃ¶rn Ommer&lt;/a&gt;&lt;br&gt; * equal contribution&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/assets/modelfigure.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;h3&gt;April 2022&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;Katherine Crowson&lt;/a&gt;, classifier-free guidance received a ~2x speedup and the &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;PLMS sampler&lt;/a&gt; is available. See also &lt;a href=&#34;https://github.com/CompVis/latent-diffusion/pull/51&#34;&gt;this PR&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Our 1.45B &lt;a href=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/#text-to-image&#34;&gt;latent diffusion LAION model&lt;/a&gt; was integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces ðŸ¤—&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo: &lt;a href=&#34;https://huggingface.co/spaces/multimodalart/latentdiffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;More pre-trained LDMs are available:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A 1.45B &lt;a href=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/#text-to-image&#34;&gt;model&lt;/a&gt; trained on the &lt;a href=&#34;https://arxiv.org/abs/2111.02114&#34;&gt;LAION-400M&lt;/a&gt; database.&lt;/li&gt; &#xA;   &lt;li&gt;A class-conditional model on ImageNet, achieving a FID of 3.6 when using &lt;a href=&#34;https://openreview.net/pdf?id=qw8AKxfYbI&#34;&gt;classifier-free guidance&lt;/a&gt; Available via a &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;colab notebook&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;A suitable &lt;a href=&#34;https://conda.io/&#34;&gt;conda&lt;/a&gt; environment named &lt;code&gt;ldm&lt;/code&gt; can be created and activated with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate ldm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Pretrained Models&lt;/h1&gt; &#xA;&lt;p&gt;A general list of all available checkpoints is available in via our &lt;a href=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/#model-zoo&#34;&gt;model zoo&lt;/a&gt;. If you use any of these models in your work, we are always happy to receive a &lt;a href=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/#bibtex&#34;&gt;citation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Text-to-Image&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/assets/txt2img-preview.png&#34; alt=&#34;text2img-figure&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the pre-trained weights (5.7GB)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p models/ldm/text2img-large/&#xA;wget -O models/ldm/text2img-large/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and sample with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a virus monster is playing guitar, oil on canvas&#34; --ddim_eta 0.0 --n_samples 4 --n_iter 4 --scale 5.0  --ddim_steps 50&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will save each sample individually as well as a grid of size &lt;code&gt;n_iter&lt;/code&gt; x &lt;code&gt;n_samples&lt;/code&gt; at the specified output location (default: &lt;code&gt;outputs/txt2img-samples&lt;/code&gt;). Quality, sampling speed and diversity are best controlled via the &lt;code&gt;scale&lt;/code&gt;, &lt;code&gt;ddim_steps&lt;/code&gt; and &lt;code&gt;ddim_eta&lt;/code&gt; arguments. As a rule of thumb, higher values of &lt;code&gt;scale&lt;/code&gt; produce better samples at the cost of a reduced output diversity.&lt;br&gt; Furthermore, increasing &lt;code&gt;ddim_steps&lt;/code&gt; generally also gives higher quality samples, but returns are diminishing for values &amp;gt; 250. Fast sampling (i.e. low values of &lt;code&gt;ddim_steps&lt;/code&gt;) while retaining good quality can be achieved by using &lt;code&gt;--ddim_eta 0.0&lt;/code&gt;.&lt;br&gt; Faster sampling (i.e. even lower values of &lt;code&gt;ddim_steps&lt;/code&gt;) while retaining good quality can be achieved by using &lt;code&gt;--ddim_eta 0.0&lt;/code&gt; and &lt;code&gt;--plms&lt;/code&gt; (see &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;Pseudo Numerical Methods for Diffusion Models on Manifolds&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;Beyond 256Â²&lt;/h4&gt; &#xA;&lt;p&gt;For certain inputs, simply running the model in a convolutional fashion on larger features than it was trained on can sometimes result in interesting results. To try it out, tune the &lt;code&gt;H&lt;/code&gt; and &lt;code&gt;W&lt;/code&gt; arguments (which will be integer-divided by 8 in order to calculate the corresponding latent size), e.g. run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a sunset behind a mountain range, vector image&#34; --ddim_eta 1.0 --n_samples 1 --n_iter 1 --H 384 --W 1024 --scale 5.0  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to create a sample of size 384x1024. Note, however, that controllability is reduced compared to the 256x256 setting.&lt;/p&gt; &#xA;&lt;p&gt;The example below was generated using the above command. &lt;img src=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/assets/txt2img-convsample.png&#34; alt=&#34;text2img-figure-conv&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inpainting&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/assets/inpainting.png&#34; alt=&#34;inpainting&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the pre-trained weights&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget -O models/ldm/inpainting_big/last.ckpt https://heibox.uni-heidelberg.de/f/4d9ac7ea40c64582b7c9/?dl=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and sample with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/inpaint.py --indir data/inpainting_examples/ --outdir outputs/inpainting_results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;indir&lt;/code&gt; should contain images &lt;code&gt;*.png&lt;/code&gt; and masks &lt;code&gt;&amp;lt;image_fname&amp;gt;_mask.png&lt;/code&gt; like the examples provided in &lt;code&gt;data/inpainting_examples&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Class-Conditional ImageNet&lt;/h2&gt; &#xA;&lt;p&gt;Available via a &lt;a href=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;notebook&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/CompVis/latent-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;. &lt;img src=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/assets/birdhouse.png&#34; alt=&#34;class-conditional&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Unconditional Models&lt;/h2&gt; &#xA;&lt;p&gt;We also provide a script for sampling from unconditional LDMs (e.g. LSUN, FFHQ, ...). Start it via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python scripts/sample_diffusion.py -r models/ldm/&amp;lt;model_spec&amp;gt;/model.ckpt -l &amp;lt;logdir&amp;gt; -n &amp;lt;\#samples&amp;gt; --batch_size &amp;lt;batch_size&amp;gt; -c &amp;lt;\#ddim steps&amp;gt; -e &amp;lt;\#eta&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Train your own LDMs&lt;/h1&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;h3&gt;Faces&lt;/h3&gt; &#xA;&lt;p&gt;For downloading the CelebA-HQ and FFHQ datasets, proceed as described in the &lt;a href=&#34;https://github.com/CompVis/taming-transformers#celeba-hq&#34;&gt;taming-transformers&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h3&gt;LSUN&lt;/h3&gt; &#xA;&lt;p&gt;The LSUN datasets can be conveniently downloaded via the script available &lt;a href=&#34;https://github.com/fyu/lsun&#34;&gt;here&lt;/a&gt;. We performed a custom split into training and validation images, and provide the corresponding filenames at &lt;a href=&#34;https://ommer-lab.com/files/lsun.zip&#34;&gt;https://ommer-lab.com/files/lsun.zip&lt;/a&gt;. After downloading, extract them to &lt;code&gt;./data/lsun&lt;/code&gt;. The beds/cats/churches subsets should also be placed/symlinked at &lt;code&gt;./data/lsun/bedrooms&lt;/code&gt;/&lt;code&gt;./data/lsun/cats&lt;/code&gt;/&lt;code&gt;./data/lsun/churches&lt;/code&gt;, respectively.&lt;/p&gt; &#xA;&lt;h3&gt;ImageNet&lt;/h3&gt; &#xA;&lt;p&gt;The code will try to download (through &lt;a href=&#34;http://academictorrents.com/&#34;&gt;Academic Torrents&lt;/a&gt;) and prepare ImageNet the first time it is used. However, since ImageNet is quite large, this requires a lot of disk space and time. If you already have ImageNet on your disk, you can speed things up by putting the data into &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt; (which defaults to &lt;code&gt;~/.cache/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt;), where &lt;code&gt;{split}&lt;/code&gt; is one of &lt;code&gt;train&lt;/code&gt;/&lt;code&gt;validation&lt;/code&gt;. It should have the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&#xA;â”œâ”€â”€ n01440764&#xA;â”‚   â”œâ”€â”€ n01440764_10026.JPEG&#xA;â”‚   â”œâ”€â”€ n01440764_10027.JPEG&#xA;â”‚   â”œâ”€â”€ ...&#xA;â”œâ”€â”€ n01443537&#xA;â”‚   â”œâ”€â”€ n01443537_10007.JPEG&#xA;â”‚   â”œâ”€â”€ n01443537_10014.JPEG&#xA;â”‚   â”œâ”€â”€ ...&#xA;â”œâ”€â”€ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you haven&#39;t extracted the data, you can also place &lt;code&gt;ILSVRC2012_img_train.tar&lt;/code&gt;/&lt;code&gt;ILSVRC2012_img_val.tar&lt;/code&gt; (or symlinks to them) into &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_train/&lt;/code&gt; / &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_validation/&lt;/code&gt;, which will then be extracted into above structure without downloading it again. Note that this will only happen if neither a folder &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/&lt;/code&gt; nor a file &lt;code&gt;${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/.ready&lt;/code&gt; exist. Remove them if you want to force running the dataset preparation again.&lt;/p&gt; &#xA;&lt;h2&gt;Model Training&lt;/h2&gt; &#xA;&lt;p&gt;Logs and checkpoints for trained models are saved to &lt;code&gt;logs/&amp;lt;START_DATE_AND_TIME&amp;gt;_&amp;lt;config_spec&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Training autoencoder models&lt;/h3&gt; &#xA;&lt;p&gt;Configs for training a KL-regularized autoencoder on ImageNet are provided at &lt;code&gt;configs/autoencoder&lt;/code&gt;. Training can be started by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python main.py --base configs/autoencoder/&amp;lt;config_spec&amp;gt;.yaml -t --gpus 0,    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;config_spec&lt;/code&gt; is one of {&lt;code&gt;autoencoder_kl_8x8x64&lt;/code&gt;(f=32, d=64), &lt;code&gt;autoencoder_kl_16x16x16&lt;/code&gt;(f=16, d=16), &lt;code&gt;autoencoder_kl_32x32x4&lt;/code&gt;(f=8, d=4), &lt;code&gt;autoencoder_kl_64x64x3&lt;/code&gt;(f=4, d=3)}.&lt;/p&gt; &#xA;&lt;p&gt;For training VQ-regularized models, see the &lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;taming-transformers&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h3&gt;Training LDMs&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;configs/latent-diffusion/&lt;/code&gt; we provide configs for training LDMs on the LSUN-, CelebA-HQ, FFHQ and ImageNet datasets. Training can be started by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=&amp;lt;GPU_ID&amp;gt; python main.py --base configs/latent-diffusion/&amp;lt;config_spec&amp;gt;.yaml -t --gpus 0,&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;&amp;lt;config_spec&amp;gt;&lt;/code&gt; is one of {&lt;code&gt;celebahq-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3),&lt;code&gt;ffhq-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3), &lt;code&gt;lsun_bedrooms-ldm-vq-4&lt;/code&gt;(f=4, VQ-reg. autoencoder, spatial size 64x64x3), &lt;code&gt;lsun_churches-ldm-vq-4&lt;/code&gt;(f=8, KL-reg. autoencoder, spatial size 32x32x4),&lt;code&gt;cin-ldm-vq-8&lt;/code&gt;(f=8, VQ-reg. autoencoder, spatial size 32x32x4)}.&lt;/p&gt; &#xA;&lt;h1&gt;Model Zoo&lt;/h1&gt; &#xA;&lt;h2&gt;Pretrained Autoencoding Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pesser/stable-diffusion/main/assets/reconstruction2.png&#34; alt=&#34;rec2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;All models were trained until convergence (no further substantial improvement in rFID).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;rFID vs val&lt;/th&gt; &#xA;   &lt;th&gt;train steps&lt;/th&gt; &#xA;   &lt;th&gt;PSNR&lt;/th&gt; &#xA;   &lt;th&gt;PSIM&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Comments&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, VQ (Z=8192, d=3)&lt;/td&gt; &#xA;   &lt;td&gt;0.58&lt;/td&gt; &#xA;   &lt;td&gt;533066&lt;/td&gt; &#xA;   &lt;td&gt;27.43 +/- 4.26&lt;/td&gt; &#xA;   &lt;td&gt;0.53 +/- 0.21&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f4.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f4.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, VQ (Z=8192, d=3)&lt;/td&gt; &#xA;   &lt;td&gt;1.06&lt;/td&gt; &#xA;   &lt;td&gt;658131&lt;/td&gt; &#xA;   &lt;td&gt;25.21 +/- 4.17&lt;/td&gt; &#xA;   &lt;td&gt;0.72 +/- 0.26&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://heibox.uni-heidelberg.de/f/9c6681f64bb94338a069/?dl=1&#34;&gt;https://heibox.uni-heidelberg.de/f/9c6681f64bb94338a069/?dl=1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no attention&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, VQ (Z=16384, d=4)&lt;/td&gt; &#xA;   &lt;td&gt;1.14&lt;/td&gt; &#xA;   &lt;td&gt;971043&lt;/td&gt; &#xA;   &lt;td&gt;23.07 +/- 3.99&lt;/td&gt; &#xA;   &lt;td&gt;1.17 +/- 0.36&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f8.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f8.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, VQ (Z=256, d=4)&lt;/td&gt; &#xA;   &lt;td&gt;1.49&lt;/td&gt; &#xA;   &lt;td&gt;1608649&lt;/td&gt; &#xA;   &lt;td&gt;22.35 +/- 3.81&lt;/td&gt; &#xA;   &lt;td&gt;1.26 +/- 0.37&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f8-n256.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/vq-f8-n256.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=16, VQ (Z=16384, d=8)&lt;/td&gt; &#xA;   &lt;td&gt;5.15&lt;/td&gt; &#xA;   &lt;td&gt;1101166&lt;/td&gt; &#xA;   &lt;td&gt;20.83 +/- 3.61&lt;/td&gt; &#xA;   &lt;td&gt;1.73 +/- 0.43&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://heibox.uni-heidelberg.de/f/0e42b04e2e904890a9b6/?dl=1&#34;&gt;https://heibox.uni-heidelberg.de/f/0e42b04e2e904890a9b6/?dl=1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=4, KL&lt;/td&gt; &#xA;   &lt;td&gt;0.27&lt;/td&gt; &#xA;   &lt;td&gt;176991&lt;/td&gt; &#xA;   &lt;td&gt;27.53 +/- 4.54&lt;/td&gt; &#xA;   &lt;td&gt;0.55 +/- 0.24&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f4.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f4.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=8, KL&lt;/td&gt; &#xA;   &lt;td&gt;0.90&lt;/td&gt; &#xA;   &lt;td&gt;246803&lt;/td&gt; &#xA;   &lt;td&gt;24.19 +/- 4.19&lt;/td&gt; &#xA;   &lt;td&gt;1.02 +/- 0.35&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f8.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f8.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=16, KL (d=16)&lt;/td&gt; &#xA;   &lt;td&gt;0.87&lt;/td&gt; &#xA;   &lt;td&gt;442998&lt;/td&gt; &#xA;   &lt;td&gt;24.08 +/- 4.22&lt;/td&gt; &#xA;   &lt;td&gt;1.07 +/- 0.36&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f16.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f16.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;f=32, KL (d=64)&lt;/td&gt; &#xA;   &lt;td&gt;2.04&lt;/td&gt; &#xA;   &lt;td&gt;406763&lt;/td&gt; &#xA;   &lt;td&gt;22.27 +/- 3.93&lt;/td&gt; &#xA;   &lt;td&gt;1.41 +/- 0.40&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/kl-f32.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/kl-f32.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Get the models&lt;/h3&gt; &#xA;&lt;p&gt;Running the following script downloads und extracts all available pretrained autoencoding models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/download_first_stages.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first stage models can then be found in &lt;code&gt;models/first_stage_models/&amp;lt;model_spec&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained LDMs&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Datset&lt;/th&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;FID&lt;/th&gt; &#xA;   &lt;th&gt;IS&lt;/th&gt; &#xA;   &lt;th&gt;Prec&lt;/th&gt; &#xA;   &lt;th&gt;Recall&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Comments&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CelebA-HQ&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;5.11 (5.11)&lt;/td&gt; &#xA;   &lt;td&gt;3.29&lt;/td&gt; &#xA;   &lt;td&gt;0.72&lt;/td&gt; &#xA;   &lt;td&gt;0.49&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/celeba.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/celeba.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FFHQ&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;4.98 (4.98)&lt;/td&gt; &#xA;   &lt;td&gt;4.50 (4.50)&lt;/td&gt; &#xA;   &lt;td&gt;0.73&lt;/td&gt; &#xA;   &lt;td&gt;0.50&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/ffhq.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/ffhq.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LSUN-Churches&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-KL-8 (400 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;4.02 (4.02)&lt;/td&gt; &#xA;   &lt;td&gt;2.72&lt;/td&gt; &#xA;   &lt;td&gt;0.64&lt;/td&gt; &#xA;   &lt;td&gt;0.52&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/lsun_churches.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/lsun_churches.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LSUN-Bedrooms&lt;/td&gt; &#xA;   &lt;td&gt;Unconditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;2.95 (3.0)&lt;/td&gt; &#xA;   &lt;td&gt;2.22 (2.23)&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;   &lt;td&gt;0.48&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/lsun_bedrooms.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/lsun_bedrooms.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ImageNet&lt;/td&gt; &#xA;   &lt;td&gt;Class-conditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-8 (200 DDIM steps, eta=1)&lt;/td&gt; &#xA;   &lt;td&gt;7.77(7.76)* /15.82**&lt;/td&gt; &#xA;   &lt;td&gt;201.56(209.52)* /78.82**&lt;/td&gt; &#xA;   &lt;td&gt;0.84* / 0.65**&lt;/td&gt; &#xA;   &lt;td&gt;0.35* / 0.63**&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/cin.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/cin.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;*: w/ guiding, classifier_scale 10 **: w/o guiding, scores in bracket calculated with script provided by &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;ADM&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Conceptual Captions&lt;/td&gt; &#xA;   &lt;td&gt;Text-conditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-f4 (100 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;16.79&lt;/td&gt; &#xA;   &lt;td&gt;13.89&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/text2img.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/text2img.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;finetuned from LAION&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenImages&lt;/td&gt; &#xA;   &lt;td&gt;Super-resolution&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/sr_bsr.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/sr_bsr.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BSR image degradation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenImages&lt;/td&gt; &#xA;   &lt;td&gt;Layout-to-Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4 (200 DDIM steps, eta=0)&lt;/td&gt; &#xA;   &lt;td&gt;32.02&lt;/td&gt; &#xA;   &lt;td&gt;15.92&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/layout2img_model.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/layout2img_model.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Landscapes&lt;/td&gt; &#xA;   &lt;td&gt;Semantic Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis256.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis256.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Landscapes&lt;/td&gt; &#xA;   &lt;td&gt;Semantic Image Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;LDM-VQ-4&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis.zip&#34;&gt;https://ommer-lab.com/files/latent-diffusion/semantic_synthesis.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;finetuned on resolution 512x512&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Get the models&lt;/h3&gt; &#xA;&lt;p&gt;The LDMs listed above can jointly be downloaded and extracted via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/download_models.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The models can then be found in &lt;code&gt;models/ldm/&amp;lt;model_spec&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Coming Soon...&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More inference scripts for conditional LDMs.&lt;/li&gt; &#xA; &lt;li&gt;In the meantime, you can play with our colab notebook &lt;a href=&#34;https://colab.research.google.com/drive/1xqzUi2iXQXDqXBHQGP9Mqt2YrYW6cx-J?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1xqzUi2iXQXDqXBHQGP9Mqt2YrYW6cx-J?usp=sharing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Comments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Our codebase for the diffusion models builds heavily on &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;OpenAI&#39;s ADM codebase&lt;/a&gt; and &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;https://github.com/lucidrains/denoising-diffusion-pytorch&lt;/a&gt;. Thanks for open-sourcing!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The implementation of the transformer encoder is from &lt;a href=&#34;https://github.com/lucidrains/x-transformers&#34;&gt;x-transformers&lt;/a&gt; by &lt;a href=&#34;https://github.com/lucidrains?tab=repositories&#34;&gt;lucidrains&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;      title={High-Resolution Image Synthesis with Latent Diffusion Models}, &#xA;      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and BjÃ¶rn Ommer},&#xA;      year={2021},&#xA;      eprint={2112.10752},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>insidesherpa/JPMC-tech-task-1</title>
    <updated>2022-07-29T01:44:05Z</updated>
    <id>tag:github.com,2022-07-29:/insidesherpa/JPMC-tech-task-1</id>
    <link href="https://github.com/insidesherpa/JPMC-tech-task-1" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.insidesherpa.com/virtual-internships/prototype/R5iK7HMxJGBgaSbvk/Technology%20Virtual%20Experience&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://insidesherpa-assets.s3-ap-southeast-2.amazonaws.com/icons/jpmorgan/github+repo+images/jpm+gitub+.png&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/insidesherpa/JPMC-tech-task-1/master/#task&#34;&gt;Task Overview&lt;/a&gt;&lt;/b&gt; | &lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/insidesherpa/JPMC-tech-task-1/master/#installation&#34;&gt;Installation Instructions&lt;/a&gt;&lt;/b&gt; | &lt;b&gt;&lt;a href=&#34;https://www.insidesherpa.com/modules/R5iK7HMxJGBgaSbvk/gtAhtcvke9AFCzqME&#34; target=&#34;_blank&#34;&gt;Link to Module 1&lt;/a&gt;&lt;/b&gt; | &lt;b&gt;&lt;a href=&#34;https://www.insidesherpa.com/virtual-internships/prototype/R5iK7HMxJGBgaSbvk/Technology%20Virtual%20Experience&#34;&gt;JP Morgan Chase &amp;amp; Co Software Engineering Virtual Experience&lt;/a&gt;&lt;/b&gt; &lt;/p&gt; &#xA;&lt;h1&gt; Introduction&lt;/h1&gt; &#xA;&lt;b&gt; Experience Technology at JP Morgan Chase &amp;amp; Co&lt;/b&gt; &#xA;&lt;p&gt;Try out what real work is like in the technology team at JP Morgan Chase &amp;amp; Co. Fast track to the tech team with your work.&lt;/p&gt; &#xA;&lt;h2 id=&#34;task&#34;&gt; Module 1 Task Overview &lt;/h2&gt; &#xA;&lt;p&gt;Interface with a stock price data feed and set up your system for analysis of the data&lt;/p&gt; &#xA;&lt;p&gt; &lt;b&gt;Aim:&lt;/b&gt; We want to process the data feed of stock A and stock Bâ€™s price to enable us to analyse when trading for the stock should occur.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Please clone this repository to start the task&lt;/li&gt; &#xA; &lt;li&gt;Adjust the getRatio, getDataPoint and main functions&lt;/li&gt; &#xA; &lt;li&gt;Bonus: Pass all unit tests and add more to cover edge cases&lt;/li&gt; &#xA; &lt;li&gt;Upload a git patch file as the submission to this task&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2 id=&#34;installation&#34;&gt;Set up / Installation&lt;/h2&gt; &#xA;&lt;p&gt;In order to get the server and client application code working on your machine, &lt;a href=&#34;https://insidesherpa.s3.amazonaws.com/vinternships/companyassets/Sj7temL583QAYpHXD/setup_devenv_m1_v6.pdf&#34;&gt;follow the setup here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to Run&lt;/h2&gt; To start the server, run &#xA;&lt;pre&gt;&lt;code&gt;python server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;this will create random market called &#39;test.csv&#39; in your working directory if one does not already exist.&lt;/p&gt; &#xA;&lt;p&gt;If you encounter an issue with &lt;code&gt;datautil.parser&lt;/code&gt;, run this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install python-dateutil&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you don&#39;t have pip yet, you can install it from: &lt;a href=&#34;https://pip.pypa.io/en/stable/installing/&#34;&gt;https://pip.pypa.io/en/stable/installing/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To start the example client, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python client.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To unit test the example client, run: python client_test.py&lt;/p&gt; &#xA;&lt;h2&gt;How to request from the server using curl&lt;/h2&gt; &#xA;&lt;!--See also [client.py](https://github.com/texodus/exchange_simulator/blob/master/client.py)--&gt; Query: &#xA;&lt;pre&gt;&lt;code&gt;$ curl &#39;http://localhost:8080/query?id=1&#39;&#xA;{&#34;id&#34;: &#34;1&#34;, &#34;top_ask&#34;: {&#34;price&#34;: 129.18, &#34;size&#34;: 70}, &#34;timestamp&#34;: &#34;2016-08-06 12:32:11.821574&#34;, &#34;top_bid&#34;: {&#34;price&#34;: 128.79, &#34;size&#34;: 61}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to fix the code to meet objectives&lt;/h2&gt; &#xA;&lt;p&gt;To make the changes necessary to complete the objectives of this task, &lt;a href=&#34;https://insidesherpa.s3.amazonaws.com/vinternships/companyassets/Sj7temL583QAYpHXD/making_changes_m1_v4a.pdf&#34;&gt;follow this guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To do the bonus task, &lt;a href=&#34;https://insidesherpa.s3.amazonaws.com/vinternships/companyassets/Sj7temL583QAYpHXD/client_test_m1_v1a.pdf&#34;&gt;read this&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to submit your work&lt;/h2&gt; &#xA;&lt;p&gt;A patch file is what is required from you to submit. To create a patch file, &lt;a href=&#34;https://insidesherpa.s3.amazonaws.com/vinternships/companyassets/Sj7temL583QAYpHXD/create_patch_file_v3a.pdf&#34;&gt;follow this guide&lt;/a&gt;. Then submit the patch file in the &lt;a href=&#34;https://www.insidesherpa.com/modules/R5iK7HMxJGBgaSbvk/gtAhtcvke9AFCzqME&#34;&gt;JPM Module 1 Page&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>