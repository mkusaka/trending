<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-25T01:39:51Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hundredblocks/large-model-parallelism</title>
    <updated>2023-02-25T01:39:51Z</updated>
    <id>tag:github.com,2023-02-25:/hundredblocks/large-model-parallelism</id>
    <link href="https://github.com/hundredblocks/large-model-parallelism" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Functional local implementations of main model parallelism approaches&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/7229234/220430946-4b4ab497-7f86-40af-ac43-3e2793900bce.jpg&#34; alt=&#34;large-model-parallelism-illustration&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Model parallelism 101&lt;/h1&gt; &#xA;&lt;p&gt;Learn how model parallelism enables training models like stable diffusion and Chat GPT in less than 300 lines of code. This &lt;a href=&#34;https://github.com/hundredblocks/large-model-parallelism/raw/main/large-model-parallelism.ipynb&#34;&gt;notebook&lt;/a&gt; provides practical local implementations of the main model parallelism methods. It explores three approaches: data parallelism, tensor parallelism, and pipeline parallelism with a 2-layer MLP example that can be naturally extended to more complex models.&lt;/p&gt; &#xA;&lt;p&gt;Reading this notebook will give you a solid overview of model parallelism techniques and an intuition for how to implement them.&lt;/p&gt; &#xA;&lt;p&gt;Pull requests welcome. Illustration above generated with Lexica&#39;s Aperture model.&lt;/p&gt;</summary>
  </entry>
</feed>