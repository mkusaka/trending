<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-28T01:29:35Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>sroecker/LLM_AppDev-HandsOn</title>
    <updated>2023-12-28T01:29:35Z</updated>
    <id>tag:github.com,2023-12-28:/sroecker/LLM_AppDev-HandsOn</id>
    <link href="https://github.com/sroecker/LLM_AppDev-HandsOn" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repository and hands-on workshop on how to develop applications with local LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLM App Dev Workshop&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sroecker/LLM_AppDev-HandsOn/main/localllamas.png&#34; alt=&#34;a bunch of happy local llamas&#34; width=&#34;256&#34;&gt; &#xA;&lt;p&gt;This repository demonstrates how to build a simple LLM-based chatbot that can answer questions based on your documents (retrieval augmented generation - RAG) and how to deploy it using &lt;a href=&#34;https://podman.io&#34;&gt;Podman&lt;/a&gt; or on the &lt;a href=&#34;https://www.openshift.com&#34;&gt;OpenShift&lt;/a&gt; Container Platform (k8s).&lt;/p&gt; &#xA;&lt;p&gt;The corresponding &lt;a href=&#34;https://raw.githubusercontent.com/sroecker/LLM_AppDev-HandsOn/main/workshop/Darmstadt_v1.md&#34;&gt;workshop&lt;/a&gt; - first run at &lt;a href=&#34;https://events.redhat.com/profile/form/index.cfm?PKformID=0x900962abcd&amp;amp;sc_cid=7013a000003SlFvAAK&#34;&gt;Red Hat Developers Hands-On Day 2023&lt;/a&gt; in Darmstadt, Germany - teaches participants the basic concepts of LLMs &amp;amp; RAG, and how to adapt this example implementation to their own specific purpose GPT.&lt;/p&gt; &#xA;&lt;p&gt;The software stack only uses open source tools &lt;a href=&#34;https://streamlit.io&#34;&gt;streamlit&lt;/a&gt;, &lt;a href=&#34;https://llamaindex.ai&#34;&gt;LlamaIndex&lt;/a&gt; and local open LLMs via &lt;a href=&#34;https://ollama.ai&#34;&gt;Ollama&lt;/a&gt;. Real open AI for the GPU poor.&lt;/p&gt; &#xA;&lt;p&gt;Everyone is invited to fork this repository, create their own specific purpose chatbot based on their documents, improve the setup or even hold your own workshop.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;For the local setup a Mac M1 with 16GB unified memory and above are recommended. First download Ollama from &lt;a href=&#34;https://ollama.ai&#34;&gt;ollama.ai&lt;/a&gt; and install it.&lt;/p&gt; &#xA;&lt;p&gt;On Linux you can disable the Ollama service for better debugging:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo systemctl disable ollama&#xA;sudo systemctl stop ollama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and then manually run &lt;code&gt;ollama serve&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For the local example have a look at the folder &lt;code&gt;streamlit&lt;/code&gt; and install the requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then start streamlit with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;streamlit run app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sroecker/LLM_AppDev-HandsOn/main/linuxbot.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Modify the system prompt and copy different data sources to &lt;code&gt;docs/&lt;/code&gt; in order to create your own version of the chatbot. You can set the ollama host via the enviroment variable &lt;code&gt;OLLAMA_HOST&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can download models locally with &lt;code&gt;ollama pull zephyr&lt;/code&gt; or via API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X POST http://ollama:11434/api/pull -d &#39;{&#34;name&#34;: &#34;zephyr&#34;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;First start the ollama service as described and download the &lt;a href=&#34;https://ollama.ai/library/zephyr&#34;&gt;Zephyr model&lt;/a&gt;. To test the ollama server you can call the generate API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X POST http://ollama:11434/api/generate -d &#39;{&#34;model&#34;: &#34;zephyr&#34;, &#34;prompt&#34;: &#34;Why is the sky blue?&#34;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All of these commands are also documented in our &lt;a href=&#34;https://raw.githubusercontent.com/sroecker/LLM_AppDev-HandsOn/main/cheatsheet.txt&#34;&gt;cheat sheet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;h3&gt;Podman&lt;/h3&gt; &#xA;&lt;p&gt;Build the container based on &lt;a href=&#34;https://catalog.redhat.com/software/containers/ubi9/python-311/63f764b03f0b02a2e2d63fff?architecture=amd64&amp;amp;image=654d1ee47c3bfba06c9c59ea&#34;&gt;UBI9 Python 3.11&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;podman build -t linuxbot-app .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re building on arm64 Mac and deploy on amd64 then generally don&#39;t forget to add &lt;code&gt;--platform&lt;/code&gt; (in this case our base image is amd64 anyways):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;podman build --platform=&#34;linux/amd64&#34; -t linuxbot-app .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We will create a network for our linuxbot and ollama:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;podman network create linuxbot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check if DNS is enabled (it&#39;s not on the default net):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;podman network inspect linuxbot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can either start Ollama locally with &lt;code&gt;ollama serve&lt;/code&gt; or start a Ollama container with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;podman run --net linuxbot --name ollama -p 11434:11434 --rm docker.io/ollama/ollama:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: We just forward the port so we can curl it more easily locally as well.&lt;/p&gt; &#xA;&lt;p&gt;This ollama service won&#39;t have GPU support enabled and much slower compared to running it locally on a Mac M1 for example.&lt;/p&gt; &#xA;&lt;p&gt;Since we create the embeddings locally in the streamlit app we need to increase shared memory for Pytorch in order to get it running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;podman run --net linuxbot --name linuxbot-app -p 8080:8080 --shm-size=2gb -e OLLAMA_HOST=ollama -it --rm localhost/linuxbot-app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can set the Ollama server via the environment variable &lt;code&gt;OLLAMA_HOST&lt;/code&gt;, the default is &lt;code&gt;localhost&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: It would be much better to generate the embeddings with the ollama service, this is not yet supported in LlamaIndex though.&lt;/p&gt; &#xA;&lt;h3&gt;OpenShift&lt;/h3&gt; &#xA;&lt;p&gt;Create a new project (namespace) for your workshop and deploy the ollama service in it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;oc new-project my-workshop&#xA;oc apply -f deployments/ollama.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to enable GPU support you have to have to install the NVIDIA GPU Operator and Node Feature Discovery (NFD) Operator as described on the &lt;a href=&#34;https://ai-on-openshift.io/odh-rhods/nvidia-gpus/&#34;&gt;AI on OpenShift&lt;/a&gt; page, then deploy &lt;code&gt;ollama-gpu.yaml&lt;/code&gt; instead.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;oc apply -f deployments/ollama-gpu.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The streamlit application (linuxbot) can deployed as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;oc apply -f deployments/linuxbot.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have published a preconfigured container image on &lt;a href=&#34;https://quay.io/sroecker/linuxbot-app&#34;&gt;quay.io/sroecker&lt;/a&gt; that is used in this deployment.&lt;/p&gt; &#xA;&lt;p&gt;In order to debug your application and ollama service you can deploy a curl image like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;oc run mycurl --image=curlimages/curl -it -- sh&#xA;oc attach mycurl -c mycurl -i -t&#xA;oc delete pod mycurl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/&#34;&gt;Build a chatbot with custom data sources, powered by LlamaIndex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gpt-index.readthedocs.io/en/latest/examples/index_structs/struct_indices/duckdb_sql_query.html&#34;&gt;SQL Query Engine with LlamaIndex + DuckDB&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai-on-openshift.io/demos/llm-chat-doc/llm-chat-doc/&#34;&gt;AI on Openshift - LLMs, Chatbots, Talk with your Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.opensourcerers.org/2023/11/06/a-personal-ai-assistant-for-developers-that-doesnt-phone-home/&#34;&gt;Open Sourcerers - A personal AI assistant for developers that doesn&#39;t phone home&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>