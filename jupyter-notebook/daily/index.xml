<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-07T01:39:33Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>collabora/spear-tts-pytorch</title>
    <updated>2023-03-07T01:39:33Z</updated>
    <id>tag:github.com,2023-03-07:/collabora/spear-tts-pytorch</id>
    <link href="https://github.com/collabora/spear-tts-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An unofficial PyTorch implementation of SPEAR-TTS.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SPEAR-TTS&lt;/h1&gt; &#xA;&lt;p&gt;An unofficial PyTorch implementation of &lt;a href=&#34;https://google-research.github.io/seanet/speartts/examples/&#34;&gt;SPEAR-TTS&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We are not targeting an exact copy ‚Äì to speed up training we want to use existing Open Source models as bases: &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt; encoder to generate semantic tokens and &lt;a href=&#34;https://github.com/facebookresearch/encodec&#34;&gt;EnCodec&lt;/a&gt; for acoustic modeling.&lt;/p&gt; &#xA;&lt;p&gt;Following Google Brain we&#39;ll train on the LibreLight and LibreTTS datasets. Ultimately we want to target multiple languages (Whisper and EnCodec are both multilanguage).&lt;/p&gt; &#xA;&lt;p&gt;UPDATE 2023-02-24: I think I finally figured out how to train the semantic encodings bottleneck. Check the issues for more detailed progress updates.&lt;/p&gt; &#xA;&lt;h2&gt;Whisper for modeling semantic tokens&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/collabora/spear-tts-pytorch/main/whisper-block.png&#34; alt=&#34;Using Whisper for semantic token extraction diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Pros:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Whisper training should be a lot better at extracting semantic information than a masked language model with contrastive loss (w2v-VERT)&lt;/li&gt; &#xA; &lt;li&gt;it&#39;s pretrained on 600k hours of multilingual speech (vs. 60k for w2v-BERT used in the paper)&lt;/li&gt; &#xA; &lt;li&gt;freely available&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cons:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2x higher &#34;symbol rate&#34; (50 vec/s) than w2v-BERT (25 vec/s) which means training the semantic-&amp;gt;acoustic transformer may take longer&lt;/li&gt; &#xA; &lt;li&gt;it seems that we&#39;ll need 6x higher symbol rate if we want to quantize the embeddings effectively, OTOH maybe later modeling tasks will be easier?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;EnCodec for modeling acoustic tokens&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/facebookresearch/encodec/raw/main/architecture.png&#34; alt=&#34;EnCodec block diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Pros:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;High-quality pretrained model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cons:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Comparing the speech samples with SPEAR-TTS, EnCodec needs 6kbps to get the same quality (SoundStream retrained only on speech seems to work with 1.5kbps)&lt;/li&gt; &#xA; &lt;li&gt;CC-BY-NC license&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We may switch to the &lt;a href=&#34;https://github.com/lucidrains/audiolm-pytorch/raw/main/audiolm_pytorch/soundstream.py&#34;&gt;OpenSource SoundStream re-implementation&lt;/a&gt; or train a new speech-only model.&lt;/p&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{SpearTTS,&#xA;  title = {Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision},&#xA;  url = {https://arxiv.org/abs/2302.03540},&#xA;  author = {Kharitonov, Eugene and Vincent, Damien and Borsos, Zal√°n and Marinier, Rapha√´l and Girgin, Sertan and Pietquin, Olivier and Sharifi, Matt and Tagliasacchi, Marco and Zeghidour, Neil},&#xA;  publisher = {arXiv},&#xA;  year = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Whisper&#xA;  title = {Robust Speech Recognition via Large-Scale Weak Supervision},&#xA;  url = {https://arxiv.org/abs/2212.04356},&#xA;  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},&#xA;  publisher = {arXiv},  &#xA;  year = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{EnCodec&#xA;  title = {High Fidelity Neural Audio Compression},&#xA;  url = {https://arxiv.org/abs/2210.13438},&#xA;  author = {D√©fossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},&#xA;  publisher = {arXiv},&#xA;  year = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>patchy631/machine-learning</title>
    <updated>2023-03-07T01:39:33Z</updated>
    <id>tag:github.com,2023-03-07:/patchy631/machine-learning</id>
    <link href="https://github.com/patchy631/machine-learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;‚ùáÔ∏è This repository contains notebooks of my Machine Learning tutorials&lt;/h1&gt; &#xA;&lt;h2&gt;Happy learning!! üçª&lt;/h2&gt;</summary>
  </entry>
</feed>