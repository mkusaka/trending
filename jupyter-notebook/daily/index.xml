<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-12T02:27:40Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mkshing/svdiff-pytorch</title>
    <updated>2023-04-12T02:27:40Z</updated>
    <id>tag:github.com,2023-04-12:/mkshing/svdiff-pytorch</id>
    <link href="https://github.com/mkshing/svdiff-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of &#34;SVDiff: Compact Parameter Space for Diffusion Fine-Tuning&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SVDiff-pytorch&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/mkshing/svdiff-pytorch/blob/main/scripts/svdiff_pytorch.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/svdiff-library/SVDiff-Training-UI&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm.svg?sanitize=true&#34; alt=&#34;Open in Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An implementation of &lt;a href=&#34;https://arxiv.org/abs/2303.11305&#34;&gt;SVDiff: Compact Parameter Space for Diffusion Fine-Tuning&lt;/a&gt; by using dðŸ§¨ffusers.&lt;/p&gt; &#xA;&lt;p&gt;My summary tweet is found &lt;a href=&#34;https://twitter.com/mk1stats/status/1642865505106272257&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mkshing/svdiff-pytorch/main/assets/dog.png&#34; alt=&#34;result&#34;&gt; left: LoRA, right: SVDiff&lt;/p&gt; &#xA;&lt;p&gt;Compared with LoRA, the number of trainable parameters is 0.6 M less parameters and the file size is only &amp;lt;1MB (LoRA: 3.1MB)!!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mkshing/svdiff-pytorch/main/assets/kumamon.png&#34; alt=&#34;kumamon&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install svdiff-pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, manually&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/mkshing/svdiff-pytorch&#xA;$ pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;The following example script is for &#34;Single-Subject Generation&#34;, which is a domain-tuning on a single object or concept (using 3-5 images). (See Section 4.1)&lt;/p&gt; &#xA;&lt;p&gt;According to the paper, the learning rate for SVDiff needs to be 1000 times larger than the lr used for fine-tuning.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export MODEL_NAME=&#34;runwayml/stable-diffusion-v1-5&#34;&#xA;export INSTANCE_DIR=&#34;path-to-instance-images&#34;&#xA;export CLASS_DIR=&#34;path-to-class-images&#34;&#xA;export OUTPUT_DIR=&#34;path-to-save-model&#34;&#xA;&#xA;accelerate launch train_svdiff.py \&#xA;  --pretrained_model_name_or_path=$MODEL_NAME  \&#xA;  --instance_data_dir=$INSTANCE_DIR \&#xA;  --class_data_dir=$CLASS_DIR \&#xA;  --output_dir=$OUTPUT_DIR \&#xA;  --with_prior_preservation --prior_loss_weight=1.0 \&#xA;  --instance_prompt=&#34;photo of a sks dog&#34; \&#xA;  --class_prompt=&#34;photo of a dog&#34; \&#xA;  --resolution=512 \&#xA;  --train_batch_size=1 \&#xA;  --gradient_accumulation_steps=1 \&#xA;  --learning_rate=5e-3 \&#xA;  --lr_scheduler=&#34;constant&#34; \&#xA;  --lr_warmup_steps=0 \&#xA;  --num_class_images=200 \&#xA;  --max_train_steps=800&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler&#xA;import torch&#xA;&#xA;from svdiff_pytorch import load_unet_for_svdiff&#xA;&#xA;pretrained_model_name_or_path = &#34;runwayml/stable-diffusion-v1-5&#34;&#xA;spectral_shifts_ckpt = &#34;spectral_shifts.safetensors-path&#34;&#xA;unet = load_unet_for_svdiff(pretrained_model_name_or_path, spectral_shifts_ckpt=spectral_shifts_ckpt, subfolder=&#34;unet&#34;)&#xA;# load pipe&#xA;pipe = StableDiffusionPipeline.from_pretrained(&#xA;    pretrained_model_name_or_path,&#xA;    unet=unet,&#xA;)&#xA;pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)&#xA;pipe.to(&#34;cuda&#34;)&#xA;image = pipe(&#34;A picture of a sks dog in a bucket&#34;, num_inference_steps=25).images[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use the following CLI too! Once it&#39;s done, you will see &lt;code&gt;grid.png&lt;/code&gt; for the result.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py \&#xA;  --pretrained_model_name_or_path=&#34;runwayml/stable-diffusion-v1-5&#34;  \&#xA;  --spectral_shifts_ckpt=&#34;spectral_shifts.safetensors-path&#34;  \&#xA;  --prompt=&#34;A picture of a sks dog in a bucket&#34;  \&#xA;  --scheduler_type=&#34;dpm_solver++&#34;  \&#xA;  --num_inference_steps=25  \&#xA;  --num_images_per_prompt=2 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Gradio&lt;/h2&gt; &#xA;&lt;p&gt;You can also try SVDiff-pytorch in a UI with &lt;a href=&#34;https://gradio.app/&#34;&gt;gradio&lt;/a&gt;. This demo supports both training and inference!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/svdiff-library/SVDiff-Training-UI&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm.svg?sanitize=true&#34; alt=&#34;Open in Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to run it locally, run the following commands step by step.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone --recursive https://github.com/mkshing/svdiff-pytorch.git&#xA;$ cd scripts/gradio&#xA;$ pip install -r requirements.txt &#xA;$ export HF_TOKEN=&#34;YOUR_HF_TOKEN_HERE&#34;&#xA;$ python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Additional Features&lt;/h2&gt; &#xA;&lt;h3&gt;Spectral Shift Scaling&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mkshing/svdiff-pytorch/main/assets/scale.png&#34; alt=&#34;scale&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can adjust the strength of the weights by &lt;code&gt;--spectral_shifts_scale&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s a result for 0.8, 1.0, 1.2 (1.0 is the default). &lt;img src=&#34;https://raw.githubusercontent.com/mkshing/svdiff-pytorch/main/assets/scale-result.png&#34; alt=&#34;scale-result&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Fast prior generation by using ToMe&lt;/h3&gt; &#xA;&lt;p&gt;By using &lt;a href=&#34;https://github.com/dbolya/tomesd&#34;&gt;ToMe for SD&lt;/a&gt;, the prior generation can be faster!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install tomesd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And, add &lt;code&gt;--enable_tome_merging&lt;/code&gt; to your training arguments!&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{https://doi.org/10.48550/arXiv.2303.11305,&#xA;      title         = {SVDiff: Compact Parameter Space for Diffusion Fine-Tuning}, &#xA;      author        = {Ligong Han and Yinxiao Li and Han Zhang and Peyman Milanfar and Dimitris Metaxas and Feng Yang},&#xA;      year          = {2023},&#xA;      eprint        = {2303.11305},&#xA;      archivePrefix = {arXiv},&#xA;      primaryClass  = {cs.CV},&#xA;      url           = {https://arxiv.org/abs/2303.11305}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{hu2021lora,&#xA;      title         = {LoRA: Low-Rank Adaptation of Large Language Models},&#xA;      author        = {Hu, Edward and Shen, Yelong and Wallis, Phil and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Lu and Chen, Weizhu},&#xA;      year          = {2021},&#xA;      eprint        = {2106.09685},&#xA;      archivePrefix = {arXiv},&#xA;      primaryClass  = {cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{bolya2023tomesd,&#xA;      title   = {Token Merging for Fast Stable Diffusion},&#xA;      author  = {Bolya, Daniel and Hoffman, Judy},&#xA;      journal = {arXiv},&#xA;      url     = {https://arxiv.org/abs/2303.17604},&#xA;      year    = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/examples/dreambooth&#34;&gt;DreamBooth in diffusers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth&#34;&gt;DreamBooth in ShivamShrirao&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/adobe-research/custom-diffusion#getting-started&#34;&gt;Data from custom-diffusion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Training&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Scaling spectral shifts&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support multiple spectral shifts (Section 3.2)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Cut-Mix-Unmix (Section 3.3)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; SVDiff + LoRA&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>aliaksandr960/segment-anything-eo</title>
    <updated>2023-04-12T02:27:40Z</updated>
    <id>tag:github.com,2023-04-12:/aliaksandr960/segment-anything-eo</id>
    <link href="https://github.com/aliaksandr960/segment-anything-eo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Earth observation tools for Meta AI Segment Anything&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aliaksandr960/segment-anything-eo/main/title_sameo.png?raw=true&#34; alt=&#34;Automatic segmentation example&#34; title=&#34;Automatic segmentation example&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Segment Anything EO tools&lt;/h1&gt; &#xA;&lt;p&gt;Earth observation tools for Meta AI Segment Anything&lt;/p&gt; &#xA;&lt;h2&gt;This tools are developed to ease the processing of spatial data (GeoTIFF and TMS) with Meta AI Segment Anything models using sliding window algorithm for big files&lt;/h2&gt; &#xA;&lt;h3&gt;You can:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;download TMS data (including OpenAerialMap and Mapbox Maxar) as GeoTIFF files&lt;/li&gt; &#xA; &lt;li&gt;process GeoTIFF files with Meta AI Segment Anything models&lt;/li&gt; &#xA; &lt;li&gt;save predicted segments as GeoTIFF raster data and GPKG vector data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;and a little bit more&lt;/p&gt; &#xA;&lt;h3&gt;Usage:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;colab notebook &lt;a href=&#34;https://colab.research.google.com/drive/1RC1V68tD1O-YissBq9nOvS2PHEjAsFkA?usp=share_link&#34;&gt;https://colab.research.google.com/drive/1RC1V68tD1O-YissBq9nOvS2PHEjAsFkA?usp=share_link&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;jupyter notebook in the repo &lt;a href=&#34;https://github.com/aliaksandr960/segment-anything-eo/raw/main/basic_usage.ipynb&#34;&gt;https://github.com/aliaksandr960/segment-anything-eo/blob/main/basic_usage.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Technical details:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using a sliding window algorithm to process large images&lt;/li&gt; &#xA; &lt;li&gt;In order to separate instances, every instance gets surrounded by 1px width spare space, so it is not the same as how original Segment Anything works&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Segment Anything was released less than a week ago, and these are the first experiments with it. I don&#39;t know how paramters affect perfomance â€” feel free to change everything.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Licensing&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Facebook Research Segment Anything&lt;/a&gt; â€” Apache-2.0 license&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gumblex/tms2geotiff&#34;&gt;Gumblex tms2geotiff&lt;/a&gt; â€” BSD-2-Clause license&lt;/p&gt; &#xA;&lt;p&gt;Other code â€” MIT license&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Segment Anything and tms2geotiff were copied to this repo 9 Apr 2022, you can update them to more recent versions if needed&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Umar-Saeed-97/Vehicle-Count</title>
    <updated>2023-04-12T02:27:40Z</updated>
    <id>tag:github.com,2023-04-12:/Umar-Saeed-97/Vehicle-Count</id>
    <link href="https://github.com/Umar-Saeed-97/Vehicle-Count" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Object Detection | Object Tracking | YOLOv8 | ByteTrack&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Vehicle Count&lt;/h1&gt; &#xA;&lt;p&gt;The Vehicle Tracking project is an advanced computer vision system developed using Supervision that utilizes cutting-edge technologies such as YOLOv8 and ByteTracker to accurately detect and count vehicles in real-time video streams. By leveraging YOLOv8&#39;s object detection capabilities and ByteTracker&#39;s advanced tracking algorithms, the system is able to identify and track vehicles with high accuracy. The system is highly customizable, allowing users to adjust the parameters of the tracking algorithm to suit their specific needs. The output of the system is a visually appealing video stream that displays the vehicles being tracked. The system is built using the latest technologies in computer vision and machine learning, and is designed to be highly scalable, making it suitable for use in a wide range of applications, from traffic management to security surveillance.&lt;/p&gt; &#xA;&lt;h2&gt;Snippet&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Umar-Saeed-97/Vehicle-Count/raw/main/Data/Example%20Results/test1.gif&#34; alt=&#34;Example GIF&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;The following packages are required to run the notebook:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;numpy&lt;/li&gt; &#xA; &lt;li&gt;opencv-python&lt;/li&gt; &#xA; &lt;li&gt;ultralytics&lt;/li&gt; &#xA; &lt;li&gt;supervision&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can install these packages and dependencies using the following pip command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s how you can use the code in this project:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone or download the repository to your local machine.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the Jupyter Notebook file in the repository.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the cells in the notebook to make detections.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can modify the code to fit your needs.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;This project utilizes the popular YOLOv8 object detection model, which is pretrained on the COCO dataset and achieves high accuracy and speed by dividing the input image into a grid. Additionally, ByteTracker is used for vehicle tracking, a simple and efficient online multi-object tracking algorithm that combines detection and tracking in a unified framework. No custom training was done in this project.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt; &#xA;&lt;h2&gt;Author&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/umar-saeed-16863a21b/&#34;&gt;Umar Saeed&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>