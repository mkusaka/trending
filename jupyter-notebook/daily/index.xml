<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-31T01:35:43Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Abdullah-khan0/100-DaysOf-Code-dataScience</title>
    <updated>2024-12-31T01:35:43Z</updated>
    <id>tag:github.com,2024-12-31:/Abdullah-khan0/100-DaysOf-Code-dataScience</id>
    <link href="https://github.com/Abdullah-khan0/100-DaysOf-Code-dataScience" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>baaivision/tokenize-anything</title>
    <updated>2024-12-31T01:35:43Z</updated>
    <id>tag:github.com,2024-12-31:/baaivision/tokenize-anything</id>
    <link href="https://github.com/baaivision/tokenize-anything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tokenize Anything via Prompting&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;Tokenize Anything via Prompting&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/PhyscalX/&#34;&gt;Ting Pan&lt;/a&gt;&lt;sup&gt;1,2*&lt;/sup&gt;, &amp;nbsp; &lt;a href=&#34;https://github.com/lulutang0608&#34;&gt;Lulu Tang&lt;/a&gt;&lt;sup&gt;2*&lt;/sup&gt;, &amp;nbsp; &lt;a href=&#34;https://www.xloong.wang/&#34;&gt;Xinlong Wang&lt;/a&gt;&lt;sup&gt;2Â¶&lt;/sup&gt;, &amp;nbsp; &lt;a href=&#34;https://scholar.google.com/citations?user=Vkzd7MIAAAAJ&amp;amp;hl=en&#34;&gt;Shiguang Shan&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;a href=&#34;http://english.ict.cas.cn/&#34;&gt;ICT-CAS&lt;/a&gt;, &amp;nbsp; &lt;sup&gt;2&lt;/sup&gt;&lt;a href=&#34;https://www.baai.ac.cn/english.html&#34;&gt;BAAI&lt;/a&gt;&lt;br&gt; &lt;sup&gt;*&lt;/sup&gt; Equal Contribution, &lt;sup&gt;Â¶&lt;/sup&gt;Project Lead&lt;/p&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;https://arxiv.org/pdf/2312.09128.pdf&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/BAAI/tokenize-anything&#34;&gt;&lt;code&gt;ðŸ¤— Demo&lt;/code&gt;&lt;/a&gt;] &lt;br&gt;&lt;br&gt;&lt;img src=&#34;https://raw.githubusercontent.com/baaivision/tokenize-anything/main/assets/model_overview.png&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We present &lt;strong&gt;T&lt;/strong&gt;okenize &lt;strong&gt;A&lt;/strong&gt;nything via &lt;strong&gt;P&lt;/strong&gt;rompting, a unified and promptable model capable of simultaneously segmenting, recognizing, and captioning arbitrary regions, with flexible visual prompts (point, box and sketch). The model is trained with exhaustive segmentation masks sourced from SA-1B, coupled with semantic priors from a pre-trained EVA-CLIP with 5 billion parameters.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Preliminaries&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;torch&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;flash-attn&lt;/code&gt; &amp;gt;= 2.3.3 (Install the pre-built wheel distribution from &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention/releases&#34;&gt;URL&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;gradio-image-prompter&lt;/code&gt; (for GradioApp, Install from &lt;a href=&#34;https://github.com/PhyscalX/gradio-image-prompter&#34;&gt;URL&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h3&gt;Installing Package&lt;/h3&gt; &#xA;&lt;p&gt;Clone this repository to local disk and install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd tokenize-anything &amp;amp;&amp;amp; pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also install from the remote repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+ssh://git@github.com/baaivision/tokenize-anything.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Development&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;strong&gt;TAP&lt;/strong&gt; models can be used for diverse vision and language tasks.&lt;/p&gt; &#xA;&lt;p&gt;We adopt a modular design that decouples all components and predictors.&lt;/p&gt; &#xA;&lt;p&gt;As a best practice, implement your custom predictor and asynchronous pipeline as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tokenize_anything import model_registry&#xA;&#xA;with &amp;lt;distributed_actor&amp;gt;:&#xA;    model = model_registry[&#34;&amp;lt;model_type&amp;gt;&#34;](checkpoint=&#34;&amp;lt;path/to/checkpoint&amp;gt;&#34;)&#xA;    results = &amp;lt;custom_predictor&amp;gt;(model, *args, **kwargs)&#xA;&#xA;server.collect_results()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See builtin examples (web-demo and evaluations) provided in &lt;a href=&#34;https://raw.githubusercontent.com/baaivision/tokenize-anything/main/scripts/&#34;&gt;scripts&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/baaivision/tokenize-anything/main/notebooks/inference.ipynb&#34;&gt;Inference Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/baaivision/tokenize-anything/main/notebooks/concept.ipynb&#34;&gt;Concept Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/baaivision/tokenize-anything/main/notebooks/evaluation_tap_vit_l.ipynb&#34;&gt;Evaluation Guide for TAP-L&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/baaivision/tokenize-anything/main/notebooks/evaluation_tap_vit_b.ipynb&#34;&gt;Evaluation Guide for TAP-B&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;h3&gt;Model weights&lt;/h3&gt; &#xA;&lt;p&gt;Two versions of the model are available with different image encoders.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;MD5&lt;/th&gt; &#xA;   &lt;th&gt;Weights&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;tap_vit_l&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ViT-L TAP model&lt;/td&gt; &#xA;   &lt;td&gt;03f8ec&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/BAAI/tokenize-anything/blob/main/models/tap_vit_l_03f8ec.pkl&#34;&gt;ðŸ¤— HF link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;tap_vit_b&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ViT-B TAP model&lt;/td&gt; &#xA;   &lt;td&gt;b45cbf&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/BAAI/tokenize-anything/blob/main/models/tap_vit_b_b45cbf.pkl&#34;&gt;ðŸ¤— HF link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Concept weights&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/em&gt;: You can generate these weights following the &lt;a href=&#34;https://raw.githubusercontent.com/baaivision/tokenize-anything/main/notebooks/concept.ipynb&#34;&gt;Concept Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Concept&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Weights&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Merged-2560&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Merged concepts&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/BAAI/tokenize-anything/blob/main/concepts/merged_2560.pkl&#34;&gt;ðŸ¤— HF link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;LVIS-1203&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LVIS concepts&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/BAAI/tokenize-anything/blob/main/concepts/lvis_1203.pkl&#34;&gt;ðŸ¤— HF link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;COCO-80&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;COCO concepts&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/BAAI/tokenize-anything/blob/main/concepts/coco_80.pkl&#34;&gt;ðŸ¤— HF link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;We are looking&lt;/strong&gt; for research interns at BAAI Vision Team. If you are interested in working with us on &lt;strong&gt;Vision Foundation Models&lt;/strong&gt; (e.g., SAM variants), please contact &lt;a href=&#34;https://www.xloong.wang/&#34;&gt;Xinlong Wang&lt;/a&gt; (&lt;code&gt;wangxinlong@baai.ac.cn&lt;/code&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/baaivision/tokenize-anything/main/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{pan2023tap,&#xA;  title={Tokenize Anything via Prompting},&#xA;  author={Pan, Ting and Tang, Lulu and Wang, Xinlong and Shan, Shiguang},&#xA;  journal={arXiv preprint arXiv:2312.09128},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We thank the repositories: &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt;, &lt;a href=&#34;https://github.com/baaivision/EVA&#34;&gt;EVA&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt;, &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention&lt;/a&gt;, &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34;&gt;Detectron2&lt;/a&gt; and &lt;a href=&#34;https://github.com/seetacloud/codewithgpu&#34;&gt;CodeWithGPU&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>quantrocket-codeload/quant-finance-lectures</title>
    <updated>2024-12-31T01:35:43Z</updated>
    <id>tag:github.com,2024-12-31:/quantrocket-codeload/quant-finance-lectures</id>
    <link href="https://github.com/quantrocket-codeload/quant-finance-lectures" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn quantitative finance with this comprehensive lecture series. Adapted from the Quantopian Lecture Series. Uses free sample data.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;quant-finance-lectures&lt;/h1&gt; &#xA;&lt;p&gt;Learn quantitative finance with this comprehensive lecture series. Adapted from the Quantopian Lecture Series. Uses free sample data.&lt;/p&gt; &#xA;&lt;h2&gt;Clone in QuantRocket&lt;/h2&gt; &#xA;&lt;p&gt;CLI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;quantrocket codeload clone &#39;quant-finance-lectures&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from quantrocket.codeload import clone&#xA;clone(&#34;quant-finance-lectures&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Browse in GitHub&lt;/h2&gt; &#xA;&lt;p&gt;Start here: &lt;a href=&#34;https://raw.githubusercontent.com/quantrocket-codeload/quant-finance-lectures/master/quant_finance_lectures/Introduction.ipynb&#34;&gt;quant_finance_lectures/Introduction.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Find more code in QuantRocket&#39;s &lt;a href=&#34;https://www.quantrocket.com/code/&#34;&gt;Code Library&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>