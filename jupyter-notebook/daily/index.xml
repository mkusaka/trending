<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-29T01:39:26Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Lux-AI-Challenge/Lux-Design-S2</title>
    <updated>2023-01-29T01:39:26Z</updated>
    <id>tag:github.com,2023-01-29:/Lux-AI-Challenge/Lux-Design-S2</id>
    <link href="https://github.com/Lux-AI-Challenge/Lux-Design-S2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repository for the Lux AI Challenge, season 2&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Lux-Design-S2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/luxai_s2&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/luxai_s2.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome to the Lux AI Challenge Season 2!&lt;/p&gt; &#xA;&lt;p&gt;The Lux AI Challenge is a competition where competitors design agents to tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. In addition to optimization, successful agents must be capable of analyzing their opponents and developing appropriate policies to get the upper hand.&lt;/p&gt; &#xA;&lt;p&gt;Key features this season!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPU/TPU optimized environment via Jax&lt;/li&gt; &#xA; &lt;li&gt;Asymmetric maps and novel mechanics (action efficiency and planning)&lt;/li&gt; &#xA; &lt;li&gt;$55,000 Prize Pool&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Go to our &lt;a href=&#34;https://raw.githubusercontent.com/Lux-AI-Challenge/Lux-Design-S2/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; section to get started programming a bot. The official competition runs until April 24th and submissions are due at 11:59PM UTC on the competition page: &lt;a href=&#34;https://www.kaggle.com/competitions/lux-ai-season-2&#34;&gt;https://www.kaggle.com/competitions/lux-ai-season-2&lt;/a&gt;. There is a &lt;strong&gt;$55,000&lt;/strong&gt; prize pool this year thanks to contributions from Kaggle, and our sponsors &lt;a href=&#34;https://quantco.com/&#34;&gt;QuantCo&lt;/a&gt;, &lt;a href=&#34;https://www.regression.gg/&#34;&gt;Regression Games&lt;/a&gt;, and &lt;a href=&#34;https://tsvcap.com&#34;&gt;TSVC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Make sure to join our community discord at &lt;a href=&#34;https://discord.gg/aWJt3UAcgn&#34;&gt;https://discord.gg/aWJt3UAcgn&lt;/a&gt; to chat, strategize, and learn with other competitors! We will be posting announcements on the Kaggle Forums and on the discord.&lt;/p&gt; &#xA;&lt;p&gt;Season 2 specifications can be found here: &lt;a href=&#34;https://lux-ai.org/specs-s2&#34;&gt;https://lux-ai.org/specs-s2&lt;/a&gt;. These detail how the game works and what rules your agent must abide by.&lt;/p&gt; &#xA;&lt;p&gt;Interested in Season 1? Check out &lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-2021&#34;&gt;last year&#39;s repository&lt;/a&gt; where we received 22,000+ submissions from 1,100+ teams around the world ranging from scripted agents to Deep Reinforcement Learning.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;You will need Python &amp;gt;=3.7, &amp;lt;3.11 installed on your system. Once installed, you can install the Lux AI season 2 environment and optionally the GPU version with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --upgrade luxai_s2&#xA;pip install juxai-s2 # installs the GPU version, requires a compatible GPU&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To verify your installation, you can run the CLI tool by replacing &lt;code&gt;path/to/bot/main.py&lt;/code&gt; with a path to a bot (e.g. the starter kit in &lt;code&gt;kits/python/main.py&lt;/code&gt;) and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;luxai-s2 path/to/bot/main.py path/to/bot/main.py -v 2 -o replay.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will turn on logging to level 2, and store the replay file at &lt;code&gt;replay.json&lt;/code&gt;. For documentation on the luxai-s2 tool, see the &lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/luxai_s2/luxai_runner/README.md&#34;&gt;tool&#39;s README&lt;/a&gt;, which also includes details on how to run a local tournament to mass evaluate your agents. To watch the replay, upload &lt;code&gt;replay.json&lt;/code&gt; to &lt;a href=&#34;https://s2vis.lux-ai.org/&#34;&gt;https://s2vis.lux-ai.org/&lt;/a&gt; (or change &lt;code&gt;-o replay.json&lt;/code&gt; to &lt;code&gt;-o replay.html&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Each supported programming language/solution type has its own starter kit, you can find general &lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits&#34;&gt;API documentation here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The kits folder in this repository holds all of the available starter kits you can use to start competing and building an AI agent. The readme shows you how to get started with your language of choice and run a match. We strongly recommend reading through the documentation for your language of choice in the links below&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/python/&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/cpp/&#34;&gt;C++&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/js/&#34;&gt;Javascript&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/java/&#34;&gt;Java&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rooklift/golux2/&#34;&gt;Go&lt;/a&gt; - (A working bare-bones Go kit)&lt;/li&gt; &#xA; &lt;li&gt;Typescript - TBA&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- - [Reinforcement Learning (Python)](https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/rl-sb3/) and [Reinforcement Learning (Python + Jax Env)](https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/rl-sb3-jax-env/) --&gt; &#xA;&lt;p&gt;Want to use another language but it&#39;s not supported? Feel free to suggest that language to our issues or even better, create a starter kit for the community to use and make a PR to this repository. See our &lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; document for more information on this.&lt;/p&gt; &#xA;&lt;!-- Finally, if you want to learn how to use the GPU optimized env see https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/examples/jax_env_tutorial.ipynb&#xA;&#xA;For the RL starter kit that trains using the jax env, see https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/rl-sb3-jax-env/ --&gt; &#xA;&lt;p&gt;To stay up to date on changes and updates to the competition and the engine, watch for announcements on the forums or the &lt;a href=&#34;https://discord.gg/aWJt3UAcgn&#34;&gt;Discord&lt;/a&gt;. See &lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/raw/main/ChangeLog.md&#34;&gt;ChangeLog.md&lt;/a&gt; for a full change log.&lt;/p&gt; &#xA;&lt;h2&gt;Community Tools&lt;/h2&gt; &#xA;&lt;p&gt;As the community builds tools for the competition, we will post them here!&lt;/p&gt; &#xA;&lt;p&gt;3rd Party Viewer (This has now been merged into the main repo so check out the lux-eye-s2 folder) - &lt;a href=&#34;https://github.com/jmerle/lux-eye-2022&#34;&gt;https://github.com/jmerle/lux-eye-2022&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/Lux-AI-Challenge/Lux-Design-S2/raw/main/CONTRIBUTING.md&#34;&gt;guide on contributing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sponsors&lt;/h2&gt; &#xA;&lt;p&gt;We are proud to announce our sponsors &lt;a href=&#34;https://quantco.com/&#34;&gt;QuantCo&lt;/a&gt;, &lt;a href=&#34;https://www.regression.gg/&#34;&gt;Regression Games&lt;/a&gt;, and &lt;a href=&#34;https://tsvcap.com&#34;&gt;TSVC&lt;/a&gt;. They help contribute to the prize pool and provide exciting opportunities to our competitors! For more information about them check out &lt;a href=&#34;https://www.lux-ai.org/sponsors-s2&#34;&gt;https://www.lux-ai.org/sponsors-s2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Core Contributors&lt;/h2&gt; &#xA;&lt;p&gt;We like to extend thanks to some of our early core contributors: &lt;a href=&#34;https://github.com/duanwilliam&#34;&gt;@duanwilliam&lt;/a&gt; (Frontend), &lt;a href=&#34;https://github.com/programjames&#34;&gt;@programjames&lt;/a&gt; (Map generation, Engine optimization), and &lt;a href=&#34;https://github.com/themmj&#34;&gt;@themmj&lt;/a&gt; (C++ kit, Engine optimization).&lt;/p&gt; &#xA;&lt;p&gt;We further like to extend thanks to some of our core contributors during the beta period: &lt;a href=&#34;https://github.com/LeFiz&#34;&gt;@LeFiz&lt;/a&gt; (Game Design/Architecture), &lt;a href=&#34;https://github.com/jmerle&#34;&gt;@jmerle&lt;/a&gt; (Visualizer)&lt;/p&gt; &#xA;&lt;p&gt;We further like to thank the following contributors during the official competition: &lt;a href=&#34;https://github.com/paradite&#34;&gt;@aradite&lt;/a&gt;(JS Kit), &lt;a href=&#34;https://github.com/MountainOrc&#34;&gt;@MountainOrc&lt;/a&gt;(Java Kit), &lt;a href=&#34;https://github.com/ArturBloch&#34;&gt;@ArturBloch&lt;/a&gt;(Java Kit), &lt;a href=&#34;https://github.com/rooklift&#34;&gt;@rooklift&lt;/a&gt;(Go Kit)&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use the Lux AI Season 2 environment in your work, please cite this repository as so&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{Lux_AI_Challenge_S1,&#xA;  author = {Tao, Stone and Doerschuk-Tiberi, Bovard},&#xA;  month = {10},&#xA;  title = {{Lux AI Challenge Season 2}},&#xA;  url = {https://github.com/Lux-AI-Challenge/Lux-Design-S2},&#xA;  version = {1.0.0},&#xA;  year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>pangeo-data/WeatherBench</title>
    <updated>2023-01-29T01:39:26Z</updated>
    <id>tag:github.com,2023-01-29:/pangeo-data/WeatherBench</id>
    <link href="https://github.com/pangeo-data/WeatherBench" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A benchmark dataset for data-driven weather forecasting&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/ai4environment/WeatherBench/raw/master/figures/logo_text_left.png?raw=true&#34; alt=&#34;Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;WeatherBench: A benchmark dataset for data-driven weather forecasting&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://binder.pangeo.io/v2/gh/pangeo-data/WeatherBench/master?filepath=quickstart.ipynb&#34;&gt;&lt;img src=&#34;https://binder.pangeo.io/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are using this dataset please cite&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Stephan Rasp, Peter D. Dueben, Sebastian Scher, Jonathan A. Weyn, Soukayna Mouatadid, and Nils Thuerey, 2020. WeatherBench: A benchmark dataset for data-driven weather forecasting. arXiv: &lt;a href=&#34;https://arxiv.org/abs/2002.00469&#34;&gt;https://arxiv.org/abs/2002.00469&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This repository contains all the code for downloding and processing the data as well as code for the baseline models in the paper.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;Note! The data has been changed from the original release. Here is a list of changes:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;New vertical levels. Used to be [1, 10, 100, 200, 300, 400, 500, 600, 700, 850, 1000], now is [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]. This is to be compatible with CMIP output. The new levels include all of the old ones with the exception of [1, 10].&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;CMIP data. Regridded CMIP data of some variables was added. This is the historical simulation of the MPI-ESM-HR model.&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If you have any questions about this dataset, please use the &lt;a href=&#34;https://github.com/pangeo-data/WeatherBench/issues&#34;&gt;Github Issue&lt;/a&gt; feature on this page!&lt;/p&gt; &#xA;&lt;h2&gt;Leaderboard&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Z500 RMSE (3 / 5 days) [m&lt;sup&gt;2&lt;/sup&gt;/s&lt;sup&gt;2&lt;/sup&gt;]&lt;/th&gt; &#xA;   &lt;th&gt;T850 RMSE (3 / 5 days) [K]&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Operational IFS&lt;/td&gt; &#xA;   &lt;td&gt;154 / 334&lt;/td&gt; &#xA;   &lt;td&gt;1.36 / 2.03&lt;/td&gt; &#xA;   &lt;td&gt;ECWMF physical model (10 km)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.00469&#34;&gt;Rasp et al. 2020&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rasp and Thuerey 2020 (direct/continuous)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;268 / 499&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.65 / 2.41&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Resnet with CMIP pretraining (5.625 deg)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://arxiv.org/abs/2008.08626&#34;&gt;Rasp and Thuerey 2020&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IFS T63&lt;/td&gt; &#xA;   &lt;td&gt;268 / 463&lt;/td&gt; &#xA;   &lt;td&gt;1.85 / 2.52&lt;/td&gt; &#xA;   &lt;td&gt;Lower resolution physical model (approx. 1.9 deg)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.00469&#34;&gt;Rasp et al. 2020&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Weyn et al. 2020 (iterative)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;373 / 611&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.98 / 2.87&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;UNet with cube-sphere mapping (2 deg)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002109&#34;&gt;Weyn et al. 2020&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Clare et al. 2021 (direct)&lt;/td&gt; &#xA;   &lt;td&gt;375 / 627&lt;/td&gt; &#xA;   &lt;td&gt;2.11 / 2.91&lt;/td&gt; &#xA;   &lt;td&gt;Stacked ResNets with probabilistic output (5.625 deg)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/qj.4180&#34;&gt;Clare et al. 2021&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IFS T42&lt;/td&gt; &#xA;   &lt;td&gt;489 / 743&lt;/td&gt; &#xA;   &lt;td&gt;3.09 / 3.83&lt;/td&gt; &#xA;   &lt;td&gt;Lower resolution physical model (approx. 2.8 deg)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.00469&#34;&gt;Rasp et al. 2020&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Weekly climatology&lt;/td&gt; &#xA;   &lt;td&gt;816&lt;/td&gt; &#xA;   &lt;td&gt;3.50&lt;/td&gt; &#xA;   &lt;td&gt;Climatology for each calendar week&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.00469&#34;&gt;Rasp et al. 2020&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Persistence&lt;/td&gt; &#xA;   &lt;td&gt;936 / 1033&lt;/td&gt; &#xA;   &lt;td&gt;4.23 / 4.56&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.00469&#34;&gt;Rasp et al. 2020&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Climatology&lt;/td&gt; &#xA;   &lt;td&gt;1075&lt;/td&gt; &#xA;   &lt;td&gt;5.51&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.00469&#34;&gt;Rasp et al. 2020&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;You can follow the quickstart guide in &lt;a href=&#34;https://github.com/pangeo-data/WeatherBench/raw/master/quickstart.ipynb&#34;&gt;this notebook&lt;/a&gt; or lauch it directly from &lt;a href=&#34;https://binder.pangeo.io/v2/gh/pangeo-data/WeatherBench/master?filepath=quickstart.ipynb&#34;&gt;Binder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Download the data&lt;/h2&gt; &#xA;&lt;p&gt;The data is hosted &lt;a href=&#34;https://mediatum.ub.tum.de/1524895&#34;&gt;here&lt;/a&gt; with the following directory structure&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;|-- 1.40625deg&#xA;|   |-- 10m_u_component_of_wind&#xA;|   |-- 10m_v_component_of_wind&#xA;|   |-- 2m_temperature&#xA;|   |-- constants&#xA;|   |-- geopotential&#xA;|   |-- old&#xA;|   |   `-- temperature&#xA;|   |-- potential_vorticity&#xA;|   |-- relative_humidity&#xA;|   |-- specific_humidity&#xA;|   |-- temperature&#xA;|   |-- toa_incident_solar_radiation&#xA;|   |-- total_cloud_cover&#xA;|   |-- total_precipitation&#xA;|   |-- u_component_of_wind&#xA;|   |-- v_component_of_wind&#xA;|   `-- vorticity&#xA;|-- 2.8125deg&#xA;|   |-- 10m_u_component_of_wind&#xA;|   |-- 10m_v_component_of_wind&#xA;|   |-- 2m_temperature&#xA;|   |-- constants&#xA;|   |-- geopotential&#xA;|   |-- potential_vorticity&#xA;|   |-- relative_humidity&#xA;|   |-- specific_humidity&#xA;|   |-- temperature&#xA;|   |-- toa_incident_solar_radiation&#xA;|   |-- total_cloud_cover&#xA;|   |-- total_precipitation&#xA;|   |-- u_component_of_wind&#xA;|   |-- v_component_of_wind&#xA;|   `-- vorticity&#xA;|-- 5.625deg&#xA;|   |-- 10m_u_component_of_wind&#xA;|   |-- 10m_v_component_of_wind&#xA;|   |-- 2m_temperature&#xA;|   |-- constants&#xA;|   |-- geopotential&#xA;|   |-- geopotential_500&#xA;|   |-- potential_vorticity&#xA;|   |-- relative_humidity&#xA;|   |-- specific_humidity&#xA;|   |-- temperature&#xA;|   |-- temperature_850&#xA;|   |-- toa_incident_solar_radiation&#xA;|   |-- total_cloud_cover&#xA;|   |-- total_precipitation&#xA;|   |-- u_component_of_wind&#xA;|   |-- v_component_of_wind&#xA;|   `-- vorticity&#xA;|-- baselines&#xA;|   `-- saved_models&#xA;|-- CMIP&#xA;|   `-- MPI-ESM&#xA;|       |-- 2.8125deg&#xA;|       |   |-- geopotential&#xA;|       |   |-- specific_humidity&#xA;|       |   |-- temperature&#xA;|       |   |-- u_component_of_wind&#xA;|       |   `-- v_component_of_wind&#xA;|       `-- 5.625deg&#xA;|           |-- geopotential&#xA;|           |-- specific_humidity&#xA;|           |-- temperature&#xA;|           |-- u_component_of_wind&#xA;|           `-- v_component_of_wind&#xA;|-- IFS_T42&#xA;|   `-- raw&#xA;|-- IFS_T63&#xA;|   `-- raw&#xA;`-- tigge&#xA;    |-- 1.40625deg&#xA;    |   |-- geopotential_500&#xA;    |   `-- temperature_850&#xA;    |-- 2.8125deg&#xA;    |   |-- geopotential_500&#xA;    |   `-- temperature_850&#xA;    `-- 5.625deg&#xA;        |-- 2m_temperature&#xA;        |-- geopotential_500&#xA;        |-- temperature_850&#xA;        `-- total_precipitation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To start out download either the entire 5.625 degree data (175G) using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget &#34;https://dataserv.ub.tum.de/s/m1524895/download?path=%2F5.625deg&amp;amp;files=all_5.625deg.zip&#34; -O all_5.625deg.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or simply the single level (500 hPa) geopotential data using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget &#34;https://dataserv.ub.tum.de/s/m1524895/download?path=%2F5.625deg%2Fgeopotential_500&amp;amp;files=geopotential_500_5.625deg.zip&#34; -O geopotential_500_5.625deg.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and then unzip the files using &lt;code&gt;unzip &amp;lt;file&amp;gt;.zip&lt;/code&gt;. You can also use &lt;code&gt;ftp&lt;/code&gt; or &lt;code&gt;rsync&lt;/code&gt; to download the data. For instructions, follow the &lt;a href=&#34;https://mediatum.ub.tum.de/1524895&#34;&gt;download link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Baselines and evaluation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT:&lt;/strong&gt; The format of the predictions file is a NetCDF dataset with dimensions &lt;code&gt;[init_time, lead_time, lat, lon]&lt;/code&gt;. Consult the notebooks for examples. You are stongly encouraged to format your predictions in the same way and then use the same evaluation functions to ensure consistent evaluation.&lt;/p&gt; &#xA;&lt;h3&gt;Baselines&lt;/h3&gt; &#xA;&lt;p&gt;The baselines are created using Jupyter notebooks in &lt;code&gt;notebooks/&lt;/code&gt;. In all notebooks, the forecasts are saved as a NetCDF file in the &lt;code&gt;predictions&lt;/code&gt; directory of the dataset.&lt;/p&gt; &#xA;&lt;h3&gt;CNN baselines&lt;/h3&gt; &#xA;&lt;p&gt;An example of how to load the data and train a CNN using Keras is given in &lt;code&gt;notebooks/3-cnn-example.ipynb&lt;/code&gt;. In addition a command line script for training CNNs is provided in &lt;code&gt;src/train_nn.py&lt;/code&gt;. For the baseline CNNs in the paper the config files are given in &lt;code&gt;src/nn_configs/&lt;/code&gt;. To reproduce the results in the paper run e.g. &lt;code&gt;python -m src.train_nn -c src/nn_configs/fccnn_3d.yml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Evaluation and comparison of the different baselines in done in &lt;code&gt;notebooks/4-evaluation.ipynb&lt;/code&gt;. The scoring is done using the functions in &lt;code&gt;src/score.py&lt;/code&gt;. The RMSE values for the baseline models are also saved in the &lt;code&gt;predictions &lt;/code&gt; directory of the dataset. This is useful for plotting your own models alongside the baselines.&lt;/p&gt; &#xA;&lt;h2&gt;Data processing&lt;/h2&gt; &#xA;&lt;p&gt;The dataset already contains the most important processed data. If you would like to download a different variable , regrid to a different resolution or extract single levels from the 3D files, here is how to do that!&lt;/p&gt; &#xA;&lt;h3&gt;Downloading and processing the raw data from the ERA5 archive&lt;/h3&gt; &#xA;&lt;p&gt;The workflow to get to the processed data that ended up in the data repository above is:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download monthly files from the ERA5 archive (&lt;code&gt;src/download.py&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Regrid the raw data to the required resolutions (&lt;code&gt;src/regrid.py&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The raw data is from the ERA5 reanalysis archive. Information on how to download the data can be found &lt;a href=&#34;https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://cds.climate.copernicus.eu/api-how-to&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Because downloading the data can take a long time (several weeks), the workflow is encoded using &lt;a href=&#34;https://snakemake.readthedocs.io/&#34;&gt;Snakemake&lt;/a&gt;. See &lt;code&gt;Snakefile&lt;/code&gt; and the configuration files for each variable in &lt;code&gt;scripts/config_ {variable}.yml&lt;/code&gt;. These files can be modified if additional variables are required. To execute Snakemake for a particular variable type : &lt;code&gt;snakemake -p -j 4 all --configfile scripts/config_toa_incident_solar_radiation.yml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In addition to the time-dependent fields, the constant fields were downloaded and processed using &lt;code&gt;scripts /download_and_regrid_constants.sh&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Downloading the TIGGE IFS baseline&lt;/h3&gt; &#xA;&lt;p&gt;To obtain the operational IFS baseline, we use the &lt;a href=&#34;https://confluence.ecmwf.int/display/TIGGE&#34;&gt;TIGGE Archive&lt;/a&gt;. Downloading the data for Z500 and T850 is done in &lt;code&gt;scripts/download_tigge.py&lt;/code&gt;; regridding is done in &lt;code&gt;scripts /convert_and_regrid_tigge.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Regridding the T21 IFS baseline&lt;/h3&gt; &#xA;&lt;p&gt;The T21 baseline was created by Peter Dueben. The raw output can be found in the dataset. To regrid the data &lt;code&gt;scripts /convert_and_regrid_IFS_TXX.sh&lt;/code&gt; was used.&lt;/p&gt; &#xA;&lt;h3&gt;Downloading and regridding CMIP historical climate model data.&lt;/h3&gt; &#xA;&lt;p&gt;To download historical climate model data use the Snakemake file in &lt;code&gt;snakemake_configs_CMIP&lt;/code&gt;. Here, we downloaded data from the &lt;code&gt;MIP-ESM-HR&lt;/code&gt; model. To download other models, search for the download links on the CMIP website and modify the scripts accordingly.&lt;/p&gt; &#xA;&lt;h3&gt;Extracting single levels from 3D files&lt;/h3&gt; &#xA;&lt;p&gt;If you would like to extract a single level from 3D data, e.g. 850 hPa temperature, you can use &lt;code&gt;src /extract_level.py&lt;/code&gt;. This could be useful to reduce the amount of data that needs to be loaded into RAM. An example usage would be: &lt;code&gt;python extract_level.py --input_fns DATADIR/5.625deg/temperature/*.nc --output_dir OUTDIR --level 850&lt;/code&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tensorflow/models</title>
    <updated>2023-01-29T01:39:26Z</updated>
    <id>tag:github.com,2023-01-29:/tensorflow/models</id>
    <link href="https://github.com/tensorflow/models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Models and examples built with TensorFlow&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://storage.googleapis.com/tf_model_garden/tf_model_garden_logo.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/tensorflow&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/tf-models-official&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/tf-models-official.svg?sanitize=true&#34; alt=&#34;tf-models-official PyPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Welcome to the Model Garden for TensorFlow&lt;/h1&gt; &#xA;&lt;p&gt;The TensorFlow Model Garden is a repository with a number of different implementations of state-of-the-art (SOTA) models and modeling solutions for TensorFlow users. We aim to demonstrate the best practices for modeling so that TensorFlow users can take full advantage of TensorFlow for their research and product development.&lt;/p&gt; &#xA;&lt;p&gt;To improve the transparency and reproducibility of our models, training logs on &lt;a href=&#34;https://tensorboard.dev&#34;&gt;TensorBoard.dev&lt;/a&gt; are also provided for models to the extent possible though not all models are suitable.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Directory&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/models/master/official&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;• A collection of example implementations for SOTA models using the latest TensorFlow 2&#39;s high-level APIs&lt;br&gt;• Officially maintained, supported, and kept up to date with the latest TensorFlow 2 APIs by TensorFlow&lt;br&gt;• Reasonably optimized for fast performance while still being easy to read&lt;br&gt; For more details on the capabilities, check the guide on the &lt;a href=&#34;https://www.tensorflow.org/tfmodels&#34;&gt;Model-garden&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/models/master/research&#34;&gt;research&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;• A collection of research model implementations in TensorFlow 1 or 2 by researchers&lt;br&gt;• Maintained and supported by researchers&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/models/master/community&#34;&gt;community&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;• A curated list of the GitHub repositories with machine learning models and implementations powered by TensorFlow 2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/models/master/orbit&#34;&gt;orbit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;• A flexible and lightweight library that users can easily use or fork when writing customized training loop code in TensorFlow 2.x. It seamlessly integrates with &lt;code&gt;tf.distribute&lt;/code&gt; and supports running on different device types (CPU, GPU, and TPU).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install the current release of tensorflow-models, please follow any one of the methods described below.&lt;/p&gt; &#xA;&lt;h4&gt;Method 1: Install the TensorFlow Model Garden pip package&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;&lt;strong&gt;tf-models-official&lt;/strong&gt; is the stable Model Garden package. Please check out the &lt;a href=&#34;https://github.com/tensorflow/models/releases&#34;&gt;releases&lt;/a&gt; to see what are available modules.&lt;/p&gt; &#xA; &lt;p&gt;pip3 will install all models and dependencies automatically.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install tf-models-official&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Please check out our examples:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/models/raw/master/tensorflow_models/tensorflow_models_pypi.ipynb&#34;&gt;basic library import&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/models/raw/master/docs/nlp/index.ipynb&#34;&gt;nlp model building&lt;/a&gt; to learn how to use a PIP package.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Note that &lt;strong&gt;tf-models-official&lt;/strong&gt; may not include the latest changes in the master branch of this github repo. To include latest changes, you may install &lt;strong&gt;tf-models-nightly&lt;/strong&gt;, which is the nightly Model Garden package created daily automatically.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install tf-models-nightly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Method 2: Clone the source&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Clone the GitHub repository:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/tensorflow/models.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Add the top-level &lt;em&gt;&lt;strong&gt;/models&lt;/strong&gt;&lt;/em&gt; folder to the Python path.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export PYTHONPATH=$PYTHONPATH:/path/to/models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If you are using in a Windows environment, you may need to use the following command with PowerShell:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$env:PYTHONPATH += &#34;:\path\to\models&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If you are using a Colab notebook, please set the Python path with os.environ.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;os.environ[&#39;PYTHONPATH&#39;] += &#34;:/path/to/models&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Install other dependencies&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install --user -r models/official/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Finally, if you are using nlp packages, please also install &lt;strong&gt;tensorflow-text-nightly&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install tensorflow-text-nightly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Announcements&lt;/h2&gt; &#xA;&lt;p&gt;Please check &lt;a href=&#34;https://github.com/tensorflow/models/wiki/Announcements&#34;&gt;this page&lt;/a&gt; for recent announcements.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tensorflow/models/labels/help%20wanted%3Apaper%20implementation&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/tensorflow/models/help%20wanted%3Apaper%20implementation&#34; alt=&#34;help wanted:paper implementation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to contribute, please review the &lt;a href=&#34;https://github.com/tensorflow/models/wiki/How-to-contribute&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/models/master/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citing TensorFlow Model Garden&lt;/h2&gt; &#xA;&lt;p&gt;If you use TensorFlow Model Garden in your research, please cite this repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{tensorflowmodelgarden2020,&#xA;  author = {Hongkun Yu, Chen Chen, Xianzhi Du, Yeqing Li, Abdullah Rashwan, Le Hou, Pengchong Jin, Fan Yang,&#xA;            Frederick Liu, Jaeyoun Kim, and Jing Li},&#xA;  title = {{TensorFlow Model Garden}},&#xA;  howpublished = {\url{https://github.com/tensorflow/models}},&#xA;  year = {2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>