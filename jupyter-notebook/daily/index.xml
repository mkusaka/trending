<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-02T01:45:56Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xianhu/LearnPython</title>
    <updated>2022-08-02T01:45:56Z</updated>
    <id>tag:github.com,2022-08-02:/xianhu/LearnPython</id>
    <link href="https://github.com/xianhu/LearnPython" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ä»¥æ’¸ä»£ç çš„å½¢å¼å­¦ä¹ Python&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LearnPython&lt;/h1&gt; &#xA;&lt;p&gt;ä»¥æ’¸ä»£ç çš„å½¢å¼å­¦ä¹ Python, å…·ä½“è¯´æ˜åœ¨&lt;a href=&#34;https://zhuanlan.zhihu.com/pythoner&#34;&gt;çŸ¥ä¹ä¸“æ -æ’¸ä»£ç ,å­¦çŸ¥è¯†&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;===================================================================================================&lt;/p&gt; &#xA;&lt;h3&gt;python_base.py: åƒè¡Œä»£ç å…¥é—¨Python&lt;/h3&gt; &#xA;&lt;h3&gt;python_visual.py: 15å¼ å›¾å…¥é—¨Matplotlib&lt;/h3&gt; &#xA;&lt;h3&gt;python_visual_animation.py: ä½¿ç”¨Matplotlibç”»åŠ¨æ€å›¾å®ä¾‹&lt;/h3&gt; &#xA;&lt;h3&gt;python_spider.py: ä¸€ä¸ªå¾ˆâ€œæ°´â€çš„Pythonçˆ¬è™«å…¥é—¨ä»£ç æ–‡ä»¶&lt;/h3&gt; &#xA;&lt;h3&gt;python_weibo.py: â€œå²ä¸Šæœ€è¯¦ç»†â€çš„Pythonæ¨¡æ‹Ÿç™»å½•æ–°æµªå¾®åšæµç¨‹&lt;/h3&gt; &#xA;&lt;h3&gt;python_lda.py: ç©ç‚¹é«˜çº§çš„--å¸¦ä½ å…¥é—¨Topicæ¨¡å‹LDAï¼ˆå°æ”¹è¿›+é™„æºç ï¼‰&lt;/h3&gt; &#xA;&lt;h3&gt;python_sqlalchemy.py: ä½œä¸ºä¸€ä¸ªPythoner, ä¸ä¼šSQLAlchemyéƒ½ä¸å¥½æ„æ€è·ŸåŒè¡Œæ‰“æ‹›å‘¼ï¼&lt;/h3&gt; &#xA;&lt;h3&gt;python_oneline.py: å‡ ä¸ªå°ä¾‹å­å‘Šè¯‰ä½ , ä¸€è¡ŒPythonä»£ç èƒ½å¹²å“ªäº›äº‹&lt;/h3&gt; &#xA;&lt;h3&gt;python_requests.py: Pythonä¸­æœ€å¥½ç”¨çš„çˆ¬è™«åº“Requestsä»£ç å®ä¾‹&lt;/h3&gt; &#xA;&lt;h3&gt;python_functional.py: Pythonè¿›é˜¶: å‡½æ•°å¼ç¼–ç¨‹å®ä¾‹ï¼ˆé™„ä»£ç ï¼‰&lt;/h3&gt; &#xA;&lt;h3&gt;python_decorator.py: Pythonè¿›é˜¶: é€šè¿‡å®ä¾‹è¯¦è§£è£…é¥°å™¨ï¼ˆé™„ä»£ç ï¼‰&lt;/h3&gt; &#xA;&lt;h3&gt;python_datetime.py: ä½ çœŸçš„äº†è§£Pythonä¸­çš„æ—¥æœŸæ—¶é—´å¤„ç†å—ï¼Ÿ&lt;/h3&gt; &#xA;&lt;h3&gt;python_metaclass.py: Pythonè¿›é˜¶: ä¸€æ­¥æ­¥ç†è§£Pythonä¸­çš„å…ƒç±»metaclass&lt;/h3&gt; &#xA;&lt;h3&gt;python_coroutine.py: Pythonè¿›é˜¶: ç†è§£Pythonä¸­çš„å¼‚æ­¥IOå’Œåç¨‹(Coroutine), å¹¶åº”ç”¨åœ¨çˆ¬è™«ä¸­&lt;/h3&gt; &#xA;&lt;h3&gt;python_aiohttp.py: Pythonä¸­æœ€å¥½ç”¨çš„å¼‚æ­¥çˆ¬è™«åº“Aiohttpä»£ç å®ä¾‹&lt;/h3&gt; &#xA;&lt;h3&gt;python_thread_multiprocess.py: Pythonè¿›é˜¶: èŠèŠIOå¯†é›†å‹ä»»åŠ¡ã€è®¡ç®—å¯†é›†å‹ä»»åŠ¡ï¼Œä»¥åŠå¤šçº¿ç¨‹ã€å¤šè¿›ç¨‹&lt;/h3&gt; &#xA;&lt;h3&gt;python_version36.py: Python3.6æ­£å¼ç‰ˆè¦æ¥äº†, ä½ æœŸå¾…å“ªäº›æ–°ç‰¹æ€§ï¼Ÿ&lt;/h3&gt; &#xA;&lt;h3&gt;python_magic_methods: Pythonè¿›é˜¶: å®ä¾‹è®²è§£Pythonä¸­çš„é­”æ³•å‡½æ•°(Magic Methods)&lt;/h3&gt; &#xA;&lt;h3&gt;python_restful_api.py: åˆ©ç”¨Pythonå’ŒFlaskå¿«é€Ÿå¼€å‘RESTful API&lt;/h3&gt; &#xA;&lt;h3&gt;python_restful_api.py: RESTful APIè¿›é˜¶: è¿æ¥æ•°æ®åº“ã€æ·»åŠ å‚æ•°ã€Tokenè®¤è¯ã€è¿”å›ä»£ç è¯´æ˜ç­‰&lt;/h3&gt; &#xA;&lt;h3&gt;python_context.py: Withè¯­å¥å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨ContextManager&lt;/h3&gt; &#xA;&lt;h3&gt;python_flask.py: Flaskç›¸å…³è¯´æ˜&lt;/h3&gt; &#xA;&lt;h3&gt;MyShow: ç©ç‚¹å¥½ç©çš„--çŸ¥ä¹å…¨éƒ¨è¯é¢˜å…³ç³»å¯è§†åŒ–&lt;/h3&gt; &#xA;&lt;h3&gt;python_markov_chain.py: ç©ç‚¹å¥½ç©çš„--ä½¿ç”¨é©¬å°”å¯å¤«æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆæ–‡ç« &lt;/h3&gt; &#xA;&lt;h3&gt;python_wechat.py: ç©ç‚¹å¥½ç©çš„--è‡ªå·±å†™ä¸€ä¸ªå¾®ä¿¡å°åŠ©æ‰‹&lt;/h3&gt; &#xA;&lt;h3&gt;python_csv.py: Pythonä¸­CSVæ–‡ä»¶çš„ç®€å•è¯»å†™&lt;/h3&gt; &#xA;&lt;h3&gt;python_numpy.py: ä½¿ç”¨numpyè¿›è¡ŒçŸ©é˜µæ“ä½œ&lt;/h3&gt; &#xA;&lt;h3&gt;python_mail.py: ä½¿ç”¨Pythonè‡ªåŠ¨å‘é€é‚®ä»¶ï¼ŒåŒ…æ‹¬å‘é€HTMLä»¥åŠå›¾ç‰‡ã€é™„ä»¶ç­‰&lt;/h3&gt; &#xA;&lt;h3&gt;python_redis.py: Pythonæ“ä½œRediså®ç°æ¶ˆæ¯çš„å‘å¸ƒä¸è®¢é˜…&lt;/h3&gt; &#xA;&lt;h3&gt;python_schedule.py: Pythonè¿›è¡Œè°ƒåº¦å¼€å‘&lt;/h3&gt; &#xA;&lt;h3&gt;python_socket.py: Pythonçš„socketå¼€å‘å®ä¾‹&lt;/h3&gt; &#xA;&lt;h3&gt;Plotlyç›®å½•: ä¸€äº›plotlyç”»å›¾çš„å®ä¾‹ï¼Œä½¿ç”¨jupyter notebookç¼–å†™&lt;/h3&gt; &#xA;&lt;p&gt;===================================================================================================&lt;/p&gt; &#xA;&lt;h3&gt;æ‚¨å¯ä»¥forkè¯¥é¡¹ç›®, å¹¶åœ¨ä¿®æ”¹åæäº¤Pull request, çœ‹åˆ°åä¼šå°½é‡è¿›è¡Œä»£ç åˆå¹¶&lt;/h3&gt;</summary>
  </entry>
  <entry>
    <title>algorithmica-org/algorithmica</title>
    <updated>2022-08-02T01:45:56Z</updated>
    <id>tag:github.com,2022-08-02:/algorithmica-org/algorithmica</id>
    <link href="https://github.com/algorithmica-org/algorithmica" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A computer science textbook&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Algorithmica v3&lt;/h1&gt; &#xA;&lt;p&gt;Algorithmica is an open-access web book dedicated to the art and science of computing.&lt;/p&gt; &#xA;&lt;p&gt;You can contribute via &lt;a href=&#34;https://prose.io/&#34;&gt;Prose&lt;/a&gt; by clicking on the pencil icon on the top right on any page or by editing its source directly on GitHub. We use a slightly different Markdown dialect, so if you are not sure that the change is correct (for example, editing an intricate LaTeX formula), you can install &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt; and build the site locally â€” or just create a pull request, and a preview link will be automatically generated for you.&lt;/p&gt; &#xA;&lt;p&gt;If you happen to speak Russian, please also read the &lt;a href=&#34;https://ru.algorithmica.org/contributing/&#34;&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Key technical changes from the &lt;a href=&#34;https://github.com/algorithmica-org/articles&#34;&gt;previous version&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;pandoc -&amp;gt; Hugo&lt;/li&gt; &#xA; &lt;li&gt;CSS -&amp;gt; Sass&lt;/li&gt; &#xA; &lt;li&gt;Github Pages -&amp;gt; Netlify&lt;/li&gt; &#xA; &lt;li&gt;Yandex.Metrica -&amp;gt; &lt;del&gt;Google Analytics&lt;/del&gt; went back to Metrica&lt;/li&gt; &#xA; &lt;li&gt;algorithmica.org/{lang}/* -&amp;gt; {lang}.algorithmica.org/*&lt;/li&gt; &#xA; &lt;li&gt;Rich metadata support (language, sections, TOCs, authors...)&lt;/li&gt; &#xA; &lt;li&gt;Automated global table of contents&lt;/li&gt; &#xA; &lt;li&gt;Theming support&lt;/li&gt; &#xA; &lt;li&gt;Search support (Lunr)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Short-term todo list:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Style adjustments for mobile and print versions&lt;/li&gt; &#xA; &lt;li&gt;A pdf version of the whole website&lt;/li&gt; &#xA; &lt;li&gt;Meta-information support (for Google Scholar and social media)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://css-tricks.com/table-of-contents-with-intersectionobserver/&#34;&gt;Sticky table of contents&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>xuebinqin/DIS</title>
    <updated>2022-08-02T01:45:56Z</updated>
    <id>tag:github.com,2022-08-02:/xuebinqin/DIS</id>
    <link href="https://github.com/xuebinqin/DIS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the repo for our new project Highly Accurate Dichotomous Image Segmentation&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;420&#34; height=&#34;320&#34; src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/dis-logo-official.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/dis5k-v1-sailship.jpeg&#34; alt=&#34;dis5k-v1-sailship&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.03041.pdf&#34;&gt;Highly Accurate Dichotomous Image Segmentation ï¼ˆECCV 2022ï¼‰&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://xuebinqin.github.io/&#34;&gt;Xuebin Qin&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.co.uk/citations?user=6yvjpQQAAAAJ&amp;amp;hl=en&#34;&gt;Hang Dai&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.de/citations?user=3lMuodUAAAAJ&amp;amp;hl=en&#34;&gt;Xiaobin Hu&lt;/a&gt;, &lt;a href=&#34;https://dengpingfan.github.io/&#34;&gt;Deng-Ping Fan*&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=z84rLjoAAAAJ&amp;amp;hl=en&#34;&gt;Ling Shao&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=TwMib_QAAAAJ&amp;amp;hl=en&#34;&gt;Luc Van Gool&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;This is the official repo for our newly formulated DIS task:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://xuebinqin.github.io/dis/index.html&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2203.03041.pdf&#34;&gt;&lt;strong&gt;Arxiv&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;PLEASE STAY TUNED FOR OUR DIS V2.0 (Jul. 30th, 2022)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/peacock.jpg&#34; alt=&#34;disv2-peacock&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Updates !!!&lt;/h1&gt; &#xA;&lt;p&gt;** (2022-Jul.-30)** Thank &lt;a href=&#34;https://github.com/AK391&#34;&gt;&lt;strong&gt;AK391&lt;/strong&gt;&lt;/a&gt; for the implementaiton of a Web Demo: Integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces ğŸ¤—&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo &lt;a href=&#34;https://huggingface.co/spaces/doevent/dis-background-removal&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;. &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Notes for official DIS group: Currently, the released DIS deep model is the academic version that was trained with DIS V1.0, which includes very few animal, human, cars, etc. So it may not work well on these targets. We will release another version for general use and test. In addition, our DIS V2.0 will cover more categories with extremely well-annotated samples. Please stay tuned. &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;** (2022-Jul.-17)** Our paper, code and dataset are now officially released!!! Please check our project page for more details: &lt;a href=&#34;https://xuebinqin.github.io/dis/index.html&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt;.&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;** (2022-Jul.-5)** Our DIS work is now accepted by ECCV 2022, the code and dataset will be released before July 17th, 2022. Please be aware of our updates.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;1. Our Dichotomous Image Segmentation (DIS) Dataset&lt;/h2&gt; &#xA;&lt;h3&gt;1.1 &lt;a href=&#34;https://xuebinqin.github.io/dis/index.html&#34;&gt;DIS dataset V1.0: DIS5K&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Downloadï¼š &lt;a href=&#34;https://drive.google.com/file/d/1jOC2zK0GowBvEt03B7dGugCRDVAoeIqq/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1y6CQJYledfYyEO0C_Gejpw?pwd=rtgw&#34;&gt;Baidu Pan æå–ç ï¼šrtgw&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/DIS5k-dataset-v1-sailship.png&#34; alt=&#34;dis5k-dataset-v1-sailship&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/complexities-qual.jpeg&#34; alt=&#34;complexities-qual&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/categories.jpeg&#34; alt=&#34;categories&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1.2 &lt;a href=&#34;https://github.com/xuebinqin/DIS&#34;&gt;DIS dataset V2.0&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Although our DIS5K V1.0 includes samples from more than 200 categories, many categories, such as human, animals, cars and so on, in real world are not included. &lt;a href=&#34;&#34;&gt;So the current version (v1.0) of our dataset may limit the robustness of the trained models.&lt;/a&gt; To build the comprehensive and large-scale highly accurate dichotomous image segmentation dataset, we are building our DIS dataset V2.0. The V2.0 will be released soon. Please stay tuned.&lt;/p&gt; &#xA;&lt;p&gt;Samples from DIS dataset V2.0. &lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/dis-v2.jpg&#34; alt=&#34;dis-v2&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;2. APPLICATIONS of Our DIS5K Dataset&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;3D Modeling&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/3d-modeling.png&#34; alt=&#34;3d-modeling&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image Editing&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/ship-demo.gif&#34; alt=&#34;ship-demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Art Design Materials&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/bg-removal.gif&#34; alt=&#34;bg-removal&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Still Image Animation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/view-move.gif&#34; alt=&#34;view-move&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;AR&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/motor-demo.gif&#34; alt=&#34;motor-demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;3D Rendering&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/video-3d.gif&#34; alt=&#34;video-3d&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;3. Architecture of Our IS-Net&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/is-net.png&#34; alt=&#34;is-net&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;4. Human Correction Efforts (HCE)&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/hce-metric.png&#34; alt=&#34;hce-metric&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;5. Experimental Results&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Predicted Maps, &lt;a href=&#34;https://drive.google.com/file/d/1FMtDLFrL6xVc41eKlLnuZWMBAErnKv0Y/view?usp=sharing&#34;&gt;(Google Drive)&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/1WUk2RYYpii2xzrvLna9Fsg?pwd=ph1d&#34;&gt;(Baidu Pan æå–ç ï¼šph1d)&lt;/a&gt;, of Our IS-Net and Other SOTAs&lt;/h3&gt; &#xA;&lt;h3&gt;Qualitative Comparisons Against SOTAs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/qual-comp.jpg&#34; alt=&#34;qual-comp&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Quantitative Comparisons Against SOTAs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/quan-comp.png&#34; alt=&#34;qual-comp&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;6. Run Our Code&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;(1) Clone this repo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/xuebinqin/DIS.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(2) Configuring the environment: go to the root &lt;code&gt;DIS&lt;/code&gt; folder and run&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f pytorch18.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can check the &lt;code&gt;requirements.txt&lt;/code&gt; to configure the dependancies.&lt;/p&gt; &#xA;&lt;h3&gt;(3) Train:&lt;/h3&gt; &#xA;&lt;p&gt;(a) Open &lt;code&gt;train_valid_inference_main.py&lt;/code&gt;, set the path of your to-be-inferenced &lt;code&gt;train_datasets&lt;/code&gt; and &lt;code&gt;valid_datasets&lt;/code&gt;, e.g., &lt;code&gt;valid_datasets=[dataset_vd]&lt;/code&gt; &lt;br&gt; (b) Set the &lt;code&gt;hypar[&#34;mode&#34;]&lt;/code&gt; to &lt;code&gt;&#34;train&#34;&lt;/code&gt; &lt;br&gt; (c) Create a new folder &lt;code&gt;your_model_weights&lt;/code&gt; in the directory &lt;code&gt;saved_models&lt;/code&gt; and set it as the &lt;code&gt;hypar[&#34;model_path&#34;] =&#34;../saved_models/your_model_weights&#34;&lt;/code&gt; and make sure &lt;code&gt;hypar[&#34;valid_out_dir&#34;]&lt;/code&gt;(line 668) is set to &lt;code&gt;&#34;&#34;&lt;/code&gt;, otherwise the prediction maps of the validation stage will be saved to that directory, which will slow the training speed down &lt;br&gt; (d) Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_valid_inference_main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(4) Inference&lt;/h3&gt; &#xA;&lt;p&gt;(a). Download the pre-trained weights (for fair academic comparisons only, the optimized model for engineering or common use will be released soon) &lt;code&gt;isnet.pth&lt;/code&gt; from &lt;a href=&#34;https://drive.google.com/file/d/1KyMpRjewZdyYfxHPYcd-ZbanIXtin0Sn/view?usp=sharing&#34;&gt;(Google Drive)&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1-X2WutiBkWPt-oakuvZ10w?pwd=xbfk&#34;&gt;(Baidu Pan æå–ç ï¼šxbfk)&lt;/a&gt; and store &lt;code&gt;isnet.pth&lt;/code&gt; in &lt;code&gt;saved_models/IS-Net&lt;/code&gt; &lt;br&gt; (b) Open &lt;code&gt;train_valid_inference_main.py&lt;/code&gt;, set the path of your to-be-inferenced &lt;code&gt;valid_datasets&lt;/code&gt;, e.g., &lt;code&gt;valid_datasets=[dataset_te1, dataset_te2, dataset_te3, dataset_te4]&lt;/code&gt; &lt;br&gt; (c) Set the &lt;code&gt;hypar[&#34;mode&#34;]&lt;/code&gt; to &lt;code&gt;&#34;valid&#34;&lt;/code&gt; &lt;br&gt; (d) Set the output directory of your predicted maps, e.g., &lt;code&gt;hypar[&#34;valid_out_dir&#34;] = &#34;../DIS5K-Results-test&#34;&lt;/code&gt; &lt;br&gt; (e) Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_valid_inference_main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(5) Use of our Human Correction Efforts(HCE) metric&lt;/h3&gt; &#xA;&lt;p&gt;Set the ground truth directory &lt;code&gt;gt_root&lt;/code&gt; and the prediction directory &lt;code&gt;pred_root&lt;/code&gt;. To reduce the time costs for computing HCE, the skeletion of the DIS5K dataset can be pre-computed and stored in &lt;code&gt;gt_ske_root&lt;/code&gt;. If &lt;code&gt;gt_ske_root=&#34;&#34;&lt;/code&gt;, the HCE code will compute the skeleton online which usually takes a lot for time for large size ground truth. Then, run &lt;code&gt;python hce_metric_main.py&lt;/code&gt;. Other metrics are evaluated based on the &lt;a href=&#34;https://github.com/mczhuge/SOCToolbox&#34;&gt;SOCToolbox&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;7. Term of Use&lt;/h2&gt; &#xA;&lt;p&gt;Our code and evaluation metric use Apache License 2.0. The Terms of use for our DIS5K dataset is provided as &lt;a href=&#34;https://raw.githubusercontent.com/xuebinqin/DIS/main/DIS5K-Dataset-Terms-of-Use.pdf&#34;&gt;DIS5K-Dataset-Terms-of-Use.pdf&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;We would like to thank Dr. &lt;a href=&#34;https://scholar.google.co.uk/citations?user=T9MTcK0AAAAJ&amp;amp;hl=en&#34;&gt;Ibrahim Almakky&lt;/a&gt; for his helps in implementing the dataloader cache machanism of loading large-size training samples and Jiayi Zhu for his efforts in re-organizing our code and dataset.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;pre&gt;&lt;code&gt;@InProceedings{qin2022,&#xA;      author={Xuebin Qin and Hang Dai and Xiaobin Hu and Deng-Ping Fan and Ling Shao and Luc Van Gool},&#xA;      title={Highly Accurate Dichotomous Image Segmentation},&#xA;      booktitle={ECCV},&#xA;      year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Our Previous Works: &lt;a href=&#34;https://github.com/xuebinqin/U-2-Net&#34;&gt;U&lt;sup&gt;2&lt;/sup&gt;-Net&lt;/a&gt;, &lt;a href=&#34;https://github.com/xuebinqin/BASNet&#34;&gt;BASNet&lt;/a&gt;.&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;@InProceedings{Qin_2020_PR,&#xA;      title = {U2-Net: Going Deeper with Nested U-Structure for Salient Object Detection},&#xA;      author = {Qin, Xuebin and Zhang, Zichen and Huang, Chenyang and Dehghan, Masood and Zaiane, Osmar and Jagersand, Martin},&#xA;      journal = {Pattern Recognition},&#xA;      volume = {106},&#xA;      pages = {107404},&#xA;      year = {2020}&#xA;}&#xA;&#xA;@InProceedings{Qin_2019_CVPR,&#xA;        author = {Qin, Xuebin and Zhang, Zichen and Huang, Chenyang and Gao, Chao and Dehghan, Masood and Jagersand, Martin},&#xA;        title = {BASNet: Boundary-Aware Salient Object Detection},&#xA;        booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;        month = {June},&#xA;        year = {2019}&#xA;}&#xA;&#xA;@article{qin2021boundary,&#xA;       title={Boundary-aware segmentation network for mobile and web applications},&#xA;       author={Qin, Xuebin and Fan, Deng-Ping and Huang, Chenyang and Diagne, Cyril and Zhang, Zichen and Sant&#39;Anna, Adri{\`a} Cabeza and Suarez, Albert and Jagersand, Martin and Shao, Ling},&#xA;       journal={arXiv preprint arXiv:2101.04704},&#xA;       year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>