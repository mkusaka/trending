<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-22T01:31:45Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xbresson/GML2023</title>
    <updated>2024-02-22T01:31:45Z</updated>
    <id>tag:github.com,2024-02-22:/xbresson/GML2023</id>
    <link href="https://github.com/xbresson/GML2023" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Graph Machine Learning course, Xavier Bresson, 2023&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Graph Machine Learning course, 2023&lt;/h1&gt; &#xA;&lt;h2&gt;Xavier Bresson&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/xbresson/GML2023/main/pic/gml.jpg&#34; align=&#34;right&#34; width=&#34;300&#34;&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Course slides&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://storage.googleapis.com/xavierbresson/index.html&#34;&gt;https://storage.googleapis.com/xavierbresson/index.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Cloud Machine : Google Colab (Free GPU)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Follow this Notebook installation :&lt;br&gt; &lt;a href=&#34;https://colab.research.google.com/github/xbresson/GML2023/blob/master/codes/installation/installation.ipynb&#34;&gt;https://colab.research.google.com/github/xbresson/GML2023/blob/master/codes/installation/installation.ipynb&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open your Google Drive :&lt;br&gt; &lt;a href=&#34;https://www.google.com/drive&#34;&gt;https://www.google.com/drive&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open in Google Drive Folder &#39;GML2023&#39; and go to Folder &#39;GML2023/codes/&#39;&lt;br&gt; Select the notebook &#39;file.ipynb&#39; and open it with Google Colab using Control Click + Open With Colaboratory&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Local Installation for OSX &amp;amp; Linux&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open a Terminal and type&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;   # Conda installation&#xA;   curl https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -o miniconda.sh -J -L -k # Linux&#xA;   curl https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -o miniconda.sh -J -L -k # OSX&#xA;   chmod +x miniconda.sh&#xA;   ./miniconda.sh&#xA;   source ~/.bashrc&#xA;&#xA;   # Clone GitHub repo&#xA;   git clone https://github.com/xbresson/GML2023.git&#xA;   cd GML2023&#xA;&#xA;   # Install python libraries&#xA;   conda env create -f environment.yml&#xA;   source activate gnn_course&#xA;&#xA;   # Run the notebooks in Chrome&#xA;   jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Local Installation for Windows&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;   # Install Anaconda &#xA;   https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe&#xA;&#xA;   # Open an Anaconda Terminal &#xA;   Go to Application =&amp;gt; Anaconda3 =&amp;gt; Anaconda Prompt &#xA;&#xA;   # Install git : Type in terminal&#xA;   conda install git &#xA;&#xA;   # Clone GitHub repo&#xA;   git clone https://github.com/xbresson/GML2023.git&#xA;   cd GML2023&#xA;&#xA;   # Install python libraries&#xA;   conda env create -f environment.yml&#xA;   conda activate gnn_course&#xA;&#xA;   # Run the notebooks in Chrome&#xA;   jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>princeton-computational-imaging/NSF</title>
    <updated>2024-02-22T01:31:45Z</updated>
    <id>tag:github.com,2024-02-22:/princeton-computational-imaging/NSF</id>
    <link href="https://github.com/princeton-computational-imaging/NSF" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official code repository for the paper: &#34;Neural Spline Fields for Burst Image Fusion and Layer Separation&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Neural Spline Fields for Burst Image Fusion and Layer Separation&lt;/h1&gt; &#xA;&lt;a href=&#34;https://colab.research.google.com/github/princeton-computational-imaging/NSF/blob/main/tutorial.ipynb&#34; style=&#34;text-decoration: none;&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34; style=&#34;height:20px;&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://github.com/Ilya-Muromets/Pani&#34; style=&#34;text-decoration: none;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/princeton-computational-imaging/NSF/main/.figs/pani-badge.svg?sanitize=true&#34; alt=&#34;Android Capture App&#34; style=&#34;height:20px;&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;This is the official code repository for the work: &lt;a href=&#34;https://light.princeton.edu/publication/nsf/&#34;&gt;Neural Spline Fields for Burst Image Fusion and Layer Separation&lt;/a&gt;. If you use parts of this work, or otherwise take inspiration from it, please considering citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{chugunov2023neural,&#xA;  title={Neural Spline Fields for Burst Image Fusion and Layer Separation},&#xA;  author={Chugunov, Ilya and Shustin, David and Yan, Ruyu and Lei, Chenyang and Heide, Felix},&#xA;  journal={arXiv preprint arXiv:2312.14235},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Requirements:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Code was written in PyTorch 2.0 on an Ubuntu 22.04 machine.&lt;/li&gt; &#xA; &lt;li&gt;Condensed package requirements are in &lt;code&gt;\requirements.txt&lt;/code&gt;. Note that this contains the exact package versions at the time of publishing. Code will most likely work with newer versions of the libraries, but you will need to watch out for changes in class/function calls.&lt;/li&gt; &#xA; &lt;li&gt;The non-standard packages you may need are &lt;code&gt;pytorch_lightning&lt;/code&gt;, &lt;code&gt;commentjson&lt;/code&gt;, &lt;code&gt;rawpy&lt;/code&gt;, and &lt;code&gt;tinycudann&lt;/code&gt;. See &lt;a href=&#34;https://github.com/NVlabs/tiny-cuda-nn&#34;&gt;NVlabs/tiny-cuda-nn&lt;/a&gt; for installation instructions. Depending on your system you might just be able to do &lt;code&gt;pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch&lt;/code&gt;, or might have to cmake and build it from source.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Project Structure:&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;NSF&#xA;  ├── checkpoints  &#xA;  │   └── // folder for network checkpoints&#xA;  ├── config&#xA;  │   └── // network and encoding configurations for different sizes of MLPs&#xA;  ├── data  &#xA;  │   └── // folder for long-burst data&#xA;  ├── lightning_logs  &#xA;  │   └── // folder for tensorboard logs&#xA;  ├── outputs  &#xA;  │   └── // folder for model outputs (e.g., final reconstructions) &#xA;  ├── scripts  &#xA;  │   └── // training scripts for different tasks (e.g., occlusion/reflection/shadow separation)&#xA;  ├── utils  &#xA;  │   └── utils.py  // network helper functions (e.g., RAW demosaicing, spline interpolation)&#xA;  ├── LICENSE  // legal stuff&#xA;  ├── README.md  // &amp;lt;- you are here&#xA;  ├── requirements.txt  // frozen package requirements&#xA;  ├── train.py  // dataloader, network, visualization, and trainer code&#xA;  └── tutorial.ipynb // interactive tutorial for training the model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started:&lt;/h2&gt; &#xA;&lt;p&gt;We highly recommend you start by going through &lt;code&gt;tutorial.ipynb&lt;/code&gt;, either on your own machine or &lt;a href=&#34;https://colab.research.google.com/github/princeton-computational-imaging/NSF/blob/main/tutorial.ipynb&#34;&gt;with this Google Colab link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;TLDR: models can be trained with:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;bash scripts/{application}.sh --bundle_path {path_to_data} --name {checkpoint_name}&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;And reconstruction outputs will get saved to &lt;code&gt;outputs/{checkpoint_name}-final&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For a full list of training arguments, we recommend looking through the argument parser section at the bottom of &lt;code&gt;\train.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Data:&lt;/h2&gt; &#xA;&lt;p&gt;You can download the long-burst data used in the paper (and extra bonus scenes) via the following links:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Main occlusion scenes: &lt;a href=&#34;https://soap.cs.princeton.edu/nsf/data/occlusion-main.zip&#34;&gt;occlusion-main.zip&lt;/a&gt; (use &lt;code&gt;scripts/occlusion.sh&lt;/code&gt; to train) &lt;img src=&#34;https://raw.githubusercontent.com/princeton-computational-imaging/NSF/main/.figs/occ-main.png&#34; alt=&#34;Main Occlusion&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Supplementary occlusion scenes: &lt;a href=&#34;https://soap.cs.princeton.edu/nsf/data/occlusion-supp.zip&#34;&gt;occlusion-supp.zip&lt;/a&gt; (use &lt;code&gt;scripts/occlusion.sh&lt;/code&gt; to train) &lt;img src=&#34;https://raw.githubusercontent.com/princeton-computational-imaging/NSF/main/.figs/occ-supp.png&#34; alt=&#34;Supplementary Occlusion&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In-the-wild occlusion scenes: &lt;a href=&#34;https://soap.cs.princeton.edu/nsf/data/occlusion-wild.zip&#34;&gt;occlusion-wild.zip&lt;/a&gt; (use &lt;code&gt;scripts/occlusion-wild.sh&lt;/code&gt; to train) &lt;img src=&#34;https://raw.githubusercontent.com/princeton-computational-imaging/NSF/main/.figs/occ-wild.png&#34; alt=&#34;Wild Occlusion&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Main reflection scenes: &lt;a href=&#34;https://soap.cs.princeton.edu/nsf/data/reflection-main.zip&#34;&gt;reflection-main.zip&lt;/a&gt; (use &lt;code&gt;scripts/reflection.sh&lt;/code&gt; to train) &lt;img src=&#34;https://raw.githubusercontent.com/princeton-computational-imaging/NSF/main/.figs/ref-main.png&#34; alt=&#34;Main Reflection&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Supplementary reflection scenes: &lt;a href=&#34;https://soap.cs.princeton.edu/nsf/data/reflection-supp.zip&#34;&gt;reflection-supp.zip&lt;/a&gt; (use &lt;code&gt;scripts/reflection.sh&lt;/code&gt; to train) &lt;img src=&#34;https://raw.githubusercontent.com/princeton-computational-imaging/NSF/main/.figs/ref-supp.png&#34; alt=&#34;Supplementary Reflection&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In-the-wild reflection scenes: &lt;a href=&#34;https://soap.cs.princeton.edu/nsf/data/reflection-wild.zip&#34;&gt;reflection-wild.zip&lt;/a&gt; (use &lt;code&gt;scripts/reflection-wild.sh&lt;/code&gt; to train) &lt;img src=&#34;https://raw.githubusercontent.com/princeton-computational-imaging/NSF/main/.figs/ref-wild.png&#34; alt=&#34;Wild Reflection&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Extra scenes: &lt;a href=&#34;https://soap.cs.princeton.edu/nsf/data/extras.zip&#34;&gt;extras.zip&lt;/a&gt; (use &lt;code&gt;scripts/dehaze.sh&lt;/code&gt;, &lt;code&gt;segmentation.sh&lt;/code&gt;, or &lt;code&gt;shadow.sh&lt;/code&gt;) &lt;img src=&#34;https://raw.githubusercontent.com/princeton-computational-imaging/NSF/main/.figs/extras.png&#34; alt=&#34;Extras&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We recommend you download and extract these into the &lt;code&gt;data/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;App:&lt;/h2&gt; &#xA;&lt;p&gt;Want to record your own long-burst data? Check out our Android RAW capture app &lt;a href=&#34;https://github.com/Ilya-Muromets/Pani&#34;&gt;Pani!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Good luck have fun, Ilya&lt;/p&gt;</summary>
  </entry>
</feed>