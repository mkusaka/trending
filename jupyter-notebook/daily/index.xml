<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-11T01:34:41Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Waeara/Python-Automated-WhatsApp-Message-Sender</title>
    <updated>2022-12-11T01:34:41Z</updated>
    <id>tag:github.com,2022-12-11:/Waeara/Python-Automated-WhatsApp-Message-Sender</id>
    <link href="https://github.com/Waeara/Python-Automated-WhatsApp-Message-Sender" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python-Automated-WhatsApp-Message-Sender&lt;/h1&gt; &#xA;&lt;p&gt;For further explanation &lt;a href=&#34;https://medium.com/gitconnected/python-automated-whatsapp-message-sender-db488c8c6bc7&#34;&gt;check out here&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cloneofsimo/lora</title>
    <updated>2022-12-11T01:34:41Z</updated>
    <id>tag:github.com,2022-12-11:/cloneofsimo/lora</id>
    <link href="https://github.com/cloneofsimo/lora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Using Low-rank adaptation to quickly fine-tune diffusion models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning&lt;/h1&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/lora/master/contents/alpha_scale.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Using LORA to fine tune on illustration dataset : $W = W_0 + \alpha \Delta W$, where $\alpha$ is the merging ratio. Above gif is scaling alpha from 0 to 1. Setting alpha to 0 is same as using the original model, and setting alpha to 1 is same as using the fully fine-tuned model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/lora/master/contents/disney_lora.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;style of sks, baby lion&#34;, with disney-style LORA model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/lora/master/contents/pop_art.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;style of sks, superman&#34;, with pop-art style LORA model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Main Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fine-tune Stable diffusion models twice as faster than dreambooth method, by Low-rank Adaptation&lt;/li&gt; &#xA; &lt;li&gt;Get insanely small end result, easy to share and download.&lt;/li&gt; &#xA; &lt;li&gt;Easy to use, compatible with diffusers&lt;/li&gt; &#xA; &lt;li&gt;Sometimes even better performance than full fine-tuning (but left as future work for extensive comparisons)&lt;/li&gt; &#xA; &lt;li&gt;Merge checkpoints by merging LORA&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Lengthy Introduction&lt;/h1&gt; &#xA;&lt;p&gt;Thanks to the generous work of Stability AI and Huggingface, so many people have enjoyed fine-tuning stable diffusion models to fit their needs and generate higher fidelity images. &lt;strong&gt;However, the fine-tuning process is very slow, and it is not easy to find a good balance between the number of steps and the quality of the results.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Also, the final results (fully fined-tuned model) is very large. Some people instead works with textual-inversion as an alternative for this. But clearly this is suboptimal: textual inversion only creates a small word-embedding, and the final image is not as good as a fully fine-tuned model.&lt;/p&gt; &#xA;&lt;p&gt;Well, what&#39;s the alternative? In the domain of LLM, researchers have developed Efficient fine-tuning methods. LORA, especially, tackles the very problem the community currently has: end users with Open-sourced stable-diffusion model want to try various other fine-tuned model that is created by the community, but the model is too large to download and use. LORA instead attempts to fine-tune the &#34;residual&#34; of the model instead of the entire model: i.e., train the $\Delta W$ instead of $W$.&lt;/p&gt; &#xA;&lt;p&gt;$$ W&#39; = W + \Delta W $$&lt;/p&gt; &#xA;&lt;p&gt;Where we can further decompose $\Delta W$ into low-rank matrices : $\Delta W = A B^T $, where $A, \in \mathbb{R}^{n \times d}, B \in \mathbb{R}^{m \times d}, d &amp;lt;&amp;lt; n$. This is the key idea of LORA. We can then fine-tune $A$ and $B$ instead of $W$. In the end, you get an insanely small model as $A$ and $B$ are much smaller than $W$.&lt;/p&gt; &#xA;&lt;p&gt;Also, not all of the parameters need tuning: they found that often, $Q, K, V, O$ (i.e., attention layer) of the transformer model is enough to tune. (This is also the reason why the end result is so small). This repo will follow the same idea.&lt;/p&gt; &#xA;&lt;p&gt;Enough of the lengthy introduction, let&#39;s get to the code.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/cloneofsimo/lora.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h2&gt;Fine-tuning Stable diffusion with LORA.&lt;/h2&gt; &#xA;&lt;p&gt;Basic usage is as follows: prepare sets of $A, B$ matrices in an unet model, and fine-tune them.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lora_diffusion import inject_trainable_lora, extract_lora_up_downs&#xA;&#xA;...&#xA;&#xA;unet = UNet2DConditionModel.from_pretrained(&#xA;    pretrained_model_name_or_path,&#xA;    subfolder=&#34;unet&#34;,&#xA;)&#xA;unet.requires_grad_(False)&#xA;unet_lora_params, train_names = inject_trainable_lora(unet)  # This will&#xA;# turn off all of the gradients of unet, except for the trainable LORA params.&#xA;optimizer = optim.Adam(&#xA;    itertools.chain(*unet_lora_params, text_encoder.parameters()), lr=1e-4&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of this can be found in &lt;code&gt;train_lora_dreambooth.py&lt;/code&gt;. Run this example with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;run_lora_db.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Loading, merging, and interpolating trained LORAs.&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve seen that people have been merging different checkpoints with different ratios, and this seems to be very useful to the community. LORA is extremely easy to merge.&lt;/p&gt; &#xA;&lt;p&gt;By the nature of LORA, one can interpolate between different fine-tuned models by adding different $A, B$ matrices.&lt;/p&gt; &#xA;&lt;p&gt;Currently, LORA cli has two options : merge unet with LORA, or merge LORA with LORA.&lt;/p&gt; &#xA;&lt;h3&gt;Merging unet with LORA&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ lora_add --path_1 PATH_TO_DIFFUSER_FORMAT_MODEL --path_2 PATH_TO_LORA.PT --mode upl --alpha 1.0 --output_path OUTPUT_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;path_1&lt;/code&gt; can be both local path or huggingface model name. When adding LORA to unet, alpha is the constant as below:&lt;/p&gt; &#xA;&lt;p&gt;$$ W&#39; = W + \alpha \Delta W $$&lt;/p&gt; &#xA;&lt;p&gt;So, set alpha to 1.0 to fully add LORA. If the LORA seems to have too much effect (i.e., overfitted), set alpha to lower value. If the LORA seems to have too little effect, set alpha to higher than 1.0. You can tune these values to your needs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ lora_add --path_1 stabilityai/stable-diffusion-2-base --path_2 lora_illust.pt --mode upl --alpha 1.0 --output_path merged_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Merging LORA with LORA&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ lora_add --path_1 PATH_TO_LORA.PT --path_2 PATH_TO_LORA.PT --mode lpl --alpha 0.5 --output_path OUTPUT_PATH.PT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;alpha is the ratio of the first model to the second model. i.e.,&lt;/p&gt; &#xA;&lt;p&gt;$$ \Delta W = (\alpha A_1 + (1 - \alpha) A_2) (B_1 + (1 - \alpha) B_2)^T $$&lt;/p&gt; &#xA;&lt;p&gt;Set alpha to 0.5 to get the average of the two models. Set alpha close to 1.0 to get more effect of the first model, and set alpha close to 0.0 to get more effect of the second model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ lora_add --path_1 lora_illust.pt --path_2 lora_pop.pt --alpha 0.3 --output_path lora_merged.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Making Inference with trained LORA&lt;/h3&gt; &#xA;&lt;p&gt;Checkout &lt;code&gt;scripts/run_inference.ipynb&lt;/code&gt; for an example of how to make inference with LORA.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lvwerra/trl</title>
    <updated>2022-12-11T01:34:41Z</updated>
    <id>tag:github.com,2022-12-11:/lvwerra/trl</id>
    <link href="https://github.com/lvwerra/trl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Train transformer language models with reinforcement learning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to Transformer Reinforcement Learning (trl)&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Train transformer language models with reinforcement learning.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;What is it?&lt;/h2&gt; &#xA;&lt;p&gt;With &lt;code&gt;trl&lt;/code&gt; you can train transformer language models with Proximal Policy Optimization (PPO). The library is built on top of the &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;&lt;code&gt;transformer&lt;/code&gt;&lt;/a&gt; library by 🤗 Hugging Face. Therefore, pre-trained language models can be directly loaded via &lt;code&gt;transformers&lt;/code&gt;. At this point only decoder architectures such as GTP2 are implemented.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PPOTrainer: A PPO trainer for language models that just needs (query, response, reward) triplets to optimise the language model.&lt;/li&gt; &#xA; &lt;li&gt;GPT2 model with a value head: A transformer model with an additional scalar output for each token which can be used as a value function in reinforcement learning.&lt;/li&gt; &#xA; &lt;li&gt;Example: Train GPT2 to generate positive movie reviews with a BERT sentiment classifier.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;Fine-tuning a language model via PPO consists of roughly three steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rollout&lt;/strong&gt;: The language model generates a response or continuation based on query which could be the start of a sentence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: The query and response are evaluated with a function, model, human feedback or some combination of them. The important thing is that this process should yield a scalar value for each query/response pair.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt;: This is the most complex part. In the optimisation step the query/response pairs are used to calculate the log-probabilities of the tokens in the sequences. This is done with the model that is trained and and a reference model, which is usually the pre-trained model before fine-tuning. The KL-divergence between the two outputs is used as an additional reward signal to make sure the generated responses don&#39;t deviate to far from the reference language model. The active language model is then trained with PPO.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This process is illustrated in the sketch below:&lt;/p&gt; &#xA;&lt;div style=&#34;text-align: center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/lvwerra/trl/master/nbs/images/trl_overview.png&#34; width=&#34;800&#34;&gt; &#xA; &lt;p style=&#34;text-align: center;&#34;&gt; &lt;b&gt;Figure:&lt;/b&gt; Sketch of the workflow. &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Python package&lt;/h3&gt; &#xA;&lt;p&gt;Install the library with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install trl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;From source&lt;/h3&gt; &#xA;&lt;p&gt;If you want to run the examples in the repository a few additional libraries are required. Clone the repository and install it with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/lvwerra/trl.git&#xA;cd tlr/&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Jupyter notebooks&lt;/h3&gt; &#xA;&lt;p&gt;If you run Jupyter notebooks you might need to run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter nbextension enable --py --sys-prefix widgetsnbextension&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For Jupyterlab additionally this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter labextension install @jupyter-widgets/jupyterlab-manager&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;h3&gt;Example&lt;/h3&gt; &#xA;&lt;p&gt;This is a basic example on how to use the library. Based on a query the language model creates a response which is then evaluated. The evaluation could be a human in the loop or another model&#39;s output.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# imports&#xA;import torch&#xA;from transformers import GPT2Tokenizer&#xA;from trl.gpt2 import GPT2HeadWithValueModel, respond_to_batch&#xA;from trl.ppo import PPOTrainer&#xA;&#xA;# get models&#xA;gpt2_model = GPT2HeadWithValueModel.from_pretrained(&#39;gpt2&#39;)&#xA;gpt2_model_ref = GPT2HeadWithValueModel.from_pretrained(&#39;gpt2&#39;)&#xA;gpt2_tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)&#xA;&#xA;# initialize trainer&#xA;ppo_config = {&#39;batch_size&#39;: 1, &#39;forward_batch_size&#39;: 1}&#xA;ppo_trainer = PPOTrainer(gpt2_model, gpt2_model_ref, gpt2_tokenizer, **ppo_config)&#xA;&#xA;# encode a query&#xA;query_txt = &#34;This morning I went to the &#34;&#xA;query_tensor = gpt2_tokenizer.encode(query_txt, return_tensors=&#34;pt&#34;)&#xA;&#xA;# get model response&#xA;response_tensor  = respond_to_batch(gpt2_model, query_tensor)&#xA;response_txt = gpt2_tokenizer.decode(response_tensor[0,:])&#xA;&#xA;# define a reward for response&#xA;# (this could be any reward such as human feedback or output from another model)&#xA;reward = [torch.tensor(1.0)]&#xA;&#xA;# train model with ppo&#xA;train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Advanced example: IMDB sentiment&lt;/h3&gt; &#xA;&lt;p&gt;For a detailed example check out the notebook &lt;code&gt;04-gpt2-sentiment-ppo-training.ipynb&lt;/code&gt;, where GPT2 is fine-tuned to generate positive movie reviews. An few examples from the language models before and after optimisation are given below:&lt;/p&gt; &#xA;&lt;div style=&#34;text-align: center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/lvwerra/trl/master/nbs/images/table_imdb_preview.png&#34; width=&#34;800&#34;&gt; &#xA; &lt;p style=&#34;text-align: center;&#34;&gt; &lt;b&gt;Figure:&lt;/b&gt; A few review continuations before and after optimisation. &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;This library is built with &lt;code&gt;nbdev&lt;/code&gt; and as such all the library code as well as examples are in Jupyter notebooks. The following list gives an overview:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;index.ipynb&lt;/code&gt;: Generates the README and the overview page.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;00-core.ipynb&lt;/code&gt;: Contains the utility functions used throughout the library and examples.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;01-gpt2-with-value-head.ipynb&lt;/code&gt;: Implementation of a &lt;code&gt;transformer&lt;/code&gt; compatible GPT2 model with an additional value head as well as a function to generate sequences.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;02-ppo.ipynb&lt;/code&gt;: Implementation of the PPOTrainer used to train language models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;03-bert-imdb-training.ipynb&lt;/code&gt;: Training of DistilBERT to classify sentiment on the IMDB dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;04-gpt2-sentiment-ppo-training.ipynb&lt;/code&gt;: Fine-tune GPT2 with the BERT sentiment classifier to produce positive movie reviews.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Currently using &lt;code&gt;trl==0.0.3&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;05-gpt2-sentiment-control.ipynb&lt;/code&gt;: Fine-tune GPT2 with the BERT sentiment classifier to produce movie reviews with controlled sentiment.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;h3&gt;Proximal Policy Optimisation&lt;/h3&gt; &#xA;&lt;p&gt;The PPO implementation largely follows the structure introduced in the paper &lt;strong&gt;&#34;Fine-Tuning Language Models from Human Preferences&#34;&lt;/strong&gt; by D. Ziegler et al. [&lt;a href=&#34;https://arxiv.org/pdf/1909.08593.pdf&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/openai/lm-human-preferences&#34;&gt;code&lt;/a&gt;].&lt;/p&gt; &#xA;&lt;h3&gt;Language models&lt;/h3&gt; &#xA;&lt;p&gt;The language models utilize the &lt;code&gt;transformers&lt;/code&gt; library by 🤗 Hugging Face.&lt;/p&gt;</summary>
  </entry>
</feed>