<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-11T01:29:24Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>SynodicMonth/ChebyKAN</title>
    <updated>2024-05-11T01:29:24Z</updated>
    <id>tag:github.com,2024-05-11:/SynodicMonth/ChebyKAN</id>
    <link href="https://github.com/SynodicMonth/ChebyKAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kolmogorov-Arnold Networks (KAN) using Chebyshev polynomials instead of B-splines.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;This is a VERY COARSE version and absolutely NOT FULLY TESTED! it&#39;s only intended for experiementing! Any discussion and criticism are welcome! Check the issues for more information!&lt;/h2&gt; &#xA;&lt;h1&gt;ChebyKAN&lt;/h1&gt; &#xA;&lt;p&gt;Kolmogorov-Arnold Networks (KAN) using Chebyshev polynomials instead of B-splines.&lt;/p&gt; &#xA;&lt;p&gt;This is inspired by Kolmogorov-Arnold Networks &lt;a href=&#34;https://arxiv.org/abs/2404.19756v2&#34;&gt;https://arxiv.org/abs/2404.19756v2&lt;/a&gt;, which uses B-splines to approximate functions. B-splines are poor in performance and not very intuitive to use. I&#39;m trying to replace B-splines with Chebyshev polynomials.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Chebyshev_polynomials&#34;&gt;Chebyshev polynomials&lt;/a&gt; are orthogonal polynomials defined on the interval [-1, 1]. They are very good at approximating functions and can be calculated recursively.&lt;/p&gt; &#xA;&lt;p&gt;A simple (and naive) implementation of ChebyKANLayer is provided in &lt;code&gt;chebyKANLayer.py&lt;/code&gt;. Its not optimized yet.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;Just copy &lt;code&gt;ChebyKANLayer.py&lt;/code&gt; to your project and import it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ChebyKANLayer import ChebyKANLayer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Example&lt;/h1&gt; &#xA;&lt;p&gt;Construct a ChebyKAN for MNIST&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class MNISTChebyKAN(nn.Module):&#xA;    def __init__(self):&#xA;        super(MNISTChebyKAN, self).__init__()&#xA;        self.chebykan1 = ChebyKANLayer(28*28, 32, 4)&#xA;        self.ln1 = nn.LayerNorm(32) # To avoid gradient vanishing caused by tanh&#xA;        self.chebykan2 = ChebyKANLayer(32, 16, 4)&#xA;        self.ln2 = nn.LayerNorm(16)&#xA;        self.chebykan3 = ChebyKANLayer(16, 10, 4)&#xA;&#xA;    def forward(self, x):&#xA;        x = x.view(-1, 28*28)  # Flatten the images&#xA;        x = self.chebykan1(x)&#xA;        x = self.ln1(x)&#xA;        x = self.chebykan2(x)&#xA;        x = self.ln2(x)&#xA;        x = self.chebykan3(x)&#xA;        return x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Since Chebyshev polynomials are defined on the interval [-1, 1], we need to use tanh to keep the input in that range. We also use LayerNorm to avoid gradient vanishing caused by tanh. Removing LayerNorm will cause the network really hard to train.&lt;/p&gt; &#xA;&lt;p&gt;Have a look at &lt;code&gt;Cheby-KAN_MNIST.ipynb&lt;/code&gt;, &lt;code&gt;Function_Interpolation_Test.ipynb&lt;/code&gt;, and &lt;code&gt;Multivar_Interpolation_Test.ipynb&lt;/code&gt; for more examples.&lt;/p&gt; &#xA;&lt;h1&gt;Experiment Results&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;MNIST:&lt;/strong&gt; ~97% accuracy after about 20 epochs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Epoch 1, Train Loss: 1.1218, Test Loss: 0.4689, Test Acc: 0.91&#xA;Epoch 2, Train Loss: 0.3302, Test Loss: 0.2599, Test Acc: 0.93&#xA;Epoch 3, Train Loss: 0.2170, Test Loss: 0.2359, Test Acc: 0.94&#xA;Epoch 4, Train Loss: 0.1696, Test Loss: 0.1857, Test Acc: 0.95&#xA;Epoch 5, Train Loss: 0.1422, Test Loss: 0.1574, Test Acc: 0.96&#xA;Epoch 6, Train Loss: 0.1241, Test Loss: 0.1597, Test Acc: 0.95&#xA;Epoch 7, Train Loss: 0.1052, Test Loss: 0.1475, Test Acc: 0.96&#xA;Epoch 8, Train Loss: 0.0932, Test Loss: 0.1321, Test Acc: 0.96&#xA;Epoch 9, Train Loss: 0.0879, Test Loss: 0.1553, Test Acc: 0.95&#xA;Epoch 10, Train Loss: 0.0780, Test Loss: 0.1239, Test Acc: 0.96&#xA;Epoch 11, Train Loss: 0.0722, Test Loss: 0.1283, Test Acc: 0.96&#xA;Epoch 12, Train Loss: 0.0629, Test Loss: 0.1236, Test Acc: 0.96&#xA;Epoch 13, Train Loss: 0.0612, Test Loss: 0.1271, Test Acc: 0.96&#xA;Epoch 14, Train Loss: 0.0521, Test Loss: 0.1390, Test Acc: 0.96&#xA;Epoch 15, Train Loss: 0.0488, Test Loss: 0.1374, Test Acc: 0.96&#xA;Epoch 16, Train Loss: 0.0487, Test Loss: 0.1309, Test Acc: 0.96&#xA;Epoch 17, Train Loss: 0.0416, Test Loss: 0.1253, Test Acc: 0.96&#xA;Epoch 18, Train Loss: 0.0402, Test Loss: 0.1346, Test Acc: 0.96&#xA;Epoch 19, Train Loss: 0.0373, Test Loss: 0.1199, Test Acc: 0.97&#xA;Epoch 20, Train Loss: 0.0346, Test Loss: 0.1434, Test Acc: 0.96&#xA;Epoch 21, Train Loss: 0.0314, Test Loss: 0.1142, Test Acc: 0.97&#xA;Epoch 22, Train Loss: 0.0285, Test Loss: 0.1258, Test Acc: 0.97&#xA;Epoch 23, Train Loss: 0.0289, Test Loss: 0.1192, Test Acc: 0.97&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SynodicMonth/ChebyKAN/main/img/MNIST.png&#34; alt=&#34;MNIST&#34;&gt; The network parameters are [28*28, 32, 16, 10] with 4 degree Chebyshev polynomials.&lt;/p&gt; &#xA;&lt;p&gt;It needs a low learning rate (2e-4) to train. The network is very sensitive to the learning rate.&lt;/p&gt; &#xA;&lt;p&gt;Note that it&#39;s still not as good as MLPs. Detailed comparison is on the way.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;del&gt;&lt;strong&gt;Function Interpolation:&lt;/strong&gt; much better than MLPs when the function is (mostly) smooth, very effective in discovering mathematical laws.&lt;/del&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SynodicMonth/ChebyKAN/main/img/Interpolation.png&#34; alt=&#34;alt text&#34;&gt; &lt;del&gt;ChebyKAN: [1, 8, 1] with 8 degree.&lt;/del&gt; &lt;del&gt;MLP: [1, 1024, 512, 1] with ReLU&lt;/del&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Edit: The comparison above is not fair.&lt;/strong&gt; &lt;strong&gt;Thanks @usamec for pointing out the mistake that the MLP was too big and not trained properly.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;!-- ~~Edit: Adding noise to the data does not affect the ChebyKAN&#39;s performance.~~ --&gt; &#xA;&lt;!-- ![alt text](img/Interpolation_noise.png) --&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fixed version:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Function Interpolation:&lt;/strong&gt; converge faster than MLPs when the function is (mostly) smooth.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SynodicMonth/ChebyKAN/main/img/Interpolation_fix.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ChebyKAN: [1, 8, 1] with 8 degree. MLP: [1, 128, 1] with Tanh.&lt;/p&gt; &#xA;&lt;p&gt;With decent training, the MLP can achieve similar performance as ChebyKAN. Note that ChebyKAN shows some overfitting.&lt;/p&gt; &#xA;&lt;p&gt;However ChebyKAN converges much faster than MLP.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SynodicMonth/ChebyKAN/main/img/Convergence_Speed.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ChebyKAN: Adam, lr=0.01. MLP: Adam, lr=0.03.&lt;/p&gt; &#xA;&lt;p&gt;@5000 epoch, ChebyKAN has already converged, while MLP is still far from convergence.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SynodicMonth/ChebyKAN/main/img/Early_Stopping.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Future Work&lt;/h1&gt; &#xA;&lt;p&gt;More experiments and optimizations are needed to prove the correctness and effectiveness of ChebyKAN. Not sure if the current parameters initialization is optimal. Maybe Xavier initialization is better. I&#39;m not sure if the current implementation is correct. Any suggestions are welcome.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google/zimtohrli</title>
    <updated>2024-05-11T01:29:24Z</updated>
    <id>tag:github.com,2024-05-11:/google/zimtohrli</id>
    <link href="https://github.com/google/zimtohrli" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/google/zimtohrli/actions&#34;&gt;&lt;img src=&#34;https://github.com/google/zimtohrli/workflows/Test%20Zimtohrli/badge.svg?sanitize=true&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Zimtohrli: A New Psychoacoustic Perceptual Metric for Audio Compression&lt;/h1&gt; &#xA;&lt;p&gt;Zimtohrli is a psychoacoustic perceptual metric that quantifies the human observable difference in two audio signals in the proximity of just-noticeable-differences.&lt;/p&gt; &#xA;&lt;p&gt;In this project we study the psychological and physiological responses associated with sound, to create a new more accurate model for measuring human-subjective similarity between sounds. The main focus will be on just-noticeable-difference to get most significant benefits in high quality audio compression. The main goals of the project is to further both existing and new practical audio (and video containing audio) compression, and also be able to plug in the resulting psychoacoustic similarity measure into audio related machine learning models.&lt;/p&gt; &#xA;&lt;p&gt;For more details about how Zimtohrli works, see &lt;a href=&#34;https://raw.githubusercontent.com/google/zimtohrli/main/zimtohrli.ipynb&#34;&gt;zimtohrli.ipynb&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;Zimtohrli is a project under development, and is built and tested in a Debian-like environment.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;Some dependencies for Zimtohrli are downloaded and managed by the build script, but others need to be installed before building.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmake&lt;/li&gt; &#xA; &lt;li&gt;ninja-build&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To build the compare tool, a few more dependencies are necessary:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;libogg-dev&lt;/li&gt; &#xA; &lt;li&gt;libvorbis-dev&lt;/li&gt; &#xA; &lt;li&gt;libflac-dev&lt;/li&gt; &#xA; &lt;li&gt;libopus-dev&lt;/li&gt; &#xA; &lt;li&gt;libasound2-dev&lt;/li&gt; &#xA; &lt;li&gt;libglfw3-dev&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Finally, to build and test the Python and Go wrappers, the following dependencies are necessary:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;golang-go&lt;/li&gt; &#xA; &lt;li&gt;python3&lt;/li&gt; &#xA; &lt;li&gt;xxd&lt;/li&gt; &#xA; &lt;li&gt;zlib1g-dev&lt;/li&gt; &#xA; &lt;li&gt;ffmpeg&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To install these in a Debian-like system:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install -y cmake ninja-build clang clang-tidy libogg-dev libvorbis-dev libflac-dev libopus-dev libasound2-dev libglfw3-dev golang-go python3 xxd zlib1g-dev ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once they are installed, configure the project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./configure.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build the project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(cd build &amp;amp;&amp;amp; ninja)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Address sanitizer build&lt;/h3&gt; &#xA;&lt;p&gt;To build with address sanitizer, configure a new build directory with asan configured:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./configure.sh asan&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build the project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(cd asan_build &amp;amp;&amp;amp; ninja)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Debug build&lt;/h3&gt; &#xA;&lt;p&gt;To build with debug symbols, configure a new build directory with debugging configured:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./configure.sh debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build the project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(cd debug_build &amp;amp;&amp;amp; ninja)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Testing&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;(cd build &amp;amp;&amp;amp; ninja &amp;amp;&amp;amp; ninja test)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>andrewgcodes/xlstm</title>
    <updated>2024-05-11T01:29:24Z</updated>
    <id>tag:github.com,2024-05-11:/andrewgcodes/xlstm</id>
    <link href="https://github.com/andrewgcodes/xlstm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;my attempts at implementing various bits of Sepp Hochreiter&#39;s new xLSTM architecture&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;xlstm&lt;/h1&gt; &#xA;&lt;p&gt;my attempts at implementing various bits of Sepp Hochreiter&#39;s new xLSTM architecture very oversimplified and probably somewhat wrong! please open PRs and make it better.&lt;/p&gt; &#xA;&lt;p&gt;mLSTM: &lt;a href=&#34;https://github.com/andrewgcodes/xlstm/raw/main/mLSTM.ipynb&#34;&gt;https://github.com/andrewgcodes/xlstm/blob/main/mLSTM.ipynb&lt;/a&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/andrewgcodes/xlstm/blob/main/mLSTM.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>