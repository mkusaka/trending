<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-01T01:44:35Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>alembics/disco-diffusion</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/alembics/disco-diffusion</id>
    <link href="https://github.com/alembics/disco-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Disco Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A frankensteinian amalgamation of notebooks, models and techniques for the generation of AI Art and Animations.&lt;/p&gt; &#xA;&lt;p&gt;[to be updated with further info soon]&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project uses a special conversion tool to convert the python files into notebooks for easier development.&lt;/p&gt; &#xA;&lt;p&gt;What this means is you do not have to touch the notebook directly to make changes to it&lt;/p&gt; &#xA;&lt;p&gt;the tool being used is called &lt;a href=&#34;https://github.com/MSFTserver/colab-convert&#34;&gt;Colab-Convert&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;install using &lt;code&gt;pip install colab-convert&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;convert .py to .ipynb &lt;code&gt;colab-convert /path/to/file.py /path/to/file.ipynb&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;convert .ipynb to .py &lt;code&gt;colab-convert /path/to/file.ipynb /path/to/file.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;h4&gt;v1 Oct 29th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial QoL improvements added, including user friendly UI, settings+prompt saving and improved google drive folder organization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v1.1 Nov 13th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Now includes sizing options, intermediate saves and fixed image prompts and perlin inits. unexposed batch option since it doesn&#39;t work&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2 Update: Nov 22nd 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial addition of Katherine Crowson&#39;s Secondary Model Method (&lt;a href=&#34;https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR&#34;&gt;https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix for incorrectly named settings files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v3 Update: Dec 24th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implemented Dango&#39;s advanced cutout method&lt;/li&gt; &#xA; &lt;li&gt;Added SLIP models, thanks to NeuralDivergent&lt;/li&gt; &#xA; &lt;li&gt;Fixed issue with NaNs resulting in black images, with massive help and testing from @Softology&lt;/li&gt; &#xA; &lt;li&gt;Perlin now changes properly within batches (not sure where this perlin_regen code came from originally, but thank you)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v4 Update: Jan 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implemented Diffusion Zooming&lt;/li&gt; &#xA; &lt;li&gt;Added Chigozie keyframing&lt;/li&gt; &#xA; &lt;li&gt;Made a bunch of edits to processes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v4.1 Update: Jan 14th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added video input mode&lt;/li&gt; &#xA; &lt;li&gt;Added license that somehow went missing&lt;/li&gt; &#xA; &lt;li&gt;Added improved prompt keyframing, fixed image_prompts and multiple prompts&lt;/li&gt; &#xA; &lt;li&gt;Improved UI&lt;/li&gt; &#xA; &lt;li&gt;Significant under the hood cleanup and improvement&lt;/li&gt; &#xA; &lt;li&gt;Refined defaults for each mode&lt;/li&gt; &#xA; &lt;li&gt;Removed SLIP models for the time being due to import conflicts&lt;/li&gt; &#xA; &lt;li&gt;Added latent-diffusion SuperRes for sharpening&lt;/li&gt; &#xA; &lt;li&gt;Added resume run mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5 Update: Feb 20th 2022 - gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added 3D animation mode. Uses weighted combination of AdaBins and MiDaS depth estimation models. Uses pytorch3d for 3D transforms on Colab and/or Linux.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.1 Update: Mar 30th 2022 - zippy / Chris Allen and gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integrated Turbo+Smooth features from Disco Diffusion Turbo -- just the implementation, without its defaults.&lt;/li&gt; &#xA; &lt;li&gt;Implemented resume of turbo animations in such a way that it&#39;s now possible to resume from different batch folders and batch numbers.&lt;/li&gt; &#xA; &lt;li&gt;3D rotation parameter units are now degrees (rather than radians)&lt;/li&gt; &#xA; &lt;li&gt;Corrected name collision in sampling_mode (now diffusion_sampling_mode for plms/ddim, and sampling_mode for 3D transform sampling)&lt;/li&gt; &#xA; &lt;li&gt;Added video_init_seed_continuity option to make init video animations more continuous&lt;/li&gt; &#xA; &lt;li&gt;Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Remove Super Resolution&lt;/li&gt; &#xA; &lt;li&gt;Remove Slip Models&lt;/li&gt; &#xA; &lt;li&gt;Update for crossplatform support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.1 Update: Apr 4th 2022 - MSFTserver aka HostsServer&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Remove Super Resolution&lt;/li&gt; &#xA; &lt;li&gt;Remove Slip Models&lt;/li&gt; &#xA; &lt;li&gt;Update for crossplatform support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.2 Update: Apr 10th 2022 - nin_artificial / Tom Mason&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;VR Mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notebook Provenance&lt;/h2&gt; &#xA;&lt;p&gt;Original notebook by Katherine Crowson (&lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;https://github.com/crowsonkb&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/RiversHaveWings&#34;&gt;https://twitter.com/RiversHaveWings&lt;/a&gt;). It uses either OpenAI&#39;s 256x256 unconditional ImageNet or Katherine Crowson&#39;s fine-tuned 512x512 diffusion model (&lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;https://github.com/openai/guided-diffusion&lt;/a&gt;), together with CLIP (&lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;https://github.com/openai/CLIP&lt;/a&gt;) to connect text prompts with images.&lt;/p&gt; &#xA;&lt;p&gt;Modified by Daniel Russell (&lt;a href=&#34;https://github.com/russelldc&#34;&gt;https://github.com/russelldc&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/danielrussruss&#34;&gt;https://twitter.com/danielrussruss&lt;/a&gt;) to include (hopefully) optimal params for quick generations in 15-100 timesteps rather than 1000, as well as more robust augmentations.&lt;/p&gt; &#xA;&lt;p&gt;Further improvements from Dango233 and nsheppard helped improve the quality of diffusion in general, and especially so for shorter runs like this notebook aims to achieve.&lt;/p&gt; &#xA;&lt;p&gt;Vark added code to load in multiple Clip models at once, which all prompts are evaluated against, which may greatly improve accuracy.&lt;/p&gt; &#xA;&lt;p&gt;The latest zoom, pan, rotation, and keyframes features were taken from Chigozie Nri&#39;s VQGAN Zoom Notebook (&lt;a href=&#34;https://github.com/chigozienri&#34;&gt;https://github.com/chigozienri&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/chigozienri&#34;&gt;https://twitter.com/chigozienri&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Advanced DangoCutn Cutout method is also from Dango223.&lt;/p&gt; &#xA;&lt;p&gt;--&lt;/p&gt; &#xA;&lt;p&gt;Somnai (&lt;a href=&#34;https://twitter.com/Somnai_dreams&#34;&gt;https://twitter.com/Somnai_dreams&lt;/a&gt;) added 2D Diffusion animation techniques, QoL improvements and various implementations of tech and techniques, mostly listed in the changelog below.&lt;/p&gt; &#xA;&lt;p&gt;3D animation implementation added by Adam Letts (&lt;a href=&#34;https://twitter.com/gandamu_ml&#34;&gt;https://twitter.com/gandamu_ml&lt;/a&gt;) in collaboration with Somnai.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bmild/nerf</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/bmild/nerf</id>
    <link href="https://github.com/bmild/nerf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code release for NeRF (Neural Radiance Fields)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NeRF: Neural Radiance Fields&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;http://tancik.com/nerf&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/JuH79E8rdKc&#34;&gt;Video&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2003.08934&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;Data&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Tiny-NeRF in Colab&#34;&gt;&lt;/a&gt;&lt;br&gt; Tensorflow implementation of optimizing a neural representation for a single scene and rendering new views.&lt;br&gt;&lt;br&gt; &lt;a href=&#34;http://tancik.com/nerf&#34;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://people.eecs.berkeley.edu/~bmild/&#34;&gt;Ben Mildenhall&lt;/a&gt;*&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~pratul/&#34;&gt;Pratul P. Srinivasan&lt;/a&gt;*&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;http://tancik.com/&#34;&gt;Matthew Tancik&lt;/a&gt;*&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;http://jonbarron.info/&#34;&gt;Jonathan T. Barron&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;http://cseweb.ucsd.edu/~ravir/&#34;&gt;Ravi Ramamoorthi&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html&#34;&gt;Ren Ng&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;br&gt; &lt;sup&gt;1&lt;/sup&gt;UC Berkeley, &lt;sup&gt;2&lt;/sup&gt;Google Research, &lt;sup&gt;3&lt;/sup&gt;UC San Diego&lt;br&gt; *denotes equal contribution&lt;br&gt; in ECCV 2020 (Oral Presentation, Best Paper Honorable Mention)&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/bmild/nerf/master/imgs/pipeline.jpg&#34;&gt; &#xA;&lt;h2&gt;TL;DR quickstart&lt;/h2&gt; &#xA;&lt;p&gt;To setup a conda environment, download example training data, begin the training process, and launch Tensorboard:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;conda activate nerf&#xA;bash download_example_data.sh&#xA;python run_nerf.py --config config_fern.txt&#xA;tensorboard --logdir=logs/summaries --port=6006&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If everything works without errors, you can now go to &lt;code&gt;localhost:6006&lt;/code&gt; in your browser and watch the &#34;Fern&#34; scene train.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Python 3 dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tensorflow 1.15&lt;/li&gt; &#xA; &lt;li&gt;matplotlib&lt;/li&gt; &#xA; &lt;li&gt;numpy&lt;/li&gt; &#xA; &lt;li&gt;imageio&lt;/li&gt; &#xA; &lt;li&gt;configargparse&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The LLFF data loader requires ImageMagick.&lt;/p&gt; &#xA;&lt;p&gt;We provide a conda environment setup file including all of the above dependencies. Create the conda environment &lt;code&gt;nerf&lt;/code&gt; by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will also need the &lt;a href=&#34;http://github.com/fyusion/llff&#34;&gt;LLFF code&lt;/a&gt; (and COLMAP) set up to compute poses if you want to run on your own real data.&lt;/p&gt; &#xA;&lt;h2&gt;What is a NeRF?&lt;/h2&gt; &#xA;&lt;p&gt;A neural radiance field is a simple fully connected network (weights are ~5MB) trained to reproduce input views of a single scene using a rendering loss. The network directly maps from spatial location and viewing direction (5D input) to color and opacity (4D output), acting as the &#34;volume&#34; so we can use volume rendering to differentiably render new views.&lt;/p&gt; &#xA;&lt;p&gt;Optimizing a NeRF takes between a few hours and a day or two (depending on resolution) and only requires a single GPU. Rendering an image from an optimized NeRF takes somewhere between less than a second and ~30 seconds, again depending on resolution.&lt;/p&gt; &#xA;&lt;h2&gt;Running code&lt;/h2&gt; &#xA;&lt;p&gt;Here we show how to run our code on two example scenes. You can download the rest of the synthetic and real data used in the paper &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Optimizing a NeRF&lt;/h3&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash download_example_data.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to get the our synthetic Lego dataset and the LLFF Fern dataset.&lt;/p&gt; &#xA;&lt;p&gt;To optimize a low-res Fern NeRF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_nerf.py --config config_fern.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After 200k iterations (about 15 hours), you should get a video like this at &lt;code&gt;logs/fern_test/fern_test_spiral_200000_rgb.mp4&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://people.eecs.berkeley.edu/~bmild/nerf/fern_200k_256w.gif&#34; alt=&#34;ferngif&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;To optimize a low-res Lego NeRF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_nerf.py --config config_lego.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After 200k iterations, you should get a video like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://people.eecs.berkeley.edu/~bmild/nerf/lego_200k_256w.gif&#34; alt=&#34;legogif&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Rendering a NeRF&lt;/h3&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash download_example_weights.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to get a pretrained high-res NeRF for the Fern dataset. Now you can use &lt;a href=&#34;https://github.com/bmild/nerf/raw/master/render_demo.ipynb&#34;&gt;&lt;code&gt;render_demo.ipynb&lt;/code&gt;&lt;/a&gt; to render new views.&lt;/p&gt; &#xA;&lt;h3&gt;Replicating the paper results&lt;/h3&gt; &#xA;&lt;p&gt;The example config files run at lower resolutions than the quantitative/qualitative results in the paper and video. To replicate the results from the paper, start with the config files in &lt;a href=&#34;https://github.com/bmild/nerf/tree/master/paper_configs&#34;&gt;&lt;code&gt;paper_configs/&lt;/code&gt;&lt;/a&gt;. Our synthetic Blender data and LLFF scenes are hosted &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt; and the DeepVoxels data is hosted by Vincent Sitzmann &lt;a href=&#34;https://drive.google.com/open?id=1lUvJWB6oFtT8EQ_NzBrXnmi25BufxRfl&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Extracting geometry from a NeRF&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://github.com/bmild/nerf/raw/master/extract_mesh.ipynb&#34;&gt;&lt;code&gt;extract_mesh.ipynb&lt;/code&gt;&lt;/a&gt; for an example of running marching cubes to extract a triangle mesh from a trained NeRF network. You&#39;ll need the install the &lt;a href=&#34;https://github.com/pmneila/PyMCubes&#34;&gt;PyMCubes&lt;/a&gt; package for marching cubes plus the &lt;a href=&#34;https://github.com/mikedh/trimesh&#34;&gt;trimesh&lt;/a&gt; and &lt;a href=&#34;https://github.com/mmatl/pyrender&#34;&gt;pyrender&lt;/a&gt; packages if you want to render the mesh inside the notebook:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install trimesh pyrender PyMCubes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generating poses for your own scenes&lt;/h2&gt; &#xA;&lt;h3&gt;Don&#39;t have poses?&lt;/h3&gt; &#xA;&lt;p&gt;We recommend using the &lt;code&gt;imgs2poses.py&lt;/code&gt; script from the &lt;a href=&#34;https://github.com/fyusion/llff&#34;&gt;LLFF code&lt;/a&gt;. Then you can pass the base scene directory into our code using &lt;code&gt;--datadir &amp;lt;myscene&amp;gt;&lt;/code&gt; along with &lt;code&gt;-dataset_type llff&lt;/code&gt;. You can take a look at the &lt;code&gt;config_fern.txt&lt;/code&gt; config file for example settings to use for a forward facing scene. For a spherically captured 360 scene, we recomment adding the &lt;code&gt;--no_ndc --spherify --lindisp&lt;/code&gt; flags.&lt;/p&gt; &#xA;&lt;h3&gt;Already have poses!&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;run_nerf.py&lt;/code&gt; and all other code, we use the same pose coordinate system as in OpenGL: the local camera coordinate system of an image is defined in a way that the X axis points to the right, the Y axis upwards, and the Z axis backwards as seen from the image.&lt;/p&gt; &#xA;&lt;p&gt;Poses are stored as 3x4 numpy arrays that represent camera-to-world transformation matrices. The other data you will need is simple pinhole camera intrinsics (&lt;code&gt;hwf = [height, width, focal length]&lt;/code&gt;) and near/far scene bounds. Take a look at &lt;a href=&#34;https://github.com/bmild/nerf/raw/master/run_nerf.py#L406&#34;&gt;our data loading code&lt;/a&gt; to see more.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{mildenhall2020nerf,&#xA;  title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},&#xA;  author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},&#xA;  year={2020},&#xA;  booktitle={ECCV},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>labmlai/annotated_deep_learning_paper_implementations</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/labmlai/annotated_deep_learning_paper_implementations</id>
    <link href="https://github.com/labmlai/annotated_deep_learning_paper_implementations" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🧑‍🏫 50! Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, ...), gans(cyclegan, stylegan2, ...), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, ... 🧠&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/labmlai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/labmlai?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://nn.labml.ai/index.html&#34;&gt;labml.ai Deep Learning Paper Implementations&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This is a collection of simple PyTorch implementations of neural networks and related algorithms. These implementations are documented with explanations,&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nn.labml.ai/index.html&#34;&gt;The website&lt;/a&gt; renders these as side-by-side formatted notes. We believe these would help you understand these algorithms better.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://nn.labml.ai/dqn-light.png&#34; alt=&#34;Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We are actively maintaining this repo and adding new implementations almost weekly. &lt;a href=&#34;https://twitter.com/labmlai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/labmlai?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; for updates.&lt;/p&gt; &#xA;&lt;h2&gt;Paper Implementations&lt;/h2&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/transformers/index.html&#34;&gt;Transformers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mha.html&#34;&gt;Multi-headed attention&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/models.html&#34;&gt;Transformer building blocks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/xl/index.html&#34;&gt;Transformer XL&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/xl/relative_mha.html&#34;&gt;Relative multi-headed attention&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/rope/index.html&#34;&gt;Rotary Positional Embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/retro/index.html&#34;&gt;RETRO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/compressive/index.html&#34;&gt;Compressive Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/gpt/index.html&#34;&gt;GPT Architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/glu_variants/simple.html&#34;&gt;GLU Variants&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/knn&#34;&gt;kNN-LM: Generalization through Memorization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/feedback/index.html&#34;&gt;Feedback Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/switch/index.html&#34;&gt;Switch Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/fast_weights/index.html&#34;&gt;Fast Weights Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/fnet/index.html&#34;&gt;FNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/aft/index.html&#34;&gt;Attention Free Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mlm/index.html&#34;&gt;Masked Language Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/mlp_mixer/index.html&#34;&gt;MLP-Mixer: An all-MLP Architecture for Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/gmlp/index.html&#34;&gt;Pay Attention to MLPs (gMLP)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/vit/index.html&#34;&gt;Vision Transformer (ViT)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/primer_ez/index.html&#34;&gt;Primer EZ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/hour_glass/index.html&#34;&gt;Hourglass&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/recurrent_highway_networks/index.html&#34;&gt;Recurrent Highway Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/lstm/index.html&#34;&gt;LSTM&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/hypernetworks/hyper_lstm.html&#34;&gt;HyperNetworks - HyperLSTM&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/resnet/index.html&#34;&gt;ResNet&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/conv_mixer/index.html&#34;&gt;ConvMixer&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/capsule_networks/index.html&#34;&gt;Capsule Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/gan/index.html&#34;&gt;Generative Adversarial Networks&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/original/index.html&#34;&gt;Original GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/dcgan/index.html&#34;&gt;GAN with deep convolutional network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/cycle_gan/index.html&#34;&gt;Cycle GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/wasserstein/index.html&#34;&gt;Wasserstein GAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/wasserstein/gradient_penalty/index.html&#34;&gt;Wasserstein GAN with Gradient Penalty&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/gan/stylegan/index.html&#34;&gt;StyleGAN 2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/diffusion/index.html&#34;&gt;Diffusion models&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/diffusion/ddpm/index.html&#34;&gt;Denoising Diffusion Probabilistic Models (DDPM)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/sketch_rnn/index.html&#34;&gt;Sketch RNN&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ Graph Neural Networks&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/graphs/gat/index.html&#34;&gt;Graph Attention Networks (GAT)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/graphs/gatv2/index.html&#34;&gt;Graph Attention Networks v2 (GATv2)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/cfr/index.html&#34;&gt;Counterfactual Regret Minimization (CFR)&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Solving games with incomplete information such as poker with CFR.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/cfr/kuhn/index.html&#34;&gt;Kuhn Poker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/rl/index.html&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/rl/ppo/index.html&#34;&gt;Proximal Policy Optimization&lt;/a&gt; with &lt;a href=&#34;https://nn.labml.ai/rl/ppo/gae.html&#34;&gt;Generalized Advantage Estimation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/rl/dqn/index.html&#34;&gt;Deep Q Networks&lt;/a&gt; with with &lt;a href=&#34;https://nn.labml.ai/rl/dqn/model.html&#34;&gt;Dueling Network&lt;/a&gt;, &lt;a href=&#34;https://nn.labml.ai/rl/dqn/replay_buffer.html&#34;&gt;Prioritized Replay&lt;/a&gt; and Double Q Network.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/optimizers/index.html&#34;&gt;Optimizers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/adam.html&#34;&gt;Adam&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/amsgrad.html&#34;&gt;AMSGrad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/adam_warmup.html&#34;&gt;Adam Optimizer with warmup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/noam.html&#34;&gt;Noam Optimizer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/radam.html&#34;&gt;Rectified Adam Optimizer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/optimizers/ada_belief.html&#34;&gt;AdaBelief Optimizer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/normalization/index.html&#34;&gt;Normalization Layers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/batch_norm/index.html&#34;&gt;Batch Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/layer_norm/index.html&#34;&gt;Layer Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/instance_norm/index.html&#34;&gt;Instance Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/group_norm/index.html&#34;&gt;Group Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/weight_standardization/index.html&#34;&gt;Weight Standardization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/batch_channel_norm/index.html&#34;&gt;Batch-Channel Normalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/normalization/deep_norm/index.html&#34;&gt;DeepNorm&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/distillation/index.html&#34;&gt;Distillation&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/adaptive_computation/index.html&#34;&gt;Adaptive Computation&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/adaptive_computation/ponder_net/index.html&#34;&gt;PonderNet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/uncertainty/index.html&#34;&gt;Uncertainty&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/uncertainty/evidence/index.html&#34;&gt;Evidential Deep Learning to Quantify Classification Uncertainty&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;✨ &lt;a href=&#34;https://nn.labml.ai/activations/index.html&#34;&gt;Activations&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/activations/fta/index.html&#34;&gt;Fuzzy Tiling Activations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Highlighted Research Paper PDFs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2204.10628.pdf&#34;&gt;Autoregressive Search Engines: Generating Substrings as Document Identifiers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2203.15556.pdf&#34;&gt;Training Compute-Optimal Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1910.02054.pdf&#34;&gt;ZeRO: Memory Optimizations Toward Training Trillion Parameter Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2204.02311.pdf&#34;&gt;PaLM: Scaling Language Modeling with Pathways&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/dall-e-2.pdf&#34;&gt;Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2203.14465.pdf&#34;&gt;STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2112.04426.pdf&#34;&gt;Improving language models by retrieving from trillions of tokens&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2003.08934.pdf&#34;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1706.03762.pdf&#34;&gt;Attention Is All You Need&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2006.11239.pdf&#34;&gt;Denoising Diffusion Probabilistic Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2109.08668.pdf&#34;&gt;Primer: Searching for Efficient Transformers for Language Modeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1803.02999.pdf&#34;&gt;On First-Order Meta-Learning Algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2103.00020.pdf&#34;&gt;Learning Transferable Visual Models From Natural Language Supervision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/2109.02869.pdf&#34;&gt;The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/1805.09801.pdf&#34;&gt;Meta-Gradient Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/google_maps_eta.pdf&#34;&gt;ETA Prediction with Graph Neural Networks in Google Maps&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/ponder_net.pdf&#34;&gt;PonderNet: Learning to Ponder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/muzero.pdf&#34;&gt;Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/gans_n_roses.pdf&#34;&gt;GANs N’ Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/vit.pdf&#34;&gt;An Image is Worth 16X16 Word: Transformers for Image Recognition at Scale&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/resnet.pdf&#34;&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labmlai/annotated_deep_learning_paper_implementations/raw/master/papers/distillation.pdf&#34;&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install labml-nn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Citing&lt;/h3&gt; &#xA;&lt;p&gt;If you use this for academic research, please cite it using the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{labml,&#xA; author = {Varuna Jayasiri, Nipun Wijerathne},&#xA; title = {labml.ai Annotated Paper Implementations},&#xA; year = {2020},&#xA; url = {https://nn.labml.ai/},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Other Projects&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://papers.labml.ai/&#34;&gt;🚀 Trending Research Papers&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This shows the most popular research papers on social media. It also aggregates links to useful resources like paper explanations videos and discussions.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://github.com/labmlai/labml&#34;&gt;🧪 labml.ai/labml&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is a library that let&#39;s you monitor deep learning model training and hardware usage from your mobile phone. It also comes with a bunch of other tools to help write deep learning code efficiently.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kubernetes/community</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/kubernetes/community</id>
    <link href="https://github.com/kubernetes/community" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kubernetes community content&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kubernetes Community&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the Kubernetes community!&lt;/p&gt; &#xA;&lt;p&gt;This is the starting point for joining and contributing to the Kubernetes community - improving docs, improving code, giving talks etc.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about the project structure and organization, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/governance.md&#34;&gt;Project Governance&lt;/a&gt; information.&lt;/p&gt; &#xA;&lt;h2&gt;Communicating&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/communication/&#34;&gt;communication&lt;/a&gt; page lists communication channels like chat, issues, mailing lists, conferences, etc.&lt;/p&gt; &#xA;&lt;p&gt;For more specific topics, try a SIG.&lt;/p&gt; &#xA;&lt;h2&gt;Governance&lt;/h2&gt; &#xA;&lt;p&gt;Kubernetes has the following types of groups that are officially supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Committees&lt;/strong&gt; are named sets of people that are chartered to take on sensitive topics. This group is encouraged to be as open as possible while achieving its mission but, because of the nature of the topics discussed, private communications are allowed. Examples of committees include the steering committee and things like security or code of conduct.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Special Interest Groups (SIGs)&lt;/strong&gt; are persistent open groups that focus on a part of the project. SIGs must have open and transparent proceedings. Anyone is welcome to participate and contribute provided they follow the Kubernetes Code of Conduct. The purpose of a SIG is to own and develop a set of &lt;strong&gt;subprojects&lt;/strong&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Subprojects&lt;/strong&gt; Each SIG can have a set of subprojects. These are smaller groups that can work independently. Some subprojects will be part of the main Kubernetes deliverables while others will be more speculative and live in the &lt;code&gt;kubernetes-sigs&lt;/code&gt; github org.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Working Groups&lt;/strong&gt; are temporary groups that are formed to address issues that cross SIG boundaries. Working groups do not own any code or other long term artifacts. Working groups can report back and act through involved SIGs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User Groups&lt;/strong&gt; are groups for facilitating communication and discovery of information related to topics that have long term relevance to large groups of Kubernetes users. They do not have ownership of parts of the Kubernetes code base.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/governance.md&#34;&gt;full governance doc&lt;/a&gt; for more details on these groups.&lt;/p&gt; &#xA;&lt;p&gt;A SIG can have its own policy for contribution, described in a &lt;code&gt;README&lt;/code&gt; or &lt;code&gt;CONTRIBUTING&lt;/code&gt; file in the SIG folder in this repo (e.g. &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/sig-cli/CONTRIBUTING.md&#34;&gt;sig-cli/CONTRIBUTING.md&lt;/a&gt;), and its own mailing list, slack channel, etc.&lt;/p&gt; &#xA;&lt;p&gt;If you want to edit details about a SIG (e.g. its weekly meeting time or its leads), please follow &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/generator&#34;&gt;these instructions&lt;/a&gt; that detail how our docs are auto-generated.&lt;/p&gt; &#xA;&lt;h2&gt;Learn to Build&lt;/h2&gt; &#xA;&lt;p&gt;Links in &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/devel/README.md&#34;&gt;contributors/devel/README.md&lt;/a&gt; lead to many relevant technical topics.&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;A first step to contributing is to pick from the &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/sig-list.md&#34;&gt;list of kubernetes SIGs&lt;/a&gt;. Start attending SIG meetings, join the slack channel and subscribe to the mailing list. SIGs will often have a set of &#34;help wanted&#34; issues that can help new contributors get involved.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/README.md&#34;&gt;Contributor Guide&lt;/a&gt; provides detailed instruction on how to get your ideas and bug fixes seen and accepted, including:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;How to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/first-contribution.md#file-an-issue&#34;&gt;file an issue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;How to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/first-contribution.md#find-something-to-work-on&#34;&gt;find something to work on&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;How to &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/contributors/guide/contributing.md#opening-a-pull-request&#34;&gt;open a pull request&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Membership&lt;/h2&gt; &#xA;&lt;p&gt;We encourage all contributors to become members. We aim to grow an active, healthy community of contributors, reviewers, and code owners. Learn more about requirements and responsibilities of membership in our &lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/community/master/community-membership.md&#34;&gt;Community Membership&lt;/a&gt; page.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/EconML</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/microsoft/EconML</id>
    <link href="https://github.com/microsoft/EconML" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ALICE (Automated Learning and Intelligence for Causation and Economics) is a Microsoft Research project aimed at applying Artificial Intelligence concepts to economic decision making. One of its goals is to build a toolkit that combines state-of-the-art machine learning techniques with econometrics in order to bring automation to complex causal …&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://dev.azure.com/ms/EconML/_build/latest?definitionId=49&amp;amp;branchName=main&#34;&gt;&lt;img src=&#34;https://dev.azure.com/ms/EconML/_apis/build/status/Microsoft.EconML?branchName=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/econml/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/econml.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/econml/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/wheel/econml.svg?sanitize=true&#34; alt=&#34;PyPI wheel&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/econml/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/econml.svg?sanitize=true&#34; alt=&#34;Supported Python versions&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/doc/econml-logo-icon.png&#34; width=&#34;80px&#34; align=&#34;left&#34; style=&#34;margin-right: 10px;&#34;&gt; EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;EconML&lt;/strong&gt; is a Python package for estimating heterogeneous treatment effects from observational data via machine learning. This package was designed and built as part of the &lt;a href=&#34;https://www.microsoft.com/en-us/research/project/alice/&#34;&gt;ALICE project&lt;/a&gt; at Microsoft Research with the goal to combine state-of-the-art machine learning techniques with econometrics to bring automation to complex causal inference problems. The promise of EconML:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implement recent techniques in the literature at the intersection of econometrics and machine learning&lt;/li&gt; &#xA; &lt;li&gt;Maintain flexibility in modeling the effect heterogeneity (via techniques such as random forests, boosting, lasso and neural nets), while preserving the causal interpretation of the learned model and often offering valid confidence intervals&lt;/li&gt; &#xA; &lt;li&gt;Use a unified API&lt;/li&gt; &#xA; &lt;li&gt;Build on standard Python packages for Machine Learning and Data Analysis&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;One of the biggest promises of machine learning is to automate decision making in a multitude of domains. At the core of many data-driven personalized decision scenarios is the estimation of heterogeneous treatment effects: what is the causal effect of an intervention on an outcome of interest for a sample with a particular set of features? In a nutshell, this toolkit is designed to measure the causal effect of some treatment variable(s) &lt;code&gt;T&lt;/code&gt; on an outcome variable &lt;code&gt;Y&lt;/code&gt;, controlling for a set of features &lt;code&gt;X, W&lt;/code&gt; and how does that effect vary as a function of &lt;code&gt;X&lt;/code&gt;. The methods implemented are applicable even with observational (non-experimental or historical) datasets. For the estimation results to have a causal interpretation, some methods assume no unobserved confounders (i.e. there is no unobserved variable not included in &lt;code&gt;X, W&lt;/code&gt; that simultaneously has an effect on both &lt;code&gt;T&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt;), while others assume access to an instrument &lt;code&gt;Z&lt;/code&gt; (i.e. an observed variable &lt;code&gt;Z&lt;/code&gt; that has an effect on the treatment &lt;code&gt;T&lt;/code&gt; but no direct effect on the outcome &lt;code&gt;Y&lt;/code&gt;). Most methods provide confidence intervals and inference results.&lt;/p&gt; &#xA;&lt;p&gt;For detailed information about the package, consult the documentation at &lt;a href=&#34;https://econml.azurewebsites.net/&#34;&gt;https://econml.azurewebsites.net/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For information on use cases and background material on causal inference and heterogeneous treatment effects see our webpage at &lt;a href=&#34;https://www.microsoft.com/en-us/research/project/econml/&#34;&gt;https://www.microsoft.com/en-us/research/project/econml/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#news&#34;&gt;News&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#usage-examples&#34;&gt;Usage Examples&lt;/a&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#estimation-methods&#34;&gt;Estimation Methods&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#interpretability&#34;&gt;Interpretability&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#causal-model-selection-and-cross-validation&#34;&gt;Causal Model Selection and Cross-Validation&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#policy-learning&#34;&gt;Policy Learning&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#for-developers&#34;&gt;For Developers&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#running-the-tests&#34;&gt;Running the tests&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#generating-the-documentation&#34;&gt;Generating the documentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#blogs-and-publications&#34;&gt;Blogs and Publications&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#contributing-and-feedback&#34;&gt;Contributing and Feedback&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;March 11, 2022:&lt;/strong&gt; Call for Content&lt;/p&gt; &#xA;&lt;p&gt;Hello everyone, Microsoft will be hosting a workshop to explore current and future applications for our EconML and DoWhy packages on Tuesday, May 3, 2022. So far, our goal has been to make cutting-edge methods for causal treatment effect estimation as accessible as machine learning models for prediction or classification. We’re charting the course for future development of EconML and need your help.&lt;/p&gt; &#xA;&lt;p&gt;What more would you like to see in the library? New kinds of tasks, better functionality for the core tasks? Let us know! We are also looking for stories of problems you have solved using DoWhy and/or EconML to highlight in the workshop. If you have one, please reach out to &lt;a href=&#34;mailto:econml@microsoft.com&#34;&gt;econml@microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;January 31, 2022:&lt;/strong&gt; Release v0.13.0, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.13.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Previous releases&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;August 13, 2021:&lt;/strong&gt; Release v0.12.0, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.12.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;August 5, 2021:&lt;/strong&gt; Release v0.12.0b6, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.12.0b6&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;August 3, 2021:&lt;/strong&gt; Release v0.12.0b5, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.12.0b5&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;July 9, 2021:&lt;/strong&gt; Release v0.12.0b4, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.12.0b4&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;June 25, 2021:&lt;/strong&gt; Release v0.12.0b3, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.12.0b3&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;June 18, 2021:&lt;/strong&gt; Release v0.12.0b2, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.12.0b2&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;June 7, 2021:&lt;/strong&gt; Release v0.12.0b1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.12.0b1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;May 18, 2021:&lt;/strong&gt; Release v0.11.1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.11.1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;May 8, 2021:&lt;/strong&gt; Release v0.11.0, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.11.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 22, 2021:&lt;/strong&gt; Release v0.10.0, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.10.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 11, 2021:&lt;/strong&gt; Release v0.9.2, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.9.2&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 3, 2021:&lt;/strong&gt; Release v0.9.1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.9.1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;February 20, 2021:&lt;/strong&gt; Release v0.9.0, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.9.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;January 20, 2021:&lt;/strong&gt; Release v0.9.0b1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.9.0b1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;November 20, 2020:&lt;/strong&gt; Release v0.8.1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.8.1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;November 18, 2020:&lt;/strong&gt; Release v0.8.0, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.8.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;September 4, 2020:&lt;/strong&gt; Release v0.8.0b1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.8.0b1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 6, 2020:&lt;/strong&gt; Release v0.7.0, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.7.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;February 18, 2020:&lt;/strong&gt; Release v0.7.0b1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.7.0b1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;January 10, 2020:&lt;/strong&gt; Release v0.6.1, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.6.1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;December 6, 2019:&lt;/strong&gt; Release v0.6, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.6&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;November 21, 2019:&lt;/strong&gt; Release v0.5, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.5&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;June 3, 2019:&lt;/strong&gt; Release v0.4, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.4&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;May 3, 2019:&lt;/strong&gt; Release v0.3, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.3&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;April 10, 2019:&lt;/strong&gt; Release v0.2, see release notes &lt;a href=&#34;https://github.com/Microsoft/EconML/releases/tag/v0.2&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 6, 2019:&lt;/strong&gt; Release v0.1, welcome to have a try and provide feedback.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install the latest release from &lt;a href=&#34;https://pypi.org/project/econml/&#34;&gt;PyPI&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install econml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install from source, see &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#for-developers&#34;&gt;For Developers&lt;/a&gt; section below.&lt;/p&gt; &#xA;&lt;h2&gt;Usage Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Estimation Methods&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Double Machine Learning (aka RLearner) (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import LinearDML&#xA;from sklearn.linear_model import LassoCV&#xA;from econml.inference import BootstrapInference&#xA;&#xA;est = LinearDML(model_y=LassoCV(), model_t=LassoCV())&#xA;### Estimate with OLS confidence intervals&#xA;est.fit(Y, T, X=X, W=W) # W -&amp;gt; high-dimensional confounders, X -&amp;gt; features&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # OLS confidence intervals&#xA;&#xA;### Estimate with bootstrap confidence intervals&#xA;est.fit(Y, T, X=X, W=W, inference=&#39;bootstrap&#39;)  # with default bootstrap parameters&#xA;est.fit(Y, T, X=X, W=W, inference=BootstrapInference(n_bootstrap_samples=100))  # or customized&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # Bootstrap confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Sparse linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import SparseLinearDML&#xA;from sklearn.linear_model import LassoCV&#xA;&#xA;est = SparseLinearDML(model_y=LassoCV(), model_t=LassoCV())&#xA;est.fit(Y, T, X=X, W=W) # X -&amp;gt; high dimensional features&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # Confidence intervals via debiased lasso&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Generic Machine Learning last stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import NonParamDML&#xA;from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier&#xA;&#xA;est = NonParamDML(model_y=RandomForestRegressor(),&#xA;                  model_t=RandomForestClassifier(),&#xA;                  model_final=RandomForestRegressor(),&#xA;                  discrete_treatment=True)&#xA;est.fit(Y, T, X=X, W=W) &#xA;treatment_effects = est.effect(X_test)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Dynamic Double Machine Learning (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dynamic.dml import DynamicDML&#xA;# Use defaults&#xA;est = DynamicDML()&#xA;# Or specify hyperparameters&#xA;est = DynamicDML(model_y=LassoCV(cv=3), &#xA;                 model_t=LassoCV(cv=3), &#xA;                 cv=3)&#xA;est.fit(Y, T, X=X, W=None, groups=groups, inference=&#34;auto&#34;)&#xA;# Effects&#xA;treatment_effects = est.effect(X_test)&#xA;# Confidence intervals&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Causal Forests (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import CausalForestDML&#xA;from sklearn.linear_model import LassoCV&#xA;# Use defaults&#xA;est = CausalForestDML()&#xA;# Or specify hyperparameters&#xA;est = CausalForestDML(criterion=&#39;het&#39;, n_estimators=500,       &#xA;                      min_samples_leaf=10, &#xA;                      max_depth=10, max_samples=0.5,&#xA;                      discrete_treatment=False,&#xA;                      model_t=LassoCV(), model_y=LassoCV())&#xA;est.fit(Y, T, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;# Confidence intervals via Bootstrap-of-Little-Bags for forests&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Orthogonal Random Forests (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.orf import DMLOrthoForest, DROrthoForest&#xA;from econml.sklearn_extensions.linear_model import WeightedLasso, WeightedLassoCV&#xA;# Use defaults&#xA;est = DMLOrthoForest()&#xA;est = DROrthoForest()&#xA;# Or specify hyperparameters&#xA;est = DMLOrthoForest(n_trees=500, min_leaf_size=10,&#xA;                     max_depth=10, subsample_ratio=0.7,&#xA;                     lambda_reg=0.01,&#xA;                     discrete_treatment=False,&#xA;                     model_T=WeightedLasso(alpha=0.01), model_Y=WeightedLasso(alpha=0.01),&#xA;                     model_T_final=WeightedLassoCV(cv=3), model_Y_final=WeightedLassoCV(cv=3))&#xA;est.fit(Y, T, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;# Confidence intervals via Bootstrap-of-Little-Bags for forests&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Meta-Learners (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;XLearner&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.metalearners import XLearner&#xA;from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor&#xA;&#xA;est = XLearner(models=GradientBoostingRegressor(),&#xA;              propensity_model=GradientBoostingClassifier(),&#xA;              cate_models=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=np.hstack([X, W]))&#xA;treatment_effects = est.effect(np.hstack([X_test, W_test]))&#xA;&#xA;# Fit with bootstrap confidence interval construction enabled&#xA;est.fit(Y, T, X=np.hstack([X, W]), inference=&#39;bootstrap&#39;)&#xA;treatment_effects = est.effect(np.hstack([X_test, W_test]))&#xA;lb, ub = est.effect_interval(np.hstack([X_test, W_test]), alpha=0.05) # Bootstrap CIs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;SLearner&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.metalearners import SLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor&#xA;&#xA;est = SLearner(overall_model=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=np.hstack([X, W]))&#xA;treatment_effects = est.effect(np.hstack([X_test, W_test]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;TLearner&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.metalearners import TLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor&#xA;&#xA;est = TLearner(models=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=np.hstack([X, W]))&#xA;treatment_effects = est.effect(np.hstack([X_test, W_test]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Doubly Robust Learners (click to expand) &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dr import LinearDRLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier&#xA;&#xA;est = LinearDRLearner(model_propensity=GradientBoostingClassifier(),&#xA;                      model_regression=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Sparse linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dr import SparseLinearDRLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier&#xA;&#xA;est = SparseLinearDRLearner(model_propensity=GradientBoostingClassifier(),&#xA;                            model_regression=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Nonparametric final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dr import ForestDRLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier&#xA;&#xA;est = ForestDRLearner(model_propensity=GradientBoostingClassifier(),&#xA;                      model_regression=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=X, W=W) &#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Double Machine Learning with Instrumental Variables (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Orthogonal instrumental variable learner&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dml import OrthoIV&#xA;&#xA;est = OrthoIV(projection=False, &#xA;              discrete_treatment=True, &#xA;              discrete_instrument=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # OLS confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Nonparametric double machine learning with instrumental variable&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dml import NonParamDMLIV&#xA;&#xA;est = NonParamDMLIV(projection=False, &#xA;                    discrete_treatment=True, &#xA;                    discrete_instrument=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W) # no analytical confidence interval available&#xA;treatment_effects = est.effect(X_test)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Doubly Robust Machine Learning with Instrumental Variables (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dr import LinearDRIV&#xA;&#xA;est = LinearDRIV(discrete_instrument=True, discrete_treatment=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # OLS confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Sparse linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dr import SparseLinearDRIV&#xA;&#xA;est = SparseLinearDRIV(discrete_instrument=True, discrete_treatment=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # Debiased lasso confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Nonparametric final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dr import ForestDRIV&#xA;&#xA;est = ForestDRIV(discrete_instrument=True, discrete_treatment=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;# Confidence intervals via Bootstrap-of-Little-Bags for forests&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Linear intent-to-treat (discrete instrument, discrete treatment)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dr import LinearIntentToTreatDRIV&#xA;from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier&#xA;&#xA;est = LinearIntentToTreatDRIV(model_y_xw=GradientBoostingRegressor(),&#xA;                              model_t_xwz=GradientBoostingClassifier(),&#xA;                              flexible_model_effect=GradientBoostingRegressor())&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # OLS confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Deep Instrumental Variables (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import keras&#xA;from econml.iv.nnet import DeepIV&#xA;&#xA;treatment_model = keras.Sequential([keras.layers.Dense(128, activation=&#39;relu&#39;, input_shape=(2,)),&#xA;                                    keras.layers.Dropout(0.17),&#xA;                                    keras.layers.Dense(64, activation=&#39;relu&#39;),&#xA;                                    keras.layers.Dropout(0.17),&#xA;                                    keras.layers.Dense(32, activation=&#39;relu&#39;),&#xA;                                    keras.layers.Dropout(0.17)])&#xA;response_model = keras.Sequential([keras.layers.Dense(128, activation=&#39;relu&#39;, input_shape=(2,)),&#xA;                                  keras.layers.Dropout(0.17),&#xA;                                  keras.layers.Dense(64, activation=&#39;relu&#39;),&#xA;                                  keras.layers.Dropout(0.17),&#xA;                                  keras.layers.Dense(32, activation=&#39;relu&#39;),&#xA;                                  keras.layers.Dropout(0.17),&#xA;                                  keras.layers.Dense(1)])&#xA;est = DeepIV(n_components=10, # Number of gaussians in the mixture density networks)&#xA;             m=lambda z, x: treatment_model(keras.layers.concatenate([z, x])), # Treatment model&#xA;             h=lambda t, x: response_model(keras.layers.concatenate([t, x])), # Response model&#xA;             n_samples=1 # Number of samples used to estimate the response&#xA;             )&#xA;est.fit(Y, T, X=X, Z=Z) # Z -&amp;gt; instrumental variables&#xA;treatment_effects = est.effect(X_test)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/#references&#34;&gt;References&lt;/a&gt; section for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Interpretability&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tree Interpreter of the CATE model (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter&#xA;intrp = SingleTreeCateInterpreter(include_model_uncertainty=True, max_depth=2, min_samples_leaf=10)&#xA;# We interpret the CATE model&#39;s behavior based on the features used for heterogeneity&#xA;intrp.interpret(est, X)&#xA;# Plot the tree&#xA;plt.figure(figsize=(25, 5))&#xA;intrp.plot(feature_names=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;], fontsize=12)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/notebooks/images/dr_cate_tree.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Policy Interpreter of the CATE model (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.cate_interpreter import SingleTreePolicyInterpreter&#xA;# We find a tree-based treatment policy based on the CATE model&#xA;intrp = SingleTreePolicyInterpreter(risk_level=0.05, max_depth=2, min_samples_leaf=1,min_impurity_decrease=.001)&#xA;intrp.interpret(est, X, sample_treatment_costs=0.2)&#xA;# Plot the tree&#xA;plt.figure(figsize=(25, 5))&#xA;intrp.plot(feature_names=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;], fontsize=12)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/notebooks/images/dr_policy_tree.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;SHAP values for the CATE model (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import shap&#xA;from econml.dml import CausalForestDML&#xA;est = CausalForestDML()&#xA;est.fit(Y, T, X=X, W=W)&#xA;shap_values = est.shap_values(X)&#xA;shap.summary_plot(shap_values[&#39;Y0&#39;][&#39;T0&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Causal Model Selection and Cross-Validation&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Causal model selection with the `RScorer` (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.score import Rscorer&#xA;&#xA;# split data in train-validation&#xA;X_train, X_val, T_train, T_val, Y_train, Y_val = train_test_split(X, T, y, test_size=.4)&#xA;&#xA;# define list of CATE estimators to select among&#xA;reg = lambda: RandomForestRegressor(min_samples_leaf=20)&#xA;clf = lambda: RandomForestClassifier(min_samples_leaf=20)&#xA;models = [(&#39;ldml&#39;, LinearDML(model_y=reg(), model_t=clf(), discrete_treatment=True,&#xA;                             linear_first_stages=False, cv=3)),&#xA;          (&#39;xlearner&#39;, XLearner(models=reg(), cate_models=reg(), propensity_model=clf())),&#xA;          (&#39;dalearner&#39;, DomainAdaptationLearner(models=reg(), final_models=reg(), propensity_model=clf())),&#xA;          (&#39;slearner&#39;, SLearner(overall_model=reg())),&#xA;          (&#39;drlearner&#39;, DRLearner(model_propensity=clf(), model_regression=reg(),&#xA;                                  model_final=reg(), cv=3)),&#xA;          (&#39;rlearner&#39;, NonParamDML(model_y=reg(), model_t=clf(), model_final=reg(),&#xA;                                   discrete_treatment=True, cv=3)),&#xA;          (&#39;dml3dlasso&#39;, DML(model_y=reg(), model_t=clf(),&#xA;                             model_final=LassoCV(cv=3, fit_intercept=False),&#xA;                             discrete_treatment=True,&#xA;                             featurizer=PolynomialFeatures(degree=3),&#xA;                             linear_first_stages=False, cv=3))&#xA;]&#xA;&#xA;# fit cate models on train data&#xA;models = [(name, mdl.fit(Y_train, T_train, X=X_train)) for name, mdl in models]&#xA;&#xA;# score cate models on validation data&#xA;scorer = RScorer(model_y=reg(), model_t=clf(),&#xA;                 discrete_treatment=True, cv=3, mc_iters=2, mc_agg=&#39;median&#39;)&#xA;scorer.fit(Y_val, T_val, X=X_val)&#xA;rscore = [scorer.score(mdl) for _, mdl in models]&#xA;# select the best model&#xA;mdl, _ = scorer.best_model([mdl for _, mdl in models])&#xA;# create weighted ensemble model based on score performance&#xA;mdl, _ = scorer.ensemble([mdl for _, mdl in models])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;First Stage Model Selection (click to expand)&lt;/summary&gt; &#xA; &lt;p&gt;First stage models can be selected either by passing in cross-validated models (e.g. &lt;code&gt;sklearn.linear_model.LassoCV&lt;/code&gt;) to EconML&#39;s estimators or perform the first stage model selection outside of EconML and pass in the selected model. Unless selecting among a large set of hyperparameters, choosing first stage models externally is the preferred method due to statistical and computational advantages.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import LinearDML&#xA;from sklearn import clone&#xA;from sklearn.ensemble import RandomForestRegressor&#xA;from sklearn.model_selection import GridSearchCV&#xA;&#xA;cv_model = GridSearchCV(&#xA;              estimator=RandomForestRegressor(),&#xA;              param_grid={&#xA;                  &#34;max_depth&#34;: [3, None],&#xA;                  &#34;n_estimators&#34;: (10, 30, 50, 100, 200),&#xA;                  &#34;max_features&#34;: (2, 4, 6),&#xA;              },&#xA;              cv=5,&#xA;           )&#xA;# First stage model selection within EconML&#xA;# This is more direct, but computationally and statistically less efficient&#xA;est = LinearDML(model_y=cv_model, model_t=cv_model)&#xA;# First stage model selection ouside of EconML&#xA;# This is the most efficient, but requires boilerplate code&#xA;model_t = clone(cv_model).fit(W, T).best_estimator_&#xA;model_y = clone(cv_model).fit(W, Y).best_estimator_&#xA;est = LinearDML(model_y=model_t, model_t=model_y)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;Whenever inference is enabled, then one can get a more structure &lt;code&gt;InferenceResults&lt;/code&gt; object with more elaborate inference information, such as p-values and z-statistics. When the CATE model is linear and parametric, then a &lt;code&gt;summary()&lt;/code&gt; method is also enabled. For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import LinearDML&#xA;# Use defaults&#xA;est = LinearDML()&#xA;est.fit(Y, T, X=X, W=W)&#xA;# Get the effect inference summary, which includes the standard error, z test score, p value, and confidence interval given each sample X[i]&#xA;est.effect_inference(X_test).summary_frame(alpha=0.05, value=0, decimals=3)&#xA;# Get the population summary for the entire sample X&#xA;est.effect_inference(X_test).population_summary(alpha=0.1, value=0, decimals=3, tol=0.001)&#xA;#  Get the parameter inference summary for the final model&#xA;est.summary()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Example Output (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;# Get the effect inference summary, which includes the standard error, z test score, p value, and confidence interval given each sample X[i]&#xA;est.effect_inference(X_test).summary_frame(alpha=0.05, value=0, decimals=3)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/notebooks/images/summary_frame.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;# Get the population summary for the entire sample X&#xA;est.effect_inference(X_test).population_summary(alpha=0.1, value=0, decimals=3, tol=0.001)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/notebooks/images/population_summary.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;#  Get the parameter inference summary for the final model&#xA;est.summary()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/notebooks/images/summary.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Policy Learning&lt;/h3&gt; &#xA;&lt;p&gt;You can also perform direct policy learning from observational data, using the doubly robust method for offline policy learning. These methods directly predict a recommended treatment, without internally fitting an explicit model of the conditional average treatment effect.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Doubly Robust Policy Learning (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.policy import DRPolicyTree, DRPolicyForest&#xA;from sklearn.ensemble import RandomForestRegressor&#xA;&#xA;# fit a single binary decision tree policy&#xA;policy = DRPolicyTree(max_depth=1, min_impurity_decrease=0.01, honest=True)&#xA;policy.fit(y, T, X=X, W=W)&#xA;# predict the recommended treatment&#xA;recommended_T = policy.predict(X)&#xA;# plot the binary decision tree&#xA;plt.figure(figsize=(10,5))&#xA;policy.plot()&#xA;# get feature importances&#xA;importances = policy.feature_importances_&#xA;&#xA;# fit a binary decision forest&#xA;policy = DRPolicyForest(max_depth=1, min_impurity_decrease=0.01, honest=True)&#xA;policy.fit(y, T, X=X, W=W)&#xA;# predict the recommended treatment&#xA;recommended_T = policy.predict(X)&#xA;# plot the first tree in the ensemble&#xA;plt.figure(figsize=(10,5))&#xA;policy.plot(0)&#xA;# get feature importances&#xA;importances = policy.feature_importances_&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/EconML/main/images/policy_tree.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;To see more complex examples, go to the &lt;a href=&#34;https://github.com/Microsoft/EconML/tree/main/notebooks&#34;&gt;notebooks&lt;/a&gt; section of the repository. For a more detailed description of the treatment effect estimation algorithms, see the EconML &lt;a href=&#34;https://econml.azurewebsites.net/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;For Developers&lt;/h1&gt; &#xA;&lt;p&gt;You can get started by cloning this repository. We use &lt;a href=&#34;https://setuptools.readthedocs.io/en/latest/index.html&#34;&gt;setuptools&lt;/a&gt; for building and distributing our package. We rely on some recent features of setuptools, so make sure to upgrade to a recent version with &lt;code&gt;pip install setuptools --upgrade&lt;/code&gt;. Then from your local copy of the repository you can run &lt;code&gt;pip install -e .&lt;/code&gt; to get started (but depending on what you&#39;re doing you might want to install with extras instead, like &lt;code&gt;pip install -e .[plt]&lt;/code&gt; if you want to use matplotlib integration, or you can use &lt;code&gt;pip install -e .[all]&lt;/code&gt; to include all extras).&lt;/p&gt; &#xA;&lt;h2&gt;Running the tests&lt;/h2&gt; &#xA;&lt;p&gt;This project uses &lt;a href=&#34;https://docs.pytest.org/&#34;&gt;pytest&lt;/a&gt; for testing. To run tests locally after installing the package, you can use &lt;code&gt;pip install pytest-runner&lt;/code&gt; followed by &lt;code&gt;python setup.py pytest&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We have added pytest marks to some tests to make it easier to run a subset, and you can set the PYTEST_ADDOPTS environment variable to take advantage of this. For instance, you can set it to &lt;code&gt;-m &#34;not (notebook or automl)&#34;&lt;/code&gt; to skip notebook and automl tests that have some additional dependencies.&lt;/p&gt; &#xA;&lt;h2&gt;Generating the documentation&lt;/h2&gt; &#xA;&lt;p&gt;This project&#39;s documentation is generated via &lt;a href=&#34;https://www.sphinx-doc.org/en/main/index.html&#34;&gt;Sphinx&lt;/a&gt;. Note that we use &lt;a href=&#34;https://graphviz.org/&#34;&gt;graphviz&lt;/a&gt;&#39;s &lt;code&gt;dot&lt;/code&gt; application to produce some of the images in our documentation, so you should make sure that &lt;code&gt;dot&lt;/code&gt; is installed and in your path.&lt;/p&gt; &#xA;&lt;p&gt;To generate a local copy of the documentation from a clone of this repository, just run &lt;code&gt;python setup.py build_sphinx -W -E -a&lt;/code&gt;, which will build the documentation and place it under the &lt;code&gt;build/sphinx/html&lt;/code&gt; path.&lt;/p&gt; &#xA;&lt;p&gt;The reStructuredText files that make up the documentation are stored in the &lt;a href=&#34;https://github.com/Microsoft/EconML/tree/main/doc&#34;&gt;docs directory&lt;/a&gt;; module documentation is automatically generated by the Sphinx build process.&lt;/p&gt; &#xA;&lt;h1&gt;Blogs and Publications&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;June 2019: &lt;a href=&#34;https://arxiv.org/pdf/1905.10176.pdf&#34;&gt;Treatment Effects with Instruments paper&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;May 2019: &lt;a href=&#34;https://odsc.com/speakers/machine-learning-estimation-of-heterogeneous-treatment-effect-the-microsoft-econml-library/&#34;&gt;Open Data Science Conference Workshop&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2018: &lt;a href=&#34;http://proceedings.mlr.press/v97/oprescu19a.html&#34;&gt;Orthogonal Random Forests paper&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2017: &lt;a href=&#34;http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf&#34;&gt;DeepIV paper&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use EconML in your research, please cite us as follows:&lt;/p&gt; &#xA;&lt;p&gt;Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Paul Oka, Miruna Oprescu, Vasilis Syrgkanis. &lt;strong&gt;EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation.&lt;/strong&gt; &lt;a href=&#34;https://github.com/microsoft/EconML&#34;&gt;https://github.com/microsoft/EconML&lt;/a&gt;, 2019. Version 0.x.&lt;/p&gt; &#xA;&lt;p&gt;BibTex:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{econml,&#xA;  author={Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Paul Oka, Miruna Oprescu, Vasilis Syrgkanis},&#xA;  title={{EconML}: {A Python Package for ML-Based Heterogeneous Treatment Effects Estimation}},&#xA;  howpublished={https://github.com/microsoft/EconML},&#xA;  note={Version 0.x},&#xA;  year={2019}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Contributing and Feedback&lt;/h1&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.microsoft.com&#34;&gt;https://cla.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;p&gt;Athey, Susan, and Stefan Wager. &lt;strong&gt;Policy learning with observational data.&lt;/strong&gt; Econometrica 89.1 (2021): 133-161.&lt;/p&gt; &#xA;&lt;p&gt;X Nie, S Wager. &lt;strong&gt;Quasi-Oracle Estimation of Heterogeneous Treatment Effects.&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1093/biomet/asaa076&#34;&gt;&lt;em&gt;Biometrika&lt;/em&gt;&lt;/a&gt;, 2020&lt;/p&gt; &#xA;&lt;p&gt;V. Syrgkanis, V. Lei, M. Oprescu, M. Hei, K. Battocchi, G. Lewis. &lt;strong&gt;Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments.&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/1905.10176&#34;&gt;&lt;em&gt;Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS)&lt;/em&gt;&lt;/a&gt;, 2019 &lt;strong&gt;(Spotlight Presentation)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;D. Foster, V. Syrgkanis. &lt;strong&gt;Orthogonal Statistical Learning.&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/1901.09036.pdf&#34;&gt;&lt;em&gt;Proceedings of the 32nd Annual Conference on Learning Theory (COLT)&lt;/em&gt;&lt;/a&gt;, 2019 &lt;strong&gt;(Best Paper Award)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;M. Oprescu, V. Syrgkanis and Z. S. Wu. &lt;strong&gt;Orthogonal Random Forest for Causal Inference.&lt;/strong&gt; &lt;a href=&#34;http://proceedings.mlr.press/v97/oprescu19a.html&#34;&gt;&lt;em&gt;Proceedings of the 36th International Conference on Machine Learning (ICML)&lt;/em&gt;&lt;/a&gt;, 2019.&lt;/p&gt; &#xA;&lt;p&gt;S. Künzel, J. Sekhon, J. Bickel and B. Yu. &lt;strong&gt;Metalearners for estimating heterogeneous treatment effects using machine learning.&lt;/strong&gt; &lt;a href=&#34;https://www.pnas.org/content/116/10/4156&#34;&gt;&lt;em&gt;Proceedings of the national academy of sciences, 116(10), 4156-4165&lt;/em&gt;&lt;/a&gt;, 2019.&lt;/p&gt; &#xA;&lt;p&gt;S. Athey, J. Tibshirani, S. Wager. &lt;strong&gt;Generalized random forests.&lt;/strong&gt; &lt;a href=&#34;https://projecteuclid.org/euclid.aos/1547197251&#34;&gt;&lt;em&gt;Annals of Statistics, 47, no. 2, 1148--1178&lt;/em&gt;&lt;/a&gt;, 2019.&lt;/p&gt; &#xA;&lt;p&gt;V. Chernozhukov, D. Nekipelov, V. Semenova, V. Syrgkanis. &lt;strong&gt;Plug-in Regularized Estimation of High-Dimensional Parameters in Nonlinear Semiparametric Models.&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/1806.04823&#34;&gt;&lt;em&gt;Arxiv preprint arxiv:1806.04823&lt;/em&gt;&lt;/a&gt;, 2018.&lt;/p&gt; &#xA;&lt;p&gt;S. Wager, S. Athey. &lt;strong&gt;Estimation and Inference of Heterogeneous Treatment Effects using Random Forests.&lt;/strong&gt; &lt;a href=&#34;https://www.tandfonline.com/doi/citedby/10.1080/01621459.2017.1319839&#34;&gt;&lt;em&gt;Journal of the American Statistical Association, 113:523, 1228-1242&lt;/em&gt;&lt;/a&gt;, 2018.&lt;/p&gt; &#xA;&lt;p&gt;Jason Hartford, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. &lt;strong&gt;Deep IV: A flexible approach for counterfactual prediction.&lt;/strong&gt; &lt;a href=&#34;http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf&#34;&gt;&lt;em&gt;Proceedings of the 34th International Conference on Machine Learning, ICML&#39;17&lt;/em&gt;&lt;/a&gt;, 2017.&lt;/p&gt; &#xA;&lt;p&gt;V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, and a. W. Newey. &lt;strong&gt;Double Machine Learning for Treatment and Causal Parameters.&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/1608.00060&#34;&gt;&lt;em&gt;ArXiv preprint arXiv:1608.00060&lt;/em&gt;&lt;/a&gt;, 2016.&lt;/p&gt; &#xA;&lt;p&gt;Dudik, M., Erhan, D., Langford, J., &amp;amp; Li, L. &lt;strong&gt;Doubly robust policy evaluation and optimization.&lt;/strong&gt; Statistical Science, 29(4), 485-511, 2014.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/AI-For-Beginners</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/microsoft/AI-For-Beginners</id>
    <link href="https://github.com/microsoft/AI-For-Beginners" rel="alternate"></link>
    <summary type="html">&lt;p&gt;12 Weeks, 24 Lessons, AI for All!&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/AI-For-Beginners/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/microsoft/AI-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/AI-For-Beginners/graphs/contributors/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/microsoft/AI-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/AI-For-Beginners/issues/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/microsoft/AI-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/AI-For-Beginners/pulls/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/microsoft/AI-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/microsoft/AI-For-Beginners/watchers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Watch&#34; alt=&#34;GitHub watchers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/AI-For-Beginners/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Fork&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/AI-For-Beginners/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Star&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mybinder.org/v2/gh/microsoft/ai-for-beginners/HEAD&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/Microsoft/ai-for-beginners?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Microsoft/ai-for-beginners.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Artificial Intelligence for Beginners - A Curriculum&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/sketchnotes/ai-overview.png&#34; alt=&#34; Sketchnote by (@girlie_mac) &#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AI For Beginners - &lt;em&gt;Sketchnote by &lt;a href=&#34;https://twitter.com/girlie_mac&#34;&gt;@girlie_mac&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 24-lesson curriculum all about &lt;strong&gt;Artificial Intelligence&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In this curriculum, you will learn:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Different approaches to Artificial Intelligence, including the &#34;good old&#34; symbolic approach with &lt;strong&gt;Knowledge Representation&lt;/strong&gt; and reasoning (&lt;a href=&#34;https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence&#34;&gt;GOFAI&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Networks&lt;/strong&gt; and &lt;strong&gt;Deep Learning&lt;/strong&gt;, which are at the core of modern AI. We will illustrate the concepts behind these important topics using code in two of the most popular frameworks - &lt;a href=&#34;http://Tensorflow.org&#34;&gt;TensorFlow&lt;/a&gt; and &lt;a href=&#34;http://pytorch.org&#34;&gt;PyTorch&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Architectures&lt;/strong&gt; for working with images and text. We will cover recent models but may lack a little bit on the state-of-the-art.&lt;/li&gt; &#xA; &lt;li&gt;Less popular AI approaches, such as &lt;strong&gt;Genetic Algorithms&lt;/strong&gt; and &lt;strong&gt;Multi-Agent Systems&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;What we will not cover in this curriculum:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Business cases of using &lt;strong&gt;AI in Business&lt;/strong&gt;. Consider taking &lt;a href=&#34;https://docs.microsoft.com/learn/paths/introduction-ai-for-business-users/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Introduction to AI for business users&lt;/a&gt; learning path on Microsoft Learn, or &lt;a href=&#34;https://www.microsoft.com/ai/ai-business-school/?WT.mc_id=academic-57639-dmitryso&#34;&gt;AI Business School&lt;/a&gt;, developed in cooperation with &lt;a href=&#34;https://www.insead.edu/&#34;&gt;INSEAD&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Classic Machine Learning&lt;/strong&gt;, which is well described in our &lt;a href=&#34;http://github.com/Microsoft/ML-for-Beginners&#34;&gt;Machine Learning for Beginners Curriculum&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Practical AI applications built using &lt;strong&gt;&lt;a href=&#34;https://azure.microsoft.com/services/cognitive-services/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Cognitive Services&lt;/a&gt;&lt;/strong&gt;. For this, we recommend that you start with modules Microsoft Learn for &lt;a href=&#34;https://docs.microsoft.com/learn/paths/create-computer-vision-solutions-azure-cognitive-services/?WT.mc_id=academic-57639-dmitryso&#34;&gt;vision&lt;/a&gt;, &lt;a href=&#34;https://docs.microsoft.com/learn/paths/explore-natural-language-processing/?WT.mc_id=academic-57639-dmitryso&#34;&gt;natural language processing&lt;/a&gt; and others.&lt;/li&gt; &#xA; &lt;li&gt;Specific ML &lt;strong&gt;Cloud Frameworks&lt;/strong&gt;, such as &lt;a href=&#34;https://azure.microsoft.com/services/machine-learning/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Azure Machine Learning&lt;/a&gt; or &lt;a href=&#34;https://docs.microsoft.com/learn/paths/data-engineer-azure-databricks?WT.mc_id=academic-57639-dmitryso&#34;&gt;Azure Databricks&lt;/a&gt;. Consider using &lt;a href=&#34;https://docs.microsoft.com/learn/paths/build-ai-solutions-with-azure-ml-service/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Build and operate machine learning solutions with Azure Machine Learning&lt;/a&gt; and &lt;a href=&#34;https://docs.microsoft.com/learn/paths/build-operate-machine-learning-solutions-azure-databricks/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Build and Operate Machine Learning Solutions with Azure Databricks&lt;/a&gt; learning paths.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conversational AI&lt;/strong&gt; and &lt;strong&gt;Chat Bots&lt;/strong&gt;. There is a separate &lt;a href=&#34;https://docs.microsoft.com/learn/paths/create-conversational-ai-solutions/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Create conversational AI solutions&lt;/a&gt; learning path, and you can also refer to &lt;a href=&#34;https://soshnikov.com/azure/hello-bot-conversational-ai-on-microsoft-platform/&#34;&gt;this blog post&lt;/a&gt; for more detail.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep Mathematics&lt;/strong&gt; behind deep learning. For this, we would recommend &lt;a href=&#34;https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618&#34;&gt;Deep Learning&lt;/a&gt; by Ian Goodfellow, Yoshua Bengio and Aaron Courville, which is also available online at &lt;a href=&#34;https://www.deeplearningbook.org/&#34;&gt;https://www.deeplearningbook.org/&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a gentle introduction to &lt;em&gt;AI in the Cloud&lt;/em&gt; topics you may consider taking the &lt;a href=&#34;https://docs.microsoft.com/learn/paths/get-started-with-artificial-intelligence-on-azure/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Get started with artificial intelligence on Azure&lt;/a&gt; Learning Path.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Content&lt;/h1&gt; &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;th&gt;No&lt;/th&gt;&#xA;   &lt;th&gt;Lesson&lt;/th&gt;&#xA;   &lt;th&gt;Intro&lt;/th&gt;&#xA;   &lt;th&gt;PyTorch&lt;/th&gt;&#xA;   &lt;th&gt;Keras/TensorFlow&lt;/th&gt;&#xA;   &lt;th&gt;Lab&lt;/th&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;I&lt;/td&gt;&#xA;   &lt;td colspan=&#34;4&#34;&gt;&lt;b&gt;Introduction to AI&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;1&lt;/td&gt;&#xA;   &lt;td&gt;Introduction and History of AI&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/1-Intro/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;II&lt;/td&gt;&#xA;   &lt;td colspan=&#34;4&#34;&gt;&lt;b&gt;Symbolic AI&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;2 &lt;/td&gt;&#xA;   &lt;td&gt;Knowledge Representation and Expert Systems&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/2-Symbolic/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/2-Symbolic/Animals.ipynb&#34;&gt;Expert System&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/2-Symbolic/FamilyOntology.ipynb&#34;&gt;Ontology&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/2-Symbolic/MSConceptGraph.ipynb&#34;&gt;Concept Graph&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;III&lt;/td&gt;&#xA;   &lt;td colspan=&#34;4&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/README.md&#34;&gt;Introduction to Neural Networks&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;3&lt;/td&gt;&#xA;   &lt;td&gt;Perceptron&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/03-Perceptron/README.md&#34;&gt;Text&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/03-Perceptron/Perceptron.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/03-Perceptron/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;4 &lt;/td&gt;&#xA;   &lt;td&gt;Multi-Layered Perceptron and Creating our own Framework&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/04-OwnFramework/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/04-OwnFramework/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;Intro to Frameworks (PyTorch/TensorFlow)&lt;br&gt;Overfitting&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/README.md&#34;&gt;Text&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/Overfitting.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb&#34;&gt;Keras&lt;/a&gt;&lt;/td&gt;/ &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;IV&lt;/td&gt;&#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/README.md&#34;&gt;Computer Vision&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;3&#34;&gt;&lt;a href=&#34;https://docs.microsoft.com/learn/paths/explore-computer-vision-microsoft-azure/?WT.mc_id=academic-57639-dmitryso&#34;&gt;&lt;i&gt;AI Fundamentals: Explore Computer Vision&lt;/i&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;i&gt;Microsoft Learn Module on Computer Vision&lt;/i&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/learn/modules/intro-computer-vision-pytorch/?WT.mc_id=academic-57639-dmitryso&#34;&gt;&lt;i&gt;PyTorch&lt;/i&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/learn/modules/intro-computer-vision-TensorFlow/?WT.mc_id=academic-57639-dmitryso&#34;&gt;&lt;i&gt;TensorFlow&lt;/i&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;6&lt;/td&gt;&#xA;   &lt;td&gt;Intro to Computer Vision. OpenCV&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/06-IntroCV/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/06-IntroCV/OpenCV.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/06-IntroCV/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;7&lt;/td&gt;&#xA;   &lt;td&gt;Convolutional Neural Networks&lt;br&gt;CNN Architectures&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/README.md&#34;&gt;Text&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/CNN_Architectures.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/ConvNetsPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/ConvNetsTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;8&lt;/td&gt;&#xA;   &lt;td&gt;Pre-trained Networks and Transfer Learning&lt;br&gt;Training Tricks&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/README.md&#34;&gt;Text&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/TrainingTricks.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/TransferLearningPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/TransferLearningTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/Dropout.ipynb&#34;&gt;Dropout sample&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;9&lt;/td&gt;&#xA;   &lt;td&gt;Autoencoders and VAEs&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/09-Autoencoders/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;10&lt;/td&gt;&#xA;   &lt;td&gt;Generative Adversarial Networks&lt;br&gt;Artistic Style Transfer&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/10-GANs/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/10-GANs/GANTF.ipynb&#34;&gt;TensorFlow GAN&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/10-GANs/StyleTransfer.ipynb&#34;&gt;Style Transfer&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;11&lt;/td&gt;&#xA;   &lt;td&gt;Object Detection&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/11-ObjectDetection/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;PyTorch&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/11-ObjectDetection/ObjectDetection.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/11-ObjectDetection/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;12&lt;/td&gt;&#xA;   &lt;td&gt;Semantic Segmentation. U-Net&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/12-Segmentation/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationPytorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;V&lt;/td&gt;&#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/README.md&#34;&gt;Natural Language Processing&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;3&#34;&gt;&lt;a href=&#34;https://docs.microsoft.com/learn/paths/explore-natural-language-processing/?WT.mc_id=academic-57639-dmitryso&#34;&gt;&lt;i&gt;AI Fundamentals: Explore Natural Language Processing&lt;/i&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;i&gt;Microsoft Learn Module on Natural Language&lt;/i&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-57639-dmitryso&#34;&gt;&lt;i&gt;PyTorch&lt;/i&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/learn/modules/intro-natural-language-processing-TensorFlow/?WT.mc_id=academic-57639-dmitryso&#34;&gt;&lt;i&gt;TensorFlow&lt;/i&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;13&lt;/td&gt;&#xA;   &lt;td&gt;Text Representation. Bow/TF-IDF&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/13-TextRep/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;14&lt;/td&gt;&#xA;   &lt;td&gt;Semantic word embeddings. Word2Vec and GloVe&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/14-Embeddings/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;15&lt;/td&gt;&#xA;   &lt;td&gt;Language Modeling. Training your own embeddings&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/15-LanguageModeling/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/15-LanguageModeling/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;16&lt;/td&gt;&#xA;   &lt;td&gt;Recurrent Neural Networks&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/16-RNN/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/16-RNN/RNNPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/16-RNN/RNNTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;17&lt;/td&gt;&#xA;   &lt;td&gt;Generative Recurrent Networks&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/17-GenerativeNetworks/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.md&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.md&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/17-GenerativeNetworks/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;18&lt;/td&gt;&#xA;   &lt;td&gt;Transformers. BERT.&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/18-Transformers/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/18-Transformers/TransformersTF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;19&lt;/td&gt;&#xA;   &lt;td&gt;Named Entity Recognition&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/19-NER/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/19-NER/NER-TF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/19-NER/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;20&lt;/td&gt;&#xA;   &lt;td&gt;Large Language Models, Prompt Programming and Few-Shot Tasks&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/20-LangModels/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;VI&lt;/td&gt;&#xA;   &lt;td colspan=&#34;4&#34;&gt;&lt;b&gt;Other AI Techniques&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;21&lt;/td&gt;&#xA;   &lt;td&gt;Genetic Algorithms&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/21-GeneticAlgorithms/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/21-GeneticAlgorithms/Genetic.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;22&lt;/td&gt;&#xA;   &lt;td&gt;Deep Reinforcement Learning&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/22-DeepRL/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb&#34;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/22-DeepRL/lab/README.md&#34;&gt;Lab&lt;/a&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;23&lt;/td&gt;&#xA;   &lt;td&gt;Multi-Agent Systems&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/23-MultiagentSystems/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;VII&lt;/td&gt;&#xA;   &lt;td colspan=&#34;4&#34;&gt;&lt;b&gt;AI Ethics&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;24&lt;/td&gt;&#xA;   &lt;td&gt;AI Ethics and Responsible AI&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/7-Ethics/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://docs.microsoft.com/learn/paths/responsible-ai-business-principles/?WT.mc_id=academic-57639-dmitryso&#34;&gt;&lt;i&gt;MS Learn: Responsible AI Principles&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;4&#34;&gt;&lt;b&gt;Extras&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;X1&lt;/td&gt;&#xA;   &lt;td&gt;Multi-Modal Networks, CLIP and VQGAN&lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/X-Extras/X1-MultiModal/README.md&#34;&gt;Text&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/X-Extras/X1-MultiModal/Clip.ipynb&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt;&#xA;   &lt;td&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://soshnikov.com/courses/ai-for-beginners/mindmap.html&#34;&gt;Mindmap of the Course&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Each lesson contains some pre-reading material (linked as &lt;strong&gt;Text&lt;/strong&gt; above), and some executable Jupyter Notebooks, which are often specific to the framework (&lt;strong&gt;PyTorch&lt;/strong&gt; or &lt;strong&gt;TensorFlow&lt;/strong&gt;). The executable notebook also contains a lot of theoretical material, so to understand the topic you need to go through at least one version of the notebooks (either PyTorch or TensorFlow). There are also &lt;strong&gt;Labs&lt;/strong&gt; available for some topics, which give you an opportunity to try applying the material you have learned to a specific problem.&lt;/p&gt; &#xA;&lt;p&gt;Some sections also contain links to &lt;strong&gt;MS Learn&lt;/strong&gt; modules that cover related topics. Microsoft Learn provides a convenient GPU-enabled learning environment, although in terms of content you can expect this curriculum to go a bit deeper.&lt;/p&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Students&lt;/strong&gt;, there are a couple of ways to use the curriculum. First of all, you can just read the text and look through the code directly on GitHub. If you want to run the code in any of the notebooks - &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/how-to-run.md&#34;&gt;read our instructions&lt;/a&gt;, and find more advice on how to do it &lt;a href=&#34;https://soshnikov.com/education/how-to-execute-notebooks-from-github/&#34;&gt;in this blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/how-to-run.md&#34;&gt;Instructions on how to run the code in this curriculum&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;However, if you would like to take the course as a self-study project, we suggest that you fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start with a pre-lecture quiz&lt;/li&gt; &#xA; &lt;li&gt;Read the intro text for the lecture&lt;/li&gt; &#xA; &lt;li&gt;If the lecture has additional notebooks, go through them, reading and executing the code. If both TensorFlow and PyTorch notebooks are provided, you can focus on one of them - chose your favorite framework&lt;/li&gt; &#xA; &lt;li&gt;Notebooks often contain some of the challenges that require you to tweak the code a little bit to experiment&lt;/li&gt; &#xA; &lt;li&gt;Take the post-lecture quiz&lt;/li&gt; &#xA; &lt;li&gt;If there is a lab attached to the module - complete the assignment&lt;/li&gt; &#xA; &lt;li&gt;Visit the &lt;a href=&#34;https://github.com/microsoft/AI-For-Beginners/discussions&#34;&gt;Discussion board&lt;/a&gt; to &#34;learn out loud&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Chat with other learners &lt;a href=&#34;https://gitter.im/Microsoft/ai-for-beginners&#34;&gt;on Gitter&lt;/a&gt; or &lt;a href=&#34;http://t.me/ai_for_beginners&#34;&gt;in Telegram channel&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For further study, we recommend following these &lt;a href=&#34;https://docs.microsoft.com/en-us/users/dmitrysoshnikov-9132/collections/31zgizg2p418yo/?WT.mc_id=academic-57639-dmitryso&#34;&gt;Microsoft Learn&lt;/a&gt; modules and learning paths.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;, we have &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/for-teachers.md&#34;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;✍️ Primary Author:&lt;/strong&gt; &lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry Soshnikov&lt;/a&gt;, PhD &lt;br&gt; &lt;strong&gt;🔥 Editor:&lt;/strong&gt; &lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen Looper&lt;/a&gt;, PhD &lt;br&gt; &lt;strong&gt;🎨 Sketchnote illustrator:&lt;/strong&gt; &lt;a href=&#34;https://twitter.com/girlie_mac&#34;&gt;Tomomi Imura&lt;/a&gt; &lt;br&gt; &lt;strong&gt;✅ Quiz Creator:&lt;/strong&gt; &lt;a href=&#34;https://github.com/CinnamonXI&#34;&gt;Lateefah Bello&lt;/a&gt;, &lt;a href=&#34;https://studentambassadors.microsoft.com/&#34;&gt;MLSA&lt;/a&gt; &lt;br&gt; &lt;strong&gt;🙏 Core Contributors:&lt;/strong&gt; &lt;a href=&#34;https://github.com/Pe4enIks&#34;&gt;Evgenii Pishchik&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Meet the Team&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/m2KrAk0cC1c&#34; title=&#34;Promo video&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/sketchnotes/ai-for-beginners.png&#34; alt=&#34;Promo video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🎥 Click the image above for a video about the project and the folks who created it!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Pedagogy&lt;/h2&gt; &#xA;&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is hands-on &lt;strong&gt;project-based&lt;/strong&gt; and that it includes &lt;strong&gt;frequent quizzes&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12 week cycle.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Find our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/TRANSLATIONS.md&#34;&gt;Translation&lt;/a&gt; guidelines. Find our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/SUPPORT.md&#34;&gt;Support Documentation here&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/SECURITY.md&#34;&gt;security information here&lt;/a&gt;. We welcome your constructive feedback!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained &lt;a href=&#34;https://black-ground-0cc93280f.1.azurestaticapps.net/&#34;&gt;in this app&lt;/a&gt;, for 50 total quizzes of three questions each. They are linked from within the lessons but the quiz app can be run locally; follow the instruction in the &lt;code&gt;etc/quiz-app&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Offline access&lt;/h2&gt; &#xA;&lt;p&gt;You can run this documentation offline by using &lt;a href=&#34;https://docsify.js.org/#/&#34;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&#34;https://docsify.js.org/#/quickstart&#34;&gt;install Docsify&lt;/a&gt; on your local machine, and then in the &lt;code&gt;etc/docsify&lt;/code&gt; folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;. A pdf of the curriculum is available &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/pdf/readme.pdf&#34;&gt;at this link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Help Wanted!&lt;/h2&gt; &#xA;&lt;p&gt;Would you like to contribute a translation? Please read our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/etc/TRANSLATIONS.md&#34;&gt;translation guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Other Curricula&lt;/h2&gt; &#xA;&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/webdev-beginners&#34;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/iot-beginners&#34;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://aka.ms/ML-for-Beginners&#34;&gt;Machine Learning for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://aka.ms/Data-Science-for-Beginners&#34;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/deep-rl-class</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/huggingface/deep-rl-class</id>
    <link href="https://github.com/huggingface/deep-rl-class" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repo contain the syllabus of the Hugging Face Deep Reinforcement Learning Class.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Hugging Face Deep Reinforcement Learning Class 🤗&lt;/h1&gt; &#xA;&lt;p&gt;In this free course, you will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📖 Study Deep Reinforcement Learning in &lt;strong&gt;theory and practice&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🧑‍💻 Learn to &lt;strong&gt;use famous Deep RL libraries&lt;/strong&gt; such as Stable Baselines3, RL Baselines3 Zoo, and RLlib.&lt;/li&gt; &#xA; &lt;li&gt;🤖 Train agents in &lt;strong&gt;unique environments&lt;/strong&gt; such as SnowballFight, Huggy the Doggo 🐶, and classical ones such as Space Invaders and PyBullet.&lt;/li&gt; &#xA; &lt;li&gt;💾 &lt;strong&gt;Publish your trained agents in one line of code to the Hugging Face Hub&lt;/strong&gt;. But also &lt;strong&gt;download powerful agents from the community&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🏆 &lt;strong&gt;Participate in challenges&lt;/strong&gt; where you will evaluate your agents against other teams.&lt;/li&gt; &#xA; &lt;li&gt;🖌️🎨 &lt;strong&gt;Learn to share your own environments made with Unity and Godot&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;➡️➡️➡️ Don&#39;t forget to sign up here: &lt;a href=&#34;http://eepurl.com/h1pElX&#34;&gt;http://eepurl.com/h1pElX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The best way to keep in touch is to &lt;strong&gt;join our discord server to exchange with the community and with us&lt;/strong&gt; 👉🏻 &lt;a href=&#34;https://discord.gg/aYka4Yhff9&#34;&gt;https://discord.gg/aYka4Yhff9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Are you new to Discord? Check our &lt;strong&gt;discord 101 to get the best practices&lt;/strong&gt; 👉 &lt;a href=&#34;https://github.com/huggingface/deep-rl-class/raw/main/DISCORD.Md&#34;&gt;https://github.com/huggingface/deep-rl-class/blob/main/DISCORD.Md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And don&#39;t forget to share with your friends who want to learn 🤗!&lt;/p&gt; &#xA;&lt;h2&gt;The Syllabus 🏗️&lt;/h2&gt; &#xA;&lt;p&gt;This course is &lt;strong&gt;self-paced&lt;/strong&gt; you can start when you want 🥳.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;📆 Publishing date&lt;/th&gt; &#xA;   &lt;th&gt;📘 Unit&lt;/th&gt; &#xA;   &lt;th&gt;👩‍💻 Hands-on&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/tree/main/unit1#unit-1-introduction-to-deep-reinforcement-learning&#34;&gt;Published 🥳&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/tree/main/unit1&#34;&gt;An Introduction to Deep Reinforcement Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/raw/main/unit1/unit1.ipynb&#34;&gt;Train a Deep Reinforcement Learning lander agent to land correctly on the Moon 🌕 using Stable-Baselines3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/tree/main/unit1/unit1-bonus&#34;&gt;Published 🥳&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/tree/main/unit1/unit1-bonus&#34;&gt;Bonus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/raw/main/unit2/README.md&#34;&gt;Published 🥳&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/raw/main/unit2/README.md&#34;&gt;Q-Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/deep-rl-class/raw/main/unit2/unit2.ipynb&#34;&gt;Train an agent to cross a Frozen lake ⛄ and train an autonomous taxi 🚖&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;June, the 1st&lt;/td&gt; &#xA;   &lt;td&gt;Deep Q-Learning and improvements&lt;/td&gt; &#xA;   &lt;td&gt;Train a Deep Q-Learning agent to play Space Invaders&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Policy-based methods&lt;/td&gt; &#xA;   &lt;td&gt;🏗️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Actor-Critic Methods&lt;/td&gt; &#xA;   &lt;td&gt;🏗️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Proximal Policy Optimization (PPO)&lt;/td&gt; &#xA;   &lt;td&gt;🏗️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Decision Transformers and offline Reinforcement Learning&lt;/td&gt; &#xA;   &lt;td&gt;🏗️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Towards better explorations methods&lt;/td&gt; &#xA;   &lt;td&gt;🏗️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;The library you&#39;ll learn during this course&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34;&gt;Stable-Baselines3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34;&gt;RL Baselines3 Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.ray.io/en/latest/rllib/index.html&#34;&gt;RLlib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vwxyzjn/cleanrl&#34;&gt;CleanRL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;More to come 🏗️&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;The Environments you&#39;ll use&lt;/h2&gt; &#xA;&lt;h3&gt;Custom environments made by the Hugging Face Team using Unity and Godot&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Huggy the Doggo 🐶 (Based on &lt;a href=&#34;https://blog.unity.com/technology/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit&#34;&gt;Unity&#39;s Puppo the Corgi work&lt;/a&gt;) &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/deep-rl-class/main/assets/img/huggy.jpg&#34; alt=&#34;huggy.jpg&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;SnowballFight ☃️ &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/deep-rl-class/main/assets/img/snowballfight.gif&#34; alt=&#34;snowballfight.gif&#34;&gt; 👉 Play it here: &lt;a href=&#34;https://huggingface.co/spaces/ThomasSimonini/SnowballFight&#34;&gt;https://huggingface.co/spaces/ThomasSimonini/SnowballFight&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;More to come 🚧&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Gym classic controls environments 🕹️&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lunar-Lander v2 🚀🌙&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/deep-rl-class/main/assets/img/lunarlander.gif&#34; alt=&#34;lunarlander.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;PyBullet 🤖&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More to come 🚧&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Gym Atari environments 👾&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Space Invaders 👾&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/deep-rl-class/main/assets/img/spaceinvaders.gif&#34; alt=&#34;spaceinvaders.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;MLAgents environments 🖌️&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More to come 🚧&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Good skills in Python 🐍&lt;/li&gt; &#xA; &lt;li&gt;Basics in Deep Learning and Pytorch&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If it&#39;s not the case yet, you can check these free resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python: &lt;a href=&#34;https://www.udacity.com/course/introduction-to-python--ud1110&#34;&gt;https://www.udacity.com/course/introduction-to-python--ud1110&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Intro to Deep Learning with PyTorch: &lt;a href=&#34;https://www.udacity.com/course/deep-learning-pytorch--ud188&#34;&gt;https://www.udacity.com/course/deep-learning-pytorch--ud188&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PyTorch in 60min: &lt;a href=&#34;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&#34;&gt;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is this class free?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes, totally free 🥳.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Do I need to have a Hugging Face account to follow the course?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes, to push your trained agents during the hands-on, you need an account (it&#39;s free) 🤗.&lt;/p&gt; &#xA;&lt;p&gt;You can create one here 👉 &lt;a href=&#34;https://huggingface.co/join&#34;&gt;https://huggingface.co/join&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What’s the format of the class?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The course consists of&amp;nbsp;&lt;strong&gt;8 Units.&lt;/strong&gt;&amp;nbsp;In each of the Units, we&#39;ll have:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;A theory explained part&lt;/strong&gt;: an article and a video (based on Deep Reinforcement Learning Course)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A hands-on Google Colab&lt;/strong&gt; where you&#39;ll learn to use famous Deep RL libraries such as Stable Baselines3, RL Baselines3 Zoo, and RLlib to train your agents in unique environments such as SnowballFight, Huggy the Doggo 🐶, and classical ones such as Space Invaders and PyBullet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Some optional challenges&lt;/strong&gt;: train an agent in another environment, and try to beat the results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It&#39;s not a live course video, so you can watch and read each unit when you want 🤗 You can check the syllabus here 👉 &lt;a href=&#34;https://github.com/huggingface/deep-rl-class&#34;&gt;https://github.com/huggingface/deep-rl-class&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What I will do during this course?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In this free course, you will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📖 Study Deep Reinforcement Learning in theory and practice.&lt;/li&gt; &#xA; &lt;li&gt;🧑‍💻 Learn to use famous Deep RL libraries such as Stable Baselines3, RL Baselines3 Zoo, and RLlib.&lt;/li&gt; &#xA; &lt;li&gt;🤖 Train agents in unique environments such as SnowballFight, Huggy the Doggo 🐶, and classical ones such as Space Invaders and PyBullet.&lt;/li&gt; &#xA; &lt;li&gt;💾 Publish your trained agents in one line of code to the Hub. But also download powerful agents from the community.&lt;/li&gt; &#xA; &lt;li&gt;🏆 Participate in challenges where you will evaluate your agents against other teams.&lt;/li&gt; &#xA; &lt;li&gt;🖌️🎨 Learn to share your own environments made with Unity and Godot.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Where do I sign up?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here 👉 &lt;a href=&#34;http://eepurl.com/h1pElX&#34;&gt;http://eepurl.com/h1pElX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Where can I find the course?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On this repository&lt;/strong&gt;, we&#39;ll publish every week the links (chapters, hands-ons, videos).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Where can I exchange with my classmates and with you?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We have a discord server where you &lt;strong&gt;can exchange with the community and with us&lt;/strong&gt; 👉🏻 &lt;a href=&#34;https://discord.gg/aYka4Yhff9&#34;&gt;https://discord.gg/aYka4Yhff9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Don’t forget to &lt;strong&gt;introduce yourself when you sign up 🤗&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I have some feedback&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We want to improve and update the course iteratively with your feedback. If you have some, please send a mail to &lt;a href=&#34;mailto:thomas.simonini@huggingface.co&#34;&gt;thomas.simonini@huggingface.co&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How much background knowledge is needed?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Some prerequisites:&lt;/p&gt; &#xA;&lt;p&gt;Good skills in &lt;strong&gt;Python&lt;/strong&gt; 🐍 Basics in &lt;strong&gt;Deep Learning and Pytorch&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If it&#39;s not the case yet, you can check these free resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python: &lt;a href=&#34;https://www.udacity.com/course/introduction-to-python--ud1110&#34;&gt;https://www.udacity.com/course/introduction-to-python--ud1110&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Intro to Deep Learning with PyTorch: &lt;a href=&#34;https://www.udacity.com/course/deep-learning-pytorch--ud188&#34;&gt;https://www.udacity.com/course/deep-learning-pytorch--ud188&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PyTorch in 60min: &lt;a href=&#34;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&#34;&gt;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is there a certificate?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes 🎉. You&#39;ll &lt;strong&gt;need to upload the eight models with the eight hands-on.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citing the project&lt;/h2&gt; &#xA;&lt;p&gt;To cite this repository in publications:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{deep-rl-class,&#xA;  author = {Simonini, Thomas and Sanseviero, Omar},&#xA;  title = {The Hugging Face Deep Reinforcement Learning Class},&#xA;  year = {2022},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/huggingface/deep-rl-class}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>AI4Finance-Foundation/FinRL</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/AI4Finance-Foundation/FinRL</id>
    <link href="https://github.com/AI4Finance-Foundation/FinRL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FinRL: The first open-source project for financial reinforcement learning. Please star. 🔥&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FinRL: Deep Reinforcement Learning for Quantitative Finance &lt;a href=&#34;https://twitter.com/intent/tweet?text=FinRL-Financial-Deep-Reinforcement-Learning%20&amp;amp;url=https://github.com/AI4Finance-Foundation/FinRL&amp;amp;hashtags=DRL&amp;amp;hashtags=AI&#34;&gt;&lt;img src=&#34;http://www.tensorlet.org/wp-content/uploads/2021/01/button_twitter_22x22.png&#34; alt=&#34;twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.facebook.com/sharer.php?u=http%3A%2F%2Fgithub.com%2FAI4Finance-Foundation%2FFinRL&#34;&gt;&lt;img src=&#34;http://www.tensorlet.org/wp-content/uploads/2021/01/facebook-button_22x22.png&#34; alt=&#34;facebook&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://plus.google.com/share?url=https://github.com/AI4Finance-Foundation/FinRL&#34;&gt;&lt;img src=&#34;http://www.tensorlet.org/wp-content/uploads/2021/01/button_google_22.xx_.png&#34; alt=&#34;google+&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2Fgithub.com%2FAI4Finance-Foundation%2FFinRL&#34;&gt;&lt;img src=&#34;http://www.tensorlet.org/wp-content/uploads/2021/01/button_linkedin_22x22.png&#34; alt=&#34;linkedin&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pepy.tech/project/finrl&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/finrl&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/finrl&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/finrl/week&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-360/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.6-blue.svg?sanitize=true&#34; alt=&#34;Python 3.6&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/finrl/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/finrl.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://finrl.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/finrl/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/license/AI4Finance-Foundation/finrl.svg?color=brightgreen&#34; alt=&#34;License&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/figs/logo_transparent_background.png&#34; width=&#34;45%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&#34;&lt;em&gt;I visualise a time when we will be to robots what dogs are to humans, and I’m rooting for the machines&lt;/em&gt;.&#34; — Claude Shannon&lt;/p&gt; &#xA;&lt;p&gt;&#34;&lt;em&gt;We can only see a short distance ahead, but we can see plenty there that needs to be done.&#34;&lt;/em&gt; — Alan Turing&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Our Mission&lt;/strong&gt;: to efficiently automate trading. We continuously develop and share codes for finance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Our Vision&lt;/strong&gt;: AI community has accumulated an open-source code ocean over the past decade. Applying these intellectual and engineering properties to finance will initiate a paradigm shift from the conventional trading routine to an automated machine learning approach, even &lt;strong&gt;RLOps in finance&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FinRL&lt;/strong&gt; (&lt;a href=&#34;https://finrl.readthedocs.io/en/latest/index.html&#34;&gt;website&lt;/a&gt;) is &lt;strong&gt;the first open-source project&lt;/strong&gt; to explore the great potential of deep reinforcement learning in finance. We help practitioners pipeline a trading strategy using &lt;strong&gt;deep reinforcement learning (DRL)&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The FinRL ecosystem is a unified framework, including various markets, state-of-the-art algorithms, financial tasks (portfolio allocation, cryptocurrency trading, high-frequency trading), live trading, etc.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Roadmap&lt;/th&gt; &#xA;   &lt;th&gt;Level&lt;/th&gt; &#xA;   &lt;th&gt;Users&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;   &lt;th&gt;Desription&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.0 (Preparation)&lt;/td&gt; &#xA;   &lt;td&gt;preparation&lt;/td&gt; &#xA;   &lt;td&gt;practitioners of financial big data&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinRL-Metaverse&#34;&gt;FinRL-Meta&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a universe of market environments&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.0 (Proof-of-Concept)&lt;/td&gt; &#xA;   &lt;td&gt;entry-level&lt;/td&gt; &#xA;   &lt;td&gt;beginners&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinRL&#34;&gt;this repo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;demonstration, education&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.0 (Professional)&lt;/td&gt; &#xA;   &lt;td&gt;intermediate-level&lt;/td&gt; &#xA;   &lt;td&gt;full-stack developers, professionals&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AI4Finance-Foundation/ElegantRL&#34;&gt;ElegantRL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;financially optimized DRL algorithms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0 (Production)&lt;/td&gt; &#xA;   &lt;td&gt;advance-level&lt;/td&gt; &#xA;   &lt;td&gt;investment banks, hedge funds&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinRL_Podracer&#34;&gt;Podracer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;cloud-native solution&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Outline&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/#Overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/#File-Structure&#34;&gt;File Structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/#Supported-Data-Sources&#34;&gt;Supported Data Sources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/#Installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/#Status-Update&#34;&gt;Status Update&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/#Contributions&#34;&gt;Contributions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/#Tutorials&#34;&gt;Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/#Publications&#34;&gt;Publications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/#News&#34;&gt;News&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/#Citing-FinRL&#34;&gt;Citing FinRL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/#To-Contribute&#34;&gt;Welcome Contributions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/#Sponsorship&#34;&gt;Sponsorship&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/#LICENSE&#34;&gt;LICENSE&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;FinRL has three layers: applications, agents, and market environments.&lt;/p&gt; &#xA;&lt;p&gt;For a trading task (on the top), an agent (in the middle) interacts with an environment (at the bottom), making sequential decisions.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/figs/finrl_framework.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Run &lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinRL/raw/master/FinRL_StockTrading_NeurIPS_2018.ipynb&#34;&gt;FinRL_StockTrading_NeurIPS_2018.ipynb&lt;/a&gt; step by step for a quick start.&lt;/p&gt; &#xA;&lt;p&gt;A video about &lt;a href=&#34;http://www.youtube.com/watch?v=ZSGJjtM-5jA&#34;&gt;FinRL library&lt;/a&gt; at the &lt;a href=&#34;https://www.youtube.com/channel/UCrVri6k3KPBa3NhapVV4K5g&#34;&gt;AI4Finance Youtube Channel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#AI4Finance-Foundation/FinRL&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=AI4Finance-Foundation/FinRL&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;File Structure&lt;/h2&gt; &#xA;&lt;p&gt;Correspondingly, the main folder &lt;strong&gt;finrl&lt;/strong&gt; has three subfolders &lt;strong&gt;applications, agents, finrl_meta&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We employ a &lt;strong&gt;train-test-trade&lt;/strong&gt; pipeline by three files: train.py, test.py, and trade.py.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;FinRL&#xA;├── finrl (main folder)&#xA;│   ├── applications&#xA;│   &#x9;├── cryptocurrency_trading&#xA;│   &#x9;├── high_frequency_trading&#xA;│   &#x9;├── portfolio_allocation&#xA;│   &#x9;└── stock_trading&#xA;│   ├── agents&#xA;│   &#x9;├── elegantrl&#xA;│   &#x9;├── rllib&#xA;│   &#x9;└── stablebaseline3&#xA;│   ├── finrl_meta&#xA;│   &#x9;├── data_processors&#xA;│   &#x9;├── env_cryptocurrency_trading&#xA;│   &#x9;├── env_portfolio_allocation&#xA;│   &#x9;├── env_stock_trading&#xA;│   &#x9;├── preprocessor&#xA;│   &#x9;├── data_processor.py&#xA;│   &#x9;└── finrl_meta_config.py&#xA;│   ├── config.py&#xA;│   ├── config_tickers.py&#xA;│   ├── main.py&#xA;│   ├── plot.py&#xA;│   ├── train.py&#xA;│   ├── test.py&#xA;│   └── trade.py&#xA;│   &#xA;├── tutorial (tutorial notebooks and educational files)&#xA;├── unit_testing (make sure verified codes working on env &amp;amp; data)&#xA;│   ├── test_env&#xA;│   &#x9;└── test_env_cashpenalty.py&#xA;│   └── test_marketdata&#xA;│   &#x9;└── test_yahoodownload.py&#xA;├── setup.py&#xA;├── requirements.txt&#xA;└── README.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Supported Data Sources&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data Source&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Range and Frequency&lt;/th&gt; &#xA;   &lt;th&gt;Request Limits&lt;/th&gt; &#xA;   &lt;th&gt;Raw Data&lt;/th&gt; &#xA;   &lt;th&gt;Preprocessed Data&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://alpaca.markets/docs/introduction/&#34;&gt;Alpaca&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;US Stocks, ETFs&lt;/td&gt; &#xA;   &lt;td&gt;2015-now, 1min&lt;/td&gt; &#xA;   &lt;td&gt;Account-specific&lt;/td&gt; &#xA;   &lt;td&gt;OHLCV&lt;/td&gt; &#xA;   &lt;td&gt;Prices&amp;amp;Indicators&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://baostock.com/baostock/index.php/Python_API%E6%96%87%E6%A1%A3&#34;&gt;Baostock&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CN Securities&lt;/td&gt; &#xA;   &lt;td&gt;1990-12-19-now, 5min&lt;/td&gt; &#xA;   &lt;td&gt;Account-specific&lt;/td&gt; &#xA;   &lt;td&gt;OHLCV&lt;/td&gt; &#xA;   &lt;td&gt;Prices&amp;amp;Indicators&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://binance-docs.github.io/apidocs/spot/en/#public-api-definitions&#34;&gt;Binance&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Cryptocurrency&lt;/td&gt; &#xA;   &lt;td&gt;API-specific, 1s, 1min&lt;/td&gt; &#xA;   &lt;td&gt;API-specific&lt;/td&gt; &#xA;   &lt;td&gt;Tick-level daily aggegrated trades, OHLCV&lt;/td&gt; &#xA;   &lt;td&gt;Prices&amp;amp;Indicators&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.ccxt.com/en/latest/manual.html&#34;&gt;CCXT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Cryptocurrency&lt;/td&gt; &#xA;   &lt;td&gt;API-specific, 1min&lt;/td&gt; &#xA;   &lt;td&gt;API-specific&lt;/td&gt; &#xA;   &lt;td&gt;OHLCV&lt;/td&gt; &#xA;   &lt;td&gt;Prices&amp;amp;Indicators&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://iexcloud.io/docs/api/&#34;&gt;IEXCloud&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NMS US securities&lt;/td&gt; &#xA;   &lt;td&gt;1970-now, 1 day&lt;/td&gt; &#xA;   &lt;td&gt;100 per second per IP&lt;/td&gt; &#xA;   &lt;td&gt;OHLCV&lt;/td&gt; &#xA;   &lt;td&gt;Prices&amp;amp;Indicators&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.joinquant.com/&#34;&gt;JoinQuant&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CN Securities&lt;/td&gt; &#xA;   &lt;td&gt;2005-now, 1min&lt;/td&gt; &#xA;   &lt;td&gt;3 requests each time&lt;/td&gt; &#xA;   &lt;td&gt;OHLCV&lt;/td&gt; &#xA;   &lt;td&gt;Prices&amp;amp;Indicators&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.quantconnect.com/docs/home/home&#34;&gt;QuantConnect&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;US Securities&lt;/td&gt; &#xA;   &lt;td&gt;1998-now, 1s&lt;/td&gt; &#xA;   &lt;td&gt;NA&lt;/td&gt; &#xA;   &lt;td&gt;OHLCV&lt;/td&gt; &#xA;   &lt;td&gt;Prices&amp;amp;Indicators&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.ricequant.com/doc/rqdata/python/&#34;&gt;RiceQuant&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CN Securities&lt;/td&gt; &#xA;   &lt;td&gt;2005-now, 1ms&lt;/td&gt; &#xA;   &lt;td&gt;Account-specific&lt;/td&gt; &#xA;   &lt;td&gt;OHLCV&lt;/td&gt; &#xA;   &lt;td&gt;Prices&amp;amp;Indicators&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://tushare.pro/document/1?doc_id=131&#34;&gt;Tushare&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CN Securities, A share&lt;/td&gt; &#xA;   &lt;td&gt;-now, 1 min&lt;/td&gt; &#xA;   &lt;td&gt;Account-specific&lt;/td&gt; &#xA;   &lt;td&gt;OHLCV&lt;/td&gt; &#xA;   &lt;td&gt;Prices&amp;amp;Indicators&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://wrds-www.wharton.upenn.edu/pages/about/data-vendors/nyse-trade-and-quote-taq/&#34;&gt;WRDS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;US Securities&lt;/td&gt; &#xA;   &lt;td&gt;2003-now, 1ms&lt;/td&gt; &#xA;   &lt;td&gt;5 requests each time&lt;/td&gt; &#xA;   &lt;td&gt;Intraday Trades&lt;/td&gt; &#xA;   &lt;td&gt;Prices&amp;amp;Indicators&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/yfinance/&#34;&gt;YahooFinance&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;US Securities&lt;/td&gt; &#xA;   &lt;td&gt;Frequency-specific, 1min&lt;/td&gt; &#xA;   &lt;td&gt;2,000/hour&lt;/td&gt; &#xA;   &lt;td&gt;OHLCV&lt;/td&gt; &#xA;   &lt;td&gt;Prices&amp;amp;Indicators&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- |Data Source |Type |Max Frequency |Raw Data|Preprocessed Data|&#xA;|  ----  |  ----  |  ----  |  ----  |  ----  |&#xA;|    Alpaca |  US Stocks, ETFs |  1 min |  OHLCV |  Prices, indicators |&#xA;|    Baostock |  CN Securities |  5 min |  OHLCV |  Prices, indicators |&#xA;|    Binance |  Cryptocurrency |  1 s |  OHLCV |  Prices, indicators |&#xA;|    CCXT |  Cryptocurrency |  1 min  |  OHLCV |  Prices, indicators |&#xA;|    IEXCloud |  NMS US securities | 1 day  | OHLCV |  Prices, indicators |&#xA;|    JoinQuant |  CN Securities |  1 min  |  OHLCV |  Prices, indicators |&#xA;|    QuantConnect |  US Securities |  1 s |  OHLCV |  Prices, indicators |&#xA;|    RiceQuant |  CN Securities |  1 ms  |  OHLCV |  Prices, indicators |&#xA;|    Tushare |  CN Securities | 1 min  |  OHLCV |  Prices, indicators |&#xA;|    WRDS |  US Securities |  1 ms  |  Intraday Trades | Prices, indicators |&#xA;|    YahooFinance |  US Securities | 1 min  |  OHLCV  |  Prices, indicators |&#xA;|    AkShare |  CN Securities | 1 day  |  OHLCV |  Prices, indicators |&#xA;|    findatapy |  CN Securities | 1 day  |  OHLCV |  Prices, indicators |&#xA;|    pandas\_datareader |  US Securities |  1 day |  OHLCV | Prices, indicators |&#xA;|    pandas-finance |  US Securities |  1 day  |  OHLCV  &amp; Prices, indicators |&#xA;|    ystockquote |  US Securities |  1 day  |  OHLCV | Prices, indicators |&#xA;|    Marketstack | 50+ countries |  1 day  |  OHLCV | Prices, indicators |&#xA;|    finnhub | US Stocks, currencies, crypto |   1 day |  OHLCV  | Prices, indicators |&#xA;|    Financial Modeling prep | US stocks, currencies, crypto |  1 min |  OHLCV  | Prices, indicators |&#xA;|    EOD Historical Data | US stocks, and ETFs |  1 day  |  OHLCV  | Prices, indicators |&#xA;|    Alpha Vantage | Stock, ETF, forex, crypto, technical indicators | 1 min |  OHLCV  &amp; Prices, indicators |&#xA;|    Tiingo | Stocks, crypto |  1 day  |  OHLCV  | Prices, indicators |&#xA;|    Quandl | 250+ sources |  1 day  |  OHLCV  | Prices, indicators |&#xA;|    Polygon |  US Securities |  1 day  |  OHLCV  | Prices, indicators |&#xA;|    fixer |  Exchange rate |  1 day |  Exchange rate | Exchange rate, indicators |&#xA;|    Exchangerates |  Exchange rate |  1 day  |  Exchange rate | Exchange rate, indicators |&#xA;|    Fixer |  Exchange rate |  1 day  |  Exchange rate | Exchange rate, indicators |&#xA;|    currencylayer |  Exchange rate | 1 day  |  Exchange rate | Exchange rate, indicators |&#xA;|    currencyapi |  Exchange rate | 1 day |  Exchange rate | Exchange rate, indicators |&#xA;|    Open Exchange Rates |  Exchange rate |  1 day  |  Exchange rate | Exchange rate, indicators |&#xA;|    XE |  Exchange rate |  1 day  |  Exchange rate | Exchange rate, indicators |&#xA;|    Xignite |  Exchange rate |  1 day  |  Exchange rate | Exchange rate, indicators | --&gt; &#xA;&lt;p&gt;OHLCV: open, high, low, and close prices; volume. adjusted_close: adjusted close price&lt;/p&gt; &#xA;&lt;p&gt;Technical indicators: &#39;macd&#39;, &#39;boll_ub&#39;, &#39;boll_lb&#39;, &#39;rsi_30&#39;, &#39;dx_30&#39;, &#39;close_30_sma&#39;, &#39;close_60_sma&#39;. Users also can add new features.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/docs/source/start/installation.rst&#34;&gt;install description for all system&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai4finance.medium.com/finrl-for-quantitative-finance-install-and-setup-tutorial-for-beginners-1db80ad39159&#34;&gt;FinRL for Quantitative Finance: Install and Setup Tutorial for Beginners&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Status Update&lt;/h2&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;Version History&lt;/b&gt; &lt;i&gt;[click to expand]&lt;/i&gt;&lt;/summary&gt; &#xA; &lt;div&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;2021-08-25 0.3.1: pytorch version with a three-layer architecture, apps (financial tasks), drl_agents (drl algorithms), neo_finrl (gym env)&lt;/li&gt; &#xA;   &lt;li&gt;2020-12-14 Upgraded to &lt;strong&gt;Pytorch&lt;/strong&gt; with stable-baselines3; Remove tensorflow 1.0 at this moment, under development to support tensorflow 2.0&lt;/li&gt; &#xA;   &lt;li&gt;2020-11-27 0.1: Beta version with tensorflow 1.5&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FinRL is the first open-source framework to demonstrate the great potential of applying DRL algorithms in quantitative finance. We build an ecosystem around the FinRL framework, which seeds the rapidly growing AI4Finance community.&lt;/li&gt; &#xA; &lt;li&gt;The application layer provides interfaces for users to customize FinRL to their own trading tasks. Automated backtesting tool and performance metrics are provided to help quantitative traders iterate trading strategies at a high turnover rate. Profitable trading strategies are reproducible and hands-on tutorials are provided in a beginner-friendly fashion. Adjusting the trained models to the rapidly changing markets is also possible.&lt;/li&gt; &#xA; &lt;li&gt;The agent layer provides state-of-the-art DRL algorithms that are adapted to finance with fine-tuned hyperparameters. Users can add new DRL algorithms.&lt;/li&gt; &#xA; &lt;li&gt;The environment layer includes not only a collection of historical data APIs, but also live trading APIs. They are reconfigured into standard OpenAI gym-style environments. Moreover, it incorporates market frictions and allows users to customize the trading time granularity.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Towardsdatascience] &lt;a href=&#34;https://towardsdatascience.com/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02&#34;&gt;Deep Reinforcement Learning for Automated Stock Trading&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Towardsdatascience] &lt;a href=&#34;https://towardsdatascience.com/finrl-for-quantitative-finance-tutorial-for-multiple-stock-trading-7b00763b7530&#34;&gt;FinRL for Quantitative Finance: Tutorial for Multiple Stock Trading&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Towardsdatascience] &lt;a href=&#34;https://towardsdatascience.com/finrl-for-quantitative-finance-tutorial-for-portfolio-allocation-9b417660c7cd&#34;&gt;FinRL for Quantitative Finance: Tutorial for Portfolio Allocation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Towardsdatascience] &lt;a href=&#34;https://towardsdatascience.com/finrl-for-quantitative-finance-tutorial-for-single-stock-trading-37d6d7c30aac&#34;&gt;FinRL for Quantitative Finance: Tutorial for Single Stock Trading&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Towardsdatascience] &lt;a href=&#34;https://elegantrl.medium.com/elegantrl-podracer-scalable-and-elastic-library-for-cloud-native-deep-reinforcement-learning-bafda6f7fbe0&#34;&gt;ElegantRL-Podracer: A Scalable and Elastic Library for Cloud-Native Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Towardsdatascience] &lt;a href=&#34;https://towardsdatascience.com/elegantrl-a-lightweight-and-stable-deep-reinforcement-learning-library-95cef5f3460b&#34;&gt;ElegantRL: A Lightweight and Stable Deep Reinforcement Learning Library&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Towardsdatascience] &lt;a href=&#34;https://medium.com/@elegantrl/elegantrl-mastering-the-ppo-algorithm-part-i-9f36bc47b791&#34;&gt;ElegantRL: Mastering PPO Algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[MLearning.ai] &lt;a href=&#34;https://medium.com/mlearning-ai/hyperparameter-optimization-using-ray-tune-for-finrl-models-42df2937d53d&#34;&gt;Hyperparameter Optimization using Ray tune for FinRL models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[MLearning.ai] &lt;a href=&#34;https://medium.com/mlearning-ai/an-empirical-approach-to-explain-deep-reinforcement-learning-in-portfolio-management-task-e65a42225d9d&#34;&gt;An Empirical Approach to Explain Deep Reinforcement Learning in Portfolio Management Task&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[MLearning.ai] &lt;a href=&#34;https://medium.com/mlearning-ai/finrl-for-quantitative-finance-plug-and-play-drl-algorithms-11cf494d28b1&#34;&gt;FinRL for Quantitative Finance: plug-and-play DRL algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[MLearning.ai] &lt;a href=&#34;https://elegantrl.medium.com/elegantrl-demo-stock-trading-using-ddpg-part-i-e77d7dc9d208&#34;&gt;ElegantRL Demo: Stock Trading Using DDPG (Part I)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[MLearning.ai] &lt;a href=&#34;https://medium.com/mlearning-ai/elegantrl-demo-stock-trading-using-ddpg-part-ii-d3d97e01999f&#34;&gt;ElegantRL Demo: Stock Trading Using DDPG (Part II)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[DataDrivenInvestor] &lt;a href=&#34;https://medium.datadriveninvestor.com/finrl-meta-a-universe-of-near-real-market-en-vironments-for-data-driven-financial-reinforcement-e1894e1ebfbd&#34;&gt;FinRL-Meta: A Universe of Near Real-Market En­vironments for Data­-Driven Financial Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[DataDrivenInvestor] &lt;a href=&#34;https://medium.datadriveninvestor.com/a-data-scientists-approach-for-algorithmic-trading-using-deep-reinforcement-learning-an-be8da40b2230&#34;&gt;A Data Scientist’s Approach for Algorithmic Trading using Deep Reinforcement Learning: An End-to-end Tutorial for Paper Trading&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Analytics Vidhya] &lt;a href=&#34;https://medium.com/analytics-vidhya/weights-and-biases-ify-stable-baselines-models-in-finrl-f11b67f2a6a7&#34;&gt;Weights and Biases-ify FinRL with Stable Baselines3 models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Analytics Vidhya] &lt;a href=&#34;https://medium.com/analytics-vidhya/hyperparameter-tuning-using-optuna-for-finrl-8a49506d2741&#34;&gt;Hyperparameter tuning using optuna for FinRL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Analytics Vidhya] &lt;a href=&#34;https://medium.com/analytics-vidhya/a-hitchhikers-guide-to-finrl-a-deep-reinforcement-learning-framework-for-quantitative-finance-e624c508f763&#34;&gt;A hitchhikers guide to FinRL: A Deep Reinforcement Learning Framework for Quantitative Finance&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Analyticsindiamag.com] &lt;a href=&#34;https://analyticsindiamag.com/stock-market-prediction-using-finrl/&#34;&gt;How To Automate Stock Market Using FinRL (Deep Reinforcement Learning Library)?&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Publications&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Title&lt;/th&gt; &#xA;   &lt;th&gt;Conference&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Citations&lt;/th&gt; &#xA;   &lt;th&gt;Year&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;FinRL-Meta&lt;/strong&gt;: A Universe of Near-Real Market Environments for Data-Driven Deep Reinforcement Learning in Quantitative Finance&lt;/td&gt; &#xA;   &lt;td&gt;NeurIPS 2021 Data-Centric AI Workshop&lt;/td&gt; &#xA;   &lt;td&gt;paper: &lt;a href=&#34;https://arxiv.org/abs/2112.06753&#34;&gt;https://arxiv.org/abs/2112.06753&lt;/a&gt; ;&lt;br&gt;code: &lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinRL-Meta&#34;&gt;https://github.com/AI4Finance-Foundation/FinRL-Meta&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Explainable deep reinforcement learning for portfolio management: An empirical approach&lt;/td&gt; &#xA;   &lt;td&gt;ICAIF 2021 : ACM International Conference on AI in Finance&lt;/td&gt; &#xA;   &lt;td&gt;paper: &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3958005&#34;&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3958005&lt;/a&gt;;&lt;br&gt;code: &lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinRL&#34;&gt;https://github.com/AI4Finance-Foundation/FinRL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;FinRL-Podracer&lt;/strong&gt;: High performance and scalable deep reinforcement learning for quantitative finance&lt;/td&gt; &#xA;   &lt;td&gt;ICAIF 2021 : ACM International Conference on AI in Finance&lt;/td&gt; &#xA;   &lt;td&gt;paper: &lt;a href=&#34;https://arxiv.org/abs/2111.05188&#34;&gt;https://arxiv.org/abs/2111.05188&lt;/a&gt;;&lt;br&gt;code: &lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinRL_Podracer&#34;&gt;https://github.com/AI4Finance-Foundation/FinRL_Podracer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;FinRL&lt;/strong&gt;: Deep reinforcement learning framework to automate trading in quantitative finance&lt;/td&gt; &#xA;   &lt;td&gt;ICAIF 2021 : ACM International Conference on AI in Finance&lt;/td&gt; &#xA;   &lt;td&gt;paper: &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3955949&#34;&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3955949&lt;/a&gt;;&lt;br&gt;code: &lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinRL&#34;&gt;https://github.com/AI4Finance-Foundation/FinRL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;FinRL&lt;/strong&gt;: A deep reinforcement learning library for automated stock trading in quantitative finance&lt;/td&gt; &#xA;   &lt;td&gt;NeurIPS 2020 Deep RL Workshop&lt;/td&gt; &#xA;   &lt;td&gt;paper: &lt;a href=&#34;https://arxiv.org/abs/2011.09607&#34;&gt;https://arxiv.org/abs/2011.09607&lt;/a&gt;;&lt;br&gt;code: &lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinRL&#34;&gt;https://github.com/AI4Finance-Foundation/FinRL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;2020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deep reinforcement learning for automated stock trading: An ensemble strategy&lt;/td&gt; &#xA;   &lt;td&gt;ICAIF 2020 : ACM International Conference on AI in Finance&lt;/td&gt; &#xA;   &lt;td&gt;paper: &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3690996&#34;&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3690996&lt;/a&gt;;&lt;br&gt;code: &lt;a href=&#34;https://github.com/AI4Finance-Foundation/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020&#34;&gt;https://github.com/AI4Finance-Foundation/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;44&lt;/td&gt; &#xA;   &lt;td&gt;2020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi-agent reinforcement learning for liquidation strategy analysis&lt;/td&gt; &#xA;   &lt;td&gt;ICML 2019 Workshop on AI in Finance: Applications and Infrastructure for Multi-Agent Learning&lt;/td&gt; &#xA;   &lt;td&gt;paper: &lt;a href=&#34;https://arxiv.org/abs/1906.11046&#34;&gt;https://arxiv.org/abs/1906.11046&lt;/a&gt;; &lt;br&gt;code: &lt;a href=&#34;https://github.com/AI4Finance-Foundation/Liquidation-Analysis-using-Multi-Agent-Reinforcement-Learning-ICML-2019&#34;&gt;https://github.com/AI4Finance-Foundation/Liquidation-Analysis-using-Multi-Agent-Reinforcement-Learning-ICML-2019&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;19&lt;/td&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Practical deep reinforcement learning approach for stock trading&lt;/td&gt; &#xA;   &lt;td&gt;NeurIPS 2018 Workshop on Challenges and Opportunities for AI in Financial Services&lt;/td&gt; &#xA;   &lt;td&gt;paper: &lt;a href=&#34;https://arxiv.org/abs/1811.07522&#34;&gt;https://arxiv.org/abs/1811.07522&lt;/a&gt;; &lt;br&gt;code: &lt;a href=&#34;https://github.com/AI4Finance-Foundation/DQN-DDPG_Stock_Trading&#34;&gt;https://github.com/AI4Finance-Foundation/DQN-DDPG_Stock_Trading&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;86&lt;/td&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[央广网] &lt;a href=&#34;http://tech.cnr.cn/techph/20211123/t20211123_525669092.shtml&#34;&gt;2021 IDEA大会于福田圆满落幕：群英荟萃论道AI 多项目发布亮点纷呈&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[央广网] &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1717101783873523790&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;2021 IDEA大会开启AI思想盛宴 沈向洋理事长发布六大前沿产品&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[IDEA新闻] &lt;a href=&#34;https://idea.edu.cn/news/20211213143128.html&#34;&gt;2021 IDEA大会发布产品FinRL-Meta——基于数据驱动的强化学习金融风险模拟系统&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[知乎] &lt;a href=&#34;https://zhuanlan.zhihu.com/p/437804814&#34;&gt;FinRL-Meta基于数据驱动的强化学习金融元宇宙&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[量化投资与机器学习] &lt;a href=&#34;https://www.mdeditor.tw/pl/p5Gg&#34;&gt;基于深度强化学习的股票交易策略框架（代码+文档)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[运筹OR帷幄] &lt;a href=&#34;https://zhuanlan.zhihu.com/p/353557417&#34;&gt;领读计划NO.10 | 基于深度增强学习的量化交易机器人：从AlphaGo到FinRL的演变过程&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[深度强化实验室] &lt;a href=&#34;https://blog.csdn.net/deeprl/article/details/114828024&#34;&gt;【重磅推荐】哥大开源“FinRL”: 一个用于量化金融自动交易的深度强化学习库&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[商业新知] &lt;a href=&#34;https://www.shangyexinzhi.com/article/4170766.html&#34;&gt;金融科技讲座回顾|AI4Finance: 从AlphaGo到FinRL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Kaggle] &lt;a href=&#34;https://www.kaggle.com/c/jane-street-market-prediction/discussion/199313&#34;&gt;Jane Street Market Prediction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[矩池云Matpool] &lt;a href=&#34;http://www.python88.com/topic/111918&#34;&gt;在矩池云上如何运行FinRL股票交易策略框架&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[财智无界] &lt;a href=&#34;https://www.sohu.com/a/486837028_120929319&#34;&gt;金融学会常务理事陈学彬: 深度强化学习在金融资产管理中的应用&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Neurohive] &lt;a href=&#34;https://neurohive.io/ru/gotovye-prilozhenija/finrl-glubokoe-obuchenie-s-podkrepleniem-dlya-trejdinga/&#34;&gt;FinRL: глубокое обучение с подкреплением для трейдинга&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[ICHI.PRO] &lt;a href=&#34;https://ichi.pro/ko/yangjeog-geum-yung-eul-wihan-finrl-dan-il-jusig-geolaeleul-wihan-tyutolieol-61395882412716&#34;&gt;양적 금융을위한 FinRL: 단일 주식 거래를위한 튜토리얼&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citing FinRL&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{finrl2020,&#xA;    author  = {Liu, Xiao-Yang and Yang, Hongyang and Chen, Qian and Zhang, Runjia and Yang, Liuqing and Xiao, Bowen and Wang, Christina Dan},&#xA;    title   = {{FinRL}: A deep reinforcement learning library for automated stock trading in quantitative finance},&#xA;    journal = {Deep RL Workshop, NeurIPS 2020},&#xA;    year    = {2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{liu2021finrl,&#xA;    author  = {Liu, Xiao-Yang and Yang, Hongyang and Gao, Jiechao and Wang, Christina Dan},&#xA;    title   = {{FinRL}: Deep reinforcement learning framework to automate trading in quantitative finance},&#xA;    journal = {ACM International Conference on AI in Finance (ICAIF)},&#xA;    year    = {2021}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We published &lt;a href=&#34;http://tensorlet.org/projects/ai-in-finance/&#34;&gt;FinTech papers&lt;/a&gt;, check &lt;a href=&#34;https://scholar.google.com/citations?view_op=list_works&amp;amp;hl=en&amp;amp;hl=en&amp;amp;user=XsdPXocAAAAJ&#34;&gt;Google Scholar&lt;/a&gt;, resulting in this project. Closely related papers are given in the &lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinRL/raw/master/tutorials/FinRL_papers.md&#34;&gt;list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Join and Contribute&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to the &lt;strong&gt;AI4Finance Foundation&lt;/strong&gt; community!&lt;/p&gt; &#xA;&lt;p&gt;Join to discuss FinRL: &lt;a href=&#34;https://groups.google.com/u/1/g/ai4finance&#34;&gt;AI4Finance mailing list&lt;/a&gt;, AI4Finance Slack channel:&lt;/p&gt; &#xA;&lt;a href=&#34;https://join.slack.com/t/ai4financeworkspace/shared_invite/zt-v670l1jm-dzTgIT9fHZIjjrqprrY0kg&#34; target=&#34;\_blank&#34;&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/figs/join_slack.png&#34; width=&#34;35%&#34;&gt; &#xA; &lt;/div&gt; &lt;/a&gt; &#xA;&lt;b&gt;Follow us on WeChat:&lt;/b&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;http://www.tensorlet.org/wp-content/uploads/2021/01/qrcode_for_gh_feece88824ab_258.jpg&#34; width=&#34;25%&#34;&gt; &#xA;&lt;/div&gt;  &#xA;&lt;p&gt;Please check &lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinRL/raw/master/tutorials/Contributing.md&#34;&gt;Contributing Guidances&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Contributors&lt;/h3&gt; &#xA;&lt;p&gt;Thanks!&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/AI4Finance-LLC/FinRL-Library/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=AI4Finance-LLC/FinRL-Library&#34;&gt; &lt;/a&gt; &#xA;&lt;h3&gt;Sponsorship&lt;/h3&gt; &#xA;&lt;p&gt;Welcome gift money to support AI4Finance, a non-profit academic community. Use the links in the right, or scan the following vemo QR code:&lt;/p&gt; &#xA;&lt;p&gt;Detailed sponsorship records can be found at &lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinRL/issues/425&#34;&gt;Issue #425&lt;/a&gt;&lt;/p&gt; &#xA;&lt;a target=&#34;\_blank&#34;&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL/master/figs/Xiao-Yang-Liu_AI4Finance_vemo.png&#34; width=&#34;35%&#34;&gt; &#xA; &lt;/div&gt; &lt;/a&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;MIT License&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Disclaimer: Nothing herein is financial advice, and NOT a recommendation to trade real money. Please use common sense and always first consult a professional before trading or investing.&lt;/strong&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/Data-Science-For-Beginners</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/microsoft/Data-Science-For-Beginners</id>
    <link href="https://github.com/microsoft/Data-Science-For-Beginners" rel="alternate"></link>
    <summary type="html">&lt;p&gt;10 Weeks, 20 Lessons, Data Science for All!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Data Science for Beginners - A Curriculum&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/Data-Science-For-Beginners/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/graphs/contributors/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/issues/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/pulls/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/watchers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Watch&#34; alt=&#34;GitHub watchers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Fork&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Star&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 10-week, 20-lesson curriculum all about Data Science. Each lesson includes pre-lesson and post-lesson quizzes, written instructions to complete the lesson, a solution, and an assignment. Our project-based pedagogy allows you to learn while building, a proven way for new skills to &#39;stick&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hearty thanks to our authors:&lt;/strong&gt; &lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine Greenaway&lt;/a&gt;, &lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry Soshnikov&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya Narasimhan&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/JalenMcG&#34;&gt;Jalen McGee&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen Looper&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud Levy&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany Souterre&lt;/a&gt;, &lt;a href=&#34;https://www.twitter.com/geektrainer&#34;&gt;Christopher Harrison&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🙏 Special thanks 🙏 to our &lt;a href=&#34;https://studentambassadors.microsoft.com/&#34;&gt;Microsoft Student Ambassador&lt;/a&gt; authors, reviewers and content contributors,&lt;/strong&gt; notably Aaryan Arora, &lt;a href=&#34;https://github.com/AdityaGarg00&#34;&gt;Aditya Garg&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/alondra-sanchez-molina/&#34;&gt;Alondra Sanchez&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/ankitasingh007&#34;&gt;Ankita Singh&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/anupam--mishra/&#34;&gt;Anupam Mishra&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/arpitadas01/&#34;&gt;Arpita Das&lt;/a&gt;, ChhailBihari Dubey, &lt;a href=&#34;https://www.linkedin.com/in/dibrinsofor&#34;&gt;Dibri Nsofor&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/dishita-bhasin-7065281bb&#34;&gt;Dishita Bhasin&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/majd-s/&#34;&gt;Majd Safi&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/max-blum-6036a1186/&#34;&gt;Max Blum&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/miguelmque/&#34;&gt;Miguel Correa&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/iftu119&#34;&gt;Mohamma Iftekher (Iftu) Ebne Jalal&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/nawrin-tabassum&#34;&gt;Nawrin Tabassum&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/raymond-wp/&#34;&gt;Raymond Wangsa Putra&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/rty2423&#34;&gt;Rohit Yadav&lt;/a&gt;, Samridhi Sharma, &lt;a href=&#34;https://www.linkedin.com/mwlite/in/sanya-sinha-13aab1200&#34;&gt;Sanya Sinha&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/sheena-narua-n/&#34;&gt;Sheena Narula&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/tauqeerahmad5201/&#34;&gt;Tauqeer Ahmad&lt;/a&gt;, Yogendrasingh Pawar , &lt;a href=&#34;https://www.linkedin.com/in/vidushi-gupta07/&#34;&gt;Vidushi Gupta&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jasleen-sondhi/&#34;&gt;Jasleen Sondhi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Title.png&#34; alt=&#34; Sketchnote by (@sketchthedocs) &#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science For Beginners - &lt;em&gt;Sketchnote by &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;: we have &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/for-teachers.md&#34;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum. We&#39;d love your feedback &lt;a href=&#34;https://github.com/microsoft/Data-Science-For-Beginners/discussions&#34;&gt;in our discussion forum&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://aka.ms/student-page&#34;&gt;Students&lt;/a&gt;&lt;/strong&gt;: to use this curriculum on your own, fork the entire repo and complete the exercises on your own, starting with a pre-lecture quiz. Then read the lecture and complete the rest of the activities. Try to create the projects by comprehending the lessons rather than copying the solution code; however, that code is available in the /solutions folders in each project-oriented lesson. Another idea would be to form a study group with friends and go through the content together. For further study, we recommend &lt;a href=&#34;https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/qprpajyoy3x0g7?WT.mc_id=academic-40229-cxa&#34;&gt;Microsoft Learn&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Meet the Team&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/8mzavjQSMM4&#34; title=&#34;Promo video&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/ds-for-beginners.gif&#34; alt=&#34;Promo video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gif by&lt;/strong&gt; &lt;a href=&#34;https://www.linkedin.com/in/mohitjaisal&#34;&gt;Mohit Jaisal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🎥 Click the image above for a video about the project the folks who created it!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Pedagogy&lt;/h2&gt; &#xA;&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is project-based and that it includes frequent quizzes. By the end of this series, students will have learned basic principles of data science, including ethical concepts, data preparation, different ways of working with data, data visualization, data analysis, real-world use cases of data science, and more.&lt;/p&gt; &#xA;&lt;p&gt;In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 10 week cycle.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Find our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&#34;&gt;Translation&lt;/a&gt; guidelines. We welcome your constructive feedback!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Each lesson includes:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optional sketchnote&lt;/li&gt; &#xA; &lt;li&gt;Optional supplemental video&lt;/li&gt; &#xA; &lt;li&gt;Pre-lesson warmup quiz&lt;/li&gt; &#xA; &lt;li&gt;Written lesson&lt;/li&gt; &#xA; &lt;li&gt;For project-based lessons, step-by-step guides on how to build the project&lt;/li&gt; &#xA; &lt;li&gt;Knowledge checks&lt;/li&gt; &#xA; &lt;li&gt;A challenge&lt;/li&gt; &#xA; &lt;li&gt;Supplemental reading&lt;/li&gt; &#xA; &lt;li&gt;Assignment&lt;/li&gt; &#xA; &lt;li&gt;Post-lesson quiz&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained &lt;a href=&#34;https://red-water-0103e7a0f.azurestaticapps.net/&#34;&gt;in this app&lt;/a&gt;, for 40 total quizzes of three questions each. They are linked from within the lessons, but the quiz app can be run locally; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder. They are gradually being localized.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Lessons&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Roadmap.png&#34; alt=&#34; Sketchnote by (@sketchthedocs) &#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science For Beginners: Roadmap - &lt;em&gt;Sketchnote by &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Number&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Topic&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Grouping&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Learning Objectives&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Linked Lesson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Author&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Defining Data Science&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Learn the basic concepts behind data science and how it’s related to artificial intelligence, machine learning, and big data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/01-defining-data-science/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/beZ7Mb_oz9I&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science Ethics&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Ethics Concepts, Challenges &amp;amp; Frameworks.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/02-ethics/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Defining Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;How data is classified and its common sources.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/03-defining-data/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to Statistics &amp;amp; Probability&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The mathematical techniques of probability and statistics to understand data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/04-stats-and-probability/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/Z5Zy85g4Yjw&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with Relational Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to relational data and the basics of exploring and analyzing relational data with the Structured Query Language, also known as SQL (pronounced “see-quell”).&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/05-relational-databases/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/geektrainer&#34;&gt;Christopher&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with NoSQL Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to non-relational data, its various types and the basics of exploring and analyzing document databases.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/06-non-relational/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with Python&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Basics of using Python for data exploration with libraries such as Pandas. Foundational understanding of Python programming is recommended.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/07-python/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/dZjWOGbsN4Y&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Preparation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Topics on data techniques for cleaning and transforming the data to handle challenges of missing, inaccurate, or incomplete data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/08-data-preparation/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Quantities&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Learn how to use Matplotlib to visualize bird data 🦆&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/09-visualization-quantities/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Distributions of Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing observations and trends within an interval.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/10-visualization-distributions/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Proportions&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing discrete and grouped percentages.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/11-visualization-proportions/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Relationships&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing connections and correlations between sets of data and their variables.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/12-visualization-relationships/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meaningful Visualizations&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Techniques and guidance for making your visualizations valuable for effective problem solving and insights.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/13-meaningful-visualizations/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to the Data Science lifecycle&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to the data science lifecycle and its first step of acquiring and extracting data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/14-Introduction/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Analyzing&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This phase of the data science lifecycle focuses on techniques to analyze data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/15-analyzing/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Communication&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This phase of the data science lifecycle focuses on presenting the insights from the data in a way that makes it easier for decision makers to understand.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/16-communication/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/JalenMcG&#34;&gt;Jalen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This series of lessons introduces data science in the cloud and its benefits.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/17-Introduction/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Training models using Low Code tools.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/18-Low-Code/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Deploying models with Azure Machine Learning Studio.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/19-Azure/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Wild&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/README.md&#34;&gt;In the Wild&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data science driven projects in the real world.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/20-Real-World-Examples/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Offline access&lt;/h2&gt; &#xA;&lt;p&gt;You can run this documentation offline by using &lt;a href=&#34;https://docsify.js.org/#/&#34;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&#34;https://docsify.js.org/#/quickstart&#34;&gt;install Docsify&lt;/a&gt; on your local machine, then in the root folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note, notebooks will not be rendered via Docsify, so when you need to run a notebook, do that separately in VS Code running a Python kernel.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;PDF&lt;/h2&gt; &#xA;&lt;p&gt;A PDF of all of the lessons can be found &lt;a href=&#34;https://microsoft.github.io/Data-Science-For-Beginners/pdf/readme.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Help Wanted!&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to translate all or part of the curriculum, please follow our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&#34;&gt;Translations&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;h2&gt;Other Curricula&lt;/h2&gt; &#xA;&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ml-beginners&#34;&gt;Machine Learning for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/iot-beginners&#34;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/webdev-beginners&#34;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-beginners&#34;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ageron/handson-ml2</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/ageron/handson-ml2</id>
    <link href="https://github.com/ageron/handson-ml2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Notebooks&lt;/h1&gt; &#xA;&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in python. It contains the example code and solutions to the exercises in the second edition of my O&#39;Reilly book &lt;a href=&#34;https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/&#34;&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;img src=&#34;https://images-na.ssl-images-amazon.com/images/I/51aqYc1QyrL._SX379_BO1,204,203,200_.jpg&#34; title=&#34;book&#34; width=&#34;150&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are looking for the first edition notebooks, check out &lt;a href=&#34;https://github.com/ageron/handson-ml&#34;&gt;ageron/handson-ml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Want to play with these notebooks online without having to install anything?&lt;/h3&gt; &#xA;&lt;p&gt;Use any of the following services (I recommended Colab or Kaggle, since they offer free GPUs and TPUs).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: &lt;em&gt;Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you download any data you care about.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/ageron/handson-ml2/blob/master/&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://homl.info/kaggle/&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Open in Kaggle&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/ageron/handson-ml2/HEAD?filepath=%2Findex.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Launch binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://homl.info/deepnote/&#34;&gt;&lt;img src=&#34;https://deepnote.com/buttons/launch-in-deepnote-small.svg?sanitize=true&#34; alt=&#34;Launch in Deepnote&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Just want to quickly look at some notebooks, without executing any code?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg?sanitize=true&#34; alt=&#34;Render nbviewer&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ageron/handson-ml2/raw/master/index.ipynb&#34;&gt;github.com&#39;s notebook viewer&lt;/a&gt; also works but it&#39;s not ideal: it&#39;s slower, the math equations are not always displayed correctly, and large notebooks often fail to open.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Want to run this project using a Docker image?&lt;/h3&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://github.com/ageron/handson-ml2/tree/master/docker&#34;&gt;Docker instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Want to install this project on your own machine?&lt;/h3&gt; &#xA;&lt;p&gt;Start by installing &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;Anaconda&lt;/a&gt; (or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;), &lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;git&lt;/a&gt;, and if you have a TensorFlow-compatible GPU, install the &lt;a href=&#34;https://www.nvidia.com/Download/index.aspx&#34;&gt;GPU driver&lt;/a&gt;, as well as the appropriate version of CUDA and cuDNN (see TensorFlow&#39;s documentation for more details).&lt;/p&gt; &#xA;&lt;p&gt;Next, clone this project by opening a terminal and typing the following commands (do not type the first &lt;code&gt;$&lt;/code&gt; signs on each line, they just indicate that these are terminal commands):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml2.git&#xA;$ cd handson-ml2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml&#xA;$ conda activate tf2&#xA;$ python -m ipykernel install --user --name=python3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, start Jupyter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you need further instructions, read the &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;detailed installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Which Python version should I use?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I recommend Python 3.8. If you follow the installation instructions above, that&#39;s the version you will get. Most code will work with other versions of Python 3, but some libraries do not support Python 3.9 or 3.10 yet, which is why I recommend Python 3.8.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;m getting an error when I call &lt;code&gt;load_housing_data()&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Make sure you call &lt;code&gt;fetch_housing_data()&lt;/code&gt; &lt;em&gt;before&lt;/em&gt; you call &lt;code&gt;load_housing_data()&lt;/code&gt;. If you&#39;re getting an HTTP error, make sure you&#39;re running the exact same code as in the notebook (copy/paste it if needed). If the problem persists, please check your network configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;m getting an SSL error on MacOSX&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You probably need to install the SSL certificates (see this &lt;a href=&#34;https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error&#34;&gt;StackOverflow question&lt;/a&gt;). If you downloaded Python from the official website, then run &lt;code&gt;/Applications/Python\ 3.8/Install\ Certificates.command&lt;/code&gt; in a terminal (change &lt;code&gt;3.8&lt;/code&gt; to whatever version you installed). If you installed Python using MacPorts, run &lt;code&gt;sudo port install curl-ca-bundle&lt;/code&gt; in a terminal.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;ve installed this project locally. How do I update it to the latest version?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How do I update my Python libraries to the latest versions, when using Anaconda?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;I would like to thank everyone &lt;a href=&#34;https://github.com/ageron/handson-ml2/graphs/contributors&#34;&gt;who contributed to this project&lt;/a&gt;, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park and Ian Beauregard who reviewed every notebook and submitted many PRs, including help on some of the exercise solutions. Thanks as well to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory, and to github user SuperYorio who helped on some exercise solutions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>coodict/python3-in-one-pic</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/coodict/python3-in-one-pic</id>
    <link href="https://github.com/coodict/python3-in-one-pic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn python3 in one picture.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python3 in one pic&lt;/h1&gt; &#xA;&lt;p&gt;[ Languages: &lt;a href=&#34;https://raw.githubusercontent.com/coodict/python3-in-one-pic/master/README.md&#34;&gt;English&lt;/a&gt; ]&lt;/p&gt; &#xA;&lt;!-- BADGES/ --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/coodict/python3-in-one-pic&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/gitter-Join%20Chat-brightgreen.svg?sanitize=true&#34; alt=&#34;Gitter chat button&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.coinbase.com/rainyear&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/bitcoin-donate-yellow.svg?sanitize=true&#34; alt=&#34;BitCoin donate button&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- /BADGES --&gt; &#xA;&lt;h1&gt;Online Version&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://git.io/Coo-py3&#34;&gt;https://git.io/Coo-py3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Notebook&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/coodict/python3-in-one-pic/raw/master/notebooks/py3-in-one-pic.ipynb&#34;&gt;IPython Notebook Version&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Preview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/coodict/python3-in-one-pic/master/py3%20in%20one%20pic.png&#34; alt=&#34;py3 in one pic&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Releated projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/coodict/javascript-in-one-pic&#34;&gt;Javascript in one pic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Go in one pic (in preparation)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Use IPython notebook, it&#39;s really very cool!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;&lt;code&gt;import this&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Basic Syntax&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Native Datatypes&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Number&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; String&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Boolean&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; None&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Byte&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; List&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Tuple&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Set&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Dict&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Operators &amp;amp; Casting&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Flow Control&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;code&gt;if/elif/else&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;code&gt;for...in...&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;code&gt;while&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;code&gt;break&lt;/code&gt; &amp;amp; &lt;code&gt;continue&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Iterators &amp;amp; Generators&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Comprehensions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Function&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Definition&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Arguments&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Lambda&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Documentation&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; @decorator&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Class(OOP)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;code&gt;class&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;code&gt;__init__()&lt;/code&gt; &amp;amp; &lt;code&gt;self&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Instance&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inheritance&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Override&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Module&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;code&gt;import&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Search Path&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Package&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Pythonic&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Standard Libraries&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;code&gt;os, sys&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;code&gt;datetime&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Donation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project helpful, please consider making a donation with &lt;a href=&#34;https://www.coinbase.com/rainyear&#34;&gt;bitcoin&lt;/a&gt; or &lt;a href=&#34;https://github.com/rainyear/lolita/wiki/Donation&#34;&gt;other way&lt;/a&gt; &lt;span&gt;🍻&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.python.org/3/index.html&#34;&gt;Python 3.4.3 documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.diveintopython3.net/table-of-contents.html&#34;&gt;Dive Into Python 3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Writing Idiomatic Python 3.3&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://google.github.io/styleguide/pyguide.html&#34;&gt;Google Python Style Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000&#34;&gt;廖雪峰的Python教程&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/coodict/python3-in-one-pic/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for license rights and limitations (MIT).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sokrypton/ColabFold</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/sokrypton/ColabFold</id>
    <link href="https://github.com/sokrypton/ColabFold" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Making Protein folding accessible to all via Google Colab!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ColabFold&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/sokrypton/ColabFold/raw/main/.github/ColabFold_Marv_Logo.png&#34; height=&#34;250&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Making Protein folding accessible to all via Google Colab!&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Notebooks&lt;/th&gt; &#xA;   &lt;th&gt;monomers&lt;/th&gt; &#xA;   &lt;th&gt;complexes&lt;/th&gt; &#xA;   &lt;th&gt;mmseqs2&lt;/th&gt; &#xA;   &lt;th&gt;jackhmmer&lt;/th&gt; &#xA;   &lt;th&gt;templates&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb&#34;&gt;AlphaFold2_mmseqs2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/batch/AlphaFold2_batch.ipynb&#34;&gt;AlphaFold2_batch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/RoseTTAFold.ipynb&#34;&gt;RoseTTAFold&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb&#34;&gt;AlphaFold2&lt;/a&gt; (from Deepmind)&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;BETA (in development) notebooks&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/AlphaFold2_advanced.ipynb&#34;&gt;AlphaFold2_advanced&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;OLD retired notebooks&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2_complexes.ipynb&#34;&gt;AlphaFold2_complexes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/AlphaFold_wJackhmmer.ipynb&#34;&gt;AlphaFold2_jackhmmer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/verbose/alphafold_noTemplates_noMD.ipynb&#34;&gt;AlphaFold2_noTemplates_noMD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/verbose/alphafold_noTemplates_yesMD.ipynb&#34;&gt;AlphaFold2_noTemplates_yesMD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;FAQ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Can I use the models for &lt;strong&gt;Molecular Replacement&lt;/strong&gt;? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yes, but be &lt;strong&gt;CAREFUL&lt;/strong&gt;, the bfactor column is populated with pLDDT confidence values (higher = better). Phenix.phaser expects a &#34;real&#34; bfactor, where (lower = better). See &lt;a href=&#34;https://twitter.com/cheshireminima/status/1423929241675120643&#34;&gt;post&lt;/a&gt; from Claudia Millán.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;What is the maximum length? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Limits depends on free GPU provided by Google-Colab &lt;code&gt;fingers-crossed&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;For GPU: &lt;code&gt;Tesla T4&lt;/code&gt; or &lt;code&gt;Tesla P100&lt;/code&gt; with ~16G the max length is ~1400&lt;/li&gt; &#xA;   &lt;li&gt;For GPU: &lt;code&gt;Tesla K80&lt;/code&gt; with ~12G the max length is ~1000&lt;/li&gt; &#xA;   &lt;li&gt;To check what GPU you got, open a new code cell and type &lt;code&gt;!nvidia-smi&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Is it okay to use the MMseqs2 MSA server (&lt;code&gt;cf.run_mmseqs2&lt;/code&gt;) on a local computer? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can access the server from a local computer if you queries are serial from a single IP. Please do not use multiple computers to query the server.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Where can I download the databases used by ColabFold? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The databases are available at &lt;a href=&#34;https://colabfold.mmseqs.com&#34;&gt;colabfold.mmseqs.com&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;I want to render my own images of the predicted structures, how do I color by pLDDT? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;In pymol for AlphaFold structures: &lt;code&gt;spectrum b, red_yellow_green_cyan_blue, minimum=50, maximum=90&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;In pymol for RoseTTAFold structures: &lt;code&gt;spectrum b, red_yellow_green_cyan_blue, minimum=0.5, maximum=0.9&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;What is the difference between the AlphaFold2_advanced and AlphaFold2_mmseqs2 (_batch) notebook for complex prediction? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We currently have two different ways to predict protein complexes: (1) using the AlphaFold2 model with residue index jump and (2) using the AlphaFold2-multimer model. AlphaFold2_advanced supports (1) and AlphaFold2_mmseqs2 (_batch) (2).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;What is the difference between localcolabfold and the pip installable colabfold_batch? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;localcolabfold is a command line interface for our advanced notebooks. pip is a command line version of the alphafold_mmseqs2 and alphafold_batch notebook.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Running locally&lt;/h3&gt; &#xA;&lt;p&gt;_Note: Checkout &lt;a href=&#34;https://github.com/YoshitakaMo/localcolabfold&#34;&gt;localcolabfold&lt;/a&gt; too&lt;/p&gt; &#xA;&lt;p&gt;Install ColabFold using the &lt;code&gt;pip&lt;/code&gt; commands below. &lt;code&gt;pip&lt;/code&gt; will resolve and install all required dependencies and ColabFold should be ready within a few minutes to use. Please check the &lt;a href=&#34;https://github.com/google/jax#pip-installation-gpu-cuda&#34;&gt;JAX documentation&lt;/a&gt; for how to get JAX to work on your GPU or TPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install &#34;colabfold[alphafold] @ git+https://github.com/sokrypton/ColabFold&#34;&#xA;pip install --upgrade &#34;jax[cuda]&amp;lt;0.3.0&#34; -f https://storage.googleapis.com/jax-releases/jax_releases.html  # Note: wheels only available on linux.&#xA;# For template-based predictions also install kalign and hhsuite&#xA;conda install -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0&#xA;# For amber also install openmm and pdbfixer&#xA;conda install -c conda-forge openmm=7.5.1 pdbfixer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;colabfold_batch &amp;lt;directory_with_fasta_files&amp;gt; &amp;lt;result_dir&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If no GPU or TPU is present, &lt;code&gt;colabfold_batch&lt;/code&gt; can be executed (slowly) using only a CPU with the &lt;code&gt;--cpu&lt;/code&gt; parameter.&lt;/p&gt; &#xA;&lt;h3&gt;Generating MSAs for large scale structure/complex predictions&lt;/h3&gt; &#xA;&lt;p&gt;First create a directory for the databases on a disk with sufficient storage (940GB (!)). Depending on where you are, this will take a couple of hours:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./setup_databases.sh /path/to/db_folder&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download and unpack mmseqs (Note: The required features aren&#39;t in a release yet, so currently, you need to compile the latest version from source yourself or use a &lt;a href=&#34;https://mmseqs.com/latest/mmseqs-linux-avx2.tar.gz&#34;&gt;static binary&lt;/a&gt;). If mmseqs is not in your &lt;code&gt;PATH&lt;/code&gt;, replace &lt;code&gt;mmseqs&lt;/code&gt; below with the path to your mmseqs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# This needs a lot of CPU&#xA;colabfold_search input_sequences.fasta /path/to/db_folder msas&#xA;# This needs a GPU&#xA;colabfold_batch msas predictions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create intermediate folder &lt;code&gt;msas&lt;/code&gt; that contains all input multiple sequence alignments formated as a3m files and a &lt;code&gt;predictions&lt;/code&gt; folder with all predicted pdb,json and png files.&lt;/p&gt; &#xA;&lt;p&gt;Searches against the ColabFoldDB can be done in two different modes:&lt;/p&gt; &#xA;&lt;p&gt;(1) Batch searches with many sequences against the ColabFoldDB quires a machine with approx. 128GB RAM. The search should be performed on the same machine that called &lt;code&gt;setup_databases.sh&lt;/code&gt; since the database index size is adjusted to the main memory size. To search on computers with less main memory delete the index by removing all &lt;code&gt;.idx&lt;/code&gt; files, this will force MMseqs2 to create an index on the fly in memory. MMSeqs2 is optimized for large input sequence sets sizes. For batch searches use the &lt;code&gt;--db-load-mode 0&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;p&gt;(2) single query searches require the full index (the .idx files) to be kept in memory. This can be done with e.g. by using &lt;a href=&#34;https://github.com/hoytech/vmtouch&#34;&gt;vmtouch&lt;/a&gt;. Thus, this type of search requires a machine with at least 768GB RAM for the ColabfoldDB. If the index is in memory use to &lt;code&gt;--db-load-mode 3&lt;/code&gt; parameter in &lt;code&gt;colabfold_search&lt;/code&gt; to avoid index loading overhead. If they database is already in memory use &lt;code&gt;--db-load-mode 2&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;h3&gt;Tutorials &amp;amp; Presentations&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ColabFold Tutorial presented at the Boston Protein Design and Modeling Club. &lt;a href=&#34;https://www.youtube.com/watch?v=Rfw7thgGTwI&#34;&gt;[video]&lt;/a&gt; &lt;a href=&#34;https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI&#34;&gt;[slides]&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Projects based on ColabFold or helpers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YoshitakaMo/localcolabfold&#34;&gt;Run ColabFold on your local computer&lt;/a&gt; by Yoshitaka Moriwaki&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zephyris/discoba_alphafold&#34;&gt;ColabFold/AlphaFold2 for protein structure predictions for Discoba species&lt;/a&gt; by Richard John Wheeler&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pablo-arantes/Making-it-rain&#34;&gt;Cloud-based molecular simulations for everyone&lt;/a&gt; by Pablo R. Arantes, Marcelo D. Polêto, Conrado Pedebos and Rodrigo Ligabue-Braun&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.getmoonbear.com/AlphaFold2&#34;&gt;getmoonbear is a webserver to predict protein structures&lt;/a&gt; by Stephanie Zhang and Neil Deshmukh&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/normandavey/AlphaFold2-IDR-complex-prediction&#34;&gt;ColabFold/AlphaFold2 IDR complex prediction&lt;/a&gt; by Balint Meszaros&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/phenix-project/Colabs/blob/main/alphafold2/AlphaFold2.ipynb&#34;&gt;ColabFold/AlphaFold2 (Phenix version) for macromolecular structure determination&lt;/a&gt; by Tom Terwilliger&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/mattarnoldbio/alphapickle/blob/main/AlphaPickle.ipynb&#34;&gt;AlphaPickle: making AlphaFold2/ColabFold outputs interpretable&lt;/a&gt; by Matt Arnold&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Acknowledgments&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We would like to thank the &lt;a href=&#34;https://github.com/RosettaCommons/RoseTTAFold&#34;&gt;RoseTTAFold&lt;/a&gt; and &lt;a href=&#34;https://github.com/deepmind/alphafold&#34;&gt;AlphaFold&lt;/a&gt; team for doing an excellent job open sourcing the software.&lt;/li&gt; &#xA; &lt;li&gt;Also credit to &lt;a href=&#34;https://github.com/dkoes&#34;&gt;David Koes&lt;/a&gt; for his awesome &lt;a href=&#34;https://3dmol.csb.pitt.edu/&#34;&gt;py3Dmol&lt;/a&gt; plugin, without whom these notebooks would be quite boring!&lt;/li&gt; &#xA; &lt;li&gt;A colab by Sergey Ovchinnikov (@sokrypton), Milot Mirdita (@milot_mirdita) and Martin Steinegger (@thesteinegger).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How do I reference this work?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mirdita M, Schütze K, Moriwaki Y, Heo L, Ovchinnikov S and Steinegger M. ColabFold: Making protein folding accessible to all. &lt;br&gt; Nature Methods (2022) doi: &lt;a href=&#34;https://www.nature.com/articles/s41592-022-01488-1&#34;&gt;10.1038/s41592-022-01488-1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you’re using &lt;strong&gt;AlphaFold&lt;/strong&gt;, please also cite: &lt;br&gt; Jumper et al. &#34;Highly accurate protein structure prediction with AlphaFold.&#34; &lt;br&gt; Nature (2021) doi: &lt;a href=&#34;https://doi.org/10.1038/s41586-021-03819-2&#34;&gt;10.1038/s41586-021-03819-2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you’re using &lt;strong&gt;AlphaFold-multimer&lt;/strong&gt;, please also cite: &lt;br&gt; Evans et al. &#34;Protein complex prediction with AlphaFold-Multimer.&#34; &lt;br&gt; biorxiv (2021) doi: &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1&#34;&gt;10.1101/2021.10.04.463034v1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you are using &lt;strong&gt;RoseTTAFold&lt;/strong&gt;, please also cite: &lt;br&gt; Minkyung et al. &#34;Accurate prediction of protein structures and interactions using a three-track neural network.&#34; &lt;br&gt; Science (2021) doi: &lt;a href=&#34;https://doi.org/10.1126/science.abj8754&#34;&gt;10.1126/science.abj8754&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://doi.org/10.5281/zenodo.5123296&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/doi/10.5281/zenodo.5123296.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;OLD Updates&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;  11Mar2022: We use in default AlphaFold-multimer-v2 weights for complex modeling. &#xA;             We also offer the old complex modes &#34;AlphaFold-ptm&#34; or &#34;AlphaFold-multimer-v1&#34;&#xA;  04Mar2022: ColabFold now uses a much more powerful server for MSAs and searches through the ColabFoldDB instead of BFD/MGnify. &#xA;             Please let us know if you observe any issues.&#xA;  26Jan2022: AlphaFold2_mmseqs2, AlphaFold2_batch and colabfold_batch&#39;s multimer complexes predictions are &#xA;             now in default reranked by iptmscore*0.8+ptmscore*0.2 instead of ptmscore&#xA;  16Aug2021: WARNING - MMseqs2 API is undergoing upgrade, you may see error messages.&#xA;  17Aug2021: If you see any errors, please report them.&#xA;  17Aug2021: We are still debugging the MSA generation procedure...&#xA;  20Aug2021: WARNING - MMseqs2 API is undergoing upgrade, you may see error messages.&#xA;             To avoid Google Colab from crashing, for large MSA we did -diff 1000 to get &#xA;             1K most diverse sequences. This caused some large MSA to degrade in quality,&#xA;             as sequences close to query were being merged to single representive.&#xA;             We are working on updating the server (today) to fix this, by making sure&#xA;             that both diverse and sequences close to query are included in the final MSA.&#xA;             We&#39;ll post update here when update is complete.&#xA;  21Aug2021  The MSA issues should now be resolved! Please report any errors you see.&#xA;             In short, to reduce MSA size we filter (qsc &amp;gt; 0.8, id &amp;gt; 0.95) and take 3K&#xA;             most diverse sequences at different qid (sequence identity to query) intervals &#xA;             and merge them. More specifically 3K sequences at qid at (0→0.2),(0.2→0.4),&#xA;             (0.4→0.6),(0.6→0.8) and (0.8→1). If you submitted your sequence between&#xA;             16Aug2021 and 20Aug2021, we recommend submitting again for best results!&#xA;  21Aug2021  The use_templates option in AlphaFold2_mmseqs2 is not properly working. We are&#xA;             working on fixing this. If you are not using templates, this does not affect the&#xA;             the results. Other notebooks that do not use_templates are unaffected.&#xA;  21Aug2021  The templates issue is resolved!&#xA;  11Nov2021  [AlphaFold2_mmseqs2] now uses Alphafold-multimer for complex (homo/hetero-oligomer) modeling.&#xA;             Use [AlphaFold2_advanced] notebook for the old complex prediction logic. &#xA;  11Nov2021  ColabFold can be installed locally using pip!&#xA;  14Nov2021  Template based predictions works again in the Alphafold2_mmseqs2 notebook.&#xA;  14Nov2021  WARNING &#34;Single-sequence&#34; mode in AlphaFold2_mmseqs2 and AlphaFold2_batch was broken &#xA;             starting 11Nov2021. The MMseqs2 MSA was being used regardless of selection.&#xA;  14Nov2021  &#34;Single-sequence&#34; mode is now fixed.&#xA;  20Nov2021  WARNING &#34;AMBER&#34; mode in AlphaFold2_mmseqs2 and AlphaFold2_batch was broken &#xA;             starting 11Nov2021. Unrelaxed proteins were returned instead.&#xA;  20Nov2021  &#34;AMBER&#34; is fixed thanks to Kevin Pan&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>rlabbe/Kalman-and-Bayesian-Filters-in-Python</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/rlabbe/Kalman-and-Bayesian-Filters-in-Python</id>
    <link href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kalman Filter book using Jupyter Notebook. Focuses on building intuition and experience, not formal proofs. Includes Kalman filters,extended Kalman filters, unscented Kalman filters, particle filters, and more. All exercises include solutions.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python&#34;&gt;Kalman and Bayesian Filters in Python&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Introductory text for Kalman and Bayesian filters. All code is written in Python, and the book itself is written using Jupyter Notebook so that you can run and modify the code in your browser. What better way to learn?&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&#34;Kalman and Bayesian Filters in Python&#34; looks amazing! ... your book is just what I needed&lt;/strong&gt; - Allen Downey, Professor and O&#39;Reilly author.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Thanks for all your work on publishing your introductory text on Kalman Filtering, as well as the Python Kalman Filtering libraries. We’ve been using it internally to teach some key state estimation concepts to folks and it’s been a huge help.&lt;/strong&gt; - Sam Rodkey, SpaceX&lt;/p&gt; &#xA;&lt;p&gt;Start reading online now by clicking the binder or Azure badge below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master&#34;&gt;&lt;img src=&#34;http://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master/animations/05_dog_track.gif&#34; alt=&#34;alt tag&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What are Kalman and Bayesian Filters?&lt;/h2&gt; &#xA;&lt;p&gt;Sensors are noisy. The world is full of data and events that we want to measure and track, but we cannot rely on sensors to give us perfect information. The GPS in my car reports altitude. Each time I pass the same point in the road it reports a slightly different altitude. My kitchen scale gives me different readings if I weigh the same object twice.&lt;/p&gt; &#xA;&lt;p&gt;In simple cases the solution is obvious. If my scale gives slightly different readings I can just take a few readings and average them. Or I can replace it with a more accurate scale. But what do we do when the sensor is very noisy, or the environment makes data collection difficult? We may be trying to track the movement of a low flying aircraft. We may want to create an autopilot for a drone, or ensure that our farm tractor seeded the entire field. I work on computer vision, and I need to track moving objects in images, and the computer vision algorithms create very noisy and unreliable results.&lt;/p&gt; &#xA;&lt;p&gt;This book teaches you how to solve these sorts of filtering problems. I use many different algorithms, but they are all based on Bayesian probability. In simple terms Bayesian probability determines what is likely to be true based on past information.&lt;/p&gt; &#xA;&lt;p&gt;If I asked you the heading of my car at this moment you would have no idea. You&#39;d prefer a number between 1° and 360° degrees, and have a 1 in 360 chance of being right. Now suppose I told you that 2 seconds ago its heading was 243°. In 2 seconds my car could not turn very far, so you could make a far more accurate prediction. You are using past information to more accurately infer information about the present or future.&lt;/p&gt; &#xA;&lt;p&gt;The world is also noisy. That prediction helps you make a better estimate, but it also subject to noise. I may have just braked for a dog or swerved around a pothole. Strong winds and ice on the road are external influences on the path of my car. In control literature we call this noise though you may not think of it that way.&lt;/p&gt; &#xA;&lt;p&gt;There is more to Bayesian probability, but you have the main idea. Knowledge is uncertain, and we alter our beliefs based on the strength of the evidence. Kalman and Bayesian filters blend our noisy and limited knowledge of how a system behaves with the noisy and limited sensor readings to produce the best possible estimate of the state of the system. Our principle is to never discard information.&lt;/p&gt; &#xA;&lt;p&gt;Say we are tracking an object and a sensor reports that it suddenly changed direction. Did it really turn, or is the data noisy? It depends. If this is a jet fighter we&#39;d be very inclined to believe the report of a sudden maneuver. If it is a freight train on a straight track we would discount it. We&#39;d further modify our belief depending on how accurate the sensor is. Our beliefs depend on the past and on our knowledge of the system we are tracking and on the characteristics of the sensors.&lt;/p&gt; &#xA;&lt;p&gt;The Kalman filter was invented by Rudolf Emil Kálmán to solve this sort of problem in a mathematically optimal way. Its first use was on the Apollo missions to the moon, and since then it has been used in an enormous variety of domains. There are Kalman filters in aircraft, on submarines, and on cruise missiles. Wall street uses them to track the market. They are used in robots, in IoT (Internet of Things) sensors, and in laboratory instruments. Chemical plants use them to control and monitor reactions. They are used to perform medical imaging and to remove noise from cardiac signals. If it involves a sensor and/or time-series data, a Kalman filter or a close relative to the Kalman filter is usually involved.&lt;/p&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;p&gt;The motivation for this book came out of my desire for a gentle introduction to Kalman filtering. I&#39;m a software engineer that spent almost two decades in the avionics field, and so I have always been &#39;bumping elbows&#39; with the Kalman filter, but never implemented one myself. As I moved into solving tracking problems with computer vision the need became urgent. There are classic textbooks in the field, such as Grewal and Andrew&#39;s excellent &lt;em&gt;Kalman Filtering&lt;/em&gt;. But sitting down and trying to read many of these books is a dismal experience if you do not have the required background. Typically the first few chapters fly through several years of undergraduate math, blithely referring you to textbooks on topics such as Itō calculus, and present an entire semester&#39;s worth of statistics in a few brief paragraphs. They are good texts for an upper undergraduate course, and an invaluable reference to researchers and professionals, but the going is truly difficult for the more casual reader. Symbology is introduced without explanation, different texts use different terms and variables for the same concept, and the books are almost devoid of examples or worked problems. I often found myself able to parse the words and comprehend the mathematics of a definition, but had no idea as to what real world phenomena they describe. &#34;But what does that &lt;em&gt;mean?&lt;/em&gt;&#34; was my repeated thought.&lt;/p&gt; &#xA;&lt;p&gt;However, as I began to finally understand the Kalman filter I realized the underlying concepts are quite straightforward. A few simple probability rules, some intuition about how we integrate disparate knowledge to explain events in our everyday life and the core concepts of the Kalman filter are accessible. Kalman filters have a reputation for difficulty, but shorn of much of the formal terminology the beauty of the subject and of their math became clear to me, and I fell in love with the topic.&lt;/p&gt; &#xA;&lt;p&gt;As I began to understand the math and theory more difficulties present themselves. A book or paper&#39;s author makes some statement of fact and presents a graph as proof. Unfortunately, why the statement is true is not clear to me, nor is the method for making that plot obvious. Or maybe I wonder &#34;is this true if R=0?&#34; Or the author provides pseudocode at such a high level that the implementation is not obvious. Some books offer Matlab code, but I do not have a license to that expensive package. Finally, many books end each chapter with many useful exercises. Exercises which you need to understand if you want to implement Kalman filters for yourself, but exercises with no answers. If you are using the book in a classroom, perhaps this is okay, but it is terrible for the independent reader. I loathe that an author withholds information from me, presumably to avoid &#39;cheating&#39; by the student in the classroom.&lt;/p&gt; &#xA;&lt;p&gt;From my point of view none of this is necessary. Certainly if you are designing a Kalman filter for an aircraft or missile you must thoroughly master all of the mathematics and topics in a typical Kalman filter textbook. I just want to track an image on a screen, or write some code for an Arduino project. I want to know how the plots in the book are made, and chose different parameters than the author chose. I want to run simulations. I want to inject more noise in the signal and see how a filter performs. There are thousands of opportunities for using Kalman filters in everyday code, and yet this fairly straightforward topic is the provenance of rocket scientists and academics.&lt;/p&gt; &#xA;&lt;p&gt;I wrote this book to address all of those needs. This is not the book for you if you program navigation computers for Boeing or design radars for Raytheon. Go get an advanced degree at Georgia Tech, UW, or the like, because you&#39;ll need it. This book is for the hobbyist, the curious, and the working engineer that needs to filter or smooth data.&lt;/p&gt; &#xA;&lt;p&gt;This book is interactive. While you can read it online as static content, I urge you to use it as intended. It is written using Jupyter Notebook, which allows me to combine text, math, Python, and Python output in one place. Every plot, every piece of data in this book is generated from Python that is available to you right inside the notebook. Want to double the value of a parameter? Click on the Python cell, change the parameter&#39;s value, and click &#39;Run&#39;. A new plot or printed output will appear in the book.&lt;/p&gt; &#xA;&lt;p&gt;This book has exercises, but it also has the answers. I trust you. If you just need an answer, go ahead and read the answer. If you want to internalize this knowledge, try to implement the exercise before you read the answer.&lt;/p&gt; &#xA;&lt;p&gt;This book has supporting libraries for computing statistics, plotting various things related to filters, and for the various filters that we cover. This does require a strong caveat; most of the code is written for didactic purposes. It is rare that I chose the most efficient solution (which often obscures the intent of the code), and in the first parts of the book I did not concern myself with numerical stability. This is important to understand - Kalman filters in aircraft are carefully designed and implemented to be numerically stable; the naive implementation is not stable in many cases. If you are serious about Kalman filters this book will not be the last book you need. My intention is to introduce you to the concepts and mathematics, and to get you to the point where the textbooks are approachable.&lt;/p&gt; &#xA;&lt;p&gt;Finally, this book is free. The cost for the books required to learn Kalman filtering is somewhat prohibitive even for a Silicon Valley engineer like myself; I cannot believe they are within the reach of someone in a depressed economy, or a financially struggling student. I have gained so much from free software like Python, and free books like those from Allen B. Downey &lt;a href=&#34;http://www.greenteapress.com/&#34;&gt;here&lt;/a&gt;. It&#39;s time to repay that. So, the book is free, it is hosted on free servers, and it uses only free and open software such as IPython and MathJax to create the book.&lt;/p&gt; &#xA;&lt;h2&gt;Reading Online&lt;/h2&gt; &#xA;&lt;p&gt;The book is written as a collection of Jupyter Notebooks, an interactive, browser based system that allows you to combine text, Python, and math into your browser. There are multiple ways to read these online, listed below.&lt;/p&gt; &#xA;&lt;h3&gt;binder&lt;/h3&gt; &#xA;&lt;p&gt;binder serves interactive notebooks online, so you can run the code and change the code within your browser without downloading the book or installing Jupyter.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master&#34;&gt;&lt;img src=&#34;http://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;nbviewer&lt;/h3&gt; &#xA;&lt;p&gt;The website &lt;a href=&#34;http://nbviewer.org&#34;&gt;http://nbviewer.org&lt;/a&gt; provides a Jupyter Notebook server that renders notebooks stored at github (or elsewhere). The rendering is done in real time when you load the book. You may use &lt;a href=&#34;http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb&#34;&gt;&lt;em&gt;this nbviewer link&lt;/em&gt;&lt;/a&gt; to access my book via nbviewer. If you read my book today, and then I make a change tomorrow, when you go back tomorrow you will see that change. Notebooks are rendered statically - you can read them, but not modify or run the code.&lt;/p&gt; &#xA;&lt;p&gt;nbviewer seems to lag the checked in version by a few days, so you might not be reading the most recent content.&lt;/p&gt; &#xA;&lt;h3&gt;GitHub&lt;/h3&gt; &#xA;&lt;p&gt;GitHub is able to render the notebooks directly. The quickest way to view a notebook is to just click on them above. However, it renders the math incorrectly, and I cannot recommend using it if you are doing more than just dipping into the book.&lt;/p&gt; &#xA;&lt;h2&gt;PDF Version&lt;/h2&gt; &#xA;&lt;p&gt;A PDF version of the book is available [here]&lt;a href=&#34;https://drive.google.com/file/d/0By_SW19c1BfhSVFzNHc0SjduNzg/view?usp=sharing&amp;amp;resourcekey=0-41olC9ht9xE3wQe2zHZ45A&#34;&gt;https://drive.google.com/file/d/0By_SW19c1BfhSVFzNHc0SjduNzg/view?usp=sharing&amp;amp;resourcekey=0-41olC9ht9xE3wQe2zHZ45A&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;The PDF will usually lag behind what is in github as I don&#39;t update it for every minor check in.&lt;/p&gt; &#xA;&lt;h2&gt;Downloading and Running the Book&lt;/h2&gt; &#xA;&lt;p&gt;However, this book is intended to be interactive and I recommend using it in that form. It&#39;s a little more effort to set up, but worth it. If you install IPython and some supporting libraries on your computer and then clone this book you will be able to run all of the code in the book yourself. You can perform experiments, see how filters react to different data, see how different filters react to the same data, and so on. I find this sort of immediate feedback both vital and invigorating. You do not have to wonder &#34;what happens if&#34;. Try it and see!&lt;/p&gt; &#xA;&lt;p&gt;The book and supporting software can be downloaded from GitHub by running this command on the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --depth=1 https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python.git&#xA;pip install filterpy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instructions for installation of the IPython ecosystem can be found in the Installation appendix, found &lt;a href=&#34;http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/Appendix-A-Installation.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once the software is installed you can navigate to the installation directory and run Jupyter notebook with the command line instruction&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will open a browser window showing the contents of the base directory. The book is organized into chapters, each contained within one IPython Notebook (these notebook files have a .ipynb file extension). For example, to read Chapter 2, click on the file &lt;em&gt;02-Discrete-Bayes.ipynb&lt;/em&gt;. Sometimes there are supporting notebooks for doing things like generating animations that are displayed in the chapter. These are not intended to be read by the end user, but of course if you are curious as to how an animation is made go ahead and take a look. You can find these notebooks in the folder named &lt;em&gt;Supporting_Notebooks&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This is admittedly a somewhat cumbersome interface to a book; I am following in the footsteps of several other projects that are somewhat repurposing Jupyter Notebook to generate entire books. I feel the slight annoyances have a huge payoff - instead of having to download a separate code base and run it in an IDE while you try to read a book, all of the code and text is in one place. If you want to alter the code, you may do so and immediately see the effects of your change. If you find a bug, you can make a fix, and push it back to my repository so that everyone in the world benefits. And, of course, you will never encounter a problem I face all the time with traditional books - the book and the code are out of sync with each other, and you are left scratching your head as to which source to trust.&lt;/p&gt; &#xA;&lt;h2&gt;Companion Software&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://pypi.python.org/pypi/filterpy&#34;&gt;&lt;img src=&#34;http://img.shields.io/pypi/v/filterpy.svg?sanitize=true&#34; alt=&#34;Latest Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;I wrote an open source Bayesian filtering Python library called &lt;strong&gt;FilterPy&lt;/strong&gt;. I have made the project available on PyPi, the Python Package Index. To install from PyPi, at the command line issue the command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install filterpy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you do not have pip, you may follow the instructions here: &lt;a href=&#34;https://pip.pypa.io/en/latest/installing.html&#34;&gt;https://pip.pypa.io/en/latest/installing.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All of the filters used in this book as well as others not in this book are implemented in my Python library FilterPy, available &lt;a href=&#34;https://github.com/rlabbe/filterpy&#34;&gt;here&lt;/a&gt;. You do not need to download or install this to read the book, but you will likely want to use this library to write your own filters. It includes Kalman filters, Fading Memory filters, H infinity filters, Extended and Unscented filters, least square filters, and many more. It also includes helper routines that simplify the designing the matrices used by some of the filters, and other code such as Kalman based smoothers.&lt;/p&gt; &#xA;&lt;p&gt;FilterPy is hosted on github at (&lt;a href=&#34;https://github.com/rlabbe/filterpy&#34;&gt;https://github.com/rlabbe/filterpy&lt;/a&gt;). If you want the bleeding edge release you will want to grab a copy from github, and follow your Python installation&#39;s instructions for adding it to the Python search path. This might expose you to some instability since you might not get a tested release, but as a benefit you will also get all of the test scripts used to test the library. You can examine these scripts to see many examples of writing and running filters while not in the Jupyter Notebook environment.&lt;/p&gt; &#xA;&lt;h2&gt;Alternative Way of Running the Book in Conda environment&lt;/h2&gt; &#xA;&lt;p&gt;If you have conda or miniconda installed, you can create an environment by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env update -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate kf_bf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda deactivate kf_bf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to activate and deactivate the environment.&lt;/p&gt; &#xA;&lt;h2&gt;Issues or Questions&lt;/h2&gt; &#xA;&lt;p&gt;If you have comments, you can write an issue at GitHub so that everyone can read it along with my response. Please don&#39;t view it as a way to report bugs only. Alternatively I&#39;ve created a gitter room for more informal discussion. &lt;a href=&#34;https://gitter.im/rlabbe/Kalman-and-Bayesian-Filters-in-Python?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/rlabbe/Kalman-and-Bayesian-Filters-in-Python&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img alt=&#34;Creative Commons License&#34; style=&#34;border-width:0&#34; src=&#34;https://i.creativecommons.org/l/by/4.0/88x31.png&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;span xmlns:dct=&#34;http://purl.org/dc/terms/&#34; property=&#34;dct:title&#34;&gt;Kalman and Bayesian Filters in Python&lt;/span&gt; by &lt;a xmlns:cc=&#34;http://creativecommons.org/ns#&#34; href=&#34;https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python&#34; property=&#34;cc:attributionName&#34; rel=&#34;cc:attributionURL&#34;&gt;Roger R. Labbe&lt;/a&gt; is licensed under a &lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All software in this book, software that supports this book (such as in the the code directory) or used in the generation of the book (in the pdf directory) that is contained in this repository is licensed under the following MIT license:&lt;/p&gt; &#xA;&lt;p&gt;The MIT License (MIT)&lt;/p&gt; &#xA;&lt;p&gt;Copyright (c) 2015 Roger R. Labbe Jr&lt;/p&gt; &#xA;&lt;p&gt;Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &#34;Software&#34;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:&lt;/p&gt; &#xA;&lt;p&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/p&gt; &#xA;&lt;p&gt;THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.TION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;rlabbejr at gmail.com&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dsacademybr/PythonFundamentos</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/dsacademybr/PythonFundamentos</id>
    <link href="https://github.com/dsacademybr/PythonFundamentos" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repositório do Curso Online Python Fundamentos Para Análise de Dados.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python Fundamentos Para Análise de Dados 3.0&lt;/h1&gt; &#xA;&lt;p&gt;Data Science Academy - Repositório dos Arquivos do Curso Gratuito Python Fundamentos Para Análise de Dados, Versão 3.0.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.datascienceacademy.com.br&#34;&gt;https://www.datascienceacademy.com.br&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lujiazho/MachineLearningPlayground</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/lujiazho/MachineLearningPlayground</id>
    <link href="https://github.com/lujiazho/MachineLearningPlayground" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of basic mathematical pattern recognition/machine learning techniques for fun&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MachineLearningPlayground&lt;/h1&gt; &#xA;&lt;p&gt;Implementation of basic mathematical pattern recognition/machine learning techniques for fun&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download with pip&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Prompt&#34;&gt;pip install MLplayground&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download with git&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Prompt&#34;&gt;git clone https://github.com/lujiazho/MachineLearningPlayground.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tutorials&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;⭐&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/SVC.ipynb&#34;&gt;Support Vector Classifier (SVC)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;😃&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/SVR.ipynb&#34;&gt;Support Vector Regressor (SVR)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;🔢&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/Ridge.ipynb&#34;&gt;Ridge Regression&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;🎓&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/NearestMean.ipynb&#34;&gt;Nearest Mean&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;📕&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/KMeans.ipynb&#34;&gt;K-Means&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;📗&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/KNN.ipynb&#34;&gt;K-Nearest Neighbors (KNN)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;👀&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/Perceptron.ipynb&#34;&gt;Perceptron Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;📷&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/MSE.ipynb&#34;&gt;MSE techniques (classification&amp;amp;Regression)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;🀄&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/DenEstimate_NP.ipynb&#34;&gt;Density Estimation (Non-parametric)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;👥&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/DenEstimate_P.ipynb&#34;&gt;Density Estimation (parametric)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;📊&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/ANN.ipynb&#34;&gt;ANN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;🐍&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/PCA.ipynb&#34;&gt;PCA&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Math Derivation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;⭐&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Math_Derivation/SVC.pdf&#34;&gt;Support Vector Classifier (SVC)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;😃&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Math_Derivation/SVR.pdf&#34;&gt;Support Vector Regressor (SVR)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;🔢&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Math_Derivation/Ridge_Regression.pdf&#34;&gt;Ridge Regression&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;🎓&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Math_Derivation/K-means_n_Nearest-means.pdf&#34;&gt;Nearest Mean &amp;amp; K-Means&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;📗&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Math_Derivation/KNN.pdf&#34;&gt;K-Nearest Neighbors (KNN)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;👀&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Math_Derivation/Perceptron_Learning_n_Gradient_Descent.pdf&#34;&gt;Perceptron Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;📷&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Math_Derivation/MSE_techniques.pdf&#34;&gt;MSE techniques (classification&amp;amp;Regression)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;🀄&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Math_Derivation/Density_Estimation.pdf&#34;&gt;Density Estimation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;📊&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Math_Derivation/ANN.pdf&#34;&gt;ANN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;🐍&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Math_Derivation/Feature_Reduction.pdf&#34;&gt;PCA&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;ML Playground&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;🏄&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/_Project_1_digit_recognizer.ipynb&#34;&gt;Digit Recognizer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;🎆&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/_Project_2_auto_encoder.ipynb&#34;&gt;Auto Encoder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;span&gt;✏&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lujiazho/MachineLearningPlayground/main/Tutorials/_Project_3_NNLanguageModel.ipynb&#34;&gt;Neural Network Language Model (NNLM)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;span&gt;💡&lt;/span&gt; &lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;span&gt;📣&lt;/span&gt; &lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>salesforce/BLIP</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/salesforce/BLIP</id>
    <link href="https://github.com/salesforce/BLIP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/salesforce/BLIP/main/BLIP.gif&#34; width=&#34;700&#34;&gt; &#xA;&lt;p&gt;This is the PyTorch code of the &lt;a href=&#34;https://arxiv.org/abs/2201.12086&#34;&gt;BLIP paper&lt;/a&gt; [&lt;a href=&#34;https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/&#34;&gt;blog&lt;/a&gt;]. The code has been tested on PyTorch 1.10. To install the dependencies, run &lt;/p&gt;&#xA;&lt;pre&gt;&lt;/pre&gt;pip install -r requirements.txt&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;Catalog:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pre-trained and finetuned checkpoints&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Finetuning code for Image-Text Retrieval, Image Captioning, VQA, and NLVR2&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pre-training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Zero-shot video-text retrieval&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Download of bootstrapped pre-training datasets&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Inference demo:&lt;/h3&gt; &#xA;&lt;p&gt;Run our interactive demo using &lt;a href=&#34;https://colab.research.google.com/github/salesforce/BLIP/blob/main/demo.ipynb&#34;&gt;Colab notebook&lt;/a&gt; (no GPU needed). The demo includes code for:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Image captioning&lt;/li&gt; &#xA; &lt;li&gt;Open-ended visual question answering&lt;/li&gt; &#xA; &lt;li&gt;Multimodal / unimodal feature extraction&lt;/li&gt; &#xA; &lt;li&gt;Image-text matching&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Try out the &lt;a href=&#34;https://huggingface.co/spaces/Salesforce/BLIP&#34;&gt;Web demo&lt;/a&gt;, integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces 🤗&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Replicate web demo and Docker image is also available at &lt;a href=&#34;https://replicate.com/salesforce/blip&#34;&gt;&lt;img src=&#34;https://replicate.com/salesforce/blip/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Pre-trained checkpoints:&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Num. pre-train images&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BLIP w/ ViT-B&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BLIP w/ ViT-B and CapFilt-L&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BLIP w/ ViT-L&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_14M.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;129M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Finetuned checkpoints:&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BLIP w/ ViT-B&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BLIP w/ ViT-B and CapFilt-L&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BLIP w/ ViT-L&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Image-Text Retrieval (COCO)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_coco.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Image-Text Retrieval (Flickr30k)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_flickr.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_flickr.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Image Captioning (COCO)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VQA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_vqa.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NLVR2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_nlvr.pth&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Image-Text Retrieval:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download COCO and Flickr30k datasets from the original websites, and set &#39;image_root&#39; in configs/retrieval_{dataset}.yaml accordingly.&lt;/li&gt; &#xA; &lt;li&gt;To evaluate the finetuned BLIP model on COCO, run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 train_retrieval.py \&#xA;--config ./configs/retrieval_coco.yaml \&#xA;--output_dir output/retrieval_coco \&#xA;--evaluate&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;To finetune the pre-trained checkpoint using 8 A100 GPUs, first set &#39;pretrained&#39; in configs/retrieval_coco.yaml as &#34;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth&#34;&gt;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth&lt;/a&gt;&#34;. Then run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 train_retrieval.py \&#xA;--config ./configs/retrieval_coco.yaml \&#xA;--output_dir output/retrieval_coco &lt;/pre&gt; &#xA;&lt;h3&gt;Image-Text Captioning:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download COCO and NoCaps datasets from the original websites, and set &#39;image_root&#39; in configs/caption_coco.yaml and configs/nocaps.yaml accordingly.&lt;/li&gt; &#xA; &lt;li&gt;To evaluate the finetuned BLIP model on COCO, run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 train_caption.py --evaluate&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;To evaluate the finetuned BLIP model on NoCaps, generate results with: (evaluation needs to be performed on official server)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 eval_nocaps.py &lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;To finetune the pre-trained checkpoint using 8 A100 GPUs, first set &#39;pretrained&#39; in configs/caption_coco.yaml as &#34;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth&#34;&gt;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth&lt;/a&gt;&#34;. Then run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 train_caption.py &lt;/pre&gt; &#xA;&lt;h3&gt;VQA:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download VQA v2 dataset and Visual Genome dataset from the original websites, and set &#39;vqa_root&#39; and &#39;vg_root&#39; in configs/vqa.yaml.&lt;/li&gt; &#xA; &lt;li&gt;To evaluate the finetuned BLIP model, generate results with: (evaluation needs to be performed on official server)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 train_vqa.py --evaluate&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;To finetune the pre-trained checkpoint using 16 A100 GPUs, first set &#39;pretrained&#39; in configs/vqa.yaml as &#34;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth&#34;&gt;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth&lt;/a&gt;&#34;. Then run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=16 train_vqa.py &lt;/pre&gt; &#xA;&lt;h3&gt;NLVR2:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download NLVR2 dataset from the original websites, and set &#39;image_root&#39; in configs/nlvr.yaml.&lt;/li&gt; &#xA; &lt;li&gt;To evaluate the finetuned BLIP model, run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 train_nlvr.py --evaluate&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;To finetune the pre-trained checkpoint using 16 A100 GPUs, first set &#39;pretrained&#39; in configs/nlvr.yaml as &#34;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth&#34;&gt;https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth&lt;/a&gt;&#34;. Then run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=16 train_nlvr.py &lt;/pre&gt; &#xA;&lt;h3&gt;Finetune with ViT-L:&lt;/h3&gt; &#xA;&lt;p&gt;In order to finetune a model with ViT-L, simply change the config file to set &#39;vit&#39; as large. Batch size and learning rate may also need to be adjusted accordingly (please see the paper&#39;s appendix for hyper-parameter details). &lt;a href=&#34;https://github.com/facebookresearch/fairscale&#34;&gt;Gradient checkpoint&lt;/a&gt; can also be activated in the config file to reduce GPU memory usage.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-train:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prepare training json files where each json file contains a list. Each item in the list is a dictonary with two key-value pairs: {&#39;image&#39;: path_of_image, &#39;caption&#39;: text_of_image}.&lt;/li&gt; &#xA; &lt;li&gt;In configs/pretrain.yaml, set &#39;train_file&#39; as the paths for the json files .&lt;/li&gt; &#xA; &lt;li&gt;Pre-train the model using 8 A100 GPUs:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 pretrain.py --config ./configs/Pretrain.yaml --output_dir output/Pretrain &lt;/pre&gt; &#xA;&lt;h3&gt;Zero-shot video-text retrieval:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download MSRVTT dataset following the instructions from &lt;a href=&#34;https://github.com/salesforce/ALPRO&#34;&gt;https://github.com/salesforce/ALPRO&lt;/a&gt;, and set &#39;video_root&#39; accordingly in configs/retrieval_msrvtt.yaml.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://github.com/dmlc/decord&#34;&gt;decord&lt;/a&gt; with &lt;pre&gt;pip install decord&lt;/pre&gt;&lt;/li&gt; &#xA; &lt;li&gt;To perform zero-shot evaluation, run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;python -m torch.distributed.run --nproc_per_node=8 eval_retrieval_video.py&lt;/pre&gt; &#xA;&lt;h3&gt;Pre-training datasets download:&lt;/h3&gt; &#xA;&lt;p&gt;We provide bootstrapped pre-training datasets as json files. Each json file contains a list. Each item in the list is a dictonary with two key-value pairs: {&#39;url&#39;: url_of_image, &#39;caption&#39;: text_of_image}.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Image source&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Filtered web caption&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Filtered synthetic caption&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Filtered synthetic caption by ViT-L&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CC3M+CC12M+SBU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_filtered.json&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_synthetic_filtered.json&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_synthetic_filtered_large.json&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LAION115M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_filtered.json&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_synthetic_filtered.json&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_synthetic_filtered_large.json&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;If you find this code to be useful for your research, please consider citing.&lt;/p&gt; &#xA;&lt;pre&gt;&#xA;@inproceedings{li2022blip,&#xA;      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, &#xA;      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},&#xA;      year={2022},&#xA;      booktitle={ICML},&#xA;}&lt;/pre&gt; &#xA;&lt;h3&gt;Acknowledgement&lt;/h3&gt; &#xA;&lt;p&gt;The implementation of BLIP relies on resources from &lt;a href=&#34;https://github.com/salesforce/ALBEF&#34;&gt;ALBEF&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Huggingface Transformers&lt;/a&gt;, and &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models/tree/master/timm&#34;&gt;timm&lt;/a&gt;. We thank the original authors for their open-sourcing.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/ML-For-Beginners</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/microsoft/ML-For-Beginners</id>
    <link href="https://github.com/microsoft/ML-For-Beginners" rel="alternate"></link>
    <summary type="html">&lt;p&gt;12 weeks, 26 lessons, 52 quizzes, classic Machine Learning for all&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/ML-For-Beginners/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/issues/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/pulls/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/watchers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&amp;amp;label=Watch&#34; alt=&#34;GitHub watchers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&amp;amp;label=Fork&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/ML-For-Beginners/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&amp;amp;label=Star&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Machine Learning for Beginners - A Curriculum&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about &lt;strong&gt;Machine Learning&lt;/strong&gt;. In this curriculum, you will learn about what is sometimes called &lt;strong&gt;classic machine learning&lt;/strong&gt;, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming &#39;AI for Beginners&#39; curriculum. Pair these lessons with our &lt;a href=&#34;https://aka.ms/datascience-beginners&#34;&gt;&#39;Data Science for Beginners&#39; curriculum&lt;/a&gt;, as well!&lt;/p&gt; &#xA;&lt;p&gt;Travel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to &#39;stick&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;✍️ Hearty thanks to our authors&lt;/strong&gt; Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, and Amy Boyd&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🎨 Thanks as well to our illustrators&lt;/strong&gt; Tomomi Imura, Dasani Madipalli, and Jen Looper&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🙏 Special thanks 🙏 to our Microsoft Student Ambassador authors, reviewers, and content contributors&lt;/strong&gt;, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🤩 Extra gratitude to Microsoft Student Ambassador Eric Wanjau for our R lessons!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://aka.ms/student-page&#34;&gt;Students&lt;/a&gt;&lt;/strong&gt;, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start with a pre-lecture quiz.&lt;/li&gt; &#xA; &lt;li&gt;Read the lecture and complete the activities, pausing and reflecting at each knowledge check.&lt;/li&gt; &#xA; &lt;li&gt;Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the &lt;code&gt;/solution&lt;/code&gt; folders in each project-oriented lesson.&lt;/li&gt; &#xA; &lt;li&gt;Take the post-lecture quiz.&lt;/li&gt; &#xA; &lt;li&gt;Complete the challenge.&lt;/li&gt; &#xA; &lt;li&gt;Complete the assignment.&lt;/li&gt; &#xA; &lt;li&gt;After completing a lesson group, visit the &lt;a href=&#34;https://github.com/microsoft/ML-For-Beginners/discussions&#34;&gt;Discussion Board&lt;/a&gt; and &#34;learn out loud&#34; by filling out the appropriate PAT rubric. A &#39;PAT&#39; is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For further study, we recommend following these &lt;a href=&#34;https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa&#34;&gt;Microsoft Learn&lt;/a&gt; modules and learning paths.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;, we have &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/for-teachers.md&#34;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Meet the Team&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/Tj1XWrDSYJU&#34; title=&#34;Promo video&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/ml.gif&#34; alt=&#34;Promo video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gif by&lt;/strong&gt; &lt;a href=&#34;https://linkedin.com/in/mohitjaisal&#34;&gt;Mohit Jaisal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🎥 Click the image above for a video about the project and the folks who created it!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Pedagogy&lt;/h2&gt; &#xA;&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is hands-on &lt;strong&gt;project-based&lt;/strong&gt; and that it includes &lt;strong&gt;frequent quizzes&lt;/strong&gt;. In addition, this curriculum has a common &lt;strong&gt;theme&lt;/strong&gt; to give it cohesion.&lt;/p&gt; &#xA;&lt;p&gt;By ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12-week cycle. This curriculum also includes a postscript on real-world applications of ML, which can be used as extra credit or as a basis for discussion.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Find our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/TRANSLATIONS.md&#34;&gt;Translation&lt;/a&gt; guidelines. We welcome your constructive feedback!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Each lesson includes:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;optional sketchnote&lt;/li&gt; &#xA; &lt;li&gt;optional supplemental video&lt;/li&gt; &#xA; &lt;li&gt;pre-lecture warmup quiz&lt;/li&gt; &#xA; &lt;li&gt;written lesson&lt;/li&gt; &#xA; &lt;li&gt;for project-based lessons, step-by-step guides on how to build the project&lt;/li&gt; &#xA; &lt;li&gt;knowledge checks&lt;/li&gt; &#xA; &lt;li&gt;a challenge&lt;/li&gt; &#xA; &lt;li&gt;supplemental reading&lt;/li&gt; &#xA; &lt;li&gt;assignment&lt;/li&gt; &#xA; &lt;li&gt;post-lecture quiz&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about languages&lt;/strong&gt;: These lessons are primarily written in Python, but many are also available in R. To complete an R lesson, go to the &lt;code&gt;/solution&lt;/code&gt; folder and look for R lessons. They include an .rmd extension that represents an &lt;strong&gt;R Markdown&lt;/strong&gt; file which can be simply defined as an embedding of &lt;code&gt;code chunks&lt;/code&gt; (of R or other languages) and a &lt;code&gt;YAML header&lt;/code&gt; (that guides how to format outputs such as PDF) in a &lt;code&gt;Markdown document&lt;/code&gt;. As such, it serves as an exemplary authoring framework for data science since it allows you to combine your code, its output, and your thoughts by allowing you to write them down in Markdown. Moreover, R Markdown documents can be rendered to output formats such as PDF, HTML, or Word.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained &lt;a href=&#34;https://white-water-09ec41f0f.azurestaticapps.net/&#34;&gt;in this app&lt;/a&gt;, for 52 total quizzes of three questions each. They are linked from within the lessons but the quiz app can be run locally; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Number&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Topic&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Grouping&lt;/th&gt; &#xA;   &lt;th&gt;Learning Objectives&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Linked Lesson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Author&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn the basic concepts behind machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/1-intro-to-ML/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Muhammad&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The History of machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn the history underlying this field&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/2-history-of-ML/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Jen and Amy&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Fairness and machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;What are the important philosophical issues around fairness that students should consider when building and applying ML models?&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/3-fairness/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Tomomi&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Techniques for machine learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;What techniques do ML researchers use to build ML models?&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/1-Introduction/4-techniques-of-ML/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chris and Jen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to regression&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Get started with Python and Scikit-learn for regression models&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/1-Tools/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/1-Tools/solution/R/lesson_1-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;North American pumpkin prices 🎃&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Visualize and clean data in preparation for ML&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/2-Data/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/2-Data/solution/R/lesson_2-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;North American pumpkin prices 🎃&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build linear and polynomial regression models&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/3-Linear/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/3-Linear/solution/R/lesson_3-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Dmitry&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;North American pumpkin prices 🎃&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/README.md&#34;&gt;Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build a logistic regression model&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/4-Logistic/README.md&#34;&gt;Python&lt;/a&gt; &lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/2-Regression/4-Logistic/solution/R/lesson_4-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A Web App 🔌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/3-Web-App/README.md&#34;&gt;Web App&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build a web app to use your trained model&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/3-Web-App/1-Web-App/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Jen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to classification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Clean, prep, and visualize your data; introduction to classification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/1-Introduction/README.md&#34;&gt;Python&lt;/a&gt; &lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/1-Introduction/solution/R/lesson_10-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Cassie&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Delicious Asian and Indian cuisines 🍜&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduction to classifiers&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/2-Classifiers-1/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/2-Classifiers-1/solution/R/lesson_11-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Cassie&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Delicious Asian and Indian cuisines 🍜&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;More classifiers&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/3-Classifiers-2/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen and Cassie&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Delicious Asian and Indian cuisines 🍜&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/README.md&#34;&gt;Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build a recommender web app using your model&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/4-Applied/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Jen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to clustering&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/README.md&#34;&gt;Clustering&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Clean, prep, and visualize your data; Introduction to clustering&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/1-Visualize/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Exploring Nigerian Musical Tastes 🎧&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/README.md&#34;&gt;Clustering&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Explore the K-Means clustering method&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/2-K-Means/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb&#34;&gt;R&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Jen&lt;/li&gt;&#xA;     &lt;li&gt;Eric Wanjau&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to natural language processing ☕️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn the basics about NLP by building a simple bot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/1-Introduction-to-NLP/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Common NLP Tasks ☕️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deepen your NLP knowledge by understanding common tasks required when dealing with language structures&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/2-Tasks/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Translation and sentiment analysis ♥️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Translation and sentiment analysis with Jane Austen&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/3-Translation-Sentiment/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Romantic hotels of Europe ♥️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sentiment analysis with hotel reviews 1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/4-Hotel-Reviews-1/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Romantic hotels of Europe ♥️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/README.md&#34;&gt;Natural language processing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sentiment analysis with hotel reviews 2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/6-NLP/5-Hotel-Reviews-2/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Stephen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to time series forecasting&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/README.md&#34;&gt;Time series&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduction to time series forecasting&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/1-Introduction/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Francesca&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;⚡️ World Power Usage ⚡️ - time series forecasting with ARIMA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/README.md&#34;&gt;Time series&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Time series forecasting with ARIMA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/2-ARIMA/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Francesca&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;⚡️ World Power Usage ⚡️ - time series forecasting with SVR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/README.md&#34;&gt;Time series&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Time series forecasting with Support Vector Regressor&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/7-TimeSeries/3-SVR/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Anirban&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to reinforcement learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/README.md&#34;&gt;Reinforcement learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduction to reinforcement learning with Q-Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/1-QLearning/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dmitry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Help Peter avoid the wolf! 🐺&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/README.md&#34;&gt;Reinforcement learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Reinforcement learning Gym&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/8-Reinforcement/2-Gym/README.md&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dmitry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Postscript&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Real-World ML scenarios and applications&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/README.md&#34;&gt;ML in the Wild&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Interesting and revealing real-world applications of classical ML&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/9-Real-World/1-Applications/README.md&#34;&gt;Lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Team&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Offline access&lt;/h2&gt; &#xA;&lt;p&gt;You can run this documentation offline by using &lt;a href=&#34;https://docsify.js.org/#/&#34;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&#34;https://docsify.js.org/#/quickstart&#34;&gt;install Docsify&lt;/a&gt; on your local machine, and then in the root folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;PDFs&lt;/h2&gt; &#xA;&lt;p&gt;Find a pdf of the curriculum with links &lt;a href=&#34;https://microsoft.github.io/ML-For-Beginners/pdf/readme.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Help Wanted!&lt;/h2&gt; &#xA;&lt;p&gt;Would you like to contribute a translation? Please read our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/TRANSLATIONS.md&#34;&gt;translation guidelines&lt;/a&gt; and add a templated issue to manage the workload &lt;a href=&#34;https://github.com/microsoft/ML-For-Beginners/issues&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Other Curricula&lt;/h2&gt; &#xA;&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/webdev-beginners&#34;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/iot-beginners&#34;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/datascience-beginners&#34;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-beginners&#34;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>nlp-with-transformers/notebooks</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/nlp-with-transformers/notebooks</id>
    <link href="https://github.com/nlp-with-transformers/notebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Jupyter notebooks for the Natural Language Processing with Transformers book&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Transformers Notebooks&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the example code from our O&#39;Reilly book &lt;a href=&#34;https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/&#34;&gt;Natural Language Processing with Transformers&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;img alt=&#34;book-cover&#34; height=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/nlp-with-transformers/notebooks/main/images/book_cover.jpg&#34; id=&#34;book-cover&#34;&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;You can run these notebooks on cloud platforms like &lt;a href=&#34;https://colab.research.google.com/&#34;&gt;Google Colab&lt;/a&gt; or your local machine. Note that most chapters require a GPU to run in a reasonable amount of time, so we recommend one of the cloud platforms as they come pre-installed with CUDA.&lt;/p&gt; &#xA;&lt;h3&gt;Running on a cloud platform&lt;/h3&gt; &#xA;&lt;p&gt;To run these notebooks on a cloud platform, just click on one of the badges in the table below:&lt;/p&gt; &#xA;&lt;!--This table is automatically generated, do not fill manually!--&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Chapter&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Colab&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Kaggle&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Gradient&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Studio Lab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Introduction&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/raw/main/01_introduction.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/01_introduction.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Text Classification&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/raw/main/02_classification.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Transformer Anatomy&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/raw/main/03_transformer-anatomy.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Multilingual Named Entity Recognition&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/raw/main/04_multilingual-ner.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/04_multilingual-ner.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Text Generation&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/raw/main/05_text-generation.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Summarization&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/raw/main/06_summarization.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/06_summarization.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Question Answering&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/raw/main/07_question-answering.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/07_question-answering.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Making Transformers Efficient in Production&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/raw/main/08_model-compression.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/08_model-compression.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Dealing with Few to No Labels&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/raw/main/09_few-to-no-labels.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/09_few-to-no-labels.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Training Transformers from Scratch&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/raw/main/10_transformers-from-scratch.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/10_transformers-from-scratch.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Future Directions&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/nlp-with-transformers/notebooks/raw/main/11_future-directions.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/nlp-with-transformers/notebooks/blob/main/11_future-directions.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!--End of table--&gt; &#xA;&lt;p&gt;Nowadays, the GPUs on Colab tend to be K80s (which have limited memory), so we recommend using &lt;a href=&#34;https://www.kaggle.com/docs/notebooks&#34;&gt;Kaggle&lt;/a&gt;, &lt;a href=&#34;https://gradient.run/notebooks&#34;&gt;Gradient&lt;/a&gt;, or &lt;a href=&#34;https://studiolab.sagemaker.aws/&#34;&gt;SageMaker Studio Lab&lt;/a&gt;. These platforms tend to provide more performant GPUs like P100s, all for free!&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: some cloud platforms like Kaggle require you to restart the notebook after installing new packages.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Running on your machine&lt;/h3&gt; &#xA;&lt;p&gt;To run the notebooks on your own machine, first clone the repository and navigate to it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/nlp-with-transformers/notebooks.git&#xA;$ cd notebooks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, run the following command to create a &lt;code&gt;conda&lt;/code&gt; virtual environment that contains all the libraries needed to run the notebooks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: You&#39;ll need a GPU that supports NVIDIA&#39;s &lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA Toolkit&lt;/a&gt; to build the environment. Currently, this means you cannot build locally on Apple silicon 😢.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Chapter 7 (Question Answering) has a special set of dependencies, so to run that chapter you&#39;ll need a separate environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda env create -f environment-chapter7.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once you&#39;ve installed the dependencies, you can activate the &lt;code&gt;conda&lt;/code&gt; environment and spin up the notebooks as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda activate book # or conda activate book-chapter7&#xA;$ jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;When trying to clone the notebooks on Kaggle I get a message that I am unable to access the book&#39;s Github repository. How can I solve this issue?&lt;/h3&gt; &#xA;&lt;p&gt;This issue is likely due to a missing internet connection. When running your first notebook on Kaggle you need to enable internet access in the settings menu on the right side.&lt;/p&gt; &#xA;&lt;h3&gt;How do you select a GPU on Kaggle?&lt;/h3&gt; &#xA;&lt;p&gt;You can enable GPU usage by selecting &lt;em&gt;GPU&lt;/em&gt; as &lt;em&gt;Accelerator&lt;/em&gt; in the settings menu on the right side.&lt;/p&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to cite this book, you can use the following BibTeX entry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@book{tunstall2022natural,&#xA;  title={Natural Language Processing with Transformers: Building Language Applications with Hugging Face},&#xA;  author={Tunstall, Lewis and von Werra, Leandro and Wolf, Thomas},&#xA;  isbn={1098103246},&#xA;  url={https://books.google.ch/books?id=7hhyzgEACAAJ},&#xA;  year={2022},&#xA;  publisher={O&#39;Reilly Media, Incorporated}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>gedeck/practical-statistics-for-data-scientists</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/gedeck/practical-statistics-for-data-scientists</id>
    <link href="https://github.com/gedeck/practical-statistics-for-data-scientists" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code repository for O&#39;Reilly book&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/gedeck/dmba/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;Python&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Code repository&lt;/h1&gt; &#xA;&lt;table width=&#34;100%&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-english.jpg&#34; width=&#34;300&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Practical Statistics for Data Scientists:&lt;/b&gt; &lt;p&gt;50+ Essential Concepts Using R and Python&lt;/p&gt; &lt;p&gt;by Peter Bruce, Andrew Bruce, and &lt;a href=&#34;https://www.amazon.com/Peter-Gedeck/e/B082BJZJKX/&#34;&gt;Peter Gedeck&lt;/a&gt;&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Publisher: &lt;a href=&#34;https://oreil.ly/practicalStats_dataSci_2e&#34;&gt;O&#39;Reilly Media&lt;/a&gt;; 2 edition (June 9, 2020)&lt;/li&gt; &#xA;     &lt;li&gt;ISBN-13: 978-1492072942&lt;/li&gt; &#xA;     &lt;li&gt;Buy on &lt;a href=&#34;https://www.amazon.com/Practical-Statistics-Data-Scientists-Essential/dp/149207294X&#34;&gt;Amazon&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Errata: &lt;a href=&#34;http://oreilly.com/catalog/errata.csp?isbn=9781492072942&#34;&gt;http://oreilly.com/catalog/errata.csp?isbn=9781492072942&lt;/a&gt; &lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Online&lt;/h2&gt; &#xA;&lt;p&gt;View the notebooks online: &lt;a href=&#34;https://nbviewer.jupyter.org/github/gedeck/practical-statistics-for-data-scientists/tree/master/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg?sanitize=true&#34; alt=&#34;nbviewer&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Excecute the notebooks in Binder: &lt;a href=&#34;https://mybinder.org/v2/gh/gedeck/practical-statistics-for-data-scientists/HEAD&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This can take some time if the binder environment needs to be rebuilt.&lt;/p&gt; &#xA;&lt;h2&gt;Other language versions&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-english.jpg&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;English:&lt;/b&gt;&lt;br&gt; Practical Statistics for Data Scientists: 50+ Essential Concepts Using R and Python&lt;br&gt; 2020: ISBN 149207294X&lt;br&gt; &lt;a href=&#34;https://www.google.com/books/edition/Practical_Statistics_for_Data_Scientists/F2bcDwAAQBAJ?hl=en&#34;&gt;Google books&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.com/Practical-Statistics-Data-Scientists-Essential/dp/149207294X&#34;&gt;Amazon&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-japanese.jpg&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Japanese (2020-06-11):&lt;/b&gt;&lt;br&gt; データサイエンスのための統計学入門 第2版 ―予測、分類、統計モデリング、統計的機械学習とR/Pythonプログラミング &lt;br&gt; 2020: ISBN 978-4-873-11926-7, Shinya Ohashi (supervised), Toshiaki Kurokawa (translated), O&#39;Reilly Japan Inc.&lt;br&gt; &lt;a href=&#34;https://www.google.com/books/edition/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E7%B5%B1/d7EJzgEACAAJ?hl=en&#34;&gt;Google books&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.co.jp/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E7%B5%B1%E8%A8%88%E5%AD%A6%E5%85%A5%E9%96%80-%E2%80%95%E4%BA%88%E6%B8%AC%E3%80%81%E5%88%86%E9%A1%9E%E3%80%81%E7%B5%B1%E8%A8%88%E3%83%A2%E3%83%87%E3%83%AA%E3%83%B3%E3%82%B0%E3%80%81%E7%B5%B1%E8%A8%88%E7%9A%84%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%A8R-Python%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-Peter-Bruce/dp/487311926X&#34;&gt;Amazon&lt;/a&gt;, &lt;a href=&#34;https://www.oreilly.co.jp/books/9784873119267/&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-german.jpg&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;German (2021-03-29):&lt;/b&gt;&lt;br&gt; Praktische Statistik für Data Scientists: 50+ essenzielle Konzepte mit R und Python&amp;nbsp;&lt;br&gt; 2021: ISBN 978-3-960-09153-0, Marcus Fraaß&amp;nbsp;(Übersetzer), dpunkt.verlag GmbH&lt;br&gt; &lt;a href=&#34;https://www.google.com/books/edition/Praktische_Statistik_f%C3%BCr_Data_Scientist/yeMCzgEACAAJ?hl=en&#34;&gt;Google books&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.de/Praktische-Statistik-f%C3%BCr-Data-Scientists/dp/3960091532&#34;&gt;Amazon&lt;/a&gt; &lt;a href=&#34;https://dpunkt.de/produkt/praktische-statistik-fuer-data-scientists/&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-korean.jpg&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Korean (2021-05-07):&lt;/b&gt;&lt;br&gt; Practical Statistics for Data Scientists: 데이터 과학을 위한 통계(2판)&lt;br&gt; 2021: ISBN 979-1-162-24418-0, Junyong Lee (translation), Hanbit Media, Inc. &lt;br&gt; &lt;a href=&#34;https://www.google.com/books/edition/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EA%B3%BC%ED%95%99%EC%9D%84_%EC%9C%84%ED%95%9C_%ED%86%B5%EA%B3%84_2%ED%8C%90/9E9qzgEACAAJ?hl=en&#34;&gt;Google books&lt;/a&gt;, &lt;a href=&#34;https://www.hanbit.co.kr/store/books/look.php?p_code=B2862122581&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-polish.jpg&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Polish (2021-06-16):&lt;/b&gt;&lt;br&gt; Statystyka praktyczna w data science. 50 kluczowych zagadnien w jezykach R i Python&lt;br&gt; 2021: ISBN 978-8-328-37427-0, Helion &lt;br&gt; &lt;a href=&#34;https://www.google.com/books/edition/Statystyka_praktyczna_w_data_science/GyqSzgEACAAJ&#34;&gt;Google books&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.com/Statystyka-praktyczna-science-kluczowych-zagadnien/dp/8328374277/&#34;&gt;Amazon&lt;/a&gt;, &lt;a href=&#34;https://helion.pl/ksiazki/statystyka-praktyczna-w-data-science-50-kluczowych-zagadnien-w-jezykach-r-i-python-wydanie-ii-peter-bruce-andrew-bruce-peter-gedeck,stpra2.htm&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-russian.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Russian (2021-05-31):&lt;/b&gt;&lt;br&gt; Практическая статистика для специалистов Data Science, 2-е изд.&lt;br&gt; 2021: ISBN 978-5-9775-6705-3, BHV St Petersburg &lt;br&gt; &lt;a href=&#34;https://www.google.com/books/edition/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%81%D1%82%D0%B0%D1%82%D0%B8%D1%81%D1%82/l_6MDwAAQBAJ&#34;&gt;Google books&lt;/a&gt;, &lt;a href=&#34;https://bhv.ru/product/prakticheskaya-statistika-dlya-spetsialistov-data-science-2-e-izd/&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-chinese-complex.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Chinese complex (2021-07-29):&lt;/b&gt;&lt;br&gt; Practical Statistics for Data Scientists: 資料科學家的實用統計學 第二版&lt;br&gt; 2021: ISBN 978-9-865-02841-1, Hong Weien (translation), GoTop Information Inc. &lt;br&gt; &#xA;    &lt;!-- &lt;a href=&#39;https://www.google.com/books/edition/&#39;&gt;Google books&lt;/a&gt;, --&gt; &lt;a href=&#34;http://books.gotop.com.tw/o_A643&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-chinese.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Chinese simplified (2021-10-15):&lt;/b&gt;&lt;br&gt; Practical Statistics for Data Scientists: 数据科学中的实用统计学（第2版）&lt;br&gt; 2021: ISBN 978-7-115-56902-8, Chen Guangxin (translation), Posts &amp;amp; Telecom Press &lt;br&gt; &#xA;    &lt;!-- &lt;a href=&#39;https://www.google.com/books/edition/&#39;&gt;Google books&lt;/a&gt;, --&gt; &lt;a href=&#34;https://item.jd.com/12971155.html&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/SPD-english.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;English (Indian subcontinent &amp;amp; select countries only):&lt;/b&gt;&lt;br&gt; Practical Statistics for Data Scientists: 50+ Essential Concepts Using R And Python, Second Edition&lt;br&gt; 2021: ISBN 978-8-194-43500-6, Shroff Publishers and Distributors Pvt. Ltd. &lt;br&gt; &#xA;    &lt;!-- &lt;a href=&#39;https://www.google.com/books/edition/&#39;&gt;Google books&lt;/a&gt;, --&gt; &lt;a href=&#34;https://www.shroffpublishers.com/books/9788194435006/&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/images/OReilly-spanish.png&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;Spanish (2022-02-22):&lt;/b&gt;&lt;br&gt; Estadística práctica para ciencia de datos con R y Python, Second Edition&lt;br&gt; 2022: ISBN 978-8-426-73443-3, Marcombo S.A. &lt;br&gt; &lt;a href=&#34;https://books.google.com/books?id=IZxeEAAAQBAJ&#34;&gt;Google books&lt;/a&gt;, &lt;a href=&#34;https://www.amazon.com/Estad%C3%ADstica-pr%C3%A1ctica-ciencia-datos-Python/dp/842673443X/&#34;&gt;Amazon&lt;/a&gt;, &lt;a href=&#34;https://www.marcombo.com/estadistica-practica-para-ciencia-de-datos-con-r-y-python-9788426734433/&#34;&gt;Order here&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;See also&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The code repository for the first edition is at: &lt;a href=&#34;https://github.com/andrewgbruce/statistics-for-data-scientists&#34;&gt;https://github.com/andrewgbruce/statistics-for-data-scientists&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Setup R and Python environments&lt;/h1&gt; &#xA;&lt;h2&gt;R&lt;/h2&gt; &#xA;&lt;p&gt;Run the following commands in R to install all required packages&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;if (!require(vioplot)) install.packages(&#39;vioplot&#39;)&#xA;if (!require(corrplot)) install.packages(&#39;corrplot&#39;)&#xA;if (!require(gmodels)) install.packages(&#39;gmodels&#39;)&#xA;if (!require(matrixStats)) install.packages(&#39;matrixStats&#39;)&#xA;&#xA;if (!require(lmPerm)) install.packages(&#39;lmPerm&#39;)&#xA;if (!require(pwr)) install.packages(&#39;pwr&#39;)&#xA;&#xA;if (!require(FNN)) install.packages(&#39;FNN&#39;)&#xA;if (!require(klaR)) install.packages(&#39;klaR&#39;)&#xA;if (!require(DMwR)) install.packages(&#39;DMwR&#39;)&#xA;&#xA;if (!require(xgboost)) install.packages(&#39;xgboost&#39;)&#xA;&#xA;if (!require(ellipse)) install.packages(&#39;ellipse&#39;)&#xA;if (!require(mclust)) install.packages(&#39;mclust&#39;)&#xA;if (!require(ca)) install.packages(&#39;ca&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Python&lt;/h2&gt; &#xA;&lt;p&gt;We recommend to use a conda environment to run the Python code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n sfds python&#xA;conda activate sfds&#xA;conda env update -n sfds -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>rasbt/python-machine-learning-book-3rd-edition</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/rasbt/python-machine-learning-book-3rd-edition</id>
    <link href="https://github.com/rasbt/python-machine-learning-book-3rd-edition" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The &#34;Python Machine Learning (3rd edition)&#34; book code repository&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Python Machine Learning (3rd Ed.) Code Repository&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/#&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.7-blue.svg?sanitize=true&#34; alt=&#34;Python 3.6&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-MIT-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Code repositories for the 1st and 2nd edition are available at&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rasbt/python-machine-learning-book&#34;&gt;https://github.com/rasbt/python-machine-learning-book&lt;/a&gt; and&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rasbt/python-machine-learning-book-2nd-edition&#34;&gt;https://github.com/rasbt/python-machine-learning-book-2nd-edition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Python Machine Learning, 3rd Ed.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;to be published December 12th, 2019&lt;/p&gt; &#xA;&lt;p&gt;Paperback: 770 pages&lt;br&gt; Publisher: Packt Publishing&lt;br&gt; Language: English&lt;/p&gt; &#xA;&lt;p&gt;ISBN-10: 1789955750&lt;br&gt; ISBN-13: 978-1789955750&lt;br&gt; Kindle ASIN: B07VBLX2W7&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1789955750/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/.other/cover_1.jpg&#34; width=&#34;248&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1789955750/&#34;&gt;Amazon Page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.packtpub.com/data/python-machine-learning-third-edition&#34;&gt;Packt Page&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents and Code Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Helpful installation and setup instructions can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch01/README.md&#34;&gt;README.md file of Chapter 1&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note that these are just the code examples accompanying the book, which we uploaded for your convenience; be aware that these notebooks may not be useful without the formulae and descriptive text.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Machine Learning - Giving Computers the Ability to Learn from Data [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch01&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Training Machine Learning Algorithms for Classification [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch02&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;A Tour of Machine Learning Classifiers Using Scikit-Learn [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch03&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Building Good Training Sets – Data Pre-Processing [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch04&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Compressing Data via Dimensionality Reduction [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch05&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Learning Best Practices for Model Evaluation and Hyperparameter Optimization [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch06&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Combining Different Models for Ensemble Learning [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch07&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Applying Machine Learning to Sentiment Analysis [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch08&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Embedding a Machine Learning Model into a Web Application [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch09&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Predicting Continuous Target Variables with Regression Analysis [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch10&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Working with Unlabeled Data – Clustering Analysis [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch11&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Implementing a Multi-layer Artificial Neural Network from Scratch [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch12&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Parallelizing Neural Network Training with TensorFlow [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch13&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Going Deeper: The Mechanics of TensorFlow [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch14&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Classifying Images with Deep Convolutional Neural Networks [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch15&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Modeling Sequential Data Using Recurrent Neural Networks [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch16&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Generative Adversarial Networks for Synthesizing New Data [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch17&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Reinforcement Learning for Decision Making in Complex Environments [&lt;a href=&#34;https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch18&#34;&gt;open dir&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Raschka, Sebastian, and Vahid Mirjalili. &lt;em&gt;Python Machine Learning, 3rd Ed&lt;/em&gt;. Packt Publishing, 2019.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@book{RaschkaMirjalili2019,  &#xA;address = {Birmingham, UK},  &#xA;author = {Raschka, Sebastian and Mirjalili, Vahid},  &#xA;edition = {3},  &#xA;isbn = {978-1789955750},   &#xA;publisher = {Packt Publishing},  &#xA;title = {{Python Machine Learning, 3rd Ed.}},  &#xA;year = {2019}  &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Project-MONAI/tutorials</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/Project-MONAI/tutorials</id>
    <link href="https://github.com/Project-MONAI/tutorials" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MONAI Tutorials&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MONAI Tutorials&lt;/h1&gt; &#xA;&lt;p&gt;This repository hosts the MONAI tutorials.&lt;/p&gt; &#xA;&lt;h3&gt;1. Requirements&lt;/h3&gt; &#xA;&lt;p&gt;Most of the examples and tutorials require &lt;a href=&#34;https://matplotlib.org/&#34;&gt;matplotlib&lt;/a&gt; and &lt;a href=&#34;https://jupyter.org/&#34;&gt;Jupyter Notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;These can be installed with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install -U pip&#xA;python -m pip install -U matplotlib&#xA;python -m pip install -U notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some of the examples may require optional dependencies. In case of any optional import errors, please install the relevant packages according to MONAI&#39;s &lt;a href=&#34;https://docs.monai.io/en/latest/installation.html&#34;&gt;installation guide&lt;/a&gt;. Or install all optional requirements with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r https://raw.githubusercontent.com/Project-MONAI/MONAI/dev/requirements-dev.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Run the notebooks from Colab&lt;/h4&gt; &#xA;&lt;p&gt;Most of the Jupyter Notebooks have an &#34;Open in Colab&#34; button. Please right-click on the button, and select &#34;Open Link in New Tab&#34; to start a Colab page with the corresponding notebook content.&lt;/p&gt; &#xA;&lt;p&gt;To use GPU resources through Colab, please remember to change the runtime type to &lt;code&gt;GPU&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;From the &lt;code&gt;Runtime&lt;/code&gt; menu select &lt;code&gt;Change runtime type&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Choose &lt;code&gt;GPU&lt;/code&gt; from the drop-down menu&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;SAVE&lt;/code&gt; This will reset the notebook and may ask you if you are a robot (these instructions assume you are not).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;!nvidia-smi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;in a cell will verify this has worked and show you what kind of hardware you have access to.&lt;/p&gt; &#xA;&lt;h4&gt;Data&lt;/h4&gt; &#xA;&lt;p&gt;Some notebooks will require additional data. They can be downloaded by running the &lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/runexamples.sh&#34;&gt;runexamples.sh&lt;/a&gt; script.&lt;/p&gt; &#xA;&lt;h3&gt;2. Questions and bugs&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For questions relating to the use of MONAI, please us our &lt;a href=&#34;https://github.com/Project-MONAI/MONAI/discussions&#34;&gt;Discussions tab&lt;/a&gt; on the main repository of MONAI.&lt;/li&gt; &#xA; &lt;li&gt;For bugs relating to MONAI functionality, please create an issue on the &lt;a href=&#34;https://github.com/Project-MONAI/MONAI/issues&#34;&gt;main repository&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For bugs relating to the running of a tutorial, please create an issue in &lt;a href=&#34;https://github.com/Project-MONAI/Tutorials/issues&#34;&gt;this repository&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. Note to developers&lt;/h3&gt; &#xA;&lt;p&gt;During integration testing, we run these notebooks. To save time, we modify variables to avoid unecessary &lt;code&gt;for&lt;/code&gt; loop iterations. Hence, during training please use the variables &lt;code&gt;max_epochs&lt;/code&gt; and &lt;code&gt;val_interval&lt;/code&gt; for the number of training epochs and validation interval, respectively.&lt;/p&gt; &#xA;&lt;p&gt;If your notebook doesn&#39;t use the idea of epochs, then please add it to the variable &lt;code&gt;doesnt_contain_max_epochs&lt;/code&gt; in &lt;code&gt;runner.sh&lt;/code&gt;. This lets the runner know that it&#39;s not a problem if it doesn&#39;t find &lt;code&gt;max_epochs&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you have any other variables that would benefit by setting them to &lt;code&gt;1&lt;/code&gt; during testing, add them to &lt;code&gt;strings_to_replace&lt;/code&gt; in &lt;code&gt;runner.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;4. List of notebooks and examples&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;2D classification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/2d_classification/mednist_tutorial.ipynb&#34;&gt;mednist_tutorial&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook shows how to easily integrate MONAI features into existing PyTorch programs. It&#39;s based on the MedNIST dataset which is very suitable for beginners as a tutorial. This tutorial also makes use of MONAI&#39;s in-built occlusion sensitivity functionality.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2D segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/2d_segmentation/torch&#34;&gt;torch examples&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Training and evaluation examples of 2D segmentation based on UNet and synthetic dataset. The examples are standard PyTorch programs and have both dictionary-based and array-based versions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D classification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/3d_classification/ignite&#34;&gt;ignite examples&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Training and evaluation examples of 3D classification based on DenseNet3D and &lt;a href=&#34;https://brain-development.org/ixi-dataset&#34;&gt;IXI dataset&lt;/a&gt;. The examples are PyTorch Ignite programs and have both dictionary-based and array-based transformation versions.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/3d_classification/torch&#34;&gt;torch examples&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Training and evaluation examples of 3D classification based on DenseNet3D and &lt;a href=&#34;https://brain-development.org/ixi-dataset&#34;&gt;IXI dataset&lt;/a&gt;. The examples are standard PyTorch programs and have both dictionary-based and array-based transformation versions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/3d_segmentation/ignite&#34;&gt;ignite examples&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Training and evaluation examples of 3D segmentation based on UNet3D and synthetic dataset. The examples are PyTorch Ignite programs and have both dictionary-base and array-based transformations.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/3d_segmentation/torch&#34;&gt;torch examples&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Training, evaluation and inference examples of 3D segmentation based on UNet3D and synthetic dataset. The examples are standard PyTorch programs and have both dictionary-based and array-based versions.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/3d_segmentation/brats_segmentation_3d.ipynb&#34;&gt;brats_segmentation_3d&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This tutorial shows how to construct a training workflow of multi-labels segmentation task based on &lt;a href=&#34;http://medicaldecathlon.com&#34;&gt;MSD Brain Tumor dataset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/3d_segmentation/spleen_segmentation_3d_visualization_basic.ipynb&#34;&gt;spleen_segmentation_3d_aim&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook shows how MONAI may be used in conjunction with the &lt;a href=&#34;https://github.com/aimhubio/aim&#34;&gt;&lt;code&gt;aimhubio/aim&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/3d_segmentation/spleen_segmentation_3d_lightning.ipynb&#34;&gt;spleen_segmentation_3d_lightning&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook shows how MONAI may be used in conjunction with the &lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning&#34;&gt;PyTorch Lightning&lt;/a&gt; framework.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/3d_segmentation/spleen_segmentation_3d.ipynb&#34;&gt;spleen_segmentation_3d&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook is an end-to-end training and evaluation example of 3D segmentation based on &lt;a href=&#34;http://medicaldecathlon.com&#34;&gt;MSD Spleen dataset&lt;/a&gt;. The example shows the flexibility of MONAI modules in a PyTorch-based program:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Transforms for dictionary-based training data structure.&lt;/li&gt; &#xA; &lt;li&gt;Load NIfTI images with metadata.&lt;/li&gt; &#xA; &lt;li&gt;Scale medical image intensity with expected range.&lt;/li&gt; &#xA; &lt;li&gt;Crop out a batch of balanced image patch samples based on positive / negative label ratio.&lt;/li&gt; &#xA; &lt;li&gt;Cache IO and transforms to accelerate training and validation.&lt;/li&gt; &#xA; &lt;li&gt;3D UNet, Dice loss function, Mean Dice metric for 3D segmentation task.&lt;/li&gt; &#xA; &lt;li&gt;Sliding window inference.&lt;/li&gt; &#xA; &lt;li&gt;Deterministic training for reproducibility.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/3d_segmentation/unet_segmentation_3d_catalyst.ipynb&#34;&gt;unet_segmentation_3d_catalyst&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook shows how MONAI may be used in conjunction with the &lt;a href=&#34;https://github.com/catalyst-team/catalyst&#34;&gt;Catalyst&lt;/a&gt; framework.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/3d_segmentation/unet_segmentation_3d_ignite.ipynb&#34;&gt;unet_segmentation_3d_ignite&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook is an end-to-end training &amp;amp; evaluation example of 3D segmentation based on synthetic dataset. The example is a PyTorch Ignite program and shows several key features of MONAI, especially with medical domain specific transforms and event handlers for profiling (logging, TensorBoard, MLFlow, etc.).&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/3d_segmentation/challenge_baseline&#34;&gt;COVID 19-20 challenge baseline&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This folder provides a simple baseline method for training, validation, and inference for &lt;a href=&#34;https://covid-segmentation.grand-challenge.org/COVID-19-20/&#34;&gt;COVID-19 LUNG CT LESION SEGMENTATION CHALLENGE - 2020&lt;/a&gt; (a MICCAI Endorsed Event).&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/3d_segmentation/unetr_btcv_segmentation_3d.ipynb&#34;&gt;unetr_btcv_segmentation_3d&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook demonstrates how to construct a training workflow of UNETR on multi-organ segmentation task using the BTCV challenge dataset.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/3d_segmentation/unetr_btcv_segmentation_3d_lightning.ipynb&#34;&gt;unetr_btcv_segmentation_3d_lightning&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This tutorial demonstrates how MONAI can be used in conjunction with &lt;a href=&#34;https://www.pytorchlightning.ai/&#34;&gt;PyTorch Lightning&lt;/a&gt; framework to construct a training workflow of UNETR on multi-organ segmentation task using the BTCV challenge dataset.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2D registration&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/2d_registration/registration_mednist.ipynb&#34;&gt;registration using mednist&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook shows a quick demo for learning based affine registration of &lt;code&gt;64 x 64&lt;/code&gt; X-Ray hands.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D registration&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/3d_registration/paired_lung_ct.ipynb&#34;&gt;3D registration using paired lung CT&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This tutorial shows how to use MONAI to register lung CT volumes acquired at different time points for a single patient.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/deep_atlas/deep_atlas_tutorial.ipynb&#34;&gt;DeepAtlas&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This tutorial demonstrates the use of MONAI for training of registration and segmentation models &lt;em&gt;together&lt;/em&gt;. The DeepAtlas approach, in which the two models serve as a source of weakly supervised learning for each other, is useful in situations where one has many unlabeled images and just a few images with segmentation labels. The notebook works with 3D images from the OASIS-1 brain MRI dataset.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;deepgrow&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/deepgrow&#34;&gt;Deepgrow&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The example show how to train/validate a 2D/3D deepgrow model. It also demonstrates running an inference for trained deepgrow models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeepEdit&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/deepedit/ignite&#34;&gt;DeepEdit&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This example shows how to train/test a DeepEdit model. In this tutorial there is a Notebook that shows how to run inference on a pretrained DeepEdit model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;deployment&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/deployment/bentoml&#34;&gt;BentoML&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This is a simple example of training and deploying a MONAI network with &lt;a href=&#34;https://www.bentoml.ai/&#34;&gt;BentoML&lt;/a&gt; as a web server, either locally using the BentoML respository or as a containerized service.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/deployment/ray&#34;&gt;Ray&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This uses the previous notebook&#39;s trained network to demonstrate deployment a web server using &lt;a href=&#34;https://docs.ray.io/en/master/serve/index.html#rayserve&#34;&gt;Ray&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;federated learning&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/federated_learning/nvflare&#34;&gt;NVFlare&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The examples show how to train federated learning models with &lt;a href=&#34;https://pypi.org/project/nvflare/&#34;&gt;NVFlare&lt;/a&gt; and MONAI-based trainers.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/federated_learning/openfl&#34;&gt;OpenFL&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The examples show how to train federated learning models based on &lt;a href=&#34;https://github.com/intel/openfl&#34;&gt;OpenFL&lt;/a&gt; and MONAI.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/federated_learning/substra&#34;&gt;Substra&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The example show how to execute the 3d segmentation torch tutorial on a federated learning platform, Substra.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Digital Pathology&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/pathology/tumor_detection&#34;&gt;Whole Slide Tumor Detection&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The example show how to train and evaluate a tumor detection model (based on patch classification) on whole-slide histopathology images.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/pathology/tumor_detection&#34;&gt;Profiling Whole Slide Tumor Detection&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The example show how to use MONAI NVTX transforms to tag and profile pre- and post-processing transforms in the digital pathology whole slide tumor detection pipeline.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;acceleration&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/acceleration/fast_model_training_guide.md&#34;&gt;fast_model_training_guide&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The document introduces details of how to profile the training pipeline, how to analyze the dataset and select suitable algorithms, and how to optimize GPU utilization in single GPU, multi-GPUs or even multi-nodes.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/acceleration/distributed_training&#34;&gt;distributed_training&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The examples show how to execute distributed training and evaluation based on 3 different frameworks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch native &lt;code&gt;DistributedDataParallel&lt;/code&gt; module with &lt;code&gt;torch.distributed.launch&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Horovod APIs with &lt;code&gt;horovodrun&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;PyTorch ignite and MONAI workflows.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;They can run on several distributed nodes with multiple GPU devices on every node.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/acceleration/automatic_mixed_precision.ipynb&#34;&gt;automatic_mixed_precision&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;And compares the training speed and memory usage with/without AMP.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/acceleration/dataset_type_performance.ipynb&#34;&gt;dataset_type_performance&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook compares the performance of &lt;code&gt;Dataset&lt;/code&gt;, &lt;code&gt;CacheDataset&lt;/code&gt; and &lt;code&gt;PersistentDataset&lt;/code&gt;. These classes differ in how data is stored (in memory or on disk), and at which moment transforms are applied.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/acceleration/fast_training_tutorial.ipynb&#34;&gt;fast_training_tutorial&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This tutorial compares the training performance of pure PyTorch program and optimized program in MONAI based on NVIDIA GPU device and latest CUDA library. The optimization methods mainly include: &lt;code&gt;AMP&lt;/code&gt;, &lt;code&gt;CacheDataset&lt;/code&gt;, &lt;code&gt;GPU transforms&lt;/code&gt;, &lt;code&gt;ThreadDataLoader&lt;/code&gt;, &lt;code&gt;DiceCELoss&lt;/code&gt; and &lt;code&gt;SGD&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/acceleration/multi_gpu_test.ipynb&#34;&gt;multi_gpu_test&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook is a quick demo for devices, run the Ignite trainer engine on CPU, GPU and multiple GPUs.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/acceleration/threadbuffer_performance.ipynb&#34;&gt;threadbuffer_performance&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Demonstrates the use of the &lt;code&gt;ThreadBuffer&lt;/code&gt; class used to generate data batches during training in a separate thread.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/acceleration/transform_speed.ipynb&#34;&gt;transform_speed&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Illustrate reading NIfTI files and test speed of different transforms on different devices.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;modules&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/bundle&#34;&gt;bundle&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Get started tutorial and concrete training / inference examples for MONAI bundle features.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/engines&#34;&gt;engines&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Training and evaluation examples of 3D segmentation based on UNet3D and synthetic dataset with MONAI workflows, which contains engines, event-handlers, and post-transforms. And GAN training and evaluation example for a medical image generative adversarial network. Easy run training script uses &lt;code&gt;GanTrainer&lt;/code&gt; to train a 2D CT scan reconstruction network. Evaluation script generates random samples from a trained network.&lt;/p&gt; &#xA;&lt;p&gt;The examples are built with MONAI workflows, mainly contain: trainer/evaluator, handlers, post_transforms, etc.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/3d_image_transforms.ipynb&#34;&gt;3d_image_transforms&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook demonstrates the transformations on volumetric images.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/2d_inference_3d_volume.ipynb&#34;&gt;2d_inference_3d_volume&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Tutorial that demonstrates how monai &lt;code&gt;SlidingWindowInferer&lt;/code&gt; can be used when a 3D volume input needs to be provided slice-by-slice to a 2D model and finally, aggregated into a 3D volume.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/autoencoder_mednist.ipynb&#34;&gt;autoencoder_mednist&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This tutorial uses the MedNIST hand CT scan dataset to demonstrate MONAI&#39;s autoencoder class. The autoencoder is used with an identity encode/decode (i.e., what you put in is what you should get back), as well as demonstrating its usage for de-blurring and de-noising.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/batch_output_transform.py&#34;&gt;batch_output_transform&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Tutorial to explain and show how to set &lt;code&gt;batch_transform&lt;/code&gt; and &lt;code&gt;output_transform&lt;/code&gt; of handlers to work with MONAI engines.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/compute_metric.py&#34;&gt;compute_metric&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Example shows how to compute metrics from saved predictions and labels with PyTorch multi-processing support.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/csv_datasets.ipynb&#34;&gt;csv_datasets&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Tutorial shows the usage of &lt;code&gt;CSVDataset&lt;/code&gt; and &lt;code&gt;CSVIterableDataset&lt;/code&gt;, load multiple CSV files and execute postprocessing logic.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/decollate_batch.py&#34;&gt;decollate_batch&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Tutorial shows how to decollate batch data to simplify post processing transforms and execute more flexible following operations.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/image_dataset.py&#34;&gt;image_dataset&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Notebook introduces basic usages of &lt;code&gt;monai.data.ImageDataset&lt;/code&gt; module.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/dynunet_pipeline&#34;&gt;dynunet_tutorial&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This tutorial shows how to train 3D segmentation tasks on all the 10 decathlon datasets with the reimplementation of dynUNet in MONAI.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/integrate_3rd_party_transforms.ipynb&#34;&gt;integrate_3rd_party_transforms&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This tutorial shows how to integrate 3rd party transforms into MONAI program. Mainly shows transforms from BatchGenerator, TorchIO, Rising and ITK.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/inverse_transforms_and_test_time_augmentations.ipynb&#34;&gt;inverse transformations and test-time augmentations&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook demonstrates the use of invertible transforms, and then leveraging inverse transformations to perform test-time augmentations.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/layer_wise_learning_rate.ipynb&#34;&gt;layer wise learning rate&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook demonstrates how to select or filter out expected network layers and set customized learning rate values.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/learning_rate.ipynb&#34;&gt;learning rate finder&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook demonstrates how to use &lt;code&gt;LearningRateFinder&lt;/code&gt; API to tune the learning rate values for the network.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/load_medical_images.ipynb&#34;&gt;load_medical_imagesl&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook introduces how to easily load different formats of medical images in MONAI and execute many additional operations.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/mednist_GAN_tutorial.ipynb&#34;&gt;mednist_GAN_tutorial&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook illustrates the use of MONAI for training a network to generate images from a random input tensor. A simple GAN is employed to do with a separate Generator and Discriminator networks.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/mednist_GAN_workflow_dict.ipynb&#34;&gt;mednist_GAN_workflow_dict&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook shows the &lt;code&gt;GanTrainer&lt;/code&gt;, a MONAI workflow engine for modularized adversarial learning. Train a medical image reconstruction network using the MedNIST hand CT scan dataset. Dictionary version.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/mednist_GAN_workflow_array.ipynb&#34;&gt;mednist_GAN_workflow_array&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook shows the &lt;code&gt;GanTrainer&lt;/code&gt;, a MONAI workflow engine for modularized adversarial learning. Train a medical image reconstruction network using the MedNIST hand CT scan dataset. Array version.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/cross_validation_models_ensemble.ipynb&#34;&gt;cross_validation_models_ensemble&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This tutorial shows how to leverage &lt;code&gt;CrossValidation&lt;/code&gt;, &lt;code&gt;EnsembleEvaluator&lt;/code&gt;, &lt;code&gt;MeanEnsemble&lt;/code&gt; and &lt;code&gt;VoteEnsemble&lt;/code&gt; modules in MONAI to set up cross validation and ensemble program.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/nifti_read_example.ipynb&#34;&gt;nifti_read_example&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Illustrate reading NIfTI files and iterating over image patches of the volumes loaded from them.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/network_api.ipynb&#34;&gt;network_api&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This tutorial illustrates the flexible network APIs and utilities.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/postprocessing_transforms.ipynb&#34;&gt;postprocessing_transforms&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook shows the usage of several postprocessing transforms based on the model output of spleen segmentation task.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/public_datasets.ipynb&#34;&gt;public_datasets&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook shows how to quickly set up training workflow based on &lt;code&gt;MedNISTDataset&lt;/code&gt; and &lt;code&gt;DecathlonDataset&lt;/code&gt;, and how to create a new dataset.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/tcia_csv_processing.ipynb&#34;&gt;tcia_csv_processing&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook shows how to load the TCIA data with CSVDataset from CSV file and extract information for TCIA data to fetch DICOM images based on REST API.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/transforms_demo_2d.ipynb&#34;&gt;transforms_demo_2d&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook demonstrates the image transformations on histology images using &lt;a href=&#34;https://warwick.ac.uk/fac/sci/dcs/research/tia/glascontest/download/&#34;&gt;the GlaS Contest dataset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/UNet_input_size_constrains.ipynb&#34;&gt;UNet_input_size_constrains&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This tutorial shows how to determine a reasonable spatial size of the input data for MONAI UNet, which not only supports residual units, but also can use more hyperparameters (like &lt;code&gt;strides&lt;/code&gt;, &lt;code&gt;kernel_size&lt;/code&gt; and &lt;code&gt;up_kernel_size&lt;/code&gt;) than the basic UNet implementation.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/TorchIO_MONAI_PyTorch_Lightning.ipynb&#34;&gt;TorchIO, MONAI, PyTorch Lightning&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This notebook demonstrates how the three libraries from the official PyTorch Ecosystem can be used together to segment the hippocampus on brain MRIs from the Medical Segmentation Decathlon.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/varautoencoder_mednist.ipynb&#34;&gt;varautoencoder_mednist&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This tutorial uses the MedNIST scan (or alternatively the MNIST) dataset to demonstrate MONAI&#39;s variational autoencoder class.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/interpretability&#34;&gt;interpretability&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Tutorials in this folder demonstrate model visualisation and interpretability features of MONAI. Currently, it consists of class activation mapping and occlusion sensitivity for 3D classification model visualisations and analysis.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/transfer_mmar.ipynb&#34;&gt;Transfer learning with MMAR&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This tutorial demonstrates a transfer learning pipeline from a pretrained model in &lt;a href=&#34;https://docs.nvidia.com/clara/clara-train-sdk/pt/mmar.html&#34;&gt;Clara Train&#39;s Medical Model Archive format&lt;/a&gt;. The notebook also shows the use of LMDB-based dataset.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-MONAI/tutorials/main/modules/transform_visualization.ipynb&#34;&gt;Transform visualization&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This tutorial shows several visualization approaches for 3D image during transform augmentation.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>DataTalksClub/mlops-zoomcamp</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/DataTalksClub/mlops-zoomcamp</id>
    <link href="https://github.com/DataTalksClub/mlops-zoomcamp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free MLOps course from DataTalks.Club&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLOps Zoomcamp&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=3T5kUA3eWWc&amp;amp;list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/images/banner.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Our MLOps Zoomcamp course&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sign up here: &lt;a href=&#34;https://airtable.com/shrCb8y6eTbPKwSTL&#34;&gt;https://airtable.com/shrCb8y6eTbPKwSTL&lt;/a&gt; (it&#39;s not automated, you will not receive an email immediately after filling in the form)&lt;/li&gt; &#xA; &lt;li&gt;Register in &lt;a href=&#34;https://datatalks.club/slack.html&#34;&gt;DataTalks.Club&#39;s Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Join the &lt;a href=&#34;https://app.slack.com/client/T01ATQK62F8/C02R98X7DS9&#34;&gt;&lt;code&gt;#course-mlops-zoomcamp&lt;/code&gt;&lt;/a&gt; channel&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ctt.ac/fH67W&#34;&gt;Tweet about the course!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Subscribe to the &lt;a href=&#34;https://calendar.google.com/calendar/?cid=M3Jzbmg0ZDA2aHVsY2M1ZjcyNDJtODNyMTRAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&#34;&gt;public Google calendar&lt;/a&gt; (subscription works from desktop only)&lt;/li&gt; &#xA; &lt;li&gt;Start watching course videos! &lt;a href=&#34;https://www.youtube.com/playlist?list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK&#34;&gt;Course playlist&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/12TlBfhIiKtyBv8RnsoJR6F72bkPDGEvPOItJIxaEzE0/edit&#34;&gt;Technical FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For announcements, join our &lt;a href=&#34;https://t.me/dtc_courses&#34;&gt;Telegram channel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vRhinTR4Gpxcud-xX0cPBVqboO8RE5gFY7W2dfgfhzECuPFOaCoo9TVWUTxxrSmzvbZY0D-N1vai8RN/pubhtml&#34;&gt;Leaderboard&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;h3&gt;Objective&lt;/h3&gt; &#xA;&lt;p&gt;Teach practical aspects of productionizing ML services — from collecting requirements to model deployment and monitoring.&lt;/p&gt; &#xA;&lt;h3&gt;Target audience&lt;/h3&gt; &#xA;&lt;p&gt;Data scientists and ML engineers. Also software engineers and data engineers interested in learning about putting ML in production.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-requisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python&lt;/li&gt; &#xA; &lt;li&gt;Docker&lt;/li&gt; &#xA; &lt;li&gt;Being comfortable with command line&lt;/li&gt; &#xA; &lt;li&gt;Prior exposure to machine learning (at work or from other courses, e.g. from &lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp&#34;&gt;ML Zoomcamp&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Prior programming experience (at least 1+ year)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Timeline&lt;/h3&gt; &#xA;&lt;p&gt;Course start: 16 of May&lt;/p&gt; &#xA;&lt;h3&gt;Asking for help in Slack&lt;/h3&gt; &#xA;&lt;p&gt;The best way to get support is to use &lt;a href=&#34;https://datatalks.club/slack.html&#34;&gt;DataTalks.Club&#39;s Slack&lt;/a&gt;. Join the &lt;a href=&#34;https://app.slack.com/client/T01ATQK62F8/C02R98X7DS9&#34;&gt;&lt;code&gt;#course-mlops-zoomcamp&lt;/code&gt;&lt;/a&gt; channel.&lt;/p&gt; &#xA;&lt;p&gt;To make discussions in Slack more organized:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow &lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/asking-questions.md&#34;&gt;these recommendations&lt;/a&gt; when asking for help&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://datatalks.club/slack/guidelines.html&#34;&gt;DataTalks.Club community guidelines&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Syllabus&lt;/h2&gt; &#xA;&lt;p&gt;This is a draft and will change.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/01-intro&#34;&gt;Module 1: Introduction&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What is MLOps&lt;/li&gt; &#xA; &lt;li&gt;MLOps maturity model&lt;/li&gt; &#xA; &lt;li&gt;Running example: NY Taxi trips dataset&lt;/li&gt; &#xA; &lt;li&gt;Why do we need MLOps&lt;/li&gt; &#xA; &lt;li&gt;Course overview&lt;/li&gt; &#xA; &lt;li&gt;Environment preparation&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/01-intro&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/02-experiment-tracking&#34;&gt;Module 2: Experiment tracking and model management&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Experiment tracking intro&lt;/li&gt; &#xA; &lt;li&gt;Getting started with MLflow&lt;/li&gt; &#xA; &lt;li&gt;Experiment tracking with MLflow&lt;/li&gt; &#xA; &lt;li&gt;Saving and loading models with MLflow&lt;/li&gt; &#xA; &lt;li&gt;Model registry&lt;/li&gt; &#xA; &lt;li&gt;MLflow in practice&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/02-experiment-tracking&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/03-orchestration&#34;&gt;Module 3: Orchestration and ML Pipelines&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Workflow orchestration&lt;/li&gt; &#xA; &lt;li&gt;Prefect 2.0&lt;/li&gt; &#xA; &lt;li&gt;Turning a notebook into a pipeline&lt;/li&gt; &#xA; &lt;li&gt;Deployment of Prefect flow&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/03-orchestration&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Module 4: Model Deployment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Batch vs online&lt;/li&gt; &#xA; &lt;li&gt;For online: web services vs streaming&lt;/li&gt; &#xA; &lt;li&gt;Serving models in Batch mode&lt;/li&gt; &#xA; &lt;li&gt;Web services&lt;/li&gt; &#xA; &lt;li&gt;Streaming (Kinesis/SQS + AWS Lambda)&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Module 5: Model Monitoring&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ML monitoring vs software monitoring&lt;/li&gt; &#xA; &lt;li&gt;Data quality monitoring&lt;/li&gt; &#xA; &lt;li&gt;Data drift / concept drift&lt;/li&gt; &#xA; &lt;li&gt;Batch vs real-time monitoring&lt;/li&gt; &#xA; &lt;li&gt;Tools: Evidently, Prometheus and Grafana&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Module 6: Best Practices&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Devops&lt;/li&gt; &#xA; &lt;li&gt;Virtual environments and Docker&lt;/li&gt; &#xA; &lt;li&gt;Python: logging, linting&lt;/li&gt; &#xA; &lt;li&gt;Testing: unit, integration, regression&lt;/li&gt; &#xA; &lt;li&gt;CI/CD (github actions)&lt;/li&gt; &#xA; &lt;li&gt;Infrastructure as code (terraform, cloudformation)&lt;/li&gt; &#xA; &lt;li&gt;Cookiecutter&lt;/li&gt; &#xA; &lt;li&gt;Makefiles&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Module 7: Processes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CRISP-DM, CRISP-ML&lt;/li&gt; &#xA; &lt;li&gt;ML Canvas&lt;/li&gt; &#xA; &lt;li&gt;Data Landscape canvas&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://miro.com/miroverse/mlops-stack-canvas/&#34;&gt;MLOps Stack Canvas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Documentation practices in ML projects (Model Cards Toolkit)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Project&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;End-to-end project with all the things above&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running example&lt;/h2&gt; &#xA;&lt;p&gt;To make it easier to connect different modules together, we’d like to use the same running example throughout the course.&lt;/p&gt; &#xA;&lt;p&gt;Possible candidates:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page&#34;&gt;https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page&lt;/a&gt; - predict the ride duration or if the driver is going to be tipped or not&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Instructors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Larysa Visengeriyeva&lt;/li&gt; &#xA; &lt;li&gt;Cristian Martinez&lt;/li&gt; &#xA; &lt;li&gt;Kevin Kho&lt;/li&gt; &#xA; &lt;li&gt;Theofilos Papapanagiotou&lt;/li&gt; &#xA; &lt;li&gt;Alexey Grigorev&lt;/li&gt; &#xA; &lt;li&gt;Emeli Dral&lt;/li&gt; &#xA; &lt;li&gt;Sejal Vaidya&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Other courses from DataTalks.Club:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp&#34;&gt;Machine Learning Zoomcamp - free 4-month course about ML Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DataTalksClub/data-engineering-zoomcamp/&#34;&gt;Data Engineering Zoomcamp - free 9-week course about Data Engineering&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;I want to start preparing for the course. What can I do?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you haven&#39;t used Flask or Docker&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check &lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/05-deployment&#34;&gt;Module 5&lt;/a&gt; form ML Zoomcamp&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/2_docker_sql&#34;&gt;section about Docker&lt;/a&gt; from Data Engineering Zoomcamp could also be useful&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you have no previous experience with ML&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check &lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/01-intro&#34;&gt;Module 1&lt;/a&gt; from ML Zoomcamp for an overview&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/03-classification&#34;&gt;Module 3&lt;/a&gt; will also be helpful if you want to learn Scikit-Learn (we&#39;ll use it in this course)&lt;/li&gt; &#xA; &lt;li&gt;We&#39;ll also use XGBoost. You don&#39;t have to know it well, but if you want to learn more about it, refer to &lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/06-trees&#34;&gt;module 6&lt;/a&gt; of ML Zoomcamp&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;I registered but haven&#39;t received an invite link. Is it normal?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes, we haven&#39;t automated it. You&#39;ll get a mail from us eventually, don&#39;t worry.&lt;/p&gt; &#xA;&lt;p&gt;If you want to make sure you don&#39;t miss anything:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Register in &lt;a href=&#34;https://datatalks.club/slack.html&#34;&gt;our Slack&lt;/a&gt; and join the &lt;code&gt;#course-mlops-zoomcamp&lt;/code&gt; channel&lt;/li&gt; &#xA; &lt;li&gt;Subscribe to &lt;a href=&#34;https://youtube.com/c/datatalksclub&#34;&gt;our YouTube channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is it going to be live?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;No and yes. There will be two parts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lectures: Pre-recorded, you can watch them when it&#39;s convenient for you.&lt;/li&gt; &#xA; &lt;li&gt;Office hours: Live on Mondays (17:00 CET), but recorded, so you can watch later.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supporters and partners&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to the course sponsors for making it possible to create this course&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.prefect.io/&#34;&gt; &lt;img height=&#34;100&#34; src=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/images/prefect.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Thanks to our friends for spreading the word about the course&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://dphi.tech/&#34;&gt; &lt;img height=&#34;75&#34; src=&#34;https://datatalks.club/images/partners/dphi.png&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.confetti.ai/&#34;&gt; &lt;img height=&#34;75&#34; src=&#34;https://datatalks.club/images/partners/confetti.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://mlopsworld.com/&#34;&gt; &lt;img height=&#34;75&#34; src=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/images/mlops-world.png&#34;&gt; &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>deepmind/deepmind-research</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/deepmind/deepmind-research</id>
    <link href="https://github.com/deepmind/deepmind-research" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains implementations and illustrative code to accompany DeepMind publications&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepMind Research&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains implementations and illustrative code to accompany DeepMind publications. Along with publishing papers to accompany research conducted at DeepMind, we release open-source &lt;a href=&#34;https://deepmind.com/research/open-source/open-source-environments/&#34;&gt;environments&lt;/a&gt;, &lt;a href=&#34;https://deepmind.com/research/open-source/open-source-datasets/&#34;&gt;data sets&lt;/a&gt;, and &lt;a href=&#34;https://deepmind.com/research/open-source/open-source-code/&#34;&gt;code&lt;/a&gt; to enable the broader research community to engage with our work and build upon it, with the ultimate goal of accelerating scientific progress to benefit society. For example, you can build on our implementations of the &lt;a href=&#34;https://github.com/deepmind/dqn&#34;&gt;Deep Q-Network&lt;/a&gt; or &lt;a href=&#34;https://github.com/deepmind/dnc&#34;&gt;Differential Neural Computer&lt;/a&gt;, or experiment in the same environments we use for our research, such as &lt;a href=&#34;https://github.com/deepmind/lab&#34;&gt;DeepMind Lab&lt;/a&gt; or &lt;a href=&#34;https://github.com/deepmind/pysc2&#34;&gt;StarCraft II&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you enjoy building tools, environments, software libraries, and other infrastructure of the kind listed below, you can view open positions to work in related areas on our &lt;a href=&#34;https://deepmind.com/careers/&#34;&gt;careers page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a full list of our publications, please see &lt;a href=&#34;https://deepmind.com/research/publications/&#34;&gt;https://deepmind.com/research/publications/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/fusion_tcv&#34;&gt;Magnetic control of tokamak plasmas through deep reinforcement learning&lt;/a&gt;, Nature 2022&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/density_functional_approximation_dm21&#34;&gt;Pushing the Frontiers of Density Functionals by Solving the Fractional Electron Problem&lt;/a&gt;, Science 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/pitfalls_static_language_models&#34;&gt;Mind the Gap: Assessing Temporal Generalization in Neural Language Models&lt;/a&gt;, NeurIPS 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/tandem_dqn&#34;&gt;The Difficulty of Passive Learning in Deep Reinforcement Learning&lt;/a&gt;, NeurIPS 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/nowcasting&#34;&gt;Skilful precipitation nowcasting using deep generative models of radar&lt;/a&gt;, Nature 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/cadl&#34;&gt;Compute-Aided Design as Language&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/continual_learning&#34;&gt;Encoders and ensembles for continual learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/hierarchical_transformer_memory&#34;&gt;Towards mental time travel: a hierarchical memory for reinforcement learning agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver&#34;&gt;Perceiver IO: A General Architecture for Structured Inputs &amp;amp; Outputs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/neural_mip_solving&#34;&gt;Solving Mixed Integer Programs Using Neural Networks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/noisy_label&#34;&gt;A Realistic Simulation Framework for Learning with Label Noise&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/rapid_task_solving&#34;&gt;Rapid Task-Solving in Novel Environments&lt;/a&gt;, ICLR 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/wikigraphs&#34;&gt;WikiGraphs: A Wikipedia - Knowledge Graph Paired Dataset&lt;/a&gt;, TextGraphs 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/box_arrangement&#34;&gt;Behavior Priors for Efficient Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/meshgraphnets&#34;&gt;Learning Mesh-Based Simulation with Graph Networks&lt;/a&gt;, ICLR 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/ogb_lsc&#34;&gt;Open Graph Benchmark - Large-Scale Challenge (OGB-LSC)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/synthetic_returns&#34;&gt;Synthetic Returns for Long-Term Credit Assignment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/galaxy_mergers&#34;&gt;A Deep Learning Approach for Characterizing Major Galaxy Mergers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/kfac_ferminet_alpha&#34;&gt;Better, Faster Fermionic Neural Networks&lt;/a&gt; (KFAC implementation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/object_attention_for_reasoning&#34;&gt;Object-based attention for spatio-temporal reasoning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer&#34;&gt;Effective gene expression prediction from sequence by integrating long-range interactions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/satore&#34;&gt;Satore: First-order logic saturation with atom rewriting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/nfnets&#34;&gt;Characterizing signal propagation to close the performance gap in unnormalized ResNets&lt;/a&gt;, ICLR 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/adversarial_robustness&#34;&gt;Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/cmtouch&#34;&gt;Learning rich touch representations through cross-modal self-supervision&lt;/a&gt;, CoRL 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/functional_regularisation_for_continual_learning&#34;&gt;Functional Regularisation for Continual Learning&lt;/a&gt;, ICLR 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/avae&#34;&gt;The Autoencoding Variational Autoencoder&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/mmv&#34;&gt;Self-Supervised MultiModal Versatile Networks&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/ode_gan&#34;&gt;ODE-GAN: Training GANs by Solving Ordinary Differential Equations&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/causal_reasoning&#34;&gt;Algorithms for Causal Reasoning in Probability Trees&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/gated_linear_networks&#34;&gt;Gated Linear Networks&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/himo&#34;&gt;Value-driven Hindsight Modelling&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/learned_free_energy_estimation&#34;&gt;Targeted free energy estimation via learned mappings&lt;/a&gt;, Journal of Chemical Physics 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/learning_to_simulate&#34;&gt;Learning to Simulate Complex Physics with Graph Networks&lt;/a&gt;, ICML 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/physics_planning_games&#34;&gt;Physically Embedded Planning Problems&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/polygen&#34;&gt;PolyGen: PolyGen: An Autoregressive Generative Model of 3D Meshes&lt;/a&gt;, ICML 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/byol&#34;&gt;Bootstrap Your Own Latent&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/catch_carry&#34;&gt;Catch &amp;amp; Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks&lt;/a&gt;, SIGGRAPH 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/memo&#34;&gt;MEMO: A Deep Network For Flexible Combination Of Episodic Memories&lt;/a&gt;, ICLR 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/rl_unplugged&#34;&gt;RL Unplugged: Benchmarks for Offline Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/geomancer&#34;&gt;Disentangling by Subspace Diffusion (GEOMANCER)&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/affordances_theory&#34;&gt;What can I do here? A theory of affordances in reinforcement learning&lt;/a&gt;, ICML 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/sketchy&#34;&gt;Scaling data-driven robotics with reward sketching and batch reinforcement learning&lt;/a&gt;, RSS 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/counterfactual_fairness&#34;&gt;Path-Specific Counterfactual Fairness&lt;/a&gt;, AAAI 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/option_keyboard&#34;&gt;The Option Keyboard: Combining Skills in Reinforcement Learning&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/visr&#34;&gt;VISR - Fast Task Inference with Variational Intrinsic Successor Features&lt;/a&gt;, ICLR 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/glassy_dynamics&#34;&gt;Unveiling the predictive power of static structure in glassy systems&lt;/a&gt;, Nature Physics 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/iodine&#34;&gt;Multi-Object Representation Learning with Iterative Variational Inference (IODINE)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/alphafold_casp13&#34;&gt;AlphaFold CASP13&lt;/a&gt;, Nature 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/unrestricted_advx&#34;&gt;Unrestricted Adversarial Challenge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/hierarchical_probabilistic_unet&#34;&gt;Hierarchical Probabilistic U-Net (HPU-Net)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/scratchgan&#34;&gt;Training Language GANs from Scratch&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/tvt&#34;&gt;Temporal Value Transport&lt;/a&gt;, Nature Communications 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/curl&#34;&gt;Continual Unsupervised Representation Learning (CURL)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/transporter&#34;&gt;Unsupervised Learning of Object Keypoints (Transporter)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/bigbigan&#34;&gt;BigBiGAN&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/cs_gan&#34;&gt;Deep Compressed Sensing&lt;/a&gt;, ICML 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/side_effects_penalties&#34;&gt;Side Effects Penalties&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/PrediNet&#34;&gt;PrediNet Architecture and Relations Game Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/unsupervised_adversarial_training&#34;&gt;Unsupervised Adversarial Training&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/graph_matching_networks&#34;&gt;Graph Matching Networks for Learning the Similarity of Graph Structured Objects&lt;/a&gt;, ICML 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/regal&#34;&gt;REGAL: Transfer Learning for Fast Optimization of Computation Graphs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/ensemble_loss_landscape&#34;&gt;Deep Ensembles: A Loss Landscape Perspective&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/powerpropagation&#34;&gt;Powerpropagation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/deepmind-research/master/physics_inspired_models&#34;&gt;Physics Inspired Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;This is not an official Google product.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tensorflow/examples</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/tensorflow/examples</id>
    <link href="https://github.com/tensorflow/examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TensorFlow examples&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TensorFlow Examples&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://www.tensorflow.org/images/tf_logo_social.png&#34;&gt;&#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Most important links!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/examples/master/community&#34;&gt;Community examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/examples/master/courses/udacity_deep_learning&#34;&gt;Course materials&lt;/a&gt; for the &lt;a href=&#34;https://www.udacity.com/course/deep-learning--ud730&#34;&gt;Deep Learning&lt;/a&gt; class on Udacity&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you are looking to learn TensorFlow, don&#39;t miss the &lt;a href=&#34;http://github.com/tensorflow/docs&#34;&gt;core TensorFlow documentation&lt;/a&gt; which is largely runnable code. Those notebooks can be opened in Colab from &lt;a href=&#34;https://tensorflow.org&#34;&gt;tensorflow.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;What is this repo?&lt;/h2&gt; &#xA;&lt;p&gt;This is the TensorFlow example repo. It has several classes of material:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Showcase examples and documentation for our fantastic &lt;a href=&#34;https://tensorflow.org/community&#34;&gt;TensorFlow Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Provide examples mentioned on TensorFlow.org&lt;/li&gt; &#xA; &lt;li&gt;Publish material supporting official TensorFlow courses&lt;/li&gt; &#xA; &lt;li&gt;Publish supporting material for the &lt;a href=&#34;https://blog.tensorflow.org&#34;&gt;TensorFlow Blog&lt;/a&gt; and &lt;a href=&#34;https://youtube.com/tensorflow&#34;&gt;TensorFlow YouTube Channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We welcome community contributions, see &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/examples/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; and, for style help, &lt;a href=&#34;https://www.tensorflow.org/community/contribute/docs_style&#34;&gt;Writing TensorFlow documentation&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;p&gt;To file an issue, use the tracker in the &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md&#34;&gt;tensorflow/tensorflow&lt;/a&gt; repo.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/examples/master/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>slundberg/shap</title>
    <updated>2022-06-01T01:44:35Z</updated>
    <id>tag:github.com,2022-06-01:/slundberg/shap</id>
    <link href="https://github.com/slundberg/shap" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A game theoretic approach to explain the output of any machine learning model.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_header.svg?sanitize=true&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/slundberg/shap/actions/workflows/run_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;example workflow&#34;&gt; &lt;a href=&#34;https://mybinder.org/v2/gh/slundberg/shap/master&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://shap.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/shap/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SHAP (SHapley Additive exPlanations)&lt;/strong&gt; is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see &lt;a href=&#34;https://raw.githubusercontent.com/slundberg/shap/master/#citations&#34;&gt;papers&lt;/a&gt; for details and citations).&lt;/p&gt; &#xA;&lt;!--**SHAP (SHapley Additive exPlanations)** is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see our [papers](#citations) for details and citations).--&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;SHAP can be installed from either &lt;a href=&#34;https://pypi.org/project/shap&#34;&gt;PyPI&lt;/a&gt; or &lt;a href=&#34;https://anaconda.org/conda-forge/shap&#34;&gt;conda-forge&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&#xA;pip install shap&#xA;&lt;i&gt;or&lt;/i&gt;&#xA;conda install -c conda-forge shap&#xA;&lt;/pre&gt; &#xA;&lt;h2&gt;Tree ensemble example (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)&lt;/h2&gt; &#xA;&lt;p&gt;While SHAP can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (see our &lt;a href=&#34;https://rdcu.be/b0z70&#34;&gt;Nature MI paper&lt;/a&gt;). Fast C++ implementations are supported for &lt;em&gt;XGBoost&lt;/em&gt;, &lt;em&gt;LightGBM&lt;/em&gt;, &lt;em&gt;CatBoost&lt;/em&gt;, &lt;em&gt;scikit-learn&lt;/em&gt; and &lt;em&gt;pyspark&lt;/em&gt; tree models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import xgboost&#xA;import shap&#xA;&#xA;# train an XGBoost model&#xA;X, y = shap.datasets.boston()&#xA;model = xgboost.XGBRegressor().fit(X, y)&#xA;&#xA;# explain the model&#39;s predictions using SHAP&#xA;# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)&#xA;explainer = shap.Explainer(model)&#xA;shap_values = explainer(X)&#xA;&#xA;# visualize the first prediction&#39;s explanation&#xA;shap.plots.waterfall(shap_values[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;616&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_waterfall.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue. Another way to visualize the same explanation is to use a force plot (these are introduced in our &lt;a href=&#34;https://rdcu.be/baVbR&#34;&gt;Nature BME paper&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# visualize the first prediction&#39;s explanation with a force plot&#xA;shap.plots.force(shap_values[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;811&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;If we take many force plot explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset (in the notebook this plot is interactive):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# visualize all the training set predictions&#xA;shap.plots.force(shap_values)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;811&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;To understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature&#39;s responsibility for a change in the model output, the plot below represents the change in predicted house price as RM (the average number of rooms per house in an area) changes. Vertical dispersion at a single value of RM represents interaction effects with other features. To help reveal these interactions we can color by another feature. If we pass the whole explanation tensor to the &lt;code&gt;color&lt;/code&gt; argument the scatter plot will pick the best feature to color by. In this case it picks RAD (index of accessibility to radial highways) since that highlights that the average number of rooms per house has less impact on home price for areas with a high RAD value.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# create a dependence scatter plot to show the effect of a single feature across the whole dataset&#xA;shap.plots.scatter(shap_values[:,&#34;RM&#34;], color=shap_values)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;544&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_scatter.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;To get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). This reveals for example that a high LSTAT (% lower status of the population) lowers the predicted home price.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# summarize the effects of all the features&#xA;shap.plots.beeswarm(shap_values)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;583&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_beeswarm.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;shap.plots.bar(shap_values)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;570&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_global_bar.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Natural language example (transformers)&lt;/h2&gt; &#xA;&lt;p&gt;SHAP has specific support for natural language models like those in the Hugging Face transformers library. By adding coalitional rules to traditional Shapley values we can form games that explain large modern NLP model using very few function evaluations. Using this functionality is as simple as passing a supported transformers pipeline to SHAP:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import transformers&#xA;import shap&#xA;&#xA;# load a transformers pipeline model&#xA;model = transformers.pipeline(&#39;sentiment-analysis&#39;, return_all_scores=True)&#xA;&#xA;# explain the model on two sample inputs&#xA;explainer = shap.Explainer(model) &#xA;shap_values = explainer([&#34;What a great movie! ...if you have no taste.&#34;])&#xA;&#xA;# visualize the first prediction&#39;s explanation for the POSITIVE output class&#xA;shap.plots.text(shap_values[0, :, &#34;POSITIVE&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;811&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/sentiment_analysis_plot.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Deep learning example with DeepExplainer (TensorFlow/Keras models)&lt;/h2&gt; &#xA;&lt;p&gt;Deep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with &lt;a href=&#34;https://arxiv.org/abs/1704.02685&#34;&gt;DeepLIFT&lt;/a&gt; described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. TensorFlow models and Keras models using the TensorFlow backend are supported (there is also preliminary support for PyTorch):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# ...include code from https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py&#xA;&#xA;import shap&#xA;import numpy as np&#xA;&#xA;# select a set of background examples to take an expectation over&#xA;background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]&#xA;&#xA;# explain predictions of the model on four images&#xA;e = shap.DeepExplainer(model, background)&#xA;# ...or pass tensors directly&#xA;# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)&#xA;shap_values = e.shap_values(x_test[1:5])&#xA;&#xA;# plot the feature attributions&#xA;shap.image_plot(shap_values, -x_test[1:5])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;820&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model&#39;s output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the &#39;zero&#39; image the blank middle is important, while for the &#39;four&#39; image the lack of a connection on top makes it a four instead of a nine.&lt;/p&gt; &#xA;&lt;h2&gt;Deep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)&lt;/h2&gt; &#xA;&lt;p&gt;Expected gradients combines ideas from &lt;a href=&#34;https://arxiv.org/abs/1703.01365&#34;&gt;Integrated Gradients&lt;/a&gt;, SHAP, and &lt;a href=&#34;https://arxiv.org/abs/1706.03825&#34;&gt;SmoothGrad&lt;/a&gt; into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras.applications.vgg16 import VGG16&#xA;from keras.applications.vgg16 import preprocess_input&#xA;import keras.backend as K&#xA;import numpy as np&#xA;import json&#xA;import shap&#xA;&#xA;# load pre-trained model and choose two images to explain&#xA;model = VGG16(weights=&#39;imagenet&#39;, include_top=True)&#xA;X,y = shap.datasets.imagenet50()&#xA;to_explain = X[[39,41]]&#xA;&#xA;# load the ImageNet class names&#xA;url = &#34;https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json&#34;&#xA;fname = shap.datasets.cache(url)&#xA;with open(fname) as f:&#xA;    class_names = json.load(f)&#xA;&#xA;# explain how the input to the 7th layer of the model explains the top two classes&#xA;def map2layer(x, layer):&#xA;    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))&#xA;    return K.get_session().run(model.layers[layer].input, feed_dict)&#xA;e = shap.GradientExplainer(&#xA;    (model.layers[7].input, model.layers[-1].output),&#xA;    map2layer(X, 7),&#xA;    local_smoothing=0 # std dev of smoothing noise&#xA;)&#xA;shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)&#xA;&#xA;# get the names for the classes&#xA;index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)&#xA;&#xA;# plot the explanations&#xA;shap.image_plot(shap_values, to_explain, index_names)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Predictions for two input images are explained in the plot above. Red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. By using &lt;code&gt;ranked_outputs=2&lt;/code&gt; we explain only the two most likely classes for each input (this spares us from explaining all 1,000 classes).&lt;/p&gt; &#xA;&lt;h2&gt;Model agnostic example with KernelExplainer (explains any function)&lt;/h2&gt; &#xA;&lt;p&gt;Kernel SHAP uses a specially-weighted local linear regression to estimate SHAP values for any model. Below is a simple example for explaining a multi-class SVM on the classic iris dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sklearn&#xA;import shap&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;# print the JS visualization code to the notebook&#xA;shap.initjs()&#xA;&#xA;# train a SVM classifier&#xA;X_train,X_test,Y_train,Y_test = train_test_split(*shap.datasets.iris(), test_size=0.2, random_state=0)&#xA;svm = sklearn.svm.SVC(kernel=&#39;rbf&#39;, probability=True)&#xA;svm.fit(X_train, Y_train)&#xA;&#xA;# use Kernel SHAP to explain test set predictions&#xA;explainer = shap.KernelExplainer(svm.predict_proba, X_train, link=&#34;logit&#34;)&#xA;shap_values = explainer.shap_values(X_test, nsamples=100)&#xA;&#xA;# plot the SHAP values for the Setosa output of the first instance&#xA;shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link=&#34;logit&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;810&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The above explanation shows four features each contributing to push the model output from the base value (the average model output over the training dataset we passed) towards zero. If there were any features pushing the class label higher they would be shown in red.&lt;/p&gt; &#xA;&lt;p&gt;If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset. This is exactly what we do below for all the examples in the iris test set:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# plot the SHAP values for the Setosa output of all instances&#xA;shap.force_plot(explainer.expected_value[0], shap_values[0], X_test, link=&#34;logit&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;813&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;SHAP Interaction Values&lt;/h2&gt; &#xA;&lt;p&gt;SHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented for tree models with &lt;code&gt;shap.TreeExplainer(model).shap_interaction_values(X)&lt;/code&gt;. This returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. These values often reveal interesting hidden relationships, such as how the increased risk of death peaks for men at age 60 (see the NHANES notebook for details):&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;483&#34; src=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Sample notebooks&lt;/h2&gt; &#xA;&lt;p&gt;The notebooks below demonstrate different use cases for SHAP. Look inside the notebooks directory of the repository if you want to try playing with the original notebooks yourself.&lt;/p&gt; &#xA;&lt;h3&gt;TreeExplainer&lt;/h3&gt; &#xA;&lt;p&gt;An implementation of Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html&#34;&gt;&lt;strong&gt;NHANES survival model with XGBoost and SHAP interaction values&lt;/strong&gt;&lt;/a&gt; - Using mortality data from 20 years of followup this notebook demonstrates how to use XGBoost and &lt;code&gt;shap&lt;/code&gt; to uncover complex risk factor relationships.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/tree_explainer/Census%20income%20classification%20with%20LightGBM.html&#34;&gt;&lt;strong&gt;Census income classification with LightGBM&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a gradient boosting tree model with LightGBM and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html&#34;&gt;&lt;strong&gt;League of Legends Win Prediction with XGBoost&lt;/strong&gt;&lt;/a&gt; - Using a Kaggle dataset of 180,000 ranked matches from League of Legends we train and explain a gradient boosting tree model with XGBoost to predict if a player will win their match.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;DeepExplainer&lt;/h3&gt; &#xA;&lt;p&gt;An implementation of Deep SHAP, a faster (but only approximate) algorithm to compute SHAP values for deep learning models that is based on connections between SHAP and the DeepLIFT algorithm.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html&#34;&gt;&lt;strong&gt;MNIST Digit classification with Keras&lt;/strong&gt;&lt;/a&gt; - Using the MNIST handwriting recognition dataset, this notebook trains a neural network with Keras and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html&#34;&gt;&lt;strong&gt;Keras LSTM for IMDB Sentiment Classification&lt;/strong&gt;&lt;/a&gt; - This notebook trains an LSTM with Keras on the IMDB text sentiment analysis dataset and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;GradientExplainer&lt;/h3&gt; &#xA;&lt;p&gt;An implementation of expected gradients to approximate SHAP values for deep learning models. It is based on connections between SHAP and the Integrated Gradients algorithm. GradientExplainer is slower than DeepExplainer and makes different approximation assumptions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.html&#34;&gt;&lt;strong&gt;Explain an Intermediate Layer of VGG16 on ImageNet&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain the output of a pre-trained VGG16 ImageNet model using an internal convolutional layer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LinearExplainer&lt;/h3&gt; &#xA;&lt;p&gt;For a linear model with independent features we can analytically compute the exact SHAP values. We can also account for feature correlation if we are willing to estimate the feature covariance matrix. LinearExplainer supports both of these options.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html&#34;&gt;&lt;strong&gt;Sentiment Analysis with Logistic Regression&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain a linear logistic regression sentiment analysis model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;KernelExplainer&lt;/h3&gt; &#xA;&lt;p&gt;An implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes no assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20scikit-learn.html&#34;&gt;&lt;strong&gt;Census income classification with scikit-learn&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a k-nearest neighbors classifier using scikit-learn and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html&#34;&gt;&lt;strong&gt;ImageNet VGG16 Model with Keras&lt;/strong&gt;&lt;/a&gt; - Explain the classic VGG16 convolutional nerual network&#39;s predictions for an image. This works by applying the model agnostic Kernel SHAP method to a super-pixel segmented image.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html&#34;&gt;&lt;strong&gt;Iris classification&lt;/strong&gt;&lt;/a&gt; - A basic demonstration using the popular iris species dataset. It explains predictions from six different models in scikit-learn using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation notebooks&lt;/h2&gt; &#xA;&lt;p&gt;These notebooks comprehensively demonstrate how to use specific functions and objects.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/plots/decision_plot.html&#34;&gt;&lt;code&gt;shap.decision_plot&lt;/code&gt; and &lt;code&gt;shap.multioutput_decision_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html&#34;&gt;&lt;code&gt;shap.dependence_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Methods Unified by SHAP&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;LIME:&lt;/em&gt; Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &#34;Why should i trust you?: Explaining the predictions of any classifier.&#34; Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Shapley sampling values:&lt;/em&gt; Strumbelj, Erik, and Igor Kononenko. &#34;Explaining prediction models and individual predictions with feature contributions.&#34; Knowledge and information systems 41.3 (2014): 647-665.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;DeepLIFT:&lt;/em&gt; Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. &#34;Learning important features through propagating activation differences.&#34; arXiv preprint arXiv:1704.02685 (2017).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;QII:&lt;/em&gt; Datta, Anupam, Shayak Sen, and Yair Zick. &#34;Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems.&#34; Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Layer-wise relevance propagation:&lt;/em&gt; Bach, Sebastian, et al. &#34;On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.&#34; PloS one 10.7 (2015): e0130140.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Shapley regression values:&lt;/em&gt; Lipovetsky, Stan, and Michael Conklin. &#34;Analysis of regression in game theory approach.&#34; Applied Stochastic Models in Business and Industry 17.4 (2001): 319-330.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Tree interpreter:&lt;/em&gt; Saabas, Ando. Interpreting random forests. &lt;a href=&#34;http://blog.datadive.net/interpreting-random-forests/&#34;&gt;http://blog.datadive.net/interpreting-random-forests/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;The algorithms and visualizations used in this package came primarily out of research in &lt;a href=&#34;https://suinlee.cs.washington.edu&#34;&gt;Su-In Lee&#39;s lab&lt;/a&gt; at the University of Washington, and Microsoft Research. If you use SHAP in your research we would appreciate a citation to the appropriate paper(s):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For general use of SHAP you can read/cite our &lt;a href=&#34;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&#34;&gt;NeurIPS paper&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/references/shap_nips.bib&#34;&gt;bibtex&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;For TreeExplainer you can read/cite our &lt;a href=&#34;https://www.nature.com/articles/s42256-019-0138-9&#34;&gt;Nature Machine Intelligence paper&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/references/tree_explainer.bib&#34;&gt;bibtex&lt;/a&gt;; &lt;a href=&#34;https://rdcu.be/b0z70&#34;&gt;free access&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;For GPUTreeExplainer you can read/cite &lt;a href=&#34;https://arxiv.org/abs/2010.13972&#34;&gt;this article&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For &lt;code&gt;force_plot&lt;/code&gt; visualizations and medical applications you can read/cite our &lt;a href=&#34;https://www.nature.com/articles/s41551-018-0304-0&#34;&gt;Nature Biomedical Engineering paper&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/slundberg/shap/master/docs/references/nature_bme.bib&#34;&gt;bibtex&lt;/a&gt;; &lt;a href=&#34;https://rdcu.be/baVbR&#34;&gt;free access&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img height=&#34;1&#34; width=&#34;1&#34; style=&#34;display:none&#34; src=&#34;https://www.facebook.com/tr?id=189147091855991&amp;amp;ev=PageView&amp;amp;noscript=1&#34;&gt;</summary>
  </entry>
</feed>