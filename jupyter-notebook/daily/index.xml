<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-13T01:32:36Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mickymult/RAG-Mistral7b</title>
    <updated>2023-11-13T01:32:36Z</updated>
    <id>tag:github.com,2023-11-13:/mickymult/RAG-Mistral7b</id>
    <link href="https://github.com/mickymult/RAG-Mistral7b" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RAG (Retrievel Augmented Generation) implementation using the Mistral-7B-Instruct-v0.1&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RAG-Mistral7b&lt;/h1&gt; &#xA;&lt;p&gt;RAG (Retrievel Augmented Generation) implementation using the &lt;strong&gt;Mistral-7B-Instruct-v0.1&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/mickymult/RAG-Mistral7b/assets/42827572/39e3a804-6667-4f7d-b8e9-aefc05140978&#34; alt=&#34;RAG-mistral&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Project Description&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains the implementation of the Retrieval Augmented Generation (RAG) model, using the newly released Mistral-7B-Instruct-v0.1 as the Language Model, SentenceTransformers for embedding, and llama-index for data ingestion, vectorization, and storage. The model has been implemented in a Google Colab notebook, optimized for a v100 instance.&lt;/p&gt; &#xA;&lt;p&gt;The implementation focuses on querying data from Amazon’s Annual Report for the fiscal year ended December 31, 2022. This enables the extraction of insightful information and knowledge encapsulated in the fiscal documents.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;RAG_testing_mistral7b.ipynb&lt;/code&gt; : The main Google Colab notebook containing the entire implementation and execution details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Google Colab with v100 instance.&lt;/li&gt; &#xA; &lt;li&gt;Knowledge on RAG, SentenceTransformers, and Mistral 7B models.&lt;/li&gt; &#xA; &lt;li&gt;Access token for HuggingFace (read)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository: &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/mickymult/RAG-Mistral7b.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Open the &lt;code&gt;RAG_testing_mistral7b.ipynb&lt;/code&gt; notebook in Google Colab.&lt;/li&gt; &#xA; &lt;li&gt;Set up the environment with the necessary libraries and dependencies.&lt;/li&gt; &#xA; &lt;li&gt;Create a new folder called &#34;data&#34; and store the Amazon 10k repport in it.&lt;/li&gt; &#xA; &lt;li&gt;Run the notebook cells in sequence.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Implementation Details&lt;/h2&gt; &#xA;&lt;h3&gt;1. &lt;strong&gt;Mistral 7B Model (LLM)&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This implementation utilizes Mistral 7B as the Large Language Model to generate human-like, coherent responses based on the retrieved documents.&lt;/p&gt; &#xA;&lt;h3&gt;2. &lt;strong&gt;SentenceTransformers (Embedding Model)&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;SentenceTransformers is used to create embeddings for the sentences, enabling efficient and semantic similarity search among them. &lt;strong&gt;all-mpnet-base-v2&lt;/strong&gt; pretrained model was used as it had the best performance.&lt;/p&gt; &#xA;&lt;h3&gt;3. &lt;strong&gt;llama-index (Data Ingestion, Vectorization, and Storage)&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;llama-index is employed for ingesting and vectorizing the dataset and for storing the vectorized representations of the data.&lt;/p&gt; &#xA;&lt;h3&gt;4. &lt;strong&gt;Data Source&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The data queried in this implementation is sourced from Amazon’s Annual Report for the fiscal year ended December 31, 2022.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Follow the instructions in the &lt;code&gt;RAG_testing_mistral7b.ipynb&lt;/code&gt; notebook to run the cells and execute the implementation on Google Colab.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The entire team at Mistral for the powerful language model.&lt;/li&gt; &#xA; &lt;li&gt;SentenceTransformers for the efficient embedding model.&lt;/li&gt; &#xA; &lt;li&gt;llama-index for data ingestion and vectorization.&lt;/li&gt; &#xA; &lt;li&gt;Amazon for the annual report data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;For any queries or discussions related to this implementation, feel free to raise an issue in this GitHub repository.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rahulnyk/knowledge_graph</title>
    <updated>2023-11-13T01:32:36Z</updated>
    <id>tag:github.com,2023-11-13:/rahulnyk/knowledge_graph</id>
    <link href="https://github.com/rahulnyk/knowledge_graph" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Convert any text to a graph of knowledge. This can be used for Graph Augmented Generation or Knowledge Graph based QnA&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Convert any Corpus of Text into a &lt;em&gt;Graph of Knowledge&lt;/em&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rahulnyk/knowledge_graph/main/assets/KG_banner.png&#34; alt=&#34;Knowledge Graph Banner&#34;&gt; &lt;em&gt;A knowledge graph generated using this code&lt;/em&gt; ghpages link of this graph: &lt;a href=&#34;https://rahulnyk.github.io/knowledge_graph/&#34;&gt;https://rahulnyk.github.io/knowledge_graph/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is a knowledge graph?&lt;/h2&gt; &#xA;&lt;p&gt;A knowledge graph, also known as a semantic network, represents a network of real-world entities—i.e. objects, events, situations, or concepts—and illustrates the relationship between them. This information is usually stored in a graph database and visualized as a graph structure, prompting the term knowledge “graph.”&lt;/p&gt; &#xA;&lt;p&gt;Source: &lt;a href=&#34;https://www.ibm.com/topics/knowledge-graph&#34;&gt;https://www.ibm.com/topics/knowledge-graph&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to create a simple knowledge graph from a body of work?&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clean the text corpus (The body of work).&lt;/li&gt; &#xA; &lt;li&gt;Extract concepts and entities from the body of work.&lt;/li&gt; &#xA; &lt;li&gt;Extract relations between the entities.&lt;/li&gt; &#xA; &lt;li&gt;Convert a graph schema.&lt;/li&gt; &#xA; &lt;li&gt;Populate nodes (concepts) and edges (relations).&lt;/li&gt; &#xA; &lt;li&gt;Visualise and Query.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Step 6 is purely optional, but it has certain artistic gratification associated with it. Network graphs are beautiful objects (just look at the banner image above, isn&#39;t it beautiful?). Fortunately, there are a good number of Python libraries available for generating graph visualisations.&lt;/p&gt; &#xA;&lt;h2&gt;Why Graph?&lt;/h2&gt; &#xA;&lt;p&gt;Once the Knowledge Graph (KG) is build, we can use it for many purposes. We can run graph algorithms and calculate centralities of any node, to understand how important a concept (node) is to this body of work. We can calculate communities to bunch the concepts together to better analyse the text. We can understand the connectedness between seemingly disconnected concepts.&lt;/p&gt; &#xA;&lt;p&gt;The best of all, we can achieve &lt;strong&gt;Graph Retrieval Augmented Generation (GRAG)&lt;/strong&gt; and chat with our text in a much more profound way using Graph as a retriever. This is a new and improved version of &lt;strong&gt;Retrieval Augmented Generation (RAG)&lt;/strong&gt; where we use a vectory db as a retriever to chat with our documents.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;This project&lt;/h2&gt; &#xA;&lt;p&gt;Here I have created a simple knowledge graph from a PDF document. The process I follow here is very similar to what is outlined in the above sections, with some simplifications.&lt;/p&gt; &#xA;&lt;p&gt;First I split the entire text into chunks. Then I extract concepts mentioned within each chunk using an LLM. Note that I am not extracting entities using an NER model here. There is a difference between concepts and entities. For example &#39;Bangalore&#39; is an entity, and &#39;Pleasant weather in Bangalore&#39; is a concept. In my experience, concepts make more meaningful KG than entities.&lt;/p&gt; &#xA;&lt;p&gt;I assume that the concepts that are mentioned in the vicinity of each other are related. So every edge in the KG is a text chunk in which the two connected concepts are mentioned.&lt;/p&gt; &#xA;&lt;p&gt;Once the nodes (concepts) and the edges (text chunks) are calculated, It is easy to create a graph out of them using the libraries mentioned here. All the components I used here are set up locally, so this project can be run very easily on a personal machine. I have adopted a no-GPT approach here to keep things economical. I am using the fantastic Mistral 7B openorca instruct, which crushes this use case wonderfully. The model can be set up locally using Ollama so generating the KG is basically free (No calls to GPT).&lt;/p&gt; &#xA;&lt;p&gt;To generate a graph this the notebook you have to tweak.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/rahulnyk/knowledge_graph/raw/main/extract_graph.ipynb&#34;&gt;extract_graph.ipynb&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The notebook implements the method outlined in the following flowchart.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/rahulnyk/knowledge_graph/main/assets/Method.png&#34;&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Split the corpus of text into chunks. Assign a chunk_id to each of these chunks.&lt;/li&gt; &#xA; &lt;li&gt;For every text chunk extract concepts and their semantic relationships using an LLM. Let’s assign this relation a weightage of W1. There can be multiple relationships between the same pair of concepts. Every such relation is an edge between a pair of concepts.&lt;/li&gt; &#xA; &lt;li&gt;Consider that the concepts that occur in the same text chunk are also related by their contextual proximity. Let’s assign this relation a weightage of W2. Note that the same pair of concepts may occur in multiple chunks.&lt;/li&gt; &#xA; &lt;li&gt;Group similar pairs, sum their weights, and concatenate their relationships. So now we have only one edge between any distinct pair of concepts. The edge has a certain weight and a list of relations as its name.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Additional it also calculates the Degree of each node, and Communities of nodes, for sizing and coloring the nodes in the graph respectively.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://medium.com/towards-data-science/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a&#34;&gt;Here is a Medium article explaining the method in detail &lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Tech Stack&lt;/h2&gt; &#xA;&lt;h3&gt;Mistral 7B&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mistral.ai/news/announcing-mistral-7b/&#34;&gt;&lt;img src=&#34;https://mistral.ai/images/logo_hubc88c4ece131b91c7cb753f40e9e1cc5_2589_256x0_resize_q97_h2_lanczos_3.webp&#34; height=&#34;50&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;I am using the &lt;a href=&#34;https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca&#34;&gt;Mistral 7B Openorca&lt;/a&gt; for extracting concepts out of text chunks. It can follow the system prompt instructions very well.&lt;/p&gt; &#xA;&lt;h3&gt;Ollama&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ollama.ai&#34;&gt;&lt;img src=&#34;https://github.com/jmorganca/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7 &#34; height=&#34;50&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ollama makes it easy to host any model locally. Mistral 7B OpenOrca version is already available with Ollama to use out of the box.&lt;/p&gt; &#xA;&lt;h3&gt;Pandas&lt;/h3&gt; &#xA;&lt;p&gt;dataframes for graph schema (can use a graphdb at a later stage).&lt;/p&gt; &#xA;&lt;h3&gt;NetworkX&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://networkx.org&#34;&gt;&lt;img src=&#34;https://networkx.org/_static/networkx_logo.svg?sanitize=true&#34; height=&#34;50&#34;&gt;&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a python library that makes dealing with graphs super easy&lt;/p&gt; &#xA;&lt;h3&gt;Pyvis&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/WestHealth/pyvis/tree/master&#34;&gt;Pyvis python library&lt;/a&gt; for visualisation. Pyvis generates Javascript Graph visualisations using python, so the final graphs can be hosted on the web. For example the &lt;a href=&#34;https://rahulnyk.github.io/knowledge_graph/&#34;&gt;github link of this repo&lt;/a&gt; is a graph generated by pyvis&lt;/p&gt; &#xA;&lt;h1&gt;Looking for contributions&lt;/h1&gt; &#xA;&lt;p&gt;This project needs a lot more work. There are some wonderful ideas suggested by folks on medium and here on Github. If this interests you, Please join hands and lets&#39; build this together. Here are a few suggested imrpovements.&lt;/p&gt; &#xA;&lt;h3&gt;Back End&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Use embeddings to deduplicate semantically similar concepts (&lt;strong&gt;Suggested by William Claude on the &lt;a href=&#34;https://medium.com/towards-data-science/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a&#34;&gt;Medium Article&lt;/a&gt;&lt;/strong&gt;)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Avoid having similar concepts written differently by the LLM (eg: &#34;doctor&#34; and &#34;doctors&#34;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Reinforce the clustering of strongly similar concepts (eg: &#34;doctor&#34; and &#34;medical practitioner&#34;)?&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Filter out the redundant, or outlier concepts that may not be useful in understanding the text. For example, generic concepts that occur too often in the text. (&lt;strong&gt;Suggested by Luke Chesley&lt;/strong&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Better implement the concept of contextual proximity to avoide overweighting certain concepts that occur too frequently, or to weed out useless edges. (&lt;strong&gt;Suggested by Luke Chesley&lt;/strong&gt;)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Front End&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create a Frontend for rendering Graph of Concepts in a more useful way. for example here is a flow. (&lt;strong&gt;Suggested by David Garcia on the &lt;a href=&#34;https://medium.com/towards-data-science/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a&#34;&gt;Medium Article&lt;/a&gt;&lt;/strong&gt;). &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Provide a list concept/interest/topics&lt;/li&gt; &#xA;   &lt;li&gt;User selects what they&#39;re interested in&lt;/li&gt; &#xA;   &lt;li&gt;This expands to show sub-topics, sub-concepts, sub-x, etc.&lt;/li&gt; &#xA;   &lt;li&gt;This is how you get deep into a specialty&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>mickymult/RAG-ChromaDB-Mistral7B</title>
    <updated>2023-11-13T01:32:36Z</updated>
    <id>tag:github.com,2023-11-13:/mickymult/RAG-ChromaDB-Mistral7B</id>
    <link href="https://github.com/mickymult/RAG-ChromaDB-Mistral7B" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RAG (Retrievel Augmented Generation) implementation using ChromaDB, Mistral-7B-Instruct-v0.1 and gte-base for embeddings.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RAG-ChromaDB-Mistral7B&lt;/h1&gt; &#xA;&lt;p&gt;RAG (Retrievel Augmented Generation) implementation using ChromaDB, Mistral-7B-Instruct-v0.1 and gte-base for embeddings.&lt;/p&gt; &#xA;&lt;h1&gt;RAG Implementation with Mistral 7B and ChromaDB: Readme&lt;/h1&gt; &#xA;&lt;h2&gt;Project Description&lt;/h2&gt; &#xA;&lt;p&gt;This repository hosts the implementation of a sophisticated Retrieval Augmented Generation (RAG) model, leveraging the cutting-edge Mistral 7B model for Language Generation. It utilizes the gte-base model for embedding and ChromaDB as the vector database to store these embeddings. This project is embodied in a Google Colab notebook, fine-tuned for an A100 instance.&lt;/p&gt; &#xA;&lt;p&gt;The implementation queries data from the “Climate Change 2023 Synthesis Report,” allowing for the extraction of in-depth, coherent, and relevant information pertaining to climate change. With a context window of 8000, the results demonstrate impressive coherence, precise data match-retrieval, and low latency, making it a valuable tool for processing extensive datasets.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;RAG_Chromadb_mistral7b.ipynb&lt;/code&gt; : The main Google Colab notebook containing the complete implementation and execution details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Google Colab with A100 instance.&lt;/li&gt; &#xA; &lt;li&gt;Familiarity with RAG, gte-base model, Mistral 7B, and ChromaDB.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository: &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/mickymult/RAG-ChromaDB-Mistral7B.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Open the &lt;code&gt;RAG_Chromadb_mistral7b.ipynb&lt;/code&gt; notebook in Google Colab.&lt;/li&gt; &#xA; &lt;li&gt;Set up the environment with the necessary libraries and dependencies.&lt;/li&gt; &#xA; &lt;li&gt;Run the notebook cells in sequence.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Implementation Details&lt;/h2&gt; &#xA;&lt;h3&gt;1. &lt;strong&gt;Mistral-7B-Instruct-v0.1 (LLM)&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Mistral 7B serves as the foundational Language Model, producing coherent, contextually relevant responses based on retrieved documents.&lt;/p&gt; &#xA;&lt;h3&gt;2. &lt;strong&gt;gte-base Model (Embedding Model)&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The gte-base model is used for embedding sentences, which facilitates efficient and semantically rich similarity search among them.&lt;/p&gt; &#xA;&lt;h3&gt;3. &lt;strong&gt;ChromaDB (Vector Database)&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;ChromaDB is used as the vector database for storing the embedded representations of the data, ensuring efficient data retrieval.&lt;/p&gt; &#xA;&lt;h3&gt;4. &lt;strong&gt;Data Source&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The implementation is based on the “Climate Change 2023 Synthesis Report,” enabling detailed inquiry into the comprehensive insights on climate change covered in the report.&lt;/p&gt; &#xA;&lt;h3&gt;5. &lt;strong&gt;Enhanced Context Window&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The context window has been increased to 8000 tokens to allow for more extensive contextual understanding and coherence in the generated responses.&lt;/p&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;The implementation yields highly coherent responses with accurate data match-retrieval and minimal latency, demonstrating its effectiveness in handling extensive and complex datasets like the Climate Change 2023 Synthesis Report.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Execute the implementation by running the cells in sequence in the &lt;code&gt;RAG_Chromadb_mistral7b.ipynb&lt;/code&gt; notebook on Google Colab.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Feel free to contribute to the enhancement of this implementation.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is distributed under the MIT License.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mistral 7B for providing the advanced language model.&lt;/li&gt; &#xA; &lt;li&gt;gte-base for the robust embedding model.&lt;/li&gt; &#xA; &lt;li&gt;ChromaDB for efficient vector database storage.&lt;/li&gt; &#xA; &lt;li&gt;The authors and contributors to the Climate Change 2023 Synthesis Report.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;For any inquiries, discussions, or clarifications related to this implementation, please create an issue in this GitHub repository.&lt;/p&gt;</summary>
  </entry>
</feed>