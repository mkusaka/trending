<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-26T01:32:14Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nexusflowai/NexusRaven-V2</title>
    <updated>2023-12-26T01:32:14Z</updated>
    <id>tag:github.com,2023-12-26:/nexusflowai/NexusRaven-V2</id>
    <link href="https://github.com/nexusflowai/NexusRaven-V2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NexusRaven-V2&lt;/h1&gt; &#xA;&lt;h3&gt;Pushing The Boundaries of Open Source Function Calling Models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/CODE_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/Model%20License-LLaMa-red.svg?sanitize=true&#34; alt=&#34;Model License&#34;&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-3100/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.10+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.10+&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://huggingface.co/Nexusflow&#34; target=&#34;_blank&#34;&gt;Nexusflow HF&lt;/a&gt; - &lt;a href=&#34;https://discord.gg/HDSVmNAs3y&#34; target=&#34;_blank&#34;&gt;Nexusflow Discord&lt;/a&gt; - &lt;a href=&#34;http://nexusflow.ai/blogs/ravenv2&#34; target=&#34;_blank&#34;&gt;NexusRaven-V2 blog post&lt;/a&gt; - &lt;a href=&#34;https://colab.research.google.com/drive/19JYixRPPlanmW5q49WYi_tU8rhHeCEKW?usp=sharing&#34; target=&#34;_blank&#34;&gt;Prompting Notebook CoLab&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/spaces/Nexusflow/Nexus_Function_Calling_Leaderboard&#34; target=&#34;_blank&#34;&gt;Leaderboard&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/spaces/Nexusflow/NexusRaven-V2-Demo&#34; target=&#34;_blank&#34;&gt;Read-World Demo&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/Nexusflow/NexusRaven-V2-13B&#34; target=&#34;_blank&#34;&gt;NexusRaven-V2-13B HF&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/imgs/raven.png&#34; alt=&#34;NexusRaven&#34; style=&#34;width: 40%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Welcome to NexusRaven-V2! We want to provide the evaluation data, evaluation code, and a prompting guide to help you reproduce our results, as well as use NexusRaven-V2 to the best of its ability!&lt;/p&gt; &#xA;&lt;h2&gt;Introducing NexusRaven-V2&lt;/h2&gt; &#xA;&lt;p&gt;NexusRaven-V2 is an open-source and commercially viable function calling LLM that surpasses the state-of-the-art in function calling capabilities. It has 13B parameters. NexusRaven-V2 is capable of doing single function calls, parallel function calls (where multiple &#34;disconnected&#34; function calls are necessary to answer the user query), and nested function calls (where the argument for the necessary function requires a chain of other function calls).&lt;/p&gt; &#xA;&lt;p&gt;ğŸ’ª &lt;strong&gt;Versatile Function Calling Capability&lt;/strong&gt;: NexusRaven-V2 is capable of generating single function calls, nested calls, and parallel calls in many challenging cases.&lt;/p&gt; &#xA;&lt;p&gt;ğŸ¤“ &lt;strong&gt;Fully Explainable&lt;/strong&gt;: NexusRaven-V2 is capable of generating very detailed explanations for the function calls it generates. This behavior can be turned off, to save tokens during inference.&lt;/p&gt; &#xA;&lt;p&gt;ğŸ“Š &lt;strong&gt;Performance Highlights&lt;/strong&gt;: NexusRaven-V2 surpasses GPT-4 by up to 7% in function calling success rates in human-generated use cases involving nested and composite functions.&lt;/p&gt; &#xA;&lt;p&gt;ğŸ”§ &lt;strong&gt;Generalization to the Unseen&lt;/strong&gt;: NexusRaven-V2 has never been trained on the functions used in evaluation.&lt;/p&gt; &#xA;&lt;p&gt;ğŸ”¥ &lt;strong&gt;Commercially Permissive&lt;/strong&gt;: The training of NexusRaven-V2 does not involve any data generated by proprietary LLMs such as GPT-4. You have full control of the model when deployed in commercial applications.&lt;/p&gt; &#xA;&lt;p&gt;Please checkout the following links!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/19JYixRPPlanmW5q49WYi_tU8rhHeCEKW?usp=sharing&#34;&gt;Prompting Notebook CoLab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/Nexusflow/Nexus_Function_Calling_Leaderboard&#34;&gt;Evaluation Leaderboard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/Nexusflow/NexusRaven-V2-Demo&#34;&gt;NexusRaven-V2 Real-World Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;NexusRaven-V2 model usage&lt;/h2&gt; &#xA;&lt;p&gt;NexusRaven-V2 accepts a list of python functions. These python functions can do anything (including sending GET/POST requests to external APIs!). The two requirements include the python function signature and the appropriate docstring to generate the function call.&lt;/p&gt; &#xA;&lt;h3&gt;NexusRaven-V2&#39;s Capabilities&lt;/h3&gt; &#xA;&lt;p&gt;NexusRaven-V2 is capable of generating deeply nested function calls, parallel function calls, and simple single calls. It can also justify the function calls it generated. If you would like to generate the call only, please set a stop criteria of &#34;&amp;lt;bot_end&amp;gt;&#34;. Otherwise, please allow NexusRaven-V2 to run until its stop token (i.e. &#34;&amp;lt;/s&amp;gt;&#34;).&lt;/p&gt; &#xA;&lt;h3&gt;Quick Start Prompting Guide&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to our notebook, &lt;a href=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/How-To-Prompt.ipynb&#34;&gt;How-To-Prompt.ipynb&lt;/a&gt;, for more advanced tutorials on using NexusRaven-V2!&lt;/p&gt; &#xA;&lt;h3&gt;Quickstart&lt;/h3&gt; &#xA;&lt;p&gt;You can run the model on a GPU using the following code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Please `pip install transformers accelerate`&#xA;from transformers import pipeline&#xA;&#xA;&#xA;pipeline = pipeline(&#xA;    &#34;text-generation&#34;,&#xA;    model=&#34;Nexusflow/NexusRaven-V2-13B&#34;,&#xA;    torch_dtype=&#34;auto&#34;,&#xA;    device_map=&#34;auto&#34;,&#xA;)&#xA;&#xA;prompt_template = \&#xA;&#39;&#39;&#39;&#xA;Function:&#xA;def get_weather_data(coordinates):&#xA;    &#34;&#34;&#34;&#xA;    Fetches weather data from the Open-Meteo API for the given latitude and longitude.&#xA;&#xA;    Args:&#xA;    coordinates (tuple): The latitude of the location.&#xA;&#xA;    Returns:&#xA;    float: The current temperature in the coordinates you&#39;ve asked for&#xA;    &#34;&#34;&#34;&#xA;&#xA;Function:&#xA;def get_coordinates_from_city(city_name):&#xA;    &#34;&#34;&#34;&#xA;    Fetches the latitude and longitude of a given city name using the Maps.co Geocoding API.&#xA;&#xA;    Args:&#xA;    city_name (str): The name of the city.&#xA;&#xA;    Returns:&#xA;    tuple: The latitude and longitude of the city.&#xA;    &#34;&#34;&#34;&#xA;&#xA;User Query: {query}&amp;lt;human_end&amp;gt;&#xA;&#xA;&#39;&#39;&#39;&#xA;&#xA;prompt = prompt_template.format(query=&#34;What&#39;s the weather like in Seattle right now?&#34;)&#xA;&#xA;result = pipeline(prompt, max_new_tokens=2048, return_full_text=False, do_sample=False, temperature=0.001)[0][&#34;generated_text&#34;]&#xA;print (result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should generate the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Call: get_weather_data(coordinates=get_coordinates_from_city(city_name=&#39;Seattle&#39;))&amp;lt;bot_end&amp;gt;&#xA;Thought: The function call `get_weather_data(coordinates=get_coordinates_from_city(city_name=&#39;Seattle&#39;))` answers the question &#34;What&#39;s the weather like in Seattle right now?&#34; by following these steps:&#xA;&#xA;1. `get_coordinates_from_city(city_name=&#39;Seattle&#39;)`: This function call fetches the latitude and longitude of the city &#34;Seattle&#34; using the Maps.co Geocoding API.&#xA;2. `get_weather_data(coordinates=...)`: This function call fetches the current weather data for the coordinates returned by the previous function call.&#xA;&#xA;Therefore, the function call `get_weather_data(coordinates=get_coordinates_from_city(city_name=&#39;Seattle&#39;))` answers the question &#34;What&#39;s the weather like in Seattle right now?&#34; by first fetching the coordinates of the city &#34;Seattle&#34; and then fetching the current weather data for those coordinates.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you would like to prevent the generation of the explanation of the function call (for example, to save on inference tokens), please set a stopping criteria of &#34;&amp;lt;bot_end&amp;gt;&#34;.&lt;/p&gt; &#xA;&lt;p&gt;Please follow this prompting template to maximize the performance of RavenV2.&lt;/p&gt; &#xA;&lt;h3&gt;Using with OpenAI FC Schematics&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nexusflowai/nexusraven-pip&#34;&gt;If you currently have a workflow that is built around OpenAI&#39;s function calling and you want to try NexusRaven-V2, we have a package that helps you drop in NexusRaven-V2.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please give it a try!&lt;/p&gt; &#xA;&lt;h2&gt;NexusRaven-V2 on Nexus Function Calling Benchmark&lt;/h2&gt; &#xA;&lt;h3&gt;Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;We curated 9 tasks within the Nexus Function Calling Benchmark specifically around function calling. We release 8 of them, but keep one internally to avoid situations where new community models overfit to these benchmarks.&lt;/p&gt; &#xA;&lt;p&gt;They are based on real-world APIs. Here are the 8 datasets for evaluation:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/Function_Call_Definitions&#34;&gt;The function definitions for all benchmarks can be found here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/NVDLibraryBenchmark&#34; target=&#34;_blank&#34;&gt; NVDLibrary &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/VirusTotalBenchmark&#34; target=&#34;_blank&#34;&gt; VirusTotal &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/OTXAPIBenchmark&#34; target=&#34;_blank&#34;&gt; OTX &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/PlacesAPIBenchmark&#34; target=&#34;_blank&#34;&gt; Places API &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/ClimateAPIBenchmark&#34; target=&#34;_blank&#34;&gt; Climate API &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/VirusTotalMultiple&#34; target=&#34;_blank&#34;&gt; VirusTotal-Parallel Calls &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/VirusTotalMultiple&#34; target=&#34;_blank&#34;&gt; VirusTotal-Nested Calls &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/Nexusflow/CVECPEAPIBenchmark&#34; target=&#34;_blank&#34;&gt; NVDLibrary-Nested Calls &lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We report accuracy on The Stack API but do not release the data. We keep it internal as a means of benchmarking new models to ensure generalizability.&lt;/p&gt; &#xA;&lt;h3&gt;Benchmark Classifications&lt;/h3&gt; &#xA;&lt;p&gt;We classify the benchmarks above into three categories, based on the type of API usage they require to get the answer correctly. These include: single calls, parallel calls, and nested calls.&lt;/p&gt; &#xA;&lt;h4&gt;Single Calls&lt;/h4&gt; &#xA;&lt;p&gt;Single Calls are defined as API usage that only require a single function call. Here is an example of this:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/imgs/singleapi.png&#34; alt=&#34;singleapi_example&#34; style=&#34;width: 100%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4&gt;Nested Calls&lt;/h4&gt; &#xA;&lt;p&gt;Nested Calls are defined as API usage that require multiple calls all at once to get the right answer, where the argument for the outer call depends on the call graph. Here is an example of this:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/imgs/multiapi.png&#34; alt=&#34;multiapi_example&#34; style=&#34;width: 100%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4&gt;Parallel Calls&lt;/h4&gt; &#xA;&lt;p&gt;Parallel calls are when seperate calls are generated, but they do not connect into each other.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/imgs/parallelapi.png&#34; alt=&#34;parallelapi_example&#34; style=&#34;width: 100%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4&gt;Task Classification&lt;/h4&gt; &#xA;&lt;p&gt;We classify the tasks within Nexus Function Calling Benchmark into the following categories:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;API&lt;/th&gt; &#xA;    &lt;th&gt;Category&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;OTX&lt;/td&gt; &#xA;    &lt;td&gt;Single Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;NVDLibrary&lt;/td&gt; &#xA;    &lt;td&gt;Single Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;VirusTotal&lt;/td&gt; &#xA;    &lt;td&gt;Single Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;VirusTotal-Nested Calls&lt;/td&gt; &#xA;    &lt;td&gt;Nested Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Climate&lt;/td&gt; &#xA;    &lt;td&gt;Nested and Parallel Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;VirusTotal-Parallel Calls&lt;/td&gt; &#xA;    &lt;td&gt;Parallel Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Places API&lt;/td&gt; &#xA;    &lt;td&gt;Nested Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;NVDLibrary-Nested Calls&lt;/td&gt; &#xA;    &lt;td&gt;Nested Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;The Stack API&lt;/td&gt; &#xA;    &lt;td&gt;Mostly Single Calls&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;NexusRaven-V2 Capability Breakdown&lt;/h3&gt; &#xA;&lt;p&gt;We benchmark NexusRaven-V2 against GPT4 primarily. Our benchmarking effort consists of several categories of benchmarks. This includes: single calls, parallel calls, and nested calls.&lt;/p&gt; &#xA;&lt;h4&gt;Capability Performance&lt;/h4&gt; &#xA;&lt;p&gt;The performance of NexusRaven-V2 on each category is reflected below:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/docs/radar-2.png&#34; alt=&#34;NexusRaven&#34; style=&#34;width: 80%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Overall Scores&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/docs/blog2-fc.png&#34; alt=&#34;Scores&#34; style=&#34;width: 80%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Reproducing The Results&lt;/h3&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/evaluation_notebook/Benchmark_Models.ipynb&#34;&gt;Benchmarking Notebook&lt;/a&gt; for reproducing Raven&#39;s results. The cells have already been run and the output has been recorded in the notebook itself. The output of each cell is what we report in the leaderboard. However, you are welcome to rerun the cells to verify. The NexusRaven-V2 endpoint will be accessible for this effort, and the link is present in the notebook itself.&lt;/p&gt; &#xA;&lt;p&gt;NexusRaven-V2&#39;s benchmark performance should be identical to the reported accuracy in leaderboard.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT NOTE:&lt;/strong&gt; However, it is currently impossible to generate deterministic outputs for GPT4, so we see WILD swings in the per-task accuracy for GPT4 (but, the average accuracy across task categories are relatively stable within a few percent). Please note this when rerunning the GPT4 cells in the notebook.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code in this repository for running the NexusRaven-V2 model, the evaluation notebook, the prompting notebook, and the evaluation data are licensed under &lt;a href=&#34;https://raw.githubusercontent.com/nexusflowai/NexusRaven-V2/master/CODE_LICENSE&#34;&gt;Apache 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;We thank the CodeLLaMa Team for their great foundational models that made NexusRaven-V2 possible.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{roziÃ¨re2023code,&#xA;      title={Code Llama: Open Foundation Models for Code}, &#xA;      author={Baptiste RoziÃ¨re and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Tal Remez and JÃ©rÃ©my Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre DÃ©fossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},&#xA;      year={2023},&#xA;      eprint={2308.12950},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{nexusraven,&#xA;      title={NexusRaven-V2: Surpassing GPT-4 for Zero-shot Function Calling},&#xA;      author={Nexusflow.ai team},&#xA;      year={2023},&#xA;      url={https://nexusflow.ai/blogs/ravenv2}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Please join our &lt;a href=&#34;https://discord.gg/HDSVmNAs3y&#34;&gt;Discord Channel&lt;/a&gt; to reach out for any issues and comments!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ytzfhqs/AAAMLP-CN</title>
    <updated>2023-12-26T01:32:14Z</updated>
    <id>tag:github.com,2023-12-26:/ytzfhqs/AAAMLP-CN</id>
    <link href="https://github.com/ytzfhqs/AAAMLP-CN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Approaching (Almost) Any Machine Learning Problemä¸­è¯‘ç‰ˆï¼Œåœ¨çº¿æ–‡æ¡£åœ°å€ï¼šhttps://ytzfhqs.github.io/AAAMLP-CN/&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AAAMLP-CN&lt;/h1&gt; &#xA;&lt;h2&gt;æ–°ç‰¹æ€§&lt;/h2&gt; &#xA;&lt;h3&gt;2023.10.25&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ˜å®Œæˆå…¨éƒ¨ç¿»è¯‘&lt;/li&gt; &#xA; &lt;li&gt;ğŸ“è®¡åˆ’å¯¹kaggleæ¸¸ä¹å›­ç³»åˆ—ä¼˜ç§€è§£å†³æ–¹æ¡ˆä»£ç è¿›è¡Œè§£æ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2023.09.07&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;âš¡ä¿®æ­£éƒ¨åˆ†å·²çŸ¥æ–‡å­—é”™è¯¯å’Œä»£ç é”™è¯¯&lt;/li&gt; &#xA; &lt;li&gt;ğŸ¤—æ·»åŠ &lt;a href=&#34;https://ytzfhqs.github.io/AAAMLP-CN/&#34;&gt;åœ¨çº¿æ–‡æ¡£&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç®€ä»‹&lt;/h2&gt; &#xA;&lt;p&gt;Abhishek Thakurï¼Œå¾ˆå¤š kaggler å¯¹ä»–éƒ½éå¸¸ç†Ÿæ‚‰ï¼Œ2017 å¹´ï¼Œä»–åœ¨ Linkedin å‘è¡¨äº†ä¸€ç¯‡åä¸º&lt;strong&gt;Approaching (Almost) Any Machine Learning Problem&lt;/strong&gt;çš„æ–‡ç« ï¼Œä»‹ç»ä»–å»ºç«‹çš„ä¸€ä¸ªè‡ªåŠ¨çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œå‡ ä¹å¯ä»¥è§£å†³ä»»ä½•æœºå™¨å­¦ä¹ é—®é¢˜ï¼Œè¿™ç¯‡æ–‡ç« æ›¾ç«é Kaggleã€‚&lt;/p&gt; &#xA;&lt;p&gt;Abhishek åœ¨ Kaggle ä¸Šçš„æˆå°±ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Competitions Grandmasterï¼ˆ17 æšé‡‘ç‰Œï¼Œä¸–ç•Œæ’åç¬¬ 3ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;Kernels Expert ï¼ˆKagglers æ’åå‰ 1ï¼…ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;Discussion Grandmasterï¼ˆ65 æšé‡‘ç‰Œï¼Œä¸–ç•Œæ’åç¬¬ 2ï¼‰&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ç›®å‰ï¼ŒAbhishek åœ¨æŒªå¨ boost å…¬å¸æ‹…ä»»é¦–å¸­æ•°æ®ç§‘å­¦å®¶çš„èŒä½ï¼Œè¿™æ˜¯ä¸€å®¶ä¸“é—¨ä»äº‹ä¼šè¯äººå·¥æ™ºèƒ½çš„è½¯ä»¶å…¬å¸ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æœ¬æ–‡å¯¹&lt;strong&gt;Approaching (Almost) Any Machine Learning Problem&lt;/strong&gt;è¿›è¡Œäº†&lt;strong&gt;ä¸­æ–‡ç¿»è¯‘&lt;/strong&gt;ï¼Œç”±äºæœ¬äººæ°´å¹³æœ‰é™ï¼Œä¸”æœªä½¿ç”¨æœºå™¨ç¿»è¯‘ï¼Œå¯èƒ½æœ‰éƒ¨åˆ†è¨€è¯­ä¸é€šé¡ºæˆ–æœ¬åœŸåŒ–ç¨‹åº¦ä¸è¶³ï¼Œä¹Ÿè¯·å¤§å®¶åœ¨é˜…è¯»è¿‡ç¨‹ä¸­å¤šæä¾›å®è´µæ„è§ã€‚å¦é™„ä¸Šä¹¦ç±åŸ&lt;a href=&#34;https://github.com/abhishekkrthakur/approachingalmost&#34;&gt;é¡¹ç›®åœ°å€&lt;/a&gt;ï¼Œ&lt;strong&gt;è½¬è½½è¯·ä¸€å®šæ ‡æ˜å‡ºå¤„ï¼&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®&lt;strong&gt;æ”¯æŒåœ¨çº¿é˜…è¯»&lt;/strong&gt;ï¼Œæ–¹ä¾¿æ‚¨éšæ—¶éšåœ°è¿›è¡ŒæŸ¥é˜…ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å› ä¸ºæœ‰å‡ ç« å†…å®¹å¤ªè¿‡åŸºç¡€ï¼Œæ‰€ä»¥æœªè¿›è¡Œç¿»è¯‘ï¼Œè¯¦ç»†æƒ…å†µè¯·å‚ç…§ä¹¦ç±ç›®å½•ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;å‡†å¤‡ç¯å¢ƒï¼ˆå·²ç¿»è¯‘ï¼‰&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;æ— ç›‘ç£å’Œæœ‰ç›‘ç£å­¦ä¹ ï¼ˆå·²ç¿»è¯‘ï¼‰&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;äº¤å‰æ£€éªŒï¼ˆå·²ç¿»è¯‘ï¼‰&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;è¯„ä¼°æŒ‡æ ‡ï¼ˆå·²ç¿»è¯‘ï¼‰&lt;/strong&gt; -&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ç»„ç»‡æœºå™¨å­¦ä¹ ï¼ˆå·²ç¿»è¯‘ï¼‰&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;å¤„ç†åˆ†ç±»å˜é‡ï¼ˆå·²ç¿»è¯‘ï¼‰&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ç‰¹å¾å·¥ç¨‹ï¼ˆå·²ç¿»è¯‘ï¼‰&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ç‰¹å¾é€‰æ‹©ï¼ˆå·²ç¿»è¯‘ï¼‰&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;è¶…å‚æ•°ä¼˜åŒ–ï¼ˆå·²ç¿»è¯‘ï¼‰&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;å›¾åƒåˆ†ç±»å’Œåˆ†å‰²æ–¹æ³•ï¼ˆå·²ç¿»è¯‘ï¼‰&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;æ–‡æœ¬åˆ†ç±»æˆ–å›å½’æ–¹æ³•ï¼ˆå·²ç¿»è¯‘ï¼‰&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ç»„åˆå’Œå †å æ–¹æ³•ï¼ˆå·²ç¿»è¯‘ï¼‰&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;å¯é‡å¤ä»£ç å’Œæ¨¡å‹æ–¹æ³•ï¼ˆå·²ç¿»è¯‘ï¼‰&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;æˆ‘å°†ä¼šæŠŠå®Œæ•´çš„ç¿»è¯‘ç‰ˆ &lt;code&gt;Markdown&lt;/code&gt; æ–‡ä»¶ä¸Šä¼ åˆ° GitHubï¼Œä»¥ä¾›å¤§å®¶å…è´¹ä¸‹è½½å’Œé˜…è¯»ã€‚ä¸ºäº†æœ€ä½³çš„é˜…è¯»ä½“éªŒï¼Œæ¨èä½¿ç”¨ PDF æ ¼å¼æˆ–æ˜¯åœ¨çº¿é˜…è¯»è¿›è¡ŒæŸ¥çœ‹&lt;/p&gt; &#xA;&lt;p&gt;è‹¥æ‚¨åœ¨é˜…è¯»è¿‡ç¨‹ä¸­å‘ç°ä»»ä½•é”™è¯¯æˆ–ä¸å‡†ç¡®ä¹‹å¤„ï¼Œéå¸¸æ¬¢è¿é€šè¿‡æäº¤ Issue æˆ– Pull Request æ¥ååŠ©æˆ‘è¿›è¡Œä¿®æ­£ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ä¸åç»™äºˆ Star æˆ–è€…è¿›è¡Œå…³æ³¨ã€‚&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rohan-paul/LLM-FineTuning-Large-Language-Models</title>
    <updated>2023-12-26T01:32:14Z</updated>
    <id>tag:github.com,2023-12-26:/rohan-paul/LLM-FineTuning-Large-Language-Models</id>
    <link href="https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM (Large Language Model) FineTuning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Multiple LLM (Large Language Models) FineTuning Projects&lt;/h1&gt; &#xA;&lt;h2&gt;For almost all of these I have detailed video in my &lt;a href=&#34;https://www.youtube.com/channel/UC0_a8SNpTFkmVv5SLMs1CIA/featured&#34;&gt;YouTube Channel&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC0_a8SNpTFkmVv5SLMs1CIA/videos&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/LLM-FineTuning-Large-Language-Models/main/assets/Youtube_Cover.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Find me here..&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ¦ TWITTER: &lt;a href=&#34;https://twitter.com/rohanpaul_ai&#34;&gt;https://twitter.com/rohanpaul_ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ğŸŸ  YouTube: &lt;a href=&#34;https://www.youtube.com/@RohanPaul-AI/featured&#34;&gt;https://www.youtube.com/@RohanPaul-AI/featured&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ğŸ‘¨ğŸ»â€ğŸ’¼ LINKEDIN: &lt;a href=&#34;https://www.linkedin.com/in/rohan-paul-b27285129/&#34;&gt;https://www.linkedin.com/in/rohan-paul-b27285129/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;â€‹ğŸ‘¨â€ğŸ”§â€‹ KAGGLE: &lt;a href=&#34;https://www.kaggle.com/paulrohan2020&#34;&gt;https://www.kaggle.com/paulrohan2020&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=815NpXvniIg&amp;amp;list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&amp;amp;index=16&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/CodeLLaMA_34B_Conversation_with_Streamlit.py&#34;&gt;CodeLLaMA-34B - Conversational Agent | Large Language Models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=RYTOQERqVsg&amp;amp;list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&amp;amp;index=14&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Inference_Yarn-Llama-2-13b-128k_Github.ipynb&#34;&gt;Inference Yarn-Llama-2-13b-128k with KV Cache to answer quiz on very long textbook&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=6DGYj1EEWOw&amp;amp;list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&amp;amp;index=13&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Mistral_FineTuning_with_PEFT_and_QLORA.ipynb&#34;&gt;Mistral 7B FineTuning with_PEFT and QLORA&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fEzuBFi35J4&amp;amp;list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&amp;amp;index=11&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Falcon-7B_FineTuning_with_PEFT_and_QLORA.ipynb&#34;&gt;Falcon finetuning on openassistant-guanaco&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=J0RbOtLrJhQ&amp;amp;list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&amp;amp;index=10&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/FineTuning_phi-1_5_with_PRFT_LoRA.ipynb&#34;&gt;Fine Tuning Phi 1_5 with PEFT and QLoRA&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Finetune_codellama-34B-with-QLoRA.ipynb&#34;&gt;Finetune codellama-34B with QLoRA&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=gNSw9JwGv4w&amp;amp;list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&amp;amp;index=16&amp;amp;t=486s&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Finetune_opt_bnb_peft.ipynb&#34;&gt;Fine-tune opt-6.7b with QLoRA while keeping some of the Model layers at full-precision of float32&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QAY82UvrsHg&amp;amp;list=PLxqBkZuBynVTiTEvP6-GYf35yA6OqIN7Y&amp;amp;index=2&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Web%20scraping%20with%20Large%20Language%20Models%20(LLM)-AnthropicAI%20%2B%20LangChainAI.ipynb&#34;&gt;Web scraping with Large Language Models (LLM)-AnthropicAI + LangChainAI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Mixtral_Chatbot_with_Gradio&#34;&gt;Mixtral Chatbot with Gradio&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/togetherai-api-with_Mixtral.ipynb&#34;&gt;togetherai api to run Mixtral&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Other Language Models (Which are technically NOT LLM )&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-rqmj_tfQLo&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=34&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/DeBERTa%20Fine%20Tuning-for%20Amazon%20Review%20Dataset%20Pytorch.ipynb&#34;&gt;DeBERTa Fine Tuning for Amazon Review Dataset Pytorch&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=4nNbg4bWDrQ&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=32&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/FineTuning_BERT_for_Multi_Class_Classification_Turkish&#34;&gt;FineTuning BERT for Multi-Class Classification on custom Dataset&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=91msLyGC-LI&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=28&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=91msLyGC-LI&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=28&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;Document STRIDE when Tokenizing with HuggingFace Transformer for NLP Projects&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=cplo2UyNw24&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=31&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;Fine-tuning of a PreTrained Transformer model - what really happens to the weights (parameters)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=pqpaHeCsuVI&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=30&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=pqpaHeCsuVI&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=30&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;Cerebras-GPT New Large Language Model Open Sourced with Apache 2.0 License&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=6X0xfXMKCjM&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=29&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Roberta-Large-NER-on-Kaggle-NLP%20Competition&#34;&gt;Roberta-Large Named Entity Recognizition on Kaggle NLP Competition with PyTorch&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=EHtHF9Kvm0Y&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=28&amp;amp;ab_channel=Rohan-Paul-AI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Longformer%20end%20to%20end%20with%20Kaggle%20NLP%20competition&#34;&gt;Longformer end to end with Kaggle NLP competition&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=tvdIF1FU7fg&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=24&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/zero_shot_multilingual_sentiment_classification_with_USEm&#34;&gt;Zero Shot Multilingual Sentiment Classification with PyTorch Lightning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=CwLPglxw1WA&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=23&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Fine_Tuning_HuggingFace_Transformer_BERT_Yelp_Customer_Review_Predictions&#34;&gt;Fine Tuning Transformer (BERT) for Customer Review Prediction | NLP | HuggingFace &lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=30zPz5Xz-8g&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=21&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Understing_BERT_Embedding_Vector&#34;&gt;Understanding BERT Embeddings and Tokenization | NLP | HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fl0ow-nD8FM&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=20&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Topic-modeling-with-bertopic-arxiv-abstract&#34;&gt;Topic Modeling with BERTopic | arxiv-abstract dataset&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=vrDdnQfav0s&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=21&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Topic_Modeling_with_LDA.ipynb&#34;&gt;Latent Dirichlet Allocation (LDA) for Topic Modeling&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=iCL1TmRQ0sk&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=19&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Add-task_specific_custom_layer_to_model.ipynb&#34;&gt;Adding a custom task-specific Layer to a HuggingFace Pretrained Model&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ZvsH09XGuZ0&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=18&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Multi-class-text-classifica_fine-tuning-distilbert.ipynb&#34;&gt;Fine Tuning DistilBERT for Multiclass Text Classification&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dzyDHMycx_c&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=18&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/YT_Fine_tuning_BERT_NER_v1.ipynb&#34;&gt;Fine Tuning BERT for Named Entity Recognition (NER)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fLqiPks4neU&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=15&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Fine_Tuning_Pegasus_for_Text_Summarization.ipynb&#34;&gt;Text Summarization by Fine Tuning Transformer Model | NLP &lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=HDSNjrxSwqw&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=14&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Text_Summarization_%20BART%20_T5_Pegasus.ipynb&#34;&gt;Text Summarization with Transformer - BART + T5 + Pegasus &lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oxEXBJQG27A&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=13&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Other-Language_Models_BERT_related/Deberta-v3-large-For_Kaggle_Competition_Feedback-Prize/deberta-v3-large-For_Kaggle_Competition_Feedback-Prize.ipynb&#34;&gt;Debarta-v3-large model fine tuning for Kaggle Competition Feedback-Prize | NLP&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SmWbKiueYVU&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=12&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Topic_Modeling_with_BERT_and_Automatic_cluster_labeling/Topic_Modeling.ipynb&#34;&gt;Topic Modeling with BERT and Automatic Cluster Labeling&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Ua_ToM-CG5Q&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=11&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Decoding_Strategies_for_text_generation/Decoding_Strategies_for_text_generation.ipynb&#34;&gt;Decoding strategies while generating text with GPT-2&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VrJwKdls6d4&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=12&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Fake_News_Classification_with_LSTM_Tensorflow.ipynb&#34;&gt;Fake News Classification with LSTM and Tensorflow&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hgg2GAgDLzA&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=11&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/FinBERT_Long_Text_Part_2.ipynb&#34;&gt;FinBERT Sentiment Analysis for very Long Text (more than 512 Tokens) | PART 2&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=WEAAs_0etJQ&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=9&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/FinBERT_Long_Text_Part_2.ipynb&#34;&gt;FinBERT Sentiment Analysis for very Long Text Corpus (more than 512 Tokens) | PART-1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=fwDTLQDKJTE&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=8&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Cosine_Similarity_between_sentences_with_Transformers.ipynb&#34;&gt;Cosine Similarity between sentences with Transformers HuggingFace&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=urMUa4Nw_B8&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=7&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Zero_Shot_Learning_multilingual-NER.ipynb&#34;&gt;Zero Shot Learning - Cross Lingual Named Entity Recognition with XLM-Roberta&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Hp8_Enwzdxk&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=6&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/BERT_HuggingFace_Basic_Usages.ipynb&#34;&gt;BERT from Hugging Face - Few Baseline Application | NLP&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=CHFiTTPeyUw&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=9&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Transformer_From_Scratch/Transformer_From_Scratch.ipynb&#34;&gt;Transformer Encoder with Scaled Dot Product from Scratch&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=_IGdekeBCoE&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=7&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Fuzzy-String-Matching.ipynb&#34;&gt;Fuzzy String Matching in Natural Language Processing | NLP&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SzSANHjYhfg&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=6&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Word-Vectors-Understanding-with-Spacy.ipynb&#34;&gt;Understanding Word Vectors usage with Spacy Word and Sentence Similarity&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=TxTxWAohW7E&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=5&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Named_Entity_Recognition_NER_using_spaCy%20-%20Extracting_Subject_Verb_Action.ipynb&#34;&gt;Named Entity Recognition NER using spaCy - Extracting Subject Verb Action&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zcW2HouIIQg&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=5&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/raw/main/Other-Language_Models_BERT_related/Fine_Tuning_DistilBert_Poem_Sentiments.ipynb&#34;&gt;Fine-Tuning-DistilBert - Hugging Face Transformer for Poem Sentiment Prediction | NLP&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=0Y03waAL4Gw&amp;amp;list=PLxqBkZuBynVTn2lkHNAcw6lgm1MD5QiMK&amp;amp;index=4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/bert-base-uncased-fine-tuned-kaggle-hate-speech-dataset.ipynb&#34;&gt;Fine Tuning BERT-Based-Uncased Hugging Face Model on Kaggle Hate Speech Dataset&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=DpzQNQI-S3s&amp;amp;list=PLxqBkZuBynVQEvXfJpq3smfuKq3AiNW-N&amp;amp;index=3&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/Text%20Analytics%20of%20Tweet%20Emotion%20-%20EDA%20with%20Plotly.ipynb&#34;&gt;Text Analytics of Tweet Emotion - EDA with Plotly&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://bit.ly/3Nk0zRA&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png&#34; alt=&#34;Youtube Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/tree/main/Other-Language_Models_BERT_related/sentiment_analysis_textblob_Vader.ipynb&#34;&gt;Sentiment analysis using TextBlob and Vader&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>