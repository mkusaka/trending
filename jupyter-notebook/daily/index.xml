<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-24T01:36:05Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>predibase/llm_distillation_playbook</title>
    <updated>2023-12-24T01:36:05Z</updated>
    <id>tag:github.com,2023-12-24:/predibase/llm_distillation_playbook</id>
    <link href="https://github.com/predibase/llm_distillation_playbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Practical best practices for distilling large language models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLM Distillation Playbook&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Justin Zhao&lt;sup&gt;†&lt;/sup&gt;, Wael Abid&lt;sup&gt;†&lt;/sup&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;† Predibase, MLX team&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#llm-distillation-playbook&#34;&gt;LLM Distillation Playbook&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#who-is-this-document-for&#34;&gt;Who is this document for?&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#why-a-distillation-playbook&#34;&gt;Why a distillation playbook?&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#commmitment-to-open-source&#34;&gt;Commmitment to open source&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#key-concepts&#34;&gt;Key Concepts&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#best-practices&#34;&gt;Best practices&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#1-understand-the-limitations-of-smaller-models&#34;&gt;1. Understand the limitations of smaller models.&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#2-build-good-logging-infrastructure&#34;&gt;2. Build good logging infrastructure.&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#bootstrap-datasets-with-real-logs&#34;&gt;Bootstrap datasets with real logs.&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#bootstrap-datasets-with-synthetic-data&#34;&gt;Bootstrap datasets with synthetic data.&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#3-define-clear-evaluation-criteria&#34;&gt;3. Define clear evaluation criteria.&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#4-maximize-the-quality-of-your-teacher-model&#34;&gt;4. Maximize the quality of your teacher model.&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#5-maximize-the-quality-of-your-training-data&#34;&gt;5. Maximize the quality of your training data.&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#6-the-best-datasets-are-diverse-and-balanced&#34;&gt;6. The best datasets are diverse and balanced.&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#7-start-simple-and-small&#34;&gt;7. Start simple and small.&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#8-assess-the-marginal-utility-of-having-more-data&#34;&gt;8. Assess the marginal utility of having more data.&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#9-consider-how-you-want-to-serve-your-student&#34;&gt;9. Consider how you want to serve your student.&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#10-experiment-broadly-one-parameter-at-a-time&#34;&gt;10. Experiment broadly, one parameter at a time.&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#11-look-at-the-models-individual-mistakes&#34;&gt;11. Look at the model&#39;s individual mistakes.&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#12-actually-deploy-and-monitor-your-models-in-production&#34;&gt;12. Actually deploy and monitor your models in production.&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#options-for-model-deployment&#34;&gt;Options for model deployment&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#infrastructure-safeguards&#34;&gt;Infrastructure safeguards&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Who is this document for?&lt;/h2&gt; &#xA;&lt;p&gt;This document is for engineers and ML practitioners interested in &lt;strong&gt;LLM distillation&lt;/strong&gt; for production applications. We assume familiarity with deep learning fundamentals and large language models (LLMs). While the advice in this guide is adaptable to other settings like academic research, our focus is on how to most effectively distill LLMs for production applications.&lt;/p&gt; &#xA;&lt;h2&gt;Why a distillation playbook?&lt;/h2&gt; &#xA;&lt;p&gt;Almost every organization we’ve worked with has built at least one novel internal application using LLMs; one larger company we spoke to had built 70 prototypes in a week.&lt;/p&gt; &#xA;&lt;p&gt;Everyone is building their prototype using large language models, however as LLMs become increasingly capable and integral to various applications, the need for more efficient, smaller counterparts has never been more pronounced.&lt;/p&gt; &#xA;&lt;p&gt;This shift is driven by the compelling performance of LLMs, juxtaposed with the significant costs, resource demands, and slower operational speeds of large models. In response, distilling these models into more efficient, smaller versions presents a solution that balances capability with cost-effectiveness and speed.&lt;/p&gt; &#xA;&lt;p&gt;Despite significant interest in model distillation, we find there is still an astonishing amount of toil and guesswork involved in actually getting distilled models to work well in practice. Anecdotes and snippets of advice are spread across arxiv, huggingface, discord, substack, and social media, but the systemization and centralization of these recommendations remains to be seen.&lt;/p&gt; &#xA;&lt;p&gt;The advice in this document draws from our experience distilling language models at Google and Predibase, combined with any SLM/LLM research we could find on the topic. We are hopeful that these strategies for the efficient refinement of LLMs provide practitioners and enthusiasts with ideas that are practical, grounded in academic research, and helpful for the growing development and utilization of open source language models.&lt;/p&gt; &#xA;&lt;p&gt;This is a living document. We anticipate making periodic improvements, both small and large. If you’d like to be notified, please watch our repository (see &lt;a href=&#34;https://docs.github.com/en/account-and-profile/managing-subscriptions-and-notifications-on-github/setting-up-notifications/configuring-notifications#configuring-your-watch-settings-for-an-individual-repository&#34;&gt;instructions&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Commmitment to open source&lt;/h2&gt; &#xA;&lt;p&gt;At Predibase, we believe that the future is fine-tuned, specialized, and &lt;strong&gt;open source&lt;/strong&gt; LLMs. Open source is in the DNA of the company. As a company, we maintain:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ludwig-ai/ludwig&#34;&gt;Ludwig&lt;/a&gt;: Low-code framework for building custom LLMs, neural networks, and other AI models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/predibase/lorax&#34;&gt;LoRAX&lt;/a&gt;: Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;em&gt;[More about Predibase]&lt;/em&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Predibase is a &lt;a href=&#34;https://predibase.com&#34;&gt;managed platform&lt;/a&gt; that&#39;s built on top of open source. If you are interested in a managed solution for fine-tuning and deploying LMs, you can sign up for a free trial &lt;a href=&#34;https://predibase.com&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Key Concepts&lt;/h2&gt; &#xA;&lt;p&gt;Before we delve into the best practices for distilling large language models (LLMs), let&#39;s define terminology commonly used for model distillation and its applications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/model_distillation_overview.jpeg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;i&gt;Illustration of model distillation. &lt;a href=&#34;https://magazine.sebastianraschka.com/p/research-papers-in-november-2023&#34;&gt;Image source&lt;/a&gt;.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Model distillation&lt;/strong&gt; enables the refinement and compression of large language models into more manageable, cost-effective versions without a significant loss in performance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Large Language Models (LLMs)&lt;/strong&gt;: Advanced AI models (see &lt;a href=&#34;https://github.com/Hannibal046/Awesome-LLM&#34;&gt;full list&lt;/a&gt;) trained on vast amounts of text data. They seem to have a deep understanding of language, and can be trained to follow instructions or other tasks involving text.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Teacher Model&lt;/strong&gt;: A capable larger model that we aim to transfer to the smaller model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Student Model&lt;/strong&gt;: The smaller model that the teacher model is distilled into.&lt;/p&gt; &#xA;&lt;h2&gt;Best practices&lt;/h2&gt; &#xA;&lt;h3&gt;1. Understand the limitations of smaller models.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary&lt;/strong&gt;: Model distillation is an empirical science and is not guaranteed to work well in all cases. The effectiveness of model distillation depends on the task and data.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;There is substantial and growing evidence that smaller models outperform zero-shot and few-shot GPT-4 when fine-tuned on golden labels (&lt;a href=&#34;https://predibase.com/blog/how-to-fine-tune-llama-70b-for-structured-json-generation-with-ludwig&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2&#34;&gt;2&lt;/a&gt;). However, due to their limited size, smaller models might not capture the full depth and nuance of language as effectively as their larger counterparts.&lt;/p&gt; &#xA;&lt;p&gt;In a canonical model distillation set up where the student model is trained on the raw outputs of the teacher model (also called imitation learning), it is more often the case that the student model will, at best, match the teacher model&#39;s quality.&lt;/p&gt; &#xA;&lt;p&gt;In &lt;a href=&#34;https://arxiv.org/abs/2305.15717&#34;&gt;The False Promise of Imitating Proprietary LLMs&lt;/a&gt;, researchers found that for certain tasks, smaller student models deceptively learned to mimic their teachers&#39; style while falling short on factual correctness.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/spectrum_nlp_tasks.png&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&lt;i&gt;Spectrum of NLP tasks. The broader the domain and higher required precision, the more difficult the problem, and the less likely distillation will &#34;just work&#34;.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;In truth, the effectiveness of model distillation depends largely on the specific task and data. Students are likely more disadvantaged than their larger pre-trained teachers when it comes to tasks that span broader domains or require substantial reasoning abilities. Conversely, for tasks that are straightforward and narrowly defined, out-of-the-box imitation learning may be entirely adequate for attaining competitive student models.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;em&gt;[Case study: Jigsaw toxic comment classification]&lt;/em&gt;&lt;/summary&gt; &#xA; &lt;p&gt;To demonstrate and contextualize the best practices of LLM distillation that we will explore in the subsequent sections of this post, we use the &lt;a href=&#34;https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge&#34;&gt;Jigsaw toxic comment classification dataset&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;The &lt;a href=&#34;https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge&#34;&gt;Jigsaw dataset&lt;/a&gt; was created to train models to classify offensive comments. It consists of 160K real comments from the internet and has a mix of offensive and non-offensive examples.&lt;/p&gt; &#xA; &lt;p&gt;The original dataset contains fine-grained labels for each comment: &lt;code&gt;toxic&lt;/code&gt;, &lt;code&gt;severe_toxic&lt;/code&gt;, &lt;code&gt;obscene&lt;/code&gt;, &lt;code&gt;threat&lt;/code&gt;, &lt;code&gt;insult&lt;/code&gt;, and &lt;code&gt;identity_hate&lt;/code&gt;. We collapse all the columns into one column &lt;code&gt;is_bad&lt;/code&gt; to obtain a binary classification dataset.[^1]&lt;/p&gt; &#xA; &lt;p&gt;[^1]: While there are established state-of-the-art (SOTA) text classification models &lt;a href=&#34;https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/leaderboard&#34;&gt;specifically designed for the Jigsaw dataset&lt;/a&gt;, our intention here is not to surpass these benchmarks. Rather, we utilize this dataset as an illustrative tool to demonstrate and contextualize the best practices of LLM distillation.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/zs_vs_finetuned.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt;&lt;i&gt;Model accuracy on a balanced test set comparing zero-shot performance of GPT-* models with OSS LLMs fine-tuned using a random subset of 10K examples.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;2. Build good logging infrastructure.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary&lt;/strong&gt;: Have basic logging infrastructure for your LLMs in production. If logs are limited due to low traffic, PII, or other constraints, synthetic data generation may be a viable option for bootstrapping a dataset for fine-tuning.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you haven&#39;t already implemented logging in your application, you really should. Tokens are expensive and &lt;a href=&#34;https://www.quora.com/Who-should-get-credit-for-the-quote-data-is-the-new-oil&#34;&gt;data is oil&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/logging.png&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&lt;i&gt;Example of basic logging infrastructure with a Model-as-a-Service (MaaS) serverless teacher model. Stream requests and responses from your MaaS endpoint to a storage solution like Amazon S3 or Snowflake.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Bootstrap datasets with real logs.&lt;/h4&gt; &#xA;&lt;p&gt;Collecting logs from production traffic that&#39;s sent to your teacher models is a great, lean option for bootstrapping a dataset for fine-tuning.[^2]&lt;/p&gt; &#xA;&lt;p&gt;[^2]: Always review the terms of service and usage policies of LLM providers when logging their outputs. While OpenAI permits the use of their models for academic or exploratory work, it&#39;s advisable to seek clarification for specific use cases and production settings.&lt;/p&gt; &#xA;&lt;p&gt;See a lightweight example of asynchronously logging requests and responses to S3 in a streamlit app &lt;a href=&#34;https://github.com/predibase/llm_distillation_playbook/tree/main/app&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Bootstrap datasets with synthetic data.&lt;/h4&gt; &#xA;&lt;p&gt;For applications with limited data either due to low traffic, PII, or other constraints, &lt;strong&gt;synthetic data generation&lt;/strong&gt; may be a viable option for fine-tuning data.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/synthetic_data_start.png&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&lt;i&gt;Bootstrap your dataset with synthetic data. The biggest challenge with synthetic data is to ensure that the examples produced are varied and non-repetitive.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;Papers like &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;Self-Instruct&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2310.14558.pdf&#34;&gt;Alpacare&lt;/a&gt; or Microsoft&#39;s &lt;a href=&#34;https://arxiv.org/pdf/2306.11644.pdf&#34;&gt;phi-1&lt;/a&gt;/&lt;a href=&#34;https://arxiv.org/pdf/2309.05463.pdf&#34;&gt;phi-1.5&lt;/a&gt;/&lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/&#34;&gt;phi-2&lt;/a&gt; show how synthetic datasets, generated through creative variations of seed queries to GPT models, can be used to fine-tune compelling smaller models.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;We speculate that the creation of synthetic datasets will become, in the near future, an important technical skill and a central topic of research in AI.&#34; ~ &lt;a href=&#34;https://arxiv.org/pdf/2309.05463.pdf&#34;&gt;phi 1.5 technical report&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;3. Define clear evaluation criteria.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary:&lt;/strong&gt; Effective evaluation of distilled models requires clearly defined criteria that align with your specific application&#39;s needs. The choice of evaluation metrics should reflect the nature of the problem and the desired outcomes of the model.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a well-known best practice for machine learning, but it&#39;s worth reiterating because it&#39;s so important.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tailoring evaluation to the application&lt;/strong&gt;: Effective evaluation requires clearly defined criteria that align with your specific application&#39;s needs. For instance, LLMs for JSON generation tasks might focus on checking for schema adherence, extraction tasks might focus on accuracy or recall, and other language generation tasks might use BLEURT, ROUGE, or perplexity. The key is to select metrics that best represent the success of the model in its intended environment.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The emergence of LLMs as judges&lt;/strong&gt;: There&#39;s a growing trend of using LLMs themselves to assess model outputs, especially in scenarios where traditional metrics might fall short or where manual evaluation by human raters is too expensive. This approach can be compelling but requires &lt;a href=&#34;https://arxiv.org/abs/2306.05685&#34;&gt;careful consideration&lt;/a&gt; to account for potential LLM biases.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Consistency and diversity in test sets&lt;/strong&gt;: Establishing clear test sets is critical. These sets should be diverse enough to cover various aspects of model performance yet consistent enough to allow for reliable tracking over time. Avoid changing your test sets frequently, as consistency is key when comparing performance across different models and iterations.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;em&gt;[Case study: Jigsaw toxic comment classification]&lt;/em&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Sampling a test set randomly from the &lt;a href=&#34;https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge&#34;&gt;Jigsaw dataset&lt;/a&gt; gives us a dataset with the distribution of: 90% non-toxic, 10% toxic.&lt;/p&gt; &#xA; &lt;p&gt;This distribution might match what we expect our hypothetical application to receive (mostly non-toxic comments), we want to be sure that any model we put in production is equally good at detecting both offensive and non-offensive comments.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/jigsaw_test_sets.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;Let&#39;s formalize 2 different test sets:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;&lt;code&gt;test-indist&lt;/code&gt;: An in-distribution test set with 90% non-bad examples and 10% toxic examples, drawn from the original test set.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;test-balanced&lt;/code&gt;: An explicitly balanced test set with 50% non-toxic and 50% toxic examples, drawn from the original test set.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;By measuring models on both of these test sets simultaneously, we can track how well a candidate model classifies comments overall, as well as how well these classifications would fare in a traffic-realisitic setting.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;4. Maximize the quality of your teacher model.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary:&lt;/strong&gt; The quality of your teacher model&#39;s outputs serves as an upper limit for the performance of your distilled student model. Invest in maximizing the quality of your teacher model&#39;s performance as much as possible.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/student_and_teacher.png&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&lt;i&gt;Get your teacher model as good as it can be before feeding its outputs for the student to imitate.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Choose a good teacher:&lt;/strong&gt; The choice of the teacher model is a critical first step. Opt for a model that demonstrates the highest accuracy and understanding of your task. GPT-4 is generally great, but it&#39;s worth checking to see if there&#39;s a better foundation model out there for your use case that may be better specialized to your task.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Metric&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;zephyr-7b-alpha&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Mixtral-8x7B-Instruct-v0.1&lt;/th&gt; &#xA;   &lt;th&gt;Llama-2-70b-hf&lt;/th&gt; &#xA;   &lt;th&gt;Yi-34B-200K&lt;/th&gt; &#xA;   &lt;th&gt;CodeLlama-34b-Instruct-hf&lt;/th&gt; &#xA;   &lt;th&gt;GPT-3.5&lt;/th&gt; &#xA;   &lt;th&gt;GPT-4&lt;/th&gt; &#xA;   &lt;th&gt;Gemini&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Overall average&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.6&lt;/td&gt; &#xA;   &lt;td&gt;67.9&lt;/td&gt; &#xA;   &lt;td&gt;70.8&lt;/td&gt; &#xA;   &lt;td&gt;57.3&lt;/td&gt; &#xA;   &lt;td&gt;70.9&lt;/td&gt; &#xA;   &lt;td&gt;88.3&lt;/td&gt; &#xA;   &lt;td&gt;90.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARC&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.2&lt;/td&gt; &#xA;   &lt;td&gt;67.3&lt;/td&gt; &#xA;   &lt;td&gt;65.4&lt;/td&gt; &#xA;   &lt;td&gt;54.3&lt;/td&gt; &#xA;   &lt;td&gt;82.9&lt;/td&gt; &#xA;   &lt;td&gt;94.9&lt;/td&gt; &#xA;   &lt;td&gt;unreported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HellaSwag&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.6&lt;/td&gt; &#xA;   &lt;td&gt;87.3&lt;/td&gt; &#xA;   &lt;td&gt;85.6&lt;/td&gt; &#xA;   &lt;td&gt;76.9&lt;/td&gt; &#xA;   &lt;td&gt;79.4&lt;/td&gt; &#xA;   &lt;td&gt;92.4&lt;/td&gt; &#xA;   &lt;td&gt;87.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMLU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.2&lt;/td&gt; &#xA;   &lt;td&gt;69.8&lt;/td&gt; &#xA;   &lt;td&gt;76.1&lt;/td&gt; &#xA;   &lt;td&gt;55.5&lt;/td&gt; &#xA;   &lt;td&gt;67.4&lt;/td&gt; &#xA;   &lt;td&gt;83.7&lt;/td&gt; &#xA;   &lt;td&gt;90.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TruthfulQA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.6&lt;/td&gt; &#xA;   &lt;td&gt;44.9&lt;/td&gt; &#xA;   &lt;td&gt;53.6&lt;/td&gt; &#xA;   &lt;td&gt;44.4&lt;/td&gt; &#xA;   &lt;td&gt;61.4&lt;/td&gt; &#xA;   &lt;td&gt;79.7&lt;/td&gt; &#xA;   &lt;td&gt;unreported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Winogrande&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.4&lt;/td&gt; &#xA;   &lt;td&gt;83.7&lt;/td&gt; &#xA;   &lt;td&gt;82.6&lt;/td&gt; &#xA;   &lt;td&gt;74.6&lt;/td&gt; &#xA;   &lt;td&gt;65.8&lt;/td&gt; &#xA;   &lt;td&gt;87.1&lt;/td&gt; &#xA;   &lt;td&gt;unreported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GSM8K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.7&lt;/td&gt; &#xA;   &lt;td&gt;54.1&lt;/td&gt; &#xA;   &lt;td&gt;61.6&lt;/td&gt; &#xA;   &lt;td&gt;38.0&lt;/td&gt; &#xA;   &lt;td&gt;68.2&lt;/td&gt; &#xA;   &lt;td&gt;92.1&lt;/td&gt; &#xA;   &lt;td&gt;94.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/radar_teachers.png&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&lt;i&gt;Sources: &lt;a href=&#34;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#34;&gt;Open LLM Leaderboard&lt;/a&gt;, &lt;a href=&#34;https://the-decoder.com/gpt-4-crushes-other-llms-according-to-new-benchmark-suite/&#34;&gt;GPT-4 &#34;crushes&#34; other LLMs according to new benchmark suite&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf&#34;&gt;Gemini: A Family of Highly Capable Multimodal Models&lt;/a&gt;&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Choose a good prompt:&lt;/strong&gt; Iterating on your prompts and prompt parameters can significantly enhance the quality of the teacher model&#39;s outputs. Thoughtfully crafted prompts often lead to more accurate and contextually relevant responses, which in turn, provide better training material for the student model.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;em&gt;[Case study: Jigsaw toxic comment classification]&lt;/em&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Varying the LLMs, prompts, and temperature can a significant effect on teacher model accuracy.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/jigsaw_prompt_engineering_temperature.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;The best temperature for this dataset appears to be 0.0. This makes sense because a higher temperature boosts the creativity of the outputs of the LLM, which likely isn&#39;t useful in a binary classification setting.&lt;/p&gt; &#xA; &lt;p&gt;We highlight accuracy on &lt;code&gt;test-balanced&lt;/code&gt; with two different prompts:&lt;/p&gt; &#xA; &lt;p&gt;Simple prompt:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;Given the input text below, please classify whether it is toxic. Provide your classification in a JSON format with the key &#39;is_bad&#39;. If the comment is indeed toxic, set is_bad=1, otherwise set is_bad=0.&#xA;&#xA;Input text: &#39;%s&#39;&#xA;&#xA;Output:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Chain-of-Thought prompt:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;For a well-intentioned content moderation app, we want to flag a comment if it may contain toxic, insulting, obscene, or threatening language. Given the input text, please classify its toxicity. Please format your response in the form of a python dictionary with the following keys:&#xA;&#xA;1. &#39;is_bad&#39;: If the comment is indeed toxic, use set is_bad=1, otherwise set is_bad=0.&#xA;2. &#39;reason&#39;: Provide an appropriate amount of detail for for your judgment.&#xA;&#xA;Input text: &#39;%s&#39;&#xA;Output:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Here were our results:&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/jigsaw_prompt_engineering_cot.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;The &lt;code&gt;simple_prompt&lt;/code&gt; seems to be more aligned with human labels than a more sophisticated &lt;code&gt;Chain-of-Thought&lt;/code&gt; prompt. The quality gap between the two prompts is smaller when using GPT-4, however it does appear that a more sophisticated prompt does not always lead to better quality. Perhaps the additional reasoning that is spurred by Chain-of-Thought prompting increases the rate of false positives.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;5. Maximize the quality of your training data.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary:&lt;/strong&gt; If you can continue enhancing the quality of your training data, with or without involvement from teachers, you absolutely should. Consider how you might fundamentally improve the quality of your data.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Most mistakes made by converged student models can be traced back to issues with the source data. For student models, addressing data quality issues at the source is typically more efficient than trying to correct these issues with auxiliary systems.&lt;/p&gt; &#xA;&lt;p&gt;Here are some of the most popular techniques.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Technique&lt;/th&gt; &#xA;   &lt;th&gt;Difficulty&lt;/th&gt; &#xA;   &lt;th&gt;General applicability&lt;/th&gt; &#xA;   &lt;th&gt;Manual labor&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Manually fix or curate your data.&lt;/td&gt; &#xA;   &lt;td&gt;★&lt;/td&gt; &#xA;   &lt;td&gt;★★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★★★★&lt;/td&gt; &#xA;   &lt;td&gt;Manually fix and revise bad outputs. Annotate new data. Simple but labor-intensive, this method ensures high-quality, error-free training material.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Filter data based on rules.&lt;/td&gt; &#xA;   &lt;td&gt;★★&lt;/td&gt; &#xA;   &lt;td&gt;★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★★&lt;/td&gt; &#xA;   &lt;td&gt;Employ basic rules (length criteria, regex patterns) to eliminate poor-quality data. While setting up rules is straightforward, identifying the right criteria can be time-consuming.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rank your data with auxiliary systems (or LLMs).&lt;/td&gt; &#xA;   &lt;td&gt;★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★★&lt;/td&gt; &#xA;   &lt;td&gt;★&lt;/td&gt; &#xA;   &lt;td&gt;Use an auxiliary system, such as another model, to assess and rank data quality. For example, Microsoft&#39;s phi-1 model employs GPT-4 to score training examples, using a classifier to prioritize higher-value data, and drop the bottom X% of examples. Also see section 2.1 of &lt;a href=&#34;https://arxiv.org/abs/2107.04512&#34;&gt;this paper&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Enrich data with explanation traces.&lt;/td&gt; &#xA;   &lt;td&gt;★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★&lt;/td&gt; &#xA;   &lt;td&gt;★&lt;/td&gt; &#xA;   &lt;td&gt;Collect reasoning data. If your task requires non-trivial reasoning, you may find similar performance gains from including explanation traces or chain-of-thought (CoT) outputs from the teacher.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Aggregate your teachers.&lt;/td&gt; &#xA;   &lt;td&gt;★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★&lt;/td&gt; &#xA;   &lt;td&gt;★&lt;/td&gt; &#xA;   &lt;td&gt;For recursively-definable tasks such as summarization, use &lt;a href=&#34;https://blog.langchain.dev/fine-tuning-chatgpt-surpassing-gpt-4-summarization/&#34;&gt;chaining&lt;/a&gt;. For tasks with exact answers, take a majority vote. By consolidating multiple teachers, you enable your student model to leapfrog any single teacher.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;em&gt;[Case study: Jigsaw comment toxicity data quality experiments]&lt;/em&gt;&lt;/summary&gt; &#xA; &lt;p&gt;To assess the impact of data quality on model performance, we can derive 6 subsets of training data from the Jigsaw dataset, and train models for each of them.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;A (1.1k rows): in-distribution, GPT labels.&lt;/li&gt; &#xA;  &lt;li&gt;B (2.2k rows): A + 1.1k rows in-distribution Gold labels.&lt;/li&gt; &#xA;  &lt;li&gt;C (2.1k rows): B filtered to remove GPT errors.&lt;/li&gt; &#xA;  &lt;li&gt;D (3.2k rows): B + 1k rows with Gold toxic labels.&lt;/li&gt; &#xA;  &lt;li&gt;E (5k rows): Larger in-distribution dataset, GPT labels.&lt;/li&gt; &#xA;  &lt;li&gt;F (10k rows): Largest in-distribution dataset, GPT labels.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/jigsaw_quality_experiment.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt;&lt;i&gt;Model performance on a balanced test set.&lt;/i&gt;&lt;/p&gt; &#xA; &lt;p&gt;Performance improves both when we add high-quality human-labeled examples as well as when incorrect teacher-labeled examples are removed.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;6. The best datasets are diverse and balanced.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary:&lt;/strong&gt; Try to make your dataset diverse, non-repetitive, and balanced. The more scenarios and complexities your dataset covers, the more likely the distilled student will generalize in an unbiased way.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;One of the main challenges in creating a high-quality dataset is ensuring that the examples are varied and non-repetitive. The training data for your student model should cover a wide range of scenarios, and they should vary in their level of difficulty, complexity, and style.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diversity&lt;/strong&gt; is important for several reasons: it exposes the language model to different cases that it needs to be able to handle, it reduces the risk of overfitting or memorizing specific patterns or solutions, and it increases the generalization and robustness of the model to unseen or novel situations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Balance&lt;/strong&gt; is just as important. If certain cases are sparsely represented in the overall dataset, it may be challenging for your student model to learn these effectively.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/logs_diversity.png&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&lt;i&gt;Datasets bootstrapped from real logs can also be variation or balance-deficient. For logs-based datasets, having too many examples from power users could be detrimental to overall dataset representation. Debias logs with random mutation, augment rarer examples with paraphrasing or back-translation, or manually add missing cases.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s not essential to know or address all data distribution issues upfront, but it is useful to anticipate them. Trust that if you&#39;ve picked good test sets, meaningful biases in student models should become apparent during evaluation, and these can often be addressed with adjustments to training data.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;em&gt;[Case study: Jigsaw toxic comment classification]&lt;/em&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/jigsaw_balance_experiment.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt;&lt;i&gt;Model performance on a balanced test set.&lt;/i&gt;&lt;/p&gt; &#xA; &lt;p&gt;Perfectly balanced is not necessary, nor necessarily better.&lt;/p&gt; &#xA; &lt;p&gt;For example, it could be that the non-toxic examples are more difficult to detect than the toxic ones, so the model may very well benefit from having more examples of more difficult classes while having fewer examples of easier classes.&lt;/p&gt; &#xA; &lt;p&gt;Upfront, it&#39;s hard to know what the best &#34;balance&#34; is, or, for non-classification tasks, how to measure or productively change the balance of the dataset in the first place.&lt;/p&gt; &#xA; &lt;p&gt;The higher-level idea is that if you have good test set(s), then when you do model evaluation with (unintentionally) imbalanced training data, you’ll be able to spot bias patterns that clue you into dataset distribution adjustments.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;7. Start simple and small.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary&lt;/strong&gt;: Start with smaller, simpler model configurations that are quick to train so that you can debug issues with your setup, iterate quickly, and establish good benchmarks for comparing to more complex model configurations later.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Embrace the power of the smallest, simplest model.&lt;/strong&gt; Not just a matter of efficiency; it&#39;s a strategic approach to model development. Smaller, simpler models are significantly quicker to train and understand, allowing for the fastest iteration and feedback.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Avoid the trap of cool, but complicated large models.&lt;/strong&gt; One of the most common pitfalls in model training is starting with too large and too complex model configurations. These will be harder to understand, slow down iteration velocity, and extend experiment cycle times.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The value of naive baselines.&lt;/strong&gt; Always begin with naive, simple baseline models. These serve as a clear benchmark to measure the performance of subsequent more sophisticated model configurations.&lt;/p&gt; &#xA;&lt;h3&gt;8. Assess the marginal utility of having more data.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary:&lt;/strong&gt; Meaningful fine-tuning results are often achieved with datasets ranging from a few hundred to tens of thousands of examples as a rule of thumb. To answer the question more concretely for your task, run an ablation study varying dataset size and extrapolate.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;How much data do I need for fine-tuning my model?&#34; ~ One of the most common questions that we get asked.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;In earnest, it really depends, influenced by factors such as task difficulty, output variability, reasoning complexity, example length, task alignment with pre-training data, and hyperparameters. Some problems require minimal data for convergence, while others demand extensive training without converging at all.&lt;/p&gt; &#xA;&lt;p&gt;To determine a good dataset size for your specific case, conduct an ablation experiment holding other training parameters constant, and varying dataset size (e.g., 5%, 10%, 25%, 50%, 75%, 100%).&lt;/p&gt; &#xA;&lt;p&gt;Such experiments can reveal the marginal utility of having additional data for fine-tuning. If increasing data quantity doesn&#39;t yield much improvement, it&#39;s advisable to reevaluate other aspects of the training pipeline to identify potential areas for enhancement.&lt;/p&gt; &#xA;&lt;p&gt;If you do find that the marginal utility of having more data is high, then consider data augmentation techniques like &lt;a href=&#34;https://github.com/QData/TextAttack&#34;&gt;back translation&lt;/a&gt;, manually annotating more data.&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;em&gt;[Case study: Jigsaw toxic comment classification]&lt;/em&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/jigsaw_dataset_size_ablation.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt;&lt;i&gt;Model performance on a balanced test set.&lt;/i&gt;&lt;/p&gt; &#xA; &lt;p&gt;While there&#39;s a big jump in performance from 1.1K examples to 5K examples, the jump in quality from 5K to 10K is rather marginal. Based on these scores, we can roughly extrapolate that there is diminishing marginal utility for adding more training data for this model configuration beyond 10K examples.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;9. Consider how you want to serve your student.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary:&lt;/strong&gt; While not crucial to decide upfront, have a model serving plan in mind to prioritize experiments with models that can ultimately be served.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are planning to deploy multiple LLMs in production, it&#39;s beneficial to explore parameter-efficient fine-tuning (PEFT) techniques. PEFT, such as LoRA (Low-Rank Adaptation), involves training only a fraction of the model&#39;s weights, unlike full fine-tuning, which requires a dedicated set of GPU resources for each model. LoRA has been shown to &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;achieve performance on par with full fine-tuning&lt;/a&gt;, making it a viable option for efficient deployment.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/predibase/lorax&#34;&gt;The LoRA Exchange (LoRAX)&lt;/a&gt;, for example, is a serving solution optimized for serving numerous fine-tuned models using shared GPU resources. LoRAX stands out from traditional large language model serving methods by its ability to accommodate over a hundred task-specific, fine-tuned models on a single GPU. This capability significantly reduces the cost and complexity of serving fine-tuned models. LoRaX is especially suited for parameter-efficient fine-tuned models, offering a streamlined solution for deployment.&lt;/p&gt; &#xA;&lt;p&gt;While full fine-tuning with larger models might yield the highest absolute quality, the trade-off in terms of increased costs or serving latency might not justify the marginal gains in performance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/lorax.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;i&gt;Serving adapter-based LLMs with LoRAX.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;Consider the target serving architecture early in the model development process. The type of model you choose may greatly influence how it will be served and should inform how to prioritize experiments.&lt;/p&gt; &#xA;&lt;h3&gt;10. Experiment broadly, one parameter at a time.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary:&lt;/strong&gt; Exploration over exploitation: spend most of your time and energy to gain insight into the problem. Change one variable at a time, and try not to rathole.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tips for running lots of experiments at once:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stay organized with model repositories or spreadsheets.&lt;/li&gt; &#xA; &lt;li&gt;Parallelize but only change one parameter at a time.&lt;/li&gt; &#xA; &lt;li&gt;Expect some toil and guesswork.&lt;/li&gt; &#xA; &lt;li&gt;Optimize for iteration speed (simple -&amp;gt; complex, small -&amp;gt; large)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following suggestions came about as we tried to crystalize our own approach to fine-tuning LLMs. This is far from a comprehensive list, but here are some of our favorite ideas for exploration.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Category&lt;/th&gt; &#xA;   &lt;th&gt;Idea&lt;/th&gt; &#xA;   &lt;th&gt;Impact to Quality&lt;/th&gt; &#xA;   &lt;th&gt;Impact to Speed&lt;/th&gt; &#xA;   &lt;th&gt;Complexity&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Architecture parameter&lt;/td&gt; &#xA;   &lt;td&gt;Foundation model&lt;/td&gt; &#xA;   &lt;td&gt;★★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★&lt;/td&gt; &#xA;   &lt;td&gt;★&lt;/td&gt; &#xA;   &lt;td&gt;Try out a few different foundation models and see what happens to your student models performance. Like teachers, different foundation models may be inherently closer to your task than others.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Architecture parameter&lt;/td&gt; &#xA;   &lt;td&gt;Precision and quantization&lt;/td&gt; &#xA;   &lt;td&gt;★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★&lt;/td&gt; &#xA;   &lt;td&gt;Reducing precision significantly decreases the model&#39;s size, enabling it to train with larger batch sizes and thus higher throughput. While quantization can sometimes lead to a slight decrease in model accuracy due to the reduced precision, it&#39;s not always the case. In our experiments, oftentimes the trade-off is minimal compared to the gains in speed and size reduction.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Architecture parameter&lt;/td&gt; &#xA;   &lt;td&gt;Adapter parameters (rank and alpha)&lt;/td&gt; &#xA;   &lt;td&gt;★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★&lt;/td&gt; &#xA;   &lt;td&gt;Rank in LoRA determines the size of the low-rank matrices that are used to approximate the full-rank weight matrices in the model. A higher rank can increase the model&#39;s capacity to learn complex patterns but at the cost of more parameters to train. Conversely, a lower rank is more parameter-efficient but limits the model&#39;s expressiveness.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Architecture parameter&lt;/td&gt; &#xA;   &lt;td&gt;Base model size&lt;/td&gt; &#xA;   &lt;td&gt;★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★&lt;/td&gt; &#xA;   &lt;td&gt;Experiment with different sizes to get a sense of the trade-off between model performance and model size. Some tasks may benefit significantly from larger models due to the task&#39;s complexity. However, larger models are more likely to overfit to training data, especially if the dataset is not large or varied enough, or the gains in quality may be marginal. Increasingly, it&#39;s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Architecture parameter&lt;/td&gt; &#xA;   &lt;td&gt;Prompt&lt;/td&gt; &#xA;   &lt;td&gt;★★&lt;/td&gt; &#xA;   &lt;td&gt;★★&lt;/td&gt; &#xA;   &lt;td&gt;★&lt;/td&gt; &#xA;   &lt;td&gt;Prompts have an outsized impact with teacher models, but in supervised fine-tuning (SFT) and the weights of the model are updated directly, wordsmithing the prompt is not as directly impactful to quality.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Training parameter&lt;/td&gt; &#xA;   &lt;td&gt;Epochs&lt;/td&gt; &#xA;   &lt;td&gt;★★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★&lt;/td&gt; &#xA;   &lt;td&gt;Simply training a model for longer (more epochs) will often result in a better model.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Training parameter&lt;/td&gt; &#xA;   &lt;td&gt;Learning rate (LR) and LR schedule&lt;/td&gt; &#xA;   &lt;td&gt;★★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★&lt;/td&gt; &#xA;   &lt;td&gt;★&lt;/td&gt; &#xA;   &lt;td&gt;An optimal learning rate ensures that the model learns efficiently without missing or overshooting the optimal weights. A proper warm-up can improve model training stability and performance while decay helps maintain the balance between learning complex patterns and avoiding overfitting to the training data.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Training parameter&lt;/td&gt; &#xA;   &lt;td&gt;Max sequence length&lt;/td&gt; &#xA;   &lt;td&gt;★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★★&lt;/td&gt; &#xA;   &lt;td&gt;★&lt;/td&gt; &#xA;   &lt;td&gt;For long-tailed data, consider truncating your data to maximize GPU utilization.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Training parameter&lt;/td&gt; &#xA;   &lt;td&gt;Batch size&lt;/td&gt; &#xA;   &lt;td&gt;★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★&lt;/td&gt; &#xA;   &lt;td&gt;Max out your GPU. Choose the highest batch size that doesn&#39;t OOM.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Training strategy&lt;/td&gt; &#xA;   &lt;td&gt;Curriculum learning&lt;/td&gt; &#xA;   &lt;td&gt;★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★★★★&lt;/td&gt; &#xA;   &lt;td&gt;Progressive learning, also known as curriculum learning, is a training strategy where the model is fine-tuned in a series of stages, each with a different kind of training data, typically progressing from more general or noisier data to more specific, high-quality, or in-domain data. Progressive learning mirrors the natural way humans learn: starting from broad concepts and gradually focusing on more specific and complex ideas. Example of progressive learning from &lt;a href=&#34;https://arxiv.org/abs/2311.11045&#34;&gt;orca-2&lt;/a&gt;: &lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/curriculum_learning.png&#34; alt=&#34;img&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Training strategy&lt;/td&gt; &#xA;   &lt;td&gt;RLHF/RLAIF/DPO&lt;/td&gt; &#xA;   &lt;td&gt;★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★★★★★&lt;/td&gt; &#xA;   &lt;td&gt;RLHF/RLHAIF/DPO, also called &#34;preference tuning&#34; where the model undergoes reinforcement learning to align better to human preferences. This was originally popularized by OpenAI, however it&#39;s extremely costly, and seems like a last mile optimization. We have yet to speak with a company who has a critical need for this level of optimization. High-level diagram of &lt;a href=&#34;https://arxiv.org/abs/2309.00267&#34;&gt;RLHF vs. RLAIF&lt;/a&gt;: &lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/rlhf.png&#34; alt=&#34;img&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;11. Look at the model&#39;s individual mistakes.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary:&lt;/strong&gt; While aggregate metrics and advanced automated evaluation methods provide a broad overview of model performance, manually reviewing examples of your model&#39;s outputs brings unparalleled value for a deeper qualitative understanding of model performance.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Especially in generative contexts where model performance can&#39;t be neatly summarized with a clear-cut metric, taking the time to delve into specific examples of where and how your model makes mistakes is not just a step in the evaluation process; it&#39;s a critical component of the model development journey.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Identify Specific Errors:&lt;/strong&gt; Only by examining individual examples where the model errs, you can start to categorize and understand the nature of these mistakes. Is the model consistently struggling with certain types of inputs? Are there specific patterns or contexts where errors are more frequent or likely?&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Uncover Data Issues:&lt;/strong&gt; Often, patterns in mistakes can be traced back to issues in data preparation or inadequate representation in the training set. Identifying these issues early can save significant resources and time that might otherwise be spent on futile parameter optimization. There&#39;s nothing more frustrating or wasteful than to spend hundreds of GPU hours optimizing modeling parameters when you uncover an issue with data quality.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/predibase/llm_distillation_playbook/main/images/llm_loss_curves.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;i&gt;Loss curves for fine-tuned LLMs will all look like this, yet the qualitative differences between these checkpoints can be substantial.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;h3&gt;12. Actually deploy and monitor your models in production.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary:&lt;/strong&gt; While test sets provide a controlled environment for evaluation, the true test of your model’s effectiveness is how it performs with actual users and real-time inputs. Deploy your model and observe its performance in a real-world setting!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Deploy and monitor your models in production… actually. Whether you are a resesarcher, an engineer, or somewhere in between, there&#39;s a lot to learn from going through the due dilligence of productionizing your model for real.&lt;/p&gt; &#xA;&lt;h4&gt;Options for model deployment&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Live Experiment and Gradual Rollout:&lt;/strong&gt; Begin by directing a small percentage of traffic (e.g., 1%, then 10%) to the student model. Closely monitor changes in key application metrics like latency and user interactions before scaling up.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dark Launch:&lt;/strong&gt; Continue using the teacher model in production but route a portion of traffic to the student model in the background. Compare instances where the student model’s predictions differ from the teacher’s to evaluate the student&#39;s quality readiness.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hybrid Launch:&lt;/strong&gt; If the teacher model outperforms the student model, consider a hybrid deployment. The student model can handle simpler, less resource-intensive queries, while the teacher model addresses more complex requests. This approach balances efficiency with quality.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Infrastructure safeguards&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Monitor Inputs:&lt;/strong&gt; Fine-tuned models, being more specialized, can be sensitive to feature drift.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Monitor Outputs:&lt;/strong&gt; Establish failsafe mechanisms to scrutinize generated outputs. LLMs in production are often accompanied by rules-based or model-based systems to identify issues and trigger fallbacks. Be aware that using another LLM for output monitoring can add latency.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Maintain Logs:&lt;/strong&gt; Continue logging the inputs and outputs for any production LLMs. Logs will be invaluable for future model refinements or re-distillations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;d love to hear your feedback!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If you like the playbook, please &lt;a href=&#34;https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars#starring-a-repository&#34;&gt;leave a star&lt;/a&gt;! You can also reach us by pinging the &lt;a href=&#34;https://ludwig-ai.slack.com/join/shared_invite/zt-mrxo87w6-DlX5~73T2B4v_g6jj0pJcQ&#34;&gt;Ludwig slack&lt;/a&gt; or the &lt;a href=&#34;https://discord.gg/CBgdrGnZjy&#34;&gt;LoRAX Discord&lt;/a&gt;, or finding us on LinkedIn. Testimonials help us justify creating more resources like this.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If anything seems incorrect, please file an issue to start a discussion. For questions or other messages where an issue isn&#39;t appropriate, please open a new discussion topic on GitHub.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;This is a living document. We anticipate making periodic improvements, both small and large. If you’d like to be notified, please watch our repository (see &lt;a href=&#34;https://docs.github.com/en/account-and-profile/managing-subscriptions-and-notifications-on-github/setting-up-notifications/configuring-notifications#configuring-your-watch-settings-for-an-individual-repository&#34;&gt;instructions&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Are there other best practices missing from this list? Feel free to create a PR! We promise to review your suggestions with expediency.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>viktoriasemaan/boa303</title>
    <updated>2023-12-24T01:36:05Z</updated>
    <id>tag:github.com,2023-12-24:/viktoriasemaan/boa303</id>
    <link href="https://github.com/viktoriasemaan/boa303" rel="alternate"></link>
    <summary type="html">&lt;p&gt;repo for demos Unlock insights with AWS GenAI services (re:Invent BOA303)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Demos for Unlock insights with AWS GenAI services (re:Invent BOA303)&lt;/h1&gt; &#xA;&lt;p&gt;This repository is complementary to breakout session on &#34;BOA303 - Unlock insights with AWS GenAI services&#34; at &lt;a href=&#34;https://reinvent.awsevents.com/&#34;&gt;re:Invent 2023&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;🎥 Watch the session &lt;a href=&#34;https://bit.ly/BOA303-reinvent-23&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.youtube.com/watch?v=ZdjFqPwlmLU&#34; title=&#34;BOA303&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/ZdjFqPwlmLU/0.jpg&#34; alt=&#34;&amp;quot;BOA303&amp;quot;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In these demos, you will explore how RAG ( Retrieval-Augmented Generation ) can enhance AI models by leveraging external data sources to provide context-aware answers and unlock insights.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/viktoriasemaan/boa303/main/images/demo_1_2.gif&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Learn 2 ways to use your data with GenAI models:&lt;/p&gt; &#xA;&lt;p&gt;1️⃣ DEMO 1: &lt;a href=&#34;https://raw.githubusercontent.com/viktoriasemaan/boa303/main/#demo-1---rag-approach-with-amazon-kendra&#34;&gt;Deploying Llama2 with Sagemaker, CodeWhisperer + Kendra for data retrieval&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/viktoriasemaan/boa303/main/images/demo1.gif&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;2️⃣ DEMO 2: &lt;a href=&#34;https://raw.githubusercontent.com/viktoriasemaan/boa303/main/#demo-2---rag-approach-with-vectordb&#34;&gt;Using Claude 2 with Bedrock + Vector DB for data retrieval&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/viktoriasemaan/boa303/main/images/demo2.gif&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;DEMO 1 - RAG approach with Amazon Kendra&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Follow &lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated-launch.html#studio-updated-launch-prereq&#34;&gt;instructions&lt;/a&gt; to setup Amazon SageMaker Studio prerequisites&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Follow &lt;a href=&#34;https://docs.aws.amazon.com/codewhisperer/latest/userguide/sagemaker-setup.html&#34;&gt;instructions&lt;/a&gt; to enable Amazon CodeWhisperer extension for SageMaker Studio&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Setup for Demo 1&lt;/h3&gt; &#xA;&lt;h4&gt;1. Create S3 Bucket and download data&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open Amazon S3 console. Click &lt;strong&gt;Create a bucket&lt;/strong&gt;. Provide a unique name &lt;code&gt;reinvent-demo-yourname&lt;/code&gt;. Keep other options as default. Click &lt;strong&gt;Create bucket&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Go to &lt;a href=&#34;https://www.macrotrends.net/countries/USA/united-states/inflation-rate-cpi&#34;&gt;https://www.macrotrends.net/countries/USA/united-states/inflation-rate-cpi&lt;/a&gt;. Click &lt;strong&gt;download historical data&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download Starbucks annual report PDF.&lt;/li&gt; &#xA; &lt;li&gt;Upload downloaded csv and pdf files into S3 bucket.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Amazon Kendra Index &amp;amp; data sources&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open Amazon Kendra console. Click &lt;strong&gt;Create an index&lt;/strong&gt;. Provide index name. Under IAM Select &lt;strong&gt;Create a new role (Recommended)&lt;/strong&gt;. Enter role name. Click &lt;strong&gt;Next&lt;/strong&gt;, then &lt;strong&gt;Next&lt;/strong&gt; in the follow-up screens, and Click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/viktoriasemaan/boa303/main/images/image-01.jpg&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Copy index name and save for future references. Wait for index to become active and click &lt;strong&gt;Add data sources&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Find Amazon S3 Connector. Click &lt;strong&gt;Add connector&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/viktoriasemaan/boa303/main/images/image-02.jpg&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Provide Data Source name. Click &lt;strong&gt;Next&lt;/strong&gt;. Create a new role or select an existing role.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;strong&gt;Browse S3&lt;/strong&gt; and select the bucket that you created in Step 1. At the bottom select &lt;strong&gt;Frequency Run on demand&lt;/strong&gt;. Click &lt;strong&gt;Next&lt;/strong&gt;, then &lt;strong&gt;Next&lt;/strong&gt; again then &lt;strong&gt;Add data resources&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/viktoriasemaan/boa303/main/images/image-03.jpg&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Once the data source is created, click &lt;strong&gt;Sync Now&lt;/strong&gt; button at the top.&lt;/li&gt; &#xA; &lt;li&gt;Let’s add one more data source. Click on &lt;strong&gt;Data sources&lt;/strong&gt; on the left pane then click &lt;strong&gt;Add data source&lt;/strong&gt;. Search for Web and pick &lt;strong&gt;Web Crawler V2.0&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/viktoriasemaan/boa303/main/images/image-04.jpg&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Provide data source name and click &lt;strong&gt;Next&lt;/strong&gt;. Copy URL from step 1 into &lt;strong&gt;Source URLs&lt;/strong&gt;. Select &lt;strong&gt;Create a new role (Recommended)&lt;/strong&gt; at the bottom, type name, and click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/viktoriasemaan/boa303/main/images/image-05.jpg&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Keep default options and select Frequency &lt;strong&gt;Run on demand&lt;/strong&gt;. Click &lt;strong&gt;Next&lt;/strong&gt;, then &lt;strong&gt;Next&lt;/strong&gt; again then &lt;strong&gt;Add data resources&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Once the data source is created, click &lt;strong&gt;Sync Now&lt;/strong&gt; button at the top.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Demo 1 - Part 1: Notebook updates&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Open SageMaker Studio and copy &lt;a href=&#34;https://github.com/viktoriasemaan/boa303/raw/main/Demo1%20-%20RAG%20with%20SageMaker%20and%20Kendra.ipynb&#34;&gt;&lt;code&gt;Demo1 - RAG with SageMaker and Kendra.ipynb&lt;/code&gt;&lt;/a&gt; notebook.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies as needed&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;!pip install —upgrade —quiet sagemaker&#xA;!pip install langchain&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Update s3_path with path to your bucket created in prerequisites. Update if needed csv file name&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/viktoriasemaan/boa303/main/images/image-06.jpg&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Demo 1 - Part 2: Notebook updates&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copy your SageMaker Endpoint and replace in the code. In SageMaker Console, go to Sidebar &amp;gt; Home &amp;gt; Deployment dropdown &amp;gt; Endpoints — copy the name of the LLM and paste in the 3rd row below&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/viktoriasemaan/boa303/main/images/image-07.jpg&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Demo 1 - Part 3: Notebook updates&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Update Kendra IndexID in the code and AWS region.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Run your notebook and ask questions! 🚀&lt;/p&gt; &#xA;&lt;h2&gt;DEMO 2 - RAG approach with VectorDB&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Follow &lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated-launch.html#studio-updated-launch-prereq&#34;&gt;instructions&lt;/a&gt; to setup Amazon SageMaker Studio prerequisites&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Follow &lt;a href=&#34;https://docs.aws.amazon.com/codewhisperer/latest/userguide/sagemaker-setup.html&#34;&gt;instructions&lt;/a&gt; to enable Amazon CodeWhisperer extension for SageMaker Studio&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Follow &lt;a href=&#34;https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html&#34;&gt;instructions&lt;/a&gt; to request access to Antropic Claude model inside Amazon Bedrock&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a free account with &lt;a href=&#34;https://www.pinecone.io/&#34;&gt;pinecone.io&lt;/a&gt; and create index “demoindex”.&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Click indexes on the left pane then click button Create index.&lt;/li&gt; &#xA;   &lt;li&gt;Give an index the name “demoindex”. Configure dimensions 1536, metric cosine, then click Create index. Save index name and environment name in your notes. We will use it later.&lt;/li&gt; &#xA;   &lt;li&gt;Go to API keys on the left pane and copy API key. Save it. We will use it later.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/viktoriasemaan/boa303/main/images/image-09.jpg&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Demo 2 - Part 1:Bedrock Configuration&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Open SageMaker Studio and copy &lt;a href=&#34;https://github.com/viktoriasemaan/boa303/raw/main/Demo2%20-%20RAG%20with%20Bedrock%20and%20Vector%20DB.ipynb&#34;&gt;&lt;code&gt;Demo2 - RAG with Bedrock and Vector DB.ipynb&lt;/code&gt;&lt;/a&gt; notebook.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;keys.env&lt;/code&gt; file and add your Pinecone account information. Save this file in the same folder as your notebooks.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;PINECONE_API_KEY=your_pinecone_API_key&#xA;PINECONE_ENVIRONMENT=your_pinecone_env&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies as needed&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;!pip install -U \&#xA;langchain==0.0.306 \&#xA;boto3 \&#xA;botocore \&#xA;pypdf==3.15.2 \&#xA;pinecone-client==2.2.4 \&#xA;apache-beam==2.50.0 \&#xA;datasets==2.14.5 \&#xA;tiktoken==0.4.0 —force-reinstall —quiet&#xA;&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Execute commands in the notebook! 🚀&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>DAGWorks-Inc/hamilton</title>
    <updated>2023-12-24T01:36:05Z</updated>
    <id>tag:github.com,2023-12-24:/DAGWorks-Inc/hamilton</id>
    <link href="https://github.com/DAGWorks-Inc/hamilton" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hamilton helps data scientists and engineers define testable, modular, self-documenting dataflows, that encode lineage and metadata. Runs and scales everywhere python does.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/DAGWorks-Inc/hamilton/assets/2328071/feb6abaa-b6d5-4271-a320-0ae4a18d8aa7&#34; width=&#34;100&#34;&gt; &#xA; &lt;h1&gt;Welcome to the official Hamilton Github Repository&lt;/h1&gt; &#xA; &lt;a href=&#34;https://circleci.com/gh/DAGWorks-Inc/hamilton/tree/main&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://circleci.com/gh/DAGWorks-Inc/hamilton/tree/main.svg?style=svg&#34; alt=&#34;Hamilton CircleCI&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://hamilton.dagworks.io/en/latest/?badge=latest&#34;&gt; &lt;img src=&#34;https://readthedocs.org/projects/hamilton/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://join.slack.com/t/hamilton-opensource/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Join-Hamilton_Slack-brightgreen?logo=slack&#34; alt=&#34;Hamilton Slack&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://twitter.com/hamilton_os&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/http/shields.io.svg?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://www.python.org/downloads/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.8|%203.9|%203.10|%203.11|%203.12-brightgreen.svg&#34; alt=&#34;Python supported&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://pypi.org/project/sf-hamilton/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/sf-hamilton.svg?sanitize=true&#34; alt=&#34;PyPi Version&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://pepy.tech/project/sf-hamilton&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/sf-hamilton&#34; alt=&#34;Total Downloads&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://anaconda.org/hamilton-opensource/sf-hamilton&#34;&gt; &lt;img src=&#34;https://anaconda.org/hamilton-opensource/sf-hamilton/badges/version.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://anaconda.org/hamilton-opensource/sf-hamilton&#34;&gt; &lt;img src=&#34;https://anaconda.org/hamilton-opensource/sf-hamilton/badges/platforms.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Hamilton&lt;/h1&gt; &#xA;&lt;p&gt;The general purpose micro-orchestration framework for creating &lt;a href=&#34;https://en.wikipedia.org/wiki/Dataflow&#34;&gt;dataflows&lt;/a&gt; from python functions! That is, your single tool to express things like data, ML, LLM pipelines/workflows, and even web request logic!&lt;/p&gt; &#xA;&lt;p&gt;Hamilton is a novel paradigm for specifying a flow of delayed execution in python. It works on python objects of any type and dataflows of any complexity. Core to the design of Hamilton is a clear mapping of function name to artifact, allowing you to quickly grok the relationship between the code you write and the data you produce.&lt;/p&gt; &#xA;&lt;p&gt;This paradigm makes modifications easy to build and track, ensures code is self-documenting, and makes it natural to unit test your data transformations. When connected together, these functions form a &lt;a href=&#34;https://en.wikipedia.org/wiki/Directed_acyclic_graph&#34;&gt;Directed Acyclic Graph&lt;/a&gt; (DAG), which the Hamilton framework can execute, optimize, and report on.&lt;/p&gt; &#xA;&lt;h2&gt;Problems Hamilton Solves&lt;/h2&gt; &#xA;&lt;p&gt;✅ Model a dataflow -- If you can model your problem as a DAG in python, Hamilton is the cleanest way to build it.&lt;br&gt; ✅ Unmaintainable spaghetti code -- Hamilton dataflows are unit testable, self-documenting, and provide lineage.&lt;br&gt; ✅ Long iteration/experimentation cycles -- Hamilton provides a clear, quick, and methodical path to debugging/modifying/extending your code.&lt;br&gt; ✅ Reusing code across contexts -- Hamilton encourages code that is independent of infrastructure and can run regardless of execution setting.&lt;/p&gt; &#xA;&lt;h2&gt;Problems Hamilton Does not Solve&lt;/h2&gt; &#xA;&lt;p&gt;❌ Provisioning infrastructure -- you want a macro-orchestration system (see airflow, kubeflow, sagemaker, etc...).&lt;br&gt; ❌ Doing your ML for you -- we organize your code, BYOL (bring your own libraries).&lt;br&gt; ❌ Tracking execution + associated artifacts -- Hamilton is lightweight, but if this is important to you see the &lt;a href=&#34;https://raw.githubusercontent.com/DAGWorks-Inc/hamilton/main/www.dagworks.io&#34;&gt;DAGWorks product&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See the table below for more specifics/how it compares to other common tooling.&lt;/p&gt; &#xA;&lt;h2&gt;Full Feature Comparison&lt;/h2&gt; &#xA;&lt;p&gt;Here are common things that Hamilton is compared to, and how Hamilton compares to them.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Feature&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Hamilton&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Macro orchestration systems (e.g. Airflow)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Feast&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;dbt&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dask&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python 3.8+&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Helps you structure your code base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Code is always unit testable&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Documentation friendly&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Can visualize lineage easily&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Is just a library&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Runs anywhere python runs&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Built for managing python transformations&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Can model GenerativeAI/LLM based workflows&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Replaces macro orchestration systems&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Is a feature store&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Can model transforms at row/column/object/dataset level&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;If you don&#39;t want to install anything to try Hamilton, we recommend trying &lt;a href=&#34;https://www.tryhamilton.dev/?utm_source=README&#34;&gt;www.tryhamilton.dev&lt;/a&gt;. Otherwise, here&#39;s a quick getting started guide to get you up and running in less than 15 minutes. If you need help join our &lt;a href=&#34;https://join.slack.com/t/hamilton-opensource/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg&#34;&gt;slack&lt;/a&gt; community to chat/ask Qs/etc. For the latest updates, follow us on &lt;a href=&#34;https://twitter.com/hamilton_os&#34;&gt;twitter&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To get started, first you need to install hamilton. It is published to pypi under &lt;code&gt;sf-hamilton&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;pip install sf-hamilton&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Note: to use the DAG visualization functionality, you should instead do:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;pip install &#34;sf-hamilton[visualization]&#34;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;While it is installing we encourage you to start on the next section.&lt;/p&gt; &#xA;&lt;p&gt;Note: the content (i.e. names, function bodies) of our example code snippets are for illustrative purposes only, and don&#39;t reflect what we actually do internally.&lt;/p&gt; &#xA;&lt;h2&gt;Hamilton in &amp;lt;15 minutes&lt;/h2&gt; &#xA;&lt;p&gt;Hamilton is a new paradigm when it comes to creating, um, dataframes (let&#39;s use dataframes as an example, otherwise you can create &lt;em&gt;ANY&lt;/em&gt; python object). Rather than thinking about manipulating a central dataframe, as is normal in some data engineering/data science work, you instead think about the column(s) you want to create, and what inputs are required. There is no need for you to think about maintaining this dataframe, meaning you do not need to think about any &#34;glue&#34; code; this is all taken care of by the Hamilton framework.&lt;/p&gt; &#xA;&lt;p&gt;For example rather than writing the following to manipulate a central dataframe object &lt;code&gt;df&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;col_c&#39;] = df[&#39;col_a&#39;] + df[&#39;col_b&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;you write&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def col_c(col_a: pd.Series, col_b: pd.Series) -&amp;gt; pd.Series:&#xA;    &#34;&#34;&#34;Creating column c from summing column a and column b.&#34;&#34;&#34;&#xA;    return col_a + col_b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In diagram form: &lt;img src=&#34;https://raw.githubusercontent.com/DAGWorks-Inc/hamilton/main/hamiltondag.png&#34; alt=&#34;example&#34;&gt; The Hamilton framework will then be able to build a DAG from this function definition.&lt;/p&gt; &#xA;&lt;p&gt;So let&#39;s create a &#34;Hello World&#34; and start using Hamilton!&lt;/p&gt; &#xA;&lt;h3&gt;Your first hello world.&lt;/h3&gt; &#xA;&lt;p&gt;By now, you should have installed Hamilton, so let&#39;s write some code.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a file &lt;code&gt;my_functions.py&lt;/code&gt; and add the following functions:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd&#xA;&#xA;def avg_3wk_spend(spend: pd.Series) -&amp;gt; pd.Series:&#xA;    &#34;&#34;&#34;Rolling 3 week average spend.&#34;&#34;&#34;&#xA;    return spend.rolling(3).mean()&#xA;&#xA;def spend_per_signup(spend: pd.Series, signups: pd.Series) -&amp;gt; pd.Series:&#xA;    &#34;&#34;&#34;The cost per signup in relation to spend.&#34;&#34;&#34;&#xA;    return spend / signups&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The astute observer will notice we have not defined &lt;code&gt;spend&lt;/code&gt; or &lt;code&gt;signups&lt;/code&gt; as functions. That is okay, this just means these need to be provided as input when we come to actually wanting to create a dataframe.&lt;/p&gt; &#xA;&lt;p&gt;Note: functions can take or create scalar values, in addition to any python object type.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;my_script.py&lt;/code&gt; which is where code will live to tell Hamilton what to do:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys&#xA;import logging&#xA;import importlib&#xA;&#xA;import pandas as pd&#xA;from hamilton import driver&#xA;&#xA;logging.basicConfig(stream=sys.stdout)&#xA;initial_columns = {  # load from actuals or wherever -- this is our initial data we use as input.&#xA;    # Note: these do not have to be all series, they could be scalar inputs.&#xA;    &#39;signups&#39;: pd.Series([1, 10, 50, 100, 200, 400]),&#xA;    &#39;spend&#39;: pd.Series([10, 10, 20, 40, 40, 50]),&#xA;}&#xA;# we need to tell hamilton where to load function definitions from&#xA;module_name = &#39;my_functions&#39;&#xA;module = importlib.import_module(module_name) # or we could just do `import my_functions`&#xA;dr = driver.Driver(initial_columns, module)  # can pass in multiple modules&#xA;# we need to specify what we want in the final dataframe.&#xA;output_columns = [&#xA;    &#39;spend&#39;,  # or module.spend&#xA;    &#39;signups&#39;,  # or module.signups&#xA;    &#39;avg_3wk_spend&#39;,  # or module.avg_3wk_spend&#xA;    &#39;spend_per_signup&#39;,  # or module.spend_per_signup&#xA;]&#xA;# let&#39;s create the dataframe!&#xA;# if you only did `pip install sf-hamilton` earlier:&#xA;df = dr.execute(output_columns)&#xA;# else if you did `pip install &#34;sf-hamilton[visualization]&#34;` earlier:&#xA;# dr.visualize_execution(output_columns, &#39;./my-dag.dot&#39;, {})&#xA;print(df)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run my_script.py&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;python my_script.py&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;You should see the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;   spend  signups  avg_3wk_spend  spend_per_signup&#xA;0     10        1            NaN            10.000&#xA;1     10       10            NaN             1.000&#xA;2     20       50      13.333333             0.400&#xA;3     40      100      23.333333             0.400&#xA;4     40      200      33.333333             0.200&#xA;5     50      400      43.333333             0.125&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see the following image if you ran &lt;code&gt;dr.visualize_execution(output_columns, &#39;./my-dag.dot&#39;, {&#34;format&#34;: &#34;png&#34;}, orient=&#34;TB&#34;)&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DAGWorks-Inc/hamilton/main/hello_world_image.png&#34; alt=&#34;hello_world_image&#34;&gt; Note: we treat displaying &lt;code&gt;Inputs&lt;/code&gt; in a special manner for readability in our visualizations. So you&#39;ll likely see input nodes repeated.&lt;/p&gt; &#xA;&lt;p&gt;Congratulations - you just created your Hamilton dataflow that created a dataframe!&lt;/p&gt; &#xA;&lt;h2&gt;Example Hamilton Dataflows&lt;/h2&gt; &#xA;&lt;p&gt;We have a growing list of examples showcasing how one might use Hamilton. You currently have two places to find them:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://hub.dagworks.io/&#34;&gt;Hamilton Dataflow Hub&lt;/a&gt; -- which makes it easy to pull and then modify code.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://github.com/dagworks-inc/hamilton/tree/main/examples&#34;&gt;&lt;code&gt;examples/&lt;/code&gt;&lt;/a&gt; folder in this repository.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For the Hub, this will contain user contributed dataflows, e.g. text_summarization, forecasting, data processing, that will be continually added to.&lt;/p&gt; &#xA;&lt;p&gt;For the &lt;a href=&#34;https://github.com/dagworks-inc/hamilton/tree/main/examples&#34;&gt;&lt;code&gt;examples/&lt;/code&gt;&lt;/a&gt; directory, you&#39;ll have to copy/fork the repository to run them. E.g.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dagworks-inc/hamilton/tree/main/examples/hello_world&#34;&gt;Hello world&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Scaling on to &lt;a href=&#34;https://github.com/dagworks-inc/hamilton/tree/main/examples/ray&#34;&gt;Ray&lt;/a&gt;, &lt;a href=&#34;https://github.com/dagworks-inc/hamilton/tree/main/examples/dask&#34;&gt;Dask&lt;/a&gt;, or &lt;a href=&#34;https://github.com/dagworks-inc/hamilton/tree/main/examples/spark&#34;&gt;Pandas on Spark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Training &lt;a href=&#34;https://github.com/dagworks-inc/hamilton/tree/main/examples/model_examples&#34;&gt;a model with scikit-learn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Doing &lt;a href=&#34;https://github.com/dagworks-inc/hamilton/tree/main/examples/numpy/air-quality-analysis&#34;&gt;air quality analysis solely in numpy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also have a docker container that contains some of these examples so you can pull that and run them locally. See the &lt;a href=&#34;https://github.com/DAGWorks-Inc/hamilton/raw/main/examples/README.md#running-examples-through-a-docker-image&#34;&gt;examples folder README&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h1&gt;We forked and lost some stars&lt;/h1&gt; &#xA;&lt;p&gt;This repository is maintained by the original creators of Hamilton, who have since founded &lt;a href=&#34;https://dagworks.io/&#34;&gt;DAGWorks inc.&lt;/a&gt;, a company largely dedicated to building and maintaining the Hamilton library. We decided to fork the original because Stitch Fix did not want to transfer ownership to us; we had grown the star count in the original repository to 893: &lt;img width=&#34;141&#34; alt=&#34;Screen Shot 2023-02-23 at 12 58 43 PM&#34; src=&#34;https://user-images.githubusercontent.com/2328071/221029806-36a4e088-809b-4b08-9199-2fbf8049be34.png&#34;&gt; before forking.&lt;/p&gt; &#xA;&lt;p&gt;For the backstory on how Hamilton came about, see the original Stitch Fix &lt;a href=&#34;https://multithreaded.stitchfix.com/blog/2021/10/14/functions-dags-hamilton/&#34;&gt;blog post!&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Slack Community&lt;/h1&gt; &#xA;&lt;p&gt;We have a small but active community on &lt;a href=&#34;https://join.slack.com/t/hamilton-opensource/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg&#34;&gt;slack&lt;/a&gt;. Come join us!&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Hamilton is released under the &lt;a href=&#34;https://github.com/DAGWorks-Inc/hamilton/raw/main/LICENSE.md&#34;&gt;BSD 3-Clause Clear License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Used internally by:&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.stitchfix.com/&#34;&gt;Stitch Fix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alphagov/govuk-feedback-analysis&#34;&gt;UK Government Digital Services&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ibm.com/&#34;&gt;IBM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.britishcycling.org.uk/&#34;&gt;British Cycling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pnnl.gov/&#34;&gt;PNNL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ornl.gov/&#34;&gt;ORNL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.federalreserve.gov/&#34;&gt;Federal Reserve Board&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jobyaviation.com/&#34;&gt;Joby Aviation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.two.inc/&#34;&gt;Two&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://transfix.io/&#34;&gt;Transfix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.railofy.com&#34;&gt;Railofy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.habitat.energy/&#34;&gt;Habitat Energy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ki-insurance.com/&#34;&gt;KI-Insurance&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ascena.com/&#34;&gt;Ascena Retail&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.opendoor.com/&#34;&gt;Opendoor&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To add your company, make a pull request to add it here.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We take contributions, large and small. We operate via a &lt;a href=&#34;https://github.com/DAGWorks-Inc/hamilton/raw/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt; and expect anyone contributing to do the same.&lt;/p&gt; &#xA;&lt;p&gt;To see how you can contribute, please read our &lt;a href=&#34;https://github.com/DAGWorks-Inc/hamilton/raw/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; and then our &lt;a href=&#34;https://github.com/DAGWorks-Inc/hamilton/raw/main/developer_setup.md&#34;&gt;developer setup guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Blog Posts&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/lineage-hamilton-in-10-minutes-c2b8a944e2e6&#34;&gt;Lineage + Hamilton in 10 minutes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@thijean/the-perks-of-creating-dataflows-with-hamilton-36e8c56dd2a&#34;&gt;(Organic Content) The perks of creating dataflows with Hamilton by Thierry Jean&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://outerbounds.com/blog/developing-scalable-feature-engineering-dags&#34;&gt;Developing Scalable Feature Engineering DAGs with Metaflow &amp;amp; Hamilton&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/tidy-production-pandas-with-hamilton-3b759a2bf562&#34;&gt;Tidy Production Pandas with Hamilton&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/functions-dags-introducing-hamilton-a-microframework-for-dataframe-generation-more-8e34b84efc1d&#34;&gt;Towards Data Science post on backstory &amp;amp; introduction&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@stefan.krawczyk/how-to-use-hamilton-with-pandas-in-5-minutes-89f63e5af8f5&#34;&gt;How to use Hamilton with Pandas in 5 minutes&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/how-to-iterate-with-hamilton-in-a-notebook-8ec0f85851ed&#34;&gt;How to iterate with Hamilton in a Notebook&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://multithreaded.stitchfix.com/blog/2021/10/14/functions-dags-hamilton/&#34;&gt;Original Stitch Fix Post&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://multithreaded.stitchfix.com/blog/2021/10/14/functions-dags-hamilton/&#34;&gt;Extension Stitch Fix Post&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Videos of talks&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=PDGIt37dov8&amp;amp;ab_channel=AICamp&#34;&gt;Hamilton: a python micro-framework for data/feature engineering at Stitch Fix - 40 mins&lt;/a&gt;: &lt;a href=&#34;https://youtu.be/PDGIt37dov8&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/PDGIt37dov8/hqdefault.jpg&#34; alt=&#34;Watch the video&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=m_rjCzxQj4c&amp;amp;ab_channel=Ponder&#34;&gt;Hamilton: a python micro-framework for tidy scalable pandas - ~20 mins&lt;/a&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=m_rjCzxQj4c&amp;amp;ab_channel=Ponder&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/m_rjCzxQj4c/hqdefault.jpg&#34; alt=&#34;Watch the video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Citing Hamilton&lt;/h1&gt; &#xA;&lt;p&gt;We&#39;d appreciate citing Hamilton by referencing one of the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{DBLP:conf/vldb/KrawczykI22,&#xA;  author    = {Stefan Krawczyk and Elijah ben Izzy},&#xA;  editor    = {Satyanarayana R. Valluri and Mohamed Za{\&#34;{\i}}t},&#xA;  title     = {Hamilton: a modular open source declarative paradigm for high level&#xA;               modeling of dataflows},&#xA;  booktitle = {1st International Workshop on Composable Data Management Systems,&#xA;               CDMS@VLDB 2022, Sydney, Australia, September 9, 2022},&#xA;  year      = {2022},&#xA;  url       = {https://cdmsworkshop.github.io/2022/Proceedings/ShortPapers/Paper6\_StefanKrawczyk.pdf},&#xA;  timestamp = {Wed, 19 Oct 2022 16:20:48 +0200},&#xA;  biburl    = {https://dblp.org/rec/conf/vldb/KrawczykI22.bib},&#xA;  bibsource = {dblp computer science bibliography, https://dblp.org}&#xA;}&#xA;&#xA;@inproceedings{CEURWS:conf/vldb/KrawczykIQ22,&#xA;  author    = {Stefan Krawczyk and Elijah ben Izzy and Danielle Quinn},&#xA;  editor    = {Cinzia Cappiello and Sandra Geisler and Maria-Esther Vidal},&#xA;  title     = {Hamilton: enabling software engineering best practices for data transformations via generalized dataflow graphs},&#xA;  booktitle = {1st International Workshop on Data Ecosystems co-located with 48th International Conference on Very Large Databases (VLDB 2022)},&#xA;  pages     = {41--50},&#xA;  url       = {https://ceur-ws.org/Vol-3306/paper5.pdf},&#xA;  year      = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;🛣🗺 Roadmap / Things you can do with Hamilton&lt;/h1&gt; &#xA;&lt;p&gt;Hamilton is an ambitious project to provide a unified way to describe any dataflow, independent of where it runs. You can find currently support integrations and high-level roadmap below. Please reach out via &lt;a href=&#34;https://join.slack.com/t/hamilton-opensource/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg&#34;&gt;slack&lt;/a&gt; or email (stefan / elijah at dagworks.io) to contribute or share feedback!&lt;/p&gt; &#xA;&lt;h2&gt;Object types:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Any python object type! E.g. Pandas, Spark dataframes, Dask dataframes, Ray datasets, Polars, dicts, lists, primitives, your custom objects, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Workflows:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; data processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; feature engineering&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; model training&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LLM application workflows&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; all of them together&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Quality&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://hamilton.dagworks.io/en/latest/how-tos/run-data-quality-checks.html&#34;&gt;data quality&lt;/a&gt; docs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Ability to define data quality check on an object.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pandera schema integration.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Custom object type validators.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Integration with other data quality libraries (e.g. Great Expectations, Deequ, whylogs, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Online Monitoring&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Open telemetry/tracing plugin.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Caching:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Checkpoint caching (e.g. save a function&#39;s result to disk, independent of input) - &lt;a href=&#34;https://github.com/DAGWorks-Inc/hamilton/pull/195&#34;&gt;WIP&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Finergrained caching (e.g. save a function&#39;s result to disk, dependent on input).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Execution:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Runs anywhere python runs. E.g. airflow, prefect, dagster, kubeflow, sagemaker, jupyter, fastAPI, snowpark, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Backend integrations:&lt;/h2&gt; &#xA;&lt;p&gt;Specific integrations with other systems where we help you write code that runs on those systems.&lt;/p&gt; &#xA;&lt;h3&gt;Ray&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Delegate function execution to Ray.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Function grouping (e.g. fuse multiple functions into a single Ray task)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Dask&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Delegate function execution to Dask.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Function grouping (e.g. fuse multiple functions into a single Dask task)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Spark&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pandas on spark integration (via GraphAdapter)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; PySpark native UDF map function integration (via GraphAdapter)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; PySpark native aggregation function integration&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; PySpark join, filter, groupby, etc. integration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Snowpark&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Packaging functions for Snowpark&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLVMs &amp;amp; related&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Numba integration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Custom Backends&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Generate code to execute on a custom topology, e.g. microservices, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Integrations with other systems/tools:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Generating Airflow | Prefect | Metaflow | Dagster | Kubeflow Pipelines | Sagemaker Pipelines | etc from Hamilton.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Plugins for common MLOps/DataOps tools: MLFlow, DBT, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dataflow/DAG Walking:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Depth first search traversal&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Async function support via AsyncDriver&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Parallel walk over a generator&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Python multiprocessing execution (still in beta)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Python threading support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Grouping of nodes into tasks for efficient parallel computation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Breadth first search traversal&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Sequential walk over a generator&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;DAG/Dataflow resolution:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; At Driver instantiation time, using configuration/modules and &lt;a href=&#34;https://hamilton.dagworks.io/en/latest/reference/api-reference/decorators.html#config&#34;&gt;&lt;code&gt;@config.when&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; With &lt;a href=&#34;https://hamilton.dagworks.io/en/latest/reference/api-reference/decorators.html#resolve&#34;&gt;&lt;code&gt;@resolve&lt;/code&gt;&lt;/a&gt; during Driver instantiation time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Prescribed Development Workflow&lt;/h1&gt; &#xA;&lt;p&gt;In general we prescribe the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ensure you understand &lt;a href=&#34;https://github.com/DAGWorks-Inc/hamilton/raw/main/basics.md&#34;&gt;Hamilton Basics&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Familiarize yourself with some of the &lt;a href=&#34;https://github.com/DAGWorks-Inc/hamilton/raw/main/decorators.md&#34;&gt;Hamilton decorators&lt;/a&gt;. They will help keep your code DRY.&lt;/li&gt; &#xA; &lt;li&gt;Start creating Hamilton Functions that represent your work. We suggest grouping them in modules where it makes sense.&lt;/li&gt; &#xA; &lt;li&gt;Write a simple script so that you can easily run things end to end.&lt;/li&gt; &#xA; &lt;li&gt;Join our &lt;a href=&#34;https://join.slack.com/t/hamilton-opensource/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg&#34;&gt;Slack&lt;/a&gt; community to chat/ask Qs/etc.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For the backstory on Hamilton we invite you to watch a roughly-9 minute lightning talk on it that we gave at the apply conference: &lt;a href=&#34;https://www.youtube.com/watch?v=B5Zp_30Knoo&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;https://www.slideshare.net/StefanKrawczyk/hamilton-a-micro-framework-for-creating-dataframes&#34;&gt;slides&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;PyCharm Tips&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re using Hamilton, it&#39;s likely that you&#39;ll need to migrate some code. Here are some useful tricks we found to speed up that process.&lt;/p&gt; &#xA;&lt;h3&gt;Live templates&lt;/h3&gt; &#xA;&lt;p&gt;Live templates are a cool feature and allow you to type in a name which expands into some code.&lt;/p&gt; &#xA;&lt;p&gt;E.g. For example, we wrote one to make it quick to stub out Hamilton functions: typing &lt;code&gt;graphfunc&lt;/code&gt; would turn into -&amp;gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def _(_: pd.Series) -&amp;gt; pd.Series:&#xA;   &#34;&#34;&#34;&#34;&#34;&#34;&#xA;   return _&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where the blanks are where you can tab with the cursor and fill things in. See your pycharm preferences for setting this up.&lt;/p&gt; &#xA;&lt;h3&gt;Multiple Cursors&lt;/h3&gt; &#xA;&lt;p&gt;If you are doing a lot of repetitive work, one might consider multiple cursors. Multiple cursors allow you to do things on multiple lines at once.&lt;/p&gt; &#xA;&lt;p&gt;To use it hit &lt;code&gt;option + mouse click&lt;/code&gt; to create multiple cursors. &lt;code&gt;Esc&lt;/code&gt; to revert back to a normal mode.&lt;/p&gt; &#xA;&lt;h1&gt;Usage analytics &amp;amp; data privacy&lt;/h1&gt; &#xA;&lt;p&gt;By default, when using Hamilton, it collects anonymous usage data to help improve Hamilton and know where to apply development efforts.&lt;/p&gt; &#xA;&lt;p&gt;We capture three types of events: one when the &lt;code&gt;Driver&lt;/code&gt; object is instantiated, one when the &lt;code&gt;execute()&lt;/code&gt; call on the &lt;code&gt;Driver&lt;/code&gt; object completes, and one for most &lt;code&gt;Driver&lt;/code&gt; object function invocations. No user data or potentially sensitive information is or ever will be collected. The captured data is limited to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Operating System and Python version&lt;/li&gt; &#xA; &lt;li&gt;A persistent UUID to indentify the session, stored in ~/.hamilton.conf.&lt;/li&gt; &#xA; &lt;li&gt;Error stack trace limited to Hamilton code, if one occurs.&lt;/li&gt; &#xA; &lt;li&gt;Information on what features you&#39;re using from Hamilton: decorators, adapters, result builders.&lt;/li&gt; &#xA; &lt;li&gt;How Hamilton is being used: number of final nodes in DAG, number of modules, size of objects passed to &lt;code&gt;execute()&lt;/code&gt;, the name of the Driver function being invoked.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;re worried, see telemetry.py for details.&lt;/p&gt; &#xA;&lt;p&gt;If you do not wish to participate, one can opt-out with one of the following methods:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set it to false programmatically in your code before creating a Hamilton driver: &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from hamilton import telemetry&#xA;telemetry.disable_telemetry()&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Set the key &lt;code&gt;telemetry_enabled&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt; in ~/.hamilton.conf under the &lt;code&gt;DEFAULT&lt;/code&gt; section: &lt;pre&gt;&lt;code&gt;[DEFAULT]&#xA;telemetry_enabled = False&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Set HAMILTON_TELEMETRY_ENABLED=false as an environment variable. Either setting it for your shell session: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export HAMILTON_TELEMETRY_ENABLED=false&#xA;&lt;/code&gt;&lt;/pre&gt; or passing it as part of the run command: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;HAMILTON_TELEMETRY_ENABLED=false python NAME_OF_MY_DRIVER.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Contributors&lt;/h1&gt; &#xA;&lt;h2&gt;Code Contributors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stefan Krawczyk (@skrawcz)&lt;/li&gt; &#xA; &lt;li&gt;Elijah ben Izzy (@elijahbenizzy)&lt;/li&gt; &#xA; &lt;li&gt;Danielle Quinn (@danfisher-sf)&lt;/li&gt; &#xA; &lt;li&gt;Rachel Insoft (@rinsoft-sf)&lt;/li&gt; &#xA; &lt;li&gt;Shelly Jang (@shellyjang)&lt;/li&gt; &#xA; &lt;li&gt;Vincent Chu (@vslchusf)&lt;/li&gt; &#xA; &lt;li&gt;Christopher Prohm (@chmp)&lt;/li&gt; &#xA; &lt;li&gt;James Lamb (@jameslamb)&lt;/li&gt; &#xA; &lt;li&gt;Avnish Pal (@bovem)&lt;/li&gt; &#xA; &lt;li&gt;Sarah Haskins (@frenchfrywpepper)&lt;/li&gt; &#xA; &lt;li&gt;Thierry Jean (@zilto)&lt;/li&gt; &#xA; &lt;li&gt;Michał Siedlaczek (@elshize)&lt;/li&gt; &#xA; &lt;li&gt;Benjamin Hack (@benhhack)&lt;/li&gt; &#xA; &lt;li&gt;Bryan Galindo (@bryangalindo)&lt;/li&gt; &#xA; &lt;li&gt;Jordan Smith (@JoJo10Smith)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Bug Hunters/Special Mentions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nils Olsson (@nilsso)&lt;/li&gt; &#xA; &lt;li&gt;Michał Siedlaczek (@elshize)&lt;/li&gt; &#xA; &lt;li&gt;Alaa Abedrabbo (@AAbedrabbo)&lt;/li&gt; &#xA; &lt;li&gt;Shreya Datar (@datarshreya)&lt;/li&gt; &#xA; &lt;li&gt;Baldo Faieta (@baldofaieta)&lt;/li&gt; &#xA; &lt;li&gt;Anwar Brini (@AnwarBrini)&lt;/li&gt; &#xA; &lt;li&gt;Gourav Kumar (@gms101)&lt;/li&gt; &#xA; &lt;li&gt;Amos Aikman (@amosaikman)&lt;/li&gt; &#xA; &lt;li&gt;Ankush Kundaliya (@akundaliya)&lt;/li&gt; &#xA; &lt;li&gt;David Weselowski (@j7zAhU)&lt;/li&gt; &#xA; &lt;li&gt;Peter Robinson (@Peter4137)&lt;/li&gt; &#xA; &lt;li&gt;Seth Stokes (@sT0v&lt;/li&gt; &#xA; &lt;li&gt;Louis Maddox (@lmmx)&lt;/li&gt; &#xA; &lt;li&gt;Stephen Bias (@s-ducks)&lt;/li&gt; &#xA; &lt;li&gt;Anup Joseph (@AnupJoseph)&lt;/li&gt; &#xA; &lt;li&gt;Jan Hurst (@janhurst)&lt;/li&gt; &#xA; &lt;li&gt;Flavia Santos (@flaviassantos)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>