<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-12T01:31:06Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bkitano/llama-from-scratch</title>
    <updated>2023-08-12T01:31:06Z</updated>
    <id>tag:github.com,2023-08-12:/bkitano/llama-from-scratch</id>
    <link href="https://github.com/bkitano/llama-from-scratch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Llama from scratch&lt;/h1&gt; &#xA;&lt;p&gt;I want to provide some tips from my experience implementing a paper. I&#39;m going to cover my tips so far from implementing a dramatically scaled-down version of &lt;a href=&#34;https://arxiv.org/pdf/2302.13971.pdf&#34;&gt;Llama&lt;/a&gt; for training &lt;a href=&#34;https://github.com/karpathy/char-rnn/raw/master/data/tinyshakespeare/input.txt&#34;&gt;TinyShakespeare&lt;/a&gt;. This post is heavily inspired by Karpathy&#39;s &lt;a href=&#34;https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&#34;&gt;Makemore series&lt;/a&gt;, which I highly recommend.&lt;/p&gt; &#xA;&lt;p&gt;I&#39;m only going to &lt;em&gt;loosely&lt;/em&gt; follow the layout of their paper; while the formatting and order of sections makes sense for publication, we&#39;re going to be implementing the paper. I&#39;ll also be skipping over some of the more obvious steps, like setting up a virtual environment and installing dependencies.&lt;/p&gt; &#xA;&lt;h1&gt;Takeaways&lt;/h1&gt; &#xA;&lt;h2&gt;Always work iteratively: start small, stay certain, and build up.&lt;/h2&gt; &#xA;&lt;p&gt;My approach for implementing papers is:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make all of the helper functions required to test your model quantitatively (data splits, training, plotting the loss).&lt;/li&gt; &#xA; &lt;li&gt;Before you even look at the paper, pick a small, simple, and fast model that you&#39;ve done in the past. Then make a helper function to evaluate the model qualitatively.&lt;/li&gt; &#xA; &lt;li&gt;Start by picking apart different components of the paper, and then implementing them one-by-one, training and evaluating as you go.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Make sure your layers do what you think.&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use &lt;code&gt;.shape&lt;/code&gt; religiously. &lt;code&gt;assert&lt;/code&gt; and &lt;code&gt;plt.imshow&lt;/code&gt; are also your friends.&lt;/li&gt; &#xA; &lt;li&gt;Work out the results without matrix multiplication first, and then use the &lt;code&gt;torch&lt;/code&gt; functions to make it efficient after.&lt;/li&gt; &#xA; &lt;li&gt;Have a test to see that your layer is right. For example, the RoPE embeddings have a specific property that you can test for. For the Transformer, you can test that the attention is working by looking at the attention map.&lt;/li&gt; &#xA; &lt;li&gt;Test your layers on various batch, sequence, and embedding sizes. Even if it works for one size, it might not work for others, which will cause problems at inference time.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;About Llama&lt;/h2&gt; &#xA;&lt;p&gt;Llama is a transformer-based model for language modeling. Meta AI &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;open-sourced&lt;/a&gt; Llama this summer, and it&#39;s gained a lot of attention (pun intended). When you&#39;re reading the introduction, they clearly indicate their goal: make a model that&#39;s cheaper for running inference, rather than optimizing training costs.&lt;/p&gt; &#xA;&lt;p&gt;At this point, we&#39;ll just load our libraries and get started.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;import numpy as np&#xA;from matplotlib import pyplot as plt&#xA;import time&#xA;import pandas as pd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setting up our dataset&lt;/h2&gt; &#xA;&lt;p&gt;While in Llama they train on 1.4T tokens, our dataset TinyShakespeare, the collection of all of Shakespeare&#39;s works, is about 1M characters.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lines = open(&#39;./input.txt&#39;, &#39;r&#39;).read()&#xA;&#xA;vocab = sorted(list(set(lines)))&#xA;itos = {i:ch for i, ch in enumerate(vocab)}&#xA;stoi = {ch:i for i, ch in enumerate(vocab)}&#xA;&#xA;print(lines[:30])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;First Citizen:&#xA;Before we proce&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;They use the &lt;a href=&#34;https://github.com/google/sentencepiece&#34;&gt;SentencePiece&lt;/a&gt; byte-pair encoding tokenizer, but we&#39;re going to just use a simple character-level tokenizer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# simple tokenization by characters&#xA;def encode(s):&#xA;    return [stoi[ch] for ch in s]&#xA;&#xA;def decode(l):&#xA;    return &#39;&#39;.join([itos[i] for i in l])&#xA;&#xA;print(&#39;vocab size:&#39;, len(vocab))&#xA;decode(encode(&#34;hello&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;vocab size: 65&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#39;hello&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since our dataset is small enough, we don&#39;t need to worry about how we store it in memory etc.&lt;/p&gt; &#xA;&lt;p&gt;First tip: I&#39;m creating a &lt;code&gt;config&lt;/code&gt; object that stores some basic model params. It makes our code way more readable and removes constants and magic numbers from the code. I&#39;m not going to use types, as I want to keep things flexible for now and be able to add more parameters later on.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;MASTER_CONFIG = {&#xA;    &#34;vocab_size&#34;: len(vocab),&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset = torch.tensor(encode(lines), dtype=torch.int8)&#xA;dataset.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([1115394])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s create a method to generate our training data and labels for batches. We&#39;ll use the same method for validation and test data. Note that I like to test my functions in the same block that I define them, just to make sure they work as expected before moving on.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_batches(data, split, batch_size, context_window, config=MASTER_CONFIG):&#xA;    train = data[:int(.8 * len(data))]&#xA;    val = data[int(.8 * len(data)): int(.9 * len(data))]&#xA;    test = data[int(.9 * len(data)):]&#xA;    &#xA;    batch_data = train&#xA;    if split == &#39;val&#39;:&#xA;        batch_data = val&#xA;&#xA;    if split == &#39;test&#39;:&#xA;        batch_data = test&#xA;    &#xA;    # pick random starting points&#xA;    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))&#xA;    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()&#xA;    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()&#xA;    return x, y&#xA;&#xA;MASTER_CONFIG.update({&#xA;    &#39;batch_size&#39;: 32,&#xA;    &#39;context_window&#39;: 16&#xA;})&#xA;&#xA;xs, ys = get_batches(dataset, &#39;train&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])&#xA;&#xA;[(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;[(&#39;t us sup betimes&#39;, &#39; us sup betimes,&#39;),&#xA; (&#39;IO:\nRight.\n\nROME&#39;, &#39;O:\nRight.\n\nROMEO&#39;),&#xA; (&#39;Nurse:\nO, she sa&#39;, &#39;urse:\nO, she say&#39;),&#xA; (&#34; seem&#39;d to know,&#34;, &#34;seem&#39;d to know,\n&#34;),&#xA; (&#39; let her brother&#39;, &#39;let her brother &#39;),&#xA; (&#39;ood,\nEven with s&#39;, &#39;od,\nEven with su&#39;),&#xA; (&#39;en\nWhisper the s&#39;, &#39;n\nWhisper the sp&#39;),&#xA; (&#39;, fly! for all y&#39;, &#39; fly! for all yo&#39;),&#xA; (&#34; Saint Peter&#39;s C&#34;, &#34;Saint Peter&#39;s Ch&#34;),&#xA; (&#39;, but that\nWhich&#39;, &#39; but that\nWhich &#39;),&#xA; (&#39;uld as willingly&#39;, &#39;ld as willingly &#39;),&#xA; (&#39;y brother,\nIs pr&#39;, &#39; brother,\nIs pri&#39;),&#xA; (&#39; you ready your &#39;, &#39;you ready your s&#39;),&#xA; (&#39;rth the audience&#39;, &#39;th the audience &#39;),&#xA; (&#39;\n\nQUEEN ELIZABET&#39;, &#39;\nQUEEN ELIZABETH&#39;),&#xA; (&#39;ection,\nwhich ca&#39;, &#39;ction,\nwhich can&#39;),&#xA; (&#39;is wisdom hastes&#39;, &#39;s wisdom hastes &#39;),&#xA; (&#39; and quinces in &#39;, &#39;and quinces in t&#39;),&#xA; (&#39;nt death.\n\nSICIN&#39;, &#39;t death.\n\nSICINI&#39;),&#xA; (&#34;y she&#39;s mad.\n\nBR&#34;, &#34; she&#39;s mad.\n\nBRU&#34;),&#xA; (&#39;eware of him;\nSi&#39;, &#39;ware of him;\nSin&#39;),&#xA; (&#39;s\nAnd make pursu&#39;, &#39;\nAnd make pursui&#39;),&#xA; (&#39;r and be slain; &#39;, &#39; and be slain; n&#39;),&#xA; (&#39; I, with grief a&#39;, &#39;I, with grief an&#39;),&#xA; (&#39;?\n\nSecond Keeper&#39;, &#39;\n\nSecond Keeper:&#39;),&#xA; (&#39;\nNow, Thomas Mow&#39;, &#39;Now, Thomas Mowb&#39;),&#xA; (&#39;or this once, ye&#39;, &#39;r this once, yea&#39;),&#xA; (&#34;l &#39;tis just.\n\nLU&#34;, &#34; &#39;tis just.\n\nLUC&#34;),&#xA; (&#39;es like a lamb. &#39;, &#39;s like a lamb. Y&#39;),&#xA; (&#39;t night, I warra&#39;, &#39; night, I warran&#39;),&#xA; (&#39;y tears would wa&#39;, &#39; tears would was&#39;),&#xA; (&#39;\n\nANGELO:\nWell, &#39;, &#39;\nANGELO:\nWell, l&#39;)]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;What&#39;s interesting about implementing papers is that there are two aspects to the model &lt;em&gt;working&lt;/em&gt;: compilation (do your tensors all match up from layer to layer), and training (does the loss go down). Figuring out how to ensure that each of your compoenents is working is key to developing your model in a predictable, engineering-minded way.&lt;/p&gt; &#xA;&lt;p&gt;That&#39;s why we&#39;re also going to define the method for how we&#39;re going to evaluate the model. We want to do this before we even define the model, because we want to be able to use it to evaluate the model as we&#39;re training it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@torch.no_grad()  # don&#39;t compute gradients for this function&#xA;def evaluate_loss(model, config=MASTER_CONFIG):&#xA;    out = {}&#xA;    model.eval()&#xA;    for split in [&#34;train&#34;, &#34;val&#34;]:&#xA;        losses = []&#xA;        for _ in range(10):&#xA;            xb, yb = get_batches(dataset, split, config[&#39;batch_size&#39;], config[&#39;context_window&#39;])&#xA;            _, loss = model(xb, yb)&#xA;            losses.append(loss.item())&#xA;        out[split] = np.mean(losses)&#xA;    model.train()&#xA;    return out&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setting up a working base model&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s a basic feed-forward neural network with embeddings. It&#39;s the base model we&#39;re going to start with, and then swap out parts of it as we go along until we eventually end up with the model as described in Llama.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SimpleBrokenModel(nn.Module):&#xA;    def __init__(self, config=MASTER_CONFIG):&#xA;        super().__init__()&#xA;        self.config = config&#xA;&#xA;        self.embedding = nn.Embedding(config[&#39;vocab_size&#39;], config[&#39;d_model&#39;])&#xA;        self.linear = nn.Sequential(&#xA;            nn.Linear(config[&#39;d_model&#39;], config[&#39;d_model&#39;]),&#xA;            nn.ReLU(),&#xA;            nn.Linear(config[&#39;d_model&#39;], config[&#39;vocab_size&#39;]),&#xA;        )&#xA;&#xA;        print(&#34;model params:&#34;, sum([m.numel() for m in self.parameters()]))&#xA;&#xA;    def forward(self, idx, targets=None):&#xA;        x = self.embedding(idx)&#xA;        a = self.linear(x)&#xA;        logits = F.softmax(a, dim=-1)&#xA;&#xA;        if targets is not None:&#xA;            loss = F.cross_entropy(logits.view(-1, self.config[&#39;vocab_size&#39;]), targets.view(-1))&#xA;            return logits, loss&#xA;&#xA;        else:&#xA;            return logits&#xA;&#xA;MASTER_CONFIG.update({&#xA;    &#39;d_model&#39;: 128,&#xA;})&#xA;model = SimpleBrokenModel(MASTER_CONFIG)&#xA;xs, ys = get_batches(dataset, &#39;train&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])&#xA;&#xA;logits, loss = model(xs, ys)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;model params: 33217&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It&#39;s at this point that we have to start worrying about the shape of our tensors and making indices match. Check out this line of our model definition:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;loss = F.cross_entropy(logits.view(-1, config[&#39;vocab_size&#39;]), targets.view(-1))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have to reshape the &lt;code&gt;logits&lt;/code&gt; and &lt;code&gt;targets&lt;/code&gt; tensors so that their dimensions match when we compare. We do this with the &lt;code&gt;view&lt;/code&gt; method. The &lt;code&gt;-1&lt;/code&gt; argument means &#34;infer this dimension from the others&#34;. So, in this case, we&#39;re saying &#34;reshape &lt;code&gt;logits&lt;/code&gt; and &lt;code&gt;targets&lt;/code&gt; to have the same number of rows, and however many columns are needed to make that happen&#34;. This is a common pattern when you&#39;re working with batches of data.&lt;/p&gt; &#xA;&lt;p&gt;Alright, let&#39;s train our &lt;code&gt;SimpleBrokenModel&lt;/code&gt; to make sure gradients flow. After we confirm that, we can swap out parts of it to match Llama, train again, and track our progress. It&#39;s at this point that I start keeping a &lt;em&gt;log&lt;/em&gt; of my training runs, so that I can easily just go back to a previous run in the event that I mess something up.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;MASTER_CONFIG.update({&#xA;    &#39;epochs&#39;: 1000,&#xA;    &#39;log_interval&#39;: 10 &#xA;})&#xA;model = SimpleBrokenModel(MASTER_CONFIG)&#xA;&#xA;optimizer = torch.optim.Adam(&#xA;    model.parameters(), &#xA;)&#xA;&#xA;def train(model, optimizer, scheduler=None, config=MASTER_CONFIG, print_logs=False):&#xA;    losses = []&#xA;    start_time = time.time()&#xA;    for epoch in range(config[&#39;epochs&#39;]):&#xA;        optimizer.zero_grad()&#xA;        &#xA;        xs, ys = get_batches(dataset, &#39;train&#39;, config[&#39;batch_size&#39;], config[&#39;context_window&#39;])&#xA;        logits, loss = model(xs, targets=ys)&#xA;        loss.backward()&#xA;        optimizer.step()&#xA;        &#xA;        if scheduler:&#xA;            scheduler.step()&#xA;        &#xA;        if epoch % config[&#39;log_interval&#39;] == 0:&#xA;            batch_time = time.time() - start_time&#xA;            x = evaluate_loss(model)&#xA;            losses += [x]&#xA;            if print_logs:&#xA;                print(f&#34;Epoch {epoch} | val loss {x[&#39;val&#39;]:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config[&#39;epochs&#39;] - epoch)/config[&#39;log_interval&#39;] :.3f}&#34;)&#xA;            start_time = time.time()&#xA;&#xA;            if scheduler:&#xA;                print(&#34;lr: &#34;, scheduler.get_lr())&#xA;&#xA;    print(&#34;validation loss: &#34;, losses[-1][&#39;val&#39;])&#xA;    return pd.DataFrame(losses).plot()&#xA;&#xA;train(model, optimizer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;model params: 33217&#xA;validation loss:  3.9457355260849&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&amp;lt;Axes: &amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bkitano/llama-from-scratch/main/llama_files/llama_22_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Notice how we get a training curve that goes down, but barely by anything. How do we know it&#39;s barely training? We have to use first principles. The cross-entropy loss before training is 4.17, and after 1000 epochs is 3.93. How can we make sense of it intuitively?&lt;/p&gt; &#xA;&lt;p&gt;Cross-entropy in this context is referring to how likely we are to pick the wrong word. So here,&lt;/p&gt; &#xA;&lt;p&gt;$$ H(T, q) = - \sum_{i = 1}^{N} \frac{1}{N} \log q(x_i) $$&lt;/p&gt; &#xA;&lt;p&gt;where $q(x_i)$ is the probability of picking the right word, as estimated by the model. If $q(x_i)$ is close to 1, then $\log q$ is close to 0; similarly, if $q$ is small, then $\log q$ is a large negative number, so $-\log q$ will be a large positive number. Now to build the intuition: to start, $-\log q = 4.17$, so $q = 0.015$, or around $\frac{1}{64.715}$. Recall that the vocabulary size $|V| = 65$, so what we&#39;re basically saying here is that the model is as good at choosing the next letter as randomly picking from our vocabulary. After training, $-\log q = 3.93$, so we&#39;re now basically choosing between 50 letters. This is a very small improvement, so something is probably wrong.&lt;/p&gt; &#xA;&lt;p&gt;To get an intuition for how the loss relates to the model&#39;s performance, think about the model choosing among $\tilde V$ tokens; when $\tilde V$ is small, the model is more likely to guess right. In addition, we know $\max \tilde V = V$, which can help us understand if our model is learning at all.&lt;/p&gt; &#xA;&lt;p&gt;$$\tilde V = \exp(L)$$&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s try to debug what&#39;s going on. Notice that in our model we&#39;re using a softmax layer on our logits, which is a function that takes a vector of numbers and squashes them into a probability distribution. But for using the built in &lt;code&gt;F.cross_entropy&lt;/code&gt; function, we need to pass in the &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html&#34;&gt;unnormalized logits directly&lt;/a&gt;. So let&#39;s remove that from our model and try again.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SimpleModel(nn.Module):&#xA;    def __init__(self, config):&#xA;        super().__init__()&#xA;        self.config = config&#xA;&#xA;        self.embedding = nn.Embedding(config[&#39;vocab_size&#39;], config[&#39;d_model&#39;])&#xA;        self.linear = nn.Sequential(&#xA;            nn.Linear(config[&#39;d_model&#39;], config[&#39;d_model&#39;]),&#xA;            nn.ReLU(),&#xA;            nn.Linear(config[&#39;d_model&#39;], config[&#39;vocab_size&#39;]),&#xA;        )&#xA;&#xA;        print(&#34;model params:&#34;, sum([m.numel() for m in self.parameters()]))&#xA;&#xA;    def forward(self, idx, targets=None):&#xA;        x = self.embedding(idx)&#xA;        logits = self.linear(x)&#xA;&#xA;        if targets is not None:&#xA;            loss = F.cross_entropy(logits.view(-1, self.config[&#39;vocab_size&#39;]), targets.view(-1))&#xA;            return logits, loss&#xA;&#xA;        else:&#xA;            return logits&#xA;&#xA;model = SimpleModel(MASTER_CONFIG)&#xA;xs, ys = get_batches(dataset, &#39;train&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])&#xA;&#xA;logits, loss = model(xs, ys)&#xA;optimizer = torch.optim.Adam(model.parameters())&#xA;train(model, optimizer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;model params: 33217&#xA;validation loss:  2.5113263607025145&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&amp;lt;Axes: &amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bkitano/llama-from-scratch/main/llama_files/llama_24_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Great, now our loss is $2.54$, so we&#39;re choosing from $12.67$ characters. That&#39;s way better than the 65 we started with. Let&#39;s add a generate method to our model so we visually see the results of our model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate(model, config=MASTER_CONFIG, max_new_tokens=30):&#xA;    idx = torch.zeros(5, 1).long()&#xA;    for _ in range(max_new_tokens):&#xA;        # call the model&#xA;        logits = model(idx[:, -config[&#39;context_window&#39;]:])&#xA;        last_time_step_logits = logits[&#xA;            :, -1, :&#xA;        ]  # all the batches (1), last time step, all the logits&#xA;        p = F.softmax(last_time_step_logits, dim=-1)  # softmax to get probabilities&#xA;        idx_next = torch.multinomial(&#xA;            p, num_samples=1&#xA;        )  # sample from the distribution to get the next token&#xA;        idx = torch.cat([idx, idx_next], dim=-1)  # append to the sequence&#xA;    return [decode(x) for x in idx.tolist()]&#xA;&#xA;generate(model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;[&#39;\nWI in\nThed grtend\nA yod ys wit&#39;,&#xA; &#39;\nY aroticunutser\nE oy mendomed &#39;,&#xA; &#34;\n\nRIf t fan f ses, k be wn&#39;d mo&#34;,&#xA; &#39;\nRu hiseedst den t wat onderyou&#39;,&#xA; &#34;\nARaceps hond wr f\nI&#39; fu kn be &#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It&#39;s not half bad, but also not half good. But now we have a working model that is training to a validation loss. So here we&#39;ll iterate on our model to make it closer to Llama.&lt;/p&gt; &#xA;&lt;h2&gt;Llama specifics&lt;/h2&gt; &#xA;&lt;p&gt;Llama describes three architectural modifications to the original Transformer:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;RMSNorm for pre-normalization&lt;/li&gt; &#xA; &lt;li&gt;Rotary embeddings&lt;/li&gt; &#xA; &lt;li&gt;SwiGLU activation function&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We&#39;re going to add each one, one at a time to our base model, and iterate.&lt;/p&gt; &#xA;&lt;h3&gt;RMSNorm&lt;/h3&gt; &#xA;&lt;p&gt;In Vaswani 2017, the original transformer uses BatchNormalization. In Llama, the authors use RMSNorm, which is where you scale the bector by the variance without centering it. In addition, while Vaswani applies normalization to the output of the attention layer (post-normalization), Llama applies it to the inputs before (pre-normalization).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class RMSNorm(nn.Module):&#xA;    def __init__(self, layer_shape, eps=1e-8, bias=False):&#xA;        super(RMSNorm, self).__init__()&#xA;        self.register_parameter(&#34;scale&#34;, nn.Parameter(torch.ones(layer_shape)))&#xA;&#xA;    def forward(self, x):&#xA;        &#34;&#34;&#34;&#xA;        assumes shape is (batch, seq_len, d_model)&#xA;        &#34;&#34;&#34;&#xA;        # frob norm is not the same as RMS. RMS = 1/sqrt(N) * frob norm&#xA;        ff_rms = torch.linalg.norm(x, dim=(1,2)) * x[0].numel() ** -.5&#xA;        raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)&#xA;        return self.scale[:x.shape[1], :].unsqueeze(0) * raw&#xA;&#xA;config = {&#xA;    &#39;batch_size&#39;: 5,&#xA;    &#39;context_window&#39;: 11,&#xA;    &#39;d_model&#39;: 13,&#xA;}&#xA;batch = torch.randn((config[&#39;batch_size&#39;], config[&#39;context_window&#39;], config[&#39;d_model&#39;]))&#xA;m = RMSNorm((config[&#39;context_window&#39;], config[&#39;d_model&#39;]))&#xA;g = m(batch)&#xA;print(g.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([5, 11, 13])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We want to test to ensure that the RMSNorm is doing what we think it should. We can do this the old-fashioned way: row-wise comparisons. The RMSNorm has the property where the norm of the layer will be the square root of the number of elements in the layer, so we can check that for every layer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rms = torch.linalg.norm(batch, dim=(1,2)) * (batch[0].numel() ** -.5)&#xA;&#xA;# scaled_batch.var(dim=(1,2))&#xA;assert torch.linalg.norm( torch.arange(5).float() ) == (torch.arange(5).float() ** 2 ).sum() ** .5&#xA;rms = torch.linalg.norm( torch.arange(5).float() ) * (torch.arange(5).numel() ** -.5)&#xA;assert torch.allclose(torch.linalg.norm(torch.arange(5).float() / rms), torch.tensor(5 ** .5))&#xA;ff_rms = torch.linalg.norm(batch, dim=(1,2)) * batch.shape[1:].numel() ** -.5&#xA;&#xA;# RMS for sure&#xA;ffx = torch.zeros_like(batch)&#xA;for i in range(batch.shape[0]):&#xA;    ffx[i] = batch[i] / ff_rms[i]&#xA;assert torch.allclose(torch.linalg.norm(ffx, dim=(1,2)) ** 2, torch.tensor(143).float())&#xA;assert torch.allclose(ffx, g)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alright, so that&#39;s RMSNorm, and it seems like it&#39;s working. Again, let&#39;s test it out.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SimpleModel_RMS(nn.Module):&#xA;    def __init__(self, config):&#xA;        super().__init__()&#xA;        self.config = config&#xA;&#xA;        self.embedding = nn.Embedding(config[&#39;vocab_size&#39;], config[&#39;d_model&#39;])&#xA;        self.rms = RMSNorm((config[&#39;context_window&#39;], config[&#39;d_model&#39;]))&#xA;        self.linear = nn.Sequential(&#xA;            nn.Linear(config[&#39;d_model&#39;], config[&#39;d_model&#39;]),&#xA;            nn.ReLU(),&#xA;            nn.Linear(config[&#39;d_model&#39;], config[&#39;vocab_size&#39;]),&#xA;        )&#xA;&#xA;        print(&#34;model params:&#34;, sum([m.numel() for m in self.parameters()]))&#xA;&#xA;    def forward(self, idx, targets=None):&#xA;        x = self.embedding(idx)&#xA;        x = self.rms(x) # rms pre-normalization&#xA;        logits = self.linear(x)&#xA;&#xA;        if targets is not None:&#xA;            loss = F.cross_entropy(logits.view(-1, self.config[&#39;vocab_size&#39;]), targets.view(-1))&#xA;            return logits, loss&#xA;&#xA;        else:&#xA;            return logits&#xA;&#xA;model = SimpleModel_RMS(MASTER_CONFIG)&#xA;xs, ys = get_batches(dataset, &#39;train&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])&#xA;&#xA;logits, loss = model(xs, ys)&#xA;optimizer = torch.optim.Adam(model.parameters())&#xA;train(model, optimizer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;model params: 35265&#xA;validation loss:  2.4792869329452514&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&amp;lt;Axes: &amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bkitano/llama-from-scratch/main/llama_files/llama_34_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;So RMSNorm works, and it got our loss down by a small amount.&lt;/p&gt; &#xA;&lt;h3&gt;Rotary Embeddings&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2104.09864.pdf&#34;&gt;RoPE&lt;/a&gt; is a kind of positional encoding for transformers. In Attention is All You Need, the authors propose two kinds of positional encodings, learned and fixed. In RoPE, the authors propose embedding the position of a token in a sequence by rotating the embedding, with a different rotation at each position.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_rotary_matrix(context_window, embedding_dim):&#xA;        R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)&#xA;        for position in range(context_window):&#xA;            for i in range(embedding_dim//2):&#xA;                theta = 10000. ** (-2.*(i - 1) / embedding_dim)&#xA;                m_theta = position * theta&#xA;                R[position, 2*i,2*i] = np.cos(m_theta)&#xA;                R[position, 2*i,2*i+1] = - np.sin(m_theta)&#xA;                R[position, 2*i+1,2*i] = np.sin(m_theta)&#xA;                R[position, 2*i+1,2*i+1] = np.cos(m_theta)&#xA;        return R&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;K = 3&#xA;config = {&#xA;    &#39;batch_size&#39;: 10,&#xA;    &#39;d_model&#39;: 32,&#xA;    &#39;n_heads&#39;: 8,&#xA;    &#39;context_window&#39;: K**2,&#xA;}&#xA;batch = torch.randn(1, config[&#39;context_window&#39;], config[&#39;d_model&#39;])&#xA;R = get_rotary_matrix(config[&#39;context_window&#39;], config[&#39;d_model&#39;])&#xA;fig, ax = plt.subplots(K, K, figsize=(K * 3, K * 4))&#xA;&#xA;for i in range(K):&#xA;    for j in range(K):&#xA;        ax[i, j].imshow(R[i * K + j, :, :].detach().numpy())&#xA;        ax[i, j].set_title(f&#39;rotation at {i * K + j}&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bkitano/llama-from-scratch/main/llama_files/llama_38_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s make sure these work. They should exhibit the quality that $$ q_m^T k_n = (R^d_{\Theta, m}W_q x_m)^T (R^d_{\Theta, n} W_k x_n) = x^T W_q R^d_{\Theta, n-m} W_k x_n. $$&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;config = {&#xA;    &#39;d_model&#39;: 128,&#xA;    &#39;context_window&#39;: 16,&#xA;}&#xA;&#xA;R = get_rotary_matrix(config[&#39;context_window&#39;], config[&#39;d_model&#39;])&#xA;x = torch.randn(config[&#39;d_model&#39;])&#xA;y = torch.randn(config[&#39;d_model&#39;])&#xA;&#xA;m = 3&#xA;n = 13&#xA;&#xA;x_m = R[m,:,:] @ x&#xA;x_n = R[n,:,:] @ y&#xA;&#xA;assert torch.isclose(x_m @ x_n, x @ R[n-m,:,:] @ y)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;So the RoPE rotations work as expected.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;config = {&#xA;    &#39;batch_size&#39;: 10,&#xA;    &#39;d_model&#39;: 512,&#xA;    &#39;n_heads&#39;: 8,&#xA;    &#39;context_window&#39;: 16,&#xA;}&#xA;&#xA;class RoPEAttention(nn.Module):&#xA;    def __init__(self, config):&#xA;        super().__init__()&#xA;        self.config = config&#xA;        self.w_q = nn.Linear(config[&#39;d_model&#39;], config[&#39;d_model&#39;], bias=False)&#xA;        self.w_k = nn.Linear(config[&#39;d_model&#39;], config[&#39;d_model&#39;], bias=False)&#xA;        self.w_v = nn.Linear(config[&#39;d_model&#39;], config[&#39;d_model&#39;], bias=False)&#xA;&#xA;        self.multihead = nn.MultiheadAttention(config[&#39;d_model&#39;], config[&#39;n_heads&#39;], dropout=0.1, batch_first=True)&#xA;        self.R = get_rotary_matrix(config[&#39;context_window&#39;], config[&#39;d_model&#39;])&#xA;&#xA;    def get_rotary_matrix(context_window, embedding_dim):&#xA;        R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)&#xA;        for position in range(context_window):&#xA;            for i in range(embedding_dim//2):&#xA;                theta = 10000. ** (-2.*(i - 1) / embedding_dim)&#xA;                m_theta = position * theta&#xA;                R[position, 2*i,2*i] = np.cos(m_theta)&#xA;                R[position, 2*i,2*i+1] = - np.sin(m_theta)&#xA;                R[position, 2*i+1,2*i] = np.sin(m_theta)&#xA;                R[position, 2*i+1,2*i+1] = np.cos(m_theta)&#xA;        return R&#xA;    &#xA;    def forward(self, x, return_attn_weights=False):&#xA;        b,m,d = x.shape&#xA;        &#xA;        q = self.w_q(x)&#xA;        k = self.w_k(x)&#xA;        v = self.w_v(x)&#xA;&#xA;        q_out = (torch.bmm(q.transpose(0,1), self.R)).transpose(0,1)&#xA;        k_out = (torch.bmm(k.transpose(0,1), self.R)).transpose(0,1)&#xA;        v_out = (torch.bmm(v.transpose(0,1), self.R)).transpose(0,1)&#xA;&#xA;        activations, attn_weights = self.multihead(&#xA;            q_out,k_out,v_out, &#xA;        )&#xA;&#xA;        if return_attn_weights:&#xA;            return activations, attn_weights&#xA;        return activations&#xA;&#xA;layer = RoPEAttention(config)&#xA;batch = torch.randn((config[&#39;batch_size&#39;], config[&#39;context_window&#39;], config[&#39;d_model&#39;]))&#xA;output, attn_weights = layer(batch, return_attn_weights=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Tip here: know the difference between tensor dimensions at train time vs tensor dimensions at inference time.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Although at train time, you can expect your tensor dimensions to match your model parameters closely, eg &lt;code&gt;batch.shape = (config[&#39;batch_size&#39;], config[&#39;context_window&#39;], config[&#39;d_model&#39;])&lt;/code&gt;, at inference time, you may have to deal with a single example, eg &lt;code&gt;batch.shape = (1, 1, config[&#39;d_model&#39;])&lt;/code&gt;. For this reason, you need to make sure that when you&#39;re indexing in the &lt;code&gt;forward&lt;/code&gt; pass, you&#39;re indexing using shapes derived from the input, not necessarily the model parameters.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s make sure it does what we think it does. For this layer, we&#39;re going to want to test three things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;that it rotates embeddings the way we think it does&lt;/li&gt; &#xA; &lt;li&gt;that the attention mask used for causal attention is working properly.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = torch.randn((config[&#39;batch_size&#39;], config[&#39;context_window&#39;], config[&#39;d_model&#39;]))&#xA;&#xA;q = layer.w_q(x)&#xA;k = layer.w_k(x)&#xA;v = layer.w_v(x)&#xA;&#xA;q_rotated = torch.zeros_like(x)&#xA;k_rotated = torch.zeros_like(x)&#xA;v_rotated = torch.zeros_like(x)&#xA;&#xA;for position in range(config[&#39;context_window&#39;]):&#xA;    q_rotated[:,position,:] = torch.matmul(q[:,position,:], layer.R[position,:,:])&#xA;    k_rotated[:,position,:] = torch.matmul(k[:,position,:], layer.R[position,:,:])&#xA;    v_rotated[:,position,:] = torch.matmul(v[:,position,:], layer.R[position,:,:])&#xA;&#xA;q_out = (torch.bmm(q.transpose(0,1), layer.R)).transpose(0,1)&#xA;k_out = (torch.bmm(k.transpose(0,1), layer.R)).transpose(0,1)&#xA;v_out = (torch.bmm(v.transpose(0,1), layer.R)).transpose(0,1)&#xA;&#xA;assert torch.allclose(q.transpose(0,1)[0], q[:,0,:])&#xA;assert torch.allclose(q.transpose(0,1)[0] @ layer.R[0], q[:,0,:] @ layer.R[0])&#xA;assert torch.allclose(q_rotated, q_out)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;config = {&#xA;    &#39;batch_size&#39;: 1,&#xA;    &#39;d_model&#39;: 2,&#xA;    &#39;n_heads&#39;: 2,&#xA;    &#39;context_window&#39;: 3,&#xA;}&#xA;&#xA;layer = RoPEAttention(config)&#xA;batch = torch.ones((config[&#39;batch_size&#39;], config[&#39;context_window&#39;], config[&#39;d_model&#39;]))&#xA;output, attn_weights = layer(batch, return_attn_weights=True)&#xA;&#xA;m = 0&#xA;x_q = batch[0, m]&#xA;q = layer.R[m,:,:] @ layer.w_q(x_q)&#xA;&#xA;assert torch.allclose(layer.w_q(x_q), layer.w_q.weight @ x_q)&#xA;assert torch.allclose(q, layer.R[m, :, :] @ layer.w_q.weight @ x_q)&#xA;&#xA;n = 2&#xA;x_k = batch[0, n]&#xA;k = layer.R[n,:,:] @ layer.w_k(x_k)&#xA;&#xA;assert torch.allclose(layer.w_k(x_k), layer.w_k.weight @ x_k)&#xA;assert torch.allclose(k, layer.R[n, :, :] @ layer.w_k.weight @ x_k)&#xA;&#xA;assert q.T @ k == q @ k # transpose is redundant&#xA;assert torch.allclose(q @ k, x_k.T @ layer.w_k.weight.T @ layer.R[n, :, :].T @ layer.R[m, :, :] @ layer.w_q.weight @ x_q)&#xA;assert torch.allclose(q @ k, x_k.T @ layer.w_k.weight.T @ layer.R[n-m, :, :].T @ layer.w_q.weight @ x_q)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;/var/folders/w4/2j887mvs097bkhhjpgfzjlyr0000gn/T/ipykernel_17478/2550954139.py:26: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3575.)&#xA;  assert q.T @ k == q @ k # transpose is redundant&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now let&#39;s inspect the attention weights. Since this is causal, we would expect that due to masking, the upper triangular of the attention should be 0.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;MASTER_CONFIG.update({&#xA;    &#39;n_heads&#39;: 8,&#xA;})&#xA;layer = RoPEAttention(MASTER_CONFIG)&#xA;batch = torch.ones((MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;], MASTER_CONFIG[&#39;d_model&#39;]))&#xA;output, attn_weights = layer(batch, return_attn_weights=True)&#xA;&#xA;plt.imshow(attn_weights[0].detach().numpy(), interpolation=&#39;nearest&#39;)&#xA;plt.colorbar()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.colorbar.Colorbar at 0x16c039090&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bkitano/llama-from-scratch/main/llama_files/llama_48_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is not good; it means that information is leaking across the attention. We need to ensure the causal mask is working.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class RoPEAttention_wMask(nn.Module):&#xA;    def __init__(self, config):&#xA;        super().__init__()&#xA;        self.config = config&#xA;        self.w_q = nn.Linear(config[&#39;d_model&#39;], config[&#39;d_model&#39;], bias=False)&#xA;        self.w_k = nn.Linear(config[&#39;d_model&#39;], config[&#39;d_model&#39;], bias=False)&#xA;        self.w_v = nn.Linear(config[&#39;d_model&#39;], config[&#39;d_model&#39;], bias=False)&#xA;&#xA;        self.multihead = nn.MultiheadAttention(config[&#39;d_model&#39;], config[&#39;n_heads&#39;], dropout=0.1, batch_first=True)&#xA;        self.R = get_rotary_matrix(config[&#39;context_window&#39;], config[&#39;d_model&#39;])&#xA;&#xA;    def get_rotary_matrix(context_window, embedding_dim):&#xA;        R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)&#xA;        for position in range(context_window):&#xA;            for i in range(embedding_dim//2):&#xA;                theta = 10000. ** (-2.*(i - 1) / embedding_dim)&#xA;                m_theta = position * theta&#xA;                R[position, 2*i,2*i] = np.cos(m_theta)&#xA;                R[position, 2*i,2*i+1] = - np.sin(m_theta)&#xA;                R[position, 2*i+1,2*i] = np.sin(m_theta)&#xA;                R[position, 2*i+1,2*i+1] = np.cos(m_theta)&#xA;        return R&#xA;    &#xA;    def forward(self, x, return_attn_weights=False):&#xA;        b,m,d = x.shape&#xA;        &#xA;        q = self.w_q(x)&#xA;        k = self.w_k(x)&#xA;        v = self.w_v(x)&#xA;&#xA;        q_out = (torch.bmm(q.transpose(0,1), self.R[:m, ...])).transpose(0,1)&#xA;        k_out = (torch.bmm(k.transpose(0,1), self.R[:m, ...])).transpose(0,1)&#xA;        v_out = (torch.bmm(v.transpose(0,1), self.R[:m, ...])).transpose(0,1)&#xA;&#xA;        activations, attn_weights = self.multihead(&#xA;            q_out,k_out,v_out, &#xA;            attn_mask=nn.Transformer.generate_square_subsequent_mask(m),&#xA;            is_causal=True&#xA;        )&#xA;&#xA;        if return_attn_weights:&#xA;            return activations, attn_weights&#xA;        return activations&#xA;&#xA;layer = RoPEAttention(config)&#xA;batch = torch.randn((config[&#39;batch_size&#39;], config[&#39;context_window&#39;], config[&#39;d_model&#39;]))&#xA;output, attn_weights = layer(batch, return_attn_weights=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;layer = RoPEAttention_wMask(MASTER_CONFIG)&#xA;batch = torch.ones((MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;], MASTER_CONFIG[&#39;d_model&#39;]))&#xA;output, attn_weights = layer(batch, return_attn_weights=True)&#xA;&#xA;plt.imshow(attn_weights[0].detach().numpy())&#xA;plt.colorbar()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.colorbar.Colorbar at 0x16c2c7b50&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bkitano/llama-from-scratch/main/llama_files/llama_51_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Alright, let&#39;s run it and see what happens.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class RopeModel(nn.Module):&#xA;    def __init__(self, config):&#xA;        super().__init__()&#xA;        self.config = config&#xA;&#xA;        self.embedding = nn.Embedding(config[&#39;vocab_size&#39;], config[&#39;d_model&#39;])&#xA;        self.rms = RMSNorm((config[&#39;context_window&#39;], config[&#39;d_model&#39;]))&#xA;        self.rope_attention = RoPEAttention_wMask(config)&#xA;&#xA;        self.linear = nn.Sequential(&#xA;            nn.Linear(config[&#39;d_model&#39;], config[&#39;d_model&#39;]),&#xA;            nn.ReLU(),&#xA;        )&#xA;&#xA;        self.last_linear = nn.Linear(config[&#39;d_model&#39;], config[&#39;vocab_size&#39;])&#xA;        &#xA;        print(&#34;model params:&#34;, sum([m.numel() for m in self.parameters()]))&#xA;&#xA;    def forward(self, idx, targets=None):&#xA;        x = self.embedding(idx)&#xA;        &#xA;        # one block of attention&#xA;        x = self.rms(x) # rms pre-normalization&#xA;        x = x + self.rope_attention(x)&#xA;&#xA;        x = self.rms(x) # rms pre-normalization&#xA;        x = x + self.linear(x)&#xA;&#xA;        logits = self.last_linear(x)&#xA;&#xA;        if targets is not None:&#xA;            loss = F.cross_entropy(logits.view(-1, self.config[&#39;vocab_size&#39;]), targets.view(-1))&#xA;            return logits, loss&#xA;&#xA;        else:&#xA;            return logits&#xA;&#xA;model = RopeModel(MASTER_CONFIG)&#xA;xs, ys = get_batches(dataset, &#39;train&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])&#xA;&#xA;logits, loss = model(xs, ys)&#xA;optimizer = torch.optim.Adam(model.parameters())&#xA;train(model, optimizer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;model params: 150465&#xA;validation loss:  2.1157416343688964&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&amp;lt;Axes: &amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bkitano/llama-from-scratch/main/llama_files/llama_53_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;It looks like we can drive our loss down even lower. Let&#39;s do that by updating master config.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;MASTER_CONFIG.update({&#xA;    &#34;epochs&#34;: 5000,&#xA;    &#34;log_interval&#34;: 10,&#xA;})&#xA;train(model, optimizer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;validation loss:  1.9027801871299743&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&amp;lt;Axes: &amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bkitano/llama-from-scratch/main/llama_files/llama_55_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;SwiGLU&lt;/h3&gt; &#xA;&lt;p&gt;As it says in the paper, &#34;We replace the ReLU non-linearity by the SwiGLU activation function...we use a dimension of $\frac{2}{3} 4d$ isntead of $4d$ as in PaLM.&#34; SwiGLU is defined as: $$ \text{SwiGLU}(x) = \text{Swish}&lt;em&gt;\beta (xW + b) \otimes (xV + c) $$ where $\otimes$ is a component-wise product. The Swish function is defined as: $$ \text{Swish}&lt;/em&gt;\beta(x) = x \sigma(\beta x) $$ where $\beta$ is a learnable parameter.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SwiGLU(nn.Module):&#xA;    &#34;&#34;&#34;&#xA;    Swish-Gated Linear Unit&#xA;    https://arxiv.org/pdf/2002.05202v1.pdf&#xA;    &#34;&#34;&#34;&#xA;    def __init__(self, size):&#xA;        super().__init__()&#xA;        self.config = config&#xA;        self.linear_gate = nn.Linear(size, size)&#xA;        self.linear = nn.Linear(size, size)&#xA;        self.beta = torch.randn(1, requires_grad=True)&#xA;&#xA;        self.beta = nn.Parameter(torch.ones(1))&#xA;        self.register_parameter(&#34;beta&#34;, self.beta)&#xA;&#xA;    def forward(self, x): &#xA;        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))&#xA;        out = swish_gate * self.linear(x)&#xA;        return out&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class RopeModel(nn.Module):&#xA;    def __init__(self, config):&#xA;        super().__init__()&#xA;        self.config = config&#xA;&#xA;        self.embedding = nn.Embedding(config[&#39;vocab_size&#39;], config[&#39;d_model&#39;])&#xA;        self.rms = RMSNorm((config[&#39;context_window&#39;], config[&#39;d_model&#39;]))&#xA;        self.rope_attention = RoPEAttention_wMask(config)&#xA;&#xA;        self.linear = nn.Sequential(&#xA;            nn.Linear(config[&#39;d_model&#39;], config[&#39;d_model&#39;]),&#xA;            SwiGLU(config[&#39;d_model&#39;]),&#xA;        )&#xA;&#xA;        self.last_linear = nn.Linear(config[&#39;d_model&#39;], config[&#39;vocab_size&#39;])&#xA;        &#xA;        print(&#34;model params:&#34;, sum([m.numel() for m in self.parameters()]))&#xA;&#xA;    def forward(self, idx, targets=None):&#xA;        x = self.embedding(idx)&#xA;        &#xA;        # one block of attention&#xA;        x = self.rms(x) # rms pre-normalization&#xA;        x = x + self.rope_attention(x)&#xA;&#xA;        x = self.rms(x) # rms pre-normalization&#xA;        x = x + self.linear(x)&#xA;&#xA;        logits = self.last_linear(x)&#xA;&#xA;        if targets is not None:&#xA;            loss = F.cross_entropy(logits.view(-1, self.config[&#39;vocab_size&#39;]), targets.view(-1))&#xA;            return logits, loss&#xA;&#xA;        else:&#xA;            return logits&#xA;&#xA;model = RopeModel(MASTER_CONFIG)&#xA;xs, ys = get_batches(dataset, &#39;train&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])&#xA;&#xA;logits, loss = model(xs, ys)&#xA;optimizer = torch.optim.Adam(model.parameters())&#xA;train(model, optimizer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;model params: 183490&#xA;validation loss:  1.8666490197181702&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&amp;lt;Axes: &amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bkitano/llama-from-scratch/main/llama_files/llama_58_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now, let&#39;s add multiple layers of RopeAttention by creating blocks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# add RMSNorm and residual conncection&#xA;class LlamaBlock(nn.Module):&#xA;    def __init__(self, config):&#xA;        super().__init__()&#xA;        self.config = config&#xA;&#xA;        self.rms = RMSNorm((config[&#39;context_window&#39;], config[&#39;d_model&#39;]))&#xA;        &#xA;        self.attention = RoPEAttention_wMask(config)&#xA;        self.feedforward = nn.Sequential(&#xA;            nn.Linear(config[&#39;d_model&#39;], config[&#39;d_model&#39;]),&#xA;            SwiGLU(config[&#39;d_model&#39;]),&#xA;        )&#xA;&#xA;    def forward(self, x):&#xA;        x = self.rms(x) # rms pre-normalization&#xA;        x = x + self.attention(x)&#xA;&#xA;        x = self.rms(x) # rms pre-normalization&#xA;        x = x + self.feedforward(x)&#xA;        return x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;block = LlamaBlock(MASTER_CONFIG)&#xA;block(torch.randn(MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;], MASTER_CONFIG[&#39;d_model&#39;]));&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import OrderedDict&#xA;&#xA;MASTER_CONFIG.update({&#xA;    &#39;n_layers&#39;: 4,&#xA;})&#xA;class Llama(nn.Module):&#xA;    def __init__(self, config):&#xA;        super().__init__()&#xA;        self.config = config&#xA;        self.embeddings = nn.Embedding(config[&#39;vocab_size&#39;], config[&#39;d_model&#39;])&#xA;        self.llama_blocks = nn.Sequential(&#xA;            OrderedDict([(f&#34;llama_{i}&#34;, LlamaBlock(config)) for i in range(config[&#39;n_layers&#39;])])&#xA;        )&#xA;&#xA;        self.ffn = nn.Sequential(&#xA;            nn.Linear(config[&#39;d_model&#39;], config[&#39;d_model&#39;]),&#xA;            SwiGLU(config[&#39;d_model&#39;]),&#xA;            nn.Linear(config[&#39;d_model&#39;], config[&#39;vocab_size&#39;]),&#xA;        )&#xA;&#xA;        print(&#34;model params:&#34;, sum([m.numel() for m in self.parameters()]))&#xA;&#xA;    def forward(self, idx, targets=None):&#xA;        x = self.embeddings(idx)&#xA;        x = self.llama_blocks(x)&#xA;        logits = self.ffn(x)&#xA;&#xA;        if targets is None:&#xA;            return logits&#xA;        &#xA;        else:&#xA;            loss = F.cross_entropy(logits.view(-1, self.config[&#39;vocab_size&#39;]), targets.view(-1))&#xA;            return logits, loss&#xA;&#xA;llama = Llama(MASTER_CONFIG)&#xA;optimizer = torch.optim.Adam(llama.parameters())&#xA;train(llama, optimizer, config=MASTER_CONFIG)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;model params: 733382&#xA;validation loss:  1.616115140914917&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&amp;lt;Axes: &amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bkitano/llama-from-scratch/main/llama_files/llama_62_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;It looks like we can drive the loss down even more, and although we&#39;re overfitting a little, I think we can still do better. Let&#39;s train longer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;MASTER_CONFIG.update({&#xA;    &#39;epochs&#39;: 10000,&#xA;})&#xA;train(llama, optimizer, scheduler=None, config=MASTER_CONFIG)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;validation loss:  0.9024032115936279&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&amp;lt;Axes: &amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bkitano/llama-from-scratch/main/llama_files/llama_64_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;It seems we can go even lower, still without serious overfitting. Either there is a leak, or it&#39;s actually doing well. The loss here is 1.08, which is equivalent to choosing between 2.9 tokens randomly.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train(llama, optimizer, config=MASTER_CONFIG)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;validation loss:  0.746810007095337&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&amp;lt;Axes: &amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bkitano/llama-from-scratch/main/llama_files/llama_66_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(generate(llama, MASTER_CONFIG, 500)[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Evend her break of thou thire xoing dieble had side, did foesors exenatedH in siffied up,&#xA;No, none,&#xA;And you ling as thought depond.&#xA;&#xA;MENENIUS:&#xA;Tell officien:&#xA;To pesiding be&#xA;Best wanty and to spiege,&#xA;To uncine shee patss again,&#xA;I will hen: then they&#xA;Moieth:&#xA;I my cast in letch:&#xA;For bereful, give toan I may&#xA;&#xA;LINT OF AUMERLE:&#xA;Out, or me but thee here sir,&#xA;Why first with canse pring;&#xA;Now!&#xA;&#xA;Gide me couuse&#xA;The haster:&#xA;And suilt harming,&#xA;Then as pereise with and go.&#xA;&#xA;FROMNIUS:&#xA;I well? speak and wieke ac&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;At this point, we&#39;ve hit the bottom with our training. Let&#39;s test on the test set.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xs, ys = get_batches(dataset, &#39;test&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])&#xA;&#xA;logits, loss = llama(xs, ys)&#xA;&#xA;print(loss)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;tensor(0.8304, grad_fn=&amp;lt;NllLossBackward0&amp;gt;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Miscellaneous&lt;/h1&gt; &#xA;&lt;h2&gt;Check for Gradient Flows&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s inspect the gradients, we want to see how they&#39;re flowing. If there are too many gradients where the value is close to 0, that&#39;s a problem.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# print the percentage that are near 0&#xA;def show_grads(model, tol=1e-2):&#xA;    return sorted([(name, 100.0 * float(torch.sum(torch.abs(param) &amp;lt;= tol)) / float(param.nelement())) for name, param in model.named_parameters() if param.requires_grad], key=lambda t: t[1], reverse=True)&#xA;&#xA;show_grads(llama)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;[(&#39;llama_blocks.llama_0.attention.multihead.in_proj_bias&#39;, 36.71875),&#xA; (&#39;llama_blocks.llama_3.attention.multihead.in_proj_bias&#39;, 35.9375),&#xA; (&#39;llama_blocks.llama_1.attention.multihead.in_proj_bias&#39;, 33.59375),&#xA; (&#39;llama_blocks.llama_2.attention.multihead.in_proj_bias&#39;, 33.333333333333336),&#xA; (&#39;llama_blocks.llama_0.attention.multihead.in_proj_weight&#39;,&#xA;  21.840413411458332),&#xA; (&#39;llama_blocks.llama_0.attention.w_q.weight&#39;, 14.892578125),&#xA; (&#39;llama_blocks.llama_0.attention.multihead.out_proj.weight&#39;, 13.4765625),&#xA; (&#39;llama_blocks.llama_0.attention.w_k.weight&#39;, 12.9638671875),&#xA; (&#39;llama_blocks.llama_0.attention.w_v.weight&#39;, 11.8896484375),&#xA; (&#39;llama_blocks.llama_1.attention.w_v.weight&#39;, 11.285400390625),&#xA; (&#39;llama_blocks.llama_3.attention.multihead.out_proj.weight&#39;, 11.12060546875),&#xA; (&#39;llama_blocks.llama_2.attention.w_v.weight&#39;, 10.68115234375),&#xA; (&#39;llama_blocks.llama_0.feedforward.0.weight&#39;, 10.4248046875),&#xA; (&#39;llama_blocks.llama_2.attention.multihead.out_proj.weight&#39;, 10.36376953125),&#xA; (&#39;llama_blocks.llama_1.attention.multihead.out_proj.weight&#39;, 10.2783203125),&#xA; (&#39;llama_blocks.llama_3.attention.multihead.out_proj.bias&#39;, 10.15625),&#xA; (&#39;llama_blocks.llama_3.attention.w_v.weight&#39;, 9.991455078125),&#xA; (&#39;llama_blocks.llama_0.feedforward.1.linear.weight&#39;, 9.9609375),&#xA; (&#39;llama_blocks.llama_1.attention.multihead.in_proj_weight&#39;,&#xA;  9.908040364583334),&#xA; (&#39;llama_blocks.llama_2.attention.multihead.in_proj_weight&#39;, 9.75341796875),&#xA; (&#39;llama_blocks.llama_0.feedforward.1.linear_gate.weight&#39;, 9.66796875),&#xA; (&#39;llama_blocks.llama_1.attention.multihead.out_proj.bias&#39;, 9.375),&#xA; (&#39;llama_blocks.llama_3.attention.multihead.in_proj_weight&#39;, 9.16748046875),&#xA; (&#39;llama_blocks.llama_1.feedforward.0.weight&#39;, 8.77685546875),&#xA; (&#39;llama_blocks.llama_0.feedforward.1.linear_gate.bias&#39;, 8.59375),&#xA; (&#39;llama_blocks.llama_2.feedforward.1.linear.bias&#39;, 8.59375),&#xA; (&#39;llama_blocks.llama_2.feedforward.0.weight&#39;, 8.4228515625),&#xA; (&#39;llama_blocks.llama_1.feedforward.1.linear.weight&#39;, 7.720947265625),&#xA; (&#39;llama_blocks.llama_1.feedforward.1.linear_gate.weight&#39;, 7.501220703125),&#xA; (&#39;llama_blocks.llama_3.feedforward.0.weight&#39;, 7.440185546875),&#xA; (&#39;llama_blocks.llama_2.feedforward.1.linear.weight&#39;, 7.31201171875),&#xA; (&#39;llama_blocks.llama_2.feedforward.1.linear_gate.weight&#39;, 7.196044921875),&#xA; (&#39;llama_blocks.llama_1.feedforward.1.linear_gate.bias&#39;, 7.03125),&#xA; (&#39;llama_blocks.llama_2.attention.multihead.out_proj.bias&#39;, 7.03125),&#xA; (&#39;llama_blocks.llama_2.feedforward.1.linear_gate.bias&#39;, 7.03125),&#xA; (&#39;llama_blocks.llama_2.attention.w_k.weight&#39;, 6.94580078125),&#xA; (&#39;llama_blocks.llama_3.feedforward.1.linear.weight&#39;, 6.927490234375),&#xA; (&#39;llama_blocks.llama_1.attention.w_k.weight&#39;, 6.82373046875),&#xA; (&#39;llama_blocks.llama_2.attention.w_q.weight&#39;, 6.82373046875),&#xA; (&#39;llama_blocks.llama_3.attention.w_k.weight&#39;, 6.585693359375),&#xA; (&#39;llama_blocks.llama_3.feedforward.1.linear_gate.weight&#39;, 6.494140625),&#xA; (&#39;llama_blocks.llama_1.attention.w_q.weight&#39;, 6.396484375),&#xA; (&#39;llama_blocks.llama_1.feedforward.1.linear.bias&#39;, 6.25),&#xA; (&#39;llama_blocks.llama_3.feedforward.1.linear.bias&#39;, 6.25),&#xA; (&#39;llama_blocks.llama_3.attention.w_q.weight&#39;, 6.2255859375),&#xA; (&#39;ffn.0.weight&#39;, 5.633544921875),&#xA; (&#39;llama_blocks.llama_0.feedforward.1.linear.bias&#39;, 5.46875),&#xA; (&#39;ffn.1.linear_gate.bias&#39;, 5.46875),&#xA; (&#39;ffn.1.linear_gate.weight&#39;, 5.2978515625),&#xA; (&#39;ffn.1.linear.weight&#39;, 5.26123046875),&#xA; (&#39;ffn.2.weight&#39;, 4.447115384615385),&#xA; (&#39;ffn.1.linear.bias&#39;, 3.90625),&#xA; (&#39;llama_blocks.llama_0.feedforward.0.bias&#39;, 3.125),&#xA; (&#39;ffn.2.bias&#39;, 3.076923076923077),&#xA; (&#39;llama_blocks.llama_3.feedforward.0.bias&#39;, 2.34375),&#xA; (&#39;ffn.0.bias&#39;, 2.34375),&#xA; (&#39;llama_blocks.llama_0.attention.multihead.out_proj.bias&#39;, 1.5625),&#xA; (&#39;llama_blocks.llama_2.feedforward.0.bias&#39;, 1.5625),&#xA; (&#39;llama_blocks.llama_3.feedforward.1.linear_gate.bias&#39;, 1.5625),&#xA; (&#39;llama_blocks.llama_1.feedforward.0.bias&#39;, 0.78125),&#xA; (&#39;embeddings.weight&#39;, 0.7572115384615384),&#xA; (&#39;llama_blocks.llama_0.rms.scale&#39;, 0.146484375),&#xA; (&#39;llama_blocks.llama_0.feedforward.1.beta&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_1.rms.scale&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_1.feedforward.1.beta&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_2.rms.scale&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_2.feedforward.1.beta&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_3.rms.scale&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_3.feedforward.1.beta&#39;, 0.0),&#xA; (&#39;ffn.1.beta&#39;, 0.0)]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, for all of our parameter gradients, the vast majority are non-zero, which is great. If we start to see this number peak higher, then our gradients would not be flowing.&lt;/p&gt; &#xA;&lt;h2&gt;Experiment with hyperparams, aka &#34;change the oven settings&#34;&lt;/h2&gt; &#xA;&lt;p&gt;In the original Llama paper, the authors use Cosine Annealing learning schedule. We didn&#39;t do that here, because I experimented and saw that it was worse.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;MASTER_CONFIG.update({&#xA;    &#34;epochs&#34;: 1000&#xA;})&#xA;llama_with_cosine = Llama(MASTER_CONFIG)&#xA;llama_optimizer = torch.optim.Adam(&#xA;    llama.parameters(), &#xA;    betas=(.9, .95), &#xA;    weight_decay=.1, &#xA;    eps=1e-9, &#xA;    lr=1e-3&#xA;)&#xA;scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(llama_optimizer, 300, eta_min=1e-5)&#xA;train(llama_with_cosine, llama_optimizer, scheduler=scheduler)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;model params: 733382&#xA;&#xA;&#xA;/Users/bkitano/Desktop/projects/llama/.llama/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:814: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.&#xA;  warnings.warn(&#34;To get the last learning rate computed by the scheduler, &#34;&#xA;&#xA;&#xA;lr:  [0.0009999457184159408]&#xA;lr:  [0.0009961510274583004]&#xA;lr:  [0.0009869757772816292]&#xA;lr:  [0.0009725204933511963]&#xA;lr:  [0.0009529435502760634]&#xA;lr:  [0.0009284594366176498]&#xA;lr:  [0.0008993364049014041]&#xA;lr:  [0.0008658935325782156]&#xA;lr:  [0.0008284972261358946]&#xA;lr:  [0.0007875572066618724]&#xA;lr:  [0.0007435220208394261]&#xA;lr:  [0.0006968741265588948]&#xA;lr:  [0.0006481246069855613]&#xA;lr:  [0.0005978075709959357]&#xA;lr:  [0.0005464743013294258]&#xA;lr:  [0.0004946872145648923]&#xA;lr:  [0.00044301369909075325]&#xA;lr:  [0.00039201989856974814]&#xA;lr:  [0.0003422645089892466]&#xA;lr:  [0.0002942926572257539]&#xA;lr:  [0.0002486299281339155]&#xA;lr:  [0.0002057766054928797]&#xA;lr:  [0.00016620218969383484]&#xA;lr:  [0.00013034025177986015]&#xA;lr:  [9.858367916643242e-05]&#xA;lr:  [7.128036241775617e-05]&#xA;lr:  [4.872936226262451e-05]&#xA;lr:  [3.117756953567661e-05]&#xA;lr:  [1.8816750064937722e-05]&#xA;lr:  [1.1779739336260682e-05]&#xA;lr:  [1.0054282328142978e-05]&#xA;lr:  [1.396862020283371e-05]&#xA;lr:  [2.3138198807489664e-05]&#xA;lr:  [3.7591595433673565e-05]&#xA;lr:  [5.716757986058726e-05]&#xA;lr:  [8.165110310035915e-05]&#xA;lr:  [0.00011077372717831143]&#xA;lr:  [0.00014421629539013414]&#xA;lr:  [0.00018161236159547207]&#xA;lr:  [0.00022255218257041353]&#xA;lr:  [0.000266587198202317]&#xA;lr:  [0.0003132349418762423]&#xA;lr:  [0.0003619843243986437]&#xA;lr:  [0.00041230123246673267]&#xA;lr:  [0.0004636343798727907]&#xA;lr:  [0.0005154213471150504]&#xA;lr:  [0.0005670947431326733]&#xA;lr:  [0.0006180884215964123]&#xA;lr:  [0.0006678436836148733]&#xA;lr:  [0.0007158153988789833]&#xA;lr:  [0.000761477978168448]&#xA;lr:  [0.0008043311317769176]&#xA;lr:  [0.0008439053507606739]&#xA;lr:  [0.0008797670509540533]&#xA;lr:  [0.0009115233233906334]&#xA;lr:  [0.0009388262390821259]&#xA;lr:  [0.0009613766609899272]&#xA;lr:  [0.0009789275214238276]&#xA;lr:  [0.000991286528959492]&#xA;lr:  [0.0009983182752167242]&#xA;lr:  [0.0009999457184159228]&#xA;lr:  [0.0009961510274582826]&#xA;lr:  [0.0009869757772816119]&#xA;lr:  [0.0009725204933511793]&#xA;lr:  [0.0009529435502760463]&#xA;lr:  [0.0009284594366176337]&#xA;lr:  [0.0008993364049013882]&#xA;lr:  [0.0008658935325782011]&#xA;lr:  [0.0008284972261358803]&#xA;lr:  [0.000787557206661859]&#xA;lr:  [0.0007435220208394123]&#xA;lr:  [0.0006968741265588827]&#xA;lr:  [0.0006481246069855505]&#xA;lr:  [0.0005978075709959258]&#xA;lr:  [0.0005464743013294155]&#xA;lr:  [0.0004946872145648846]&#xA;lr:  [0.0004430136990907463]&#xA;lr:  [0.00039201989856974174]&#xA;lr:  [0.0003422645089892402]&#xA;lr:  [0.00029429265722574944]&#xA;lr:  [0.0002486299281339118]&#xA;lr:  [0.00020577660549287596]&#xA;lr:  [0.00016620218969383194]&#xA;lr:  [0.00013034025177985776]&#xA;lr:  [9.85836791664316e-05]&#xA;lr:  [7.128036241775529e-05]&#xA;lr:  [4.872936226262421e-05]&#xA;lr:  [3.117756953567644e-05]&#xA;lr:  [1.8816750064937634e-05]&#xA;lr:  [1.1779739336260654e-05]&#xA;lr:  [1.0054282328142978e-05]&#xA;lr:  [1.396862020283372e-05]&#xA;lr:  [2.3138198807489698e-05]&#xA;lr:  [3.759159543367397e-05]&#xA;lr:  [5.716757986058744e-05]&#xA;lr:  [8.165110310035937e-05]&#xA;lr:  [0.00011077372717831269]&#xA;lr:  [0.00014421629539013508]&#xA;lr:  [0.0001816123615954725]&#xA;lr:  [0.00022255218257041364]&#xA;validation loss:  4.179551410675049&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&amp;lt;Axes: &amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bkitano/llama-from-scratch/main/llama_files/llama_76_4.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;show_grads(llama_with_cosine, 1e-5)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;[(&#39;llama_blocks.llama_0.attention.multihead.in_proj_bias&#39;, 100.0),&#xA; (&#39;llama_blocks.llama_0.attention.multihead.out_proj.bias&#39;, 100.0),&#xA; (&#39;llama_blocks.llama_1.attention.multihead.in_proj_bias&#39;, 100.0),&#xA; (&#39;llama_blocks.llama_1.attention.multihead.out_proj.bias&#39;, 100.0),&#xA; (&#39;llama_blocks.llama_2.attention.multihead.in_proj_bias&#39;, 100.0),&#xA; (&#39;llama_blocks.llama_2.attention.multihead.out_proj.bias&#39;, 100.0),&#xA; (&#39;llama_blocks.llama_3.attention.multihead.in_proj_bias&#39;, 100.0),&#xA; (&#39;llama_blocks.llama_3.attention.multihead.out_proj.bias&#39;, 100.0),&#xA; (&#39;llama_blocks.llama_1.feedforward.1.linear.bias&#39;, 0.78125),&#xA; (&#39;llama_blocks.llama_2.feedforward.1.linear_gate.weight&#39;, 0.030517578125),&#xA; (&#39;llama_blocks.llama_3.attention.w_q.weight&#39;, 0.030517578125),&#xA; (&#39;llama_blocks.llama_0.attention.multihead.out_proj.weight&#39;, 0.0244140625),&#xA; (&#39;llama_blocks.llama_0.feedforward.1.linear_gate.weight&#39;, 0.0244140625),&#xA; (&#39;llama_blocks.llama_0.feedforward.1.linear.weight&#39;, 0.0244140625),&#xA; (&#39;llama_blocks.llama_1.attention.w_v.weight&#39;, 0.0244140625),&#xA; (&#39;llama_blocks.llama_1.feedforward.1.linear.weight&#39;, 0.0244140625),&#xA; (&#39;llama_blocks.llama_2.attention.w_q.weight&#39;, 0.0244140625),&#xA; (&#39;llama_blocks.llama_3.attention.w_v.weight&#39;, 0.0244140625),&#xA; (&#39;llama_blocks.llama_3.attention.multihead.out_proj.weight&#39;, 0.0244140625),&#xA; (&#39;ffn.2.weight&#39;, 0.02403846153846154),&#xA; (&#39;llama_blocks.llama_0.attention.w_v.weight&#39;, 0.018310546875),&#xA; (&#39;llama_blocks.llama_0.feedforward.0.weight&#39;, 0.018310546875),&#xA; (&#39;llama_blocks.llama_1.feedforward.0.weight&#39;, 0.018310546875),&#xA; (&#39;llama_blocks.llama_3.attention.w_k.weight&#39;, 0.018310546875),&#xA; (&#39;llama_blocks.llama_3.feedforward.0.weight&#39;, 0.018310546875),&#xA; (&#39;llama_blocks.llama_0.attention.multihead.in_proj_weight&#39;,&#xA;  0.016276041666666668),&#xA; (&#39;llama_blocks.llama_1.attention.multihead.out_proj.weight&#39;, 0.01220703125),&#xA; (&#39;llama_blocks.llama_1.feedforward.1.linear_gate.weight&#39;, 0.01220703125),&#xA; (&#39;llama_blocks.llama_3.feedforward.1.linear_gate.weight&#39;, 0.01220703125),&#xA; (&#39;llama_blocks.llama_3.feedforward.1.linear.weight&#39;, 0.01220703125),&#xA; (&#39;llama_blocks.llama_0.attention.w_q.weight&#39;, 0.006103515625),&#xA; (&#39;llama_blocks.llama_0.attention.w_k.weight&#39;, 0.006103515625),&#xA; (&#39;llama_blocks.llama_1.attention.w_q.weight&#39;, 0.006103515625),&#xA; (&#39;llama_blocks.llama_1.attention.multihead.in_proj_weight&#39;, 0.006103515625),&#xA; (&#39;llama_blocks.llama_2.attention.multihead.in_proj_weight&#39;, 0.006103515625),&#xA; (&#39;llama_blocks.llama_2.attention.multihead.out_proj.weight&#39;, 0.006103515625),&#xA; (&#39;llama_blocks.llama_2.feedforward.1.linear.weight&#39;, 0.006103515625),&#xA; (&#39;llama_blocks.llama_3.attention.multihead.in_proj_weight&#39;,&#xA;  0.004069010416666667),&#xA; (&#39;embeddings.weight&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_0.rms.scale&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_0.feedforward.0.bias&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_0.feedforward.1.beta&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_0.feedforward.1.linear_gate.bias&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_0.feedforward.1.linear.bias&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_1.rms.scale&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_1.attention.w_k.weight&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_1.feedforward.0.bias&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_1.feedforward.1.beta&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_1.feedforward.1.linear_gate.bias&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_2.rms.scale&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_2.attention.w_k.weight&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_2.attention.w_v.weight&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_2.feedforward.0.weight&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_2.feedforward.0.bias&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_2.feedforward.1.beta&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_2.feedforward.1.linear_gate.bias&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_2.feedforward.1.linear.bias&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_3.rms.scale&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_3.feedforward.0.bias&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_3.feedforward.1.beta&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_3.feedforward.1.linear_gate.bias&#39;, 0.0),&#xA; (&#39;llama_blocks.llama_3.feedforward.1.linear.bias&#39;, 0.0),&#xA; (&#39;ffn.0.weight&#39;, 0.0),&#xA; (&#39;ffn.0.bias&#39;, 0.0),&#xA; (&#39;ffn.1.beta&#39;, 0.0),&#xA; (&#39;ffn.1.linear_gate.weight&#39;, 0.0),&#xA; (&#39;ffn.1.linear_gate.bias&#39;, 0.0),&#xA; (&#39;ffn.1.linear.weight&#39;, 0.0),&#xA; (&#39;ffn.1.linear.bias&#39;, 0.0),&#xA; (&#39;ffn.2.bias&#39;, 0.0)]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Even at an extremely low tolerance, the attention biases are not getting any signal. I&#39;m not sure why the learning schedule from the paper doesn&#39;t work, but the lesson here is simple: start simple.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mshumer/gpt-llm-trainer</title>
    <updated>2023-08-12T01:31:06Z</updated>
    <id>tag:github.com,2023-08-12:/mshumer/gpt-llm-trainer</id>
    <link href="https://github.com/mshumer/gpt-llm-trainer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gpt-llm-trainer&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/mattshumer_&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/mattshumer_?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1mV9sAY4QBKLmS58dpFGHgwCXQKRASR31?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Main Version In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Training models is hard. You have to collect a dataset, clean it, get it in the right format, select a model, write the training code and train it. And that&#39;s the best-case scenario.&lt;/p&gt; &#xA;&lt;p&gt;The goal of this project is to explore an experimental new pipeline to train a high-performing task-specific model. We try to abstract away all the complexity, so it&#39;s as easy as possible to go from idea -&amp;gt; performant fully-trained model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Simply input a description of your task, and the system will generate a dataset from scratch, parse it into the right format, and fine-tune a LLaMA 2 model for you.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dataset Generation&lt;/strong&gt;: Using GPT-4, &lt;code&gt;gpt-llm-trainer&lt;/code&gt; will generate a variety of prompts and responses based on the provided use-case.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;System Message Generation&lt;/strong&gt;: &lt;code&gt;gpt-llm-trainer&lt;/code&gt; will generate an effective system prompt for your model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fine-Tuning&lt;/strong&gt;: After your dataset has been generated, the system will automatically split it into training and validation sets, fine-tune a model for you, and get it ready for inference.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1mV9sAY4QBKLmS58dpFGHgwCXQKRASR31?usp=sharing&#34;&gt;Open the notebook in Google Colab&lt;/a&gt; or in a local Jupyter notebook.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you&#39;re using Colab, switch to the best GPU available (go to Runtime -&amp;gt; change runtime type).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add your OpenAI API key to the line &lt;code&gt;openai.api_key = &#34;YOUR KEY HERE&#34;&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to Use&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Define your &lt;code&gt;prompt&lt;/code&gt;. The &lt;code&gt;prompt&lt;/code&gt; is a description of what you want the trained AI to do. The more descriptive and clear you can be, the better. Additionally, set the temperature we will use to generate your dataset (high=creative, low=precise), and the number of examples you want to generate (100 is a good starting point).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;prompt = &#34;A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, step-by-step thought out response in Spanish.&#34;&#xA;temperature = .4&#xA;number_of_examples = 100&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run all the cells (stop at &lt;code&gt;Merge the model and store in Google Drive&lt;/code&gt;).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;It&#39;ll take some time (from 10 minutes to a couple of hours, depending on how many examples you generate), but soon, you&#39;ll have your fine-tuned model!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;After your model is trained, you can use the &lt;code&gt;Run Inference&lt;/code&gt; cell to test the model, and the cells below that allow you to save and load the model to and from Google Drive for later use.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributions are welcome! Some ideas:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;improve the example generation pipeline for efficiency/cost reduction (using n=)&lt;/li&gt; &#xA; &lt;li&gt;add additional example generation prompts to create more diverse examples&lt;/li&gt; &#xA; &lt;li&gt;add example pruning, removing very similar examples to improve performance&lt;/li&gt; &#xA; &lt;li&gt;use GPT-4 to automatically choose the training hyperparameters (and potentially, even the model to fine-tune!) based on a few examples + high-level dataset details (i.e. number of examples)&lt;/li&gt; &#xA; &lt;li&gt;train multiple model variants and choose the one with the lowest eval loss&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is &lt;a href=&#34;https://github.com/mshumer/gpt-llm-trainer/raw/master/LICENSE&#34;&gt;MIT&lt;/a&gt; licensed.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Matt Shumer - &lt;a href=&#34;https://twitter.com/mattshumer_&#34;&gt;@mattshumer_&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Project Link: &lt;a href=&#34;https://raw.githubusercontent.com/mshumer/gpt-llm-trainer/main/url&#34;&gt;https://github.com/mshumer/gpt-llm-trainer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lastly, if you want to try something even cooler than this, sign up for &lt;a href=&#34;https://www.hyperwriteai.com/personal-assistant&#34;&gt;Personal Assistant&lt;/a&gt; (most of my time is spent on this). It&#39;s basically an AI that can operate your web browser to complete tasks for you.&lt;/p&gt;</summary>
  </entry>
</feed>