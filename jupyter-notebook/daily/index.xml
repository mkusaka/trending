<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-06T01:34:49Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mshukor/UnIVAL</title>
    <updated>2023-08-06T01:34:49Z</updated>
    <id>tag:github.com,2023-08-06:/mshukor/UnIVAL</id>
    <link href="https://github.com/mshukor/UnIVAL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of UnIVAL: Unified Model for Image, Video, Audio and Language Tasks.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/examples/logo.png&#34; width=&#34;400&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &amp;nbsp;&lt;a href=&#34;https://unival-model.github.io/&#34;&gt;Project Page&lt;/a&gt; &amp;nbsp; | &amp;nbsp;&lt;a href=&#34;https://arxiv.org/abs/2307.16184&#34;&gt;Paper &lt;/a&gt;&amp;nbsp; | &amp;nbsp;&lt;a href=&#34;https://huggingface.co/spaces/mshukor/UnIVAL&#34;&gt;Demo&lt;/a&gt;&amp;nbsp; | &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/#datasets-and-checkpoints&#34;&gt;Checkpoints&lt;/a&gt;&amp;nbsp; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/examples/output.gif&#34; width=&#34;1000&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UnIVAL&lt;/strong&gt; &lt;i&gt;is a 0.25B-parameter unified model that is multitask pretrained on image and video-text data and target image, video and audio-text downstream tasks.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Online Demos&lt;/h1&gt; &#xA;&lt;p&gt;Check out our demo on Huggingface Spaces: &lt;a href=&#34;https://huggingface.co/spaces/mshukor/UnIVAL&#34;&gt;Spaces&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/examples/demo.png&#34; width=&#34;1200&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;General&lt;/code&gt; means the pretrained model before finetuning.&lt;/p&gt; &#xA;&lt;p&gt;To easily play with our model we also provide several notebooks: &lt;code&gt;VG.ipynb&lt;/code&gt;, &lt;code&gt;VQA.ipynb&lt;/code&gt;, &lt;code&gt;Captioning.ipynb&lt;/code&gt;, &lt;code&gt;Video_Captioning.ipynb&lt;/code&gt;, and &lt;code&gt;Audio_Captioning.ipynb&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.7.31]&lt;/strong&gt;: we provide &lt;a href=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/rewarded_soups.md&#34;&gt;here&lt;/a&gt; more details to reproduce the results with UnIVAL on Visual Grounding used in our &lt;a href=&#34;https://github.com/alexrame/rewardedsoups&#34;&gt;Rewarded soups&lt;/a&gt; work.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.7.31]&lt;/strong&gt;: Released of UnIVAL code and model weights! We will release the scripts to train and evaluate audio/video tasks later.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Table of Content&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/#results&#34;&gt;Quantitative Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/#datasets-and-checkpoints&#34;&gt;Datasets and Checkpoints&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/#training-and-inference&#34;&gt;Training and Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/#zero-shot-evaluation&#34;&gt;Zero-shot Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/#parameter-efficient-finetuning&#34;&gt;Parameter Efficient Finetuning (PEFT): Training only the linear layer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/#multimodal-model-merging&#34;&gt;Multimodal Model Merging/Weight Interpolation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/#qualitative-results&#34;&gt;Qualitative results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/#acknowledgment&#34;&gt;Acknowledgment&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Results&lt;/h1&gt; &#xA;&lt;p&gt;Here are some results on several multimodal tasks.&lt;/p&gt; &#xA;&lt;table border=&#34;1&#34; width=&#34;100%&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt;&#xA;   &lt;th colspan=&#34;3&#34;&gt;Visual Grounding&lt;/th&gt;&#xA;   &lt;th&gt;Image Captioning&lt;/th&gt;&#xA;   &lt;th&gt;VQA&lt;/th&gt;&#xA;   &lt;th&gt;Visual Entailment&lt;/th&gt;&#xA;   &lt;th colspan=&#34;1&#34;&gt;VideoQA&lt;/th&gt;&#xA;   &lt;th colspan=&#34;1&#34;&gt;Video Captioning&lt;/th&gt;&#xA;   &lt;th colspan=&#34;2&#34;&gt;Audio Captioning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;Dataset&lt;/td&gt;&#xA;   &lt;td&gt;RefCOCO&lt;/td&gt;&#xA;   &lt;td&gt;RefCOCO+&lt;/td&gt;&#xA;   &lt;td&gt;RefCOCOg&lt;/td&gt;&#xA;   &lt;td&gt;COCO&lt;/td&gt;&#xA;   &lt;td&gt;VQA v2&lt;/td&gt;&#xA;   &lt;td&gt;SNLI-VE&lt;/td&gt;&#xA;   &lt;td&gt;MSRVTT-QA&lt;/td&gt;&#xA;   &lt;td&gt;MSRVTT&lt;/td&gt;&#xA;   &lt;td&gt;AudioCaps&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;Split&lt;/td&gt;&#xA;   &lt;td&gt;val/test-a/test-b&lt;/td&gt;&#xA;   &lt;td&gt;val/test-a/test-b&lt;/td&gt;&#xA;   &lt;td&gt;val-u/test-u&lt;/td&gt;&#xA;   &lt;td&gt;Karpathy test&lt;/td&gt;&#xA;   &lt;td&gt;test-dev/test-std&lt;/td&gt;&#xA;   &lt;td&gt;val/test&lt;/td&gt;&#xA;   &lt;td&gt;test&lt;/td&gt;&#xA;   &lt;td&gt;test&lt;/td&gt;&#xA;   &lt;td&gt;test&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;Metric&lt;/td&gt;&#xA;   &lt;td colspan=&#34;3&#34;&gt;Acc.&lt;/td&gt;&#xA;   &lt;td&gt;CIDEr&lt;/td&gt;&#xA;   &lt;td&gt;Acc.&lt;/td&gt;&#xA;   &lt;td&gt;Acc.&lt;/td&gt;&#xA;   &lt;td&gt;Acc.&lt;/td&gt;&#xA;   &lt;td&gt;CIDEr&lt;/td&gt;&#xA;   &lt;td&gt;CIDEr&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;UnIVAL&lt;/td&gt;&#xA;   &lt;td&gt;89.1 / 91.5 / 85.2&lt;/td&gt;&#xA;   &lt;td&gt;82.2 / 86.9 / 75.3&lt;/td&gt;&#xA;   &lt;td&gt;84.7 / 85.2&lt;/td&gt;&#xA;   &lt;td&gt;137.0&lt;/td&gt;&#xA;   &lt;td&gt;77.0 / 77.1&lt;/td&gt;&#xA;   &lt;td&gt;78.2 / 78.6&lt;/td&gt;&#xA;   &lt;td&gt;43.5&lt;/td&gt;&#xA;   &lt;td&gt;60.5&lt;/td&gt;&#xA;   &lt;td&gt;71.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python 3.7.4&lt;/li&gt; &#xA; &lt;li&gt;pytorch 1.13+&lt;/li&gt; &#xA; &lt;li&gt;torchvision 0.14.1+&lt;/li&gt; &#xA; &lt;li&gt;JAVA 1.8 (for COCO evaluation)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We recommend to first install pytorch before other libraries:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/mshukor/UnIVAL.git&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the following model for captioning evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -c &#34;from pycocoevalcap.spice.spice import Spice; tmp = Spice()&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Datasets and Checkpoints&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/datasets.md&#34;&gt;datasets.md&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/checkpoints.md&#34;&gt;checkpoints.md&lt;/a&gt;. &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Training and Inference&lt;/h1&gt; &#xA;&lt;p&gt;The scripts to launch pretraining, finetuning and evaluation can be found in &lt;code&gt;run_scripts/&lt;/code&gt; folder. Below we provide more details. The data are stored in &lt;code&gt;.tsv&lt;/code&gt; files with different format depending on the training task. To restore training you need to provide the last checkpoint &lt;code&gt;checkpoint_last.pt&lt;/code&gt; to &lt;code&gt;--restore-file&lt;/code&gt;, and pass &lt;code&gt;--reset-dataloader --reset-meters --reset-optimizer&lt;/code&gt; as argument.&lt;/p&gt; &#xA;&lt;p&gt;We use slurm to launch the training/evaluation.&lt;/p&gt; &#xA;&lt;h2&gt;Image Processing&lt;/h2&gt; &#xA;&lt;p&gt;In some datasets, the images are encoded to base64 strings. To do this transformation you can use the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;from io import BytesIO&#xA;import base64&#xA;&#xA;img = Image.open(file_name) # path to file&#xA;img_buffer = BytesIO()&#xA;img.save(img_buffer, format=img.format)&#xA;byte_data = img_buffer.getvalue()&#xA;base64_str = base64.b64encode(byte_data) # bytes&#xA;base64_str = base64_str.decode(&#34;utf-8&#34;) # str&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pretraining&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;1. Prepare the Dataset&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; The format for pretraining tsv files are as follows: &lt;br&gt; &lt;/p&gt;&#xA; &lt;ul type=&#34;circle&#34;&gt; &#xA;  &lt;li&gt; Each line contains uniq-id, image/video path, caption, question, answer, ground-truth objects (objects appearing in the caption or question), dataset name (source of the data) and task type (caption, qa or visual gronunding). Prepared for the pretraining tasks of visual grounding, grounded captioning, image-text matching, image captioning and visual question answering. In addition, the folder &lt;code&gt;negative_sample&lt;/code&gt; contains three files &lt;code&gt;all_captions.txt&lt;/code&gt;, &lt;code&gt;object.txt&lt;/code&gt; and &lt;code&gt;type2ans.json&lt;/code&gt;. The data in these files are used as negative samples for the image/video-text matching task.&lt;/li&gt; &#xA;  &lt;p&gt;&lt;/p&gt; &#xA; &lt;/ul&gt;&#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;2. Pretraining&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; There is 3 scripts to train UnIVAL. &lt;code&gt;unival_s1.sh&lt;/code&gt; for stage 1 training initialized from BART weights, &lt;code&gt;unival_s2.sh&lt;/code&gt; for stage 2 training, initialized from the weights after stage 1, and &lt;code&gt;unival_s2_hs.sh&lt;/code&gt; for high-resolution training during 1 epoch, initialized from the weights of stage 2. For example to launch for stage 1: &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;cd run_scripts/pretraining&#xA;bash unival_s1.sh&#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Image Captioning&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;1. Prepare the Dataset &amp;amp; Checkpoints&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; Each image corresponds to only 1 caption in &lt;code&gt;caption_stage1_train.tsv&lt;/code&gt; and corresponds to multiple captions in other TSV files (about 5 captions per image). Each line of the dataset represents a caption sample with the following format. The information of uniq-id, image-id, caption, predicted object labels (taken from &lt;a href=&#34;https://github.com/pzzhang/VinVL&#34;&gt;VinVL&lt;/a&gt;, not used), image base64 string are separated by tabs. &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;162365  12455   the sun sets over the trees beyond some docks.  sky&amp;amp;&amp;amp;water&amp;amp;&amp;amp;dock&amp;amp;&amp;amp;pole  /9j/4AAQSkZJ....UCP/2Q==&#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;2. Finetuning&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; To finetune for image captioning: &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;cd run_scripts/caption&#xA;sh unival_caption_stage_1.sh &amp;gt; unival_caption_stage_1.out &#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;3. Inference&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; You can use the following code for inference, after setting the right weights path: &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;cd run_scripts/caption/eval ; sh eval_caption.sh  # inference &amp;amp; evaluate&#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Visual Question Answering&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;1. Prepare the Dataset &amp;amp; Checkpoints&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; Following common practice, VG-QA samples are also included in the training data. To adapt to the seq2seq paradigm of OFA, we transform original VQA training questions with multiple golden answers into multiple training samples. For the original VQA validation set, we keep around 10k samples for our validation and utilize the other samples for training. Each line of the dataset represents a VQA sample with the following format. The information of question-id, image-id, question, answer (with confidence), predicted object labels (taken from &lt;a href=&#34;https://github.com/pzzhang/VinVL&#34;&gt;VinVL&lt;/a&gt;, slightly brings around +0.1 accuracy improvement), image base64 string are separated by tabs. &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;79459   79459   is this person wearing shorts?  0.6|!+no    house&amp;amp;&amp;amp;short&amp;amp;&amp;amp;...&amp;amp;&amp;amp;sky  /9j/4AAQS...tigZ/9k=&#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;2. Shuffle the Training Data&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; (Optional, but achieves better finetuning accuracy): If the disk storage is sufficient, we recommend to prepare the shuffled training data for each epoch in advance. &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;cd dataset/vqa_data&#xA;ln vqa_train.tsv vqa_train_1.tsv&#xA;for idx in `seq 1 9`;do shuf vqa_train_${idx}.tsv &amp;gt; vqa_train_$[${idx}+1].tsv;done # each file is used for an epoch&#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;3. Finetuning&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; If you have shuffled the training data in the previous step, please correctly specify the training data path following the guide in the script comments. &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;cd run_scripts/vqa&#xA;bash unival_vqa.sh &#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;4. Inference&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; We use &lt;b&gt;beam-search&lt;/b&gt; during inference. &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;cd run_scripts/vqa/eval&#xA;bash evaluate_vqa.sh  # specify &#39;val&#39; or &#39;test&#39; in the script&#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Visual Grounding&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;1. Prepare the Dataset &amp;amp; Checkpoints&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; We use RefCOCO (split by UNC), RefCOCO+ (split by UNC) and RefCOCOg (split by UMD) datasets. See &lt;a href=&#34;https://www.tensorflow.org/datasets/catalog/ref_coco&#34;&gt;RefCOCO&lt;/a&gt; and &lt;a href=&#34;https://github.com/lichengunc/refer&#34;&gt;Refer&lt;/a&gt; for more details. Note that in the original dataset, each region-coord (or bounding box) may corresponds to multiple descriptive texts. We split these texts into multiple samples so that the region-coord in each sample corresponds to only one text. Each line of the processed dataset represents a sample with the following format. The information of uniq-id, image-id, text, region-coord (separated by commas), image base64 string are separated by tabs. &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;79_1    237367  A woman in a white blouse holding a glass of wine.  230.79,121.75,423.66,463.06 9j/4AAQ...1pAz/9k=&#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;2. Finetuning&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&#xA;cd run_scripts/refcoco&#xA;sh unival_refcoco.sh &amp;gt; train_refcoco.out &amp;amp;  # finetune for refcoco&#xA;sh unival_refcocoplus.sh &amp;gt; train_refcocoplus.out &amp;amp;  # finetune for refcoco+&#xA;sh unival_refcocog.sh &amp;gt; train_refcocog.out &amp;amp;  # finetune for refcocog&#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;3. Inference&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; Run the following commands for the evaluation. &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;cd run_scripts/refcoco/eval ; sh eva_refcoco.sh  # eva_refcocog.sh, eva_refcocoplus.sh&#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Visual Entailment&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;1. Prepare the Dataset &amp;amp; Checkpoints&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; Each line of the processed dataset represents a sample with the following format. The information of uniq-id, image-id, image base64 string, hypothesis, caption (or text premise), label are separated by tabs. &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;252244149.jpg#1r1n  252244149   /9j/4AAQ...MD/2Q==   a man in pink and gold is chewing on a wooden toothpick.   a man in pink is chewing a toothpick on the subway.   neutral &#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;2. Finetuning&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; Contrary to previous work (e.g. OFA) we do not use the text premise for this task. &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;cd run_scripts/snli_ve&#xA;nohup sh unival_snli_ve.sh &amp;gt; train_snli_ve.out &amp;amp;  # finetune for snli_ve&#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;3. Inference&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; Run the following command to obtain the results. &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;cd run_scripts/snli_ve/eval ; sh eval_snli_ve.sh  # specify &#39;dev&#39; or &#39;test&#39; in the script&#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Text-to-Image Generation&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;1. Prepare the Dataset &amp;amp; Checkpoints&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; The dataset zipfile &lt;code&gt;coco_image_gen.zip&lt;/code&gt; contains &lt;code&gt;coco_vqgan_train.tsv&lt;/code&gt;, &lt;code&gt;coco_vqgan_dev.tsv&lt;/code&gt; and &lt;code&gt;coco_vqgan_full_test.tsv&lt;/code&gt;. Each line of the dataset represents a sample with the following format. The information of uniq-id, image-code (produced by &lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;vqgan&lt;/a&gt;, a list of integers separated by single-whitespaces), lowercased caption are separated by tabs. &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;1&#x9;6674 4336 4532 5334 3251 5461 3615 2469 ...4965 4190 1846&#x9;the people are posing for a group photo.&#xA;&lt;/pre&gt; &#xA; &lt;p&gt; The checkpoint zipfile &lt;code&gt;image_gen_large_best.zip&lt;/code&gt; contains &lt;code&gt;image_gen_large_best.pt&lt;/code&gt;, &lt;code&gt;vqgan/last.ckpt&lt;/code&gt;, &lt;code&gt;vqgan/model.yaml&lt;/code&gt; and &lt;code&gt;clip/Vit-B-16.pt&lt;/code&gt;. &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;2. Finetuning&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; We divide the finetuning process of image generating into two stages. In stage 1, we finetune OFA with cross-entropy loss. In stage 2, we select the last checkpoint of stage 1 and train with CLIP Score optimization. During the validation, the generated image will be dumped into &lt;code&gt;_GEN_IMAGE_PATH_&lt;/code&gt;. &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;cd run_scripts/image_gen&#xA;nohup sh unival_image_gen_stage_1.sh # stage 1, train with cross-entropy loss&#xA;nohup sh unival_image_gen_stage_2.sh # stage 2, load the last ckpt of stage1 and train with CLIP Score optimization &#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;4. Inference&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; Run the command below to generate your images. &lt;/p&gt; &#xA; &lt;pre&gt;&#xA;cd run_scripts/image_gen/eval ; sh eval_image_gen.sh  # inference &amp;amp; evaluate (FID, IS and CLIP Score)&#xA;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Zero-shot Evaluation&lt;/h1&gt; &#xA;&lt;p&gt;Here we provide the scripts for zero-shot evaluation on image-text tasks. You need to specify the path to pretrained model in each of these scripts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Image Caption on Nocaps: &lt;code&gt;caption/eval/eval_nocaps.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;VQA on VizWiz: &lt;code&gt;vqa/eval/eval_vizwiz.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;VQA on Nocaps: &lt;code&gt;vqa/eval/eval_okvqa.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Parameter Efficient Finetuning&lt;/h1&gt; &#xA;&lt;h2&gt;Training only the linear connection&lt;/h2&gt; &#xA;&lt;p&gt;Following &lt;a href=&#34;https://github.com/mshukor/eP-ALM&#34;&gt;eP-ALM&lt;/a&gt;, we experiment with efficient finetuning by training only the linear connection between the modality spcific-encoders and the language model, while keeping all other parameters frozen:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Image Caption on COCO: &lt;code&gt;caption/onlylinear/unival_caption_stage_s2_onlylinear.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Video Caption on MSRVTT: &lt;code&gt;caption/onlylinear/unival_video_caption_stage_s2_onlylinear.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Audio Caption on Audiocaps: &lt;code&gt;caption/onlylinear/unival_audio_caption_stage_s2_onlylinear.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;VQA on VQAv2: &lt;code&gt;vqa/onlylinear/unival_vqa_s2_onlylinear.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Video QA on MSRVTT: &lt;code&gt;vqa/onlylinear/unival_video_vqa_s2_onlylinear.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To finetune the stage-1 pretrained model, you can use the scripts with &lt;code&gt;s1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Multimodal Model Merging&lt;/h1&gt; &#xA;&lt;p&gt;In this section we provide the details to reproduce the experiments for weight interpolation and different weight averaging experiments. The objective is to leverage the synergy between models finetuned on different multimodal tasks.&lt;/p&gt; &#xA;&lt;h2&gt;Weight interpolation&lt;/h2&gt; &#xA;&lt;p&gt;To average several models, you can use &lt;code&gt;preprocess/average_save_models.py&lt;/code&gt;. There is two options, either you average many models with uniform interpolation coefficient, or you interpolate between 2 models with interpolation coefficient from 0 to 1. However, you can also customise this script as you like.&lt;/p&gt; &#xA;&lt;p&gt;Once you saved the interpolated weights, you can use the following scripts to evaluate the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;## image-text tasks&#xA;sh caption/eval/eval_caption_avg.sh&#xA;sh refcoco/eval/eval_refcocoplus_avg.sh&#xA;sh snli_ve/eval/eval_snli_ve_avg.sh&#xA;sh vqa/eval/eval_vqa_avg.sh&#xA;&#xA;## video-text tasks &#xA;sh vqa/eval/video/eval_video_qa_avg.sh&#xA;sh caption/eval/video/eval_msrvtt_video_caption_avg.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Ratatouille Finetuning&lt;/h2&gt; &#xA;&lt;p&gt;For &lt;a href=&#34;https://github.com/facebookresearch/ModelRatatouille&#34;&gt;Ratatouille finetuning&lt;/a&gt;, each one of the auxiliary models (e.g. models finetuned for captioning, vqa, visual grounding and visual entailment) are re-finetuned on the target task. At the end all obtained models are uniformly averaged.&lt;/p&gt; &#xA;&lt;p&gt;The scripts to launch the finetuning and evaluation are in &lt;code&gt;averaging/ratatouille/&lt;/code&gt;. You need also to use the weight averaging script in &lt;code&gt;preprocess/average_save_models.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Fusing Finetuning&lt;/h2&gt; &#xA;&lt;p&gt;For &lt;a href=&#34;https://arxiv.org/abs/2204.03044&#34;&gt;Fusing finetuning&lt;/a&gt;, first the auxiliary models are averaged, then finetuned on the target task.&lt;/p&gt; &#xA;&lt;p&gt;The scripts to launch the finetuning and evaluation are in &lt;code&gt;averaging/fusing/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Qualitative Results&lt;/h1&gt; &#xA;&lt;p&gt;Below we provide qualitative results for some tasks.&lt;/p&gt; &#xA;&lt;h2&gt;Visual Grounding&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/examples/results/vg.jpg&#34; width=&#34;600&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2&gt;Image Captioning&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/examples/results/caption.jpg&#34; width=&#34;600&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2&gt;Open-Ended VQA&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mshukor/UnIVAL/main/examples/results/vqa.jpg&#34; width=&#34;600&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you find the work helpful, you can cite it using the following citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{shukor2023unified,&#xA;  title={Unified Model for Image, Video, Audio and Language Tasks},&#xA;  author={Shukor, Mustafa and Dancette, Corentin and Rame, Alexandre and Cord, Matthieu},&#xA;  journal={arXiv preprint arXiv:2307.16184},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Aknowledgment&lt;/h1&gt; &#xA;&lt;p&gt;This code is based mainly on the following repos:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OFA-Sys/OFA&#34;&gt;OFA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/fairseq&#34;&gt;Fairseq&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;taming-transformers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We thank the authors for releasing their code.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>BexTuychiev/tricking-data-science</title>
    <updated>2023-08-06T01:34:49Z</updated>
    <id>tag:github.com,2023-08-06:/BexTuychiev/tricking-data-science</id>
    <link href="https://github.com/BexTuychiev/tricking-data-science" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A book of subtle code tricks and gem resources for all things data, machine learning and deep learning.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/BexTuychiev/tricking-data-science&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/BexTuychiev/tricking-data-science/main/book/images/cover.png&#34; alt=&#34;Tricking Data Science Book Logo&#34; width=&#34;126&#34; height=&#34;195&#34; align=&#34;center&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;Tricking Data Science&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/BexTuychiev/tricking-data-science&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub&#34; alt=&#34;View on GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bextuychiev.github.io/tricking-data-science/README.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Book-View%20Book-red?style=plastic&amp;amp;logo=book&#34; alt=&#34;View Book&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; Subtle code tricks and gem resources for all things data, machine learning and deep learning. &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;A book with no commitment&lt;/h2&gt; It starts with my dislike of bookmarks. As a programmer, you often come across a neat little code trick on StackOverflow or blogs by some elite senior, and say, &#34;Hey, I gotta save this.&#34; You bookmark the page, and a day later you forget the bookmark&#39;s name. A week later, you even forget that it existed in the first place. &#xA;&lt;p&gt;But in your mind, you always have that fuzzy reference knowledge that there is some really cool way of doing this one task, but you can&#39;t really put your finger on it. All programmers experience this, and we all have different ways of dealing (or not dealing) with it. Well, this book is &lt;strong&gt;my&lt;/strong&gt; way of dealing with it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Tricking Data Science&lt;/em&gt; is a collection of code tricks, titbits of advice and curated resources that I picked up during the two years I have been writing about data science on Medium. Until I got recommended to the Jupyter Book project, there was no right way for me to organize such a collection but now, here it goes.&lt;/p&gt; &#xA;&lt;h2&gt;Don&#39;t ever reinvent the wheel&lt;/h2&gt; &#xA;&lt;p&gt;There is only one loose requirement for a certain code trick to be added to this book: It should perform a task in a short and concise way that most people don&#39;t know about and thus, waste their time reinventing the wheel. Today&#39;s languages and packages have been in use for such a long time now that there is always the best and shortest way of doing something. It is just a matter of looking in the right place. I hope this book will be that &lt;em&gt;right place&lt;/em&gt; for many.&lt;/p&gt; &#xA;&lt;h2&gt;Don&#39;t shy away...&lt;/h2&gt; This is an open-source book, not an exclusive property of mine. If you have a trick or a resource of your own, create a pull request and give me the great satisfaction of accepting my very first! With your permission, I will add the trick to the book using Snappify—the tool I used for those interactive code blocks and maybe share it on LinkedIn. &#xA;&lt;h2&gt;About the author&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/BexTuychiev/tricking-data-science/main/book/images/author.png&#34; width=&#34;300&#34; height=&#34;300&#34; align=&#34;center&#34; alt=&#34;The image of the author&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Here goes the semi-formal introduction of &lt;strong&gt;ME&lt;/strong&gt; in the third person 😁&lt;/p&gt; &#xA;&lt;p&gt;Bex Tuychiev is an undergraduate student studying business analytics (or something close to it) in Uzbekistan. He writes articles on data science, machine learning and statistics for the Medium publication &lt;a href=&#34;https://towardsdatascience.com/&#34;&gt;Towards Data Science&lt;/a&gt;. With over 120 articles written, he currently ranks as one of the Top 10 all time Medium writers in the topic of Artificial Intelligence among 180k writers. Oh, and he is also a Kaggle Master and DataCamp instructor.&lt;/p&gt; &#xA;&lt;p&gt;You can reach him for a chat or a sponsored article on &lt;a href=&#34;https://www.linkedin.com/in/bextuychiev/&#34;&gt;LinkedIn&lt;/a&gt;, where he posts the contents of this book daily. Make sure your connection request has a custom message, otherwise, he just ignores them (yes, that sounds arrogant, but he isn&#39;t - at least, he hopes) 😉&lt;/p&gt; &#xA;&lt;h2&gt;Support the author&lt;/h2&gt; &#xA;&lt;p&gt;Medium: Become a member through &lt;a href=&#34;https://ibexorigin.medium.com/membership&#34;&gt;my affiliate link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Tip me: &lt;a href=&#34;https://ko-fi.com/bextuychiev&#34;&gt;Ko-fi&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Follow: &lt;a href=&#34;https://ibexorigin.medium.com/&#34;&gt;Medium&lt;/a&gt; , &lt;a href=&#34;https://www.linkedin.com/in/bextuychiev/&#34;&gt;LinkedIn&lt;/a&gt; , &lt;a href=&#34;https://twitter.com/BexTuychiev&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/BexTuychiev&#34;&gt;GitHub&lt;/a&gt; , &lt;a href=&#34;https://www.kaggle.com/bextuychiev&#34;&gt;Kaggle&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h6&gt;Ⓒ Tricking Data Science&lt;/h6&gt;</summary>
  </entry>
  <entry>
    <title>philschmid/easyllm</title>
    <updated>2023-08-06T01:34:49Z</updated>
    <id>tag:github.com,2023-08-06:/philschmid/easyllm</id>
    <link href="https://github.com/philschmid/easyllm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;EasyLLM - &lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://pypi.org/project/easyllm&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/easyllm.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://pypi.org/project/easyllm&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/easyllm&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/philschmid/easyllm/raw/main/LICENSE&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/l/easyllm&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/philschmid/easyllm/actions?workflow=Unit Tests&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/philschmid/easyllm/workflows/Unit Tests/badge.svg&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/pypa/hatch&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A5%9A-Hatch-4051b5.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;EasyLLM&lt;/strong&gt; is an open source project that provides &lt;strong&gt;helpful tools and methods for working with large language models&lt;/strong&gt; (LLMs), both open source and closed source. Get immediataly started or check out the &lt;a href=&#34;https://philschmid.github.io/easyllm/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;EasyLLM implements clients that are &lt;strong&gt;compatible with OpenAI&#39;s Completion API&lt;/strong&gt;. This means you can easily replace &lt;code&gt;openai.ChatCompletion&lt;/code&gt;, &lt;code&gt;openai.Completion&lt;/code&gt;, &lt;code&gt;openai.Embedding&lt;/code&gt; with, for example, &lt;code&gt;huggingface.ChatCompletion&lt;/code&gt;, &lt;code&gt;huggingface.Completion&lt;/code&gt; or &lt;code&gt;huggingface.Embedding&lt;/code&gt; by changing one line of code.&lt;/p&gt; &#xA;&lt;h3&gt;Supported Clients&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;huggingface&lt;/code&gt; - &lt;a href=&#34;https://huggingface.co/&#34;&gt;HuggingFace&lt;/a&gt; models &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;huggingface.ChatCompletion&lt;/code&gt; - Chat with LLMs&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;huggingface.Completion&lt;/code&gt; - Text completion with LLMs&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;huggingface.Embedding&lt;/code&gt; - Create embeddings with LLMs&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://raw.githubusercontent.com/philschmid/easyllm/main/examples&#34;&gt;Examples&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;h2&gt;🚀 Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Install EasyLLM via pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install easyllm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then import and start using the clients:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from easyllm.clients import huggingface&#xA;&#xA;# helper to build llama2 prompt&#xA;huggingface.prompt_builder = &#34;llama2&#34;&#xA;&#xA;response = huggingface.ChatCompletion.create(&#xA;    model=&#34;meta-llama/Llama-2-70b-chat-hf&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;\nYou are a helpful assistant speaking like a pirate. argh!&#34;},&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is the sun?&#34;},&#xA;    ],&#xA;    temperature=0.9,&#xA;    top_p=0.6,&#xA;    max_tokens=256,&#xA;)&#xA;&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the result will look like&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;{&#xA;  &#34;id&#34;: &#34;hf-lVC2iTMkFJ&#34;,&#xA;  &#34;object&#34;: &#34;chat.completion&#34;,&#xA;  &#34;created&#34;: 1690661144,&#xA;  &#34;model&#34;: &#34;meta-llama/Llama-2-70b-chat-hf&#34;,&#xA;  &#34;choices&#34;: [&#xA;    {&#xA;      &#34;index&#34;: 0,&#xA;      &#34;message&#34;: {&#xA;        &#34;role&#34;: &#34;assistant&#34;,&#xA;        &#34;content&#34;: &#34; Arrrr, the sun be a big ol&#39; ball o&#39; fire in the sky, me hearty! It be the source o&#39; light and warmth for our fair planet, and it be a mighty powerful force, savvy? Without the sun, we&#39;d be sailin&#39; through the darkness, lost and cold, so let&#39;s give a hearty \&#34;Yarrr!\&#34; for the sun, me hearties! Arrrr!&#34;&#xA;      },&#xA;      &#34;finish_reason&#34;: null&#xA;    }&#xA;  ],&#xA;  &#34;usage&#34;: {&#xA;    &#34;prompt_tokens&#34;: 111,&#xA;    &#34;completion_tokens&#34;: 299,&#xA;    &#34;total_tokens&#34;: 410&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check out other examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/philschmid/easyllm/main/notebooks/chat-completion-api.ipynb&#34;&gt;Detailed ChatCompletion Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/philschmid/easyllm/main/notebooks/stream-chat-completions.ipynb&#34;&gt;Example how to stream chat requests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/philschmid/easyllm/main/notebooks/stream-text-completions.ipynb&#34;&gt;Example how to stream text requests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/philschmid/easyllm/main/notebooks/text-completion-api.ipynb&#34;&gt;Detailed Completion Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/philschmid/easyllm/main/notebooks/get-embeddings&#34;&gt;Create Embeddings&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://philschmid.github.io/easyllm/&#34;&gt;documentation&lt;/a&gt; for more detailed usage and examples.&lt;/p&gt; &#xA;&lt;h2&gt;💪🏻 Migration from OpenAI to HuggingFace&lt;/h2&gt; &#xA;&lt;p&gt;Migrating from OpenAI to HuggingFace is easy. Just change the import statement and the client you want to use and optionally the prompt builder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- import openai&#xA;+ from easyllm.clients import huggingface&#xA;+ huggingface.prompt_builder = &#34;llama2&#34;&#xA;&#xA;&#xA;- response = openai.ChatCompletion.create(&#xA;+ response = huggingface.ChatCompletion.create(&#xA;-    model=&#34;gpt-3.5-turbo&#34;,&#xA;+    model=&#34;meta-llama/Llama-2-70b-chat-hf&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Knock knock.&#34;},&#xA;    ],&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure when you switch your client that your hyperparameters are still valid. For example, &lt;code&gt;temperature&lt;/code&gt; of GPT-3 might be different than &lt;code&gt;temperature&lt;/code&gt; of &lt;code&gt;Llama-2&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;☑️ Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;🤝 Compatible Clients&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implementation of clients compatible with OpenAI API format of &lt;code&gt;openai.ChatCompletion&lt;/code&gt;, &lt;code&gt;openai.Completion&lt;/code&gt;, &lt;code&gt;openai.Embedding&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Easily switch between different LLMs like &lt;code&gt;openai.ChatCompletion&lt;/code&gt; and &lt;code&gt;huggingface.ChatCompletion&lt;/code&gt; by changing one line of code.&lt;/li&gt; &#xA; &lt;li&gt;Support for streaming of completions, checkout example &lt;a href=&#34;https://raw.githubusercontent.com/philschmid/easyllm/main/notebooks/stream-chat-completions.ipynb&#34;&gt;How to stream completions&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;⚙️ Helper Modules ⚙️&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;evol_instruct&lt;/code&gt; (work in progress) - Use evolutionary algorithms create instructions for LLMs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;prompt_utils&lt;/code&gt; - Helper methods to easily convert between prompt formats like OpenAI Messages to prompts for open source models like Llama 2.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🙏 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;EasyLLM is an open source project and welcomes contributions of all kinds.&lt;/p&gt; &#xA;&lt;p&gt;The project uses &lt;a href=&#34;https://hatch.pypa.io/latest/&#34;&gt;hatch&lt;/a&gt; for development. To get started, fork the repository and clone it to your local machine.&lt;/p&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;Confirm &lt;a href=&#34;https://hatch.pypa.io/latest/install/&#34;&gt;hatch&lt;/a&gt; is installed (pipx is great to make it available globally on your machine)&lt;/li&gt; &#xA; &lt;li&gt;Once in the project directory, run &lt;code&gt;hatch env create&lt;/code&gt; to create a default virtual environment for development.&lt;/li&gt; &#xA; &lt;li&gt;Activate the virtual environment with &lt;code&gt;hatch shell&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Start developing! 🤩&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;📔 Citation &amp;amp; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;If you use EasyLLM, please share it with me on social media or email. I would love to hear about it! You can also cite the project using the following BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@software{Philipp_Schmid_EasyLLM_2023,&#xA;author = {Philipp Schmid},&#xA;license = {Apache-2.0},&#xA;month = juj,&#xA;title = {EasyLLM: Streamlined Tools for LLMs},&#xA;url = {https://github.com/philschmid/easyllm},&#xA;year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>