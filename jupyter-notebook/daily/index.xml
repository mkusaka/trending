<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-07T01:31:25Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>deep-learning-indaba/indaba-pracs-2023</title>
    <updated>2023-09-07T01:31:25Z</updated>
    <id>tag:github.com,2023-09-07:/deep-learning-indaba/indaba-pracs-2023</id>
    <link href="https://github.com/deep-learning-indaba/indaba-pracs-2023" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notebooks for the Practicals at the Deep Learning Indaba 2023.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deep Learning Indaba Practicals 2023&lt;/h1&gt; &#xA;&lt;h2&gt;The Practicals&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Topic 💥&lt;/th&gt; &#xA;   &lt;th&gt;Description 📘&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Dive into Machine Learning: Learning by Implementing &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/deep-learning-indaba/indaba-pracs-2023/raw/main/practicals/Intro_ML_English_Prac.ipynb&#34;&gt;English&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/Intro_ML_English_Prac.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/deep-learning-indaba/indaba-pracs-2023/raw/introduction-to-ml-french/practicals/Intro_ML_French_Prac.ipynb&#34;&gt;French&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/Intro_ML_French_Prac.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This tutorial offers an immersive exploration of the world of machine learning. Our primary goal is to demystify complex concepts, presenting them in a simplified manner. We adopt an interactive approach, fostering a gradual and intuitive understanding that enables learners to construct their very first machine-learning model step by step.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/deep-learning-indaba/indaba-pracs-2023/raw/main/practicals/ML_for_Bio_Indaba_Practical_2023.ipynb&#34;&gt;Machine Learning for Biology: Learning the Language of Life&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/ML_for_Bio_Indaba_Practical_2023.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;In this tutorial we invite you to explore the language of proteins using machine learning. You don&#39;t require any previous bio experience, just some basic python, machine learning knowledge and a curious spirit! We will be working with cutting edge techniques in protein langauge modelling, such as using large language models to compute embeddings, how to visualise and explore these embeddings, and finally you will have a chance to train your own model!&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;&#34;&gt;Responsible AI&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;As AI systems are increasingly used in high-stakes situations, there are important considerations on how to deploy the systems safely and responsibly. In this prac, we will discuss various principles for responsible AI. We will then concentrate on definitions of fairness. In the first half, we will do an activity to brainstorm definitions of fairness for specific use cases. In the second half, we will implement definitions of fairness per existing literature. We will then discuss and implement methods to improve the fairness outcomes of simple linear models.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/deep-learning-indaba/indaba-pracs-2023/raw/main/practicals/Introduction_to_Probabilistic_Thinking_and_Programming.ipynb&#34;&gt;Introduction to Probabilistic Thinking and Programming&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/Introduction_to_Probabilistic_Thinking_and_Programming.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Thinking probabilistically and working with probability distributions can be very powerful tools for any machine learning practitioner. Unfortunately, they are tools that are often disregarded due to their perceived complexity. In this practical we hope to demistify these ideas by building intuition, provided practical tips, and introducing a very powerful framework for embracing the probabilistic approach – probabilistic programming. We’ll both motivate why we need probabilistic programming and give an introduction for using it in practice. &lt;br&gt; &lt;br&gt; This prac is aimed at all knowledge levels! No matter what your prior experience with probabilistic thinking and/or programming, we are sure that you will be able to take away some useful knowledge from this practical. However, this means that depending on your level, some of the content will not be aimed at you. Don’t worry, this will be clearly marked at all points. We reccomend that you try and stick to our suggestions in order to get the most out of this prac in the given time, but if curiosity gets the better of you that’s also great!&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/deep-learning-indaba/indaba-pracs-2023/raw/main/practicals/Recommender_Systems.ipynb&#34;&gt;Recommender Systems or Why Your Phone Isn&#39;t Actually Spying on You (kinda)&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/Recommender_Systems.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Recommender Systems are probably one of the most ubiquitous type of machine learning model that we encounter in our online life. They influence what we see in our social media feeds, the products we buy, the music we listen to, the food we eat, and the movies we watch. Sometimes they&#39;re so good that people feel that their phone is spying on their conversations! In this prac, we hope to convince you that this isn&#39;t the case (mostly) as well as taking you through some of the techniques popularly used in industry that recommends the content you see online.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/deep-learning-indaba/indaba-pracs-2023/raw/main/practicals/RL_2023_prac.ipynb&#34;&gt;Frozen Lake: An Icy Adventure Using Reinforcement Learning!&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/RL_2023_prac.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RL has seen tremendous success in a wide range of challenging domains such as learning to play complex games like &lt;a href=&#34;https://arxiv.org/abs/1312.5602&#34;&gt;Atari&lt;/a&gt;, &lt;a href=&#34;https://www.nature.com/articles/s41586-019-1724-z&#34;&gt;StarCraft II&lt;/a&gt; and &lt;a href=&#34;https://www.nature.com/articles/nature16961&#34;&gt;GO&lt;/a&gt;, and applications like &lt;a href=&#34;https://arxiv.org/abs/1808.00177&#34;&gt;robotics&lt;/a&gt;. Modern natural language processing (NLP) also uses RL to improve the quality of language models based on human feedback (see &lt;a href=&#34;https://huggingface.co/blog/rlhf&#34;&gt;RLHF&lt;/a&gt;). &lt;br&gt; &lt;br&gt; In this tutorial, we will show how RL can be used to help our agent cross a Frozen Lake, while avoiding perilous holes in the ice in order to get to a goal. You will use several different RL approaches, ranging from tabular Q-learning to more modern methods, such as &lt;a href=&#34;https://arxiv.org/abs/1312.5602&#34;&gt;DQN (Deep Q-Networks)&lt;/a&gt;. Along the way, you will be introduced to some of the most fundamental concepts and terminology in RL, while trying to build intuition into what RL is about.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Mathematical Tools for Machine Learning: Linear Algebra &lt;br&gt; &lt;br&gt; &lt;a href=&#34;&#34;&gt;English&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;&#34;&gt;French&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mathematics is the bedrock tool for all quantitative sciences and machine learning is no exception. Basic literacy in mathematics is essential for any machine learning scientist and fluency is often necessary for understanding or advancing the state of the art. In this practical we focus on linear algebra, providing separate tracks for those at the beginner and intermediate levels. This practical will equip you to understand the vast majority of the linear algebra used in machine learning code and papers, and provide you with the basic skills to do derivations and computations of your own. &lt;br&gt; &lt;br&gt; In the beginner track we give an introduction to the basics of vectors and matrices, as well as an introduction to inner products and norms. The beginner track concludes by applying the techniques we’ve learned to derive the solution for ridge regression, a commonly used method for linear prediction with regularization. The intermediate track covers projections, orthonormal bases, the determinant, the trace, eigenvalues and eigenvectors. The intermediate track concludes by applying the techniques to principal component analysis, a very common method for dimensionality reduction. Whether you’re taking the beginner or intermediate track (or both!), we hope you find this practical useful!&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;&#34;&gt;LLMs for everyone&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/large_language_models.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Welcome to &#34;LLMs for Everyone,&#34; a practical exploration into the captivating world of Language Models! This entire block of text was crafted only by ChatGPT, showcasing the remarkable capabilities of these type models. Throughout this practical, we will delve into the underlying fundamentals of transformers, the powerful technology that drives models like GPT, and learn how to fine-tune and train our very own Large Language Models. Let&#39;s embark on this exciting journey of understanding and creating LLMs, and discover how such impressive AI text generation is made possible!&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/deep-learning-indaba/indaba-pracs-2023/raw/main/practicals/diffusion_models.ipynb&#34;&gt;Diffusion models: Building your own Stable Diffusion&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/diffusion_models.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Denoising Diffusion Models are a variant of generative modelling that serve as the backbone in recent advances in image synthesis - including Dall-E 2, Stable Diffusion, and Midjourney. These models utilise an iterative denoising process during generation to produce high-quality samples. In this practical, we will explore the fundamentals of diffusion models, the intuition behind them, and how they work in practice. By the end of the practical, we will have covered all the steps required to train one of these models from scratch!&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/deep-learning-indaba/indaba-pracs-2023/raw/main/practicals/geospatial_machine_learning.ipynb&#34;&gt;Let&#39;s map Africa! Introductory Tutorial to Geospatial Machine Learning&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/geospatial_machine_learning.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Climate change is a pressing issue affecting the entire planet, with the Global South bearing a disproportionate burden of its impacts despite contributing less to its causes compared to more developed nations in the Global North. Extreme climate events such as droughts, floods, storms, and heatwaves have led to food insecurity and poverty. On the other hand, satellite imagery and machine learning (ML) can help address climate related challenges in food and water security, biodiversity, energy, and public health. To this end, this practical is designed to provide an introductory tutorial on geospatial machine learning for agriculture, particularly to classify farm-level crop types in Kenya using Sentinel-2 satellite imagery. Starting with an introductory session on key concepts of geospatial ML, the tutorial delves into ML development, validation, and performance evaluation techniques. Moreover, we aim to kick-start a build-up of an African GeoAI community that strives and collaborates to solve related challenges in the continent.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;This repository contains the practical notebooks for the Deep Learning Indaba 2023, held at the University of Ghana, Accra, Ghana.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;http://www.deeplearningindaba.com&#34;&gt;www.deeplearningindaba.com&lt;/a&gt; for more details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zhoujing204/python_course</title>
    <updated>2023-09-07T01:31:25Z</updated>
    <id>tag:github.com,2023-09-07:/zhoujing204/python_course</id>
    <link href="https://github.com/zhoujing204/python_course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A python course for college students.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python课程&lt;/h1&gt; &#xA;&lt;p&gt;本Python课程是为本科三年级大学生开设的。本课程为零基础课程，学习本课程不需要具备Python编程知识和基础，但是需要具备一定计算机基础的知识。&lt;/p&gt; &#xA;&lt;h2&gt;课程目标&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;掌握Python编程语言的基本语法和特性。&lt;/li&gt; &#xA; &lt;li&gt;掌握面向对象编程（Object-Oriented Programming, OOP）的概念和实践，掌握函数式编程的概念和实践。&lt;/li&gt; &#xA; &lt;li&gt;学习使用Python进行程序设计和开发，包括算法和数据结构的应用。&lt;/li&gt; &#xA; &lt;li&gt;了解Python在不同领域的应用，如科学计算、数据分析、人工智能、Web应用开发等。&lt;/li&gt; &#xA; &lt;li&gt;培养良好的编程习惯和规范，包括代码可读性、模块化设计和文档编写。&lt;/li&gt; &#xA; &lt;li&gt;学会使用常见的Python开发工具、集成开发环境（IDE）和版本控制系统（如Git）。&lt;/li&gt; &#xA; &lt;li&gt;开发实际项目和练习实践，以实践中不断提升编程技能和思维能力。&lt;/li&gt; &#xA; &lt;li&gt;培养自主学习和持续学习的能力，包括阅读官方文档、参与开源社区和探索新的Python库和工具。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;课程目标旨在帮助学生全面了解和掌握Python编程语言，培养他们在软件开发和计算机科学领域的技能，为未来的学习和职业发展打下坚实基础。&lt;/p&gt; &#xA;&lt;h2&gt;参考书籍&lt;/h2&gt; &#xA;&lt;p&gt;本课程主要使用的教材和参考书籍以及书籍源代码如下：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;《Python编程从入门到实践 第2版》&lt;a href=&#34;https://github.com/ehmatthes/pcc_2e&#34;&gt;源代码&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;《流畅的Python 第2版》&lt;a href=&#34;https://github.com/ehmatthes/pcc_2e&#34;&gt;源代码&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;《Python数据分析 第3版》&lt;a href=&#34;https://github.com/wesm/pydata-book&#34;&gt;源代码&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;课程内容&lt;/h2&gt; &#xA;&lt;p&gt;本课程在讲授过程中将完整地使用教材1讲授Python的基本语法以及三个实践项目的重要部分，在讲授Python一些高级语法和语言特性时，会使用参考书籍2的部分章节，在讲授数据分析相关知识例如Numpy和Pandas时会使用参考书籍3的相关章节。&lt;/p&gt; &#xA;&lt;p&gt;课程主要教学内容如下：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python课程简介&lt;/li&gt; &#xA; &lt;li&gt;变量和简单数据类型&lt;/li&gt; &#xA; &lt;li&gt;列表, 列表操作&lt;/li&gt; &#xA; &lt;li&gt;if语句,字典, 用户输入和while循环&lt;/li&gt; &#xA; &lt;li&gt;Python数据结构&lt;/li&gt; &#xA; &lt;li&gt;函数&lt;/li&gt; &#xA; &lt;li&gt;函数高级&lt;/li&gt; &#xA; &lt;li&gt;Python面向对象编程&lt;/li&gt; &#xA; &lt;li&gt;文件和异常&lt;/li&gt; &#xA; &lt;li&gt;单元测试&lt;/li&gt; &#xA; &lt;li&gt;项目一：外星人入侵&lt;/li&gt; &#xA; &lt;li&gt;项目二：数据可视化&lt;/li&gt; &#xA; &lt;li&gt;项目三：Web应用程序&lt;/li&gt; &#xA; &lt;li&gt;Python在数据分析以及人工智能领域的应用&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;课程内容的安排可以根据学生学习的实际情况和教学目标进行灵活调整。&lt;/p&gt; &#xA;&lt;h2&gt;课程实验&lt;/h2&gt; &#xA;&lt;p&gt;本课程共包括7次实验：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zhoujing204/python_course/main/Experiments/experiment1.md&#34;&gt;实验一 Git和Markdown基础&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zhoujing204/python_course/main/Experiments/experiment2.md&#34;&gt;实验二 Python变量、简单数据类型&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zhoujing204/python_course/main/Experiments/experiment3.md&#34;&gt;实验三 Python列表&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zhoujing204/python_course/main/Experiments/experiment4.md&#34;&gt;实验四 Python字典和while循环&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zhoujing204/python_course/main/Experiments/experiment5.md&#34;&gt;实验五 Python数据结构与数据模型&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zhoujing204/python_course/main/Experiments/experiment6.md&#34;&gt;实验六 函数&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zhoujing204/python_course/main/Experiments/experiment7.md&#34;&gt;实验七 Python面向对象编程&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;实验内容包括教材《Python从入门到实践》的课后练习，另外还包括了&lt;a href=&#34;https://www.codewars.com&#34;&gt;Codewars网站&lt;/a&gt;上的Kata挑战，kata挑战包括从8kyu(最容易)到1kyu（最难）的8个难度等级，同学们可以自由选择合适自己的难度等级的任务来完成实验。&lt;/p&gt; &#xA;&lt;h2&gt;课程项目&lt;/h2&gt; &#xA;&lt;p&gt;本课程的教材《Python从入门到实践》包括了3个项目：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;外星人入侵&lt;/li&gt; &#xA; &lt;li&gt;数据可视化&lt;/li&gt; &#xA; &lt;li&gt;Web应用程序&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;同学们也可以自由选择其他项目来完成课程项目，例如：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;其他Web应用程序项目&lt;/li&gt; &#xA; &lt;li&gt;其他游戏项目&lt;/li&gt; &#xA; &lt;li&gt;Python爬虫项目&lt;/li&gt; &#xA; &lt;li&gt;Python数据分析项目&lt;/li&gt; &#xA; &lt;li&gt;Python人工智能项目&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;如果你不知道应该如何选择合适的项目以及项目指导，可以参考这个Github Repo:&lt;a href=&#34;https://github.com/practical-tutorials/project-based-learning&#34;&gt;project-based-learning&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>baidu/puck</title>
    <updated>2023-09-07T01:31:25Z</updated>
    <id>tag:github.com,2023-09-07:/baidu/puck</id>
    <link href="https://github.com/baidu/puck" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Puck is a high-performance ANN search engine&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;This project is a library for approximate nearest neighbor(ANN) search named Puck. In Industrial deployment scenarios, limited memory, expensive computer resources and increasing database size are as important as the recall-vs-latency tradeof for all search applications. Along with the rapid development of retrieval business service, it has the big demand for the highly recall-vs-latency and precious but finite resource, the borning of Puck is precisely for meeting this kind of need.&lt;/p&gt; &#xA;&lt;p&gt;It contains two algorithms, Puck and Tinker. This project is written in C++ with wrappers for python3.&lt;br&gt; Puck is an efficient approache for large-scale dataset, which has the best performance of multiple 1B-datasets in &lt;a href=&#34;https://github.com/harsha-simhadri/big-ann-benchmarks/raw/main/neurips21/t1_t2/README.md#results-for-t1&#34;&gt;NeurIPS&#39;21 competition track&lt;/a&gt;. Since then, performance of Puck has increased by 70%. Puck includes a two-layered architectural design for inverted indices and a multi-level quantization on the dataset. If the memory is going to be a bottleneck, Puck could resolve your problems.&lt;br&gt; Tinker is an efficient approache for smaller dataset(like 10M, 100M), which has better performance than Nmslib in big-ann-benchmarks. The relationships among similarity points are well thought out, Tinker need more memory to save these. Thinker cost more memory then Puck, but has better performace than Puck. If you want a better searching performance and need not concerned about memory used, Tinker is a better choiese.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This project supports cosine similarity, L2(Euclidean) and IP(Inner Product, conditioned). When two vectors are normalized, L2 distance is equal to 2 - 2 * cos. IP2COS is a transform method that convert IP distance to cos distance. The distance value in search result is always L2.&lt;/p&gt; &#xA;&lt;p&gt;Puck use a compressed vectors(after PQ) instead of the original vectors, the memory cost just over to 1/4 of the original vectors by default. With the increase of datasize, Puck&#39;s advantage is more obvious.&lt;br&gt; Tinker need save relationships of similarity points, the memory cost is more than the original vectors (less than Nmslib) by default. More performance details in benchmarks. Please see &lt;a href=&#34;https://raw.githubusercontent.com/baidu/puck/main/ann-benchmarks/README.md&#34;&gt;this readme&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Linux install&lt;/h2&gt; &#xA;&lt;h3&gt;1.The prerequisite is mkl, python and cmake.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;MKL&lt;/strong&gt;: MKL must be installed to compile puck, download the MKL installation package corresponding to the operating system from the official website, and configure the corresponding installation path after the installation is complete. source the MKL component environment script, eg. source ${INSTALL_PATH}/mkl/latest/env/vars.sh. This will maintain many sets of environment variables, like MKLROOT.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl-download.html&#34;&gt;https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl-download.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;python&lt;/strong&gt;: Version higher than 3.6.0.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;cmake&lt;/strong&gt;: Version higher than 3.21.&lt;/p&gt; &#xA;&lt;h3&gt;2.Clone this project.&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/baidu/puck.git&#xA;cd puck&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3.Use cmake to build this project.&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cmake -DCMAKE_BUILD_TYPE=Release -DPYTHON_INCLUDE_DIR=$(python3 -c &#34;import sysconfig; print(sysconfig.get_path(&#39;include&#39;))&#34;)  \&#xA;    -DPYTHON_LIBRARY=$(python3 -c &#34;import sysconfig; print(sysconfig.get_config_var(&#39;LIBDIR&#39;))&#34;) \&#xA;    -DMKLROOT=${MKLROOT} \&#xA;    -DUSE_PYTHON=ON \&#xA;    -DBLA_VENDOR=Intel10_64lp_seq \&#xA;    -DBLA_STATIC=ON  \&#xA;    -B build .&#xA;&#xA;cd build &amp;amp;&amp;amp; make &amp;amp;&amp;amp; make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Output files are saved in build/output subdirectory by default.&lt;/p&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;p&gt;Output files include demos of train, build and search tools.&lt;br&gt; Train and build tools are in build/output/build_tools subdirectory.&lt;br&gt; Search demo tools are in build/output/bin subdirectory.&lt;/p&gt; &#xA;&lt;h3&gt;1.format vector dataset for train and build&lt;/h3&gt; &#xA;&lt;p&gt;The vectors are stored in raw little endian. Each vector takes 4+d*4 bytes for .fvecs format, where d is the dimensionality of the vector.&lt;/p&gt; &#xA;&lt;h3&gt;2.train &amp;amp; build&lt;/h3&gt; &#xA;&lt;p&gt;The default train configuration file is &#34;build/output/build_tools/conf/puck_train.conf&#34;. The length of each feature vector must be set in train configuration file (feature_dim).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd output/build_tools&#xA;cp YOUR_FEATURE_FILE puck_index/all_data.feat.bin&#xA;sh script/puck_train_control.sh -t -b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;index files are saved in puck_index subdirectory by default.&lt;/p&gt; &#xA;&lt;h3&gt;3.search&lt;/h3&gt; &#xA;&lt;p&gt;During searching, the default value of index files path is &#39;./puck_index&#39;.&lt;br&gt; The format of query file, refer to &lt;a href=&#34;https://raw.githubusercontent.com/baidu/puck/main/tools/demo/init-feature-example&#34;&gt;demo&lt;/a&gt;&lt;br&gt; Search parameters can be modified using a configuration file, refer to &lt;a href=&#34;https://raw.githubusercontent.com/baidu/puck/main/demo/conf/puck.conf&#34;&gt;demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd output/&#xA;ln -s build_tools/puck_index .&#xA;./bin/search_client YOUR_QUERY_FEATURE_FILE RECALL_FILE_NAME --flagfile=conf/puck.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;recall results are stored in file RECALL_FILE_NAME.&lt;/p&gt; &#xA;&lt;h2&gt;more details&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/baidu/puck/main/docs/README.md&#34;&gt;more details for puck&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/baidu/puck/main/ann-benchmarks/README.md&#34;&gt;this readme&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;this ann-benchmark is forked from &lt;a href=&#34;https://github.com/harsha-simhadri/big-ann-benchmarks&#34;&gt;https://github.com/harsha-simhadri/big-ann-benchmarks&lt;/a&gt; of 2021.&lt;/p&gt; &#xA;&lt;p&gt;How to run this benchmark is the same with it. We add support of faiss(IVF,IVF-Flat,HNSW) , nmslib（HNSW）,Puck and Tinker of T1 track. And We update algos.yaml of these method using recommended parameters of 4 datasets(bigann-10M, bigann-100M, deep-10M, deep-100M)&lt;/p&gt; &#xA;&lt;h2&gt;Discussion&lt;/h2&gt; &#xA;&lt;p&gt;Join our QQ group if you are interested in this project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/baidu/puck/main/docs/PuckQQGroup.jpeg&#34; alt=&#34;QQ Group&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>