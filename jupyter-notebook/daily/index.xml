<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-14T01:36:15Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>google-research/maxvit</title>
    <updated>2022-10-14T01:36:15Z</updated>
    <id>tag:github.com,2022-10-14:/google-research/maxvit</id>
    <link href="https://github.com/google-research/maxvit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ECCV 2022] Official repository for &#34;MaxViT: Multi-Axis Vision Transformer&#34;. SOTA foundation models for classification, detection, segmentation, image quality, and generative modeling...&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MaxViT: Multi-Axis Vision Transformer (ECCV 2022)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.01697&#34;&gt;&lt;img src=&#34;http://img.shields.io/badge/Paper-arXiv.2104.00298-B3181B?logo=arXiv&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/google-research/maxvit/blob/master/MaxViT_tutorial.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Tutorial In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/WEgB4lAZyKM&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Video-Presentation-F9D371&#34; alt=&#34;video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository hosts the official TensorFlow implementation of MAXViT models:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.01697&#34;&gt;MaxViT: Multi-Axis Vision Transformer&lt;/a&gt;. ECCV 2022.&lt;br&gt; &lt;a href=&#34;https://twitter.com/_vztu&#34;&gt;Zhengzhong Tu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=UOX9BigAAAAJ&#34;&gt;Hossein Talebi&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/hanzhang&#34;&gt;Han Zhang&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/feng-yang&#34;&gt;Feng Yang&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/milanfarhome/&#34;&gt;Peyman Milanfar&lt;/a&gt;, &lt;a href=&#34;https://www.ece.utexas.edu/people/faculty/alan-bovik&#34;&gt;Alan Bovik&lt;/a&gt;, and &lt;a href=&#34;https://scholar.google.com/citations?user=kZsIU74AAAAJ&amp;amp;hl=en&#34;&gt;Yinxiao Li&lt;/a&gt;&lt;br&gt; Google Research, University of Texas at Austin\&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Disclaimer: This is not an officially supported Google product.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Oct 12, 2022: Added the remaining ImageNet-1K and -21K checkpoints.&lt;/li&gt; &#xA; &lt;li&gt;Oct 4, 2022: A list of updates &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Added MaxViTTiny and MaxViTSmall checkpoints.&lt;/li&gt; &#xA;   &lt;li&gt;Added a Colab tutorial.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Sep 8, 2022: our Google AI blog covering both &lt;a href=&#34;https://arxiv.org/abs/2204.01697&#34;&gt;MaxViT&lt;/a&gt; and &lt;a href=&#34;https://github.com/google-research/maxim&#34;&gt;MAXIM&lt;/a&gt; is &lt;a href=&#34;https://ai.googleblog.com/2022/09/a-multi-axis-approach-for-vision.html&#34;&gt;live&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Sep 7, 2022: &lt;a href=&#34;https://github.com/rwightman&#34;&gt;@rwightman&lt;/a&gt; released a few small model weights in &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models#aug-26-2022&#34;&gt;timm&lt;/a&gt;. Achieves even better results than our paper. See more &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models#aug-26-2022&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Aug 26, 2022: our MaxViT models have been implemented in &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models#aug-26-2022&#34;&gt;timm (pytorch-image-models)&lt;/a&gt;. Kudos to &lt;a href=&#34;https://github.com/rwightman&#34;&gt;@rwightman&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;July 21, 2022: Initial code release of &lt;a href=&#34;https://arxiv.org/abs/2204.01697&#34;&gt;MaxViT models&lt;/a&gt;: accepted to ECCV&#39;22.&lt;/li&gt; &#xA; &lt;li&gt;Apr 6, 2022: MaxViT has been implemented by &lt;a href=&#34;https://github.com/lucidrains&#34;&gt;@lucidrains&lt;/a&gt;: &lt;a href=&#34;https://github.com/lucidrains/vit-pytorch#maxvit&#34;&gt;vit-pytorch&lt;/a&gt; &lt;span&gt;ðŸ˜±&lt;/span&gt; &lt;span&gt;ðŸ¤¯&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Apr 4, 2022: initial uploads to &lt;a href=&#34;https://arxiv.org/abs/2204.01697&#34;&gt;Arxiv&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;MaxViT Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.01697&#34;&gt;MaxViT&lt;/a&gt; is a family of hybrid (CNN + ViT) image classification models, that achieves better performances across the board for both parameter and FLOPs efficiency than both SoTA ConvNets and Transformers. They can also scale well on large dataset sizes like ImageNet-21K. Notably, due to the linear-complexity of the grid attention used, MaxViT is able to &#39;&#39;see&#39;&#39; globally throughout the entire network, even in earlier, high-resolution stages.&lt;/p&gt; &#xA;&lt;p&gt;MaxViT meta-architecture:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/google-research/maxvit/main/doc/maxvit_arch.png&#34; width=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Results on ImageNet-1k train and test:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/google-research/maxvit/main/doc/imagenet_results.png&#34; width=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Results on ImageNet-21k and JFT pre-trained models:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/google-research/maxvit/main/doc/i21k_jft_results.png&#34; width=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Colab Demo&lt;/h2&gt; &#xA;&lt;p&gt;We have released a Google Colab Demo on the tutorials of how to run MaxViT on images. Try it here &lt;a href=&#34;https://colab.research.google.com/github/google-research/maxvit/blob/master/MaxViT_tutorial.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained MaxViT Checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;We have provided a list of results and checkpoints as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;resolution&lt;/th&gt; &#xA;   &lt;th&gt;Top1 Acc.&lt;/th&gt; &#xA;   &lt;th&gt;#Params&lt;/th&gt; &#xA;   &lt;th&gt;FLOPs&lt;/th&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-T&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;83.62%&lt;/td&gt; &#xA;   &lt;td&gt;31M&lt;/td&gt; &#xA;   &lt;td&gt;5.6B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvittiny/i1k/224&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-T&lt;/td&gt; &#xA;   &lt;td&gt;384x384&lt;/td&gt; &#xA;   &lt;td&gt;85.24%&lt;/td&gt; &#xA;   &lt;td&gt;31M&lt;/td&gt; &#xA;   &lt;td&gt;17.7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvittiny/i1k/384&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-T&lt;/td&gt; &#xA;   &lt;td&gt;512x512&lt;/td&gt; &#xA;   &lt;td&gt;85.72%&lt;/td&gt; &#xA;   &lt;td&gt;31M&lt;/td&gt; &#xA;   &lt;td&gt;33.7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvittiny/i1k/512&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-S&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;84.45%&lt;/td&gt; &#xA;   &lt;td&gt;69M&lt;/td&gt; &#xA;   &lt;td&gt;11.7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitsmall/i1k/224&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-S&lt;/td&gt; &#xA;   &lt;td&gt;384x384&lt;/td&gt; &#xA;   &lt;td&gt;85.74%&lt;/td&gt; &#xA;   &lt;td&gt;69M&lt;/td&gt; &#xA;   &lt;td&gt;36.1B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitsmall/i1k/384&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-S&lt;/td&gt; &#xA;   &lt;td&gt;512x512&lt;/td&gt; &#xA;   &lt;td&gt;86.19%&lt;/td&gt; &#xA;   &lt;td&gt;69M&lt;/td&gt; &#xA;   &lt;td&gt;67.6B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitsmall/i1k/512&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-B&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;84.95%&lt;/td&gt; &#xA;   &lt;td&gt;119M&lt;/td&gt; &#xA;   &lt;td&gt;24.2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitbase/i1k/224&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-B&lt;/td&gt; &#xA;   &lt;td&gt;384x384&lt;/td&gt; &#xA;   &lt;td&gt;86.34%&lt;/td&gt; &#xA;   &lt;td&gt;119M&lt;/td&gt; &#xA;   &lt;td&gt;74.2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitbase/i1k/384&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-B&lt;/td&gt; &#xA;   &lt;td&gt;512x512&lt;/td&gt; &#xA;   &lt;td&gt;86.66%&lt;/td&gt; &#xA;   &lt;td&gt;119M&lt;/td&gt; &#xA;   &lt;td&gt;138.5B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitbase/i1k/512&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-L&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;85.17%&lt;/td&gt; &#xA;   &lt;td&gt;212M&lt;/td&gt; &#xA;   &lt;td&gt;43.9B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitlarge/i1k/224&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-L&lt;/td&gt; &#xA;   &lt;td&gt;384x384&lt;/td&gt; &#xA;   &lt;td&gt;86.40%&lt;/td&gt; &#xA;   &lt;td&gt;212M&lt;/td&gt; &#xA;   &lt;td&gt;133.1B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitlarge/i1k/384&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-L&lt;/td&gt; &#xA;   &lt;td&gt;512x512&lt;/td&gt; &#xA;   &lt;td&gt;86.70%&lt;/td&gt; &#xA;   &lt;td&gt;212M&lt;/td&gt; &#xA;   &lt;td&gt;245.4B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitlarge/i1k/512&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Here are a list of ImageNet-21K pretrained and ImageNet-1K finetuned models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;resolution&lt;/th&gt; &#xA;   &lt;th&gt;Top1 Acc.&lt;/th&gt; &#xA;   &lt;th&gt;#Params&lt;/th&gt; &#xA;   &lt;th&gt;FLOPs&lt;/th&gt; &#xA;   &lt;th&gt;21k model&lt;/th&gt; &#xA;   &lt;th&gt;1k model&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-B&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;119M&lt;/td&gt; &#xA;   &lt;td&gt;24.2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitbase/i21k_pt/224&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-B&lt;/td&gt; &#xA;   &lt;td&gt;384x384&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;119M&lt;/td&gt; &#xA;   &lt;td&gt;74.2B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitbase/i21k_i1k/384&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-B&lt;/td&gt; &#xA;   &lt;td&gt;512x512&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;119M&lt;/td&gt; &#xA;   &lt;td&gt;138.5B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitbase/i21k_i1k/512&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-L&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;212M&lt;/td&gt; &#xA;   &lt;td&gt;43.9B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitlarge/i21k_pt/224&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-L&lt;/td&gt; &#xA;   &lt;td&gt;384x384&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;212M&lt;/td&gt; &#xA;   &lt;td&gt;133.1B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitlarge/i21k_i1k/384&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-L&lt;/td&gt; &#xA;   &lt;td&gt;512x512&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;212M&lt;/td&gt; &#xA;   &lt;td&gt;245.4B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitlarge/i21k_i1k/512&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-XL&lt;/td&gt; &#xA;   &lt;td&gt;224x224&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;475M&lt;/td&gt; &#xA;   &lt;td&gt;97.8B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitxlarge/i21k_pt/224&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-XL&lt;/td&gt; &#xA;   &lt;td&gt;384x384&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;475M&lt;/td&gt; &#xA;   &lt;td&gt;293.7B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitxlarge/i21k_i1k/384&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxViT-XL&lt;/td&gt; &#xA;   &lt;td&gt;512x512&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;475M&lt;/td&gt; &#xA;   &lt;td&gt;535.2B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/gresearch/maxvit/ckpts/maxvitxlarge/i21k_i1k/512&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Should you find this repository useful, please consider citing:\&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{tu2022maxvit,&#xA;  title={MaxViT: Multi-Axis Vision Transformer},&#xA;  author={Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao},&#xA;  journal={ECCV},&#xA;  year={2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Other Related Works&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MAXIM: Multi-Axis MLP for Image Processing, CVPR 2022. &lt;a href=&#34;https://arxiv.org/abs/2201.02973&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://github.com/google-research/maxim&#34;&gt;Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CoBEVT: Cooperative Bird&#39;s Eye View Semantic Segmentation with Sparse Transformers, CoRL 2022. &lt;a href=&#34;https://arxiv.org/abs/2207.02202&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://github.com/DerrickXuNu/CoBEVT&#34;&gt;Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Improved Transformer for High-Resolution GANs, NeurIPS 2021. &lt;a href=&#34;https://arxiv.org/abs/2106.07631&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://github.com/google-research/hit-gan&#34;&gt;Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CoAtNet: Marrying Convolution and Attention for All Data Sizes, NeurIPS 2021. &lt;a href=&#34;https://arxiv.org/abs/2106.04803&#34;&gt;Paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;EfficientNetV2: Smaller Models and Faster Training, ICML 2021. &lt;a href=&#34;https://arxiv.org/abs/2104.00298&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://github.com/google/automl/tree/master/efficientnetv2&#34;&gt;Code&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Acknowledgement:&lt;/strong&gt; This repository is built on the &lt;a href=&#34;https://github.com/google/automl&#34;&gt;EfficientNets&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2106.04803&#34;&gt;CoAtNet&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google-research/simclr</title>
    <updated>2022-10-14T01:36:15Z</updated>
    <id>tag:github.com,2022-10-14:/google-research/simclr</id>
    <link href="https://github.com/google-research/simclr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SimCLRv2 - Big Self-Supervised Models are Strong Semi-Supervised Learners&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SimCLR - A Simple Framework for Contrastive Learning of Visual Representations&lt;/h1&gt; &#xA;&lt;p&gt;&lt;span style=&#34;color: red&#34;&gt;&lt;strong&gt;News! &lt;/strong&gt;&lt;/span&gt; We have released a TF2 implementation of SimCLR (along with converted checkpoints in TF2), they are in &lt;a href=&#34;https://raw.githubusercontent.com/google-research/simclr/master/tf2/&#34;&gt;tf2/ folder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span style=&#34;color: red&#34;&gt;&lt;strong&gt;News! &lt;/strong&gt;&lt;/span&gt; Colabs for &lt;a href=&#34;https://arxiv.org/abs/2011.02803&#34;&gt;Intriguing Properties of Contrastive Losses&lt;/a&gt; are added, see &lt;a href=&#34;https://raw.githubusercontent.com/google-research/simclr/master/colabs/intriguing_properties/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;50%&#34; alt=&#34;SimCLR Illustration&#34; src=&#34;https://1.bp.blogspot.com/--vH4PKpE9Yo/Xo4a2BYervI/AAAAAAAAFpM/vaFDwPXOyAokAC8Xh852DzOgEs22NhbXwCLcBGAsYHQ/s1600/image4.gif&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  An illustration of SimCLR (from &#xA; &lt;a href=&#34;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html&#34;&gt;our blog here&lt;/a&gt;). &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Pre-trained models for SimCLRv2&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google-research/simclr/master/colabs/finetuning.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We opensourced total 65 pretrained models here, corresponding to those in Table 1 of the &lt;a href=&#34;https://arxiv.org/abs/2006.10029&#34;&gt;SimCLRv2&lt;/a&gt; paper:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Depth&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Width&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;SK&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Param (M)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;F-T (1%)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;F-T(10%)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;F-T(100%)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Linear eval&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Supervised&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;50&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1X&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;False&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;24&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;57.9&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;68.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;76.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;71.7&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;76.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;50&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1X&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;True&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;35&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;64.5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;72.1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;78.7&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;74.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;78.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;50&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2X&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;False&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;94&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;66.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;73.9&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;75.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;77.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;50&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2X&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;True&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;140&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;70.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;77.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;81.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;77.7&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;101&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1X&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;False&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;43&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;62.1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;71.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;78.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;73.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;78.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;101&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1X&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;True&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;65&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;68.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;75.1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;80.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;76.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;101&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2X&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;False&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;170&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;69.1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;75.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;80.7&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;77.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;78.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;101&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2X&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;True&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;257&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;73.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;78.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;82.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;80.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;152&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1X&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;False&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;58&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;64.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;73.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;74.5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;78.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;152&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1X&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;True&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;89&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;70.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;76.5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;81.3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;77.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;152&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2X&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;False&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;233&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;70.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;76.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;81.1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;77.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;152&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2X&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;True&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;354&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;74.2&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;82.9&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;80.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;152&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3X&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;True&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;795&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;74.9&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;80.1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;80.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;These checkpoints are stored in Google Cloud Storage:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pretrained SimCLRv2 models (with linear eval head): &lt;a href=&#34;https://console.cloud.google.com/storage/browser/simclr-checkpoints/simclrv2/pretrained&#34;&gt;gs://simclr-checkpoints/simclrv2/pretrained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fine-tuned SimCLRv2 models on 1% of labels: &lt;a href=&#34;https://console.cloud.google.com/storage/browser/simclr-checkpoints/simclrv2/finetuned_1pct&#34;&gt;gs://simclr-checkpoints/simclrv2/finetuned_1pct&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fine-tuned SimCLRv2 models on 10% of labels: &lt;a href=&#34;https://console.cloud.google.com/storage/browser/simclr-checkpoints/simclrv2/finetuned_10pct&#34;&gt;gs://simclr-checkpoints/simclrv2/finetuned_10pct&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fine-tuned SimCLRv2 models on 100% of labels: &lt;a href=&#34;https://console.cloud.google.com/storage/browser/simclr-checkpoints/simclrv2/finetuned_100pct&#34;&gt;gs://simclr-checkpoints/simclrv2/finetuned_100pct&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Supervised models with the same architectures: &lt;a href=&#34;https://console.cloud.google.com/storage/browser/simclr-checkpoints/simclrv2/supervised&#34;&gt;gs://simclr-checkpoints/simclrv2/supervised&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The distilled / self-trained models (after fine-tuning) are also provided: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/simclr-checkpoints/simclrv2/distill_1pct&#34;&gt;gs://simclr-checkpoints/simclrv2/distill_1pct&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://console.cloud.google.com/storage/browser/simclr-checkpoints/simclrv2/distill_10pct&#34;&gt;gs://simclr-checkpoints/simclrv2/distill_10pct&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also provide examples on how to use the checkpoints in &lt;code&gt;colabs/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Pre-trained models for SimCLRv1&lt;/h2&gt; &#xA;&lt;p&gt;The pre-trained models (base network with linear classifier layer) can be found below. Note that for these SimCLRv1 checkpoints, the projection head is not available.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model checkpoint and hub-module&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet Top-1&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.cloud.google.com/simclr-gcs/checkpoints/ResNet50_1x.zip&#34;&gt;ResNet50 (1x)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;69.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.cloud.google.com/simclr-gcs/checkpoints/ResNet50_2x.zip&#34;&gt;ResNet50 (2x)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;74.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.cloud.google.com/simclr-gcs/checkpoints/ResNet50_4x.zip&#34;&gt;ResNet50 (4x)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;76.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Additional SimCLRv1 checkpoints are available: &lt;a href=&#34;https://console.cloud.google.com/storage/browser/simclr-checkpoints/simclrv1&#34;&gt;gs://simclr-checkpoints/simclrv1&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A note on the signatures of the TensorFlow Hub module: &lt;code&gt;default&lt;/code&gt; is the representation output of the base network; &lt;code&gt;logits_sup&lt;/code&gt; is the supervised classification logits for ImageNet 1000 categories. Others (e.g. &lt;code&gt;initial_max_pool&lt;/code&gt;, &lt;code&gt;block_group1&lt;/code&gt;) are middle layers of ResNet; refer to resnet.py for the specifics. See this &lt;a href=&#34;https://www.tensorflow.org/hub/tf1_hub_module&#34;&gt;tutorial&lt;/a&gt; for additional information regarding use of TensorFlow Hub modules.&lt;/p&gt; &#xA;&lt;h2&gt;Enviroment setup&lt;/h2&gt; &#xA;&lt;p&gt;Our models are trained with TPUs. It is recommended to run distributed training with TPUs when using our code for pretraining.&lt;/p&gt; &#xA;&lt;p&gt;Our code can also run on a &lt;em&gt;single&lt;/em&gt; GPU. It does not support multi-GPUs, for reasons such as global BatchNorm and contrastive loss across cores.&lt;/p&gt; &#xA;&lt;p&gt;The code is compatible with both TensorFlow v1 and v2. See requirements.txt for all prerequisites, and you can also install them using the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pretraining&lt;/h2&gt; &#xA;&lt;p&gt;To pretrain the model on CIFAR-10 with a &lt;em&gt;single&lt;/em&gt; GPU, try the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --train_mode=pretrain \&#xA;  --train_batch_size=512 --train_epochs=1000 \&#xA;  --learning_rate=1.0 --weight_decay=1e-4 --temperature=0.5 \&#xA;  --dataset=cifar10 --image_size=32 --eval_split=test --resnet_depth=18 \&#xA;  --use_blur=False --color_jitter_strength=0.5 \&#xA;  --model_dir=/tmp/simclr_test --use_tpu=False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To pretrain the model on ImageNet with Cloud TPUs, first check out the &lt;a href=&#34;https://cloud.google.com/tpu/docs/tutorials/mnist&#34;&gt;Google Cloud TPU tutorial&lt;/a&gt; for basic information on how to use Google Cloud TPUs.&lt;/p&gt; &#xA;&lt;p&gt;Once you have created virtual machine with Cloud TPUs, and pre-downloaded the ImageNet data for &lt;a href=&#34;https://www.tensorflow.org/datasets/catalog/imagenet2012&#34;&gt;tensorflow_datasets&lt;/a&gt;, please set the following enviroment variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;TPU_NAME=&amp;lt;tpu-name&amp;gt;&#xA;STORAGE_BUCKET=gs://&amp;lt;storage-bucket&amp;gt;&#xA;DATA_DIR=$STORAGE_BUCKET/&amp;lt;path-to-tensorflow-dataset&amp;gt;&#xA;MODEL_DIR=$STORAGE_BUCKET/&amp;lt;path-to-store-checkpoints&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following command can be used to pretrain a ResNet-50 on ImageNet (which reflects the default hyperparameters in our paper):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --train_mode=pretrain \&#xA;  --train_batch_size=4096 --train_epochs=100 --temperature=0.1 \&#xA;  --learning_rate=0.075 --learning_rate_scaling=sqrt --weight_decay=1e-4 \&#xA;  --dataset=imagenet2012 --image_size=224 --eval_split=validation \&#xA;  --data_dir=$DATA_DIR --model_dir=$MODEL_DIR \&#xA;  --use_tpu=True --tpu_name=$TPU_NAME --train_summary_steps=0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A batch size of 4096 requires at least 32 TPUs. 100 epochs takes around 6 hours with 32 TPU v3s. Note that learning rate of 0.3 with &lt;code&gt;learning_rate_scaling=linear&lt;/code&gt; is equivalent to that of 0.075 with &lt;code&gt;learning_rate_scaling=sqrt&lt;/code&gt; when the batch size is 4096. However, using sqrt scaling allows it to train better when smaller batch size is used.&lt;/p&gt; &#xA;&lt;h2&gt;Finetuning the linear head (linear eval)&lt;/h2&gt; &#xA;&lt;p&gt;To fine-tune a linear head (with a single GPU), try the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --mode=train_then_eval --train_mode=finetune \&#xA;  --fine_tune_after_block=4 --zero_init_logits_layer=True \&#xA;  --variable_schema=&#39;(?!global_step|(?:.*/|^)Momentum|head)&#39; \&#xA;  --global_bn=False --optimizer=momentum --learning_rate=0.1 --weight_decay=0.0 \&#xA;  --train_epochs=100 --train_batch_size=512 --warmup_epochs=0 \&#xA;  --dataset=cifar10 --image_size=32 --eval_split=test --resnet_depth=18 \&#xA;  --checkpoint=/tmp/simclr_test --model_dir=/tmp/simclr_test_ft --use_tpu=False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can check the results using tensorboard, such as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m tensorboard.main --logdir=/tmp/simclr_test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a reference, the above runs on CIFAR-10 should give you around 91% accuracy, though it can be further optimized.&lt;/p&gt; &#xA;&lt;p&gt;For fine-tuning a linear head on ImageNet using Cloud TPUs, first set the &lt;code&gt;CHKPT_DIR&lt;/code&gt; to pretrained model dir and set a new &lt;code&gt;MODEL_DIR&lt;/code&gt;, then use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --mode=train_then_eval --train_mode=finetune \&#xA;  --fine_tune_after_block=4 --zero_init_logits_layer=True \&#xA;  --variable_schema=&#39;(?!global_step|(?:.*/|^)Momentum|head)&#39; \&#xA;  --global_bn=False --optimizer=momentum --learning_rate=0.1 --weight_decay=1e-6 \&#xA;  --train_epochs=90 --train_batch_size=4096 --warmup_epochs=0 \&#xA;  --dataset=imagenet2012 --image_size=224 --eval_split=validation \&#xA;  --data_dir=$DATA_DIR --model_dir=$MODEL_DIR --checkpoint=$CHKPT_DIR \&#xA;  --use_tpu=True --tpu_name=$TPU_NAME --train_summary_steps=0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a reference, the above runs on ImageNet should give you around 64.5% accuracy.&lt;/p&gt; &#xA;&lt;h2&gt;Semi-supervised learning and fine-tuning the whole network&lt;/h2&gt; &#xA;&lt;p&gt;You can access 1% and 10% ImageNet subsets used for semi-supervised learning via &lt;a href=&#34;https://www.tensorflow.org/datasets/catalog/imagenet2012_subset&#34;&gt;tensorflow datasets&lt;/a&gt;: simply set &lt;code&gt;dataset=imagenet2012_subset/1pct&lt;/code&gt; and &lt;code&gt;dataset=imagenet2012_subset/10pct&lt;/code&gt; in the command line for fine-tuning on these subsets.&lt;/p&gt; &#xA;&lt;p&gt;You can also find image IDs of these subsets in &lt;code&gt;imagenet_subsets/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To fine-tune the whole network on ImageNet (1% of labels), refer to the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --mode=train_then_eval --train_mode=finetune \&#xA;  --fine_tune_after_block=-1 --zero_init_logits_layer=True \&#xA;  --variable_schema=&#39;(?!global_step|(?:.*/|^)Momentum|head_supervised)&#39; \&#xA;  --global_bn=True --optimizer=lars --learning_rate=0.005 \&#xA;  --learning_rate_scaling=sqrt --weight_decay=0 \&#xA;  --train_epochs=60 --train_batch_size=1024 --warmup_epochs=0 \&#xA;  --dataset=imagenet2012_subset/1pct --image_size=224 --eval_split=validation \&#xA;  --data_dir=$DATA_DIR --model_dir=$MODEL_DIR --checkpoint=$CHKPT_DIR \&#xA;  --use_tpu=True --tpu_name=$TPU_NAME --train_summary_steps=0 \&#xA;  --num_proj_layers=3 --ft_proj_selector=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set the &lt;code&gt;checkpoint&lt;/code&gt; to those that are only pre-trained but not fine-tuned. Given that SimCLRv1 checkpoints do not contain projection head, it is recommended to run with SimCLRv2 checkpoints (you can still run with SimCLRv1 checkpoints, but &lt;code&gt;variable_schema&lt;/code&gt; needs to exclude &lt;code&gt;head&lt;/code&gt;). The &lt;code&gt;num_proj_layers&lt;/code&gt; and &lt;code&gt;ft_proj_selector&lt;/code&gt; need to be adjusted accordingly following SimCLRv2 paper to obtain best performances.&lt;/p&gt; &#xA;&lt;h2&gt;Other resources&lt;/h2&gt; &#xA;&lt;h3&gt;Model conversion to Pytorch format&lt;/h3&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://github.com/tonylins/simclr-converter&#34;&gt;repo&lt;/a&gt; provides a solution for converting the pretrained SimCLRv1 Tensorflow checkpoints into Pytorch ones.&lt;/p&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://github.com/Separius/SimCLRv2-Pytorch&#34;&gt;repo&lt;/a&gt; provides a solution for converting the pretrained SimCLRv2 Tensorflow checkpoints into Pytorch ones.&lt;/p&gt; &#xA;&lt;h3&gt;Other &lt;em&gt;non-offical&lt;/em&gt; / &lt;em&gt;unverified&lt;/em&gt; implementations&lt;/h3&gt; &#xA;&lt;p&gt;(Feel free to share your implementation by creating an issue)&lt;/p&gt; &#xA;&lt;p&gt;Implementations in PyTorch:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/leftthomas/SimCLR&#34;&gt;leftthomas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AndrewAtanov/simclr-pytorch&#34;&gt;AndrewAtanov&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ae-foster/pytorch-simclr&#34;&gt;ae-foster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Spijkervet/SimCLR&#34;&gt;Spijkervet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning-bolts.readthedocs.io/en/latest/self_supervised_models.html#simclr&#34;&gt;williamFalcon&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Implementations in Tensorflow 2 / Keras (official TF2 implementation was added in &lt;a href=&#34;https://raw.githubusercontent.com/google-research/simclr/master/tf2/&#34;&gt;tf2/ folder&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sayakpaul/SimCLR-in-TensorFlow-2&#34;&gt;sayakpaul&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mwdhont/SimCLRv1-keras-tensorflow&#34;&gt;mwdhont&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Known issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Batch size&lt;/strong&gt;: original results of SimCLR were tuned under a large batch size (i.e. 4096), which leads to suboptimal results when training using a smaller batch size. However, with a good set of hyper-parameters (mainly learning rate, temperature, projection head depth), small batch sizes can yield results that are on par with large batch sizes (e.g., see Table 2 in &lt;a href=&#34;https://arxiv.org/pdf/2011.02803.pdf&#34;&gt;this paper&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pretrained models / Checkpoints&lt;/strong&gt;: SimCLRv1 and SimCLRv2 are pretrained with different weight decays, so the pretrained models from the two versions have very different weight norm scales (convolutional weights in SimCLRv1 ResNet-50 are on average 16.8X of that in SimCLRv2). For fine-tuning the pretrained models from both versions, it is fine if you use an LARS optimizer, but it requires very different hyperparameters (e.g. learning rate, weight decay) if you use the momentum optimizer. So for the latter case, you may want to either search for very different hparams according to which version used, or re-scale th weight (i.e. conv &lt;code&gt;kernel&lt;/code&gt; parameters of &lt;code&gt;base_model&lt;/code&gt; in the checkpoints) to make sure they&#39;re roughly in the same scale.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Cite&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.05709&#34;&gt;SimCLR paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{chen2020simple,&#xA;  title={A Simple Framework for Contrastive Learning of Visual Representations},&#xA;  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},&#xA;  journal={arXiv preprint arXiv:2002.05709},&#xA;  year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.10029&#34;&gt;SimCLRv2 paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{chen2020big,&#xA;  title={Big Self-Supervised Models are Strong Semi-Supervised Learners},&#xA;  author={Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},&#xA;  journal={arXiv preprint arXiv:2006.10029},&#xA;  year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is not an official Google product.&lt;/p&gt;</summary>
  </entry>
</feed>