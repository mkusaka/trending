<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-01T01:39:25Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>beyondguo/LLM-Tuning</title>
    <updated>2023-07-01T01:39:25Z</updated>
    <id>tag:github.com,2023-07-01:/beyondguo/LLM-Tuning</id>
    <link href="https://github.com/beyondguo/LLM-Tuning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tuning LLMs with no tears💦, sharing LLM-tools with love❤️.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Tuning LLMs with no tears.&lt;/h2&gt; &#xA;&lt;p&gt;目前支持：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;清华 &lt;a href=&#34;https://huggingface.co/THUDM/chatglm2-6b&#34;&gt;ChatGLM2-6B&lt;/a&gt; 的 LoRA 微调 (New!🔥)&lt;/li&gt; &#xA; &lt;li&gt;百川智能 &lt;a href=&#34;https://huggingface.co/baichuan-inc/baichuan-7B&#34;&gt;baichuan-7B&lt;/a&gt; 的 LoRA 微调&lt;/li&gt; &#xA; &lt;li&gt;清华 &lt;a href=&#34;https://huggingface.co/THUDM/chatglm-6b&#34;&gt;ChatGLM-6B&lt;/a&gt; 的 LoRA 微调&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;两行代码开启训练：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;数据集分词预处理：&lt;code&gt;sh tokenize.sh&lt;/code&gt;，对比不同的 LLM，需在 tokenize.sh 文件里切换 model_checkpoint 参数&lt;/li&gt; &#xA; &lt;li&gt;开启 LoRA 微调：&lt;code&gt;sh train.sh&lt;/code&gt;，对于不同的 LLM，需切换不同的 python 文件来执行： &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ChatGLM-6B 应使用 &lt;code&gt;chatglm_lora_tuning.py&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ChatGLM2-6B 应使用 &lt;code&gt;chatglm2_lora_tuning.py&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;baichuan-7B 应使用 &lt;code&gt;baichuan_lora_tuning.py&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;可复现的小项目：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/beyondguo/LLM-Tuning/master/projects/ChatBaichuan-HC3/&#34;&gt;&lt;strong&gt;ChatBaichuan&lt;/strong&gt;：基于 HC3 数据集让 百川大模型（baichuan-7B）有对话能力！&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/beyondguo/LLM-Tuning/master/projects/RulaiGPT/&#34;&gt;【娱乐向】&lt;strong&gt;RulaiGPT&lt;/strong&gt;：如来~诶，它真来了吗？如~来~（拍桌！）&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;：&lt;br&gt; &lt;code&gt;pip install transformers datasets accelerate sentencepiece tensorboard peft&lt;/code&gt;&lt;br&gt; 目前测试的环境为：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;- Python 3.9.16&#xA;- torch, Version: 2.0.1&#xA;- transformers, Version: 4.29.1&#xA;- datasets, Version: 2.12.0&#xA;- accelerate, Version: 0.19.0&#xA;- peft, Version: 0.3.0&#xA;- sentencepiece, Version: 0.1.99&#xA;- tensorboard, Version: 2.13.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;教程：&lt;/h2&gt; &#xA;&lt;p&gt;下面的教程以及代码使用 &lt;code&gt;ChatGLM-6B&lt;/code&gt; 作为例子，如果更换其他模型，可能需要略微修改具体文件代码。&lt;/p&gt; &#xA;&lt;h3&gt;1. 指令微调数据准备 Instruction Data Preparation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;原始文件的准备&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;指令微调数据一般有输入和输出两部分，输入是特定的content加上instruction，这里我们将二者直接拼在一起，不单独区分；输出则是希望模型的回答。 我们统一使用&lt;code&gt;json&lt;/code&gt;的格式在整理数据，可以自定义输出输出的字段名，例如下面的例子中我使用的是&lt;code&gt;q&lt;/code&gt;和&lt;code&gt;a&lt;/code&gt;代表模型的输入和输出：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#34;q&#34;: &#34;请计算：39 * 0 = 什么？&#34;, &#34;a&#34;: &#34;这是简单的乘法运算，39乘以0得到的是0&#34;}&#xA;{&#34;q&#34;: &#34;题目：51/186的答案是什么?&#34;, &#34;a&#34;: &#34;这是简单的除法运算，51除以186大概为0.274&#34;}&#xA;{&#34;q&#34;: &#34;鹿妈妈买了24个苹果，她想平均分给她的3只小鹿吃，每只小鹿可以分到几个苹果？&#34;, &#34;a&#34;: &#34;鹿妈妈买了24个苹果，平均分给3只小鹿吃，那么每只小鹿可以分到的苹果数就是总苹果数除以小鹿的只数。\n24÷3=8\n每只小鹿可以分到8个苹果。所以，答案是每只小鹿可以分到8个苹果。&#34;}&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;整理好数据后，保存为&lt;code&gt;.json&lt;/code&gt;或者&lt;code&gt;.jsonl&lt;/code&gt;文件，然后放入目录中的&lt;code&gt;data/&lt;/code&gt;文件夹中。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;对数据集进行分词&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;为了避免每次训练的时候都要重新对数据集分词，我们先分好词形成特征后保存成可直接用于训练的数据集。&lt;/p&gt; &#xA;&lt;p&gt;例如，&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;我们的原始指令微调文件为：&lt;code&gt;data/&lt;/code&gt; 文件夹下的 &lt;code&gt;simple_math_4op.json&lt;/code&gt; 文件&lt;/li&gt; &#xA; &lt;li&gt;输入字段为&lt;code&gt;q&lt;/code&gt;，输出字段为&lt;code&gt;a&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;希望经过 tokenize 之后保存到 &lt;code&gt;data/tokenized_data/&lt;/code&gt; 下名为 &lt;code&gt;simple_math_4op&lt;/code&gt; 的文件夹中&lt;/li&gt; &#xA; &lt;li&gt;设定文本最大程度为 2000&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;则我们可以直接使用下面这段命令(即&lt;code&gt;tokenize.sh&lt;/code&gt;文件)进行处理：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1 python tokenize_dataset_rows.py \&#xA;    --model_checkpoint THUDM/chatglm-6b \&#xA;    --input_file simple_math_4op.json \&#xA;    --prompt_key q \&#xA;    --target_key a \&#xA;    --save_name simple_math_4op \&#xA;    --max_seq_length 2000 \&#xA;    --skip_overlength False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;处理完毕之后，我们会在 &lt;code&gt;data/tokenized_data/&lt;/code&gt; 下发现名为 &lt;code&gt;simple_math_4op&lt;/code&gt; 的文件夹，这就是下一步中我们可以直接用于训练的数据。&lt;/p&gt; &#xA;&lt;h3&gt;2. 使用 &lt;code&gt;LoRA&lt;/code&gt; 微调&lt;/h3&gt; &#xA;&lt;p&gt;得到 tokenize 之后的数据集，就可以直接运行 &lt;code&gt;chatglm_lora_tuning.py&lt;/code&gt; 来训练 LoRA 模型了，具体可设置的主要参数包括：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;tokenized_dataset&lt;/code&gt;, 分词后的数据集，即在 data/tokenized_data/ 地址下的文件夹名称&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;lora_rank&lt;/code&gt;, 设置 LoRA 的秩，推荐为4或8，显存够的话使用8&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;per_device_train_batch_size&lt;/code&gt;, 每块 GPU 上的 batch size&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gradient_accumulation_steps&lt;/code&gt;, 梯度累加，可以在不提升显存占用的情况下增大 batch size&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max_steps&lt;/code&gt;, 训练步数&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;save_steps&lt;/code&gt;, 多少步保存一次&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;save_total_limit&lt;/code&gt;, 保存多少个checkpoint&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;logging_steps&lt;/code&gt;, 多少步打印一次训练情况(loss, lr, etc.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output_dir&lt;/code&gt;, 模型文件保存地址&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;例如我们的数据集为 simple_math_4op，希望保存到 weights/simple_math_4op ，则执行下面命令(即&lt;code&gt;train.sh&lt;/code&gt;文件)：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=2,3 python chatglm_lora_tuning.py \&#xA;    --tokenized_dataset simple_math_4op \&#xA;    --lora_rank 8 \&#xA;    --per_device_train_batch_size 10 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --max_steps 100000 \&#xA;    --save_steps 200 \&#xA;    --save_total_limit 2 \&#xA;    --learning_rate 1e-4 \&#xA;    --fp16 \&#xA;    --remove_unused_columns false \&#xA;    --logging_steps 50 \&#xA;    --output_dir weights/simple_math_4op&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;训练完之后，可以在 output_dir 中找到 LoRA 的相关模型权重，主要是&lt;code&gt;adapter_model.bin&lt;/code&gt;和&lt;code&gt;adapter_config.json&lt;/code&gt;两个文件。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;如何查看 tensorboard：&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;在 output_dir 中找到 runs 文件夹，复制其中日期最大的文件夹的地址，假设为 &lt;code&gt;your_log_path&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;执行 &lt;code&gt;tensorboard --logdir your_log_path&lt;/code&gt; 命令，就会在 &lt;a href=&#34;http://localhost:6006/&#34;&gt;http://localhost:6006/&lt;/a&gt; 上开启tensorboard&lt;/li&gt; &#xA; &lt;li&gt;如果是在服务器上开启，则还需要做端口映射到本地。推荐使用 VSCode 在服务器上写代码，可以自动帮你进行端口映射。&lt;/li&gt; &#xA; &lt;li&gt;如果要自己手动进行端口映射，具体方式是在使用 ssh 登录时，后面加上 &lt;code&gt;-L 6006:127.0.0.1:6006&lt;/code&gt; 参数，将服务器端的6006端口映射到本地的6006端口。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. 拿走 LoRA 小小的文件，到你本地的大模型上加载并推理&lt;/h3&gt; &#xA;&lt;p&gt;我们可以把上面的 output_dir 打包带走，假设文件夹为 &lt;code&gt;weights/simple_math_4op&lt;/code&gt;， 其中（至少）包含 &lt;code&gt;adapter_model.bin&lt;/code&gt; 和 &lt;code&gt;adapter_config.json&lt;/code&gt; 两个文件，则我们可以用下面的方式直接加载，并推理&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from peft import PeftModel&#xA;from transformers import AutoTokenizer, AutoModel&#xA;import torch&#xA;&#xA;device = torch.device(1)&#xA;# 加载原始 LLM&#xA;model_path = &#34;THUDM/chatglm-6b&#34;&#xA;model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().to(device)&#xA;tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)&#xA;model.chat(tokenizer, &#34;你好&#34;, history=[])&#xA;&#xA;&#xA;# 给原始 LLM 安装上你的 LoRA tool&#xA;model = PeftModel.from_pretrained(model, &#34;weights/simple_math_4op&#34;).half()&#xA;model.chat(tokenizer, &#34;你好&#34;, history=[])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;理论上，可以通过多次执行 &lt;code&gt;model = PeftModel.from_pretrained(model, &#34;weights/simple_math_4op&#34;).half()&lt;/code&gt; 的方式，加载多个 LoRA 模型，从而混合不同Tool的能力，但实际测试的时候，由于暂时还不支持设置不同 LoRA weights的权重，往往效果不太好，存在覆盖或者遗忘的情况。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Acknowledgement&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;首先最感谢的是 🤗Huggingface 团队开源的 &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;peft&lt;/a&gt; 工具包，懂的都懂！&lt;/li&gt; &#xA; &lt;li&gt;ChatGLM 的 LoRA 微调代码主要基于 &lt;a href=&#34;https://github.com/mymusise/ChatGLM-Tuning&#34;&gt;ChatGLM-Tuning&lt;/a&gt; 项目中的 LoRA 微调部分修改而来；&lt;/li&gt; &#xA; &lt;li&gt;baichuan-7B 微调部分，参考了 &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Efficient-Tuning/issues/43&#34;&gt;LLaMA-Efficient-Tuning&lt;/a&gt; 项目中的解决方案；&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;对这些优秀开源项目表示感谢！&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>commaai/commavq</title>
    <updated>2023-07-01T01:39:25Z</updated>
    <id>tag:github.com,2023-07-01:/commaai/commavq</id>
    <link href="https://github.com/commaai/commavq" rel="alternate"></link>
    <summary type="html">&lt;p&gt;commaVQ is a dataset of compressed driving video&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;commavq&lt;/h1&gt; &#xA;&lt;p&gt;commaVQ is a dataset of 100,000 heavily compressed driving videos for Machine Learning research. A heavily compressed driving video like this is useful to experiment with GPT-like video prediction models. This repo includes an encoder/decoder and an example of a video prediction model.&lt;/p&gt; &#xA;&lt;h2&gt;2x$500 Challenges!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Get 1.92 cross entropy loss or less in the val set and in our private val set (using &lt;code&gt;./notebooks/eval.ipynb&lt;/code&gt;). gpt2m trained on a larger dataset gets 2.02 cross entropy loss.&lt;/li&gt; &#xA; &lt;li&gt;Make gpt2m.onnx run at 0.9 sec/frame or less on a consumer GPU (e.g. NVIDIA 3090) without degredation in cross entropy loss. The current implementation runs at 1.5 sec/frame with kvcaching and float16.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;A VQ-VAE [1,2] was used to heavily compress each frame into 128 &#34;tokens&#34; of 10 bits each. Each entry of the dataset is a &#34;segment&#34; of compressed driving video, i.e. 1min of frames at 20 FPS. Each file is of shape 1200x8x16 and saved as int16.&lt;/p&gt; &#xA;&lt;p&gt;Note that the compressor is extremely lossy on purpose. It makes the dataset smaller and easy to play with (train GPT with large context size, fast autoregressive generation, etc.). We might extend the dataset to a less lossy version when we see fit.&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using huggingface datasets&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;from datasets import load_dataset&#xA;num_proc = 40 # CPUs go brrrr&#xA;ds = load_dataset(&#39;commaai/commavq&#39;, num_proc=num_proc)&#xA;tokens = np.load(ds[&#39;0&#39;][0][&#39;path&#39;]) # first segment from the first data shard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Manually download from huggingface datasets repository: &lt;a href=&#34;https://huggingface.co/datasets/commaai/commavq&#34;&gt;https://huggingface.co/datasets/commaai/commavq&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;From Academic Torrents (soon)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;In ./models/ you will find 3 Neural Networks saved in the onnx format&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;./models/encoder.onnx&lt;/code&gt;: is the encoder used to compress the frames&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;./models/decoder.onnx&lt;/code&gt;: is the decoder used to decompress the frames&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;./models/gtp2m.onnx&lt;/code&gt;: a 300M parameter GPT trained on a larger version of this dataset&lt;/li&gt; &#xA; &lt;li&gt;(experimental) &lt;code&gt;./models/temporal_decoder.onnx&lt;/code&gt;: a temporal decoder which is a stateful version of the vanilla decoder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Checkout &lt;code&gt;./notebooks/encode.ipynb&lt;/code&gt; and &lt;code&gt;./notebooks/decode.ipynb&lt;/code&gt; for an example of how to visualize the dataset using a segment of driving video from &lt;a href=&#34;https://blog.comma.ai/taco-bell/&#34;&gt;comma&#39;s drive to Taco Bell&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Checkout &lt;code&gt;./notebooks/gpt.ipynb&lt;/code&gt; for an example of how to use a pretrained GPT model to imagine future frames.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/commaai/commavq/assets/29985433/91894bf7-592b-4204-b3f2-3e805984045c&#34;&gt;https://github.com/commaai/commavq/assets/29985433/91894bf7-592b-4204-b3f2-3e805984045c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/commaai/commavq/assets/29985433/3a799ac8-781e-461c-bf14-c15cea42b985&#34;&gt;https://github.com/commaai/commavq/assets/29985433/3a799ac8-781e-461c-bf14-c15cea42b985&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/commaai/commavq/assets/29985433/f6f7699b-b6cb-4f9c-80c9-8e00d75fbfae&#34;&gt;https://github.com/commaai/commavq/assets/29985433/f6f7699b-b6cb-4f9c-80c9-8e00d75fbfae&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;[1] Van Den Oord, Aaron, and Oriol Vinyals. &#34;Neural discrete representation learning.&#34; Advances in neural information processing systems 30 (2017).&lt;/p&gt; &#xA;&lt;p&gt;[2] Esser, Patrick, Robin Rombach, and Bjorn Ommer. &#34;Taming transformers for high-resolution image synthesis.&#34; Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.&lt;/p&gt;</summary>
  </entry>
</feed>