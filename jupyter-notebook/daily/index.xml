<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-10T01:34:24Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AnswerDotAI/fsdp_qlora</title>
    <updated>2024-03-10T01:34:24Z</updated>
    <id>tag:github.com,2024-03-10:/AnswerDotAI/fsdp_qlora</id>
    <link href="https://github.com/AnswerDotAI/fsdp_qlora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Training LLMs with QLoRA + FSDP&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;fsdp_qlora&lt;/h1&gt; &#xA;&lt;p&gt;Training LLMs with Quantized LoRA + FSDP.&lt;/p&gt; &#xA;&lt;p&gt;Read our &lt;a href=&#34;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html&#34;&gt;announcement blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You should treat this script as an alpha/preview release. If youâ€™re not comfortable with testing and debugging models, weâ€™d suggest holding off for a few months while the community more fully tests the approach.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The following steps should work (tested on Cuda 11.7, 11.8 and 12.1):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone &lt;a href=&#34;https://github.com/AnswerDotAI/fsdp_qlora&#34;&gt;https://github.com/AnswerDotAI/fsdp_qlora&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install llama-recipes fastcore --extra-index-url https://download.pytorch.org/whl/test/cu118&lt;/code&gt; as an easy way to get most dependencies (replace 118 with your desired Cuda version)&lt;/li&gt; &#xA; &lt;li&gt;Install bitsandbytes &lt;code&gt;pip install bitsandbytes&amp;gt;=0.43.0&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;huggingface-cli login&lt;/code&gt; (to access Llama 2)&lt;/li&gt; &#xA; &lt;li&gt;Optional Libraries: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;HQQ quantization: follow the HQQ installation &lt;a href=&#34;https://github.com/mobiusml/hqq?tab=readme-ov-file#installation&#34;&gt;instructions&lt;/a&gt;. Our training script uses &lt;code&gt;HQQBackend.ATEN_BACKPROP&lt;/code&gt;, so also make sure to build the custom kernels &lt;code&gt;cd hqq/kernels &amp;amp;&amp;amp; python setup_cuda.py install&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Weights and Biases logging: &lt;code&gt;pip install wandb&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/blog/pytorch2-2/&#34;&gt;Pytorch &amp;gt;= 2.2&lt;/a&gt; is recommended to make use of the native flash-attention 2 kernel.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Finetune Llama-2 70B on Dual 24GB GPUs&lt;/h2&gt; &#xA;&lt;p&gt;Once installed, run &lt;code&gt;cd fsdp_qlora&lt;/code&gt; and then run the following command to begin finetuning Llama-2 70B on &lt;a href=&#34;https://huggingface.co/datasets/yahma/alpaca-cleaned&#34;&gt;Alpaca&lt;/a&gt; at a maximum sequence length of 2048 tokens.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train.py \&#xA;--model_name meta-llama/Llama-2-70b-hf \&#xA;--batch_size 2 \&#xA;--context_length 2048 \&#xA;--precision bf16 \&#xA;--train_type qlora \&#xA;--use_gradient_checkpointing true \&#xA;--use_cpu_offload true \&#xA;--dataset alpaca \&#xA;--reentrant_checkpointing true \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training Options&lt;/h2&gt; &#xA;&lt;p&gt;For quantization we support HQQ and bitsandbytes. We&#39;re currently doing benchmarking to help you decide which to use. If you do use bitsandbytes, be sure to pass &lt;code&gt;--reentrant_checkpointing True&lt;/code&gt; to avoid triggering a bug in bitsandbytes which results in high memory usage (a fix is in progress).&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;--train_type full&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Full params fine-tuning.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=4,5 # optionally set devices&#xA;python train.py \&#xA;--world_size 2 \ # optional, on a single machine will be set automatically&#xA;--master_port 12356 \ # optional, defaults to 12355&#xA;--model_name meta-llama/Llama-2-7b-hf \&#xA;--gradient_accumulation_steps 4 \&#xA;--batch_size 8 \&#xA;--context_length 512 \&#xA;--precision bf16 \&#xA;--train_type full \&#xA;--use_gradient_checkpointing true \&#xA;--use_cpu_offload false \&#xA;--use_activation_cpu_offload false \&#xA;--log_to wandb \&#xA;--dataset alpaca \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;--train_type lora&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;LoRA fine-tuning using HF PEFT library.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- --train_type full \&#xA;+ --train_type lora \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;--train_type custom_lora&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;LoRA fine-tuning using a custom LoRA module.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- --train_type full \&#xA;+ --train_type custom_lora \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;--train_type qlora&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;4-bit quantized LoRA fine-tuning using bitsanbytes Linear4bit layer with NF4 quantization and HF PEFT library.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- --train_type full \&#xA;+ --train_type qlora \&#xA;+ --reentrant_checkpointing true \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;--train_type custom_qlora&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;4-bit quantized LoRA fine-tuning using bitsanbytes Linear4bit layer with NF4 quantization and a custom LoRA module.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- --train_type full \&#xA;+ --train_type custom_qlora \&#xA;+ --reentrant_checkpointing true \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;--train_type hqq_lora&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;4-bit quantized LoRA fine-tuning using HQQ library and a custom LoRA module.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- --train_type full \&#xA;+ --train_type hqq_lora \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Low Memory Loading&lt;/h2&gt; &#xA;&lt;p&gt;During quantized LoRA training we use a custom quantization and loading code to avoid loading the entire model into GPU memory before sharding it across GPUs. This is the default behavior of our training script when any of the following training options &lt;code&gt;&#34;qlora&#34;, &#34;custom_qlora&#34;, &#34;hqq_lora&#34;&lt;/code&gt; is used. Other training options are already optimized for low memory loading to their best extent.&lt;/p&gt; &#xA;&lt;p&gt;We load the weights iteratively, quantize them on the GPU and place them back to CPU or meta device (based on their rank) concurrently a few layers at a time. We do this across all GPUs to initialize the quantization parameters, such as zero and scale, while using &lt;code&gt;sync_module_states=True&lt;/code&gt; to sync the model parameters and buffers across all GPUs during FSDP initialization.&lt;/p&gt; &#xA;&lt;h2&gt;Mixed Precision Training&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;--precision bf16&lt;/code&gt; (pure bfloat16)&lt;/h3&gt; &#xA;&lt;p&gt;This will cast all the model parameters to &lt;code&gt;torch.bfloat16&lt;/code&gt; before training and won&#39;t use FSDP mixed precision. As a result, sharded and unsharded params will be stored in bf16, forward and backward passes will be done in bf16, and gradient reduction and updates will be done in bf16.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;--precision fp32&lt;/code&gt; (pure float32)&lt;/h3&gt; &#xA;&lt;p&gt;This will cast all the model parameters to &lt;code&gt;torch.float32&lt;/code&gt; before training and won&#39;t use FSDP mixed precision. As a result, sharded and unsharded params will be stored in fp32, forward and backward passes will be done in fp32, and gradient reduction and updates will be done in fp32.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;--precision mp_fp16_autocast&lt;/code&gt; (mixed float16 with autocast)&lt;/h3&gt; &#xA;&lt;p&gt;This will cast all the model parameters to &lt;code&gt;torch.float32&lt;/code&gt; before training and will use FSDP mixed precision with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mp_policy = MixedPrecision(param_dtype=torch.float32, reduce_dtype=torch.float32, buffer_dtype=torch.float32)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a results, sharded and unsharded params will be stored in fp32. It will use &lt;code&gt;autocast(torch.float16)&lt;/code&gt; for forward and backward passes, and &lt;code&gt;autocast(torch.float16)&lt;/code&gt; for gradient reduction and updates.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;--precision mp_bf16_autocast&lt;/code&gt; (mixed bfloat16 with autocast)&lt;/h3&gt; &#xA;&lt;p&gt;This will cast all the model parameters to &lt;code&gt;torch.float32&lt;/code&gt; before training and will use FSDP mixed precision with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mp_policy = MixedPrecision(param_dtype=torch.float32, reduce_dtype=torch.float32, buffer_dtype=torch.float32)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a results, sharded and unsharded params will be stored in fp32. It will use &lt;code&gt;autocast(torch.bfloat16)&lt;/code&gt; for forward and backward passes, and &lt;code&gt;autocast(torch.bfloat16)&lt;/code&gt; for gradient reduction and updates.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;--precision mp_bf16_buffers_autocast&lt;/code&gt; (bfloat16 params and float32 buffers with autocast)&lt;/h3&gt; &#xA;&lt;p&gt;This will cast all the model parameters to &lt;code&gt;torch.bfloat16&lt;/code&gt; before training but will keep the buffers in &lt;code&gt;torch.float32&lt;/code&gt; and will use FSDP mixed precision with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mp_policy = MixedPrecision(param_dtype=torch.bfloat16, reduce_dtype=torch.bfloat16, buffer_dtype=torch.float32)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a results, sharded and unsharded params will be stored in bf16. It will use &lt;code&gt;autocast(torch.bfloat16)&lt;/code&gt; for forward and backward passes, and &lt;code&gt;autocast(torch.bfloat16)&lt;/code&gt; for gradient reduction and updates. Buffers and only &lt;a href=&#34;https://pytorch.org/docs/stable/amp.html#cuda-ops-that-can-autocast-to-float16&#34;&gt;eligible operations&lt;/a&gt; in autocast will be performed in bf16.&lt;/p&gt; &#xA;&lt;p&gt;This option is important for RoPE layer which gives incorrect results when cast to lower precision especially with longer context lengths.&lt;/p&gt; &#xA;&lt;h2&gt;Comparinson to an existing trainer&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AnswerDotAI/fsdp_qlora/assets/6575163/97bb03fb-c2bb-4679-83ff-63a2e202826f&#34; alt=&#34;Screenshot 2024-02-01 083222&#34;&gt; &lt;code&gt;hf_train.py&lt;/code&gt; uses TRL&#39;s SFTTrainer for a comparison run. To match with our script, modify the dataloading code to train on everything (not just completions) and then run &lt;code&gt;train.py --train_type qlora --dataset guanaco --batch_size 8 --lr_scheduler cosine --log_to wandb --save_model True --output_dir guanaco_7B --gradient_accumulation_steps 2 --lr 2e-4&lt;/code&gt;. The SFTTrainer version has to run with a lower batch size (4 vs 8) so we only do 2 gradient accumulation steps vs 4 in the QLoRA+FSDP version.&lt;/p&gt; &#xA;&lt;h2&gt;Converting Saved Models&lt;/h2&gt; &#xA;&lt;p&gt;If you specify &lt;code&gt;--save_model True&lt;/code&gt; the adapter layers will be saved as a state dict. To convert to the regular Hugging Face format and upload to the hub, see: &lt;strong&gt;Converting the State Dict.ipynb&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If &lt;code&gt;&#34;custom_qlora&#34;, &#34;hqq_lora&#34;&lt;/code&gt; training options are used, then only the trainable LoRA parameters will be saved. Before inference, you need to load and quantize the base model again, and separately load the saved LoRA parameters.&lt;/p&gt; &#xA;&lt;p&gt;You can alternatively test to see if merging base model weights and trained LoRA weights and then quantizing them performs similar to keeping the parameters separately as done during training. To make use of &lt;code&gt;torch.compile&lt;/code&gt; with HQQ, see &lt;a href=&#34;https://github.com/mobiusml/hqq/issues/18&#34;&gt;https://github.com/mobiusml/hqq/issues/18&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;p&gt;While QLoRA finetuning works with FSDP, there are some rough edges to be aware of with this alpha release and our example script.&lt;/p&gt; &#xA;&lt;p&gt;First, the current release of Transformer &lt;code&gt;AutoModel.from_pretrained&lt;/code&gt; cannot be used to load models into quantized weights, as it does not support the new quant_storage or quantization flag. Loading pretrained models requires writing or using custom model loading code. We provide an example of how to load and quantize a QLoRA model for finetuning in our demo script.&lt;/p&gt; &#xA;&lt;p&gt;We are actively working with Hugging Face to resolve this incompatibility in future Transformers and PEFT releases.&lt;/p&gt; &#xA;&lt;p&gt;Secpnd, while FSDPâ€™s Mixed Precision works with QLoRA, practitioners need to be careful to set the &lt;code&gt;MixedPrecision.param_type&lt;/code&gt; to match the &lt;code&gt;Linear4Bit.quant_storage&lt;/code&gt; dtype. Otherwise, FSDPâ€™s Mixed Precision could cast the quantized weights to a different precision, essentially turning them into random weights. Our example script shows how to avoid this potential pitfall, and we will be happy to assist model training libraries in correctly exposing FSDPâ€™s Mixed Precision options to users when training with QLoRA&lt;/p&gt; &#xA;&lt;h2&gt;Example: Llama 70B 4-A100 40GB Training&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# BnB QLoRA&#xA;export CUDA_VISIBLE_DEVICES=4,5,6,7&#xA;python train.py \&#xA;--world_size 4 \&#xA;--master_port 12356 \&#xA;--model_name meta-llama/Llama-2-70b-hf \&#xA;--gradient_accumulation_steps 4 \&#xA;--batch_size 2 \&#xA;--context_length 512 \&#xA;--precision bf16_buffers_autocast \&#xA;--train_type custom_qlora \&#xA;--use_gradient_checkpointing true \&#xA;--reentrant_checkpointing true&#xA;--use_cpu_offload false \&#xA;--log_to stdout \&#xA;--dataset alpaca \&#xA;&#xA;# HQQ QLoRA&#xA;export CUDA_VISIBLE_DEVICES=4,5,6,7&#xA;python train.py \&#xA;--world_size 4 \&#xA;--master_port 12356 \&#xA;--model_name meta-llama/Llama-2-70b-hf \&#xA;--gradient_accumulation_steps 4 \&#xA;--batch_size 2 \&#xA;--context_length 512 \&#xA;--precision bf16_buffers_autocast \&#xA;--train_type hqq_lora \&#xA;--use_gradient_checkpointing true \&#xA;--use_cpu_offload false \&#xA;--log_to stdout \&#xA;--dataset alpaca \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For large batch size or long context training HQQ LoRA is a bit more memory efficient compared to BnB LoRA with re-entrant checkpointing. So if you are running into OOM issues, try using HQQ LoRA.&lt;/p&gt; &#xA;&lt;h2&gt;SLURM Training&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;code&gt;fsdp_multi_node.sh&lt;/code&gt; for an example training script using multi-node training with SLURM.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AlexZhangji/Twitter-Insight-LLM</title>
    <updated>2024-03-10T01:34:24Z</updated>
    <id>tag:github.com,2024-03-10:/AlexZhangji/Twitter-Insight-LLM</id>
    <link href="https://github.com/AlexZhangji/Twitter-Insight-LLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Twitter data scraping and more.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Twitter-InsightðŸ’¡: Data Scraping, Analysis, Image caption and More&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlexZhangji/Twitter-Insight-LLM/main/README_zh.md&#34;&gt;ä¸­æ–‡Readme&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project enables you to fetch liked tweets from Twitter (using Selenium), save it to JSON and Excel files, and perform initial data analysis and image captions.&lt;/p&gt; &#xA;&lt;p&gt;This is part of the initial steps for a larger personal project involving Large Language Models (LLMs). Stay tuned for more updates!&lt;/p&gt; &#xA;&lt;h3&gt;Example of Exported Excel sheets &amp;amp; Visualizations:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AlexZhangji/Twitter-Insight-LLM/main/images/sample_excel_with_data_viz.png&#34; alt=&#34;Sample Images&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Demo Video&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=UA35W-aWQZk&#34;&gt;Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Before running the code, ensure you have the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Required Python libraries (listed in &lt;code&gt;requirements.txt&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Get your twitter auth token (Not API key) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Quick text instruction:&lt;/li&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Go to your already logged-in twitter&lt;/li&gt; &#xA;     &lt;li&gt;F12 (open dev tools) -&amp;gt; Application -&amp;gt; Cookies -&amp;gt; Twitter.com -&amp;gt; auth_key&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;or follow the video demo in FAQ section.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;OpenAI API key (optional, only needed if you want to try the image captions feature)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository or download the project files.&lt;/li&gt; &#xA; &lt;li&gt;Install the required Python libraries by running the following command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Open the &lt;code&gt;config.py&lt;/code&gt; file and replace the placeholders with your actual API keys:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set &lt;code&gt;TWITTER_AUTH_TOKEN&lt;/code&gt; to your Twitter API authentication token.&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; to your OpenAI API key.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Ingestion&lt;/h2&gt; &#xA;&lt;p&gt;To fetch data from Twitter and save it to JSON and Excel files, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open the &lt;code&gt;twitter_data_ingestion.py&lt;/code&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;Modify the &lt;code&gt;fetch_tweets&lt;/code&gt; function call at the bottom of the script with your desired parameters:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set the URL of the Twitter page you want to fetch data from (e.g., &lt;code&gt;https://twitter.com/ilyasut/likes&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Specify the start and end dates for the data range (in YYYY-MM-DD format).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the script by executing the following command (recommend run this in IDE directly):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python twitter_data_ingestion.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The script will fetch the data from Twitter, save it to a JSON file, and then export it to an Excel file.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Data Analysis&lt;/h2&gt; &#xA;&lt;p&gt;To perform initial data analysis on the fetched data, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open the &lt;code&gt;twitter_data_initial_exploration.ipynb&lt;/code&gt; notebook in Jupyter Notebook or JupyterLab.&lt;/li&gt; &#xA; &lt;li&gt;Run the notebook cells sequentially to load the data from the JSON file and perform various data analysis tasks.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Some sample results:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Visualizing likes by media type over time &lt;img src=&#34;https://raw.githubusercontent.com/AlexZhangji/Twitter-Insight-LLM/main/images/likes_analysis.png&#34; alt=&#34;Likes Analysis by Media Type&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;Creating a calendar heatmap of liked tweets per day &lt;img src=&#34;https://raw.githubusercontent.com/AlexZhangji/Twitter-Insight-LLM/main/images/liked_tweets_per_day.png&#34; alt=&#34;Number of Liked Tweets per Day&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;The notebook also demonstrates how to use the OpenAI API to generate image captions for tweet images (with tweet metadata). &lt;img src=&#34;https://raw.githubusercontent.com/AlexZhangji/Twitter-Insight-LLM/main/images/sample_image_caption_en.jpg&#34; alt=&#34;Sample Image Caption&#34;&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Sample Output&lt;/h2&gt; &#xA;&lt;p&gt;The project includes sample output files for reference:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sample_output_json.json&lt;/code&gt;: A sample JSON file containing the fetched Twitter data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sample_exported_excel.xlsx&lt;/code&gt;: A sample Excel file exported from the JSON data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Feel free to explore and modify the code to suit your specific data analysis requirements.&lt;/p&gt; &#xA;&lt;h2&gt;FAQs:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Will I get banned? Could this affect my account?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Selenium is one of the safest scraping methods out there, but it&#39;s still best to be cautious when using it for personal projects.&lt;/li&gt; &#xA;   &lt;li&gt;I&#39;ve been using it for quite a while without any issues.&lt;/li&gt; &#xA;   &lt;li&gt;(Though, if you&#39;ve got a spare / alt account, I&#39;d recommend using that one&#39;s auth token instead)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;How do I find the auth token?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Check out this for a step-by-step guide! &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=MhKMNsbjug4&#34;&gt;video demo&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions to this project are welcome. If you find any issues or have suggestions for improvements, please open an issue or submit a pull request.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial structure and parts of the Selenium code inspired by &lt;a href=&#34;https://github.com/Mostafa-Ehab/Twitter-Scrapper&#34;&gt;Twitter-Scrapper&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The image captioning feature is powered by the OpenAI API. You should be able to achieve similar results using Gemini 1.0.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For any questions or issues, please open an issue in the repository.&lt;/p&gt;</summary>
  </entry>
</feed>