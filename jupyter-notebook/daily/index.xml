<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-14T01:32:02Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>SakanaAI/AI-Scientist</title>
    <updated>2024-11-14T01:32:02Z</updated>
    <id>tag:github.com,2024-11-14:/SakanaAI/AI-Scientist</id>
    <link href="https://github.com/SakanaAI/AI-Scientist" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery üßë‚Äçüî¨&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/docs/logo_2.png&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/docs/logo_2.png&#34; width=&#34;215&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;b&gt;The AI Scientist: Towards Fully Automated&lt;/b&gt;&lt;br&gt; &lt;b&gt;Open-Ended Scientific Discovery üßë‚Äçüî¨&lt;/b&gt;&lt;br&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; üìö &lt;a href=&#34;https://arxiv.org/abs/2408.06292&#34;&gt;[Paper]&lt;/a&gt; | üìù &lt;a href=&#34;https://sakana.ai/ai-scientist/&#34;&gt;[Blog Post]&lt;/a&gt; | üìÇ &lt;a href=&#34;https://drive.google.com/drive/folders/1G7A0wTqfXVa-cpexjk0oaXakaSJwffEt&#34;&gt;[Drive Folder]&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;One of the grand challenges of artificial intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used to aid human scientists‚Äîfor example, for brainstorming ideas or writing code‚Äîthey still require extensive manual supervision or are heavily constrained to specific tasks.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re excited to introduce &lt;strong&gt;The AI Scientist&lt;/strong&gt;, the first comprehensive system for fully automatic scientific discovery, enabling Foundation Models such as Large Language Models (LLMs) to perform research independently.&lt;/p&gt; &#xA;&lt;p&gt;We provide all runs and data from our paper &lt;a href=&#34;https://drive.google.com/drive/folders/1G7A0wTqfXVa-cpexjk0oaXakaSJwffEt?usp=sharing&#34;&gt;here&lt;/a&gt;, where we run each base model on each template for approximately 50 ideas. We &lt;em&gt;highly&lt;/em&gt; recommend reading through some of the &lt;a href=&#34;https://drive.google.com/drive/folders/1Mmpz6M1FK4q8e-SewgZcUzdeD0Q2zC39?usp=sharing&#34;&gt;Claude papers&lt;/a&gt; to get a sense of the system&#39;s strengths and weaknesses. Here are some example papers generated by &lt;strong&gt;The AI Scientist&lt;/strong&gt; üìù:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/adaptive_dual_scale_denoising.pdf&#34;&gt;DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/grid_based_noise_adaptation.pdf&#34;&gt;Multi-scale Grid Noise Adaptation: Enhancing Diffusion Models For Low-dimensional Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/gan_diffusion.pdf&#34;&gt;GAN-Enhanced Diffusion: Boosting Sample Quality and Diversity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/dual_expert_denoiser.pdf&#34;&gt;DualDiff: Enhancing Mode Capture in Low-dimensional Diffusion Models via Dual-expert Denoising&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/multi_style_adapter.pdf&#34;&gt;StyleFusion: Adaptive Multi-style Generation in Character-Level Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/rl_lr_adaptation.pdf&#34;&gt;Adaptive Learning Rates for Transformers via Q-Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/weight_initialization_grokking.pdf&#34;&gt;Unlocking Grokking: A Comparative Study of Weight Initialization Strategies in Transformer Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/layerwise_lr_grokking.pdf&#34;&gt;Grokking Accelerated: Layer-wise Learning Rates for Transformer Generalization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/mdl_grokking_correlation.pdf&#34;&gt;Grokking Through Compression: Unveiling Sudden Generalization via Minimal Description Length&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/data_augmentation_grokking.pdf&#34;&gt;Accelerating Mathematical Insight: Boosting Grokking Through Strategic Data Augmentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Caution!&lt;/strong&gt; This codebase will execute LLM-written code. There are various risks and challenges associated with this autonomy, including the use of potentially dangerous packages, web access, and potential spawning of processes. Use at your own discretion. Please make sure to &lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#containerization&#34;&gt;containerize&lt;/a&gt; and restrict web access appropriately.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/example_papers/adaptive_dual_scale_denoising/adaptive_dual_scale_denoising.pdf&#34;&gt;&lt;img src=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/docs/anim-ai-scientist.gif&#34; alt=&#34;Adaptive Dual Scale Denoising&#34; width=&#34;80%&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#requirements&#34;&gt;Requirements&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#supported-models-and-api-keys&#34;&gt;Supported Models and API Keys&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#setting-up-the-templates&#34;&gt;Setting Up the Templates&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#nanogpt-template&#34;&gt;NanoGPT Template&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#2d-diffusion-template&#34;&gt;2D Diffusion Template&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#grokking-template&#34;&gt;Grokking Template&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#run-ai-scientist-paper-generation-experiments&#34;&gt;Run AI Scientist Paper Generation Experiments&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#getting-an-llm-generated-paper-review&#34;&gt;Getting an LLM-Generated Paper Review&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#making-your-own-template&#34;&gt;Making Your Own Template&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#community-contributed-templates&#34;&gt;Community-Contributed Templates&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#template-resources&#34;&gt;Template Resources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#citing-the-ai-scientist&#34;&gt;Citing The AI Scientist&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#frequently-asked-questions&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SakanaAI/AI-Scientist/main/#containerization&#34;&gt;Containerization&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;We provide three templates, which were used in our paper, covering the following domains: &lt;strong&gt;NanoGPT&lt;/strong&gt;, &lt;strong&gt;2D Diffusion&lt;/strong&gt;, and &lt;strong&gt;Grokking&lt;/strong&gt;. These templates enable The AI Scientist to generate ideas and conduct experiments in these areas. We accept contributions of new templates from the community, but please note that they are not maintained by us. All other templates beyond the three provided are community contributions.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;This code is designed to run on Linux with NVIDIA GPUs using CUDA and PyTorch. Support for other GPU architectures may be possible by following the &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch guidelines&lt;/a&gt;. The current templates would likely take an infeasible amount of time on CPU-only machines. Running on other operating systems may require significant adjustments.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n ai_scientist python=3.11&#xA;conda activate ai_scientist&#xA;# Install pdflatex&#xA;sudo apt-get install texlive-full&#xA;&#xA;# Install PyPI requirements&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Installing &lt;code&gt;texlive-full&lt;/code&gt; can take a long time. You may need to &lt;a href=&#34;https://askubuntu.com/questions/956006/pregenerating-context-markiv-format-this-may-take-some-time-takes-forever&#34;&gt;hold Enter&lt;/a&gt; during the installation.&lt;/p&gt; &#xA;&lt;h3&gt;Supported Models and API Keys&lt;/h3&gt; &#xA;&lt;p&gt;We support a wide variety of models, including open-weight and API-only models. In general, we recommend using only frontier models above the capability of the original GPT-4. To see a full list of supported models, see &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/raw/main/ai_scientist/llm.py&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;OpenAI API (GPT-4o, GPT-4o-mini, o1 models)&lt;/h4&gt; &#xA;&lt;p&gt;By default, this uses the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;h4&gt;Anthropic API (Claude Sonnet 3.5)&lt;/h4&gt; &#xA;&lt;p&gt;By default, this uses the &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;h5&gt;Claude Models via Bedrock&lt;/h5&gt; &#xA;&lt;p&gt;For Claude models provided by &lt;a href=&#34;https://aws.amazon.com/bedrock/&#34;&gt;Amazon Bedrock&lt;/a&gt;, please install these additional packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install anthropic[bedrock]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, specify a set of valid &lt;a href=&#34;https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-envvars.html&#34;&gt;AWS Credentials&lt;/a&gt; and the target &lt;a href=&#34;https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-regions.html&#34;&gt;AWS Region&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;Set the environment variables: &lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt;, &lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt;, &lt;code&gt;AWS_REGION_NAME&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Claude Models via Vertex AI&lt;/h5&gt; &#xA;&lt;p&gt;For Claude models provided by &lt;a href=&#34;https://cloud.google.com/model-garden?hl=en&#34;&gt;Vertex AI Model Garden&lt;/a&gt;, please install these additional packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install google-cloud-aiplatform&#xA;pip install anthropic[vertex]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, set up valid authentication for a &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/authentication&#34;&gt;Google Cloud project&lt;/a&gt;, for example by providing the region and project ID:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CLOUD_ML_REGION=&#34;REGION&#34;           # for Model Garden call&#xA;export ANTHROPIC_VERTEX_PROJECT_ID=&#34;PROJECT_ID&#34;  # for Model Garden call&#xA;export VERTEXAI_LOCATION=&#34;REGION&#34;         # for Aider/LiteLLM call&#xA;export VERTEXAI_PROJECT=&#34;PROJECT_ID&#34;      # for Aider/LiteLLM call&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;DeepSeek API (DeepSeek-Coder-V2)&lt;/h4&gt; &#xA;&lt;p&gt;By default, this uses the &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;h4&gt;OpenRouter API (Llama3.1)&lt;/h4&gt; &#xA;&lt;p&gt;By default, this uses the &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;h4&gt;Semantic Scholar API (Literature Search)&lt;/h4&gt; &#xA;&lt;p&gt;Our code can also optionally use a Semantic Scholar API Key (&lt;code&gt;S2_API_KEY&lt;/code&gt;) for higher throughput &lt;a href=&#34;https://www.semanticscholar.org/product/api&#34;&gt;if you have one&lt;/a&gt;, though it should work without it in principle. If you have problems with Semantic Scholar, you can skip the literature search and citation phases of paper generation.&lt;/p&gt; &#xA;&lt;p&gt;Be sure to provide the key for the model used for your runs, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&#34;YOUR KEY HERE&#34;&#xA;export S2_API_KEY=&#34;YOUR KEY HERE&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setting Up the Templates&lt;/h2&gt; &#xA;&lt;p&gt;This section provides instructions for setting up each of the three templates used in our paper. Before running The AI Scientist experiments, please ensure you have completed the setup steps for the templates you are interested in.&lt;/p&gt; &#xA;&lt;h3&gt;NanoGPT Template&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; This template investigates transformer-based autoregressive next-token prediction tasks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Setup Steps:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prepare the data:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python data/enwik8/prepare.py&#xA;python data/shakespeare_char/prepare.py&#xA;python data/text8/prepare.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create baseline runs (machine dependent):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set up NanoGPT baseline run&#xA;# NOTE: YOU MUST FIRST RUN THE PREPARE SCRIPTS ABOVE!&#xA;cd templates/nanoGPT&#xA;python experiment.py --out_dir run_0&#xA;python plot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;2D Diffusion Template&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; This template studies improving the performance of diffusion generative models on low-dimensional datasets.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Setup Steps:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install dependencies:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set up 2D Diffusion&#xA;git clone https://github.com/gregversteeg/NPEET.git&#xA;cd NPEET&#xA;pip install .&#xA;pip install scikit-learn&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create baseline runs:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set up 2D Diffusion baseline run&#xA;cd templates/2d_diffusion&#xA;python experiment.py --out_dir run_0&#xA;python plot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Grokking Template&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Description:&lt;/strong&gt; This template investigates questions about generalization and learning speed in deep neural networks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Setup Steps:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install dependencies:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set up Grokking&#xA;pip install einops&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create baseline runs:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set up Grokking baseline run&#xA;cd templates/grokking&#xA;python experiment.py --out_dir run_0&#xA;python plot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Run AI Scientist Paper Generation Experiments&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Please ensure the setup steps above are completed before running these experiments.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate ai_scientist&#xA;# Run the paper generation.&#xA;python launch_scientist.py --model &#34;gpt-4o-2024-05-13&#34; --experiment nanoGPT_lite --num-ideas 2&#xA;python launch_scientist.py --model &#34;claude-3-5-sonnet-20241022&#34; --experiment nanoGPT_lite --num-ideas 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have more than one GPU, use the &lt;code&gt;--parallel&lt;/code&gt; option to parallelize ideas across multiple GPUs.&lt;/p&gt; &#xA;&lt;h2&gt;Getting an LLM-Generated Paper Review&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;from ai_scientist.perform_review import load_paper, perform_review&#xA;&#xA;client = openai.OpenAI()&#xA;model = &#34;gpt-4o-2024-05-13&#34;&#xA;&#xA;# Load paper from PDF file (raw text)&#xA;paper_txt = load_paper(&#34;report.pdf&#34;)&#xA;&#xA;# Get the review dictionary&#xA;review = perform_review(&#xA;    paper_txt,&#xA;    model,&#xA;    client,&#xA;    num_reflections=5,&#xA;    num_fs_examples=1,&#xA;    num_reviews_ensemble=5,&#xA;    temperature=0.1,&#xA;)&#xA;&#xA;# Inspect review results&#xA;review[&#34;Overall&#34;]    # Overall score (1-10)&#xA;review[&#34;Decision&#34;]   # &#39;Accept&#39; or &#39;Reject&#39;&#xA;review[&#34;Weaknesses&#34;] # List of weaknesses (strings)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run batch analysis:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd review_iclr_bench&#xA;python iclr_analysis.py --num_reviews 500 --batch_size 100 --num_fs_examples 1 --num_reflections 5 --temperature 0.1 --num_reviews_ensemble 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Making Your Own Template&lt;/h2&gt; &#xA;&lt;p&gt;If there is an area of study you would like &lt;strong&gt;The AI Scientist&lt;/strong&gt; to explore, it is straightforward to create your own templates. In general, follow the structure of the existing templates, which consist of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;experiment.py&lt;/code&gt; ‚Äî This is the main script where the core content is. It takes an argument &lt;code&gt;--out_dir&lt;/code&gt;, which specifies where it should create the folder and save the relevant information from the run.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;plot.py&lt;/code&gt; ‚Äî This script takes the information from the &lt;code&gt;run&lt;/code&gt; folders and creates plots. The code should be clear and easy to edit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;prompt.json&lt;/code&gt; ‚Äî Put information about your template here.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;seed_ideas.json&lt;/code&gt; ‚Äî Place example ideas here. You can also try to generate ideas without any examples and then pick the best one or two to put here.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;latex/template.tex&lt;/code&gt; ‚Äî We recommend using our LaTeX folder but be sure to replace the pre-loaded citations with ones that you expect to be more relevant.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The key to making new templates work is matching the base filenames and output JSONs to the existing format; everything else is free to change. You should also ensure that the &lt;code&gt;template.tex&lt;/code&gt; file is updated to use the correct citation style / base plots for your template.&lt;/p&gt; &#xA;&lt;h3&gt;Community-Contributed Templates&lt;/h3&gt; &#xA;&lt;p&gt;We welcome community contributions in the form of new templates. While these are not maintained by us, we are delighted to highlight your templates to others. Below, we list community-contributed templates along with links to their pull requests (PRs):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Infectious Disease Modeling (&lt;code&gt;seir&lt;/code&gt;) - &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/pull/137&#34;&gt;PR #137&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Image Classification with MobileNetV3 (&lt;code&gt;mobilenetV3&lt;/code&gt;) - &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/pull/141&#34;&gt;PR #141&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sketch RNN (&lt;code&gt;sketch_rnn&lt;/code&gt;) - &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/pull/143&#34;&gt;PR #143&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;This section is reserved for community contributions. Please submit a pull request to add your template to the list! Please describe the template in the PR description, and also show examples of the generated papers.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Template Resources&lt;/h2&gt; &#xA;&lt;p&gt;We provide three templates, which heavily use code from other repositories, credited below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;NanoGPT Template&lt;/strong&gt; uses code from &lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;NanoGPT&lt;/a&gt; and this &lt;a href=&#34;https://github.com/karpathy/nanoGPT/pull/254&#34;&gt;PR&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2D Diffusion Template&lt;/strong&gt; uses code from &lt;a href=&#34;https://github.com/tanelp/tiny-diffusion&#34;&gt;tiny-diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/lucidrains/ema-pytorch&#34;&gt;ema-pytorch&lt;/a&gt;, and &lt;a href=&#34;https://www.research.autodesk.com/publications/same-stats-different-graphs/&#34;&gt;Datasaur&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Grokking Template&lt;/strong&gt; uses code from &lt;a href=&#34;https://github.com/Sea-Snell/grokking&#34;&gt;Sea-Snell/grokking&lt;/a&gt; and &lt;a href=&#34;https://github.com/danielmamay/grokking&#34;&gt;danielmamay/grokking&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We would like to thank the developers of the open-source models and packages for their contributions and for making their work available.&lt;/p&gt; &#xA;&lt;h2&gt;Citing The AI Scientist&lt;/h2&gt; &#xA;&lt;p&gt;If you use &lt;strong&gt;The AI Scientist&lt;/strong&gt; in your research, please cite it as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{lu2024aiscientist,&#xA;  title={The {AI} {S}cientist: Towards Fully Automated Open-Ended Scientific Discovery},&#xA;  author={Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},&#xA;  journal={arXiv preprint arXiv:2408.06292},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; &#xA;&lt;p&gt;We recommend reading our paper first for any questions you have on The AI Scientist.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why am I missing files when running The AI Scientist?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ensure you have completed all the setup and preparation steps before the main experiment script.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why has a PDF or a review not been generated?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The AI Scientist finishes an idea with a success rate that depends on the template, the base foundation model, and the complexity of the idea. We advise referring to our main paper. The highest success rates are observed with Claude Sonnet 3.5. Reviews are best done with GPT-4o; all other models have issues with positivity bias or failure to conform to required outputs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What is the cost of each idea generated?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Typically less than $15 per paper with Claude Sonnet 3.5. We recommend DeepSeek Coder V2 for a much more cost-effective approach. A good place to look for new models is the &lt;a href=&#34;https://aider.chat/docs/leaderboards/&#34;&gt;Aider leaderboard&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How do I change the base conference format associated with the write-ups?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Change the base &lt;code&gt;template.tex&lt;/code&gt; files contained within each template.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How do I run The AI Scientist for different subject fields?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the instructions for different templates. In this current iteration, this is restricted to ideas that can be expressed in code. However, lifting this restriction would represent exciting future work! :)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How do I add support for a new foundation model?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You may modify &lt;code&gt;ai_scientist/llm.py&lt;/code&gt; to add support for a new foundation model. We do not advise using any model that is significantly weaker than GPT-4 level for &lt;strong&gt;The AI Scientist&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why do I need to run the baseline runs myself?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;These appear as &lt;code&gt;run_0&lt;/code&gt; and should be run per machine you execute &lt;strong&gt;The AI Scientist&lt;/strong&gt; on for accurate run-time comparisons due to hardware differences.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What if I have problems accessing the Semantic Scholar API?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We use the Semantic Scholar API to check ideas for novelty and collect citations for the paper write-up. You may be able to skip these phases if you don&#39;t have an API key or the API is slow to access.&lt;/p&gt; &#xA;&lt;h2&gt;Containerization&lt;/h2&gt; &#xA;&lt;p&gt;We include a &lt;a href=&#34;https://github.com/SakanaAI/AI-Scientist/pull/21&#34;&gt;community-contributed&lt;/a&gt; Docker image that may assist with your containerization efforts in &lt;code&gt;experimental/Dockerfile&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can use this image like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Endpoint Script&#xA;docker run -e OPENAI_API_KEY=$OPENAI_API_KEY -v `pwd`/templates:/app/AI-Scientist/templates &amp;lt;AI_SCIENTIST_IMAGE&amp;gt; \&#xA;  --model gpt-4o-2024-05-13 \&#xA;  --experiment 2d_diffusion \&#xA;  --num-ideas 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Interactive&#xA;docker run -it -e OPENAI_API_KEY=$OPENAI_API_KEY \&#xA;  --entrypoint /bin/bash \&#xA;  &amp;lt;AI_SCIENTIST_IMAGE&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#SakanaAI/AI-Scientist&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=SakanaAI/AI-Scientist&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>IDEA-Research/Grounded-SAM-2</title>
    <updated>2024-11-14T01:32:02Z</updated>
    <id>tag:github.com,2024-11-14:/IDEA-Research/Grounded-SAM-2</id>
    <link href="https://github.com/IDEA-Research/Grounded-SAM-2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Grounded SAM 2: Ground and Track Anything in Videos with Grounding DINO, Florence-2 and SAM 2&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Grounded SAM 2: Ground and Track Anything in Videos&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ai.meta.com/research/&#34;&gt;IDEA-Research&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://rentainhe.github.io/&#34;&gt;Tianhe Ren&lt;/a&gt;, &lt;a href=&#34;https://github.com/ShuoShenDe&#34;&gt;Shuo Shen&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2408.00714&#34;&gt;&lt;code&gt;SAM2 Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;&lt;code&gt;Grounding DINO Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2405.10300&#34;&gt;&lt;code&gt;Grounding DINO 1.5 Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#citation&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/f0fb0022-779a-49fb-8f46-3a18a8b4e893&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/assets/grounded_sam_2_intro.jpg&#34; alt=&#34;Video Name&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üî• Project Highlight&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Grounded SAM 2 is a foundation model pipeline towards grounding and track anything in Videos with &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Grounding DINO&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2405.10300&#34;&gt;Grounding DINO 1.5&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2311.06242&#34;&gt;Florence-2&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2408.00714&#34;&gt;SAM 2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In this repo, we&#39;ve supported the following demo with &lt;strong&gt;simple implementations&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ground and Segment Anything&lt;/strong&gt; with Grounding DINO, Grounding DINO 1.5 &amp;amp; 1.6 and SAM 2&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ground and Track Anything&lt;/strong&gt; with Grounding DINO, Grounding DINO 1.5 &amp;amp; 1.6 and SAM 2&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Detect, Segment and Track Visualization&lt;/strong&gt; based on the powerful &lt;a href=&#34;https://github.com/roboflow/supervision&#34;&gt;supervision&lt;/a&gt; library.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Grounded SAM 2 does not introduce significant methodological changes compared to &lt;a href=&#34;https://arxiv.org/abs/2401.14159&#34;&gt;Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks&lt;/a&gt;. Both approaches leverage the capabilities of open-world models to address complex visual tasks. Consequently, we try to &lt;strong&gt;simplify the code implementation&lt;/strong&gt; in this repository, aiming to enhance user convenience.&lt;/p&gt; &#xA;&lt;h2&gt;Latest updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/10/24&lt;/code&gt;: Support &lt;a href=&#34;https://docs.ultralytics.com/guides/sahi-tiled-inference/&#34;&gt;SAHI (Slicing Aided Hyper Inference)&lt;/a&gt; on Grounded SAM 2 (with Grounding DINO 1.5) which may be helpful for inferencing high resolution image with dense small objects (e.g. &lt;strong&gt;4K&lt;/strong&gt; images).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/10/10&lt;/code&gt;: Support &lt;code&gt;SAM-2.1&lt;/code&gt; models, if you want to use &lt;code&gt;SAM 2.1&lt;/code&gt; model, you need to update to the latest code and reinstall SAM 2 follow &lt;a href=&#34;https://github.com/facebookresearch/sam2?tab=readme-ov-file#latest-updates&#34;&gt;SAM 2.1 Installation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/08/31&lt;/code&gt;: Support &lt;code&gt;dump json results&lt;/code&gt; in Grounded SAM 2 Image Demos (with Grounding DINO).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/08/20&lt;/code&gt;: Support &lt;strong&gt;Florence-2 SAM 2 Image Demo&lt;/strong&gt; which includes &lt;code&gt;dense region caption&lt;/code&gt;, &lt;code&gt;object detection&lt;/code&gt;, &lt;code&gt;phrase grounding&lt;/code&gt;, and cascaded auto-label pipeline &lt;code&gt;caption + phrase grounding&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/08/09&lt;/code&gt;: Support &lt;strong&gt;Ground and Track New Object&lt;/strong&gt; throughout the whole videos. This feature is still under development now. Credits to &lt;a href=&#34;https://github.com/ShuoShenDe&#34;&gt;Shuo Shen&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/08/07&lt;/code&gt;: Support &lt;strong&gt;Custom Video Inputs&lt;/strong&gt;, users need only submit their video file (e.g. &lt;code&gt;.mp4&lt;/code&gt; file) with specific text prompts to get an impressive demo videos.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-demos&#34;&gt;Grounded SAM 2 Demos&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-image-demo-with-grounding-dino&#34;&gt;Grounded SAM 2 Image Demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-image-demo-with-grounding-dino-15--16&#34;&gt;Grounded SAM 2 Image Demo (with Grounding DINO 1.5 &amp;amp; 1.6)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#sahi-slicing-aided-hyper-inference-with-grounding-dino-15-and-sam-2&#34;&gt;Grounded SAM 2 with SAHI for High Resolution Image Inference&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#automatically-saving-grounding-results-image-demo&#34;&gt;Automatically Saving Grounding and Segmentation Results&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-video-object-tracking-demo&#34;&gt;Grounded SAM 2 Video Object Tracking Demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-video-object-tracking-demo-with-grounding-dino-15--16&#34;&gt;Grounded SAM 2 Video Object Tracking Demo (with Grounding DINO 1.5 &amp;amp; 1.6)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-video-object-tracking-demo-with-custom-video-input-with-grounding-dino&#34;&gt;Grounded SAM 2 Video Object Tracking with Custom Video Input (using Grounding DINO)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-video-object-tracking-demo-with-custom-video-input-with-grounding-dino-15--16&#34;&gt;Grounded SAM 2 Video Object Tracking with Custom Video Input (using Grounding DINO 1.5 &amp;amp; 1.6)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-video-object-tracking-with-continuous-id-with-grounding-dino&#34;&gt;Grounded SAM 2 Video Object Tracking with Continues ID (using Grounding DINO)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-florence-2-demos&#34;&gt;Grounded SAM 2 Florence-2 Demos&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-florence-2-image-demo&#34;&gt;Grounded SAM 2 Florence-2 Image Demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#grounded-sam-2-florence-2-image-auto-labeling-demo&#34;&gt;Grounded SAM 2 Florence-2 Image Auto-Labeling Demo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download the pretrained &lt;code&gt;SAM 2&lt;/code&gt; checkpoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd checkpoints&#xA;bash download_ckpts.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the pretrained &lt;code&gt;Grounding DINO&lt;/code&gt; checkpoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd gdino_checkpoints&#xA;bash download_ckpts.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation without docker&lt;/h3&gt; &#xA;&lt;p&gt;Install PyTorch environment first. We use &lt;code&gt;python=3.10&lt;/code&gt;, as well as &lt;code&gt;torch &amp;gt;= 2.3.1&lt;/code&gt;, &lt;code&gt;torchvision&amp;gt;=0.18.1&lt;/code&gt; and &lt;code&gt;cuda-12.1&lt;/code&gt; in our environment to run this demo. Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended. You can easily install the latest version of PyTorch as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install torch torchvision torchaudio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since we need the CUDA compilation environment to compile the &lt;code&gt;Deformable Attention&lt;/code&gt; operator used in Grounding DINO, we need to check whether the CUDA environment variables have been set correctly (which you can refer to &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO?tab=readme-ov-file#hammer_and_wrench-install&#34;&gt;Grounding DINO Installation&lt;/a&gt; for more details). You can set the environment variable manually as follows if you want to build a local GPU environment for Grounding DINO to run Grounded SAM 2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_HOME=/path/to/cuda-12.1/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install &lt;code&gt;Segment Anything 2&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install &lt;code&gt;Grounding DINO&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --no-build-isolation -e grounding_dino&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation with docker&lt;/h3&gt; &#xA;&lt;p&gt;Build the Docker image and Run the Docker container:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd Grounded-SAM-2&#xA;make build-image&#xA;make run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After executing these commands, you will be inside the Docker environment. The working directory within the container is set to: &lt;code&gt;/home/appuser/Grounded-SAM-2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once inside the Docker environment, you can start the demo by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python grounded_sam2_tracking_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Grounded SAM 2 Demos&lt;/h2&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Image Demo (with Grounding DINO)&lt;/h3&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;Grounding DINO&lt;/code&gt; has already been supported in &lt;a href=&#34;https://huggingface.co/IDEA-Research/grounding-dino-tiny&#34;&gt;Huggingface&lt;/a&gt;, so we provide two choices for running &lt;code&gt;Grounded SAM 2&lt;/code&gt; model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use huggingface API to inference Grounding DINO (which is simple and clear)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_hf_model_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] üö® If you encounter network issues while using the &lt;code&gt;HuggingFace&lt;/code&gt; model, you can resolve them by setting the appropriate mirror source as &lt;code&gt;export HF_ENDPOINT=https://hf-mirror.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Load local pretrained Grounding DINO checkpoint and inference with Grounding DINO original API (make sure you&#39;ve already downloaded the pretrained checkpoint)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_local_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Image Demo (with Grounding DINO 1.5 &amp;amp; 1.6)&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ve already released our most capable open-set detection model &lt;a href=&#34;https://github.com/IDEA-Research/Grounding-DINO-1.5-API&#34;&gt;Grounding DINO 1.5 &amp;amp; 1.6&lt;/a&gt;, which can be combined with SAM 2 for stronger open-set detection and segmentation capability. You can apply the API token first and run Grounded SAM 2 with Grounding DINO 1.5 as follows:&lt;/p&gt; &#xA;&lt;p&gt;Install the latest DDS cloudapi:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install dds-cloudapi-sdk --upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Apply your API token from our official website here: &lt;a href=&#34;https://deepdataspace.com/request_api&#34;&gt;request API token&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_gd1.5_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SAHI (Slicing Aided Hyper Inference) with Grounding DINO 1.5 and SAM 2&lt;/h3&gt; &#xA;&lt;p&gt;If your images are high resolution with dense objects, directly using Grounding DINO 1.5 for inference on the original image may not be the best choice. We support &lt;a href=&#34;https://docs.ultralytics.com/guides/sahi-tiled-inference/&#34;&gt;SAHI (Slicing Aided Hyper Inference)&lt;/a&gt;, which works by first dividing the original image into smaller overlapping patches. Inference is then performed separately on each patch, and the final detection results are merged. This method is highly effective and accuracy for dense and small objects detection in high resolution images.&lt;/p&gt; &#xA;&lt;p&gt;You can run SAHI inference by setting the following param in &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/grounded_sam2_gd1.5_demo.py&#34;&gt;grounded_sam2_gd1.5_demo.py&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;WITH_SLICE_INFERENCE = True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The visualization is shown as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Text Prompt&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Input Image&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Grounded SAM 2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Grounded SAM 2 with SAHI&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;Person&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam_2/demo_images/dense%20people.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam_2/grounding_dino_1.5_slice_inference/grounded_sam2_annotated_image_with_mask.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/IDEA-Research/detrex-storage/raw/main/assets/grounded_sam_2/grounding_dino_1.5_slice_inference/grounded_sam2_annotated_image_with_mask_with_slice_inference.jpg?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Notes:&lt;/strong&gt; We only support SAHI on Grounding DINO 1.5 because it works better with stronger grounding model which may produce less hallucination results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Automatically Saving Grounding Results (Image Demo)&lt;/h3&gt; &#xA;&lt;p&gt;After setting &lt;code&gt;DUMP_JSON_RESULTS=True&lt;/code&gt; in the following Grounded SAM 2 Image Demos:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/grounded_sam2_local_demo.py&#34;&gt;grounded_sam2_local_demo.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/grounded_sam2_hf_model_demo.py&#34;&gt;grounded_sam2_hf_model_demo.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/grounded_sam2_gd1.5_demo.py&#34;&gt;grounded_sam2_gd1.5_demo.py&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;grounding&lt;/code&gt; and &lt;code&gt;segmentation&lt;/code&gt; results will be automatically saved in the &lt;code&gt;outputs&lt;/code&gt; dir with the following format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#xA;    &#34;image_path&#34;: &#34;path/to/image.jpg&#34;,&#xA;    &#34;annotations&#34;: [&#xA;        {&#xA;            &#34;class_name&#34;: &#34;class_name&#34;,&#xA;            &#34;bbox&#34;: [x1, y1, x2, y2],&#xA;            &#34;segmentation&#34;: {&#xA;                &#34;size&#34;: [h, w],&#xA;                &#34;counts&#34;: &#34;rle_encoded_mask&#34;&#xA;            },&#xA;            &#34;score&#34;: confidence score&#xA;        }&#xA;    ],&#xA;    &#34;box_format&#34;: &#34;xyxy&#34;,&#xA;    &#34;img_width&#34;: w,&#xA;    &#34;img_height&#34;: h&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Video Object Tracking Demo&lt;/h3&gt; &#xA;&lt;p&gt;Based on the strong tracking capability of SAM 2, we can combined it with Grounding DINO for open-set object segmentation and tracking. You can run the following scripts to get the tracking results with Grounded SAM 2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The tracking results of each frame will be saved in &lt;code&gt;./tracking_results&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The video will be save as &lt;code&gt;children_tracking_demo_video.mp4&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can refine this file with different text prompt and video clips yourself to get more tracking results.&lt;/li&gt; &#xA; &lt;li&gt;We only prompt the first video frame with Grounding DINO here for simple usage.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Support Various Prompt Type for Tracking&lt;/h4&gt; &#xA;&lt;p&gt;We&#39;ve supported different types of prompt for Grounded SAM 2 tracking demo:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Point Prompt&lt;/strong&gt;: In order to &lt;strong&gt;get a stable segmentation results&lt;/strong&gt;, we re-use the SAM 2 image predictor to get the prediction mask from each object based on Grounding DINO box outputs, then we &lt;strong&gt;uniformly sample points from the prediction mask&lt;/strong&gt; as point prompts for SAM 2 video predictor&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Box Prompt&lt;/strong&gt;: We directly use the box outputs from Grounding DINO as box prompts for SAM 2 video predictor&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mask Prompt&lt;/strong&gt;: We use the SAM 2 mask prediction results based on Grounding DINO box outputs as mask prompt for SAM 2 video predictor.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/assets/g_sam2_tracking_pipeline_vis_new.png&#34; alt=&#34;Grounded SAM 2 Tracking Pipeline&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Video Object Tracking Demo (with Grounding DINO 1.5 &amp;amp; 1.6)&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ve also support video object tracking demo based on our stronger &lt;code&gt;Grounding DINO 1.5&lt;/code&gt; model and &lt;code&gt;SAM 2&lt;/code&gt;, you can try the following demo after applying the API keys for running &lt;code&gt;Grounding DINO 1.5&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo_with_gd1.5.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Video Object Tracking Demo with Custom Video Input (with Grounding DINO)&lt;/h3&gt; &#xA;&lt;p&gt;Users can upload their own video file (e.g. &lt;code&gt;assets/hippopotamus.mp4&lt;/code&gt;) and specify their custom text prompts for grounding and tracking with Grounding DINO and SAM 2 by using the following scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo_custom_video_input_gd1.0_hf_model.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are not convenient to use huggingface demo, you can also run tracking demo with local grounding dino model with the following scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo_custom_video_input_gd1.0_local_model.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Video Object Tracking Demo with Custom Video Input (with Grounding DINO 1.5 &amp;amp; 1.6)&lt;/h3&gt; &#xA;&lt;p&gt;Users can upload their own video file (e.g. &lt;code&gt;assets/hippopotamus.mp4&lt;/code&gt;) and specify their custom text prompts for grounding and tracking with Grounding DINO 1.5 and SAM 2 by using the following scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo_custom_video_input_gd1.5.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify the params in this file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;VIDEO_PATH = &#34;./assets/hippopotamus.mp4&#34;&#xA;TEXT_PROMPT = &#34;hippopotamus.&#34;&#xA;OUTPUT_VIDEO_PATH = &#34;./hippopotamus_tracking_demo.mp4&#34;&#xA;API_TOKEN_FOR_GD1_5 = &#34;Your API token&#34; # api token for G-DINO 1.5&#xA;PROMPT_TYPE_FOR_VIDEO = &#34;mask&#34; # using SAM 2 mask prediction as prompt for video predictor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running our demo code, you can get the tracking results as follows:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/1fbdc6f4-3e50-4221-9600-98c397beecdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/assets/hippopotamus_seg.jpg&#34; alt=&#34;Video Name&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And we will automatically save the tracking visualization results in &lt;code&gt;OUTPUT_VIDEO_PATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] We initialize the box prompts on the first frame of the input video. If you want to start from different frame, you can refine &lt;code&gt;ann_frame_idx&lt;/code&gt; by yourself in our code.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Grounded-SAM-2 Video Object Tracking with Continuous ID (with Grounding DINO)&lt;/h3&gt; &#xA;&lt;p&gt;In above demos, we only prompt Grounded SAM 2 in specific frame, which may not be friendly to find new object during the whole video. In this demo, we try to &lt;strong&gt;find new objects&lt;/strong&gt; and assign them with new ID across the whole video, this function is &lt;strong&gt;still under develop&lt;/strong&gt;. it&#39;s not that stable now.&lt;/p&gt; &#xA;&lt;p&gt;Users can upload their own video files and specify custom text prompts for grounding and tracking using the Grounding DINO and SAM 2 frameworks. To do this, execute the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo_with_continuous_id.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can customize various parameters including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;text&lt;/code&gt;: The grounding text prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;video_dir&lt;/code&gt;: Directory containing the video files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output_dir&lt;/code&gt;: Directory to save the processed output.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output_video_path&lt;/code&gt;: Path for the output video.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;step&lt;/code&gt;: Frame stepping for processing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;box_threshold&lt;/code&gt;: box threshold for groundingdino model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;text_threshold&lt;/code&gt;: text threshold for groundingdino model Note: This method supports only the mask type of text prompt.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After running our demo code, you can get the tracking results as follows:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/d3f91ad0-3d32-43c4-a0dc-0bed661415f4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/Grounded-SAM-2/main/assets/tracking_car_mask_1.jpg&#34; alt=&#34;Video Name&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to try &lt;code&gt;Grounding DINO 1.5&lt;/code&gt; model, you can run the following scripts after setting your API token:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo_with_continuous_id_gd1.5.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Grounded-SAM-2 Video Object Tracking with Continuous ID plus Reverse Tracking(with Grounding DINO)&lt;/h3&gt; &#xA;&lt;p&gt;This method could simply cover the whole lifetime of the object&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_tracking_demo_with_continuous_id_plus.py&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Grounded SAM 2 Florence-2 Demos&lt;/h2&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Florence-2 Image Demo&lt;/h3&gt; &#xA;&lt;p&gt;In this section, we will explore how to integrate the feature-rich and robust open-source models &lt;a href=&#34;https://arxiv.org/abs/2311.06242&#34;&gt;Florence-2&lt;/a&gt; and SAM 2 to develop practical applications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.06242&#34;&gt;Florence-2&lt;/a&gt; is a powerful vision foundation model by Microsoft which supports a series of vision tasks by prompting with special &lt;code&gt;task_prompt&lt;/code&gt; includes but not limited to:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task Prompt&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Text Input&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task Introduction&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Object Detection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;OD&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Detect main objects with single category name&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dense Region Caption&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;DENSE_REGION_CAPTION&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Detect main objects with short description&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Region Proposal&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;REGION_PROPOSAL&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Generate proposals without category name&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Phrase Grounding&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;CAPTION_TO_PHRASE_GROUNDING&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Ground main objects in image mentioned in caption&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Referring Expression Segmentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;REFERRING_EXPRESSION_SEGMENTATION&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Ground the object which is most related to the text input&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Open Vocabulary Detection and Segmentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;OPEN_VOCABULARY_DETECTION&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Ground any object with text input&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Integrate &lt;code&gt;Florence-2&lt;/code&gt; with &lt;code&gt;SAM-2&lt;/code&gt;, we can build a strong vision pipeline to solve complex vision tasks, you can try the following scripts to run the demo:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] üö® If you encounter network issues while using the &lt;code&gt;HuggingFace&lt;/code&gt; model, you can resolve them by setting the appropriate mirror source as &lt;code&gt;export HF_ENDPOINT=https://hf-mirror.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Object Detection and Segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_florence2_image_demo.py \&#xA;    --pipeline object_detection_segmentation \&#xA;    --image_path ./notebooks/images/cars.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dense Region Caption and Segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_florence2_image_demo.py \&#xA;    --pipeline dense_region_caption_segmentation \&#xA;    --image_path ./notebooks/images/cars.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Region Proposal and Segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_florence2_image_demo.py \&#xA;    --pipeline region_proposal_segmentation \&#xA;    --image_path ./notebooks/images/cars.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Phrase Grounding and Segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_florence2_image_demo.py \&#xA;    --pipeline phrase_grounding_segmentation \&#xA;    --image_path ./notebooks/images/cars.jpg \&#xA;    --text_input &#34;The image shows two vintage Chevrolet cars parked side by side, with one being a red convertible and the other a pink sedan, \&#xA;            set against the backdrop of an urban area with a multi-story building and trees. \&#xA;            The cars have Cuban license plates, indicating a location likely in Cuba.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Referring Expression Segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_florence2_image_demo.py \&#xA;    --pipeline referring_expression_segmentation \&#xA;    --image_path ./notebooks/images/cars.jpg \&#xA;    --text_input &#34;The left red car.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Open-Vocabulary Detection and Segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_florence2_image_demo.py \&#xA;    --pipeline open_vocabulary_detection_segmentation \&#xA;    --image_path ./notebooks/images/cars.jpg \&#xA;    --text_input &#34;car &amp;lt;and&amp;gt; building&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note that if you want to &lt;strong&gt;detect multiple classes&lt;/strong&gt; you should split them with &lt;code&gt;&amp;lt;and&amp;gt;&lt;/code&gt; in your input text.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Grounded SAM 2 Florence-2 Image Auto-Labeling Demo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;Florence-2&lt;/code&gt; can be used as a auto image annotator by cascading its caption capability with its grounding capability.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task Prompt&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Text Input&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Caption + Phrase Grounding&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;CAPTION&amp;gt;&lt;/code&gt; + &lt;code&gt;&amp;lt;CAPTION_TO_PHRASE_GROUNDING&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Detailed Caption + Phrase Grounding&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;DETAILED_CAPTION&amp;gt;&lt;/code&gt; + &lt;code&gt;&amp;lt;CAPTION_TO_PHRASE_GROUNDING&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;More Detailed Caption + Phrase Grounding&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&amp;lt;MORE_DETAILED_CAPTION&amp;gt;&lt;/code&gt; + &lt;code&gt;&amp;lt;CAPTION_TO_PHRASE_GROUNDING&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can try the following scripts to run these demo:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Caption to Phrase Grounding&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python grounded_sam2_florence2_autolabel_pipeline.py \&#xA;    --image_path ./notebooks/images/groceries.jpg \&#xA;    --pipeline caption_to_phrase_grounding \&#xA;    --caption_type caption&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can specify &lt;code&gt;caption_type&lt;/code&gt; to control the granularity of the caption, if you want a more detailed caption, you can try &lt;code&gt;--caption_type detailed_caption&lt;/code&gt; or &lt;code&gt;--caption_type more_detailed_caption&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;If you find this project helpful for your research, please consider citing the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTex&#34;&gt;@misc{ravi2024sam2segmentimages,&#xA;      title={SAM 2: Segment Anything in Images and Videos}, &#xA;      author={Nikhila Ravi and Valentin Gabeur and Yuan-Ting Hu and Ronghang Hu and Chaitanya Ryali and Tengyu Ma and Haitham Khedr and Roman R√§dle and Chloe Rolland and Laura Gustafson and Eric Mintun and Junting Pan and Kalyan Vasudev Alwala and Nicolas Carion and Chao-Yuan Wu and Ross Girshick and Piotr Doll√°r and Christoph Feichtenhofer},&#xA;      year={2024},&#xA;      eprint={2408.00714},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV},&#xA;      url={https://arxiv.org/abs/2408.00714}, &#xA;}&#xA;&#xA;@article{liu2023grounding,&#xA;  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},&#xA;  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},&#xA;  journal={arXiv preprint arXiv:2303.05499},&#xA;  year={2023}&#xA;}&#xA;&#xA;@misc{ren2024grounding,&#xA;      title={Grounding DINO 1.5: Advance the &#34;Edge&#34; of Open-Set Object Detection}, &#xA;      author={Tianhe Ren and Qing Jiang and Shilong Liu and Zhaoyang Zeng and Wenlong Liu and Han Gao and Hongjie Huang and Zhengyu Ma and Xiaoke Jiang and Yihao Chen and Yuda Xiong and Hao Zhang and Feng Li and Peijun Tang and Kent Yu and Lei Zhang},&#xA;      year={2024},&#xA;      eprint={2405.10300},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;@misc{ren2024grounded,&#xA;      title={Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks}, &#xA;      author={Tianhe Ren and Shilong Liu and Ailing Zeng and Jing Lin and Kunchang Li and He Cao and Jiayu Chen and Xinyu Huang and Yukang Chen and Feng Yan and Zhaoyang Zeng and Hao Zhang and Feng Li and Jie Yang and Hongyang Li and Qing Jiang and Lei Zhang},&#xA;      year={2024},&#xA;      eprint={2401.14159},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;@article{kirillov2023segany,&#xA;  title={Segment Anything}, &#xA;  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\&#39;a}r, Piotr and Girshick, Ross},&#xA;  journal={arXiv:2304.02643},&#xA;  year={2023}&#xA;}&#xA;&#xA;@misc{jiang2024trex2,&#xA;      title={T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy}, &#xA;      author={Qing Jiang and Feng Li and Zhaoyang Zeng and Tianhe Ren and Shilong Liu and Lei Zhang},&#xA;      year={2024},&#xA;      eprint={2403.14610},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>