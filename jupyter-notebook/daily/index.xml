<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-16T01:44:08Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>khanhnamle1994/cracking-the-data-science-interview</title>
    <updated>2022-06-16T01:44:08Z</updated>
    <id>tag:github.com,2022-06-16:/khanhnamle1994/cracking-the-data-science-interview</id>
    <link href="https://github.com/khanhnamle1994/cracking-the-data-science-interview" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Collection of Cheatsheets, Books, Questions, and Portfolio For DS/ML Interview Prep&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Here are the sections:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/#data-science-cheatsheets&#34;&gt;Data Science Cheatsheets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/#data-science-ebooks&#34;&gt;Data Science EBooks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/#data-science-question-bank&#34;&gt;Data Science Question Bank&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/#data-science-case-studies&#34;&gt;Data Science Case Studies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/#data-science-portfolio&#34;&gt;Data Science Portfolio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/#data-journalism-portfolio&#34;&gt;Data Journalism Portfolio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/#downloadable-cheatsheets&#34;&gt;Downloadable Cheatsheets&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Science Cheatsheets&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Cheatsheets&#34;&gt;This section&lt;/a&gt; contains cheatsheets of basic concepts in data science that will be asked in interviews:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Cheatsheets#sql&#34;&gt;SQL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Cheatsheets#statistics-and-probability&#34;&gt;Statistics and Probability&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Cheatsheets#mathematics&#34;&gt;Mathematics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Cheatsheets#machine-learning-concepts&#34;&gt;Machine Learning Concepts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Cheatsheets#deep-learning-concepts&#34;&gt;Deep Learning Concepts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Cheatsheets#supervised-learning&#34;&gt;Supervised Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Cheatsheets#unsupervised-learning&#34;&gt;Unsupervised Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Cheatsheets#computer-vision&#34;&gt;Computer Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Cheatsheets#natural-language-processing&#34;&gt;Natural Language Processing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Cheatsheets#stanford-materials&#34;&gt;Stanford Materials&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Science EBooks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/EBooks&#34;&gt;This section&lt;/a&gt; contains books that I have read about data science and machine learning:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/EBooks/Intro-To-ML-with-Python&#34;&gt;Intro To Machine Learning with Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/EBooks/Machine-Learning-In-Action&#34;&gt;Machine Learning In Action&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/EBooks/Python-DataScience-Handbook&#34;&gt;Python Data Science Handbook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/EBooks/Doing-Data-Science-Straight-Talk-From-The-Front-Line&#34;&gt;Doing Data Science - Straight Talk From The Front Line&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/EBooks/Machine-Learning-For-Finance&#34;&gt;Machine Learning For Finance&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/EBooks/Practical-Statistics-For-Data-Science&#34;&gt;Practical Statistics for Data Science&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/EBooks/AB-Testing&#34;&gt;A/B Testing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Science Question Bank&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Question-Bank&#34;&gt;This section&lt;/a&gt; contains sample questions that were asked in actual data science interviews:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Question-Bank/Data-Interview-Qs&#34;&gt;Data Interview Qs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Question-Bank/Data-Science-Prep&#34;&gt;Data Science Prep&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Question-Bank/Interview-Query&#34;&gt;Interview Query&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Question-Bank/Analytics-Vidhya.md&#34;&gt;Analytics Vidhya&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Question-Bank/Springboard.md&#34;&gt;Springboard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Question-Bank/Elite-Data-Science.md&#34;&gt;Elite Data Science&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Question-Bank/Workera&#34;&gt;Workera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Question-Bank/150-Essential-Data-Science-Questions-and-Answers.pdf&#34;&gt;150 Essential Data Science Questions and Answers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Science Case Studies&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/cracking-the-data-science-interview/tree/master/Case-Studies&#34;&gt;This section&lt;/a&gt; contains case study questions that concern designing machine learning systems to solve practical problems.&lt;/p&gt; &#xA;&lt;h2&gt;Data Science Portfolio&lt;/h2&gt; &#xA;&lt;p&gt;This section contains portfolio of data science projects completed by me for academic, self learning, and hobby purposes.&lt;/p&gt; &#xA;&lt;p&gt;For a more visually pleasant experience for browsing the portfolio, check out &lt;a href=&#34;https://jameskle.com/data-portfolio&#34;&gt;jameskle.com/data-portfolio&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;h3&gt;Recommendation Systems&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/transfer-rec&#34;&gt;Transfer Rec&lt;/a&gt;: My ongoing research work that intersects deep learning and recommendation systems.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/movielens&#34;&gt;Movie Recommendation&lt;/a&gt;: Designed 4 different models that recommend items on the MovieLens dataset.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Tools: PyTorch, TensorBoard, Keras, Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-Learn, Surprise, Wordcloud&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h3&gt;Machine Learning&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/trip-optimizer&#34;&gt;Trip Optimizer&lt;/a&gt;: Used XGBoost and evolutionary algorithms to optimize the travel time for taxi vehicles in New York City.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/instacart-orders&#34;&gt;Instacart Market Basket Analysis&lt;/a&gt;: Tackled the Instacart Market Basket Analysis challenge to predict which products will be in a user&#39;s next order.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Tools: Pandas, NumPy, Matplotlib, XGBoost, Geopy, Scikit-Learn&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h3&gt;Computer Vision&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/fashion-recommendation&#34;&gt;Fashion Recommendation&lt;/a&gt;: Built a ResNet-based model that classifies and recommends fashion images in the DeepFashion database based on semantic similarity.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/fashion-mnist&#34;&gt;Fashion Classification&lt;/a&gt;: Developed 4 different Convolutional Neural Networks that classify images in the Fashion MNIST dataset.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://medium.com/nanonets/how-to-easily-build-a-dog-breed-image-classification-model-2fd214419cde&#34;&gt;Dog Breed Classification&lt;/a&gt;: Designed a Convolutional Neural Network that identifies dog breed.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef&#34;&gt;Road Segmentation&lt;/a&gt;: Implemented a Fully-Convolutional Network for semantic segmentation task in the Kitty Road Dataset.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Tools: TensorFlow, Keras, Pandas, NumPy, Matplotlib, Scikit-Learn, TensorBoard&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h3&gt;Natural Language Processing&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.wandb.com/articles/classifying-tweets-with-wandb&#34;&gt;Classifying Tweets with Weights &amp;amp; Biases&lt;/a&gt;: Developed 3 different neural network models that classify tweets on a crowdsourced dataset in Figure Eight.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h3&gt;Data Analysis and Visualization&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/world-cup-2018&#34;&gt;World Cup 2018 Team Analysis&lt;/a&gt;: Analysis and visualization of the FIFA 18 dataset to predict the best possible international squad lineups for 10 teams at the 2018 World Cup in Russia.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/khanhnamle1994/spotify-artists-analysis&#34;&gt;Spotify Artists Analysis&lt;/a&gt;: Analysis and visualization of musical styles from 50 different artists with a wide range of genres on Spotify.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Tools: Pandas, NumPy, Matplotlib, Rspotify, httr, dplyr, tidyr, radarchart, ggplot2&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Journalism Portfolio&lt;/h2&gt; &#xA;&lt;p&gt;This section contains portfolio of data journalism articles completed by me for freelance clients and self-learning purposes.&lt;/p&gt; &#xA;&lt;p&gt;For a more visually pleasant experience for browsing the portfolio, check out &lt;a href=&#34;https://jameskle.com/data-journalism&#34;&gt;jameskle.com/data-journalism&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;h3&gt;Statistics&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.kdnuggets.com/2017/11/10-statistical-techniques-data-scientists-need-master.html&#34;&gt;The 10 Statistical Techniques Data Scientists Need to Master&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.datacamp.com/community/tutorials/logistic-regression-R&#34;&gt;Logistic Regression Tutorial&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.datacamp.com/community/tutorials/decision-trees-R&#34;&gt;Decision Trees Tutorial&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.datacamp.com/community/tutorials/support-vector-machines-r&#34;&gt;Support Vector Machines Tutorial&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.topbots.com/data-driven-marketing-for-business-leaders/&#34;&gt;A Friendly Introduction to Data-Driven Marketing for Business Leaders&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h3&gt;Machine Learning&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html&#34;&gt;The 10 Algorithms Machine Learning Engineers Need to Know&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.kdnuggets.com/2018/04/12-useful-things-know-about-machine-learning.html&#34;&gt;12 Useful Things to Know About Machine Learning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://builtin.com/data-science/tour-top-10-algorithms-machine-learning-newbies&#34;&gt;A Tour of The Top 10 Algorithms for Machine Learning Newbie&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://builtin.com/data-science/10-data-mining-techniques-data-scientists-need-their-toolbox&#34;&gt;The 10 Data Mining Techniques Data Scientists Need For Their Toolbox&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://lucidworks.com/2019/01/24/clustering-classification-supervised-unsupervised-learning-ecommerce/&#34;&gt;Clustering and Classification in E-Commerce&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://lucidworks.com/post/abcs-learning-to-rank/&#34;&gt;The ABCs of Learning to Rank&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.wandb.com/articles/debug-ml-model&#34;&gt;6 Ways to Debug a Machine Learning Model&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h3&gt;Deep Learning&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.kdnuggets.com/2017/12/10-deep-learning-methods-ai-practitioners-need-apply.html&#34;&gt;The 10 Deep Learning Methods AI Practitioners Need to Apply&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html&#34;&gt;The 8 Neural Network Architectures ML Researchers Need to Learn&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://heartbeat.fritz.ai/the-5-deep-learning-frameworks-every-serious-machine-learner-should-be-familiar-with-93f4d469d24c&#34;&gt;The 5 Deep Learning Frameworks Every Serious Machine Learner Should Be Familiar With&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b&#34;&gt;The 5 Computer Vision Techniques That Will Change How You See The World&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.codementor.io/@james_aka_yale/convolutional-neural-networks-the-biologically-inspired-model-iq6s48zms&#34;&gt;Convolutional Neural Networks: The Biologically-Inspired Model&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://builtin.com/data-science/recurrent-neural-networks-powerhouse-language-modeling&#34;&gt;Recurrent Neural Networks: The Powerhouse of Language Modeling&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://heartbeat.fritz.ai/the-7-nlp-techniques-that-will-change-how-you-communicate-in-the-future-part-i-f0114b2f0497&#34;&gt;The 7 NLP Techniques That Will Change How You Communicate in the Future&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://heartbeat.fritz.ai/the-5-trends-that-dominated-computer-vision-in-2018-de38fbb9bd86&#34;&gt;The 5 Trends Dominating Computer Vision in 2018&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://heartbeat.fritz.ai/the-3-deep-learning-frameworks-for-end-to-end-speech-recognition-that-power-your-devices-37b891ddc380&#34;&gt;The 3 Deep Learning Frameworks For End-to-End Speech Recognition That Power Your Devices&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://heartbeat.fritz.ai/the-5-algorithms-for-efficient-deep-learning-inference-on-small-devices-bcc2d18aa806&#34;&gt;The 5 Algorithms for Efficient Deep Learning Inference on Small Devices&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://heartbeat.fritz.ai/the-4-research-techniques-to-train-deep-neural-network-models-more-efficiently-810ea2886205&#34;&gt;The 4 Research Techniques to Train Deep Neural Network Models More Efficiently&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://heartbeat.fritz.ai/the-2-types-of-hardware-architectures-for-efficient-training-and-inference-of-deep-neural-networks-a034850e26dd&#34;&gt;The 2 Hardware Architectures for Efficient Training and Inference of Deep Nets&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nanonets.com/blog/10-best-practices-deep-learning/&#34;&gt;10 Deep Learning Best Practices to Keep in Mind in 2020&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Downloadable Cheatsheets&lt;/h2&gt; &#xA;&lt;p&gt;These PDF cheatsheets come from &lt;a href=&#34;https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-science-pdf-f22dc900d2d7&#34;&gt;BecomingHuman.AI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;1 - Neural Network Basics&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/Neural_Nets_Basics.png&#34; alt=&#34;Neural Network Basics&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;2 - Neural Network Graphs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/Neural_Nets_Graphs.png&#34; alt=&#34;Neural Network Graphs&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;3 - Machine Learning with Emojis&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/ML_In_Emoji.png&#34; alt=&#34;Machine Learning with Emojis&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;4 - Scikit-Learn With Python&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/Scikit_Learn_With_Python.png&#34; alt=&#34;Scikit-Learn With Python&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;5 - Python Basics&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/Python_Basics.png&#34; alt=&#34;Python Basics&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;6 - NumPy Basics&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/NumPy_Basics.png&#34; alt=&#34;NumPy Basics&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;7 - Pandas Basics&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/Pandas_Basics.png&#34; alt=&#34;Pandas Basics&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;8 - Data Wrangling With Pandas&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/Data_Wrangling_With_Pandas_Part1.png&#34; alt=&#34;Data Wrangling With Pandas Part 1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/Data_Wrangling_With_Pandas_Part2.png&#34; alt=&#34;Data Wrangling With Pandas Part 2&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;9 - SciPy Linear Algebra&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/SciPy_Linear_Algebra.png&#34; alt=&#34;SciPy Linear Algebra&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;10 - Matplotlib Basics&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/Matplotlib_Basics.png&#34; alt=&#34;Matplotlib Basics&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;11 - Keras&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/Keras.png&#34; alt=&#34;Keras&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;12 - Big-O&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/khanhnamle1994/cracking-the-data-science-interview/master/Big-O.png&#34; alt=&#34;Big-O&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ai-forever/ru-dalle</title>
    <updated>2022-06-16T01:44:08Z</updated>
    <id>tag:github.com,2022-06-16:/ai-forever/ru-dalle</id>
    <link href="https://github.com/ai-forever/ru-dalle" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generate images from texts. In Russian&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ruDALL-E&lt;/h1&gt; &#xA;&lt;h3&gt;Generate images from texts&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache-blue.svg?sanitize=true&#34; alt=&#34;Apache license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/rudalle&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/rudalle&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/sberbank-ai/ru-dalle&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/sberbank-ai/ru-dalle/branch/master/graphs/badge.svg?sanitize=true&#34; alt=&#34;Coverage Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitlab.com/shonenkov/ru-dalle/-/pipelines&#34;&gt;&lt;img src=&#34;https://gitlab.com/shonenkov/ru-dalle/badges/master/pipeline.svg?sanitize=true&#34; alt=&#34;pipeline&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://results.pre-commit.ci/latest/github/sberbank-ai/ru-dalle/master&#34;&gt;&lt;img src=&#34;https://results.pre-commit.ci/badge/github/sberbank-ai/ru-dalle/master.svg?sanitize=true&#34; alt=&#34;pre-commit.ci status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install rudalle==1.1.0rc0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ü§ó HF Models:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/sberbank-ai/rudalle-Malevich&#34;&gt;ruDALL-E Malevich (XL)&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://huggingface.co/sberbank-ai/rudalle-Emojich&#34;&gt;ruDALL-E Emojich (XL)&lt;/a&gt; (readme &lt;a href=&#34;https://github.com/sberbank-ai/ru-dalle/raw/master/Emojich.md&#34;&gt;here&lt;/a&gt;) &lt;br&gt; &lt;a href=&#34;https://huggingface.co/shonenkov-AI/rudalle-xl-surrealist&#34;&gt;ruDALL-E Surrealist (XL)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Minimal Example:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1wGE-046et27oHvNlBNPH07qrEQNE04PQ?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.kaggle.com/shonenkov/rudalle-example-generation&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/anton-l/rudall-e&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example usage ruDALL-E Malevich (XL) with 3.5GB vRAM!&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1AoolDYePUpPkRCKIu0cP9zV7lX5QGD3Z?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Finetuning example&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1Tb7J4PvvegWOybPfUubl5O7m5I24CBg5?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;generation by ruDALLE:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import ruclip&#xA;from rudalle.pipelines import generate_images, show, super_resolution, cherry_pick_by_ruclip&#xA;from rudalle import get_rudalle_model, get_tokenizer, get_vae, get_realesrgan&#xA;from rudalle.utils import seed_everything&#xA;&#xA;# prepare models:&#xA;device = &#39;cuda&#39;&#xA;dalle = get_rudalle_model(&#39;Malevich&#39;, pretrained=True, fp16=True, device=device)&#xA;tokenizer = get_tokenizer()&#xA;vae = get_vae(dwt=True).to(device)&#xA;&#xA;# pipeline utils:&#xA;realesrgan = get_realesrgan(&#39;x2&#39;, device=device)&#xA;clip, processor = ruclip.load(&#39;ruclip-vit-base-patch32-384&#39;, device=device)&#xA;clip_predictor = ruclip.Predictor(clip, processor, device, bs=8)&#xA;text = &#39;—Ä–∞–¥—É–≥–∞ –Ω–∞ —Ñ–æ–Ω–µ –Ω–æ—á–Ω–æ–≥–æ –≥–æ—Ä–æ–¥–∞&#39;&#xA;&#xA;seed_everything(42)&#xA;pil_images = []&#xA;scores = []&#xA;for top_k, top_p, images_num in [&#xA;    (2048, 0.995, 24),&#xA;]:&#xA;    _pil_images, _scores = generate_images(text, tokenizer, dalle, vae, top_k=top_k, images_num=images_num, bs=8, top_p=top_p)&#xA;    pil_images += _pil_images&#xA;    scores += _scores&#xA;&#xA;show(pil_images, 6)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/ru-dalle/master/pics/malevich/rainbow-full.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;auto cherry-pick by ruCLIP:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;top_images, clip_scores = cherry_pick_by_ruclip(pil_images, text, clip_predictor, count=6)&#xA;show(top_images, 3)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/ru-dalle/master/pics/malevich/rainbow-cherry-pick.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;super resolution:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sr_images = super_resolution(top_images, realesrgan)&#xA;show(sr_images, 3)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/ru-dalle/master/pics/malevich/rainbow-super-resolution.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text, seed = &#39;–∫—Ä–∞—Å–∏–≤–∞—è —Ç—è–Ω –∏–∑ –∞–Ω–∏–º–µ&#39;, 6955&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/ru-dalle/master/pics/malevich/anime-girl-super-resolution.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image Prompt&lt;/h3&gt; &#xA;&lt;p&gt;see &lt;code&gt;jupyters/ruDALLE-image-prompts-A100.ipynb&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text, seed = &#39;–•—Ä–∞–º –í–∞—Å–∏–ª–∏—è –ë–ª–∞–∂–µ–Ω–Ω–æ–≥–æ&#39;, 42&#xA;skyes = [red_sky, sunny_sky, cloudy_sky, night_sky]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/ru-dalle/master/pics/malevich/russian-temple-image-prompt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;VideoDALL-E | ru&lt;a href=&#34;https://github.com/THUDM/CogVideo&#34;&gt;CogVideo&lt;/a&gt; by &lt;a href=&#34;https://github.com/cene555&#34;&gt;@cene555&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video generation example&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1A_3Oe9r9DP3Ayd6DPvqKHIKlwNfLhVP5?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;strong&gt;Finetuning example&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1R_joYWlvToA24tsa9BFYa2D6ffiMtyVy?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Aspect ratio images &lt;a href=&#34;https://github.com/shonenkov-AI/rudalle-aspect-ratio&#34;&gt;&lt;strong&gt;--&amp;gt;NEW&amp;lt;--&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shonenkov-AI/rudalle-aspect-ratio/main/pics/h_example.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;üöÄ Contributors üöÄ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bes-dev&#34;&gt;@bes&lt;/a&gt; shared &lt;a href=&#34;https://github.com/bes-dev/vqvae_dwt_distiller.pytorch&#34;&gt;great idea and realization with IDWT&lt;/a&gt; for decoding images with higher quality 512x512! üòàüí™ thanks a lot for your constructive advices, appreciate it&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/neverix&#34;&gt;@neverix&lt;/a&gt; thanks a lot for contributing for speed up of inference&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/boomb0om&#34;&gt;@Igor Pavlov&lt;/a&gt; trained model and prepared code with &lt;a href=&#34;https://github.com/boomb0om/Real-ESRGAN-colab&#34;&gt;super-resolution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oriBetelgeuse&#34;&gt;@oriBetelgeuse&lt;/a&gt; thanks a lot for easy API of generation using image prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AlexWortega&#34;&gt;@Alex Wortega&lt;/a&gt; created first FREE version colab notebook with fine-tuning &lt;a href=&#34;https://huggingface.co/sberbank-ai/rudalle-Malevich&#34;&gt;ruDALL-E Malevich (XL)&lt;/a&gt; on sneakers domain üí™&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/anton-l&#34;&gt;@Anton Lozhkov&lt;/a&gt; Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;, see &lt;a href=&#34;https://huggingface.co/spaces/anton-l/rudall-e&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Supported by&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://airi.net&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sberbank-ai/ru-dolph/master/pics/logo/airi-logo.png&#34; height=&#34;50&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Social Media&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://habr.com/ru/company/sberbank/blog/589673/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/ru-dalle/master/pics/habr_eng.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://habr.com/ru/company/sberdevices/blog/586926/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ai-forever/ru-dalle/master/pics/habr.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>williamyang1991/DualStyleGAN</title>
    <updated>2022-06-16T01:44:08Z</updated>
    <id>tag:github.com,2022-06-16:/williamyang1991/DualStyleGAN</id>
    <link href="https://github.com/williamyang1991/DualStyleGAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2022] Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DualStyleGAN - Official PyTorch Implementation&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/williamyang1991/DualStyleGAN/main/doc_images/overview.jpg&#34; width=&#34;96%&#34; height=&#34;96%&#34;&gt; &#xA;&lt;p&gt;This repository provides the official PyTorch implementation for the following paper:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://williamyang1991.github.io/&#34;&gt;Shuai Yang&lt;/a&gt;, &lt;a href=&#34;https://liming-jiang.com/&#34;&gt;Liming Jiang&lt;/a&gt;, &lt;a href=&#34;https://liuziwei7.github.io/&#34;&gt;Ziwei Liu&lt;/a&gt; and &lt;a href=&#34;https://www.mmlab-ntu.com/person/ccloy/&#34;&gt;Chen Change Loy&lt;/a&gt;&lt;br&gt; In CVPR 2022.&lt;br&gt; &lt;a href=&#34;https://www.mmlab-ntu.com/project/dualstylegan/&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2203.13248&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/watch?v=scZTu77jixI&#34;&gt;&lt;strong&gt;Supplementary Video&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; &lt;em&gt;Recent studies on StyleGAN show high performance on artistic portrait generation by transfer learning with limited data. In this paper, we explore more challenging exemplar-based high-resolution portrait style transfer by introducing a novel &lt;b&gt;DualStyleGAN&lt;/b&gt; with flexible control of dual styles of the original face domain and the extended artistic portrait domain. Different from StyleGAN, DualStyleGAN provides a natural way of style transfer by characterizing the content and style of a portrait with an &lt;b&gt;intrinsic style path&lt;/b&gt; and a new &lt;b&gt;extrinsic style path&lt;/b&gt;, respectively. The delicately designed extrinsic style path enables our model to modulate both the color and complex structural styles hierarchically to precisely pastiche the style example. Furthermore, a novel progressive fine-tuning scheme is introduced to smoothly transform the generative space of the model to the target domain, even with the above modifications on the network architecture. Experiments demonstrate the superiority of DualStyleGAN over state-of-the-art methods in high-quality portrait style transfer and flexible style control.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;br&gt; &lt;strong&gt;High-Resolution&lt;/strong&gt; (1024) | &lt;strong&gt;Training Data-Efficient&lt;/strong&gt; (~200 Images) | &lt;strong&gt;Exemplar-Based Color and Structure Transfer&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[03/2022] Paper and supplementary video are released.&lt;/li&gt; &#xA; &lt;li&gt;[03/2022] Web demo is created.&lt;/li&gt; &#xA; &lt;li&gt;[03/2022] Code is released.&lt;/li&gt; &#xA; &lt;li&gt;[03/2022] This website is created.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Web Demo&lt;/h2&gt; &#xA;&lt;p&gt;Integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces ü§ó&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo: &lt;a href=&#34;https://huggingface.co/spaces/hysts/DualStyleGAN&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/spaces/Gradio-Blocks/DualStyleGAN&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Clone this repo:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/williamyang1991/DualStyleGAN.git&#xA;cd DualStyleGAN&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dependencies:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;All dependencies for defining the environment are provided in &lt;code&gt;environment/dualstylegan_env.yaml&lt;/code&gt;. We recommend running this repository using &lt;a href=&#34;https://docs.anaconda.com/anaconda/install/&#34;&gt;Anaconda&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f ./environment/dualstylegan_env.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We use CUDA 10.1 so it will install PyTorch 1.7.1 (corresponding to &lt;a href=&#34;https://github.com/williamyang1991/DualStyleGAN/raw/main/environment/dualstylegan_env.yaml#L22&#34;&gt;Line 22&lt;/a&gt;, &lt;a href=&#34;https://github.com/williamyang1991/DualStyleGAN/raw/main/environment/dualstylegan_env.yaml#L25&#34;&gt;Line 25&lt;/a&gt;, &lt;a href=&#34;https://github.com/williamyang1991/DualStyleGAN/raw/main/environment/dualstylegan_env.yaml#L26&#34;&gt;Line 26&lt;/a&gt; of &lt;code&gt;dualstylegan_env.yaml&lt;/code&gt;). Please install PyTorch that matches your own CUDA version following &lt;a href=&#34;https://pytorch.org/&#34;&gt;https://pytorch.org/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;(1) Dataset Preparation&lt;/h2&gt; &#xA;&lt;p&gt;Cartoon, Caricature and Anime datasets can be downloaded from their official pages. We also provide the script to build new datasets.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://mega.nz/file/HslSXS4a#7UBanJTjJqUl_2Z-JmAsreQYiJUKC-8UlZDR0rUsarw&#34;&gt;Cartoon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;317 cartoon face images from &lt;a href=&#34;https://github.com/justinpinkney/toonify&#34;&gt;Toonify&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Caricature&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;199 images from &lt;a href=&#34;https://cs.nju.edu.cn/rl/WebCaricature.htm&#34;&gt;WebCaricature&lt;/a&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/williamyang1991/DualStyleGAN/main/data_preparation/readme.md#caricature-dataset&#34;&gt;dataset preparation&lt;/a&gt; for more details.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Anime&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;174 images from &lt;a href=&#34;https://www.gwern.net/Crops#danbooru2019-portraits&#34;&gt;Danbooru Portraits&lt;/a&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/williamyang1991/DualStyleGAN/main/data_preparation/readme.md#anime-dataset&#34;&gt;dataset preparation&lt;/a&gt; for more details.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Other styles&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/williamyang1991/DualStyleGAN/main/data_preparation/readme.md#build-your-own-dataset&#34;&gt;dataset preparation&lt;/a&gt; for the way of building new datasets.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;(2) Inference for Style Transfer and Artistic Portrait Generation&lt;/h2&gt; &#xA;&lt;h3&gt;Inference Notebook&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://colab.research.google.com/github/williamyang1991/DualStyleGAN/blob/master/notebooks/inference_playground.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; height=&#34;22.5&#34;&gt;&lt;/a&gt;&lt;br&gt; To help users get started, we provide a Jupyter notebook found in &lt;code&gt;./notebooks/inference_playground.ipynb&lt;/code&gt; that allows one to visualize the performance of DualStyleGAN. The notebook will download the necessary pretrained models and run inference on the images found in &lt;code&gt;./data/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If no GPU is available, you may refer to &lt;a href=&#34;https://raw.githubusercontent.com/williamyang1991/DualStyleGAN/main/model/stylegan/op_cpu#readme&#34;&gt;Inference on CPU&lt;/a&gt;, and set &lt;code&gt;device = &#39;cpu&#39;&lt;/code&gt; in the notebook.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Models&lt;/h3&gt; &#xA;&lt;p&gt;Pretrained models can be downloaded from &lt;a href=&#34;https://drive.google.com/drive/folders/1GZQ6Gs5AzJq9lUL-ldIQexi0JYPKNy8b?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1sOpPszHfHSgFsgw47S6aAA&#34;&gt;Baidu Cloud&lt;/a&gt; (access code: cvpr):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1NgI4mPkboYvYw3MWcdUaQhkr0OWgs9ej/view?usp=sharing&#34;&gt;encoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pixel2style2pixel encoder that embeds FFHQ images into StyleGAN2 Z+ latent code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1xPo8PcbMXzcUyvwe5liJrfbA5yx4OF1j?usp=sharing&#34;&gt;cartoon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DualStyleGAN and sampling models trained on Cartoon dataset, 317 (refined) extrinsic style codes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1BwLXWkSyWDApblBPvaHKsRCTqnhiHxUZ?usp=sharing&#34;&gt;caricature&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DualStyleGAN and sampling models trained on Caricature dataset, 199 (refined) extrinsic style codes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1YvFj33Bfum4YuBeqNNCYLfiBrD4tpzg7?usp=sharing&#34;&gt;anime&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DualStyleGAN and sampling models trained on Anime dataset, 174 (refined) extrinsic style codes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1-MYwaEQthhAJ_ScWVb0LOQiVkKeSzpBm?usp=sharing&#34;&gt;arcane&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DualStyleGAN and sampling models trained on Arcane dataset, 100 extrinsic style codes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1qC2onFGs2R-XCXRQTP_yyNbY1fT0BdZG?usp=sharing&#34;&gt;comic&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DualStyleGAN and sampling models trained on Comic dataset, 101 extrinsic style codes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1ve4P8Yb4EZ9g_sRy_RCw3N74p46tNpeW?usp=sharing&#34;&gt;pixar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DualStyleGAN and sampling models trained on Pixar dataset, 122 extrinsic style codes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1X345yn_YbMEHBcj7K91O-oQZ2YjVpAcI?usp=sharing&#34;&gt;slamdunk&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DualStyleGAN and sampling models trained on Slamdunk dataset, 120 extrinsic style codes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The saved checkpoints are under the following folder structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;checkpoint&#xA;|--encoder.pt                     % Pixel2style2pixel model&#xA;|--cartoon&#xA;    |--generator.pt               % DualStyleGAN model&#xA;    |--sampler.pt                 % The extrinsic style code sampling model&#xA;    |--exstyle_code.npy           % extrinsic style codes of Cartoon dataset&#xA;    |--refined_exstyle_code.npy   % refined extrinsic style codes of Cartoon dataset&#xA;|--caricature&#xA;    % the same files as in Cartoon&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Exemplar-Based Style Transfer&lt;/h3&gt; &#xA;&lt;p&gt;Transfer the style of a default Cartoon image onto a default face:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python style_transfer.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The result &lt;code&gt;cartoon_transfer_53_081680.jpg&lt;/code&gt; is saved in the folder &lt;code&gt;.\output\&lt;/code&gt;, where &lt;code&gt;53&lt;/code&gt; is the id of the style image in the Cartoon dataset, &lt;code&gt;081680&lt;/code&gt; is the name of the content face image. An corresponding overview image &lt;code&gt;cartoon_transfer_53_081680_overview.jpg&lt;/code&gt; is additionally saved to illustrate the input content image, the encoded content image, the style image (* the style image will be shown only if it is in your folder), and the result:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/williamyang1991/DualStyleGAN/main/output/cartoon_transfer_53_081680_overview.jpg&#34;&gt; &#xA;&lt;p&gt;Specify the style image with &lt;code&gt;--style&lt;/code&gt; and &lt;code&gt;--style_id&lt;/code&gt; (find the mapping between id and filename &lt;a href=&#34;https://raw.githubusercontent.com/williamyang1991/DualStyleGAN/main/data_preparation/id_filename_list.txt&#34;&gt;here&lt;/a&gt;, find the visual mapping between id and the style image &lt;a href=&#34;https://raw.githubusercontent.com/williamyang1991/DualStyleGAN/main/doc_images&#34;&gt;here&lt;/a&gt;). Specify the filename of the saved images with &lt;code&gt;--name&lt;/code&gt;. Specify the weight to adjust the degree of style with &lt;code&gt;--weight&lt;/code&gt;. The following script generates the style transfer results in the teaser of the paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python style_transfer.py&#xA;python style_transfer.py --style cartoon --style_id 10&#xA;python style_transfer.py --style caricature --name caricature_transfer --style_id 0 --weight 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1&#xA;python style_transfer.py --style caricature --name caricature_transfer --style_id 187 --weight 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1&#xA;python style_transfer.py --style anime --name anime_transfer --style_id 17 --weight 0 0 0 0 0.75 0.75 0.75 1 1 1 1 1 1 1 1 1 1 1&#xA;python style_transfer.py --style anime --name anime_transfer --style_id 48 --weight 0 0 0 0 0.75 0.75 0.75 1 1 1 1 1 1 1 1 1 1 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Specify the content image with &lt;code&gt;--content&lt;/code&gt;. If the content image is not well aligned with FFHQ, use &lt;code&gt;--align_face&lt;/code&gt;. For preserving the color style of the content image, use &lt;code&gt;--preserve_color&lt;/code&gt; or set the last 11 elements of &lt;code&gt;--weight&lt;/code&gt; to all zeros.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python style_transfer.py --content ./data/content/unsplash-rDEOVtE7vOs.jpg --align_face --preserve_color \&#xA;       --style arcane --name arcane_transfer --style_id 13 \&#xA;       --weight 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 1 1 1 1 1 1 1 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18130694/159124661-fbb58871-7c7b-449f-95b5-83e44f5973e8.jpg&#34; width=&#34;32%&#34;&gt; ‚Üí &lt;img src=&#34;https://raw.githubusercontent.com/williamyang1991/DualStyleGAN/main/output/arcane_transfer_13_unsplash-rDEOVtE7vOs_overview.jpg&#34; width=&#34;64%&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;More options can be found via &lt;code&gt;python style_transfer.py -h&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Remarks&lt;/strong&gt;: Our trained pSp encoder on Z+ space cannot perfectly encode the content image. If the style transfer result more consistent with the content image is desired, one may use latent optimization to better fit the content image or using other StyleGAN encoders (as discussed in &lt;a href=&#34;https://github.com/williamyang1991/DualStyleGAN/issues/11&#34;&gt;https://github.com/williamyang1991/DualStyleGAN/issues/11&lt;/a&gt; and &lt;a href=&#34;https://github.com/williamyang1991/DualStyleGAN/issues/29&#34;&gt;https://github.com/williamyang1991/DualStyleGAN/issues/29&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Artistic Portrait Generation&lt;/h3&gt; &#xA;&lt;p&gt;Generate random Cartoon face images (Results are saved in the &lt;code&gt;./output/&lt;/code&gt; folder):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python generate.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Specify the style type with &lt;code&gt;--style&lt;/code&gt; and the filename of the saved images with &lt;code&gt;--name&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python generate.py --style arcane --name arcane_generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Specify the weight to adjust the degree of style with &lt;code&gt;--weight&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Keep the intrinsic style code, extrinsic color code or extrinsic structure code fixed using &lt;code&gt;--fix_content&lt;/code&gt;, &lt;code&gt;--fix_color&lt;/code&gt; and &lt;code&gt;--fix_structure&lt;/code&gt;, respectively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python generate.py --style caricature --name caricature_generate --weight 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 --fix_content&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More options can be found via &lt;code&gt;python generate.py -h&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;(3) Training DualStyleGAN&lt;/h2&gt; &#xA;&lt;p&gt;Download the supporting models to the &lt;code&gt;./checkpoint/&lt;/code&gt; folder:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT/view&#34;&gt;stylegan2-ffhq-config-f.pt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;StyleGAN model trained on FFHQ taken from &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;rosinality&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1KW7bjndL3QG3sxBbZxreGHigcCCpsDgn/view?usp=sharing&#34;&gt;model_ir_se50.pth&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Pretrained IR-SE50 model taken from &lt;a href=&#34;https://github.com/TreB1eN/InsightFace_Pytorch&#34;&gt;TreB1eN&lt;/a&gt; for ID loss.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Facial Destylization&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 1: Prepare data.&lt;/strong&gt; Prepare the dataset in &lt;code&gt;./data/DATASET_NAME/images/train/&lt;/code&gt;. First create lmdb datasets:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python ./model/stylegan/prepare_data.py --out LMDB_PATH --n_worker N_WORKER --size SIZE1,SIZE2,SIZE3,... DATASET_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example, download 317 Cartoon images into &lt;code&gt;./data/cartoon/images/train/&lt;/code&gt; and run&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;python ./model/stylegan/prepare_data.py --out ./data/cartoon/lmdb/ --n_worker 4 --size 1024 ./data/cartoon/images/&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 2: Fine-tune StyleGAN.&lt;/strong&gt; Fine-tune StyleGAN in distributed settings:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT finetune_stylegan.py --batch BATCH_SIZE \&#xA;       --ckpt FFHQ_MODEL_PATH --iter ITERATIONS --style DATASET_NAME --augment LMDB_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Take the cartoon dataset for example, run (batch size of 8*4=32 is recommended)&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 finetune_stylegan.py --iter 600 --batch 4 --ckpt ./checkpoint/stylegan2-ffhq-config-f.pt --style cartoon --augment ./data/cartoon/lmdb/&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The fine-tuned model can be found in &lt;code&gt;./checkpoint/cartoon/finetune-000600.pt&lt;/code&gt;. Intermediate results are saved in &lt;code&gt;./log/cartoon/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 3: Destylize artistic portraits.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python destylize.py --model_name FINETUNED_MODEL_NAME --batch BATCH_SIZE --iter ITERATIONS DATASET_NAME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Take the cartoon dataset for example, run:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;python destylize.py --model_name fintune-000600.pt --batch 1 --iter 300 cartoon&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The intrinsic and extrinsic style codes are saved in &lt;code&gt;./checkpoint/cartoon/instyle_code.npy&lt;/code&gt; and &lt;code&gt;./checkpoint/cartoon/exstyle_code.npy&lt;/code&gt;, respectively. Intermediate results are saved in &lt;code&gt;./log/cartoon/destylization/&lt;/code&gt;. To speed up destylization, set &lt;code&gt;--batch&lt;/code&gt; to large value like 16. For styles severely different from real faces, set &lt;code&gt;--truncation&lt;/code&gt; to small value like 0.5 to make the results more photo-realistic (it enables DualStyleGAN to learn larger structrue deformations).&lt;/p&gt; &#xA;&lt;h3&gt;Progressive Fine-Tuning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stage 1 &amp;amp; 2: Pretrain DualStyleGAN on FFHQ.&lt;/strong&gt; We provide our pretrained model &lt;a href=&#34;https://drive.google.com/file/d/1j8sIvQZYW5rZ0v1SDMn2VEJFqfRjMW3f/view?usp=sharing&#34;&gt;generator-pretrain.pt&lt;/a&gt; at &lt;a href=&#34;https://drive.google.com/drive/folders/1GZQ6Gs5AzJq9lUL-ldIQexi0JYPKNy8b?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1sOpPszHfHSgFsgw47S6aAA&#34;&gt;Baidu Cloud&lt;/a&gt; (access code: cvpr). This model is obtained by:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;python -m torch.distributed.launch --nproc_per_node=1 --master_port=8765 pretrain_dualstylegan.py --iter 3000 --batch 4 ./data/ffhq/lmdb/&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;where &lt;code&gt;./data/ffhq/lmdb/&lt;/code&gt; contains the lmdb data created from the FFHQ dataset via &lt;code&gt;./model/stylegan/prepare_data.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stage 3: Fine-Tune DualStyleGAN on Target Domain.&lt;/strong&gt; Fine-tune DualStyleGAN in distributed settings:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python -m torch.distributed.launch --nproc_per_node=N_GPU --master_port=PORT finetune_dualstylegan.py --iter ITERATIONS \ &#xA;                          --batch BATCH_SIZE --ckpt PRETRAINED_MODEL_PATH --augment DATASET_NAME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The loss term weights can be specified by &lt;code&gt;--style_loss&lt;/code&gt; (Œª&lt;sub&gt;FM&lt;/sub&gt;), &lt;code&gt;--CX_loss&lt;/code&gt; (Œª&lt;sub&gt;CX&lt;/sub&gt;), &lt;code&gt;--perc_loss&lt;/code&gt; (Œª&lt;sub&gt;perc&lt;/sub&gt;), &lt;code&gt;--id_loss&lt;/code&gt; (Œª&lt;sub&gt;ID&lt;/sub&gt;) and &lt;code&gt;--L2_reg_loss&lt;/code&gt; (Œª&lt;sub&gt;reg&lt;/sub&gt;). Œª&lt;sub&gt;ID&lt;/sub&gt; and Œª&lt;sub&gt;reg&lt;/sub&gt; are suggested to be tuned for each style dataset to achieve ideal performance. More options can be found via &lt;code&gt;python finetune_dualstylegan.py -h&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Take the Cartoon dataset as an example, run (multi-GPU enables a large batch size of 8*4=32 for better performance):&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 finetune_dualstylegan.py --iter 1500 --batch 4 --ckpt ./checkpoint/generator-pretrain.pt --style_loss 0.25 --CX_loss 0.25 --perc_loss 1 --id_loss 1 --L2_reg_loss 0.015 --augment cartoon&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The fine-tuned models can be found in &lt;code&gt;./checkpoint/cartoon/generator-ITER.pt&lt;/code&gt; where ITER = 001000, 001100, ..., 001500. Intermediate results are saved in &lt;code&gt;./log/cartoon/&lt;/code&gt;. Large ITER has strong cartoon styles but at the cost of artifacts, and users may select the most balanced one from 1000-1500. We use 1400 for our paper experiments.&lt;/p&gt; &#xA;&lt;h3&gt;(optional) Latent Optimization and Sampling&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Refine extrinsic style code.&lt;/strong&gt; Refine the color and structure styles to better fit the example style images.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python refine_exstyle.py --lr_color COLOR_LEARNING_RATE --lr_structure STRUCTURE_LEARNING_RATE DATASET_NAME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the code will load &lt;code&gt;instyle_code.npy&lt;/code&gt;, &lt;code&gt;exstyle_code.npy&lt;/code&gt;, and &lt;code&gt;generator.pt&lt;/code&gt; in &lt;code&gt;./checkpoint/DATASET_NAME/&lt;/code&gt;. Use &lt;code&gt;--instyle_path&lt;/code&gt;, &lt;code&gt;--exstyle_path&lt;/code&gt;, &lt;code&gt;--ckpt&lt;/code&gt; to specify other saved style codes or models. Take the Cartoon dataset as an example, run:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;python refine_exstyle.py --lr_color 0.1 --lr_structure 0.005 --ckpt ./checkpoint/cartoon/generator-001400.pt cartoon&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The refined extrinsic style codes are saved in &lt;code&gt;./checkpoint/DATASET_NAME/refined_exstyle_code.npy&lt;/code&gt;. &lt;code&gt;lr_color&lt;/code&gt; and &lt;code&gt;lr_structure&lt;/code&gt; are suggested to be tuned to better fit the example styles.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training sampling network.&lt;/strong&gt; Train a sampling network to map unit Gaussian noises to the distribution of extrinsic style codes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python train_sampler.py DATASET_NAME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the code will load &lt;code&gt;refined_exstyle_code.npy&lt;/code&gt; or &lt;code&gt;exstyle_code.npy&lt;/code&gt; in &lt;code&gt;./checkpoint/DATASET_NAME/&lt;/code&gt;. Use &lt;code&gt;--exstyle_path&lt;/code&gt; to specify other saved extrinsic style codes. The saved model can be found in &lt;code&gt;./checkpoint/DATASET_NAME/sampler.pt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;(4) Results&lt;/h2&gt; &#xA;&lt;h4&gt;Exemplar-based cartoon style trasnfer&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/18130694/158047991-77c31137-c077-415e-bae2-865ed3ec021f.mp4&#34;&gt;https://user-images.githubusercontent.com/18130694/158047991-77c31137-c077-415e-bae2-865ed3ec021f.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Exemplar-based caricature style trasnfer&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/18130694/158048107-7b0aa439-5e3a-45a9-be0e-91ded50e9136.mp4&#34;&gt;https://user-images.githubusercontent.com/18130694/158048107-7b0aa439-5e3a-45a9-be0e-91ded50e9136.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Exemplar-based anime style trasnfer&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/18130694/158048114-237b8b81-eff3-4033-89f4-6e8a7bbf67f7.mp4&#34;&gt;https://user-images.githubusercontent.com/18130694/158048114-237b8b81-eff3-4033-89f4-6e8a7bbf67f7.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Other styles&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18130694/158049559-5450568f-170d-4847-88e1-d9bd12901966.jpg&#34; width=&#34;48%&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18130694/158049562-e9971b49-ebd9-4300-bd08-34fc2473729f.jpg&#34; width=&#34;48%&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/18130694/158049563-72718807-4bef-472d-8875-71eee22ae934.jpg&#34; width=&#34;48%&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18130694/158049565-0322a005-c402-40bc-8bef-9b22a8ca3fd4.jpg&#34; width=&#34;48%&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this work useful for your research, please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{yang2022Pastiche,&#xA;  title={Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer},&#xA;  author={Yang, Shuai and Jiang, Liming and Liu, Ziwei and Loy, Chen Change},&#xA;  booktitle={CVPR},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;The code is mainly developed based on &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;stylegan2-pytorch&lt;/a&gt; and &lt;a href=&#34;https://github.com/eladrich/pixel2style2pixel&#34;&gt;pixel2style2pixel&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>