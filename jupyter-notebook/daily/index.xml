<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-20T01:38:08Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>SociallyIneptWeeb/LanguageLeapAI</title>
    <updated>2023-03-20T01:38:08Z</updated>
    <id>tag:github.com,2023-03-20:/SociallyIneptWeeb/LanguageLeapAI</id>
    <link href="https://github.com/SociallyIneptWeeb/LanguageLeapAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Your Personal Multilingual AI Translator&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LanguageLeapAI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/docs/screenshots/LanguageLeapAI_logo.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;LEAP across Language barriers by using AI to converse with other online users from across the globe! &lt;strong&gt;LanguageLeapAI&lt;/strong&gt; aims to provide you a real-time language AI assistant that can understand and speak your desired language fluently. &lt;em&gt;(Targeted towards English to Japanese and German as of right now)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Setup Guide: &lt;a href=&#34;https://www.youtube.com/watch?v=bN5UaEkIPGM&#34;&gt;https://www.youtube.com/watch?v=bN5UaEkIPGM&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Showcase: &lt;a href=&#34;https://www.youtube.com/watch?v=UY7sRB60wZ4&#34;&gt;https://www.youtube.com/watch?v=UY7sRB60wZ4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Integration of AI Entities&lt;/h2&gt; &#xA;&lt;p&gt;This project integrates 3 free and open-source AI systems:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;WhisperAI&lt;/a&gt;: General-purpose Speech Recognition Model developed by OpenAI that can perform multilingual speech recognition.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepl.com/translator&#34;&gt;DeepL Translator&lt;/a&gt;: Powered by neural networks and the latest AI innovations for natural-sounding translations&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://voicevox.hiroshiba.jp/&#34;&gt;Voicevox&lt;/a&gt;: Japanese Deep-Learning AI Voice Synthesizer&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;WhisperAI and Voicevox both have docker images available on DockerHub, so we will be building and running them both via a &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/docker-compose.yml&#34;&gt;Docker Compose file&lt;/a&gt;. DeepL can be interacted with by signing up for a free plan and interacting with its &lt;a href=&#34;https://www.deepl.com/pro-api?cta=header-pro-api/&#34;&gt;REST API&lt;/a&gt; up to 500,000 character limit / month. If DeepL is unavailable in your country, an option to use Google Translate instead is available.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/docs/screenshots/ai_integrations.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;LanguageLeapAI&lt;/strong&gt; is made up of 2 main python programs.&lt;/p&gt; &#xA;&lt;h3&gt;Voice Translator&lt;/h3&gt; &#xA;&lt;p&gt;The first, &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/src/voice_translator.py&#34;&gt;voice_translator.py&lt;/a&gt;, records your microphone whenever a push-to-talk key is held down on the keyboard. Once this key is released, it saves your voice in an audio file which is then sent to WhisperAI&#39;s transcribe endpoint which runs Automatic Speech Recognition (ASR) on it. After a response containing your speech as text is received, this text is then translated using DeepL&#39;s REST API.&lt;/p&gt; &#xA;&lt;p&gt;The translated text is then sent to Voicevox which performs text-to-speech and generates an audio file voiced in Japanese. This file is then played to your target application&#39;s microphone input and your speakers/headphones.&lt;/p&gt; &#xA;&lt;p&gt;Since Voicevox only takes in Japanese text as input and generates speech in Japanese, the project is technically only limited to Japanese as the target language. However, Voicevox can be replaced with any other text to speech program that can speak your desired language for limitless possibilities.&lt;/p&gt; &#xA;&lt;p&gt;Thorsten has been added as a German TTS program.&lt;/p&gt; &#xA;&lt;h3&gt;Audio Subtitler&lt;/h3&gt; &#xA;&lt;p&gt;The second, &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/src/subtitler.py&#34;&gt;subtitler.py&lt;/a&gt;, records your application&#39;s audio output and listens in the background for any speech. Once it has detected that a phrase/sentence is complete, it saves the audio into a wav file and sends it to WhisperAI&#39;s translate endpoint which translates the speech from the target language to English.&lt;/p&gt; &#xA;&lt;p&gt;This English text is then displayed on screen using python&#39;s tkinter module, essentially acting as subtitles.&lt;/p&gt; &#xA;&lt;h2&gt;Applications&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;LanguageLeapAI&#39;s&lt;/strong&gt; target audience is for users who want to chat with another but do not speak the same language. An example is an English-speaking user playing an online game in the Japan server but wants to use voice chat despite not knowing Japanese.&lt;/p&gt; &#xA;&lt;p&gt;By running both &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/src/subtitler.py&#34;&gt;subtitler.py&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/src/voice_translator.py&#34;&gt;voice_translator.py&lt;/a&gt;, they can understand their fellow Japanese teammates by reading the english subtitles generated in real time. They can also speak English and the Japanese teammates will instead hear the translated Japanese speech generated by Voicevox.&lt;/p&gt; &#xA;&lt;p&gt;However, this is not the only application of &lt;strong&gt;LanguageLeapAI&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Only using Audio Subtitler&lt;/h3&gt; &#xA;&lt;p&gt;User simply wants to understand what is being said with no need to speak. E.g. Watching a video / stream / movie in another language without subtitles. The user can choose to not run &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/src/voice_translator.py&#34;&gt;voice_translator.py&lt;/a&gt; and simply use &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/src/subtitler.py&#34;&gt;subtitler.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Only using Voice Translator&lt;/h3&gt; &#xA;&lt;p&gt;User understands the language enough to listen and understand, but is afraid to speak the language for various reasons, e.g. Anonymity / Fear of messing up or offending. The user can choose to not run &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/src/subtitler.py&#34;&gt;subtitler.py&lt;/a&gt; and simply use &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/src/voice_translator.py&#34;&gt;voice_translator.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Setting up &lt;strong&gt;LanguageLeapAI&lt;/strong&gt; requires 3 crucial steps, so don&#39;t miss out on any of them!&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/docs/INSTALLATION.md&#34;&gt;Installing Services and Dependencies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/docs/AUDIO.md&#34;&gt;Audio Routing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/docs/ENV.md&#34;&gt;Writing your Environment file&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To run &lt;strong&gt;LanguageLeapAI&lt;/strong&gt;, you need to first run WhisperAI and Voicevox. They can either be run via Docker or using Google Colab.&lt;/p&gt; &#xA;&lt;h3&gt;Google Colab&lt;/h3&gt; &#xA;&lt;p&gt;If your GPU is not powerful enough, you may want to consider running WhisperAI and Voicevox using Google Colab&#39;s GPU.&lt;/p&gt; &#xA;&lt;p&gt;Upload &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/src/run_whisper_n_voicevox.ipynb&#34;&gt;run_whisper_n_voicevox.ipynb&lt;/a&gt; file to Google drive, open the notebook with Google Colab and simply follow the instructions!&lt;/p&gt; &#xA;&lt;p&gt;To run only whisper or voicevox on the cloud: Use either the &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/src/run_whisper_colab.ipynb&#34;&gt;run_whisper_colab.ipynb&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/src/run_voicevox_colab.ipynb&#34;&gt;run_voicevox_colab.ipynb&lt;/a&gt; Colab files instead!&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;h4&gt;Voicevox - JA&lt;/h4&gt; &#xA;&lt;p&gt;If you still want to run both Whisper and Voicevox on your computer, run these commands in the folder containing the &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/docker-compose.yml&#34;&gt;docker-compose.yml&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;To run both WhisperAI and Voicevox:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;docker-compose up -d&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To stop running the containers:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;docker-compose down&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re running Windows Subsystem for Linux (WSL) don&#39;t forget to shut it down to reclaim your ram. This should only after you have stopped the containers and are done using the program.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;wsl --shutdown&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;TTS Thorsten - DE&lt;/h4&gt; &#xA;&lt;p&gt;If you want to run a German version of Voicevox, you need to change the docker-compose file to the corresponding &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/docker-compose-de.yml&#34;&gt;one&lt;/a&gt;. The TTS is the only thing that&#39;s changing, so make sure to also change the &lt;code&gt;TARGET_LANGUAGE_CODE&lt;/code&gt; in your &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/.env.sample&#34;&gt;.env&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;To run both WhisperAI and Thorsten:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;docker-compose -f docker-compose-de.yml up -d&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To stop running the containers:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;docker-compose down&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Python Program&lt;/h3&gt; &#xA;&lt;p&gt;Run these commands in the &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/src&#34;&gt;src/&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;To run the Audio Subtitler:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python subtitler.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run the Voice Translator:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python voice_translator.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To stop the python scripts, simply press &lt;code&gt;Ctrl+C&lt;/code&gt; in the terminal.&lt;/p&gt; &#xA;&lt;h3&gt;Things to note&lt;/h3&gt; &#xA;&lt;p&gt;Some important things to keep in mind while using &lt;strong&gt;LanguageLeapAI&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Whisper&#39;s inconsistency&lt;/h4&gt; &#xA;&lt;p&gt;Do note that WhisperAI is not exactly the most accurate and will not transcribe speech correctly 100% of the time, so use at your own risk. Until OpenAI decides to improve the dataset that was used to train the Whisper models, this will have to do.&lt;/p&gt; &#xA;&lt;p&gt;Also, Whisper is not designed to handle multiple concurrent requests at once. However, for subtitles to be updated in time, multiple requests are being sent asynchronously, so some requests might return an error.&lt;/p&gt; &#xA;&lt;h4&gt;Antivirus Web Protection&lt;/h4&gt; &#xA;&lt;p&gt;If you are running Whisper and Voicevox on the cloud using Google Colab, since we are using ngrok and localtunnel to host our services, the randomised public IP address that they provide might be blacklisted by your antivirus software. If the AI seems to stop working, it may be due to your antivirus blocking the connections to these public IP addresses. You may whitelist these IP addresses or just turn off your antivirus web protection &lt;strong&gt;at your own risk&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Voicevox voices&lt;/h4&gt; &#xA;&lt;p&gt;There are certain terms and conditions for using the voices from Voicevox, so do read up on &lt;a href=&#34;https://voicevox.hiroshiba.jp/&#34;&gt;these&lt;/a&gt; before using a specific speaker.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code of LanguageLeapAI is released under the MIT License. See &lt;a href=&#34;https://raw.githubusercontent.com/SociallyIneptWeeb/LanguageLeapAI/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for further details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Ayanaminn/N46Whisper</title>
    <updated>2023-03-20T01:38:08Z</updated>
    <id>tag:github.com,2023-03-20:/Ayanaminn/N46Whisper</id>
    <link href="https://github.com/Ayanaminn/N46Whisper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Whisper based Japanese subtitle generator&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;N46Whisper&lt;/h1&gt; &#xA;&lt;p&gt;Language : English | &lt;a href=&#34;https://raw.githubusercontent.com/Ayanaminn/N46Whisper/main/README_CN.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;N46Whisper is a Google Colab notebook application that developed for streamlined video subtitle file generation to improve productivity of Nogizaka46 (and Sakamichi groups) subbers.&lt;/p&gt; &#xA;&lt;p&gt;The notebook is based on &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt;, a general-prupose speech recognition model.&lt;/p&gt; &#xA;&lt;p&gt;The output file will be in Advanced SubStation Alpha(ass) format with built-in style of selected sub group so it can be directly imported into &lt;a href=&#34;https://github.com/Aegisub/Aegisub&#34;&gt;Aegisub&lt;/a&gt; for subsequent editing.&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New：&lt;/h2&gt; &#xA;&lt;p&gt;2023.3.15:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add functions to split multiple words/sententces in one line.&lt;/li&gt; &#xA; &lt;li&gt;Update documents and other minor fixes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;2023.3.12:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add chatGPT translation and bilingual subtitle file generation features.&lt;/li&gt; &#xA; &lt;li&gt;Update documents and other minor fixes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;2023.01.26：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Update scripts to reflect recent changes in Whisper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;2022.12.31：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Allow user to select files directly from mounted google drive.&lt;/li&gt; &#xA; &lt;li&gt;Other minor fixes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/Ayanaminn/N46Whisper/blob/main/N46Whisper.ipynb&#34;&gt;Click here&lt;/a&gt; to open the notebook in Google Colab.&lt;/li&gt; &#xA; &lt;li&gt;Upload file and follow the instruction to run the notebook.&lt;/li&gt; &#xA; &lt;li&gt;The ass file will be automatically downloaded once done.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;AI translation&lt;/h2&gt; &#xA;&lt;p&gt;The notebook now allow users to translate transcribed subtitle text line by line using AT translation tools.&lt;/p&gt; &#xA;&lt;p&gt;Currently, it supports &lt;code&gt;chatGPT&lt;/code&gt; and the default target language is &lt;code&gt;zh-CN&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The translated text will be append in the same line after the original text and sepearted by &lt;code&gt;/N&lt;/code&gt;, such that a new bilingual subtitle file is generated.&lt;/p&gt; &#xA;&lt;p&gt;For instance:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/49441654/224525469-18a43cbc-33b9-4b2f-b7ca-7ae0c1865b17.png&#34; alt=&#34;QQ截图20230312155700&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;An example of bilingual subtitle:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/49441654/224525526-51e2123c-6e1c-427c-8d67-9ccd4a7e6630.png&#34; alt=&#34;QQ截图20230312160015&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;To use the AI translation, users must use their own OpenAI API Key. To obtain a free Key, go to &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;https://platform.openai.com/account/api-keys&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please note there will be limitaions on usage for free keys, choose a paid plan to speed up at your own cost.&lt;/p&gt; &#xA;&lt;h2&gt;Split lines&lt;/h2&gt; &#xA;&lt;p&gt;Users can choose to split text in a single line by space.The child lines will have same time stamp with the parent line, respectively.&lt;/p&gt; &#xA;&lt;p&gt;For instance, for a line contains multiple long sentences:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Dialogue: 0,0:01:00.52,0:01:17.52,default,,0,0,0,,Birthday Liveについて話そうかなと思います よろしくお願いします&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;After split:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Dialogue: 0,0:01:00.52,0:01:17.52,default,,0,0,0,,Birthday Liveについて話そうかなと思います(adjust_required)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Dialogue: 0,0:01:00.52,0:01:17.52,default,,0,0,0,,ろしくお願いします(adjust_required)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;The application could significantly reduce the labour and time costs of sub-groups or individual subbers. However, despite its impressive performance, the Whisper model and the application itself are not without limitations.Please read the orgininal documents and Discussions to learn more about the usage of Whisper and the common issues.&lt;/p&gt; &#xA;&lt;p&gt;However, if you have any throughts, requests or questions that directly related to making subtitiles for Sakamichi group girls, please feel free to post here or &lt;a href=&#34;mailto:admin@ikedateresa.cc&#34;&gt;contact me&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code is released under the MIT license. See &lt;a href=&#34;https://raw.githubusercontent.com/Ayanaminn/N46Whisper/main/LICENSE.md&#34;&gt;License&lt;/a&gt; for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sunnysavita10/Statistics-With-Python-TheCompleteGuide</title>
    <updated>2023-03-20T01:38:08Z</updated>
    <id>tag:github.com,2023-03-20:/sunnysavita10/Statistics-With-Python-TheCompleteGuide</id>
    <link href="https://github.com/sunnysavita10/Statistics-With-Python-TheCompleteGuide" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository is entirely dedicated to statistics and the Python implementation of statistics.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Statistics_With_Python&lt;/h1&gt; &#xA;&lt;p&gt;This repository is entirely dedicated to statistics and the Python implementation of statistics.&lt;/p&gt;</summary>
  </entry>
</feed>