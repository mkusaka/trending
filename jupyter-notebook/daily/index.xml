<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-19T01:34:33Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>open-spaced-repetition/fsrs4anki</title>
    <updated>2022-12-19T01:34:33Z</updated>
    <id>tag:github.com,2022-12-19:/open-spaced-repetition/fsrs4anki</id>
    <link href="https://github.com/open-spaced-repetition/fsrs4anki" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A modern Anki custom scheduling based on free spaced repetition scheduler algorithm&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FSRS4Anki&lt;/h1&gt; &#xA;&lt;p&gt;FSRS4Anki is an Anki &lt;a href=&#34;https://faqs.ankiweb.net/the-2021-scheduler.html#add-ons-and-custom-scheduling&#34;&gt;custom scheduling&lt;/a&gt; implementing the &lt;a href=&#34;https://github.com/open-spaced-repetition/fsrs4anki/wiki/Free-Spaced-Repetition-Scheduler&#34;&gt;Free Spaced Repetition Scheduler&lt;/a&gt;. FSRS4Anki consists of two main parts: scheduler and optimizer.&lt;/p&gt; &#xA;&lt;p&gt;The scheduler is based on a variant of the DSR (Difficulty, Stability, Retrievability) model, which is used to predict memory states. The scheduler aims to achieve the requested retention for each card and each review.&lt;/p&gt; &#xA;&lt;p&gt;The optimizer applies &lt;em&gt;Maximum Likelihood Estimation&lt;/em&gt; and &lt;em&gt;Backpropagation Through Time&lt;/em&gt; to estimate the stability of memory and learn the laws of memory from time-series review logs.&lt;/p&gt; &#xA;&lt;p&gt;For more detail on the mechanism of the FSRS algorithm, please see this paper: &lt;a href=&#34;https://www.maimemo.com/paper/&#34;&gt;A Stochastic Shortest Path Algorithm for Optimizing Spaced Repetition Scheduling&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;For the tutorial on FSRS4Anki scheduler, optimizer, helper, and simulator, please see: &lt;a href=&#34;https://github.com/open-spaced-repetition/fsrs4anki/wiki/Usage&#34;&gt;Usage&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;Here collect some questions from issues, forums, and others: &lt;a href=&#34;https://github.com/open-spaced-repetition/fsrs4anki/wiki/FAQ&#34;&gt;FAQ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;Some add-ons modify the scheduling of Anki, which would cause conflict with FSRS4Anki scheduler. Please see &lt;a href=&#34;https://github.com/open-spaced-repetition/fsrs4anki/wiki/Compatibility&#34;&gt;Compatibility&lt;/a&gt; for more details. I will test these add-ons. Let me know via &lt;a href=&#34;https://github.com/open-spaced-repetition/fsrs4anki/issues&#34;&gt;issues&lt;/a&gt; if I miss any add-ons.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#open-spaced-repetition/fsrs4anki&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=open-spaced-repetition/fsrs4anki&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PacktPublishing/The-Machine-Learning-Solutions-Architect-Handbook</title>
    <updated>2022-12-19T01:34:33Z</updated>
    <id>tag:github.com,2022-12-19:/PacktPublishing/The-Machine-Learning-Solutions-Architect-Handbook</id>
    <link href="https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-Handbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Machine Learning Solutions Architect Handbook, published by Packt&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;Get this product for $5&lt;/h3&gt; &#xA;&lt;p&gt;&lt;i&gt;Packt is having its biggest sale of the year. Get this eBook or any other book, video, or course that you like just for $5 each&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://packt.link/9781801072168&#34;&gt;Buy now&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://subscription.packtpub.com/search&#34;&gt;Buy similar titles for just $5&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h1&gt;The Machine Learning Solutions Architect Handbook&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.packtpub.com/product/the-machine-learning-solutions-architect-handbook/9781801072168?utm_source=github&amp;amp;utm_medium=repository&amp;amp;utm_campaign=9781801072168&#34;&gt;&lt;img src=&#34;https://static.packt-cdn.com/products/9781801072168/cover/smaller&#34; alt=&#34;The Machine Learning Solutions Architect Handbook&#34; height=&#34;256px&#34; align=&#34;right&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the code repository for &lt;a href=&#34;https://www.packtpub.com/product/the-machine-learning-solutions-architect-handbook/9781801072168?utm_source=github&amp;amp;utm_medium=repository&amp;amp;utm_campaign=9781801072168&#34;&gt;The Machine Learning Solutions Architect Handbook&lt;/a&gt;, published by Packt.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Create machine learning platforms to run solutions in an enterprise setting&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is this book about?&lt;/h2&gt; &#xA;&lt;p&gt;As machine learning becomes increasingly important across different industries, organizations need to build secure and scalable ML platforms. This handbook demonstrates the entire process, including data science, system architecture, and ML governance to help you become a professional ML solutions architect.&lt;/p&gt; &#xA;&lt;p&gt;This book covers the following exciting features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apply ML methodologies to solve business problems&lt;/li&gt; &#xA; &lt;li&gt;Design a practical enterprise ML platform architecture&lt;/li&gt; &#xA; &lt;li&gt;Implement MLOps for ML workflow automation&lt;/li&gt; &#xA; &lt;li&gt;Build an end-to-end data management architecture using AWS&lt;/li&gt; &#xA; &lt;li&gt;Train large-scale ML models and optimize model inference latency&lt;/li&gt; &#xA; &lt;li&gt;Create a business application using an AI service and a custom ML model&lt;/li&gt; &#xA; &lt;li&gt;Use AWS services to detect data and model bias and explain models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you feel this book is for you, get your &lt;a href=&#34;https://www.amazon.com/dp/1801072167&#34;&gt;copy&lt;/a&gt; today!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.packtpub.com/?utm_source=github&amp;amp;utm_medium=banner&amp;amp;utm_campaign=GitHubBanner&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PacktPublishing/GitHub/master/GitHub.png&#34; alt=&#34;https://www.packtpub.com/&#34; border=&#34;5&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Instructions and Navigations&lt;/h2&gt; &#xA;&lt;p&gt;All of the code is organized into folders.&lt;/p&gt; &#xA;&lt;p&gt;The code will look like the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;churn_data = pd.read_csv(&#34;churn.csv&#34;)&#xA;churn_data.head()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Following is what you need for this book:&lt;/strong&gt; This book is for data scientists, data engineers, cloud architects, and machine learning enthusiasts who want to become machine learning solutions architects. Basic knowledge of the Python programming language, AWS, linear algebra, probability, and networking concepts is assumed.&lt;/p&gt; &#xA;&lt;p&gt;With the following software and hardware list you can run all code files present in the book (Chapter 3-12).&lt;/p&gt; &#xA;&lt;h3&gt;Software and Hardware List&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Chapter&lt;/th&gt; &#xA;   &lt;th&gt;Software required&lt;/th&gt; &#xA;   &lt;th&gt;OS required&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3-12&lt;/td&gt; &#xA;   &lt;td&gt;Angular 9&lt;/td&gt; &#xA;   &lt;td&gt;Windows, Mac OS X, and Linux (Any)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3-12&lt;/td&gt; &#xA;   &lt;td&gt;TypeScript 3.7&lt;/td&gt; &#xA;   &lt;td&gt;Windows, Mac OS X, and Linux (Any)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3-12&lt;/td&gt; &#xA;   &lt;td&gt;ECMAScript 11&lt;/td&gt; &#xA;   &lt;td&gt;Windows, Mac OS X, and Linux (Any)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We also provide a PDF file that has color images of the screenshots/diagrams used in this book. &lt;a href=&#34;https://static.packt-cdn.com/downloads/9781801072168_ColorImages.pdf&#34;&gt;Click here to download it&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Related products &#xA; &lt;other books you may enjoy&gt;&lt;/other&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Machine Learning with Amazon SageMaker Cookbook &lt;a href=&#34;https://www.packtpub.com/product/machine-learning-with-amazon-sagemaker-cookbook/9781800567030?utm_source=github&amp;amp;utm_medium=repository&amp;amp;utm_campaign=9781800567030&#34;&gt;[Packt]&lt;/a&gt; &lt;a href=&#34;https://www.amazon.com/dp/1800567030&#34;&gt;[Amazon]&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Amazon SageMaker Best Practices &lt;a href=&#34;https://www.packtpub.com/product/amazon-sagemaker-best-practices/9781801070522?utm_source=github&amp;amp;utm_medium=repository&amp;amp;utm_campaign=9781801070522&#34;&gt;[Packt]&lt;/a&gt; &lt;a href=&#34;https://www.amazon.com/dp/1801070520&#34;&gt;[Amazon]&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Get to Know the Author&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;David Ping&lt;/strong&gt; is a senior technology leader with over 25 years of experience in the technology and financial services industry. His technology focus areas include cloud architecture, enterprise ML platform design, large-scale model training, intelligent document processing, intelligent media processing, intelligent search, and data platforms. He currently leads an AI/ML solutions architecture team at AWS, where he helps global companies design and build AI/ML solutions in the AWS cloud. Before joining AWS, David held various senior technology leadership roles at Credit Suisse and JPMorgan. He started his career as a software engineer at Intel. David has an engineering degree from Cornell University.&lt;/p&gt; &#xA;&lt;h3&gt;Download a free PDF&lt;/h3&gt; &#xA;&lt;p&gt;&lt;i&gt;If you have already purchased a print or Kindle version of this book, you can get a DRM-free PDF version at no cost.&lt;br&gt;Simply click on the link to claim your free PDF.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://packt.link/free-ebook/9781801072168&#34;&gt;https://packt.link/free-ebook/9781801072168 &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>paperswithcode/galai</title>
    <updated>2022-12-19T01:34:33Z</updated>
    <id>tag:github.com,2022-12-19:/paperswithcode/galai</id>
    <link href="https://github.com/paperswithcode/galai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Model API for GALACTICA&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://github.com/paperswithcode/galai/raw/main/docs/source/img/logo.png#gh-dark-mode-only&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://github.com/paperswithcode/galai/raw/main/docs/source/img/logo_black.png#gh-light-mode-only&#34; width=&#34;400&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/paperswithcode/galai/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/license/paperswithcode/galai.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/paperswithcode/galai/releases&#34;&gt; &lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/github/release/paperswithcode/galai.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GALACTICA&lt;/strong&gt; is a general-purpose scientific language model. It is trained on a large corpus of scientific text and data. It can perform scientific NLP tasks at a high level, as well as tasks such as citation prediction, mathematical reasoning, molecular property prediction and protein annotation. More information is available at &lt;a href=&#34;https://galactica.org&#34;&gt;galactica.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;From pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install galai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/paperswithcode/galai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;There are five GALACTICA models available which we detail below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Parameters&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;mini&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;125 M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.3 B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;standard&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.7 B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;large&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30 B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;huge&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;120 B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import galai as gal&#xA;&#xA;model = gal.load_model(&#34;standard&#34;)&#xA;model.generate(&#34;Scaled dot product attention:\n\n\\[&#34;)&#xA;# Scaled dot product attention:\n\n\\[ \\displaystyle\\text{Attention}(Q,K,V)=\\text{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}}%\n)V \\]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Read the full introduction to Galactica models as a &lt;a href=&#34;https://github.com/paperswithcode/galai/raw/main/notebooks/Introduction%20to%20Galactica%20Models.pdf&#34;&gt;PDF&lt;/a&gt; or a &lt;a href=&#34;https://github.com/paperswithcode/galai/raw/main/notebooks/Introduction%20to%20Galactica%20Models.ipynb&#34;&gt;jupyter notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also find all the model weights with their model cards and inference widget in the &lt;a href=&#34;https://huggingface.co/models?other=galactica&#34;&gt;Hugging Face Hub&lt;/a&gt;. All the models can be used out of the box with the &lt;code&gt;transformers&lt;/code&gt; library.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install transformers accelerate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can run inference using the high-level &lt;code&gt;pipeline&lt;/code&gt; API&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import pipeline&#xA;&#xA;model = pipeline(&#34;text-generation&#34;, model=&#34;facebook/galactica-6.7b&#34;)&#xA;input_text = &#34;The Transformer architecture [START_REF]&#34;&#xA;model(input_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or for more control you can use the lower level &lt;code&gt;OPTForCausalLM&lt;/code&gt; class. See the model cards of the respective repo to learn how to use the model in CPU, GPU, and different precisions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, OPTForCausalLM&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;facebook/galactica-6.7b&#34;)&#xA;model = OPTForCausalLM.from_pretrained(&#34;facebook/galactica-6.7b&#34;, device_map=&#34;auto&#34;)&#xA;&#xA;input_text = &#34;The Transformer architecture [START_REF]&#34;&#xA;input_ids = tokenizer(input_text, return_tensors=&#34;pt&#34;).input_ids.to(&#34;cuda&#34;)&#xA;&#xA;outputs = model.generate(input_ids)&#xA;print(tokenizer.decode(outputs[0]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Capabilities&lt;/h2&gt; &#xA;&lt;p&gt;GALACTICA is a stand-alone LM which is not instruction tuned. Because of this you need to use the correct prompts to get good results. In this note, we go over some of the special tokens, and prompt styles you will need to use to get good results.&lt;/p&gt; &#xA;&lt;p&gt;We demonstrate some examples using the standard (6.7B) model below.&lt;/p&gt; &#xA;&lt;p&gt;üìö &lt;strong&gt;Predict Citations&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;You need to use &lt;code&gt;[START_REF]&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.generate(&#34;The Transformer architecture [START_REF]&#34;)&#xA;# The Transformer architecture [START_REF] Attention is All you Need, Vaswani[END_REF] is a sequence-to-sequence model that uses self-attention to capture long-range dependencies between input and output tokens. The Transformer has been shown to achieve state-of-the-art results on a wide range of natural&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;üî¢ &lt;strong&gt;Predict LaTeX&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.generate(&#34;The Schwarzschild radius is defined as: \\[&#34;)&#xA;# The Schwarzschild radius is defined as: \\[r_{s}=\\frac{2GM}{c^{2}}\\]\n\nwhere \\(G\\) is the gravitational constant, \\(M\\) is the mass of the black hole, and&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ü§î &lt;strong&gt;Reasoning&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;Reasoning uses the special &lt;code&gt;&amp;lt;work&amp;gt;&lt;/code&gt; token:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.generate(&#34;A force of 0.6N is applied to an object, which accelerates at 3m/s. What is its mass? &amp;lt;work&amp;gt;&#34;)&#xA;# What force should be applied to accelerate an object of mass 3kg to 10m/s? &amp;lt;work&amp;gt;\nWe can use Newton&#39;s second law: F = ma. We can substitute variables to get:\n\n\\[ F = \\left(66kg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;‚öõÔ∏è &lt;strong&gt;Generate Molecules&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.generate(&#34;[START_I_SMILES]&#34;, max_length=200)&#xA;# [START_I_SMILES]CCC1=CC=C(C=C1)C(=O)NC2=CC=CC(=C2)C(=O)NC3=CC=C(C=C3)S(=O)(=O)N[END_I_SMILES]\n\n### Molecular Formula\n\nC22H21N3O4S\n\n## Chemical and Physical Properties\n\nThe following are chemical properties for 3-[[3-(4-ethylphenyl)-3-oxo-propanoyl]amino]-N-(4-sulfamoylphenyl)benzamide.\n\n### Computed Properties\n\n| Property Name | Property Value\n| --- | ----------- |\n| Molecular Weight | 423.5\n| XLogP3-AA Log P | 3.2\n| Hydrogen Bond Donor Count | 3\n| Hydrogen Bond Acceptor Count &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;üßë‚Äçüî¨ &lt;strong&gt;Predict Protein Annotations&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.generate(&#34;[START_AMINO]GHMQSITAGQKVISKHKNGRFYQCEVVRLTTETFYEVNFDDGSFSDNLYPEDIVSQDCLQFGPPAEGEVVQVRWTDGQVYGAKFVASHPIQMYQVEFEDGSQLVVKRDDVYTLDEELP[END_AMINO] ## Keywords&#34;, max_length=200)&#xA;# &#39;[START_AMINO]GHMQSITAGQKVISKHKNGRFYQCEVVRLTTETFYEVNFDDGSFSDNLYPEDIVSQDCLQFGPPAEGEVVQVRWTDGQVYGAKFVASHPIQMYQVEFEDGSQLVVKRDDVYTLDEELP[END_AMINO] ## Keywords\n\nCytoplasm, Methyltransferase, rRNA processing, S-adenosyl-L-methionine, Transferase\n\n## References\n\nQuestion: What are some articles for Ribosomal RNA small subunit methyltransferase H?\n\nAnswer: \n\n[START_REF] Comparative Genomics of 28 Salmonella enterica Isolates: Evidence for CRISPR-Mediated Adaptive Sublineage Evolution, Fricke[END_REF]\n\n&amp;lt;/s&amp;gt;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;üñ±Ô∏è &lt;strong&gt;Free-Form Generation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want autocomplete based functionality, it is often good to experiment with turning off &lt;code&gt;new_doc=True&lt;/code&gt;. This makes it more likely for the model to think it is in the middle of a document, as opposed to the beginning.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.generate(&#34;The reason why Transformers replaced RNNs was because&#34;, new_doc=False)&#xA;# The reason why Transformers replaced RNNs was because they were able to capture long-term dependencies in the input sequence.\n\n# 2.2.2. Attention Mechanism\n\nThe attention mechanism was introduced in [START_REF] Neural Machine Translation by Jointly Learning to Align and Translate, Bahdan&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;‚ùì &lt;strong&gt;Question Answering&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the paper we prefix questions with &#34;Q:&#34; or &#34;Question:&#34;. A typical format is &#34;Question: question.\n\nAnswer:&#34;, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.generate(&#34;Question: What is the notch signaling pathway?\n\nAnswer:&#34;)&#xA;# &#39;Question: What is the notch signaling pathway?\n\nAnswer: \n\nNotch signaling pathway is a cell-cell communication pathway that regulates cell fate decisions during development. It is involved in cell proliferation, differentiation, apoptosis, and cell migration. The Notch signaling pathway is activated by the binding of&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;üìÑ &lt;strong&gt;Documents&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;When starting a document, you must use the start document token for good results. To do this, set &lt;code&gt;new_doc=True&lt;/code&gt; in generate:&lt;/p&gt; &#xA;&lt;p&gt;For some article types, like Wikipedia style articles, lecture notes and GitHub repositories, use &lt;code&gt;#&lt;/code&gt; to begin, e.g:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.generate(&#34;# Multi-Head Attention\n\n&#34;, new_doc=True)&#xA;# # Multi-Head Attention\n\nThe multi-head attention mechanism is a generalization of the single-head attention mechanism. The multi-head attention mechanism is a combination of multiple single-head attention mechanisms. The multi-head attention mechanism is shown in Figure 2.\n\nThe multi-&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For paper documents, use Title, e.g:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.generate(&#34;Title: Self-Supervised Learning, A Survey\n\nAuthors: John Smith\n\n&#34;, new_doc=True)&#xA;# Title: Self-Supervised Learning, A Survey\n\nAuthors: John Smith\n\n# Abstract\n\nSelf-supervised learning is a class of machine learning methods that learn representations of data without the need for human-provided labels.\nIn this survey, we provide a comprehensive overview of the field&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also try alternative sampling techniques for less repetitions, e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.generate(&#34;Lecture 1: The Ising Model\n\n&#34;, new_doc=True, top_p=0.7, max_length=200)&#xA;# &#39;Lecture 1: The Ising Model\n\n# 13 Introduction\n\nWe will now look at a simple model for magnetism, the Ising model, which is\na lattice model in which we consider only two spin values, up or down, and\nwe want to understand how these spins interact with each other and how\nthey get arranged in a particular state.\n\nWe will first consider the one-dimensional case, and then move on to\nthe case of two-dimensional lattices, and then to higher dimensions.\n\n# 14 The One-Dimensional Ising Model\n\n# 14.1 The Model\n\nThe one-dimensional Ising model is the simplest case of the model, in\nwhich the lattice is a line of \\(N\\) spins, each with two possible spin\nvalues, up or down. In other words, we consider a line of \\(N\\) spins\nwhere each spin can point up or down&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;üìú &lt;strong&gt;Summarization&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can add &#34;TLDR:&#34; for TLDR summaries:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TEXT = &#34;&#34;&#34;Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.&#34;&#34;&#34;&#xA;&#xA;model.generate(TEXT + &#34;\n\nTLDR:&#34;, max_length=400)&#xA;# ...TLDR: We introduce Galactica, a large language model that can store, combine and reason about scientific knowledge.&amp;lt;/s&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;üíé &lt;strong&gt;Entity extraction&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can extract entities from documents. We use the abstract example (&lt;code&gt;TEXT&lt;/code&gt;) from the previous section, and add questions&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ENT_TEXT = TEXT + &#39;\n\nWhat scientific entities are mentioned in the abstract above?\n\n&#39;&#xA;&#xA;model.generate(ENT_TEXT, max_length=400)&#xA;# ...What scientific entities are mentioned in the abstract above?\n\nA: LaTeX equations, mathematical MMLU, MATH, PubMedQA, MedMCQA, BIG-bench&amp;lt;/s&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;üë®‚Äçüî¨ &lt;strong&gt;IUPAC Name prediction&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For this task, we used a prompt based off the PubChem document and prompted for the completion. We use the 6.7bn model for below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;context = &#34;[START_I_SMILES]C(C(=O)O)N[END_I_SMILES]\n\n## Chemical and Physical Properties\n\nThe following are chemical properties for&#34;&#xA;model.generate(context, max_length=400)&#xA;# [START_I_SMILES]C(C(=O)O)N[END_I_SMILES]\n\n## Chemical and Physical Properties\n\nThe following are chemical properties for 2-amino-2-oxo-acetic acid&#xA;# Note this is an incorrect prediction&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{GALACTICA,&#xA;    title={GALACTICA: A Large Language Model for Science},&#xA;    author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},&#xA;    year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>