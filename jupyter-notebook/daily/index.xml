<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-17T01:34:55Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lkwq007/stablediffusion-infinity</title>
    <updated>2022-09-17T01:34:55Z</updated>
    <id>tag:github.com,2022-09-17:/lkwq007/stablediffusion-infinity</id>
    <link href="https://github.com/lkwq007/stablediffusion-infinity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Outpainting with Stable Diffusion on an infinite canvas&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;stablediffusion-infinity&lt;/h1&gt; &#xA;&lt;p&gt;Outpainting with Stable Diffusion on an infinite canvas.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/lkwq007/stablediffusion-infinity/blob/master/stablediffusion_infinity_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Start with init_image:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1665437/190231611-fc263115-0fb9-4f2d-a71b-7e500c1e311d.mp4&#34;&gt;https://user-images.githubusercontent.com/1665437/190231611-fc263115-0fb9-4f2d-a71b-7e500c1e311d.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Start with text2img:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1665437/190212025-f4a82c46-0ff1-4ca2-b79b-6c81601e3eed.mp4&#34;&gt;https://user-images.githubusercontent.com/1665437/190212025-f4a82c46-0ff1-4ca2-b79b-6c81601e3eed.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;It is recommended to run the notebook on a local server for better interactive control.&lt;/p&gt; &#xA;&lt;p&gt;The notebook might work on Windows (see this issue &lt;a href=&#34;https://github.com/lkwq007/stablediffusion-infinity/issues/12&#34;&gt;https://github.com/lkwq007/stablediffusion-infinity/issues/12&lt;/a&gt; for more information) and Apple Silicon devices (untested, check guide here: &lt;a href=&#34;https://huggingface.co/docs/diffusers/optimization/mps&#34;&gt;https://huggingface.co/docs/diffusers/optimization/mps&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Setup environment&lt;/h2&gt; &#xA;&lt;p&gt;setup with &lt;code&gt;environment.yml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recurse-submodules https://github.com/lkwq007/stablediffusion-infinity&#xA;cd stablediffusion-infinity&#xA;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if the &lt;code&gt;environment.yml&lt;/code&gt; doesn&#39;t work for you, you may install dependencies manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n sd-inf python=3.10&#xA;conda activate sd-inf&#xA;conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch&#xA;conda install scipy&#xA;conda install -c conda-forge jupyterlab&#xA;conda install -c conda-forge ipywidgets=7.7.1&#xA;conda install -c conda-forge ipycanvas&#xA;conda install -c conda-forge diffusers transformers ftfy&#xA;pip install opencv-python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For windows, you may need to replace &lt;code&gt;pip install opencv-python&lt;/code&gt; with &lt;code&gt;conda install -c conda-forge opencv&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;CPP library (optional)&lt;/h2&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;opencv&lt;/code&gt; library (e.g. &lt;code&gt;libopencv-dev&lt;/code&gt;/&lt;code&gt;opencv-devel&lt;/code&gt;, the package name may differ on different distributions) is required for &lt;code&gt;PyPatchMatch&lt;/code&gt;. You may need to install &lt;code&gt;opencv&lt;/code&gt; by yourself. If no &lt;code&gt;opencv&lt;/code&gt; installed, the &lt;code&gt;patch_match&lt;/code&gt; option (usually better quality) won&#39;t work.&lt;/p&gt; &#xA;&lt;h2&gt;How-to&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate sd-inf&#xA;huggingface-cli login # ignore this if you have already logged in&#xA;jupyter lab&#xA;# and then open stablediffusion_infinity.ipynb and run cells&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Troubleshooting on Windows: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/lkwq007/stablediffusion-infinity/issues/12&#34;&gt;https://github.com/lkwq007/stablediffusion-infinity/issues/12&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;False positive rate of safety checker is quite high: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/lkwq007/stablediffusion-infinity/issues/8#issuecomment-1248448453&#34;&gt;https://github.com/lkwq007/stablediffusion-infinity/issues/8#issuecomment-1248448453&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;What is the init_mode &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;init_mode indicates how to fill the empty/masked region, usually &lt;code&gt;patch_match&lt;/code&gt; is better than others&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The GUI is lagging on colab &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;It is recommended to run the notebook on a local server since the interactions and canvas content updates are actually handled by the python backend on the serverside, and that&#39;s how &lt;code&gt;ipycanvas&lt;/code&gt; works&lt;/li&gt; &#xA;   &lt;li&gt;colab doesn&#39;t support the latest version of &lt;code&gt;ipycanvas&lt;/code&gt;, which may have better performance&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>MrTornado24/IDE-3D</title>
    <updated>2022-09-17T01:34:55Z</updated>
    <id>tag:github.com,2022-09-17:/MrTornado24/IDE-3D</id>
    <link href="https://github.com/MrTornado24/IDE-3D" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[SIGGRAPH Asia 2022] IDE-3D: Interactive Disentangled Editing For High-Resolution 3D-aware Portrait Synthesis&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis&lt;/h1&gt; &#xA;&lt;h2&gt;SIGGRAPH ASIA 2022 (ToG)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MrTornado24/IDE-3D/main/docs/rep.png&#34; alt=&#34;Teaser image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.15517&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2108.00946-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://mrtornado24.github.io/&#34;&gt;Jingxiang Sun&lt;/a&gt;, &lt;a href=&#34;https://xuanwangvc.github.io/&#34;&gt;Xuan Wang&lt;/a&gt;, &lt;a href=&#34;https://seasonsh.github.io/&#34;&gt;Yichun Shi&lt;/a&gt;, &lt;a href=&#34;https://lizhenwangt.github.io/&#34;&gt;Lizhen Wang&lt;/a&gt;, &lt;a href=&#34;https://juewang725.github.io/&#34;&gt;Jue Wang&lt;/a&gt;, &lt;a href=&#34;http://www.liuyebin.com/&#34;&gt;Yebin Liu&lt;/a&gt;&lt;br&gt;&lt;br&gt; &lt;br&gt;&lt;a href=&#34;https://mrtornado24.github.io/IDE-3D/&#34;&gt;https://mrtornado24.github.io/IDE-3D/&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Abstract: &lt;em&gt;Existing 3D-aware facial generation methods face a dilemma in quality versus editability: they either generate editable results in low resolution, or high quality ones with no editing flexibility. In this work, we propose a new approach that brings the best of both worlds together. Our system consists of three major components: (1) a 3D-semantics-aware generative model that produces view-consistent, disentangled face images and semantic masks; (2) a hybrid GAN inversion approach that initialize the latent codes from the semantic and texture encoder, and further optimized them for faithful reconstruction; and (3) a canonical editor that enables efficient manipulation of semantic masks in canonical view and producs high quality editing results. Our approach is competent for many applications, e.g. free-view face drawing, editing and style control. Both quantitative and qualitative results show that our method reaches the state-of-the-art in terms of photorealism, faithfulness and efficiency.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone --recursive https://github.com/MrTornado24/IDE-3D.git &lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd IDE-3D&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;conda env create -f environment.yml&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Please download our pre-trained checkpoints from &lt;a href=&#34;https://drive.google.com/drive/folders/1-i1WLR5YCXOKjNuQEB6ECf-JxMyvqC4l?usp=sharing&#34;&gt;link&lt;/a&gt; and put them under &lt;code&gt;pretrained_models/&lt;/code&gt;. The link mainly contains the pretrained generator &lt;code&gt;ide3d-ffhq-64-512.pkl&lt;/code&gt; and the style encoder &lt;code&gt;encoder-base-hybrid.pkl&lt;/code&gt;. More pretrianed models will be released soon.&lt;/p&gt; &#xA;&lt;h2&gt;Semantic-aware image synthesis&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Generate videos using pre-trained model&#xA;&#xA;python gen_videos.py --outdir=out --trunc=0.7 --seeds=0-3 --grid=2x2 \&#xA;    --network=pretrained_models/ide3d-ffhq-64-512.pkl --interpolate 1 --image_mode image_seg&#xA;&#xA;# Generate the same 4 seeds in an interpolation sequence&#xA;&#xA;python gen_videos.py --outdir=out --trunc=0.7 --seeds=0-3 --grid=1x1 \&#xA;    --network=pretrained_models/ide3d-ffhq-64-512.pkl --interpolate 1 --image_mode image_seg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Generate images using pre-trained model&#xA;&#xA;python gen_images.py --outdir=out --trunc=0.7 --seeds=0-3 \&#xA;    --network=pretrained_models/ide3d-ffhq-64-512.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Extract shapes (saved as .mrc and .npy) using pre-trained model&#xA;&#xA;python extract_shapes.py --outdir out --trunc 0.7 --seeds 0-3 \&#xA;    --network networks/network_snapshot.pkl --cube_size 1 &#xA;    &#xA;# Render meshes to video&#xA;&#xA;python render_mesh.py --fname out/0.npy --outdir out&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We visualize our .mrc shape files with &lt;a href=&#34;https://www.cgl.ucsf.edu/chimerax/&#34;&gt;UCSF Chimerax&lt;/a&gt;. Please refer to &lt;a href=&#34;https://github.com/NVlabs/eg3d&#34;&gt;EG3D&lt;/a&gt; for detailed instruction of Chimerax.&lt;/p&gt; &#xA;&lt;h2&gt;Interactive editing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MrTornado24/IDE-3D/main/docs/ui.gif&#34; alt=&#34;UI&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We provide an interactive tool that can be used for 3D-aware face drawing and editng in real-time. Before using it, please install the enviroment with &lt;code&gt;pip install -r ./Painter/requirements.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;python Painter/run_ui.py&#xA;    --g_ckpt pretrained_models/ide3d-ffhq-64-512.pkl &#xA;    --e_ckpt pretrained_models/encoder-base-hybrid.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Preparing datasets&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FFHQ&lt;/strong&gt;: Download and process the &lt;a href=&#34;https://github.com/NVlabs/ffhq-dataset&#34;&gt;Flickr-Faces-HQ dataset&lt;/a&gt; following &lt;a href=&#34;https://github.com/NVlabs/eg3d&#34;&gt;EG3D&lt;/a&gt;. Then, parse semantic masks for all processed images using a &lt;a href=&#34;https://drive.google.com/file/d/17H1JR-UJllJ3TCnEbtJscx_GgupTBtqS/view?usp=sharing&#34;&gt;pretrained parsing model&lt;/a&gt;. The processed data would be placed as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    ├── /path/to/dataset&#xA;    │   ├── masks512x512&#xA;    │   ├── maskscolor512x512&#xA;    │   ├── images512x512&#xA;    │   │   ├── 00000&#xA;                ├──img00000000.png&#xA;    │   │   ├── ...&#xA;    │   │   ├── dataset.json&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Custom dataset&lt;/strong&gt;: You can process your own dataset using the following commands. It would be useful for real portrait image editing.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd dataset_preprocessing/ffhq&#xA;python preprocess_in_the_wild.py --indir=INPUT_IMAGE_FOLDER&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Real portrait image editing&lt;/h2&gt; &#xA;&lt;p&gt;IDE-3D supports 3D-aware real protrait image editing using our interactive tool. Please run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# infer latent code as initialization&#xA;&#xA;python apps/infer_hybrid_encoder.py &#xA;    --target_img /path/to/img_0.png&#xA;    --g_ckpt pretrained_models/ide3d-ffhq-64-512.pkl &#xA;    --e_ckpt pretrained_models/encoder-base-hybrid.pkl&#xA;    --outdir out&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above command would return &lt;code&gt;rec_ws.pt&lt;/code&gt; under &lt;code&gt;out/img_0&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# run pti&#xA;&#xA;python inversion/scripts/run_pti.py &#xA;    --run_name ide3d_plus_initial_code &#xA;    --projector_type ide3d_plus &#xA;    --pivotal_tuning&#xA;    --viz_image &#xA;    --viz_mesh &#xA;    --viz_video &#xA;    --label_path /path/to/dataset.json &#xA;    --image_name img_0&#xA;    --initial_w out/img_0/rec_ws.pt&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We adopt &lt;a href=&#34;https://github.com/danielroich/PTI&#34;&gt;PTI&lt;/a&gt; for 3D inverison. Before running, please place the images into &lt;code&gt;examples/&lt;/code&gt;. You can pass Flag &lt;code&gt;ide3d_plus&lt;/code&gt; or &lt;code&gt;ide3d&lt;/code&gt; to choose different inversion types (&#39;w&#39; and &#39;w+&#39;). Flag &lt;code&gt;initial_w&lt;/code&gt; specifies the latent code obtained from the last step. It benefits more reasonable shape especially for images with steep viewing angles. The command would return pose label &lt;code&gt;label.pt&lt;/code&gt;, reconstructed latent code &lt;code&gt;latent.pt&lt;/code&gt;, finetuned generator and some visualizations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# (optional) finetune encoder&#xA;&#xA;python apps/finetune_hybrid_encoder.py&#xA;    --target_img /path/to/img_0.png&#xA;    --target_code /path/to/latent.pt&#xA;    --target_label /path/to/label.pt &#xA;    --g_ckpt /path/to/finetuned_generator.pt &#xA;    --e_ckpt pretrained_models/encoder-base-hybrid.pkl &#xA;    --outdir out &#xA;    --max-steps 1000&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This step is to align the shapes reconstructed by encoders and PTI. The finetuned encoder would be saved as &lt;code&gt;finetuned_encoder.pkl&lt;/code&gt;. Besides, a semantic mask &lt;code&gt;mask.png&lt;/code&gt; would be saved under the same folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# run UI&#xA;&#xA;python Painter/run_ui.py&#xA;    --g_ckpt /path/to/finetuned_generator.pt&#xA;    --e_ckpt /path/to/finetuned_encoder.pkl&#xA;    --target_code /path/to/latent.pt&#xA;    --target_label /path/to/label.pt&#xA;    --inversion&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; you should click &lt;code&gt;Open Image&lt;/code&gt; and load &lt;code&gt;mask.png&lt;/code&gt; that is returned in the last step.&lt;/p&gt; &#xA;&lt;h2&gt;3D-aware CLIP-guided domain adaptation&lt;/h2&gt; &#xA;&lt;p&gt;Please obtain the adapted generators following &lt;a href=&#34;https://github.com/MrTornado24/ide3d-nada/raw/main/README.md&#34;&gt;IDE3D-NADA&lt;/a&gt;. You can perform interactive editing in other domains by simply replacing the original generator by the adapted one:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;python Painter/run_ui.py&#xA;    --g_ckpt /path/to/adapted_generator.pt&#xA;    --e_ckpt pretrained_models/encoder-base-hybrid.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Semantic-guided style animation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MrTornado24/IDE-3D/main/docs/animation.png&#34; alt=&#34;Teaser image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;IDE-3D supports animating stylized virtual faces through semantic masks. Please process a video clip and prepare a &lt;code&gt;dataset.json&lt;/code&gt;. Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;&#xA;python apps/infer_face_animation.py &#xA;    --drive_root /path/to/images&#xA;    --network pretrained_models/ide3d-ffhq-64-512.pkl &#xA;    --encoder pretrained_models/encoder-base-hybrid.pkl&#xA;    --grid 4x1 &#xA;    --seeds 52,197,229&#xA;    --outdir out&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;Training scipts will be released soon.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Part of the codes are borrowed from &lt;a href=&#34;https://github.com/NVlabs/stylegan3&#34;&gt;StyleGAN3&lt;/a&gt;, &lt;a href=&#34;https://github.com/danielroich/PTI&#34;&gt;PTI&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVlabs/eg3d&#34;&gt;EG3D&lt;/a&gt; and &lt;a href=&#34;https://github.com/rinongal/StyleGAN-nada&#34;&gt;StyleGAN-nada&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code for your research, please cite the following works:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{sun2022ide,&#xA;  title={IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis},&#xA;  author={Sun, Jingxiang and Wang, Xuan and Shi, Yichun and Wang, Lizhen and Wang, Jue and Liu, Yebin},&#xA;  journal={arXiv preprint arXiv:2205.15517},&#xA;  year={2022}&#xA;}&#xA;&#xA;@inproceedings{sun2022fenerf,&#xA;  title={Fenerf: Face editing in neural radiance fields},&#xA;  author={Sun, Jingxiang and Wang, Xuan and Zhang, Yong and Li, Xiaoyu and Zhang, Qi and Liu, Yebin and Wang, Jue},&#xA;  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},&#xA;  pages={7672--7682},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>jupyter/notebook</title>
    <updated>2022-09-17T01:34:55Z</updated>
    <id>tag:github.com,2022-09-17:/jupyter/notebook</id>
    <link href="https://github.com/jupyter/notebook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Jupyter Interactive Notebook&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Jupyter Notebook&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/jupyter/notebook/workflows/Build/badge.svg?sanitize=true&#34; alt=&#34;Github Actions Status&#34;&gt; &lt;a href=&#34;https://jupyter-notebook.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/jupyter-notebook/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mybinder.org/v2/gh/jupyter/notebook/main?urlpath=tree&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/jupyter/notebook&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/jupyter/notebook/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Jupyter notebook is a web-based notebook environment for interactive computing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jupyter/notebook/main/docs/resources/running_code_med.png&#34; alt=&#34;Jupyter notebook example&#34; title=&#34;Jupyter notebook example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Maintained versions&lt;/h2&gt; &#xA;&lt;p&gt;We maintain the &lt;strong&gt;two most recently released major versions of Jupyter Notebook&lt;/strong&gt;, Notebook v5 and Classic Notebook v6. After Notebook v7.0 is released, we will no longer maintain Notebook v5. All Notebook v5 users are strongly advised to upgrade to Classic Notebook v6 as soon as possible.&lt;/p&gt; &#xA;&lt;p&gt;The Jupyter Notebook project is currently undertaking a transition to a more modern code base built from the ground-up using JupyterLab components and extensions.&lt;/p&gt; &#xA;&lt;p&gt;There is new stream of work which was submitted and then accepted as a Jupyter Enhancement Proposal (JEP) as part of the next version (v7): &lt;a href=&#34;https://jupyter.org/enhancement-proposals/79-notebook-v7/notebook-v7.html&#34;&gt;https://jupyter.org/enhancement-proposals/79-notebook-v7/notebook-v7.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;There is also a plan to continue maintaining Notebook v6 with bug and security fixes only, to ease the transition to Notebook v7: &lt;a href=&#34;https://github.com/jupyter/notebook-team-compass/issues/5#issuecomment-1085254000&#34;&gt;https://github.com/jupyter/notebook-team-compass/issues/5#issuecomment-1085254000&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Notebook v7&lt;/h3&gt; &#xA;&lt;p&gt;The next major version of Notebook will be based on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;JupyterLab components for the frontend&lt;/li&gt; &#xA; &lt;li&gt;Jupyter Server for the Python server&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This represents a significant change to the &lt;code&gt;jupyter/notebook&lt;/code&gt; code base.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about Notebook v7: &lt;a href=&#34;https://jupyter.org/enhancement-proposals/79-notebook-v7/notebook-v7.html&#34;&gt;https://jupyter.org/enhancement-proposals/79-notebook-v7/notebook-v7.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Classic Notebook v6&lt;/h3&gt; &#xA;&lt;p&gt;Maintainance and security-related issues are now being addressed in the &lt;a href=&#34;https://github.com/jupyter/notebook/tree/6.4.x&#34;&gt;&lt;code&gt;6.4.x&lt;/code&gt;&lt;/a&gt; branch.&lt;/p&gt; &#xA;&lt;p&gt;A &lt;code&gt;6.5.x&lt;/code&gt; branch will be soon created and will depend on &lt;a href=&#34;https://github.com/jupyter/nbclassic&#34;&gt;&lt;code&gt;nbclassic&lt;/code&gt;&lt;/a&gt; for the HTML/JavaScript/CSS assets.&lt;/p&gt; &#xA;&lt;p&gt;New features and continuous improvement is now focused on Notebook v7 (see section above).&lt;/p&gt; &#xA;&lt;p&gt;If you have an open pull request with a new feature or if you were planning to open one, we encourage switching over to the Jupyter Server and JupyterLab architecture, and distribute it as a server extension and / or JupyterLab prebuilt extension. That way your new feature will also be compatible with the new Notebook v7.&lt;/p&gt; &#xA;&lt;h2&gt;Jupyter notebook, the language-agnostic evolution of IPython notebook&lt;/h2&gt; &#xA;&lt;p&gt;Jupyter notebook is a language-agnostic HTML notebook application for Project Jupyter. In 2015, Jupyter notebook was released as a part of The Big Split™ of the IPython codebase. IPython 3 was the last major monolithic release containing both language-agnostic code, such as the &lt;em&gt;IPython notebook&lt;/em&gt;, and language specific code, such as the &lt;em&gt;IPython kernel for Python&lt;/em&gt;. As computing spans across many languages, Project Jupyter will continue to develop the language-agnostic &lt;strong&gt;Jupyter notebook&lt;/strong&gt; in this repo and with the help of the community develop language specific kernels which are found in their own discrete repos.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.jupyter.org/the-big-split-9d7b88a031a7&#34;&gt;The Big Split™ announcement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.jupyter.org/jupyter-ascending-1bf5b362d97e&#34;&gt;Jupyter Ascending blog post&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can find the installation documentation for the &lt;a href=&#34;https://jupyter.readthedocs.io/en/latest/install.html&#34;&gt;Jupyter platform, on ReadTheDocs&lt;/a&gt;. The documentation for advanced usage of Jupyter notebook can be found &lt;a href=&#34;https://jupyter-notebook.readthedocs.io/en/latest/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a local installation, make sure you have &lt;a href=&#34;https://pip.readthedocs.io/en/stable/installing/&#34;&gt;pip installed&lt;/a&gt; and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage - Running Jupyter notebook&lt;/h2&gt; &#xA;&lt;h3&gt;Running in a local installation&lt;/h3&gt; &#xA;&lt;p&gt;Launch with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running in a remote installation&lt;/h3&gt; &#xA;&lt;p&gt;You need some configuration before starting Jupyter notebook remotely. See &lt;a href=&#34;https://jupyter-notebook.readthedocs.io/en/stable/public_server.html&#34;&gt;Running a notebook server&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Development Installation&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/jupyter/notebook/main/CONTRIBUTING.md&#34;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; for how to set up a local development installation.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in contributing to the project, see &lt;a href=&#34;https://raw.githubusercontent.com/jupyter/notebook/main/CONTRIBUTING.md&#34;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community Guidelines and Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This repository is a Jupyter project and follows the Jupyter &lt;a href=&#34;https://jupyter.readthedocs.io/en/latest/community/content-community.html&#34;&gt;Community Guides and Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jupyter.org&#34;&gt;Project Jupyter website&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jupyter.org/try&#34;&gt;Online Demo at jupyter.org/try&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jupyter-notebook.readthedocs.io/en/latest/&#34;&gt;Documentation for Jupyter notebook&lt;/a&gt; [&lt;a href=&#34;https://media.readthedocs.org/pdf/jupyter-notebook/latest/jupyter-notebook.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ChungJooHo/Jupyter_Kor_doc/&#34;&gt;Korean Version of Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jupyter.readthedocs.io/en/latest/index.html&#34;&gt;Documentation for Project Jupyter&lt;/a&gt; [&lt;a href=&#34;https://media.readthedocs.org/pdf/jupyter/latest/jupyter.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jupyter/notebook/issues&#34;&gt;Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discourse.jupyter.org/&#34;&gt;Technical support - Jupyter Google Group&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>