<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-14T01:35:43Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>datawhalechina/agent-tutorial</title>
    <updated>2023-12-14T01:35:43Z</updated>
    <id>tag:github.com,2023-12-14:/datawhalechina/agent-tutorial</id>
    <link href="https://github.com/datawhalechina/agent-tutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;agent-tutorial&lt;/h1&gt; &#xA;&lt;h2&gt;教程介绍&lt;/h2&gt; &#xA;&lt;p&gt;本教程是 Datawhale 成员写作的关于 Agent 的教程，特点是通过实践引导学习者加深对Agent的理解。&lt;/p&gt; &#xA;&lt;p&gt;目前已有的内容，主要用于支持《动手学Agent开发》学习活动，介绍如何使用 &lt;a href=&#34;https://github.com/modelscope/modelscope-agent/tree/master&#34;&gt;ModelScope Agent&lt;/a&gt; 开发一个智能助手，并探讨一些 Agent 的通用创作思路及应用展望。&lt;/p&gt; &#xA;&lt;p&gt;学习活动详见&lt;a href=&#34;https://datawhaler.feishu.cn/docx/DqaydpsFdovWonxDrYxcrBYxnkf&#34;&gt;《动手学Agent应用开发》学习手册&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;教程大纲&lt;/h2&gt; &#xA;&lt;p&gt;第1章：Agent入门简介&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/agent-tutorial/main/notebook/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9AAgent%E7%AE%80%E4%BB%8B/1.1%20Agent%E5%8E%9F%E7%90%86.md&#34;&gt;Agent原理&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/agent-tutorial/main/notebook/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9AAgent%E7%AE%80%E4%BB%8B/1.2%20Agent%E5%8E%9F%E7%90%86%E6%B7%B1%E5%85%A5+%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE.md&#34;&gt;Agent原理深入+环境配置&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;第2章：Agent实践：日程规划小助手&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/agent-tutorial/main/notebook/%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9AAgent%E5%AE%9E%E8%B7%B5/2.1%20%E9%AB%98%E5%BE%B7%E5%BC%80%E6%94%BEAPI%E5%AE%9E%E8%B7%B5.md&#34;&gt;高德开放API实践&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/agent-tutorial/main/notebook/%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9AAgent%E5%AE%9E%E8%B7%B5/2.2%20%E6%97%A5%E7%A8%8B%E8%A7%84%E5%88%92%E5%B0%8F%E5%8A%A9%E6%89%8B.md&#34;&gt;基础准备：日程规划小助手&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;完整版：日程规划 + 绘图&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;第3章：Agent应用展望&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/agent-tutorial/main/notebook/%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9AAgent%E5%BA%94%E7%94%A8%E5%B1%95%E6%9C%9B/3.1%20Agent%E9%80%9A%E7%94%A8%E5%88%9B%E4%BD%9C%E6%80%9D%E8%B7%AF.md&#34;&gt;Agent通用创作思路&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/agent-tutorial/main/notebook/%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9AAgent%E5%BA%94%E7%94%A8%E5%B1%95%E6%9C%9B/3.2%20Agent%E5%BA%94%E7%94%A8%E5%B1%95%E6%9C%9B.md&#34;&gt;Agent应用展望&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>SunzeY/AlphaCLIP</title>
    <updated>2023-12-14T01:35:43Z</updated>
    <id>tag:github.com,2023-12-14:/SunzeY/AlphaCLIP</id>
    <link href="https://github.com/SunzeY/AlphaCLIP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Alpha-CLIP: A CLIP Model Focusing on Wherever You Want&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SunzeY/AlphaCLIP/main/img/alpha_icon.png&#34; style=&#34;vertical-align: -10px;&#34; :height=&#34;40px&#34; width=&#34;40px&#34;&gt; Alpha-CLIP&lt;/h1&gt; &#xA;&lt;p&gt;This repository is the official implementation of AlphaCLIP&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.03818&#34;&gt;Alpha-CLIP: A CLIP Model Focusing on Wherever You Want&lt;/a&gt;&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/SunzeY&#34;&gt;Zeyi Sun&lt;/a&gt;*, &lt;a href=&#34;https://github.com/Aleafy&#34;&gt;Ye Fang&lt;/a&gt;*, &lt;a href=&#34;https://wutong16.github.io/&#34;&gt;Tong Wu&lt;/a&gt;, &lt;a href=&#34;https://panzhang0212.github.io/&#34;&gt;Pan Zhang&lt;/a&gt;, &lt;a href=&#34;https://yuhangzang.github.io/&#34;&gt;Yuhang Zang&lt;/a&gt;, &lt;a href=&#34;https://aimerykong.github.io/&#34;&gt;Shu Kong&lt;/a&gt;, &lt;a href=&#34;http://yjxiong.me/&#34;&gt;Yuanjun Xiong&lt;/a&gt;, &lt;a href=&#34;http://dahua.site/&#34;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&#34;https://myownskyw7.github.io/&#34;&gt;Jiaqi Wang&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p style=&#34;font-size: 0.6em; margin-top: -1em&#34;&gt;*Equal Contribution&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.03818&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-&lt;color&gt;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aleafy.github.io/alpha-clip&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Website-red&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Demo &lt;code&gt;Alpha-CLIP&lt;/code&gt; with &lt;code&gt;Stable Diffusion&lt;/code&gt;: &lt;a href=&#34;https://huggingface.co/spaces/Zery/Alpha_CLIP_ImgVar&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-yellow&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openxlab.org.cn/apps/detail/SunzeY/Alpha-CLIP_Image_Var1&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg?sanitize=true&#34; alt=&#34;Open in OpenXLab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Demo &lt;code&gt;Alpha-CLIP&lt;/code&gt; with &lt;code&gt;LLaVA&lt;/code&gt;: coming soon&lt;/p&gt; &#xA;&lt;h2&gt;📜 News&lt;/h2&gt; &#xA;&lt;p&gt;[2023/12/7] The &lt;a href=&#34;https://arxiv.org/abs/2312.03818&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://aleafy.github.io/alpha-clip&#34;&gt;project page&lt;/a&gt; are released!&lt;/p&gt; &#xA;&lt;h2&gt;💡 Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🔥 &lt;strong&gt;3.93%&lt;/strong&gt; improved zero-shot ImageNet classification accuracy when providing foreground alpha-map.&lt;/li&gt; &#xA; &lt;li&gt;🔥 &lt;strong&gt;Plug-in and play&lt;/strong&gt; with region focus in &lt;strong&gt;any work&lt;/strong&gt; that use CLIP vision encoder.&lt;/li&gt; &#xA; &lt;li&gt;🔥 &lt;strong&gt;A strong visual encoder&lt;/strong&gt; as vasatile tool when foreground mask is available.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;👨‍💻 Todo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training and evaluation code for Alpha-CLIP&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Web demo and local demo of Alpha-CLIP with LLaVA&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Web demo and local demo of Alpha-CLIP with Stable Diffusion&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Usage example notebook of Alpha-CLIP&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Checkpoints of Alpha-CLIP&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🛠️ Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;our model is based on &lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;CLIP&lt;/a&gt;, please first prepare environment for CLIP, then directly install Alpha-CLIP.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;install loralib&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install loralib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;how to use&lt;/h3&gt; &#xA;&lt;p&gt;Download model from &lt;a href=&#34;https://github.com/SunzeY/AlphaCLIP/raw/main/model-zoo.md&#34;&gt;model-zoo&lt;/a&gt; and place it under &lt;code&gt;checkpoints&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import alpha_clip&#xA;alpha_clip.load(&#34;ViT-B/16&#34;, alpha_vision_ckpt_pth=&#34;checkpoints/clip_b16_grit1m_fultune_8xe.pth&#34;, device=&#34;cpu&#34;), &#xA;image_features = model.visual(image, alpha)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;alpha&lt;/code&gt; need to be normalized via transforms when using &lt;code&gt;binary_mask&lt;/code&gt; in (0, 1)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mask_transform = transforms.Compose([&#xA;    transforms.ToTensor(), &#xA;    transforms.Resize((224, 224)),&#xA;    transforms.Normalize(0.5, 0.26)&#xA;])&#xA;alpha = mask_transform(binary_mask * 255)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage examples are available&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Visualization of attention map: &lt;a href=&#34;https://github.com/SunzeY/AlphaCLIP/raw/main/notebooks/attn_visual.ipynb&#34;&gt;notebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Alpha-CLIP used in BLIP-Diffusion: &lt;a href=&#34;https://github.com/SunzeY/AlphaCLIP/raw/main/notebooks/blip_diffusion.ipynb&#34;&gt;notebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Alpha-CLIP used in SD_ImageVar: &lt;a href=&#34;https://github.com/SunzeY/AlphaCLIP/tree/main/demo/with_diffusion&#34;&gt;demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;⭐ Demos&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SunzeY/AlphaCLIP/main/img/demo1.gif&#34; width=&#34;900&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;❤️ Acknowledgments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;CLIP&lt;/a&gt;: The codebase we built upon. Thanks for their wonderful work.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/salesforce/LAVIS&#34;&gt;LAVIS&lt;/a&gt;: The amazing open-sourced multimodality learning codebase, where we test Alpha-CLIP in &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/blip2&#34;&gt;BLIP-2&lt;/a&gt; and &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion&#34;&gt;BLIP-Diffusion&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/point-e&#34;&gt;Point-E&lt;/a&gt;: Wonderful point-cloud generation model, where we test Alpha-CLIP for 3D generation task.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt;: Wounderful MLLM that use CLIP as visual bacbone where we test the effectiveness of Alpha-CLIP.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;✒️ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful for your research, please consider giving a star ⭐ and citation 📝&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{sun2023alphaclip,&#xA;      title={Alpha-CLIP: A CLIP Model Focusing on Wherever You Want}, &#xA;      author={Zeyi Sun and Ye Fang and Tong Wu and Pan Zhang and Yuhang Zang and Shu Kong and Yuanjun Xiong and Dahua Lin and Jiaqi Wang},&#xA;      year={2023},&#xA;      eprint={2312.03818},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt; &lt;strong&gt;Usage and License Notices&lt;/strong&gt;: The data and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of CLIP. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.&lt;/p&gt;</summary>
  </entry>
</feed>