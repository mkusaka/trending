<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-03T01:36:08Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PRIS-CV/DemoFusion</title>
    <updated>2023-12-03T01:36:08Z</updated>
    <id>tag:github.com,2023-12-03:/PRIS-CV/DemoFusion</id>
    <link href="https://github.com/PRIS-CV/DemoFusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Let us democratise high-resolution generation! (arXiv 2023)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DemoFusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ruoyidu.github.io/demofusion/demofusion.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-green.svg?sanitize=true&#34; alt=&#34;Project Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2311.16973.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2101.12345-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pytorch.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PyTorch-v2.1.0-red.svg?sanitize=true&#34; alt=&#34;PyTorch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/docs/diffusers/index&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hugging%20Face-Diffusers-orange.svg?sanitize=true&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/lucataco/demofusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-%F0%9F%9A%80%20Replicate-blue&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badges.toozhao.com/stats/01HFMAPCVTA1T32KN2PASNYGYK&#34; title=&#34;Get your own page views count badge on badges.toozhao.com&#34;&gt;&lt;img src=&#34;https://badges.toozhao.com/badges/01HFMAPCVTA1T32KN2PASNYGYK/blue.svg?sanitize=true&#34; alt=&#34;Page Views Count&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Code release for &#34;DemoFusion: Democratising High-Resolution Image Generation With No üí∞&#34; (arXiv 2023)&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/PRIS-CV/DemoFusion/main/illustration.jpg&#34; width=&#34;800&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: High-resolution image generation with Generative Artificial Intelligence (GenAI) has immense potential but, due to the enormous capital investment required for training, it is increasingly centralised to a few large corporations, and hidden behind paywalls. This paper aims to democratise high-resolution GenAI by advancing the frontier of high-resolution generation while remaining accessible to a broad audience. We demonstrate that existing Latent Diffusion Models (LDMs) possess untapped potential for higher-resolution image generation. Our novel DemoFusion framework seamlessly extends open-source GenAI models, employing Progressive Upscaling, Skip Residual, and Dilated Sampling mechanisms to achieve higher-resolution image generation. The progressive nature of DemoFusion requires more passes, but the intermediate results can serve as &#34;previews&#34;, facilitating rapid prompt iteration.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/PRIS-CV/DemoFusion/main/progressive_process.jpg&#34; width=&#34;800&#34;&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.12.01&lt;/strong&gt;: Integrated to &lt;span&gt;üöÄ&lt;/span&gt; &lt;a href=&#34;https://replicate.com/explore&#34;&gt;Replicate&lt;/a&gt;. Try out online demo! &lt;a href=&#34;https://replicate.com/lucataco/demofusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-%F0%9F%9A%80%20Replicate-blue&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt; Thank &lt;a href=&#34;https://github.com/lucataco&#34;&gt;Luis C.&lt;/a&gt; for the implementation!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023.11.29&lt;/strong&gt;: &#39;pipeline_demofusion_sdxl&#39; is released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The version requirements of core dependencies.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch==2.1.0&#xA;diffusers==0.21.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download &lt;code&gt;pipeline_demofusion_sdxl.py&lt;/code&gt; and run it as follows. A use case can be found in &lt;code&gt;demo.ipynb&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;from pipeline_demofusion_sdxl import DemoFusionSDXLPipeline&#xA;&#xA;model_ckpt = &#34;stabilityai/stable-diffusion-xl-base-1.0&#34;&#xA;pipe = DemoFusionSDXLPipeline.from_pretrained(model_ckpt, torch_dtype=torch.float16)&#xA;pipe = pipe.to(&#34;cuda&#34;)&#xA;&#xA;prompt = &#34;Envision a portrait of an elderly woman, her face a canvas of time, framed by a headscarf with muted tones of rust and cream. Her eyes, blue like faded denim. Her attire, simple yet dignified.&#34;&#xA;negative_prompt = &#34;blurry, ugly, duplicate, poorly drawn, deformed, mosaic&#34;&#xA;&#xA;images = pipe(prompt, negative_prompt=negative_prompt,&#xA;              height=3072, width=3072, view_batch_size=16, stride=64,&#xA;              num_inference_steps=50, guidance_scale=7.5,&#xA;              cosine_scale_1=3, cosine_scale_2=1, cosine_scale_3=1, sigma=0.8,&#xA;              multi_decoder=True, show_image=True&#xA;             )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please feel free to try different prompts and resolutions.&lt;/li&gt; &#xA; &lt;li&gt;Default hyper-parameters are recommended, but they may not be optimal for all cases. For specific impacts of each hyper-parameter, please refer to Appendix C in the DemoFusion paper.&lt;/li&gt; &#xA; &lt;li&gt;The code was cleaned before the release. If you encounter any issues, please contact us.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this paper useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{du2023demofusion,&#xA;    title={DemoFusion: Democratising High-Resolution Image Generation With No $$$},&#xA;    author={Ruoyi Du and Dongliang Chang and Timothy M. Hospedales and Yi-Zhe Song and Zhanyu Ma},&#xA;    journal={arXiv},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python</title>
    <updated>2023-12-03T01:36:08Z</updated>
    <id>tag:github.com,2023-12-03:/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python</id>
    <link href="https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hands-On Graph Neural Networks Using Python, published by Packt&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;&lt;a href=&#34;https://packt.link/JGIEY&#34;&gt;Packt Conference : Put Generative AI to work on Oct 11-13 (Virtual)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://packt.link/JGIEY&#34;&gt;&lt;img src=&#34;https://hub.packtpub.com/wp-content/uploads/2023/08/put-generative-ai-to-work-packt.png&#34; alt=&#34;Packt Conference&#34;&gt;&lt;/a&gt;&lt;/b&gt;&lt;/p&gt; 3 Days, 20+ AI Experts, 25+ Workshops and Power Talks&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;Code: &lt;b&gt;USD75OFF&lt;/b&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Hands-On Graph Neural Networks Using Python&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.packtpub.com/product/hands-on-graph-neural-networks-using-python/9781804617526?utm_source=github&amp;amp;utm_medium=repository&amp;amp;utm_campaign=9781804617526&#34;&gt;&lt;img src=&#34;https://static.packt-cdn.com/products/9781804617526/cover/smaller&#34; alt=&#34;Hands-On Graph Neural Networks Using Python&#34; height=&#34;256px&#34; align=&#34;right&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the code repository for &lt;a href=&#34;https://www.packtpub.com/product/hands-on-graph-neural-networks-using-python/9781804617526?utm_source=github&amp;amp;utm_medium=repository&amp;amp;utm_campaign=9781804617526&#34;&gt;Hands-On Graph Neural Networks Using Python&lt;/a&gt;, published by Packt.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Practical techniques and architectures for building powerful graph and deep learning apps with PyTorch&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is this book about?&lt;/h2&gt; &#xA;&lt;p&gt;Graph neural networks are a highly effective tool for analyzing data that can be represented as a graph, such as social networks, chemical compounds, or transportation networks. The past few years have seen an explosion in the use of graph neural networks, with their application ranging from natural language processing and computer vision to recommendation systems and drug discovery.&lt;/p&gt; &#xA;&lt;p&gt;This book covers the following exciting features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understand the fundamental concepts of graph neural networks&lt;/li&gt; &#xA; &lt;li&gt;Implement graph neural networks using Python and PyTorch Geometric&lt;/li&gt; &#xA; &lt;li&gt;Classify nodes, graphs, and edges using millions of samples&lt;/li&gt; &#xA; &lt;li&gt;Predict and generate realistic graph topologies&lt;/li&gt; &#xA; &lt;li&gt;Combine heterogeneous sources to improve performance&lt;/li&gt; &#xA; &lt;li&gt;Forecast future events using topological information&lt;/li&gt; &#xA; &lt;li&gt;Apply graph neural networks to solve real-world problems&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you feel this book is for you, get your &lt;a href=&#34;https://www.amazon.com/dp/1804617520&#34;&gt;copy&lt;/a&gt; today!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.packtpub.com/?utm_source=github&amp;amp;utm_medium=banner&amp;amp;utm_campaign=GitHubBanner&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PacktPublishing/GitHub/master/GitHub.png&#34; alt=&#34;https://www.packtpub.com/&#34; border=&#34;5&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Instructions and Navigations&lt;/h2&gt; &#xA;&lt;p&gt;All of the code is organized into folders.&lt;/p&gt; &#xA;&lt;p&gt;The code will look like the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;DG = nx.DiGraph()&#xA;DG.add_edges_from([(&#39;A&#39;, &#39;B&#39;), (&#39;A&#39;, &#39;C&#39;), (&#39;B&#39;, &#39;D&#39;), (&#39;B&#39;, &#39;E&#39;), (&#39;C&#39;, &#39;F&#39;), (&#39;C&#39;, &#39;G&#39;)])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Following is what you need for this book:&lt;/strong&gt; This book is for machine learning practitioners and data scientists interested in learning about graph neural networks and their applications, as well as students looking for a comprehensive reference on this rapidly growing field. Whether you‚Äôre new to graph neural networks or looking to take your knowledge to the next level, this book has something for you. Basic knowledge of machine learning and Python programming will help you get the most out of this book.&lt;/p&gt; &#xA;&lt;p&gt;Basic knowledge of Python will help you get more from the examples. With the following software and hardware list you can run all code files present in the book (Chapter 2-14).&lt;/p&gt; &#xA;&lt;h3&gt;Software and Hardware List&lt;/h3&gt; &#xA;&lt;p&gt;You should have a basic understanding of graph theory and machine learning concepts, such as supervised and unsupervised learning, training, and the evaluation of models to maximize your learning experience. Familiarity with deep learning frameworks, such as PyTorch, will also be useful, although not essential, as the book will provide a comprehensive introduction to the mathematical concepts and their implementation.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Software required&lt;/th&gt; &#xA;   &lt;th&gt;OS required&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python 3.8.15&lt;/td&gt; &#xA;   &lt;td&gt;Windows, Mac OS X, and Linux (Any)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PyTorch 1.13.1&lt;/td&gt; &#xA;   &lt;td&gt;Windows, Mac OS X, and Linux (Any)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PyTorch Geometric 2.2.0&lt;/td&gt; &#xA;   &lt;td&gt;Windows, Mac OS X, and Linux (Any)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To install Python 3.8.15, you can download the latest version from the official Python website: &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;https://www.python.org/downloads/&lt;/a&gt;. We strongly recommend using a virtual environment, such as venv or conda. Optionally, if you want to use a Graphics Processing Unit (GPU) from NVIDIA to accelerate training and inference, you will need to install CUDA and cuDNN:&lt;/p&gt; &#xA;&lt;p&gt;CUDA is a parallel computing platform and API developed by NVIDIA for general computing on GPUs. To install CUDA, you can follow the instructions on the NVIDIA website: &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;https://developer.nvidia.com/cuda-downloads&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;cuDNN is a library developed by NVIDIA, which provides highly optimized GPU implementations of primitives for deep learning algorithms. To install cuDNN, you need to create an account on the NVIDIA website and download the library from the cuDNN download page: &lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;https://developer.nvidia.com/cudnn&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can check out the list of CUDA-enabled GPU products on the NVIDIA website: &lt;a href=&#34;https://developer.nvidia.com/cuda-gpus&#34;&gt;https://developer.nvidia.com/cuda-gpus&lt;/a&gt;. To install PyTorch 1.13.1, you can follow the instructions on the official PyTorch website: &lt;a href=&#34;https://pytorch.org/&#34;&gt;https://pytorch.org/&lt;/a&gt;. You can choose the installation method that is most appropriate for your system (including CUDA and cuDNN).&lt;/p&gt; &#xA;&lt;p&gt;To install PyTorch Geometric 2.2.0, you can follow the instructions in the GitHub repository: &lt;a href=&#34;https://pytorch-eometric.readthedocs.io/en/2.2.0/notes/installation.html&#34;&gt;https://pytorch-eometric.readthedocs.io/en/2.2.0/notes/installation.html&lt;/a&gt;. You will need to have PyTorch installed on your system first.&lt;/p&gt; &#xA;&lt;p&gt;Chapter 11 requires TensorFlow 2.4. To install it, you can follow the instructions on the official TensorFlow website: &lt;a href=&#34;https://www.tensorflow.org/install&#34;&gt;https://www.tensorflow.org/install&lt;/a&gt;. You can choose the installation method that is most appropriate for your system and the version of TensorFlow you want to use.&lt;/p&gt; &#xA;&lt;p&gt;Chapter 14 requires an older version of PyTorch Geometric (version 2.0.4). It is recommended to create a specific virtual environment for this chapter.&lt;/p&gt; &#xA;&lt;p&gt;Chapter 15, Chapter 16, and Chapter 17 require a high GPU memory usage. You can lower it by decreasing the size of the training set in the code. Other Python libraries are required in some or most chapters. You can install them using pip install &amp;lt;name==version&amp;gt;, or using another installer depending on your configuration (such as conda).&lt;/p&gt; &#xA;&lt;p&gt;Here is the complete list of required packages with the corresponding versions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;torch==1.13.1+cu117&lt;/li&gt; &#xA; &lt;li&gt;torchvision==0.14.1+cu117&lt;/li&gt; &#xA; &lt;li&gt;torchaudio==0.13.1+cu117&lt;/li&gt; &#xA; &lt;li&gt;pandas==1.5.2&lt;/li&gt; &#xA; &lt;li&gt;gensim==4.3.0&lt;/li&gt; &#xA; &lt;li&gt;torch-scatter==2.1.0+pt113cu117&lt;/li&gt; &#xA; &lt;li&gt;torch-sparse==0.6.16+pt113cu117&lt;/li&gt; &#xA; &lt;li&gt;torch-cluster==1.6.0+pt113cu117&lt;/li&gt; &#xA; &lt;li&gt;torch-spline-conv==1.2.1+pt113cu117&lt;/li&gt; &#xA; &lt;li&gt;torch-geometric==2.2.0&lt;/li&gt; &#xA; &lt;li&gt;networkx==2.8.8&lt;/li&gt; &#xA; &lt;li&gt;matplotlib==3.6.3&lt;/li&gt; &#xA; &lt;li&gt;node2vec==0.4.6&lt;/li&gt; &#xA; &lt;li&gt;seaborn==0.12.2&lt;/li&gt; &#xA; &lt;li&gt;scikit-learn==1.2.0&lt;/li&gt; &#xA; &lt;li&gt;tensorflow-gpu~=2.4&lt;/li&gt; &#xA; &lt;li&gt;deepchem==2.7.1&lt;/li&gt; &#xA; &lt;li&gt;torch-geometric-temporal==0.54.0&lt;/li&gt; &#xA; &lt;li&gt;captum==0.6.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The complete list of requirements is available on GitHub at &lt;a href=&#34;https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python&#34;&gt;https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python&lt;/a&gt;. Alternatively, you can directly import notebooks in Google Colab at &lt;a href=&#34;https://colab.research.google.com&#34;&gt;https://colab.research.google.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a PDF file that has color images of the screenshots/diagrams used in this book. &lt;a href=&#34;https://packt.link/gaFU6&#34;&gt;Click here to download it&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Errata&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Page 109: Formula of GAT should be &lt;img src=&#34;https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/assets/108388790/8d00dd02-8296-4a85-9f3a-c1b91a386f4b&#34; alt=&#34;image&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Related products &#xA; &lt;other books you may enjoy&gt;&lt;/other&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Network Science with Python &lt;a href=&#34;https://www.packtpub.com/product/network-science-with-python/9781801073691&#34;&gt;[Packt]&lt;/a&gt; &lt;a href=&#34;https://www.amazon.com/dp/B0BJKP7R4P&#34;&gt;[Amazon]&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;3D Deep Learning with Python &lt;a href=&#34;https://www.packtpub.com/product/3d-deep-learning-with-python/9781803247823&#34;&gt;[Packt]&lt;/a&gt; &lt;a href=&#34;https://www.amazon.com/dp/B0BJVQG1VS&#34;&gt;[Amazon]&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Get to Know the Author&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Maxime Labonne&lt;/strong&gt; is a senior applied researcher at J.P. Morgan with a Ph.D. in machine learning and cyber security from the Polytechnic Institute of Paris. During his Ph.D., Maxime worked on developing machine learning algorithms for anomaly detection in computer networks. He then joined the AI Connectivity Lab at Airbus, where he applied his expertise in machine learning to improve the security and performance of computer networks. He then joined J.P. Morgan, where he now develops techniques for solving a variety of challenging problems in finance and other domains. In addition to his research work, Maxime is passionate about sharing his knowledge and experience with others through Twitter (@maximelabonne) and his personal blog.&lt;/p&gt; &#xA;&lt;h3&gt;Download a free PDF&lt;/h3&gt; &#xA;&lt;p&gt;&lt;i&gt;If you have already purchased a print or Kindle version of this book, you can get a DRM-free PDF version at no cost.&lt;br&gt;Simply click on the link to claim your free PDF.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://packt.link/free-ebook/9781804617526&#34;&gt;https://packt.link/free-ebook/9781804617526 &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>aws-samples/amazon-bedrock-samples</title>
    <updated>2023-12-03T01:36:08Z</updated>
    <id>tag:github.com,2023-12-03:/aws-samples/amazon-bedrock-samples</id>
    <link href="https://github.com/aws-samples/amazon-bedrock-samples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains examples for customers to get started using the Amazon Bedrock Service. This contains examples for all availble foundational models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Amazon Bedrock Samples&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains pre-built examples to help customers get started with the Amazon Bedrock service.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/introduction-to-bedrock&#34;&gt;Introduction to Bedrock&lt;/a&gt; - Learn the basics of the Bedrock service&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/prompt-engineering&#34;&gt;Prompt Engineering &lt;/a&gt; - Tips for crafting effective prompts&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/bedrock-fine-tuning&#34;&gt;Bedrock Fine-tuning&lt;/a&gt; - Fine-tune Bedrock models for your specific use case&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/generative-ai-solutions&#34;&gt;Generative AI Solutions&lt;/a&gt; - Example use cases for generative AI&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/knowledge-bases&#34;&gt;Knowledge Bases&lt;/a&gt; - Build knowledge bases with Bedrock&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/rag-solutions&#34;&gt;Retrival Augmented Generation (RAG)&lt;/a&gt; - Implementing RAG with Amazon Bedrock&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/agents&#34;&gt;Agents&lt;/a&gt; - Generative AI agents with Bedrock&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/security-and-governance&#34;&gt;Security and Governance&lt;/a&gt; - Secure your Bedrock applications&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/responsible-ai&#34;&gt;Responsible AI&lt;/a&gt; - Use Bedrock responsibly and ethically&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/ops-tooling&#34;&gt;Operational Tooling&lt;/a&gt; - Helpful samples to help operationalize your useage of Amazon Bedrock&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/multimodal&#34;&gt;Multimodal&lt;/a&gt; - Working with multimodal data using Amazon Bedrock&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To get started with the code examples, ensure you have access to &lt;a href=&#34;https://aws.amazon.com/bedrock/&#34;&gt;Amazon Bedrock&lt;/a&gt;. Then clone this repo and navigate to one of the folders above. Detailed instructions are provided in each folder&#39;s README.&lt;/p&gt; &#xA;&lt;h3&gt;Enable AWS IAM permissions for Bedrock&lt;/h3&gt; &#xA;&lt;p&gt;The AWS identity you assume from your environment (which is the &lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html&#34;&gt;&lt;em&gt;Studio/notebook Execution Role&lt;/em&gt;&lt;/a&gt; from SageMaker, or could be a role or IAM User for self-managed notebooks or other use-cases), must have sufficient &lt;a href=&#34;https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html&#34;&gt;AWS IAM permissions&lt;/a&gt; to call the Amazon Bedrock service.&lt;/p&gt; &#xA;&lt;p&gt;To grant Bedrock access to your identity, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open the &lt;a href=&#34;https://us-east-1.console.aws.amazon.com/iam/home?#&#34;&gt;AWS IAM Console&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Find your &lt;a href=&#34;https://us-east-1.console.aws.amazon.com/iamv2/home?#/roles&#34;&gt;Role&lt;/a&gt; (if using SageMaker or otherwise assuming an IAM Role), or else &lt;a href=&#34;https://us-east-1.console.aws.amazon.com/iamv2/home?#/users&#34;&gt;User&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Select &lt;em&gt;Add Permissions &amp;gt; Create Inline Policy&lt;/em&gt; to attach new inline permissions, open the &lt;em&gt;JSON&lt;/em&gt; editor and paste in the below example policy:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;    &#34;Version&#34;: &#34;2012-10-17&#34;,&#xA;    &#34;Statement&#34;: [&#xA;        {&#xA;            &#34;Sid&#34;: &#34;BedrockFullAccess&#34;,&#xA;            &#34;Effect&#34;: &#34;Allow&#34;,&#xA;            &#34;Action&#34;: [&#34;bedrock:*&#34;],&#xA;            &#34;Resource&#34;: &#34;*&#34;&#xA;        }&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; With Amazon SageMaker, your notebook execution role will typically be &lt;em&gt;separate&lt;/em&gt; from the user or role that you log in to the AWS Console with. If you&#39;d like to explore the AWS Console for Amazon Bedrock, you&#39;ll need to grant permissions to your Console user/role too.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For more information on the fine-grained action and resource permissions in Bedrock, check out the Bedrock Developer Guide.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome community contributions! Please see &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for guidelines.&lt;/p&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/CONTRIBUTING.md#security-issue-notifications&#34;&gt;CONTRIBUTING&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This library is licensed under the MIT-0 License. See the &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
</feed>