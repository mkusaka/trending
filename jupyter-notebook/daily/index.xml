<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-19T01:31:58Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>wyf3/llm_related</title>
    <updated>2025-03-19T01:31:58Z</updated>
    <id>tag:github.com,2025-03-19:/wyf3/llm_related</id>
    <link href="https://github.com/wyf3/llm_related" rel="alternate"></link>
    <summary type="html">&lt;p&gt;记录大模型相关的一些知识和方法&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;记录大模型应用过程遇到的相关问题及解决方法&lt;/h1&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/smol-course</title>
    <updated>2025-03-19T01:31:58Z</updated>
    <id>tag:github.com,2025-03-19:/huggingface/smol-course</id>
    <link href="https://github.com/huggingface/smol-course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A course on aligning smol models.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/smol-course/main/banner.png&#34; alt=&#34;smolcourse image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;a smol course&lt;/h1&gt; &#xA;&lt;p&gt;This is a practical course on aligning language models for your specific use case. It&#39;s a handy way to get started with aligning language models, because everything runs on most local machines. There are minimal GPU requirements and no paid services. The course is based on the &lt;a href=&#34;https://github.com/huggingface/smollm/tree/main&#34;&gt;SmolLM2&lt;/a&gt; series of models, but you can transfer the skills you learn here to larger models or other small language models.&lt;/p&gt; &#xA;&lt;a href=&#34;http://hf.co/join/discord&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Discord-7289DA?&amp;amp;logo=discord&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA;&lt;div style=&#34;background: linear-gradient(to right, #e0f7fa, #e1bee7, orange); padding: 20px; border-radius: 5px; margin-bottom: 20px; color: purple;&#34;&gt; &#xA; &lt;h2&gt;Participation is open, free, and now!&lt;/h2&gt; &#xA; &lt;p&gt;This course is open and peer reviewed. To get involved with the course &lt;strong&gt;open a pull request&lt;/strong&gt; and submit your work for review. Here are the steps:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Fork the repo &lt;a href=&#34;https://github.com/huggingface/smol-course/fork&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Read the material, make changes, do the exercises, add your own examples.&lt;/li&gt; &#xA;  &lt;li&gt;Open a PR on the december_2024 branch&lt;/li&gt; &#xA;  &lt;li&gt;Get it reviewed and merged&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;This should help you learn and to build a community-driven course that is always improving.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We can discuss the process in this &lt;a href=&#34;https://github.com/huggingface/smol-course/discussions/2#discussion-7602932&#34;&gt;discussion thread&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Course Outline&lt;/h2&gt; &#xA;&lt;p&gt;This course provides a practical, hands-on approach to working with small language models, from initial training through to production deployment.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Module&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Release Date&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/smol-course/main/1_instruction_tuning&#34;&gt;Instruction Tuning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn supervised fine-tuning, chat templating, and basic instruction following&lt;/td&gt; &#xA;   &lt;td&gt;✅ Ready&lt;/td&gt; &#xA;   &lt;td&gt;Dec 3, 2024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/smol-course/main/2_preference_alignment&#34;&gt;Preference Alignment&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Explore DPO and ORPO techniques for aligning models with human preferences&lt;/td&gt; &#xA;   &lt;td&gt;✅ Ready&lt;/td&gt; &#xA;   &lt;td&gt;Dec 6, 2024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/smol-course/main/3_parameter_efficient_finetuning&#34;&gt;Parameter-efficient Fine-tuning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn LoRA, prompt tuning, and efficient adaptation methods&lt;/td&gt; &#xA;   &lt;td&gt;✅ Ready&lt;/td&gt; &#xA;   &lt;td&gt;Dec 9, 2024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/smol-course/main/4_evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Use automatic benchmarks and create custom domain evaluations&lt;/td&gt; &#xA;   &lt;td&gt;✅ Ready&lt;/td&gt; &#xA;   &lt;td&gt;Dec 13, 2024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/smol-course/main/5_vision_language_models&#34;&gt;Vision-language Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Adapt multimodal models for vision-language tasks&lt;/td&gt; &#xA;   &lt;td&gt;✅ Ready&lt;/td&gt; &#xA;   &lt;td&gt;Dec 16, 2024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/smol-course/main/6_synthetic_datasets&#34;&gt;Synthetic Datasets&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Create and validate synthetic datasets for training&lt;/td&gt; &#xA;   &lt;td&gt;✅ Ready&lt;/td&gt; &#xA;   &lt;td&gt;Dec 20, 2024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/smol-course/main/7_inference&#34;&gt;Inference&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Infer with models efficiently&lt;/td&gt; &#xA;   &lt;td&gt;✅ Ready&lt;/td&gt; &#xA;   &lt;td&gt;Jan 8, 2025&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/smol-course/main/8_agents&#34;&gt;Agents&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Build your own agentic AI&lt;/td&gt; &#xA;   &lt;td&gt;✅ Ready&lt;/td&gt; &#xA;   &lt;td&gt;Jan 13, 2025&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Why Small Language Models?&lt;/h2&gt; &#xA;&lt;p&gt;While large language models have shown impressive capabilities, they often require significant computational resources and can be overkill for focused applications. Small language models offer several advantages for domain-specific applications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: Require significantly less computational resources to train and deploy&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customization&lt;/strong&gt;: Easier to fine-tune and adapt to specific domains&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Control&lt;/strong&gt;: Better understanding and control of model behavior&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: Lower operational costs for training and inference&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Privacy&lt;/strong&gt;: Can be run locally without sending data to external APIs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Green Technology&lt;/strong&gt;: Advocates efficient usage of resources with reduced carbon footprint&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easier Academic Research Development&lt;/strong&gt;: Provides an easy starter for academic research with cutting-edge LLMs with less logistical constraints&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Before starting, ensure you have the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basic understanding of machine learning and natural language processing.&lt;/li&gt; &#xA; &lt;li&gt;Familiarity with Python, PyTorch, and the &lt;code&gt;transformers&lt;/code&gt; library.&lt;/li&gt; &#xA; &lt;li&gt;Access to a pre-trained language model and a labeled dataset.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We maintain the course as a package so you can install dependencies easily via a package manager. We recommend &lt;a href=&#34;https://github.com/astral-sh/uv&#34;&gt;uv&lt;/a&gt; for this purpose, but you could use alternatives like &lt;code&gt;pip&lt;/code&gt; or &lt;code&gt;pdm&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Using &lt;code&gt;uv&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;With &lt;code&gt;uv&lt;/code&gt; installed, you can install the course like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv venv --python 3.11.0&#xA;uv sync&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;All the examples run in the same &lt;strong&gt;python 3.11&lt;/strong&gt; environment, so you should create an environment and install dependencies like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# python -m venv .venv&#xA;# source .venv/bin/activate&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Google Colab&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;From Google Colab&lt;/strong&gt; you will need to install dependencies flexibly based on the hardware you&#39;re using. Like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install transformers trl datasets huggingface_hub&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>google-deepmind/mujoco_playground</title>
    <updated>2025-03-19T01:31:58Z</updated>
    <id>tag:github.com,2025-03-19:/google-deepmind/mujoco_playground</id>
    <link href="https://github.com/google-deepmind/mujoco_playground" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source library for GPU-accelerated robot learning and sim-to-real transfer.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MuJoCo Playground&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/google-deepmind/mujoco_playground/actions&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/google-deepmind/mujoco_playground/ci.yml?branch=main&#34; alt=&#34;Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/playground/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/playground&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/google-deepmind/mujoco_playground/raw/main/assets/banner.png?raw=true&#34; alt=&#34;Banner for playground&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A comprehensive suite of GPU-accelerated environments for robot learning research and sim-to-real, built with &lt;a href=&#34;https://github.com/google-deepmind/mujoco/tree/main/mjx&#34;&gt;MuJoCo MJX&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Features include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Classic control environments from &lt;code&gt;dm_control&lt;/code&gt; reimplemented in MJX.&lt;/li&gt; &#xA; &lt;li&gt;Quadruped and bipedal locomotion environments.&lt;/li&gt; &#xA; &lt;li&gt;Non-prehensile and dexterous manipulation environments.&lt;/li&gt; &#xA; &lt;li&gt;Vision-based support available via &lt;a href=&#34;https://github.com/shacklettbp/madrona_mjx&#34;&gt;Madrona-MJX&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more details, check out the project &lt;a href=&#34;https://playground.mujoco.org/&#34;&gt;website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can install MuJoCo Playground directly from PyPI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install playground&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;From Source&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Requires Python 3.10 or later.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone git@github.com:google-deepmind/mujoco_playground.git &amp;amp;&amp;amp; cd mujoco_playground&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.astral.sh/uv/getting-started/installation/&#34;&gt;Install uv&lt;/a&gt;, a faster alternative to &lt;code&gt;pip&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create a virtual environment: &lt;code&gt;uv venv --python 3.11&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Activate it: &lt;code&gt;source .venv/bin/activate&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install CUDA 12 jax: &lt;code&gt;uv pip install -U &#34;jax[cuda12]&#34;&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Verify GPU backend: &lt;code&gt;python -c &#34;import jax; print(jax.default_backend())&#34;&lt;/code&gt; should print gpu&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Install playground: &lt;code&gt;uv pip install -e &#34;.[all]&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Verify installation (and download Menagerie): &lt;code&gt;python -c &#34;import mujoco_playground&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Madrona-MJX (optional)&lt;/h4&gt; &#xA;&lt;p&gt;For vision-based environments, please refer to the installation instructions in the &lt;a href=&#34;https://github.com/shacklettbp/madrona_mjx?tab=readme-ov-file#installation&#34;&gt;Madrona-MJX&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;Basic Tutorials&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Colab&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/google-deepmind/mujoco_playground/blob/main/learning/notebooks/dm_control_suite.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Introduction to the Playground with DM Control Suite&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/google-deepmind/mujoco_playground/blob/main/learning/notebooks/locomotion.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Locomotion Environments&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/google-deepmind/mujoco_playground/blob/main/learning/notebooks/manipulation.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Manipulation Environments&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Vision-Based Tutorials (GPU Colab)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Colab&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/google-deepmind/mujoco_playground/blob/main/learning/notebooks/training_vision_1_t4.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Training CartPole from Vision (T4 Instance)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Local Runtime Tutorials&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Requires local Madrona-MJX installation&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Colab&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/google-deepmind/mujoco_playground/blob/main/learning/notebooks/training_vision_1.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Training CartPole from Vision&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/google-deepmind/mujoco_playground/blob/main/learning/notebooks/training_vision_2.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Robotic Manipulation from Vision&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;How can I contribute?&lt;/h2&gt; &#xA;&lt;p&gt;Get started by installing the library and exploring its features! Found a bug? Report it in the issue tracker. Interested in contributing? If you are a developer with robotics experience, we would love your help—check out the &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_playground/main/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use Playground in your scientific works, please cite it as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{mujoco_playground_2025,&#xA;  title = {MuJoCo Playground: An open-source framework for GPU-accelerated robot learning and sim-to-real transfer.},&#xA;  author = {Zakka, Kevin and Tabanpour, Baruch and Liao, Qiayuan and Haiderbhai, Mustafa and Holt, Samuel and Luo, Jing Yuan and Allshire, Arthur and Frey, Erik and Sreenath, Koushil and Kahrs, Lueder A. and Sferrazza, Carlo and Tassa, Yuval and Abbeel, Pieter},&#xA;  year = {2025},&#xA;  publisher = {GitHub},&#xA;  url = {https://github.com/google-deepmind/mujoco_playground}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License and Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;The texture used in the rough terrain for the locomotion environments is from &lt;a href=&#34;https://polyhaven.com/a/rock_face&#34;&gt;Polyhaven&lt;/a&gt; and licensed under &lt;a href=&#34;https://creativecommons.org/public-domain/cc0/&#34;&gt;CC0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All other content in this repository is licensed under the Apache License, Version 2.0. A copy of this license is provided in the top-level &lt;a href=&#34;https://raw.githubusercontent.com/google-deepmind/mujoco_playground/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file in this repository. You can also obtain it from &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt;</summary>
  </entry>
</feed>