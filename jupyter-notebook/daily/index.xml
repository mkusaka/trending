<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-22T01:36:59Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AlignmentResearch/tuned-lens</title>
    <updated>2023-03-22T01:36:59Z</updated>
    <id>tag:github.com,2023-03-22:/AlignmentResearch/tuned-lens</id>
    <link href="https://github.com/AlignmentResearch/tuned-lens" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tools for understanding how transformer predictions are built layer-by-layer&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tuned Lens üîé&lt;/h1&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/AlignmentResearch/tuned-lens/blob/main/notebooks/interactive.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://huggingface.co/spaces/AlignmentResearch/tuned-lens&#34;&gt; &lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm-dark.svg?sanitize=true&#34; , alt=&#34;Open in Spaces&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Tools for understanding how transformer predictions are built layer-by-layer&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/12176390/224879115-8bc95f26-68e4-4f43-9b4c-06ca5934a29d.png&#34; alt=&#34;Using the Tuned-lens&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This package provides a simple interface training and evaluating &lt;strong&gt;tuned lenses&lt;/strong&gt;. A tuned lens allows us to peak at the iterative computations a transformer uses to compute the next token.&lt;/p&gt; &#xA;&lt;p&gt;A lens into a transformer with n layers allows you to replace the last $m$ layers of the model with an &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.Linear.html&#34;&gt;affine transformation&lt;/a&gt; (we call these affine translators).&lt;/p&gt; &#xA;&lt;p&gt;This skips over these last few layers and lets you see the best prediction that can be made from the model&#39;s intermediate representations, i.e. the residual stream, at layer $n - m$. Since the representations may be rotated, shifted, or stretched from layer to layer it&#39;s useful to train an affine specifically on each layer. This training is what differentiates this method from simpler approaches that decode the residual stream of the network directly using the unembeding layer i.e. the &lt;a href=&#34;https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens&#34;&gt;logit lens&lt;/a&gt;. We explain this process and its applications in a forthcoming paper &#34;Eliciting Latent Predictions from Transformers with the Tuned Lens&#34;.&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgments&lt;/h3&gt; &#xA;&lt;p&gt;Originally concieved by &lt;a href=&#34;https://twitter.com/igoro&#34;&gt;Igor Ostrovsky&lt;/a&gt; and &lt;a href=&#34;https://www.stellabiderman.com/&#34;&gt;Stella Biderman&lt;/a&gt; at &lt;a href=&#34;https://www.eleuther.ai/&#34;&gt;EleutherAI&lt;/a&gt;, this library was built as a collaboration between FAR and EleutherAI researchers.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; This package has not reached 1.0 yet. Expect the public interface to change regularly and without a major version bump.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Install instructions&lt;/h2&gt; &#xA;&lt;h3&gt;Installing From Source&lt;/h3&gt; &#xA;&lt;p&gt;First you will need to install the basic prerequisites into a virtual environment&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.9+&lt;/li&gt; &#xA; &lt;li&gt;Pytorch 1.12.0+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;then you can simply install the package using pip.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install tuned-lens&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install Using Docker&lt;/h3&gt; &#xA;&lt;p&gt;If you prefer to run the code from within a container you can use the provided docker file&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker pull ghcr.io/alignmentresearch/tuned-lens:latest&#xA;docker run --rm tuned-lens:latest tuned-lens --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start Guid&lt;/h2&gt; &#xA;&lt;h3&gt;Downloading the datasets&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget https://the-eye.eu/public/AI/pile/val.jsonl.zst&#xA;unzstd val.jsonl.zst&#xA;&#xA;wget https://the-eye.eu/public/AI/pile/test.jsonl.zst&#xA;unzstd test.jsonl.zst&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluating a Lens&lt;/h3&gt; &#xA;&lt;p&gt;Once you have a lens file either by training it yourself of by downloading it. You can run various evaluations on it using the provided evaluation command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;tuned-lens eval gpt2 test.jsonl --lens gpt-2-lens&#xA;    --dataset the_pile all \&#xA;    --split validation \&#xA;    --output lens_eval_results.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training a Lens&lt;/h3&gt; &#xA;&lt;p&gt;This will train a tuned lens on gpt-2 with the default hyper parameters.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tuned-lens train gpt2 val.jsonl&#xA;    --dataset the_pile all \&#xA;    --split validation \&#xA;    --output gpt-2-lens&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; This will download the entire validation set of the pile which is over 30 GBs. If you are doing this within a docker file it&#39;s recommended to mount external storage to huggingface&#39;s cache directory.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Make sure to install the dev dependencies and install the pre-commit hooks&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install -e &#34;.[dev]&#34;&#xA;$ pre-commit install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation Information&lt;/h2&gt; &#xA;&lt;p&gt;If you find this library useful, please cite it as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{belrose2023eliciting,&#xA;  title={Eliciting Latent Predictions from Transformers with the Tuned Lens},&#xA;  authors={Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and McKinney, Lev and Ostrovsky, Igor and Biderman, Stella and Steinhardt, Jacob},&#xA;  journal={to appear},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Vision-CAIR/ChatCaptioner</title>
    <updated>2023-03-22T01:36:59Z</updated>
    <id>tag:github.com,2023-03-22:/Vision-CAIR/ChatCaptioner</id>
    <link href="https://github.com/Vision-CAIR/ChatCaptioner" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Repository of ChatCaptioner&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions&lt;/h1&gt; &#xA;&lt;p&gt;Official repository of &lt;strong&gt;ChatCaptioner&lt;/strong&gt;. We discover the powerful questioning ability of LLMs and their great potential for acquiring information effectively. As an exploration, we introduce ChatCaptioner in image captioning. ChatCaptioner enrichs the image caption of BLIP-2 by prompting ChatGPT to keep asking informative questions to BLIP-2 and summarize the conversation at the end as the final caption.&lt;/p&gt; &#xA;&lt;p&gt;See our paper &lt;a href=&#34;https://arxiv.org/abs/2303.06594&#34;&gt;ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/ChatCaptioner/main/demo_pic/demo1.gif&#34; alt=&#34;demo1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/ChatCaptioner/main/demo_pic/demo2.gif&#34; alt=&#34;demo2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;System Architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Vision-CAIR/ChatCaptioner/main/demo_pic/overview.png&#34; alt=&#34;overfiew&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Note that you need a GPU with 24G memory to run ChatCaptioner due to the size of BLIP-2.&lt;/p&gt; &#xA;&lt;p&gt;To start, git clone this repository first.&lt;/p&gt; &#xA;&lt;p&gt;To install and activate the environment, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;conda activate chatcap&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set the environment variable OPENAI_API_KEY to your OpenAI API Key.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY=Your_OpenAI_Key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can add it to .bashrc so you don&#39;t need to set it manually everytime.&lt;/p&gt; &#xA;&lt;p&gt;As many scripts here are in jupyter notebook, don&#39;t forget to add the environment to jupyter&#39;s kernel list. To do so, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m ipykernel install --user --name=chatcap&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download our dataset samples from &lt;a href=&#34;https://drive.google.com/file/d/19yQP9lepLeS2_vSHnYPeOdfQz8OI1e6V/view?usp=share_link&#34;&gt;here&lt;/a&gt; and extract the zip file to the root folder. After the extraction, the datafolder should look like this. You can skip this step if you only want to run demo.py with your own images.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;‚îú‚îÄ‚îÄ chatcaptioner&#xA;‚îú‚îÄ‚îÄ datasets&#xA;‚îÇ   ‚îú‚îÄ‚îÄ artemis&#xA;‚îÇ   ‚îú‚îÄ‚îÄ coco_val&#xA;‚îÇ   ‚îî‚îÄ‚îÄ cc_val&#xA;‚îÇ       ‚îú‚îÄ‚îÄ annotation.yaml&#xA;‚îÇ       ‚îî‚îÄ‚îÄ img&#xA;‚îÇ           ‚îú‚îÄ‚îÄ annotation.yaml&#xA;‚îÇ           ‚îú‚îÄ‚îÄ 85.jpg&#xA;‚îÇ           ...&#xA;‚îú‚îÄ‚îÄ caption.ipynb&#xA;...   &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To play with ChatCaptioner with a given image, run the following command. It will use GPU 0.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To play with ChatCaptioner with a few dataset samples, check the jupyter script &#39;caption.ipynb&#39;.&lt;/p&gt; &#xA;&lt;p&gt;To caption all the images in the datasets, run &#39;main_caption.py&#39;. Using --exp_tag to tag your runs and using --datasets to specify the datasets you want to caption.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# caption all the sampled images in the datasets &#39;cc_val&#39; and &#39;artemis&#39; using GPU-0 and save results to experiments/test&#xA;python main_caption.py --exp_tag test --datasets cc_val artemis  --device_id 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Datasets available are &#39;artemis&#39;, &#39;cc_val&#39;, &#39;coco_val&#39;, &#39;pascal&#39;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.artemisdataset.org/&#34;&gt;Artemis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cocodataset.org/#home&#34;&gt;MSCOCO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.google.com/research/ConceptualCaptions/&#34;&gt;Conceptual Captions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://host.robots.ox.ac.uk/pascal/VOC/voc2010/&#34;&gt;Pascal VOC&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Visualization&lt;/h2&gt; &#xA;&lt;p&gt;To visualize the caption results, check the jupyter script &#39;visualization.ipynb&#39;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/blog/chatgpt/&#34;&gt;ChatGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/blip-2&#34;&gt;BLIP2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>andri27-ts/Reinforcement-Learning</title>
    <updated>2023-03-22T01:36:59Z</updated>
    <id>tag:github.com,2023-03-22:/andri27-ts/Reinforcement-Learning</id>
    <link href="https://github.com/andri27-ts/Reinforcement-Learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn Deep Reinforcement Learning in 60 days! Lectures &amp; Code in Python. Reinforcement Learning + Deep Learning&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/title3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Course in Deep Reinforcement Learning&lt;/h2&gt; &#xA;&lt;h3&gt;Explore the combination of neural network and reinforcement learning. Algorithms and examples in Python &amp;amp; PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;Have you heard about the amazing results achieved by &lt;a href=&#34;https://www.youtube.com/watch?time_continue=24&amp;amp;v=tXlM99xPQC8&#34;&gt;Deepmind with AlphaGo Zero&lt;/a&gt; and by &lt;a href=&#34;https://www.youtube.com/watch?v=l92J1UvHf6M&#34;&gt;OpenAI in Dota 2&lt;/a&gt;? It&#39;s all about deep neural networks and reinforcement learning. Do you want to know more about it?&lt;br&gt; This is the right opportunity for you to finally learn Deep RL and use it on new and exciting projects and applications.&lt;/p&gt; &#xA;&lt;p&gt;Here you&#39;ll find an in depth introduction to these algorithms. Among which you&#39;ll learn q learning, deep q learning, PPO, actor critic, and implement them using Python and PyTorch.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The ultimate aim is to use these general-purpose technologies and apply them to all sorts of important real world problems. &lt;strong&gt;Demis Hassabis&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This repository contains:&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img align=&#34;left&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/youtube_social_icon_dark.png&#34; alt=&#34;drawing&#34; width=&#34;64&#34;&gt; &lt;strong&gt;Lectures (&amp;amp; other content) primarily from DeepMind and Berkley Youtube&#39;s Channel.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img align=&#34;left&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/GitHub-Mark-64px.png&#34; alt=&#34;drawing&#34; width=&#34;64&#34;&gt; &lt;strong&gt;Algorithms (like DQN, A2C, and PPO) implemented in PyTorch and tested on OpenAI Gym: RoboSchool &amp;amp; Atari.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stay tuned and follow me on&lt;/strong&gt; &lt;a href=&#34;https://twitter.com/andri27_it&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/espadrine.svg?style=social&amp;amp;label=Follow&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/andri27-ts&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/followers/espadrine.svg?style=social&amp;amp;label=Follow&#34; alt=&#34;GitHub followers&#34;&gt;&lt;/a&gt; &lt;strong&gt;#60DaysRLChallenge&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now we have also a &lt;a href=&#34;https://60daysrlchallenge.slack.com/&#34;&gt;&lt;strong&gt;Slack channel&lt;/strong&gt;&lt;/a&gt;. To get an invitation, email me at &lt;a href=&#34;mailto:andrea.lonza@gmail.com&#34;&gt;andrea.lonza@gmail.com&lt;/a&gt;. Also, email me if you have any idea, suggestion or improvement.&lt;/p&gt; &#xA;&lt;p&gt;To learn Deep Learning, Computer Vision or Natural Language Processing check my &lt;strong&gt;&lt;a href=&#34;https://github.com/andri27-ts/1-Year-ML-Journey&#34;&gt;1-Year-ML-Journey&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Before starting.. Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basic level of Python and PyTorch&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andri27-ts/1-Year-ML-Journey&#34;&gt;Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://assoc-redirect.amazon.com/g/r/https://amzn.to/2N3AIlp?tag=andreaaffilia-20&#34;&gt;Basic knowledge in Deep Learning (MLP, CNN and RNN)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Quick Note: my NEW BOOK is out!&lt;/h2&gt; &#xA;&lt;p&gt;To learn Reinforcement Learning and Deep RL more in depth, check out my book &lt;a href=&#34;https://www.amazon.com/Reinforcement-Learning-Algorithms-Python-understand/dp/1789131111&#34;&gt;&lt;strong&gt;Reinforcement Learning Algorithms with Python&lt;/strong&gt;&lt;/a&gt;!!&lt;/p&gt; &#xA;&lt;a href=&#34;https://www.amazon.com/Reinforcement-Learning-Algorithms-Python-understand/dp/1789131111&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/frontcover2.jpg&#34; alt=&#34;drawing&#34; width=&#34;350&#34; align=&#34;right&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The Landscape of Reinforcement Learning&lt;/li&gt; &#xA; &lt;li&gt;Implementing RL Cycle and OpenAI Gym&lt;/li&gt; &#xA; &lt;li&gt;Solving Problems with Dynamic Programming&lt;/li&gt; &#xA; &lt;li&gt;Q learning and SARSA Applications&lt;/li&gt; &#xA; &lt;li&gt;Deep Q-Network&lt;/li&gt; &#xA; &lt;li&gt;Learning Stochastic and DDPG optimization&lt;/li&gt; &#xA; &lt;li&gt;TRPO and PPO implementation&lt;/li&gt; &#xA; &lt;li&gt;DDPG and TD3 Applications&lt;/li&gt; &#xA; &lt;li&gt;Model-Based RL&lt;/li&gt; &#xA; &lt;li&gt;Imitation Learning with the DAgger Algorithm&lt;/li&gt; &#xA; &lt;li&gt;Understanding Black-Box Optimization Algorithms&lt;/li&gt; &#xA; &lt;li&gt;Developing the ESBAS Algorithm&lt;/li&gt; &#xA; &lt;li&gt;Practical Implementation for Resolving RL Challenges&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Index - Reinforcement Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andri27-ts/60_Days_RL_Challenge#week-1---introduction&#34;&gt;Week 1 - &lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andri27-ts/60_Days_RL_Challenge#week-2---rl-basics-mdp-dynamic-programming-and-model-free-control&#34;&gt;Week 2 - &lt;strong&gt;RL Basics&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andri27-ts/60_Days_RL_Challenge#week-3---value-function-approximation-and-dqn&#34;&gt;Week 3 - &lt;strong&gt;Value based algorithms - DQN&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andri27-ts/60_Days_RL_Challenge#week-4---policy-gradient-methods-and-a2c&#34;&gt;Week 4 - &lt;strong&gt;Policy gradient algorithms - REINFORCE &amp;amp; A2C&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andri27-ts/60_Days_RL_Challenge#week-5---advanced-policy-gradients---trpo--ppo&#34;&gt;Week 5 - &lt;strong&gt;Advanced Policy Gradients - PPO&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andri27-ts/60_Days_RL_Challenge#week-6---evolution-strategies-and-genetic-algorithms&#34;&gt;Week 6 - &lt;strong&gt;Evolution Strategies and Genetic Algorithms - ES&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andri27-ts/60_Days_RL_Challenge#week-7---model-based-reinforcement-learning&#34;&gt;Week 7 - &lt;strong&gt;Model-Based reinforcement learning - MB-MF&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andri27-ts/60_Days_RL_Challenge/raw/master/README.md#week-8---advanced-concepts-and-project-of-your-choice&#34;&gt;Week 8 - &lt;strong&gt;Advanced Concepts and Project Of Your Choice&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andri27-ts/60_Days_RL_Challenge/raw/master/README.md#last-4-days---review--sharing&#34;&gt;Last 4 days - &lt;strong&gt;Review + Sharing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andri27-ts/60_Days_RL_Challenge#best-resources&#34;&gt;Best resources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andri27-ts/60_Days_RL_Challenge#additional-resources&#34;&gt;Additional resources&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Week 1 - Introduction&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://medium.com/@andrea.lonzats/the-learning-machines-fb922e539335&#34;&gt;Why is Reinforcement Learning such an important learning method - A simple explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Q4kF8sfggoI&amp;amp;index=1&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&#34;&gt;Introduction and course overview&lt;/a&gt; - CS294 by Levine, Berkley&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://karpathy.github.io/2016/05/31/rl/&#34;&gt;Deep Reinforcement Learning: Pong from Pixels&lt;/a&gt; by Karpathy&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;Other Resources&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://assoc-redirect.amazon.com/g/r/https://amzn.to/2HRSSmh?tag=andreaaffilia-20&#34;&gt;The &#34;Bible&#34; of Reinforcement Learning: Chapter 1&lt;/a&gt; - Sutton &amp;amp; Barto&lt;/li&gt; &#xA; &lt;li&gt;Great introductory paper: &lt;a href=&#34;https://www.groundai.com/project/deep-reinforcement-learning-an-overview/&#34;&gt;Deep Reinforcement Learning: An Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Start coding: &lt;a href=&#34;https://towardsdatascience.com/from-scratch-ai-balancing-act-in-50-lines-of-python-7ea67ef717&#34;&gt;From Scratch: AI Balancing Act in 50 Lines of Python&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Week 2 - RL Basics: &lt;em&gt;MDP, Dynamic Programming and Model-Free Control&lt;/em&gt;&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Those who cannot remember the past are condemned to repeat it - &lt;strong&gt;George Santayana&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This week, we will learn about the basic blocks of reinforcement learning, starting from the definition of the problem all the way through the estimation and optimization of the functions that are used to express the quality of a policy or state.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Lectures - Theory &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/youtube_social_icon_dark.png&#34; alt=&#34;drawing&#34; width=&#34;48&#34;&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=lfHX2hHRMVQ&amp;amp;list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&amp;amp;index=2&#34;&gt;Markov Decision Process&lt;/a&gt; - David Silver (DeepMind)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Markov Processes&lt;/li&gt; &#xA;   &lt;li&gt;Markov Decision Processes&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;amp;list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&amp;amp;index=3&#34;&gt;Planning by Dynamic Programming&lt;/a&gt; - David Silver (DeepMind)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Policy iteration&lt;/li&gt; &#xA;   &lt;li&gt;Value iteration&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;amp;index=4&amp;amp;list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&#34;&gt;Model-Free Prediction&lt;/a&gt; - David Silver (DeepMind)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Monte Carlo Learning&lt;/li&gt; &#xA;   &lt;li&gt;Temporal Difference Learning&lt;/li&gt; &#xA;   &lt;li&gt;TD(Œª)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&amp;amp;index=5&#34;&gt;Model-Free Control&lt;/a&gt; - David Silver (DeepMind)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;∆ê-greedy policy iteration&lt;/li&gt; &#xA;   &lt;li&gt;GLIE Monte Carlo Search&lt;/li&gt; &#xA;   &lt;li&gt;SARSA&lt;/li&gt; &#xA;   &lt;li&gt;Importance Sampling&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Project of the Week - &lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week2/frozenlake_Qlearning.ipynb&#34;&gt;&lt;strong&gt;Q-learning&lt;/strong&gt;&lt;/a&gt; &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/GitHub-Mark-64px.png&#34; alt=&#34;drawing&#34; width=&#34;48&#34;&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week2/frozenlake_Qlearning.ipynb&#34;&gt;&lt;strong&gt;Q-learning applied to FrozenLake&lt;/strong&gt;&lt;/a&gt; - For exercise, you can solve the game using SARSA or implement Q-learning by yourself. In the former case, only few changes are needed.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;Other Resources&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://assoc-redirect.amazon.com/g/r/https://amzn.to/2HRSSmh?tag=andreaaffilia-20&#34;&gt;The &#34;Bible&#34; of Reinforcement Learning: Chapters 3 and 4&lt;/a&gt; - Sutton &amp;amp; Barto&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üì∫&lt;/span&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=k1vNh4rNYec&amp;amp;index=6&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&#34;&gt;Value functions introduction&lt;/a&gt; - DRL UC Berkley by Sergey Levine&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Week 3 - Value based algorithms - DQN&lt;/h2&gt; &#xA;&lt;p&gt;This week we&#39;ll learn more advanced concepts and apply deep neural network to Q-learning algorithms.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Lectures - Theory &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/youtube_social_icon_dark.png&#34; alt=&#34;drawing&#34; width=&#34;48&#34;&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=UoPei5o4fps&amp;amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&amp;amp;index=6&#34;&gt;Value functions approximation&lt;/a&gt; - David Silver (DeepMind)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Differentiable function approximators&lt;/li&gt; &#xA;   &lt;li&gt;Incremental methods&lt;/li&gt; &#xA;   &lt;li&gt;Batch methods (DQN)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nZXC5OdDfs4&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&amp;amp;index=7&#34;&gt;Advanced Q-learning algorithms&lt;/a&gt; - Sergey Levine (UC Berkley)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Replay Buffer&lt;/li&gt; &#xA;   &lt;li&gt;Double Q-learning&lt;/li&gt; &#xA;   &lt;li&gt;Continous actions (NAF,DDPG)&lt;/li&gt; &#xA;   &lt;li&gt;Pratical tips&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Project of the Week - &lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week3&#34;&gt;&lt;strong&gt;DQN and variants&lt;/strong&gt;&lt;/a&gt; &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/GitHub-Mark-64px.png&#34; alt=&#34;drawing&#34; width=&#34;48&#34;&gt;&lt;/h3&gt; &#xA;&lt;img align=&#34;left&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week3/imgs/pong_gif.gif&#34; alt=&#34;drawing&#34; width=&#34;200&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week3&#34;&gt;&lt;strong&gt;DQN and some variants applied to Pong&lt;/strong&gt;&lt;/a&gt; - This week the goal is to develop a DQN algorithm to play an Atari game. To make it more interesting I developed three extensions of DQN: &lt;strong&gt;Double Q-learning&lt;/strong&gt;, &lt;strong&gt;Multi-step learning&lt;/strong&gt;, &lt;strong&gt;Dueling networks&lt;/strong&gt; and &lt;strong&gt;Noisy Nets&lt;/strong&gt;. Play with them, and if you feel confident, you can implement Prioritized replay, Dueling networks or Distributional RL. To know more about these improvements read the papers!&lt;/p&gt; &#xA;&lt;br clear=&#34;left&#34;&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;Papers&lt;/h4&gt; &#xA;&lt;h5&gt;Must Read&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1312.5602.pdf&#34;&gt;Playing Atari with Deep Reinforcement Learning&lt;/a&gt; - 2013&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf&#34;&gt;Human-level control through deep reinforcement learning&lt;/a&gt; - 2015&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.02298.pdf&#34;&gt;Rainbow: Combining Improvements in Deep Reinforcement Learning&lt;/a&gt; - 2017&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Extensions of DQN&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1509.06461.pdf&#34;&gt;Deep Reinforcement Learning with Double Q-learning&lt;/a&gt; - 2015&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1511.05952.pdf&#34;&gt;Prioritized Experience Replay&lt;/a&gt; - 2015&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v48/wangf16.pdf&#34;&gt;Dueling Network Architectures for Deep Reinforcement Learning&lt;/a&gt; - 2016&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.10295.pdf&#34;&gt;Noisy networks for exploration&lt;/a&gt; - 2017&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.10044.pdf&#34;&gt;Distributional Reinforcement Learning with Quantile Regression&lt;/a&gt; - 2017&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Other Resources&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://assoc-redirect.amazon.com/g/r/https://amzn.to/2HRSSmh?tag=andreaaffilia-20&#34;&gt;The &#34;Bible&#34; of Reinforcement Learning: Chapters 5 and 6&lt;/a&gt; - Sutton &amp;amp; Barto&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üì∫&lt;/span&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=GOsUHlr4DKE&#34;&gt;Deep Reinforcement Learning in the Enterprise: Bridging the Gap from Games to Industry&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Week 4 - Policy gradient algorithms - REINFORCE &amp;amp; A2C&lt;/h2&gt; &#xA;&lt;p&gt;Week 4 introduce Policy Gradient methods, a class of algorithms that optimize directly the policy. Also, you&#39;ll learn about Actor-Critic algorithms. These algorithms combine both policy gradient (the actor) and value function (the critic).&lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Lectures - Theory &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/youtube_social_icon_dark.png&#34; alt=&#34;drawing&#34; width=&#34;48&#34;&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=KHZVXao4qXs&amp;amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&amp;amp;index=7&#34;&gt;Policy gradient Methods&lt;/a&gt; - David Silver (DeepMind)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Finite Difference Policy Gradient&lt;/li&gt; &#xA;   &lt;li&gt;Monte-Carlo Policy Gradient&lt;/li&gt; &#xA;   &lt;li&gt;Actor-Critic Policy Gradient&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=XGmd3wcyDg8&amp;amp;t=0s&amp;amp;list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&amp;amp;index=3&#34;&gt;Policy gradient intro&lt;/a&gt; - Sergey Levine (RECAP, optional)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Policy Gradient (REINFORCE and Vanilla PG)&lt;/li&gt; &#xA;   &lt;li&gt;Variance reduction&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Tol_jw5hWnI&amp;amp;list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&amp;amp;index=4&#34;&gt;Actor-Critic&lt;/a&gt; - Sergey Levine (More in depth)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Actor-Critic&lt;/li&gt; &#xA;   &lt;li&gt;Discout factor&lt;/li&gt; &#xA;   &lt;li&gt;Actor-Critic algorithm design (batch mode or online)&lt;/li&gt; &#xA;   &lt;li&gt;state-dependent baseline&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Project of the Week - &lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week4&#34;&gt;&lt;strong&gt;Vanilla PG and A2C&lt;/strong&gt;&lt;/a&gt; &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/GitHub-Mark-64px.png&#34; alt=&#34;drawing&#34; width=&#34;48&#34;&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week4&#34;&gt;&lt;strong&gt;Vanilla PG and A2C applied to CartPole&lt;/strong&gt;&lt;/a&gt; - The exercise of this week is to implement a policy gradient method or a more sophisticated actor-critic. In the repository you can find an implemented version of &lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week4&#34;&gt;PG and A2C&lt;/a&gt;. Bug Alert! Pay attention that A2C give me strange result. If you find the implementation of PG and A2C easy, you can try with the &lt;a href=&#34;https://arxiv.org/pdf/1602.01783.pdf&#34;&gt;asynchronous version of A2C (A3C)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;Papers&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf&#34;&gt;Policy Gradient methods for reinforcement learning with function approximation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1602.01783.pdf&#34;&gt;Asynchronous Methods for Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Other Resources&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://assoc-redirect.amazon.com/g/r/https://amzn.to/2HRSSmh?tag=andreaaffilia-20&#34;&gt;The &#34;Bible&#34; of Reinforcement Learning: Chapters 9 and 10&lt;/a&gt; - Sutton &amp;amp; Barto&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752&#34;&gt;Intuitive RL: Intro to Advantage-Actor-Critic (A2C)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2&#34;&gt;Asynchronous Actor-Critic Agents (A3C)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Week 5 - Advanced Policy Gradients - PPO&lt;/h2&gt; &#xA;&lt;p&gt;This week is about advanced policy gradient methods that improve the stability and the convergence of the &#34;Vanilla&#34; policy gradient methods. You&#39;ll learn and implement PPO, a RL algorithm developed by OpenAI and adopted in &lt;a href=&#34;https://blog.openai.com/openai-five/&#34;&gt;OpenAI Five&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Lectures - Theory &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/youtube_social_icon_dark.png&#34; alt=&#34;drawing&#34; width=&#34;48&#34;&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ycCtmp4hcUs&amp;amp;t=0s&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&amp;amp;index=15&#34;&gt;Advanced policy gradients&lt;/a&gt; - Sergey Levine (UC Berkley)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Problems with &#34;Vanilla&#34; Policy Gradient Methods&lt;/li&gt; &#xA;   &lt;li&gt;Policy Performance Bounds&lt;/li&gt; &#xA;   &lt;li&gt;Monotonic Improvement Theory&lt;/li&gt; &#xA;   &lt;li&gt;Algorithms: NPO, TRPO, PPO&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xvRrgxcpaHY&#34;&gt;Natural Policy Gradients, TRPO, PPO&lt;/a&gt; - John Schulman (Berkey DRL Bootcamp)&lt;/strong&gt; - (RECAP, optional) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Limitations of &#34;Vanilla&#34; Policy Gradient Methods&lt;/li&gt; &#xA;   &lt;li&gt;Natural Policy Gradient&lt;/li&gt; &#xA;   &lt;li&gt;Trust Region Policy Optimization, TRPO&lt;/li&gt; &#xA;   &lt;li&gt;Proximal Policy Optimization, PPO&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Project of the Week - &lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week5&#34;&gt;&lt;strong&gt;PPO&lt;/strong&gt;&lt;/a&gt; &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/GitHub-Mark-64px.png&#34; alt=&#34;drawing&#34; width=&#34;48&#34;&gt;&lt;/h3&gt; &#xA;&lt;img align=&#34;left&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week5/imgs/walker_gif.gif&#34; alt=&#34;drawing&#34; width=&#34;300&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week5&#34;&gt;&lt;strong&gt;PPO applied to BipedalWalker&lt;/strong&gt;&lt;/a&gt; - This week, you have to implement PPO or TRPO. I suggest PPO given its simplicity (compared to TRPO). In the project folder Week5 you find an implementation of &lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week5&#34;&gt;&lt;strong&gt;PPO that learn to play BipedalWalker&lt;/strong&gt;&lt;/a&gt;. Furthermore, in the folder you can find other resources that will help you in the development of the project. Have fun!&lt;/p&gt; &#xA;&lt;br clear=&#34;left&#34;&gt; &#xA;&lt;p&gt;To learn more about PPO read the &lt;a href=&#34;https://arxiv.org/pdf/1707.06347.pdf&#34;&gt;paper&lt;/a&gt; and take a look at the &lt;a href=&#34;https://www.youtube.com/watch?v=5P7I-xPq8u8&#34;&gt;Arxiv Insights&#39;s video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;Papers&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1502.05477.pdf&#34;&gt;Trust Region Policy Optimization&lt;/a&gt; - 2015&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1707.06347.pdf&#34;&gt;Proximal Policy Optimization Algorithms&lt;/a&gt; - 2017&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Other Resources&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;üìö&lt;/span&gt; To better understand PPO and TRPO: &lt;a href=&#34;https://towardsdatascience.com/the-pursuit-of-robotic-happiness-how-trpo-and-ppo-stabilize-policy-gradient-methods-545784094e3b&#34;&gt;The Pursuit of (Robotic) Happiness&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üì∫&lt;/span&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=8EcdaCk9KaQ&amp;amp;&#34;&gt;Nuts and Bolts of Deep RL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üìö&lt;/span&gt; PPO best practice: &lt;a href=&#34;https://github.com/Unity-Technologies/ml-agents/raw/master/docs/Training-PPO.md&#34;&gt;Training with Proximal Policy Optimization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üì∫&lt;/span&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=5P7I-xPq8u8&#34;&gt;Explanation of the PPO algorithm by Arxiv Insights&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Week 6 - Evolution Strategies and Genetic Algorithms - ES&lt;/h2&gt; &#xA;&lt;p&gt;In the last year, Evolution strategies (ES) and Genetic Algorithms (GA) has been shown to achieve comparable results to RL methods. They are derivate-free black-box algorithms that require more data than RL to learn but are able to scale up across thousands of CPUs. This week we&#39;ll look at this black-box algorithms.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Lectures &amp;amp; Articles - Theory &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/youtube_social_icon_dark.png&#34; alt=&#34;drawing&#34; width=&#34;48&#34;&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evolution Strategies&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;http://blog.otoro.net/2017/10/29/visual-evolution-strategies/&#34;&gt;Intro to ES: A Visual Guide to Evolution Strategies&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;http://blog.otoro.net/2017/11/12/evolving-stable-strategies/&#34;&gt;ES for RL: Evolving Stable Strategies&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SQtOI9jsrJ0&amp;amp;feature=youtu.be&#34;&gt;Derivative-free Methods - Lecture&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://blog.openai.com/evolution-strategies/&#34;&gt;Evolution Strategies (paper discussion)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Genetic Algorithms&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3&#34;&gt;Introduction to Genetic Algorithms‚Ää‚Äî‚ÄäIncluding Example Code&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Project of the Week - &lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week6&#34;&gt;&lt;strong&gt;ES&lt;/strong&gt;&lt;/a&gt; &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/GitHub-Mark-64px.png&#34; alt=&#34;drawing&#34; width=&#34;48&#34;&gt;&lt;/h3&gt; &#xA;&lt;img align=&#34;left&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week6/imgs/LunarLanderContinuous.gif&#34; alt=&#34;drawing&#34; width=&#34;300&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week6&#34;&gt;&lt;strong&gt;Evolution Strategies applied to LunarLander&lt;/strong&gt;&lt;/a&gt; - This week the project is to implement a ES or GA. In the &lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week6&#34;&gt;&lt;strong&gt;Week6 folder&lt;/strong&gt;&lt;/a&gt; you can find a basic implementation of the paper &lt;a href=&#34;https://arxiv.org/pdf/1703.03864.pdf&#34;&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning&lt;/a&gt; to solve LunarLanderContinuous. You can modify it to play more difficult environments or add your ideas.&lt;/p&gt; &#xA;&lt;br clear=&#34;left&#34;&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;Papers&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1712.06567.pdf&#34;&gt;Deep Neuroevolution: Genetic Algorithms are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.03864.pdf&#34;&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Other Resources&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://assoc-redirect.amazon.com/g/r/https://amzn.to/34EphXc?tag=andreaaffilia-20&#34;&gt;Evolutionary Optimization Algorithms&lt;/a&gt; - Dan Simon&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Week 7 - Model-Based reinforcement learning - MB-MF&lt;/h2&gt; &#xA;&lt;p&gt;The algorithms studied up to now are model-free, meaning that they only choose the better action given a state. These algorithms achieve very good performance but require a lot of training data. Instead, model-based algorithms, learn the environment and plan the next actions accordingly to the model learned. These methods are more sample efficient than model-free but overall achieve worst performance. In this week you&#39;ll learn the theory behind these methods and implement one of the last algorithms.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Lectures - Theory &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/youtube_social_icon_dark.png&#34; alt=&#34;drawing&#34; width=&#34;48&#34;&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model-Based RL, David Silver (DeepMind) (concise version)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ItMutbeOHtc&amp;amp;index=8&amp;amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&#34;&gt;Integrating Learning and Planning&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Model-Based RL Overview&lt;/li&gt; &#xA;     &lt;li&gt;Integrated architectures&lt;/li&gt; &#xA;     &lt;li&gt;Simulation-Based search&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model-Based RL, Sergey Levine (UC Berkley) (in depth version)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=yap_g0d7iBQ&amp;amp;index=9&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&#34;&gt;Learning dynamical systems from data&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Overview of model-based RL&lt;/li&gt; &#xA;     &lt;li&gt;Global and local models&lt;/li&gt; &#xA;     &lt;li&gt;Learning with local models and trust regions&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=AwdauFLan7M&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&amp;amp;index=10&#34;&gt;Learning policies by imitating optimal controllers&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Backpropagation into a policy with learned models&lt;/li&gt; &#xA;     &lt;li&gt;Guided policy search algorithm&lt;/li&gt; &#xA;     &lt;li&gt;Imitating optimal control with DAgger&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=vRkIwM4GktE&amp;amp;index=11&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&#34;&gt;Advanced model learning and images&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Models in latent space&lt;/li&gt; &#xA;     &lt;li&gt;Models directly in image space&lt;/li&gt; &#xA;     &lt;li&gt;Inverse models&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Project of the Week - &lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week7&#34;&gt;&lt;strong&gt;MB-MF&lt;/strong&gt;&lt;/a&gt; &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/GitHub-Mark-64px.png&#34; alt=&#34;drawing&#34; width=&#34;48&#34;&gt;&lt;/h3&gt; &#xA;&lt;img align=&#34;left&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week7/imgs/animation.gif&#34; alt=&#34;drawing&#34; width=&#34;300&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week7&#34;&gt;&lt;strong&gt;MB-MF applied to RoboschoolAnt&lt;/strong&gt;&lt;/a&gt; - This week I chose to implement the model-based algorithm described in this &lt;a href=&#34;https://arxiv.org/pdf/1708.02596.pdf&#34;&gt;paper&lt;/a&gt;. You can find my implementation &lt;a href=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/Week7&#34;&gt;here&lt;/a&gt;. NB: Instead of implementing it on Mujoco as in the paper, I used &lt;a href=&#34;https://github.com/openai/roboschool&#34;&gt;RoboSchool&lt;/a&gt;, an open-source simulator for robot, integrated with OpenAI Gym.&lt;/p&gt; &#xA;&lt;br clear=&#34;left&#34;&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;Papers&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1707.06203.pdf&#34;&gt;Imagination-Augmented Agents for Deep Reinforcement Learning - 2017&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1611.05397.pdf&#34;&gt;Reinforcement learning with unsupervised auxiliary tasks - 2016&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1708.02596.pdf&#34;&gt;Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning - 2018&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Other Resources&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://assoc-redirect.amazon.com/g/r/https://amzn.to/2HRSSmh?tag=andreaaffilia-20&#34;&gt;The &#34;Bible&#34; of Reinforcement Learning: Chapter 8&lt;/a&gt; - Sutton &amp;amp; Barto&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://worldmodels.github.io/&#34;&gt;World Models - Can agents learn inside of their own dreams?&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Week 8 - Advanced Concepts and Project Of Your Choice&lt;/h2&gt; &#xA;&lt;p&gt;This last week is about advanced RL concepts and a project of your choice.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Lectures - Theory &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/andri27-ts/Reinforcement-Learning/master/images/youtube_social_icon_dark.png&#34; alt=&#34;drawing&#34; width=&#34;48&#34;&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sergey Levine (Berkley) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=iOYiPhu5GEk&amp;amp;index=13&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&amp;amp;t=0s&#34;&gt;Connection between inference and control&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-3BcZwgmZLk&amp;amp;index=14&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&amp;amp;t=0s&#34;&gt;Inverse reinforcement learning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=npi6B4VQ-7s&amp;amp;index=16&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&amp;amp;t=0s&#34;&gt;Exploration (part 1)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=0WbVUvKJpg4&amp;amp;index=17&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&amp;amp;t=0s&#34;&gt;Exploration (part 2) and transfer learning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=UqSx23W9RYE&amp;amp;index=18&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&amp;amp;t=0s&#34;&gt;Multi-task learning and transfer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Xe9bktyYB34&amp;amp;index=18&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&#34;&gt;Meta-learning and parallelism&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=mc-DtbhhiKA&amp;amp;index=20&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&amp;amp;t=0s&#34;&gt;Advanced imitation learning and open problems&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;David Silver (DeepMind) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=N1LKLc6ufGY&amp;amp;feature=youtu.be&#34;&gt;Classic Games&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;The final project&lt;/h3&gt; &#xA;&lt;p&gt;Here you can find some project ideas.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pommerman.com/&#34;&gt;Pommerman&lt;/a&gt; (Multiplayer)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge&#34;&gt;AI for Prosthetics Challenge&lt;/a&gt; (Challenge)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://worldmodels.github.io/&#34;&gt;Word Models&lt;/a&gt; (Paper implementation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.openai.com/requests-for-research-2/&#34;&gt;Request for research OpenAI&lt;/a&gt; (Research)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.openai.com/retro-contest/&#34;&gt;Retro Contest&lt;/a&gt; (Transfer learning)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;Other Resources&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AlphaGo Zero &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ&#34;&gt;Paper&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;DeepMind blog post: &lt;a href=&#34;https://deepmind.com/blog/alphago-zero-learning-scratch/&#34;&gt;AlphaGo Zero: Learning from scratch&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Arxiv Insights video: &lt;a href=&#34;https://www.youtube.com/watch?v=MgowR4pq3e8&#34;&gt;How AlphaGo Zero works - Google DeepMind&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;OpenAI Five &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OpenAI blog post: &lt;a href=&#34;https://blog.openai.com/openai-five/&#34;&gt;OpenAI Five&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Arxiv Insights video: &lt;a href=&#34;https://www.youtube.com/watch?v=0eO2TSVVP1Y&#34;&gt;OpenAI Five: Facing Human Pro&#39;s in Dota II&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Last 4 days - Review + Sharing&lt;/h2&gt; &#xA;&lt;p&gt;Congratulation for completing the 60 Days RL Challenge!! Let me know if you enjoyed it and share it!&lt;/p&gt; &#xA;&lt;p&gt;See you!&lt;/p&gt; &#xA;&lt;h2&gt;Best resources&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://assoc-redirect.amazon.com/g/r/https://amzn.to/2HRSSmh?tag=andreaaffilia-20&#34;&gt;Reinforcement Learning: An Introduction&lt;/a&gt; - by Sutton &amp;amp; Barto. The &#34;Bible&#34; of reinforcement learning. &lt;a href=&#34;https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view&#34;&gt;Here&lt;/a&gt; you can find the PDF draft of the second version.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://assoc-redirect.amazon.com/g/r/https://amzn.to/2PRxKD7?tag=andreaaffilia-20&#34;&gt;Deep Reinforcement Learning Hands-On&lt;/a&gt; - by Maxim Lapan&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://assoc-redirect.amazon.com/g/r/https://amzn.to/2N3AIlp?tag=andreaaffilia-20&#34;&gt;Deep Learning&lt;/a&gt; - Ian Goodfellow&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üì∫&lt;/span&gt; &lt;a href=&#34;https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&#34;&gt;Deep Reinforcement Learning&lt;/a&gt; - UC Berkeley class by Levine, check &lt;a href=&#34;http://rail.eecs.berkeley.edu/deeprlcourse/&#34;&gt;here&lt;/a&gt; their site.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üì∫&lt;/span&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&#34;&gt;Reinforcement Learning course&lt;/a&gt; - by David Silver, DeepMind. Great introductory lectures by Silver, a lead researcher on AlphaGo. They follow the book Reinforcement Learning by Sutton &amp;amp; Barto.&lt;/p&gt; &#xA;&lt;h2&gt;Additional resources&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://github.com/aikorea/awesome-rl&#34;&gt;Awesome Reinforcement Learning&lt;/a&gt;. A curated list of resources dedicated to reinforcement learning&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üìö&lt;/span&gt; &lt;a href=&#34;https://www.groundai.com/?text=reinforcement+learning&#34;&gt;GroundAI on RL&lt;/a&gt;. Papers on reinforcement learning&lt;/p&gt; &#xA;&lt;h2&gt;A cup of Coffe &lt;span&gt;‚òï&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Any contribution is higly appreciated! Cheers!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;amp;hosted_button_id=NKSNP93CNY4KN&#34;&gt;&lt;img src=&#34;https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif&#34; alt=&#34;paypal&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>