<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-16T01:36:34Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>facebookresearch/perception_models</title>
    <updated>2025-08-16T01:36:34Z</updated>
    <id>tag:github.com,2025-08-16:/facebookresearch/perception_models</id>
    <link href="https://github.com/facebookresearch/perception_models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;State-of-the-art Image &amp; Video CLIP, Multimodal Large Language Models, and More!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Perception Models: Powerful Models for Image and Video Perception&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code_License-Apache_2.0-olive&#34; alt=&#34;Code License&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo is the home to the state-of-the-art for image and video &lt;em&gt;perception&lt;/em&gt;: &lt;a href=&#34;https://arxiv.org/abs/2504.13181&#34;&gt;&lt;strong&gt;Perception Encoder (PE)&lt;/strong&gt;&lt;/a&gt; for image and video encoding and &lt;a href=&#34;https://arxiv.org/abs/2504.13180&#34;&gt;&lt;strong&gt;Perception Language Model (PLM)&lt;/strong&gt;&lt;/a&gt; for decoding.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Click to Navigate!&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/#perception-encoder-pe&#34;&gt;Perception Encoder&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/#perception-language-model-plm&#34;&gt;Perception Language Model&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[Jul-14-25]:&lt;/strong&gt; PerceptionLM is now available in &lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/model_doc/perception_lm&#34;&gt;Hugging Face transformers&lt;/a&gt;. &lt;span&gt;🔥&lt;/span&gt;&lt;span&gt;🔥&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[Jul-11-25]:&lt;/strong&gt; We have release 8 new checkpoints for &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/apps/pe/README.md&#34;&gt;Perception Encoder&lt;/a&gt;: 2x small core models (T and S), 2x tiling-tuned lang models (G and L), and 4x smaller spatial models (L, B, S, T). Give them a try! &lt;span&gt;🔥&lt;/span&gt;&lt;span&gt;🔥&lt;/span&gt;&lt;span&gt;🔥&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[May-28-25]:&lt;/strong&gt; Perception Encoder has been integrated into &lt;a href=&#34;https://github.com/huggingface/pytorch-image-models&#34;&gt;timm&lt;/a&gt;! &lt;span&gt;🔥&lt;/span&gt;&lt;span&gt;🔥&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[Apr-18-25]:&lt;/strong&gt; Perception Language Model (PLM) and PLM-VideoBench are added to lmms-eval. This makes it easy to reproduce PLM results and allows you to evaluate on the PLM-VideoBench. [&lt;a href=&#34;https://github.com/EvolvingLMMs-Lab/lmms-eval/pull/638&#34;&gt;&lt;code&gt;lmms-eval&lt;/code&gt;&lt;/a&gt;] &lt;span&gt;🔥&lt;/span&gt;&lt;span&gt;🔥&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[Apr-17-25]:&lt;/strong&gt; Perception Encoder (PE) and Perception Language Model (PLM) are released. [&lt;a href=&#34;https://ai.meta.com/blog/meta-fair-updates-perception-localization-reasoning&#34;&gt;&lt;code&gt;Blog&lt;/code&gt;&lt;/a&gt;] &lt;span&gt;🔥&lt;/span&gt;&lt;span&gt;🔥&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Perception Encoder (PE)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/collections/facebook/perception-encoder-67f977c9a65ca5895a7f6ba1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Collection-blue&#34; alt=&#34;Hugging Face Collection&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://ai.meta.com/research/publications/perception-encoder-the-best-visual-embeddings-are-not-at-the-output-of-the-network&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Technical%20Report-Perception%20Encoder-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2504.13181&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2504.13181-brightgreen.svg?style=flat-square&#34; alt=&#34;Paper&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/perception_models/blob/main/apps/pe/docs/pe_demo.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo&amp;amp;message=Google%20Colab&amp;amp;logo=google&amp;amp;color=orange&#34; alt=&#34;Colab Demo&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model_License-Apache_2.0-olive&#34; alt=&#34;Model License&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2504.13181&#34;&gt;Perception Encoder (PE)&lt;/a&gt; is a family of the state-of-the-art vision encoders for encoding images and video: PE core outperforms SigLIP2 on image and InternVideo2 on video bencmarks; PE lang can be used to outperform QwenVL2.5 and InternVL3 on vision language modeling; and PE spatial outperforms DINOv2 on dense prediction tasks. And all of this follows the same, easily scalable contrastive pretraining. Please see &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/apps/pe/README.md&#34;&gt;README&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/apps/pe/docs/assets/teaser.png&#34; style=&#34;width: 100%; margin: 0 auto; display: block;&#34; /&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;PE has 3 types of checkpoints, each excelling in a different area of computer vision:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/#perception-encoder-core&#34;&gt;PE core&lt;/a&gt;: a CLIP model excels in vision-language tasks such as zero-shot image and video classification and video retrieval.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/#perception-encoder-language&#34;&gt;PE lang&lt;/a&gt;: a LLM-aligned PE that powers &lt;a href=&#34;https://arxiv.org/abs/2504.13180&#34;&gt;PLM)&lt;/a&gt; to compete at the forefront of multimodal LLM benchmarks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/#perception-encoder-spatial&#34;&gt;PE spatial&lt;/a&gt;: a spatially tuned PE that outperforms best spatial models for vision-centric tasks such as detection, depth estimation, and tracking.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Vision-Language Benchmarks&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;IN-1k&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;IN-v2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;IN-A&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ObjectNet&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;COCO-T2I&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Kinetics-400&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;VTT-T2V&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🆕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;T/16&lt;/strong&gt; 384px&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/PE-Core-T16-384&#34;&gt;PE-Core-T16-384&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🆕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;S/16&lt;/strong&gt; 384px&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/PE-Core-S16-384&#34;&gt;PE-Core-S16-384&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;B/16&lt;/strong&gt; 224px&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/PE-Core-B16-224&#34;&gt;PE-Core-B16-224&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;L/14&lt;/strong&gt; 336px&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/PE-Core-L14-336&#34;&gt;PE-Core-L14-336&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;G/14&lt;/strong&gt; 448px&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/PE-Core-G14-448&#34;&gt;PE-Core-G14-448&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Multimodal LLM Benchmarks&lt;/h4&gt; &#xA;&lt;p&gt;🔬 Controlled Setting:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Encoder&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Doc VQA (val)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;InfoQA (val)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TextVQA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MVBench&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PerceptionTest (val)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;EgoSchema (val)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;L/14&lt;/strong&gt; 448px&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/PE-Lang-L14-448&#34;&gt;PE-Lang-L14-448&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;G/14&lt;/strong&gt; 448px&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/PE-Lang-G14-448&#34;&gt;PE-Lang-G14-448&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;🔥 SotA Setting:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Encoder&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Doc VQA (test)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;InfoQA (test)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TextVQA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MVBench&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PerceptionTest (test)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;EgoSchema (test)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🆕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PLM-3B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/PE-Lang-L14-448-Tiling&#34;&gt;PE-Lang-L14-448-Tiling&lt;/a&gt;*&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;93.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🆕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PLM-8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/PE-Lang-G14-448-Tiling&#34;&gt;PE-Lang-G14-448-Tiling&lt;/a&gt;*&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;94.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;* These checkpoints were aligned with tiling. Use them if you use higher than 448 resolution with tiling in the LLM decoder.&lt;/p&gt; &#xA;&lt;h4&gt;Vision-centric Benchmarks&lt;/h4&gt; &#xA;&lt;p&gt;🦾 Main model:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Encoder&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ADE20k &lt;br /&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;Segmentation&lt;/a&gt;&lt;br /&gt;Linear Probe mIoU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DAVIS&lt;br /&gt; &lt;a href=&#34;https://github.com/facebookresearch/dino/raw/main/eval_video_segmentation.py&#34;&gt;Tracking&lt;/a&gt; &lt;br /&gt;Zero-Shot J&amp;amp;F&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;LVIS &lt;br /&gt; &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/detection/detectron2_pe/&#34;&gt;Mask R-CNN&lt;/a&gt; 1024px &lt;br /&gt; Box / Mask mAP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;COCO &lt;br /&gt; &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/detection/DETA_pe/&#34;&gt;DETA&lt;/a&gt; 1824px &lt;br /&gt; Box mAP&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;G/14&lt;/strong&gt; 448px&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/PE-Spatial-G14-448&#34;&gt;PE-Spatial-G14-448&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.2 / 49.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/apps/pe/docs/assets/spatial_correspondence.png&#34; style=&#34;width: 80%; margin: 0 auto; padding-top: 20px; padding-bottom: 20px; display: block;&#34; /&gt; &#xA; &lt;p&gt;Visualization of PCA of non-maked visual tokens, mapped to RGB values.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;⚗️ Distilled Models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Encoder&lt;br /&gt;(Distilled from G)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ADE20k &lt;br /&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;Segmentation&lt;/a&gt;&lt;br /&gt;Linear Probe mIoU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DAVIS&lt;br /&gt; &lt;a href=&#34;https://github.com/facebookresearch/dino/raw/main/eval_video_segmentation.py&#34;&gt;Tracking&lt;/a&gt; &lt;br /&gt;Zero-Shot J&amp;amp;F&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🆕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;T/16&lt;/strong&gt; 512px&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/PE-Spatial-T16-512&#34;&gt;PE-Spatial-T16-512&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🆕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;S/16&lt;/strong&gt; 512px&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/PE-Spatial-S16-512&#34;&gt;PE-Spatial-S16-512&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🆕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;B/16&lt;/strong&gt; 512px&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/PE-Spatial-B16-512&#34;&gt;PE-Spatial-B16-512&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🆕&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;L/14&lt;/strong&gt; 448px&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/PE-Spatial-L14-448&#34;&gt;PE-Spatial-L14-448&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;See paper for comparison to other models.&lt;/p&gt; &#xA;&lt;h3&gt;Getting Started with PE&lt;/h3&gt; &#xA;&lt;p&gt;You can get started with the following example for image and text feature extraction or use our &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/perception_models/blob/main/apps/pe/docs/pe_demo.ipynb&#34;&gt;Colab Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from PIL import Image&#xA;import core.vision_encoder.pe as pe&#xA;import core.vision_encoder.transforms as transforms&#xA;&#xA;print(&#34;CLIP configs:&#34;, pe.CLIP.available_configs())&#xA;# CLIP configs: [&#39;PE-Core-G14-448&#39;, &#39;PE-Core-L14-336&#39;, &#39;PE-Core-B16-224&#39;, &#39;PE-Core-S16-384&#39;, &#39;PE-Core-T16-384&#39;]&#xA;&#xA;model = pe.CLIP.from_config(&#34;PE-Core-L14-336&#34;, pretrained=True)  # Downloads from HF&#xA;model = model.cuda()&#xA;&#xA;preprocess = transforms.get_image_transform(model.image_size)&#xA;tokenizer = transforms.get_text_tokenizer(model.context_length)&#xA;&#xA;image = preprocess(Image.open(&#34;docs/assets/cat.png&#34;)).unsqueeze(0).cuda()&#xA;text = tokenizer([&#34;a diagram&#34;, &#34;a dog&#34;, &#34;a cat&#34;]).cuda()&#xA;&#xA;with torch.no_grad(), torch.autocast(&#34;cuda&#34;):&#xA;    image_features, text_features, logit_scale = model(image, text)&#xA;    text_probs = (logit_scale * image_features @ text_features.T).softmax(dim=-1)&#xA;&#xA;print(&#34;Label probs:&#34;, text_probs)  # prints: [[0.0, 0.0, 1.0]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/apps/pe/README.md&#34;&gt;&lt;code&gt;apps/pe/README.md&lt;/code&gt;&lt;/a&gt; for details and how to get started!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Perception Language Model (PLM)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/collections/facebook/perception-lm-67f9783f171948c383ee7498&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Collection-blue&#34; alt=&#34;Hugging Face Collection&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://ai.meta.com/research/publications/perceptionlm-open-access-data-and-models-for-detailed-visual-understanding&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Technical%20Report-PerceptionLM-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2504.13180&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2504.13180-brightgreen.svg?style=flat-square&#34; alt=&#34;Paper&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/apps/plm/notebook_demos&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Google%20Colab-Tutorials-red&#34; alt=&#34;Colab&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/LICENSE.PLM&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model_License-FAIR_Research_License-lightgrey&#34; alt=&#34;ModelLicense&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PerceptionLM (PLM) is a family of open and fully reproducible models to facilitate research in vision-language modeling (VLM). In conjunction with PE, it is powerful enough to compete with the latest state-of-the-art VLMs such as InternVL3 and QwenVL2.5, while using &lt;em&gt;fully open data&lt;/em&gt;. We also release the largest spatiotemporally annotated video dense captioning and fine-grained human activity recognition datasets to ever exist.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/apps/plm/docs/plm_main_fig.png&#34; alt=&#34;Description of the image&#34; /&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;PLM releases models in three different sizes (1B, 3B and 8B).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/Perception-LM-1B&#34;&gt;Perception-LM-1B&lt;/a&gt;: A PLM model trained using Llama-3.2-1B-Instruct base LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/Perception-LM-3B&#34;&gt;Perception-LM-3B&lt;/a&gt;: A PLM model trained using Llama-3.2-3B-Instruct base LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/Perception-LM-8B&#34;&gt;Perception-LM-8B&lt;/a&gt;: A PLM model trained using Llama-3.1-8B-Instruct base LLM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;PLM Image Benchmark Results&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DocVQA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ChartQA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TextVQA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;InfoQA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AI2D&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;OCRBench&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;COCO&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Nocap&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Flickr&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MMMU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;VQAv2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;OKVQA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;VizWiz&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MME&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;SEED&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BLINK&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CVBench&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;RealWorldQA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;VSR&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;POPE&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PLM1B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;807&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;138.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;124.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;100.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1603&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PLM3B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;93.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;830&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;144.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;126.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;98.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1879&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PLM8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;94.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;870&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;146.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;129.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;105.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1989&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;PLM Video Benchmark Results&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;VATEX&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DREAM&amp;nbsp;1K&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;How2QA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MVBench&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;NExTQA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PerceptionTest&amp;nbsp;(test)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;STAR&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TVQA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;VideoMME&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TVBench&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ActivityNetQA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;EgoSchema&amp;nbsp;(test)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TemporalBench&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TOMATO&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MotionBench&amp;nbsp;(dev)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TempCompass&amp;nbsp;(MCQ)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CGBench&amp;nbsp;(clue)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Charades&amp;nbsp;STA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;VideoHallucer&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Halluc.&amp;nbsp;EventHallusion&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PLM1B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PLM3B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PLM8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;99.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;PLM Resources&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Resource&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Documentation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Evaluation of PLM using lmms-eval&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/apps/plm/docs/evaluation.md&#34;&gt;&lt;code&gt;docs/evaluation.md&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Training / Finetuning&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Training and finetuning instructions for PLM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/apps/plm/docs/training.md&#34;&gt;&lt;code&gt;docs/training.md&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;PLM-VideoBench&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Evaluation on PLM-VideoBench using lmms-eval&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/apps/plm/docs/plm_videobench.md&#34;&gt;&lt;code&gt;docs/plm_videobench.md&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;End-to-End Finetuning Example&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;End-to-end finetuning example on radiology images&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/apps/plm/docs/finetune_example.md&#34;&gt;&lt;code&gt;docs/finetune_example.md&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Generating Response&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Generate responses using a trained model with &lt;code&gt;generate.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/apps/plm/generate.py&#34;&gt;&lt;code&gt;generate.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/perception_models/main/apps/plm/README.md&#34;&gt;&lt;code&gt;apps/plm/README.md&lt;/code&gt;&lt;/a&gt; for details and how to get started!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Installation &lt;span&gt;🔧&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/facebookresearch/perception_models.git&#xA;cd perception_models&#xA;&#xA;conda create --name perception_models python=3.12&#xA;conda activate perception_models&#xA;&#xA;# Install PyTorch&#xA;pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 xformers --index-url https://download.pytorch.org/whl/cu124&#xA;&#xA;# We use torchcodec for decoding videos into PyTorch tensors&#xA;conda install ffmpeg -c conda-forge&#xA;pip install torchcodec==0.1 --index-url=https://download.pytorch.org/whl/cu124&#xA;&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will install an editable version of repo, allowing you to make changes to the code without needing to reinstall the package every time.&lt;/p&gt; &#xA;&lt;h2&gt;🙏 Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We are thankful to &lt;a href=&#34;https://github.com/facebookresearch/lingua&#34;&gt;Meta Lingua&lt;/a&gt; for releasing their code as open-source contributions. The code structure and code implementation of the LLM is directly forked from &lt;a href=&#34;https://github.com/facebookresearch/lingua&#34;&gt;Meta Lingua&lt;/a&gt;. We are also thankful to &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;Open_CLIP&lt;/a&gt; for open-source contributions in CLIP training, and &lt;a href=&#34;https://github.com/LAION-AI/CLIP_benchmark&#34;&gt;CLIP_benchmark&lt;/a&gt; for CLIP model evaluation.&lt;/p&gt; &#xA;&lt;h2&gt;📜 Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@article{bolya2025PerceptionEncoder,&#xA;  title={Perception Encoder: The best visual embeddings are not at the output of the network},&#xA;  author={Daniel Bolya and Po-Yao Huang and Peize Sun and Jang Hyun Cho and Andrea Madotto and Chen Wei and Tengyu Ma and Jiale Zhi and Jathushan Rajasegaran and Hanoona Rasheed and Junke Wang and Marco Monteiro and Hu Xu and Shiyu Dong and Nikhila Ravi and Daniel Li and Piotr Doll{\&#39;a}r and Christoph Feichtenhofer},&#xA;  journal={arXiv:2504.13181},&#xA;  year={2025}&#xA;}&#xA;&#xA;@article{cho2025PerceptionLM,&#xA;  title={PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding},&#xA;  author={Jang Hyun Cho and Andrea Madotto and Effrosyni Mavroudi and Triantafyllos Afouras and Tushar Nagarajan and Muhammad Maaz and Yale Song and Tengyu Ma and Shuming Hu and Hanoona Rasheed and Peize Sun and Po-Yao Huang and Daniel Bolya and Suyog Jain and Miguel Martin and Huiyu Wang and Nikhila Ravi and Shashank Jain and Temmy Stark and Shane Moon and Babak Damavandi and Vivian Lee and Andrew Westbury and Salman Khan and Philipp Kr\&#34;{a}henb\&#34;{u}hl and Piotr Doll{\&#39;a}r and Lorenzo Torresani and Kristen Grauman and Christoph Feichtenhofer},&#xA;  journal={arXiv:2504.13180},&#xA;  year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>