<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-29T01:33:24Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>saleha-muzammil/Academic-Time-Machine</title>
    <updated>2023-08-29T01:33:24Z</updated>
    <id>tag:github.com,2023-08-29:/saleha-muzammil/Academic-Time-Machine</id>
    <link href="https://github.com/saleha-muzammil/Academic-Time-Machine" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;Repository: Academic Time Machine&lt;/strong&gt; üìö&lt;/p&gt; &#xA;&lt;p&gt;Welcome to the repository containing past papers for computer science (CS), software engineering (SE), and data science (DS) courses. üéì This repository aims to provide a comprehensive collection of past papers to help students prepare for their exams effectively. Whether you&#39;re a current student or an alumnus looking to brush up on your knowledge, you&#39;ll find this repository useful. üìñüöÄ&lt;/p&gt; &#xA;&lt;p&gt;To access and contribute to this repository, please follow the instructions below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Create a pull request ü§ù&lt;/strong&gt; Fork the repository, add materials and create a pull request to merge your fork.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Contact Collaborators üìû&lt;/strong&gt; Alternatively, you can reach out to any of the current collaborators listed in the repository to request updates or share new past papers. They will be happy to assist you in keeping the repository up-to-date. üòÑ&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Fork the Repository üç¥&lt;/strong&gt; If you wish to make your own changes or updates to the repository without becoming a collaborator, you can fork the repository to create your copy. Feel free to contribute by adding new past papers, organizing the existing collection, or suggesting improvements. üí™üåü&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;4. Download Specific Folders üì•&lt;/strong&gt; If you want to download only specific course folders from a repository without downloading the whole repository, you can use the following steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go to the link of the repository (&lt;a href=&#34;https://github.com/saleha-muzammil/Academic-Time-Machine/&#34;&gt;https://github.com/saleha-muzammil/Academic-Time-Machine/&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Press &lt;code&gt;.&lt;/code&gt; or replace &lt;code&gt;.com&lt;/code&gt; with &lt;code&gt;.dev&lt;/code&gt; in URL to open the repository in GitHub&#39;s internal editor&lt;/li&gt; &#xA; &lt;li&gt;In Explorer pane (left side or press Ctrl+Shift+E), Right click on the required file/folder and select download.&lt;/li&gt; &#xA; &lt;li&gt;In the Select Folder dialog box, choose the directory on your disk under which you want the selected file/folder to download.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Happy learning and best of luck with your exams! üìö‚ú®&lt;/p&gt; &#xA;&lt;h3&gt;Credits üôå&lt;/h3&gt; &#xA;&lt;p&gt;We would like to extend our sincere gratitude to &lt;a href=&#34;https://github.com/syedmalimustafa&#34;&gt;@syedmalimustafa&lt;/a&gt; (who first compiled this repository) , &lt;a href=&#34;https://github.com/raza-h&#34;&gt;@raza-h&lt;/a&gt; , &lt;a href=&#34;https://github.com/Hassaan-T075&#34;&gt;@Hassaan-T075&lt;/a&gt; , &lt;a href=&#34;https://github.com/saimali1124&#34;&gt;@saimali1124&lt;/a&gt; ,&lt;a href=&#34;https://github.com/Mahd67&#34;&gt;@Mahd67&lt;/a&gt; , &lt;a href=&#34;https://github.com/Huzaifa-fh&#34;&gt;@Huzaifa-fh&lt;/a&gt; for their valuable contributions to the Academic Time Machine repository. Their involvement and efforts have greatly enhanced the collection of past papers and improved the overall quality of the repository. üëèüëè&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>naver-ai/DenseDiffusion</title>
    <updated>2023-08-29T01:33:24Z</updated>
    <id>tag:github.com,2023-08-29:/naver-ai/DenseDiffusion</id>
    <link href="https://github.com/naver-ai/DenseDiffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Dense Text-to-Image Generation with Attention Modulation [&lt;a href=&#34;https://arxiv.org/abs/2308.12964&#34;&gt;Paper&lt;/a&gt;]&lt;br&gt;&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Authors ‚ÄÉ‚ÄÉ &lt;a href=&#34;https://github.com/YunjiKim&#34;&gt;Yunji Kim&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://lee-jiyoung.github.io&#34;&gt;Jiyoung Lee&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;http://wityworks.com/&#34;&gt;Jin-Hwa Kim&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://github.com/jungwoo-ha&#34;&gt;Jung-Woo Ha&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;br&gt; &lt;sub&gt; ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ &lt;sup&gt;1&lt;/sup&gt;NAVER AI Lab, &lt;sup&gt;2&lt;/sup&gt;Carnegie Mellon University &lt;/sub&gt;&lt;/h4&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Abstract&lt;/h4&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Existing text-to-image diffusion models struggle to synthesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout. We first analyze the relationship between generated images&#39; layouts and the pre-trained model&#39;s intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regarding both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Method&lt;/h4&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/naver-ai/DenseDiffusion/main/figures/sample.png&#34; width=&#34;90%&#34; title=&#34;results&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/naver-ai/DenseDiffusion/main/figures/method.png&#34; width=&#34;92%&#34; title=&#34;method&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Our goal is to improve the text-to-image model&#39;s ability to reflect textual and spatial conditions without fine-tuning. We formally define our condition as a set of $N$ segments ${\lbrace(c_{n},m_{n})\rbrace}^{N}_{n=1}$, where each segment $(c_n,m_n)$ describes a single region. Here $c_n$ is a non-overlapping part of the full-text caption $c$, and $m_n$ denotes a binary map representing each region. Given the input conditions, we modulate attention maps of all attention layers on the fly so that the object described by $c_n$ can be generated in the corresponding region $m_n$. To maintain the pre-trained model&#39;s generation capacity, we design the modulation to consider original value range and each segment&#39;s area.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Examples&lt;/h4&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/naver-ai/DenseDiffusion/main/figures/example_0.png&#34; width=&#34;90%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/naver-ai/DenseDiffusion/main/figures/example_1.png&#34; width=&#34;90%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/naver-ai/DenseDiffusion/main/figures/example_2.png&#34; width=&#34;90%&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;How to launch a web interface&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Put your access token to Hugging Face Hub &lt;a href=&#34;https://raw.githubusercontent.com/naver-ai/DenseDiffusion/main/gradio_app.py#L71&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the Gradio app.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python gradio_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create the image layout.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/naver-ai/DenseDiffusion/main/figures/step1.png&#34; width=&#34;48%&#34; title=&#34;step1&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Label each segment with a text prompt.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/naver-ai/DenseDiffusion/main/figures/step2.png&#34; width=&#34;50%&#34; title=&#34;step2&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adjust the full text. The default full text is automatically concatenated from each segment&#39;s text. The default one works well, but refineing the full text will further improve the result.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/naver-ai/DenseDiffusion/main/figures/step3.png&#34; width=&#34;48%&#34; title=&#34;step3&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/naver-ai/DenseDiffusion/main/figures/step3_.png&#34; width=&#34;48%&#34; title=&#34;step3_&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check the generated images, and tune the hyperparameters if needed.&lt;br&gt; w&lt;sup&gt;c&lt;/sup&gt; : The degree of attention modulation at cross-attention layers. &lt;br&gt; w&lt;sup&gt;s&lt;/sup&gt; : The degree of attention modulation at self-attention layers. &lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/naver-ai/DenseDiffusion/main/figures/step4.png&#34; width=&#34;45%&#34; title=&#34;step4&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/naver-ai/DenseDiffusion/main/figures/step4_.png&#34; width=&#34;48%&#34; title=&#34;step4_&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Benchmark&lt;/h3&gt; &#xA;&lt;p&gt;We share the benchmark used in our model development and evaluation &lt;a href=&#34;https://raw.githubusercontent.com/naver-ai/DenseDiffusion/main/dataset&#34;&gt;here&lt;/a&gt;. The code for preprocessing segment conditions is in &lt;a href=&#34;https://raw.githubusercontent.com/naver-ai/DenseDiffusion/main/inference.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;BibTeX&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{densediffusion,&#xA;  title={Dense Text-to-Image Generation with Attention Modulation},&#xA;  author={Kim, Yunji and Lee, Jiyoung and Kim, Jin-Hwa and Ha, Jung-Woo and Zhu, Jun-Yan},&#xA;  year={2023},&#xA;  booktitle = {ICCV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Acknowledgment&lt;/h4&gt; &#xA;&lt;p&gt;The demo was developed referencing this &lt;a href=&#34;https://huggingface.co/spaces/weizmannscience/multidiffusion-region-based&#34;&gt;source code&lt;/a&gt;. Thanks for the inspiring work! üôè&lt;/p&gt;</summary>
  </entry>
</feed>