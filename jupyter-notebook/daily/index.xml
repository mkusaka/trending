<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-24T01:32:01Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lyogavin/airllm</title>
    <updated>2024-09-24T01:32:01Z</updated>
    <id>tag:github.com,2024-09-24:/lyogavin/airllm</id>
    <link href="https://github.com/lyogavin/airllm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AirLLM 70B inference with single 4GB GPU&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/lyogavin/airllm/raw/main/assets/airllm_logo_sm.png?v=3&amp;amp;raw=true&#34; alt=&#34;airllm_logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#quickstart&#34;&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#configurations&#34;&gt;&lt;strong&gt;Configurations&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#macos&#34;&gt;&lt;strong&gt;MacOS&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#example-python-notebook&#34;&gt;&lt;strong&gt;Example notebooks&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#faq&#34;&gt;&lt;strong&gt;FAQ&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AirLLM&lt;/strong&gt; optimizes inference memory usage, allowing 70B large language models to run inference on a single 4GB GPU card without quantization, distillation and pruning. And you can run &lt;strong&gt;405B Llama3.1&lt;/strong&gt; on &lt;strong&gt;8GB vram&lt;/strong&gt; now.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lyogavin/airllm/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/lyogavin/airllm?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/airllm&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/airllm?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=blue&amp;amp;left_text=downloads&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/LianjiaTech/BELLE/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://static.aicompose.cn/static/wecom_barcode.png?t=1671918938&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/wechat-Anima-brightgreen?logo=wechat&#34; alt=&#34;Generic badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/2xffU5sn&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1175437549783760896?logo=discord&amp;amp;color=7289da&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/airllm/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/format/airllm?logo=pypi&amp;amp;color=3571a3&#34; alt=&#34;PyPI - AirLLM&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://medium.com/@lyo.gavin&#34;&gt;&lt;img src=&#34;https://img.shields.io/website?up_message=blog&amp;amp;url=https%3A%2F%2Fmedium.com%2F%40lyo.gavin&amp;amp;logo=medium&amp;amp;color=black&#34; alt=&#34;Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gavinliblog.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gavin_Li-Blog-blue&#34; alt=&#34;Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://patreon.com/gavinli&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https%3A%2F%2Fshieldsio-patreon.vercel.app%2Fapi%3Fusername%3Dgavinli%26type%3Dpatrons&amp;amp;style=flat&#34; alt=&#34;Support me on Patreon&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/lyogavin&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/sponsors/lyogavin?logo=GitHub&amp;amp;color=lightgray&#34; alt=&#34;GitHub Sponsors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;[2024/08/20] v2.11.0: Support Qwen2.5&lt;/p&gt; &#xA;&lt;p&gt;[2024/08/18] v2.10.1 Support CPU inference. Support non sharded models. Thanks @NavodPeiris for the great work!&lt;/p&gt; &#xA;&lt;p&gt;[2024/07/30] Support Llama3.1 &lt;strong&gt;405B&lt;/strong&gt; (&lt;a href=&#34;https://colab.research.google.com/github/lyogavin/airllm/blob/main/air_llm/examples/run_llama3.1_405B.ipynb&#34;&gt;example notebook&lt;/a&gt;). Support &lt;strong&gt;8bit/4bit quantization&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[2024/04/20] AirLLM supports Llama3 natively already. Run Llama3 70B on 4GB single GPU.&lt;/p&gt; &#xA;&lt;p&gt;[2023/12/25] v2.8.2: Support MacOS running 70B large language models.&lt;/p&gt; &#xA;&lt;p&gt;[2023/12/20] v2.7: Support AirLLMMixtral.&lt;/p&gt; &#xA;&lt;p&gt;[2023/12/20] v2.6: Added AutoModel, automatically detect model type, no need to provide model class to initialize model.&lt;/p&gt; &#xA;&lt;p&gt;[2023/12/18] v2.5: added prefetching to overlap the model loading and compute. 10% speed improvement.&lt;/p&gt; &#xA;&lt;p&gt;[2023/12/03] added support of &lt;strong&gt;ChatGLM&lt;/strong&gt;, &lt;strong&gt;QWen&lt;/strong&gt;, &lt;strong&gt;Baichuan&lt;/strong&gt;, &lt;strong&gt;Mistral&lt;/strong&gt;, &lt;strong&gt;InternLM&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;p&gt;[2023/12/02] added support for safetensors. Now support all top 10 models in open llm leaderboard.&lt;/p&gt; &#xA;&lt;p&gt;[2023/12/01] airllm 2.0. Support compressions: &lt;strong&gt;3x run time speed up!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2023/11/20] airllm Initial verion!&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#lyogavin/airllm&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=lyogavin/airllm&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#quickstart&#34;&gt;Quick start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#model-compression---3x-inference-speed-up&#34;&gt;Model Compression&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#configurations&#34;&gt;Configurations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#macos&#34;&gt;Run on MacOS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#example-python-notebook&#34;&gt;Example notebooks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#supported-models&#34;&gt;Supported Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;1. Install package&lt;/h3&gt; &#xA;&lt;p&gt;First, install the airllm pip package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install airllm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Inference&lt;/h3&gt; &#xA;&lt;p&gt;Then, initialize AirLLMLlama2, pass in the huggingface repo ID of the model being used, or the local path, and inference can be performed similar to a regular transformer model.&lt;/p&gt; &#xA;&lt;p&gt;(&lt;em&gt;You can also specify the path to save the splitted layered model through &lt;strong&gt;layer_shards_saving_path&lt;/strong&gt; when init AirLLMLlama2.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from airllm import AutoModel&#xA;&#xA;MAX_LENGTH = 128&#xA;# could use hugging face model repo id:&#xA;model = AutoModel.from_pretrained(&#34;garage-bAInd/Platypus2-70B-instruct&#34;)&#xA;&#xA;# or use model&#39;s local path...&#xA;#model = AutoModel.from_pretrained(&#34;/home/ubuntu/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/b585e74bcaae02e52665d9ac6d23f4d0dbc81a0f&#34;)&#xA;&#xA;input_text = [&#xA;        &#39;What is the capital of United States?&#39;,&#xA;        #&#39;I like&#39;,&#xA;    ]&#xA;&#xA;input_tokens = model.tokenizer(input_text,&#xA;    return_tensors=&#34;pt&#34;, &#xA;    return_attention_mask=False, &#xA;    truncation=True, &#xA;    max_length=MAX_LENGTH, &#xA;    padding=False)&#xA;           &#xA;generation_output = model.generate(&#xA;    input_tokens[&#39;input_ids&#39;].cuda(), &#xA;    max_new_tokens=20,&#xA;    use_cache=True,&#xA;    return_dict_in_generate=True)&#xA;&#xA;output = model.tokenizer.decode(generation_output.sequences[0])&#xA;&#xA;print(output)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: During inference, the original model will first be decomposed and saved layer-wise. Please ensure there is sufficient disk space in the huggingface cache directory.&lt;/p&gt; &#xA;&lt;h2&gt;Model Compression - 3x Inference Speed Up!&lt;/h2&gt; &#xA;&lt;p&gt;We just added model compression based on block-wise quantization-based model compression. Which can further &lt;strong&gt;speed up the inference speed&lt;/strong&gt; for up to &lt;strong&gt;3x&lt;/strong&gt; , with &lt;strong&gt;almost ignorable accuracy loss!&lt;/strong&gt; (see more performance evaluation and why we use block-wise quantization in &lt;a href=&#34;https://arxiv.org/abs/2212.09720&#34;&gt;this paper&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lyogavin/airllm/raw/main/assets/airllm2_time_improvement.png?v=2&amp;amp;raw=true&#34; alt=&#34;speed_improvement&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;How to enable model compression speed up:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Step 1. make sure you have &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt; installed by &lt;code&gt;pip install -U bitsandbytes &lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Step 2. make sure airllm verion later than 2.0.0: &lt;code&gt;pip install -U airllm&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Step 3. when initialize the model, passing the argument compression (&#39;4bit&#39; or &#39;8bit&#39;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModel.from_pretrained(&#34;garage-bAInd/Platypus2-70B-instruct&#34;,&#xA;                     compression=&#39;4bit&#39; # specify &#39;8bit&#39; for 8-bit block-wise quantization &#xA;                    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;What are the differences between model compression and quantization?&lt;/h4&gt; &#xA;&lt;p&gt;Quantization normally needs to quantize both weights and activations to really speed things up. Which makes it harder to maintain accuracy and avoid the impact of outliers in all kinds of inputs.&lt;/p&gt; &#xA;&lt;p&gt;While in our case the bottleneck is mainly at the disk loading, we only need to make the model loading size smaller. So, we get to only quantize the weights&#39; part, which is easier to ensure the accuracy.&lt;/p&gt; &#xA;&lt;h2&gt;Configurations&lt;/h2&gt; &#xA;&lt;p&gt;When initialize the model, we support the following configurations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;compression&lt;/strong&gt;: supported options: 4bit, 8bit for 4-bit or 8-bit block-wise quantization, or by default None for no compression&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;profiling_mode&lt;/strong&gt;: supported options: True to output time consumptions or by default False&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;layer_shards_saving_path&lt;/strong&gt;: optionally another path to save the splitted model&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;hf_token&lt;/strong&gt;: huggingface token can be provided here if downloading gated models like: &lt;em&gt;meta-llama/Llama-2-7b-hf&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;prefetching&lt;/strong&gt;: prefetching to overlap the model loading and compute. By default, turned on. For now, only AirLLMLlama2 supports this.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;delete_original&lt;/strong&gt;: if you don&#39;t have too much disk space, you can set delete_original to true to delete the original downloaded hugging face model, only keep the transformed one to save half of the disk space.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;MacOS&lt;/h2&gt; &#xA;&lt;p&gt;Just install airllm and run the code the same as on linux. See more in &lt;a href=&#34;https://raw.githubusercontent.com/lyogavin/airllm/main/#quickstart&#34;&gt;Quick Start&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;make sure you installed &lt;a href=&#34;https://github.com/ml-explore/mlx?tab=readme-ov-file#installation&#34;&gt;mlx&lt;/a&gt; and torch&lt;/li&gt; &#xA; &lt;li&gt;you probabaly need to install python native see more &lt;a href=&#34;https://stackoverflow.com/a/65432861/21230266&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;only &lt;a href=&#34;https://support.apple.com/en-us/HT211814&#34;&gt;Apple silicon&lt;/a&gt; is supported&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example [python notebook] (&lt;a href=&#34;https://github.com/lyogavin/airllm/raw/main/air_llm/examples/run_on_macos.ipynb&#34;&gt;https://github.com/lyogavin/airllm/blob/main/air_llm/examples/run_on_macos.ipynb&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Example Python Notebook&lt;/h2&gt; &#xA;&lt;p&gt;Example colabs here:&lt;/p&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/lyogavin/airllm/blob/main/air_llm/examples/run_all_types_of_models.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;h4&gt;example of other models (ChatGLM, QWen, Baichuan, Mistral, etc):&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;ChatGLM:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from airllm import AutoModel&#xA;MAX_LENGTH = 128&#xA;model = AutoModel.from_pretrained(&#34;THUDM/chatglm3-6b-base&#34;)&#xA;input_text = [&#39;What is the capital of China?&#39;,]&#xA;input_tokens = model.tokenizer(input_text,&#xA;    return_tensors=&#34;pt&#34;, &#xA;    return_attention_mask=False, &#xA;    truncation=True, &#xA;    max_length=MAX_LENGTH, &#xA;    padding=True)&#xA;generation_output = model.generate(&#xA;    input_tokens[&#39;input_ids&#39;].cuda(), &#xA;    max_new_tokens=5,&#xA;    use_cache= True,&#xA;    return_dict_in_generate=True)&#xA;model.tokenizer.decode(generation_output.sequences[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;QWen:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from airllm import AutoModel&#xA;MAX_LENGTH = 128&#xA;model = AutoModel.from_pretrained(&#34;Qwen/Qwen-7B&#34;)&#xA;input_text = [&#39;What is the capital of China?&#39;,]&#xA;input_tokens = model.tokenizer(input_text,&#xA;    return_tensors=&#34;pt&#34;, &#xA;    return_attention_mask=False, &#xA;    truncation=True, &#xA;    max_length=MAX_LENGTH)&#xA;generation_output = model.generate(&#xA;    input_tokens[&#39;input_ids&#39;].cuda(), &#xA;    max_new_tokens=5,&#xA;    use_cache=True,&#xA;    return_dict_in_generate=True)&#xA;model.tokenizer.decode(generation_output.sequences[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Baichuan, InternLM, Mistral, etc:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from airllm import AutoModel&#xA;MAX_LENGTH = 128&#xA;model = AutoModel.from_pretrained(&#34;baichuan-inc/Baichuan2-7B-Base&#34;)&#xA;#model = AutoModel.from_pretrained(&#34;internlm/internlm-20b&#34;)&#xA;#model = AutoModel.from_pretrained(&#34;mistralai/Mistral-7B-Instruct-v0.1&#34;)&#xA;input_text = [&#39;What is the capital of China?&#39;,]&#xA;input_tokens = model.tokenizer(input_text,&#xA;    return_tensors=&#34;pt&#34;, &#xA;    return_attention_mask=False, &#xA;    truncation=True, &#xA;    max_length=MAX_LENGTH)&#xA;generation_output = model.generate(&#xA;    input_tokens[&#39;input_ids&#39;].cuda(), &#xA;    max_new_tokens=5,&#xA;    use_cache=True,&#xA;    return_dict_in_generate=True)&#xA;model.tokenizer.decode(generation_output.sequences[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;To request other model support: &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSe0Io9ANMT964Zi-OQOq1TJmnvP-G3_ZgQDhP7SatN0IEdbOg/viewform?usp=sf_link&#34;&gt;here&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;A lot of the code are based on SimJeg&#39;s great work in the Kaggle exam competition. Big shoutout to SimJeg:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SimJeg&#34;&gt;GitHub account @SimJeg&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/code/simjeg/platypus2-70b-with-wikipedia-rag&#34;&gt;the code on Kaggle&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446414&#34;&gt;the associated discussion&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;1. MetadataIncompleteBuffer&lt;/h3&gt; &#xA;&lt;p&gt;safetensors_rust.SafetensorError: Error while deserializing header: MetadataIncompleteBuffer&lt;/p&gt; &#xA;&lt;p&gt;If you run into this error, most possible cause is you run out of disk space. The process of splitting model is very disk-consuming. See &lt;a href=&#34;https://huggingface.co/TheBloke/guanaco-65B-GPTQ/discussions/12&#34;&gt;this&lt;/a&gt;. You may need to extend your disk space, clear huggingface &lt;a href=&#34;https://huggingface.co/docs/datasets/cache&#34;&gt;.cache&lt;/a&gt; and rerun.&lt;/p&gt; &#xA;&lt;h3&gt;2. ValueError: max() arg is an empty sequence&lt;/h3&gt; &#xA;&lt;p&gt;Most likely you are loading QWen or ChatGLM model with Llama2 class. Try the following:&lt;/p&gt; &#xA;&lt;p&gt;For QWen model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from airllm import AutoModel #&amp;lt;----- instead of AirLLMLlama2&#xA;AutoModel.from_pretrained(...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For ChatGLM model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from airllm import AutoModel #&amp;lt;----- instead of AirLLMLlama2&#xA;AutoModel.from_pretrained(...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. 401 Client Error....Repo model ... is gated.&lt;/h3&gt; &#xA;&lt;p&gt;Some models are gated models, needs huggingface api token. You can provide hf_token:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModel.from_pretrained(&#34;meta-llama/Llama-2-7b-hf&#34;, #hf_token=&#39;HF_API_TOKEN&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. ValueError: Asking to pad but the tokenizer does not have a padding token.&lt;/h3&gt; &#xA;&lt;p&gt;Some model&#39;s tokenizer doesn&#39;t have padding token, so you can set a padding token or simply turn the padding config off:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_tokens = model.tokenizer(input_text,&#xA;   return_tensors=&#34;pt&#34;, &#xA;   return_attention_mask=False, &#xA;   truncation=True, &#xA;   max_length=MAX_LENGTH, &#xA;   padding=False  #&amp;lt;-----------   turn off padding &#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citing AirLLM&lt;/h2&gt; &#xA;&lt;p&gt;If you find AirLLM useful in your research and wish to cite it, please use the following BibTex entry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{airllm2023,&#xA;  author = {Gavin Li},&#xA;  title = {AirLLM: scaling large language models on low-end commodity computers},&#xA;  url = {https://github.com/lyogavin/airllm/},&#xA;  version = {0.0},&#xA;  year = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Welcomed contributions, ideas and discussions!&lt;/p&gt; &#xA;&lt;p&gt;If you find it useful, please ⭐ or buy me a coffee! 🙏&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://bmc.link/lyogavinQ&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png&#34; alt=&#34;&amp;quot;Buy Me A Coffee&amp;quot;&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>