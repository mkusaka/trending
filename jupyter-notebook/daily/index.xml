<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-27T01:31:35Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mistralai/mistral-inference</title>
    <updated>2024-05-27T01:31:35Z</updated>
    <id>tag:github.com,2024-05-27:/mistralai/mistral-inference</id>
    <link href="https://github.com/mistralai/mistral-inference" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official inference library for Mistral models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mistral Inference&lt;/h1&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/mistralai/mistral-inference/blob/main/tutorials/getting_started.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;This repository contains minimal code to run our 7B, 8x7B and 8x22B models.&lt;/p&gt; &#xA;&lt;p&gt;Blog 7B: &lt;a href=&#34;https://mistral.ai/news/announcing-mistral-7b/&#34;&gt;https://mistral.ai/news/announcing-mistral-7b/&lt;/a&gt;&lt;br&gt; Blog 8x7B: &lt;a href=&#34;https://mistral.ai/news/mixtral-of-experts/&#34;&gt;https://mistral.ai/news/mixtral-of-experts/&lt;/a&gt;&lt;br&gt; Blog 8x22B: &lt;a href=&#34;https://mistral.ai/news/mixtral-8x22b/&#34;&gt;https://mistral.ai/news/mixtral-8x22b/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Discord: &lt;a href=&#34;https://discord.com/invite/mistralai&#34;&gt;https://discord.com/invite/mistralai&lt;/a&gt;&lt;br&gt; Documentation: &lt;a href=&#34;https://docs.mistral.ai/&#34;&gt;https://docs.mistral.ai/&lt;/a&gt;&lt;br&gt; Guardrailing: &lt;a href=&#34;https://docs.mistral.ai/usage/guardrailing&#34;&gt;https://docs.mistral.ai/usage/guardrailing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;PyPI&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install mistral-inference&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Local&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd $HOME &amp;amp;&amp;amp; git clone https://github.com/mistralai/mistral-inference&#xA;cd $HOME/mistral-inference &amp;amp;&amp;amp; poetry install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model download&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;   &lt;th&gt;md5sum&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B Instruct v3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar&#34;&gt;https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;80b71fcb6416085bcb4efad86dfb4d52&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8x7B Instruct&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://models.mistralcdn.com/mixtral-8x7b-v0-1/Mixtral-8x7B-v0.1-Instruct.tar&#34;&gt;https://models.mistralcdn.com/mixtral-8x7b-v0-1/Mixtral-8x7B-v0.1-Instruct.tar&lt;/a&gt; (&lt;strong&gt;Updated model coming soon!&lt;/strong&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;8e2d3930145dc43d3084396f49d38a3f&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8x22 Instruct&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://models.mistralcdn.com/mixtral-8x22b-v0-3/mixtral-8x22B-Instruct-v0.3.tar&#34;&gt;https://models.mistralcdn.com/mixtral-8x22b-v0-3/mixtral-8x22B-Instruct-v0.3.tar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;471a02a6902706a2f1e44a693813855b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B Base&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar&#34;&gt;https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0663b293810d7571dad25dae2f2a5806&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8x7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Updated model coming soon!&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8x22B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://models.mistralcdn.com/mixtral-8x22b-v0-3/mixtral-8x22B-v0.3.tar&#34;&gt;https://models.mistralcdn.com/mixtral-8x22b-v0-3/mixtral-8x22B-v0.3.tar&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;a2fa75117174f87d1197e3a4eb50371a&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Important&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;mixtral-8x22B-Instruct-v0.3.tar&lt;/code&gt; is exactly the same as &lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1&#34;&gt;Mixtral-8x22B-Instruct-v0.1&lt;/a&gt;, only stored in &lt;code&gt;.safetensors&lt;/code&gt; format&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;mixtral-8x22B-v0.3.tar&lt;/code&gt; is the same as &lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x22B-v0.1&#34;&gt;Mixtral-8x22B-v0.1&lt;/a&gt;, but has an extended vocabulary of 32768 tokens.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;All of the listed models above supports function calling. For example, Mistral 7B Base/Instruct v3 is a minor update to Mistral 7B Base/Instruct v2, with the addition of function calling capabilities.&lt;/li&gt; &#xA; &lt;li&gt;The &#34;coming soon&#34; models will include function calling as well.&lt;/li&gt; &#xA; &lt;li&gt;You can download the previous versions of our models from our &lt;a href=&#34;https://docs.mistral.ai/getting-started/open_weight_models/#downloading&#34;&gt;docs&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Create a local folder to store models&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export MISTRAL_MODEL=$HOME/mistral_models&#xA;mkdir -p $MISTRAL_MODEL&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download any of the above links and extract the content, &lt;em&gt;e.g.&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export M7B_DIR=$MISTRAL_MODEL/7B_instruct&#xA;wget https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar&#xA;mkdir -p $M7B_DIR&#xA;tar -xf mistral-7B-Instruct-v0.3.tar -C $M7B_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export M8x7B_DIR=$MISTRAL_MODEL/8x7b_instruct&#xA;wget https://models.mistralcdn.com/mixtral-8x7b-v0-1/Mixtral-8x7B-v0.1-Instruct.tar&#xA;mkdir -p $M8x7B_DIR&#xA;tar -xf Mixtral-8x7B-v0.1-Instruct.tar -C $M8x7B_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The following sections give an overview of how to run the model from the Command-line interface or from Python.&lt;/p&gt; &#xA;&lt;h3&gt;CLI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To test that a model works in your setup, you can run the &lt;code&gt;mistral-demo&lt;/code&gt; command. The 7B models can be tested on a single GPU as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mistral-demo $M7B_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Large models, such &lt;strong&gt;8x7B&lt;/strong&gt; and &lt;strong&gt;8x22B&lt;/strong&gt; have to be run in a multi-GPU setup. For these models, you can use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;torchrun --nproc-per-node 2 --no-python mistral-demo $M8x7B_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: Change &lt;code&gt;--nproc-per-node&lt;/code&gt; to more GPUs if available.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To interactively chat with the models, you can make use of the &lt;code&gt;mistral-chat&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mistral-chat $M7B_DIR --instruct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For large models, you can make use of &lt;code&gt;torchrun&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;torchrun --nproc-per-node 2 --no-python mistral-chat $M8x7B_DIR --instruct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: Change &lt;code&gt;--nproc-per-node&lt;/code&gt; to more GPUs if necessary (&lt;em&gt;e.g.&lt;/em&gt; for 8x22B).&lt;/p&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Instruction Following&lt;/em&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from mistral_inference.model import Transformer&#xA;from mistral_inference.generate import generate&#xA;&#xA;from mistral_common.tokens.tokenizers.mistral import MistralTokenizer&#xA;from mistral_common.protocol.instruct.messages import UserMessage&#xA;from mistral_common.protocol.instruct.request import ChatCompletionRequest&#xA;&#xA;&#xA;tokenizer = MistralTokenizer.from_file(&#34;./mistral_7b_instruct/tokenizer.model.v3&#34;)  # change to extracted tokenizer file&#xA;model = Transformer.from_folder(&#34;./mistral_7b_instruct&#34;)  # change to extracted model dir&#xA;&#xA;completion_request = ChatCompletionRequest(messages=[UserMessage(content=&#34;Explain Machine Learning to me in a nutshell.&#34;)])&#xA;&#xA;tokens = tokenizer.encode_chat_completion(completion_request).tokens&#xA;&#xA;out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)&#xA;result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])&#xA;&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Function Calling&lt;/em&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from mistral_common.protocol.instruct.tool_calls import Function, Tool&#xA;&#xA;completion_request = ChatCompletionRequest(&#xA;    tools=[&#xA;        Tool(&#xA;            function=Function(&#xA;                name=&#34;get_current_weather&#34;,&#xA;                description=&#34;Get the current weather&#34;,&#xA;                parameters={&#xA;                    &#34;type&#34;: &#34;object&#34;,&#xA;                    &#34;properties&#34;: {&#xA;                        &#34;location&#34;: {&#xA;                            &#34;type&#34;: &#34;string&#34;,&#xA;                            &#34;description&#34;: &#34;The city and state, e.g. San Francisco, CA&#34;,&#xA;                        },&#xA;                        &#34;format&#34;: {&#xA;                            &#34;type&#34;: &#34;string&#34;,&#xA;                            &#34;enum&#34;: [&#34;celsius&#34;, &#34;fahrenheit&#34;],&#xA;                            &#34;description&#34;: &#34;The temperature unit to use. Infer this from the users location.&#34;,&#xA;                        },&#xA;                    },&#xA;                    &#34;required&#34;: [&#34;location&#34;, &#34;format&#34;],&#xA;                },&#xA;            )&#xA;        )&#xA;    ],&#xA;    messages=[&#xA;        UserMessage(content=&#34;What&#39;s the weather like today in Paris?&#34;),&#xA;        ],&#xA;)&#xA;&#xA;tokens = tokenizer.encode_chat_completion(completion_request).tokens&#xA;&#xA;out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)&#xA;result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])&#xA;&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;One-file-ref&lt;/h3&gt; &#xA;&lt;p&gt;If you want a self-contained implementation, look at &lt;code&gt;one_file_ref.py&lt;/code&gt;, or run it with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m one_file_ref $M7B_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which should give something along the following lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;This is a test of the emergency broadcast system. This is only a test.&#xA;&#xA;If this were a real emergency, you would be told what to do.&#xA;&#xA;This is a test&#xA;=====================&#xA;This is another test of the new blogging software. I’m not sure if I’m going to keep it or not. I’m not sure if I’m going to keep&#xA;=====================&#xA;This is a third test, mistral AI is very good at testing. 🙂&#xA;&#xA;This is a third test, mistral AI is very good at testing. 🙂&#xA;&#xA;This&#xA;=====================&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: To run self-contained implementations, you need to do a local installation.&lt;/p&gt; &#xA;&lt;h3&gt;Test&lt;/h3&gt; &#xA;&lt;p&gt;To run logits equivalence:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pytest tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;deploy&lt;/code&gt; folder contains code to build a &lt;a href=&#34;https://M7B_DIR.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; image with the required dependencies to serve the Mistral AI model. In the image, the &lt;a href=&#34;https://github.com/huggingface/transformers/&#34;&gt;transformers&lt;/a&gt; library is used instead of the reference implementation. To build it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build deploy --build-arg MAX_JOBS=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instructions to run the image can be found in the &lt;a href=&#34;https://docs.mistral.ai/quickstart&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Model platforms&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use Mistral models on &lt;a href=&#34;https://console.mistral.ai/&#34;&gt;Mistral AI official API&lt;/a&gt; (La Plateforme)&lt;/li&gt; &#xA; &lt;li&gt;Use Mistral models via &lt;a href=&#34;https://docs.mistral.ai/deployment/cloud/overview/&#34;&gt;cloud providers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;[1]: &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt;: Low-Rank Adaptation of Large Language Models, Hu et al. 2021&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>quiccklabs/Labs_solutions</title>
    <updated>2024-05-27T01:31:35Z</updated>
    <id>tag:github.com,2024-05-27:/quiccklabs/Labs_solutions</id>
    <link href="https://github.com/quiccklabs/Labs_solutions" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>