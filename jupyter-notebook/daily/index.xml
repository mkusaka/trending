<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-02T01:31:45Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>GeostatsGuy/DataScienceInteractivePython</title>
    <updated>2024-11-02T01:31:45Z</updated>
    <id>tag:github.com,2024-11-02:/GeostatsGuy/DataScienceInteractivePython</id>
    <link href="https://github.com/GeostatsGuy/DataScienceInteractivePython" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python interactive dashboards for learning data science&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34; &lt;p&gt; &lt;img src=&#34;https://github.com/GeostatsGuy/GeostatsPy/raw/master/TCG_color_logo.png&#34; width=&#34;220&#34; height=&#34;200&#34;&gt; &lt;p&gt;&lt;/p&gt;&lt;/h1&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;DataScienceInteractivePython: Interactive Educational Data Science Python Dashboards Repository (0.0.1)&lt;/h1&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;Interactive dashboards to help you over the intellectual hurdles of data science!&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;To support my students in my &lt;strong&gt;Data Analytics and Geostatistics&lt;/strong&gt;, &lt;strong&gt;Spatial Data Analytics&lt;/strong&gt; and &lt;strong&gt;Machine Learning&lt;/strong&gt; courses and anyone else learning data analytics and machine learning, I have developed a set of Python interactive dashboards. When students struggle with a concept I make a new interactive dashboard so they can learn by playing with the statistics, models or theoretical concepts!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Michael Pyrcz, Professor, The University of Texas at Austin, Data Analytics, Geostatistics and Machine Learning&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://twitter.com/geostatsguy&#34;&gt;Twitter&lt;/a&gt; | &lt;a href=&#34;https://github.com/GeostatsGuy&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;http://michaelpyrcz.com&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;amp;hl=en&amp;amp;oi=ao&#34;&gt;GoogleScholar&lt;/a&gt; | &lt;a href=&#34;https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446&#34;&gt;Book&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig&#34;&gt;YouTube&lt;/a&gt; | &lt;a href=&#34;https://www.linkedin.com/in/michael-pyrcz-61a648a1&#34;&gt;LinkedIn&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Cite As:&lt;/h3&gt; &#xA;&lt;p&gt;Pyrcz, Michael J. (2021). DataScienceInteractivePython: Educational Data Science Interactive Python Dashboards Repository (0.0.1). Zenodo. &lt;a href=&#34;https://doi.org/10.5281/zenodo.5564966&#34;&gt;https://doi.org/10.5281/zenodo.5564966&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zenodo.org/doi/10.5281/zenodo.5564966&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/106843586.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Binder&lt;/h4&gt; &#xA;&lt;p&gt;To further support my students, I&#39;m using &lt;a href=&#34;https://mybinder.readthedocs.io/en/latest/index.html&#34;&gt;Binder&lt;/a&gt; to host some of my &lt;strong&gt;interactive Python spatial data analytics, geostatistics and machine learning demonstration workflows&lt;/strong&gt; online. Some of my students are having issues with setting up their local computing environments and instantiating the interactive workflows.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;I hope this will assist these students and remove barriers for these educational tools to invite a wider audience that may benefit from experiential learning - playing with the systems and machines in real-time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/GeostatsGuy/DataScience_Interactive_Python/HEAD&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Click on the link above to launch binder with container to run the included workflow.&lt;/p&gt; &#xA;&lt;h4&gt;Setup&lt;/h4&gt; &#xA;&lt;p&gt;A minimum environment includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.7.10 - due to the depdendency of GeostatsPy on the Numba package for code acceleration&lt;/li&gt; &#xA; &lt;li&gt;MatPlotLib - plotting&lt;/li&gt; &#xA; &lt;li&gt;NumPy - gridded data and array math&lt;/li&gt; &#xA; &lt;li&gt;Pandas - tabulated data&lt;/li&gt; &#xA; &lt;li&gt;SciPy - statistics module&lt;/li&gt; &#xA; &lt;li&gt;ipywidgets - for plot interactivity&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/geostatspy/&#34;&gt;GeostatsPy&lt;/a&gt; - geostatistical algorithms and functions (Pyrcz et al., 2021)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The required datasets are available in the &lt;a href=&#34;https://github.com/GeostatsGuy/GeoDataSets&#34;&gt;GeoDataSets&lt;/a&gt; repository and linked in the workflows.&lt;/p&gt; &#xA;&lt;h4&gt;Repository Summary&lt;/h4&gt; &#xA;&lt;p&gt;The interative Python examples include a variety of topics like:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bayesian and frequentist statistics&lt;/li&gt; &#xA; &lt;li&gt;univariate and bivariate statistics&lt;/li&gt; &#xA; &lt;li&gt;confidence intervals and hypothesis testing&lt;/li&gt; &#xA; &lt;li&gt;Monte Carlo methods and bootstrap&lt;/li&gt; &#xA; &lt;li&gt;inferential machine learning, principal component and cluster analysis&lt;/li&gt; &#xA; &lt;li&gt;predictive machine learning, norms, model parameter training and hyperparameter tuning, overfit models&lt;/li&gt; &#xA; &lt;li&gt;uncertainty modeling checking&lt;/li&gt; &#xA; &lt;li&gt;spatial data debiasing&lt;/li&gt; &#xA; &lt;li&gt;variogram calculation and modeling&lt;/li&gt; &#xA; &lt;li&gt;spatial estimation, issues and trend modeling&lt;/li&gt; &#xA; &lt;li&gt;spatial simulation and summarization over realizations&lt;/li&gt; &#xA; &lt;li&gt;decision making in the presence of uncertainty&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to see all my shared educational content check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/GeostatsGuy/Resources&#34;&gt;&lt;strong&gt;Resources Inventory&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GeostatsGuy/DataScienceInteractivePython/main/www.youtube.com/GeostatsGuyLectures&#34;&gt;&lt;strong&gt;GeostatsGuy Lectures&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I hope this is helpful to anyone interested to learn about spatial data analytics, geostatistics and machine learning. I&#39;m all about remoing barriers to education and encouraging folks to learn coding and data-driven modeling!&lt;/p&gt; &#xA;&lt;p&gt;Sincerely,&lt;/p&gt; &#xA;&lt;p&gt;Michael&lt;/p&gt; &#xA;&lt;h4&gt;The Author:&lt;/h4&gt; &#xA;&lt;h3&gt;Michael Pyrcz, Professor, The University of Texas at Austin&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Novel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;With over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers&#39; and geoscientists&#39; impact in subsurface resource development.&lt;/p&gt; &#xA;&lt;p&gt;For more about Michael check out these links:&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://twitter.com/geostatsguy&#34;&gt;Twitter&lt;/a&gt; | &lt;a href=&#34;https://github.com/GeostatsGuy&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;http://michaelpyrcz.com&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;amp;hl=en&amp;amp;oi=ao&#34;&gt;GoogleScholar&lt;/a&gt; | &lt;a href=&#34;https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446&#34;&gt;Book&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig&#34;&gt;YouTube&lt;/a&gt; | &lt;a href=&#34;https://www.linkedin.com/in/michael-pyrcz-61a648a1&#34;&gt;LinkedIn&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;Want to Work Together?&lt;/h4&gt; &#xA;&lt;p&gt;I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I&#39;d be happy to drop by and work with you!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;I can be reached at &lt;a href=&#34;mailto:mpyrcz@austin.utexas.edu&#34;&gt;mpyrcz@austin.utexas.edu&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I&#39;m always happy to discuss,&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Michael&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin&lt;/p&gt; &#xA;&lt;h4&gt;More Resources Available at: &lt;a href=&#34;https://twitter.com/geostatsguy&#34;&gt;Twitter&lt;/a&gt; | &lt;a href=&#34;https://github.com/GeostatsGuy&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;http://michaelpyrcz.com&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;amp;hl=en&amp;amp;oi=ao&#34;&gt;GoogleScholar&lt;/a&gt; | &lt;a href=&#34;https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446&#34;&gt;Book&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig&#34;&gt;YouTube&lt;/a&gt; | &lt;a href=&#34;https://www.linkedin.com/in/michael-pyrcz-61a648a1&#34;&gt;LinkedIn&lt;/a&gt;&lt;/h4&gt;</summary>
  </entry>
  <entry>
    <title>open-mmlab/Amphion</title>
    <updated>2024-11-02T01:31:45Z</updated>
    <id>tag:github.com,2024-11-02:/open-mmlab/Amphion</id>
    <link href="https://github.com/open-mmlab/Amphion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Amphion (/æmˈfaɪən/) is a toolkit for Audio, Music, and Speech Generation. Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Amphion: An Open-Source Audio, Music, and Speech Generation Toolkit&lt;/h1&gt; &#xA;&lt;div&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2312.09911&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/amphion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Amphion-pink&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://openxlab.org.cn/usercenter/Amphion&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://discord.com/invite/drhW7ajqAG&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-Join%20chat-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tts/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-TTS-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/svc/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-SVC-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tta/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-TTA-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/vocoder/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Vocoder-purple&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/metrics/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Evaluation-yellow&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/LICENSE-MIT-red&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://trendshift.io/repositories/5469&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/5469&#34; alt=&#34;open-mmlab%2FAmphion | Trendshift&#34; style=&#34;width: 150px; height: 33px;&#34; width=&#34;150&#34; height=&#34;33&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Amphion (/æmˈfaɪən/) is a toolkit for Audio, Music, and Speech Generation.&lt;/strong&gt; Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development. Amphion offers a unique feature: &lt;strong&gt;visualizations&lt;/strong&gt; of classic models or architectures. We believe that these visualizations are beneficial for junior researchers and engineers who wish to gain a better understanding of the model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The North-Star objective of Amphion is to offer a platform for studying the conversion of any inputs into audio.&lt;/strong&gt; Amphion is designed to support individual generation tasks, including but not limited to,&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;TTS&lt;/strong&gt;: Text to Speech (⛳&amp;nbsp;supported)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SVS&lt;/strong&gt;: Singing Voice Synthesis (👨‍💻&amp;nbsp;developing)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;VC&lt;/strong&gt;: Voice Conversion (👨‍💻&amp;nbsp;developing)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SVC&lt;/strong&gt;: Singing Voice Conversion (⛳&amp;nbsp;supported)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TTA&lt;/strong&gt;: Text to Audio (⛳&amp;nbsp;supported)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TTM&lt;/strong&gt;: Text to Music (👨‍💻&amp;nbsp;developing)&lt;/li&gt; &#xA; &lt;li&gt;more…&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In addition to the specific generation tasks, Amphion includes several &lt;strong&gt;vocoders&lt;/strong&gt; and &lt;strong&gt;evaluation metrics&lt;/strong&gt;. A vocoder is an important module for producing high-quality audio signals, while evaluation metrics are critical for ensuring consistent metrics in generation tasks. Moreover, Amphion is dedicated to advancing audio generation in real-world applications, such as building &lt;strong&gt;large-scale datasets&lt;/strong&gt; for speech synthesis.&lt;/p&gt; &#xA;&lt;h2&gt;🚀&amp;nbsp;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024/10/19&lt;/strong&gt;: We release &lt;strong&gt;MaskGCT&lt;/strong&gt;, a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision. MaskGCT is trained on &lt;a href=&#34;https://huggingface.co/datasets/amphion/Emilia-Dataset&#34;&gt;Emilia&lt;/a&gt; dataset and achieves SOTA zero-shot TTS perfermance. &lt;a href=&#34;https://arxiv.org/abs/2409.00750&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-COLOR.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/amphion/maskgct&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-model-yellow&#34; alt=&#34;hf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/amphion/maskgct&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-demo-pink&#34; alt=&#34;hf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/models/tts/maskgct/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Key%20Features-blue&#34; alt=&#34;readme&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024/09/01&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2312.09911&#34;&gt;Amphion&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2407.05361&#34;&gt;Emilia&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2310.11160&#34;&gt;DSFF-SVC&lt;/a&gt; got accepted by IEEE SLT 2024! 🤗&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024/08/28&lt;/strong&gt;: Welcome to join Amphion&#39;s &lt;a href=&#34;https://discord.com/invite/drhW7ajqAG&#34;&gt;Discord channel&lt;/a&gt; to stay connected and engage with our community！&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024/08/20&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2402.12660&#34;&gt;SingVisio&lt;/a&gt; got accepted by Computers &amp;amp; Graphics, &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0097849324001936&#34;&gt;available here&lt;/a&gt;! 🎉&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024/08/27&lt;/strong&gt;: &lt;em&gt;The Emilia dataset is now publicly available!&lt;/em&gt; Discover the most extensive and diverse speech generation dataset with 101k hours of in-the-wild speech data now at &lt;a href=&#34;https://huggingface.co/datasets/amphion/Emilia-Dataset&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow&#34; alt=&#34;hf&#34;&gt;&lt;/a&gt; or &lt;a href=&#34;https://opendatalab.com/Amphion/Emilia&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenDataLab-Dataset-blue&#34; alt=&#34;OpenDataLab&#34;&gt;&lt;/a&gt;! 👑👑👑&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024/07/01&lt;/strong&gt;: Amphion now releases &lt;strong&gt;Emilia&lt;/strong&gt;, the first open-source multilingual in-the-wild dataset for speech generation with over 101k hours of speech data, and the &lt;strong&gt;Emilia-Pipe&lt;/strong&gt;, the first open-source preprocessing pipeline designed to transform in-the-wild speech data into high-quality training data with annotations for speech generation! &lt;a href=&#34;https://arxiv.org/abs/2407.05361&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-COLOR.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/amphion/Emilia&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow&#34; alt=&#34;hf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://emilia-dataset.github.io/Emilia-Demo-Page/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/WebPage-Demo-red&#34; alt=&#34;demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/preprocessors/Emilia/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Key%20Features-blue&#34; alt=&#34;readme&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024/06/17&lt;/strong&gt;: Amphion has a new release for its &lt;strong&gt;VALL-E&lt;/strong&gt; model! It uses Llama as its underlying architecture and has better model performance, faster training speed, and more readable codes compared to our first version. &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tts/VALLE_V2/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Key%20Features-blue&#34; alt=&#34;readme&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024/03/12&lt;/strong&gt;: Amphion now support &lt;strong&gt;NaturalSpeech3 FACodec&lt;/strong&gt; and release pretrained checkpoints. &lt;a href=&#34;https://arxiv.org/abs/2403.03100&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-COLOR.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/amphion/naturalspeech3_facodec&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-model-yellow&#34; alt=&#34;hf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/amphion/naturalspeech3_facodec&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-demo-pink&#34; alt=&#34;hf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/models/codec/ns3_codec/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Key%20Features-blue&#34; alt=&#34;readme&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024/02/22&lt;/strong&gt;: The first Amphion visualization tool, &lt;strong&gt;SingVisio&lt;/strong&gt;, release. &lt;a href=&#34;https://arxiv.org/abs/2402.12660&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-COLOR.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openxlab.org.cn/apps/detail/Amphion/SingVisio&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg?sanitize=true&#34; alt=&#34;openxlab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/file/d/15097SGhQh-SwUNbdWDYNyWEP--YGLba5/view&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Video-Demo-orange&#34; alt=&#34;Video&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/visualization/SingVisio/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Key%20Features-blue&#34; alt=&#34;readme&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/12/18&lt;/strong&gt;: Amphion v0.1 release. &lt;a href=&#34;https://arxiv.org/abs/2312.09911&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/amphion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Amphion-pink&#34; alt=&#34;hf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=1aw0HhcggvQ&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/YouTube-Demo-red&#34; alt=&#34;youtube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/Amphion/pull/39&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Key%20Features-blue&#34; alt=&#34;readme&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/11/28&lt;/strong&gt;: Amphion alpha release. &lt;a href=&#34;https://github.com/open-mmlab/Amphion/pull/2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Key%20Features-blue&#34; alt=&#34;readme&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;⭐&amp;nbsp;Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;TTS: Text to Speech&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amphion achieves state-of-the-art performance compared to existing open-source repositories on text-to-speech (TTS) systems. It supports the following models or architectures: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.04558&#34;&gt;FastSpeech2&lt;/a&gt;: A non-autoregressive TTS architecture that utilizes feed-forward Transformer blocks.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.06103&#34;&gt;VITS&lt;/a&gt;: An end-to-end TTS architecture that utilizes conditional variational autoencoder with adversarial learning&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.02111&#34;&gt;VALL-E&lt;/a&gt;: A zero-shot TTS architecture that uses a neural codec language model with discrete codes.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.09116&#34;&gt;NaturalSpeech2&lt;/a&gt;: An architecture for TTS that utilizes a latent diffusion model to generate natural-sounding voices.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/Jets&#34;&gt;Jets&lt;/a&gt;: An end-to-end TTS model that jointly trains FastSpeech2 and HiFi-GAN with an alignment module.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.00750&#34;&gt;MaskGCT&lt;/a&gt;: a fully non-autoregressive TTS architecture that eliminates the need for explicit alignment information between text and speech supervision.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SVC: Singing Voice Conversion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ampion supports multiple content-based features from various pretrained models, including &lt;a href=&#34;https://github.com/wenet-e2e/wenet&#34;&gt;WeNet&lt;/a&gt;, &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt;, and &lt;a href=&#34;https://github.com/auspicious3000/contentvec&#34;&gt;ContentVec&lt;/a&gt;. Their specific roles in SVC has been investigated in our SLT 2024 paper. &lt;a href=&#34;https://arxiv.org/abs/2310.11160&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/svc/MultipleContentsSVC&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Code-red&#34; alt=&#34;code&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Amphion implements several state-of-the-art model architectures, including diffusion-, transformer-, VAE- and flow-based models. The diffusion-based architecture uses &lt;a href=&#34;https://openreview.net/pdf?id=a-xFK8Ymz5J&#34;&gt;Bidirectional dilated CNN&lt;/a&gt; as a backend and supports several sampling algorithms such as &lt;a href=&#34;https://arxiv.org/pdf/2006.11239.pdf&#34;&gt;DDPM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2010.02502.pdf&#34;&gt;DDIM&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/pdf/2202.09778.pdf&#34;&gt;PNDM&lt;/a&gt;. Additionally, it supports single-step inference based on the &lt;a href=&#34;https://openreview.net/pdf?id=FmqFfMTNnv&#34;&gt;Consistency Model&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TTA: Text to Audio&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amphion supports the TTA with a latent diffusion model. It is designed like &lt;a href=&#34;https://arxiv.org/abs/2301.12503&#34;&gt;AudioLDM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2301.12661&#34;&gt;Make-an-Audio&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2304.00830&#34;&gt;AUDIT&lt;/a&gt;. It is also the official implementation of the text-to-audio generation part of our NeurIPS 2023 paper. &lt;a href=&#34;https://arxiv.org/abs/2304.00830&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tta/RECIPE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Code-red&#34; alt=&#34;code&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Vocoder&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amphion supports various widely-used neural vocoders, including: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GAN-based vocoders: &lt;a href=&#34;https://arxiv.org/abs/1910.06711&#34;&gt;MelGAN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;HiFi-GAN&lt;/a&gt;, &lt;a href=&#34;https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts&#34;&gt;NSF-HiFiGAN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2206.04658&#34;&gt;BigVGAN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2305.07952&#34;&gt;APNet&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Flow-based vocoders: &lt;a href=&#34;https://arxiv.org/abs/1811.00002&#34;&gt;WaveGlow&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Diffusion-based vocoders: &lt;a href=&#34;https://arxiv.org/abs/2009.09761&#34;&gt;Diffwave&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Auto-regressive based vocoders: &lt;a href=&#34;https://arxiv.org/abs/1609.03499&#34;&gt;WaveNet&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1802.08435v1&#34;&gt;WaveRNN&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Amphion provides the official implementation of &lt;a href=&#34;https://arxiv.org/abs/2311.14957&#34;&gt;Multi-Scale Constant-Q Transform Discriminator&lt;/a&gt; (our ICASSP 2024 paper). It can be used to enhance any architecture GAN-based vocoders during training, and keep the inference stage (such as memory or speed) unchanged. &lt;a href=&#34;https://arxiv.org/abs/2311.14957&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/vocoder/gan/tfr_enhanced_hifigan&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Code-red&#34; alt=&#34;code&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Amphion provides a comprehensive objective evaluation of the generated audio. The evaluation metrics contain:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;F0 Modeling&lt;/strong&gt;: F0 Pearson Coefficients, F0 Periodicity Root Mean Square Error, F0 Root Mean Square Error, Voiced/Unvoiced F1 Score, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Energy Modeling&lt;/strong&gt;: Energy Root Mean Square Error, Energy Pearson Coefficients, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Intelligibility&lt;/strong&gt;: Character/Word Error Rate, which can be calculated based on &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt; and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spectrogram Distortion&lt;/strong&gt;: Frechet Audio Distance (FAD), Mel Cepstral Distortion (MCD), Multi-Resolution STFT Distance (MSTFT), Perceptual Evaluation of Speech Quality (PESQ), Short Time Objective Intelligibility (STOI), etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speaker Similarity&lt;/strong&gt;: Cosine similarity, which can be calculated based on &lt;a href=&#34;https://github.com/Jungjee/RawNet&#34;&gt;RawNet3&lt;/a&gt;, &lt;a href=&#34;https://github.com/resemble-ai/Resemblyzer&#34;&gt;Resemblyzer&lt;/a&gt;, &lt;a href=&#34;https://github.com/wenet-e2e/wespeaker&#34;&gt;WeSpeaker&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/wavlm&#34;&gt;WavLM&lt;/a&gt; and more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amphion unifies the data preprocess of the open-source datasets including &lt;a href=&#34;https://audiocaps.github.io/&#34;&gt;AudioCaps&lt;/a&gt;, &lt;a href=&#34;https://www.openslr.org/60/&#34;&gt;LibriTTS&lt;/a&gt;, &lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34;&gt;LJSpeech&lt;/a&gt;, &lt;a href=&#34;https://github.com/M4Singer/M4Singer&#34;&gt;M4Singer&lt;/a&gt;, &lt;a href=&#34;https://wenet.org.cn/opencpop/&#34;&gt;Opencpop&lt;/a&gt;, &lt;a href=&#34;https://github.com/Multi-Singer/Multi-Singer.github.io&#34;&gt;OpenSinger&lt;/a&gt;, &lt;a href=&#34;http://vc-challenge.org/&#34;&gt;SVCC&lt;/a&gt;, &lt;a href=&#34;https://datashare.ed.ac.uk/handle/10283/3443&#34;&gt;VCTK&lt;/a&gt;, and more. The supported dataset list can be seen &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/datasets/README.md&#34;&gt;here&lt;/a&gt; (updating).&lt;/li&gt; &#xA; &lt;li&gt;Amphion (exclusively) supports the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/preprocessors/Emilia/README.md&#34;&gt;&lt;strong&gt;Emilia&lt;/strong&gt;&lt;/a&gt; dataset and its preprocessing pipeline &lt;strong&gt;Emilia-Pipe&lt;/strong&gt; for in-the-wild speech data!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Visualization&lt;/h3&gt; &#xA;&lt;p&gt;Amphion provides visualization tools to interactively illustrate the internal processing mechanism of classic models. This provides an invaluable resource for educational purposes and for facilitating understandable research.&lt;/p&gt; &#xA;&lt;p&gt;Currently, Amphion supports &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/visualization/SingVisio/README.md&#34;&gt;SingVisio&lt;/a&gt;, a visualization tool of the diffusion model for singing voice conversion. &lt;a href=&#34;https://arxiv.org/abs/2402.12660&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-COLOR.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openxlab.org.cn/apps/detail/Amphion/SingVisio&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg?sanitize=true&#34; alt=&#34;openxlab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/file/d/15097SGhQh-SwUNbdWDYNyWEP--YGLba5/view&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Video-Demo-orange&#34; alt=&#34;Video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📀 Installation&lt;/h2&gt; &#xA;&lt;p&gt;Amphion can be installed through either Setup Installer or Docker Image.&lt;/p&gt; &#xA;&lt;h3&gt;Setup Installer&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/open-mmlab/Amphion.git&#xA;cd Amphion&#xA;&#xA;# Install Python Environment&#xA;conda create --name amphion python=3.9.15&#xA;conda activate amphion&#xA;&#xA;# Install Python Packages Dependencies&#xA;sh env.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Image&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;Docker&lt;/a&gt;, &lt;a href=&#34;https://www.nvidia.com/download/index.aspx&#34;&gt;NVIDIA Driver&lt;/a&gt;, &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt;, and &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;CUDA&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the following commands:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/open-mmlab/Amphion.git&#xA;cd Amphion&#xA;&#xA;docker pull realamphion/amphion&#xA;docker run --runtime=nvidia --gpus all -it -v .:/app realamphion/amphion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Mount dataset by argument &lt;code&gt;-v&lt;/code&gt; is necessary when using Docker. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/datasets/docker.md&#34;&gt;Mount dataset in Docker container&lt;/a&gt; and &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/container_run/#volume&#34;&gt;Docker Docs&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;🐍 Usage in Python&lt;/h2&gt; &#xA;&lt;p&gt;We detail the instructions of different tasks in the following recipes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tts/README.md&#34;&gt;Text to Speech (TTS)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/svc/README.md&#34;&gt;Singing Voice Conversion (SVC)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tta/README.md&#34;&gt;Text to Audio (TTA)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/vocoder/README.md&#34;&gt;Vocoder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/metrics/README.md&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/visualization/README.md&#34;&gt;Visualization&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;👨‍💻 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate all contributions to improve Amphion. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/.github/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for the contributing guideline.&lt;/p&gt; &#xA;&lt;h2&gt;🙏&amp;nbsp;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ming024/FastSpeech2&#34;&gt;ming024&#39;s FastSpeech2&lt;/a&gt; and &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;jaywalnut310&#39;s VITS&lt;/a&gt; for model architecture code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lifeiteng/vall-e&#34;&gt;lifeiteng&#39;s VALL-E&lt;/a&gt; for training pipeline and model architecture design.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ZhangXInFD/SpeechTokenizer&#34;&gt;SpeechTokenizer&lt;/a&gt; for semantic-distilled tokenizer design.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wenet-e2e/wenet&#34;&gt;WeNet&lt;/a&gt;, &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt;, &lt;a href=&#34;https://github.com/auspicious3000/contentvec&#34;&gt;ContentVec&lt;/a&gt;, and &lt;a href=&#34;https://github.com/Jungjee/RawNet&#34;&gt;RawNet3&lt;/a&gt; for pretrained models and inference code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jik876/hifi-gan&#34;&gt;HiFi-GAN&lt;/a&gt; for GAN-based Vocoder&#39;s architecture design and training strategy.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/encodec&#34;&gt;Encodec&lt;/a&gt; for well-organized GAN Discriminator&#39;s architecture and basic blocks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Latent Diffusion&lt;/a&gt; for model architecture design.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TensorSpeech/TensorFlowTTS&#34;&gt;TensorFlowTTS&lt;/a&gt; for preparing the MFA tools.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;©️&amp;nbsp;License&lt;/h2&gt; &#xA;&lt;p&gt;Amphion is under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;. It is free for both research and commercial use cases.&lt;/p&gt; &#xA;&lt;h2&gt;📚 Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{amphion,&#xA;    author={Zhang, Xueyao and Xue, Liumeng and Gu, Yicheng and Wang, Yuancheng and Li, Jiaqi and He, Haorui and Wang, Chaoren and Song, Ting and Chen, Xi and Fang, Zihao and Chen, Haopeng and Zhang, Junan and Tang, Tze Ying and Zou, Lexiao and Wang, Mingxuan and Han, Jun and Chen, Kai and Li, Haizhou and Wu, Zhizheng},&#xA;    title={Amphion: An Open-Source Audio, Music and Speech Generation Toolkit},&#xA;    booktitle={{IEEE} Spoken Language Technology Workshop, {SLT} 2024},&#xA;    year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>