<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-29T01:26:57Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hkproj/pytorch-transformer</title>
    <updated>2023-12-29T01:26:57Z</updated>
    <id>tag:github.com,2023-12-29:/hkproj/pytorch-transformer</id>
    <link href="https://github.com/hkproj/pytorch-transformer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Attention is all you need implementation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pytorch-transformer&lt;/h1&gt; &#xA;&lt;p&gt;Attention is all you need implementation&lt;/p&gt; &#xA;&lt;p&gt;YouTube video with full step-by-step implementation: &lt;a href=&#34;https://www.youtube.com/watch?v=ISNdQcPhsts&#34;&gt;https://www.youtube.com/watch?v=ISNdQcPhsts&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenGVLab/InternVL</title>
    <updated>2023-12-29T01:26:57Z</updated>
    <id>tag:github.com,2023-12-29:/OpenGVLab/InternVL</id>
    <link href="https://github.com/OpenGVLab/InternVL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks ‚Äî‚Äî An Open-Source Alternative to ViT-22B&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img width=&#34;60&#34; alt=&#34;image&#34; src=&#34;https://github.com/OpenGVLab/InternVL/assets/8529570/1e96c12b-4a1c-4b8e-84fb-b8a793a9a388&#34;&gt; InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks ‚Äî‚Äî An Open-Source Alternative to ViT-22B&lt;/h1&gt; &#xA;&lt;h2&gt;What is InternVL?&lt;/h2&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2312.14238&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://internvl.opengvlab.com/&#34;&gt;Chat Demo&lt;/a&gt;] [&lt;a href=&#34;&#34;&gt;Retrieval Demo (Coming Soon)&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/#quick-start-with-huggingface&#34;&gt;Quick Start&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;InternVL scales up the ViT to &lt;em&gt;&lt;strong&gt;6B parameters&lt;/strong&gt;&lt;/em&gt; and aligns it with LLM.&lt;/p&gt; &#xA;&lt;p&gt;It is &lt;em&gt;&lt;strong&gt;the largest open-source vision/vision-language foundation model (14B)&lt;/strong&gt;&lt;/em&gt; to date, achieving &lt;em&gt;&lt;strong&gt;32 state-of-the-art&lt;/strong&gt;&lt;/em&gt; performances on a wide range of tasks such as visual perception, cross-modal retrieval, multimodal dialogue, etc.&lt;/p&gt; &#xA;&lt;img width=&#34;1204&#34; alt=&#34;image&#34; src=&#34;https://github.com/OpenGVLab/InternVL/assets/23737120/c9f93b54-fdba-4a69-9341-e905376f7b9c&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-cross-modal-retrieval-on-coco-2014?p=internvl-scaling-up-vision-foundation-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvl-scaling-up-vision-foundation-models/zero-shot-cross-modal-retrieval-on-coco-2014&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-image-retrieval-on-coco-cn?p=internvl-scaling-up-vision-foundation-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvl-scaling-up-vision-foundation-models/zero-shot-image-retrieval-on-coco-cn&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-cross-modal-retrieval-on-flickr30k?p=internvl-scaling-up-vision-foundation-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvl-scaling-up-vision-foundation-models/zero-shot-cross-modal-retrieval-on-flickr30k&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/image-to-text-retrieval-on-flickr30k?p=internvl-scaling-up-vision-foundation-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvl-scaling-up-vision-foundation-models/image-to-text-retrieval-on-flickr30k&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-image-retrieval-on-flickr30k-cn?p=internvl-scaling-up-vision-foundation-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvl-scaling-up-vision-foundation-models/zero-shot-image-retrieval-on-flickr30k-cn&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/image-retrieval-on-flickr30k-cn?p=internvl-scaling-up-vision-foundation-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvl-scaling-up-vision-foundation-models/image-retrieval-on-flickr30k-cn&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-image-retrieval-on-xtd10?p=internvl-scaling-up-vision-foundation-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvl-scaling-up-vision-foundation-models/zero-shot-image-retrieval-on-xtd10&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-transfer-image-classification-on-cn?p=internvl-scaling-up-vision-foundation-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvl-scaling-up-vision-foundation-models/zero-shot-transfer-image-classification-on-cn&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-transfer-image-classification-on-8?p=internvl-scaling-up-vision-foundation-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvl-scaling-up-vision-foundation-models/zero-shot-transfer-image-classification-on-8&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-video-retrieval-on-msr-vtt?p=internvl-scaling-up-vision-foundation-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvl-scaling-up-vision-foundation-models/zero-shot-video-retrieval-on-msr-vtt&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-transfer-image-classification-on-6?p=internvl-scaling-up-vision-foundation-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvl-scaling-up-vision-foundation-models/zero-shot-transfer-image-classification-on-6&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-transfer-image-classification-on-5?p=internvl-scaling-up-vision-foundation-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvl-scaling-up-vision-foundation-models/zero-shot-transfer-image-classification-on-5&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-transfer-image-classification-on-3?p=internvl-scaling-up-vision-foundation-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvl-scaling-up-vision-foundation-models/zero-shot-transfer-image-classification-on-3&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-transfer-image-classification-on-1?p=internvl-scaling-up-vision-foundation-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internvl-scaling-up-vision-foundation-models/zero-shot-transfer-image-classification-on-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What can InternVL do?&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Visual Perception (click to expand)&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;* ViT-22B uses the private JFT-3B dataset.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Linear-Probe Image Classification &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/classification#-evaluation&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;#param&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-1K&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-ReaL&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-V2&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-A&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-R&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-Sketch&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;OpenCLIP-G&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1.8B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;86.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;89.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;77.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;63.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;87.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;66.4&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;DINOv2-g&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1.1B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;86.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;89.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;78.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;75.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;78.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;62.5&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;EVA-01-CLIP-g&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1.1B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;86.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;89.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;77.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;70.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;87.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;63.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;MAWS-ViT-6.5B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;6.5B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;87.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;ViT-22B*&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;21.7B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;89.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;90.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;83.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;83.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;87.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;‚àí&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternViT-6B (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;5.9B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;88.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;90.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;79.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;77.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;89.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;69.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Semantic Segmentation &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/segmentation#-evaluation&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;decoder&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;#param (train/total)&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;crop size&lt;/th&gt; &#xA;      &lt;th&gt;mIoU&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;OpenCLIP-G (frozen)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Linear&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.3M / 1.8B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;      &lt;td&gt;39.3&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;ViT-22B (frozen)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Linear&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.9M / 21.7B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;504&lt;/td&gt; &#xA;      &lt;td&gt;34.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternViT-6B (frozen)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Linear&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.5M / 5.9B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;504&lt;/td&gt; &#xA;      &lt;td&gt;47.2 (+12.6)&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;ViT-22B (frozen)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;UperNet&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.8B / 22.5B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;504&lt;/td&gt; &#xA;      &lt;td&gt;52.7&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternViT-6B (frozen)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;UperNet&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.4B / 6.3B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;504&lt;/td&gt; &#xA;      &lt;td&gt;54.9 (+2.2)&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;ViT-22B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;UperNet&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;22.5B / 22.5B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;504&lt;/td&gt; &#xA;      &lt;td&gt;55.3&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternViT-6B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;UperNet&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;6.3B / 6.3B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;504&lt;/td&gt; &#xA;      &lt;td&gt;58.9 (+3.6)&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Zero-Shot Image Classification &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/clip_benchmark#imagenet-variants-and-objectnet&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-1K&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-A&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-R&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-V2&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-Sketch&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;ObjectNet&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;OpenCLIP-G&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;80.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;69.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;92.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;73.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;68.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;73.0&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;EVA-02-CLIP-E+&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;82.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;82.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;75.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;71.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;79.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;ViT-22B*&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;85.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;90.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;80.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;‚àí&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;87.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-C (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;83.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;83.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;77.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;73.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;80.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Multilingual Zero-Shot Image Classification &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/clip_benchmark#multilingual-imagenet-1k&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EN: English, ZH: Chinese, JP: Japanese, Ar: Arabic, IT: Italian&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-1K (EN)&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-1K (ZH)&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-1K (JP)&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-1K (AR)&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-1K (IT)&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;Taiyi-CLIP-ViT-H&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;54.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;WuKong-ViT-L-G&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;57.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;CN-CLIP-ViT-H&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;59.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;AltCLIP-ViT-L&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;74.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;59.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;EVA-02-CLIP-E+&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;82.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;41.2&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;OpenCLIP-XLM-R-H&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;77.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;55.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;53.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;37.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;56.8&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-C (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;83.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;64.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;61.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;44.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;65.7&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Zero-Shot Video Classification [see details]&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;#frame&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;K400&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;K600&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;K700&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;OpenCLIP-G&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;65.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;66.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;59.2&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;EVA-02-CLIP-E+&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;69.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;69.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;63.4&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-C (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;76.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;75.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;67.5&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;ViCLIP&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;75.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;73.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;66.4&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-C (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;79.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;78.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;71.5&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Cross-Modal Retrieval (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;English Zero-Shot Image-Text Retrieval &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/clip_benchmark#flickr30k--coco&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;tbody&gt;&#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td rowspan=&#34;3&#34; align=&#34;left&#34;&gt;&lt;b&gt;model&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;6&#34; align=&#34;center&#34;&gt;&lt;b&gt;Flickr30K&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;6&#34; align=&#34;center&#34;&gt;&lt;b&gt;COCO&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td rowspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;avg&lt;/b&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;image-to-text&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;text-to-image&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;image-to-text&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;text-to-image&lt;/b&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;OpenCLIP-G&lt;/td&gt; &#xA;      &lt;td&gt;92.9&lt;/td&gt; &#xA;      &lt;td&gt;99.3&lt;/td&gt; &#xA;      &lt;td&gt;99.8&lt;/td&gt; &#xA;      &lt;td&gt;79.5&lt;/td&gt; &#xA;      &lt;td&gt;95.0&lt;/td&gt; &#xA;      &lt;td&gt;97.1&lt;/td&gt; &#xA;      &lt;td&gt;67.3&lt;/td&gt; &#xA;      &lt;td&gt;86.9&lt;/td&gt; &#xA;      &lt;td&gt;92.6&lt;/td&gt; &#xA;      &lt;td&gt;51.4&lt;/td&gt; &#xA;      &lt;td&gt;74.9&lt;/td&gt; &#xA;      &lt;td&gt;83.0&lt;/td&gt; &#xA;      &lt;td&gt;85.0&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;EVA-02-CLIP-E+&lt;/td&gt; &#xA;      &lt;td&gt;93.9&lt;/td&gt; &#xA;      &lt;td&gt;99.4&lt;/td&gt; &#xA;      &lt;td&gt;99.8&lt;/td&gt; &#xA;      &lt;td&gt;78.8&lt;/td&gt; &#xA;      &lt;td&gt;94.2&lt;/td&gt; &#xA;      &lt;td&gt;96.8&lt;/td&gt; &#xA;      &lt;td&gt;68.8&lt;/td&gt; &#xA;      &lt;td&gt;87.8&lt;/td&gt; &#xA;      &lt;td&gt;92.8&lt;/td&gt; &#xA;      &lt;td&gt;51.1&lt;/td&gt; &#xA;      &lt;td&gt;75.0&lt;/td&gt; &#xA;      &lt;td&gt;82.7&lt;/td&gt; &#xA;      &lt;td&gt;85.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;InternVL-C (ours)&lt;/td&gt; &#xA;      &lt;td&gt;94.7&lt;/td&gt; &#xA;      &lt;td&gt;99.6&lt;/td&gt; &#xA;      &lt;td&gt;99.9&lt;/td&gt; &#xA;      &lt;td&gt;81.7&lt;/td&gt; &#xA;      &lt;td&gt;96.0&lt;/td&gt; &#xA;      &lt;td&gt;98.2&lt;/td&gt; &#xA;      &lt;td&gt;70.6&lt;/td&gt; &#xA;      &lt;td&gt;89.0&lt;/td&gt; &#xA;      &lt;td&gt;93.5&lt;/td&gt; &#xA;      &lt;td&gt;54.1&lt;/td&gt; &#xA;      &lt;td&gt;77.3&lt;/td&gt; &#xA;      &lt;td&gt;84.6&lt;/td&gt; &#xA;      &lt;td&gt;86.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;InternVL-G (ours)&lt;/td&gt; &#xA;      &lt;td&gt;95.7&lt;/td&gt; &#xA;      &lt;td&gt;99.7&lt;/td&gt; &#xA;      &lt;td&gt;99.9&lt;/td&gt; &#xA;      &lt;td&gt;85.0&lt;/td&gt; &#xA;      &lt;td&gt;97.0&lt;/td&gt; &#xA;      &lt;td&gt;98.6&lt;/td&gt; &#xA;      &lt;td&gt;74.9&lt;/td&gt; &#xA;      &lt;td&gt;91.3&lt;/td&gt; &#xA;      &lt;td&gt;95.2&lt;/td&gt; &#xA;      &lt;td&gt;58.6&lt;/td&gt; &#xA;      &lt;td&gt;81.3&lt;/td&gt; &#xA;      &lt;td&gt;88.0&lt;/td&gt; &#xA;      &lt;td&gt;88.8&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt;&#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Chinese Zero-Shot Image-Text Retrieval &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/clip_benchmark#flickr30k-cn--coco-cn&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;tbody&gt;&#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td rowspan=&#34;3&#34; align=&#34;left&#34;&gt;&lt;b&gt;model&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;6&#34; align=&#34;center&#34;&gt;&lt;b&gt;Flickr30K-CN&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;6&#34; align=&#34;center&#34;&gt;&lt;b&gt;COCO-CN&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td rowspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;avg&lt;/b&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;image-to-text&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;text-to-image&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;image-to-text&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;text-to-image&lt;/b&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;CN-CLIP-ViT-H&lt;/td&gt; &#xA;      &lt;td&gt;81.6&lt;/td&gt; &#xA;      &lt;td&gt;97.5&lt;/td&gt; &#xA;      &lt;td&gt;98.8&lt;/td&gt; &#xA;      &lt;td&gt;71.2&lt;/td&gt; &#xA;      &lt;td&gt;91.4&lt;/td&gt; &#xA;      &lt;td&gt;95.5&lt;/td&gt; &#xA;      &lt;td&gt;63.0&lt;/td&gt; &#xA;      &lt;td&gt;86.6&lt;/td&gt; &#xA;      &lt;td&gt;92.9&lt;/td&gt; &#xA;      &lt;td&gt;69.2&lt;/td&gt; &#xA;      &lt;td&gt;89.9&lt;/td&gt; &#xA;      &lt;td&gt;96.1&lt;/td&gt; &#xA;      &lt;td&gt;86.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;OpenCLIP-XLM-R-H&lt;/td&gt; &#xA;      &lt;td&gt;86.1&lt;/td&gt; &#xA;      &lt;td&gt;97.5&lt;/td&gt; &#xA;      &lt;td&gt;99.2&lt;/td&gt; &#xA;      &lt;td&gt;71.0&lt;/td&gt; &#xA;      &lt;td&gt;90.5&lt;/td&gt; &#xA;      &lt;td&gt;94.9&lt;/td&gt; &#xA;      &lt;td&gt;70.0&lt;/td&gt; &#xA;      &lt;td&gt;91.5&lt;/td&gt; &#xA;      &lt;td&gt;97.0&lt;/td&gt; &#xA;      &lt;td&gt;66.1&lt;/td&gt; &#xA;      &lt;td&gt;90.8&lt;/td&gt; &#xA;      &lt;td&gt;96.0&lt;/td&gt; &#xA;      &lt;td&gt;87.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;InternVL-C (ours)&lt;/td&gt; &#xA;      &lt;td&gt;90.3&lt;/td&gt; &#xA;      &lt;td&gt;98.8&lt;/td&gt; &#xA;      &lt;td&gt;99.7&lt;/td&gt; &#xA;      &lt;td&gt;75.1&lt;/td&gt; &#xA;      &lt;td&gt;92.9&lt;/td&gt; &#xA;      &lt;td&gt;96.4&lt;/td&gt; &#xA;      &lt;td&gt;68.8&lt;/td&gt; &#xA;      &lt;td&gt;92.0&lt;/td&gt; &#xA;      &lt;td&gt;96.7&lt;/td&gt; &#xA;      &lt;td&gt;68.9&lt;/td&gt; &#xA;      &lt;td&gt;91.9&lt;/td&gt; &#xA;      &lt;td&gt;96.5&lt;/td&gt; &#xA;      &lt;td&gt;89.0&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;InternVL-G (ours)&lt;/td&gt; &#xA;      &lt;td&gt;92.9&lt;/td&gt; &#xA;      &lt;td&gt;99.4&lt;/td&gt; &#xA;      &lt;td&gt;99.8&lt;/td&gt; &#xA;      &lt;td&gt;77.7&lt;/td&gt; &#xA;      &lt;td&gt;94.8&lt;/td&gt; &#xA;      &lt;td&gt;97.3&lt;/td&gt; &#xA;      &lt;td&gt;71.4&lt;/td&gt; &#xA;      &lt;td&gt;93.9&lt;/td&gt; &#xA;      &lt;td&gt;97.7&lt;/td&gt; &#xA;      &lt;td&gt;73.8&lt;/td&gt; &#xA;      &lt;td&gt;94.4&lt;/td&gt; &#xA;      &lt;td&gt;98.1&lt;/td&gt; &#xA;      &lt;td&gt;90.9&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt;&#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Multilingual Zero-Shot Image-Text Retrieval on XTD &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/clip_benchmark#xtd&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;EN&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;ES&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;FR&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;ZH&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IT&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;KO&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;RU&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;JP&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;average&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;AltCLIP&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;92.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;91.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;91.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;93.7&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;OpenCLIP-XLM-R-H&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;97.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;90.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;93.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-C (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;97.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;92.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;93.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-G (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;98.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;97.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Multimodal Dialogue (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Zero-Shot Image Captioning &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/internvl_g#zero-shot-image-captioning&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;COCO&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Flickr30K&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;NoCaps&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;Emu-I&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;117.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;DreamLLM&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;115.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-G (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;128.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;79.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;113.7&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Multimodal Benchmarks with Frozen LLM &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/internvl_chat#-evaluation&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;visual encoder&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;glue layer&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;LLM&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;res.&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;COCO&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Flickr&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;NoCaps&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;VQAv2&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;GQA&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;VizWiz&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;TextVQA&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;MME&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;POPE&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InstructBLIP&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;EVA-g&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;QFormer&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Vicuna-7B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;224&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;‚Äì&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;82.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;123.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;‚Äì&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;49.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;34.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;50.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;‚Äì&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;‚Äì&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;BLIP-2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;EVA-g&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;QFormer&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Vicuna-13B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;224&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;‚Äì&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;71.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;103.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;41.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;41.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;19.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;42.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1293.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;85.3&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InstructBLIP&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;EVA-g&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;QFormer&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Vicuna-13B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;224&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;‚Äì&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;82.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;121.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;‚Äì&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;49.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;33.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;50.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1212.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;78.9&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-Chat (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;IViT-6B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;QLLaMA&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Vicuna-7B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;224&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;141.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;89.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;120.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;72.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;57.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;44.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;42.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1298.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;85.2&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-Chat (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;IViT-6B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;QLLaMA&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Vicuna-13B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;224&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;142.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;89.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;123.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;71.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;59.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;54.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;49.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1317.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;85.4&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Multimodal Benchmarks with Trainable LLM &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/llava&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;visual encoder&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;glue layer&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;LLM&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;res.&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;VQAv2&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;GQA&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;VizWiz&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;TextVQA&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;MME&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;POPE&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;LLaVA-1.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;CLIP-L-336&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;MLP&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Vicuna-7B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;336&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;78.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;62.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;50.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;58.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1510.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;85.9&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-Chat (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;IViT-6B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;MLP&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Vicuna-7B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;336&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;79.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;62.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;52.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;57.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1525.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;86.4&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;LLaVA-1.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;CLIP-L-336&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;MLP&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Vicuna-13B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;336&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;80.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;63.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;53.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;61.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1531.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;85.9&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-Chat (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;IViT-6B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;MLP&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Vicuna-13B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;336&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;80.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;63.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;54.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;58.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1546.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;87.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Tiny LVLM &lt;a href=&#34;https://github.com/OpenGVLab/Multi-Modality-Arena/tree/main/tiny_lvlm_evaluation&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Rank&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Version&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;Score&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;üèÖÔ∏è&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/OpenGVLab/InternVL&#34;&gt;InternVL&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;InternVL-Chat&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;327.61&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;ü•à&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer&#34;&gt;InternLM-XComposer-VL&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;InternLM-XComposer-VL-7B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;322.51&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;ü•â&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://bard.google.com/&#34;&gt;Bard&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Bard&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;319.59&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen-VL&#34;&gt;Qwen-VL-Chat&lt;/a&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Qwen-VL-Chat&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;316.81&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA-1.5&lt;/a&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Vicuna-7B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;307.17&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/instructblip&#34;&gt;InstructBLIP&lt;/a&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Vicuna-7B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;300.64&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer&#34;&gt;InternLM-XComposer&lt;/a&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;InternLM-XComposer-7B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;288.89&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/blip2&#34;&gt;BLIP2&lt;/a&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;FlanT5xl&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;284.72&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/mlpc-ucsd/BLIVA&#34;&gt;BLIVA&lt;/a&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Vicuna-7B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;284.17&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bytedance/lynx-llm&#34;&gt;Lynx&lt;/a&gt;&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Vicuna-7B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;279.24&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/INSTALLATION.md&#34;&gt;INSTALLATION.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start with Huggingface&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;using InternViT-6B (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from PIL import Image&#xA;from transformers import AutoModel, CLIPImageProcessor&#xA;&#xA;model = AutoModel.from_pretrained(&#xA;    &#39;OpenGVLab/InternViT-6B-224px&#39;,&#xA;    torch_dtype=torch.bfloat16,&#xA;    low_cpu_mem_usage=True,&#xA;    trust_remote_code=True).cuda().eval()&#xA;&#xA;image = Image.open(&#39;./examples/image1.jpg&#39;).convert(&#39;RGB&#39;)&#xA;&#xA;image_processor = CLIPImageProcessor.from_pretrained(&#39;OpenGVLab/InternViT-6B-224px&#39;)&#xA;&#xA;pixel_values = image_processor(images=image, return_tensors=&#39;pt&#39;).pixel_values&#xA;pixel_values = pixel_values.to(torch.bfloat16).cuda()&#xA;&#xA;outputs = model(pixel_values)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;using InternVL-C(ontrastive) and InternVL-G(enerative) (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from PIL import Image&#xA;from transformers import AutoModel, CLIPImageProcessor&#xA;from transformers import AutoTokenizer&#xA;&#xA;&#xA;model = AutoModel.from_pretrained(&#xA;    &#39;OpenGVLab/InternVL-14B-224px&#39;,&#xA;    torch_dtype=torch.bfloat16,&#xA;    low_cpu_mem_usage=True,&#xA;    trust_remote_code=True).cuda().eval()&#xA;&#xA;image_processor = CLIPImageProcessor.from_pretrained(&#39;OpenGVLab/InternVL-14B-224px&#39;)&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#xA;    &#39;OpenGVLab/InternVL-14B-224px&#39;, use_fast=False, add_eos_token=True)&#xA;tokenizer.pad_token_id = 0  # set pad_token_id to 0&#xA;&#xA;images = [&#xA;    Image.open(&#39;./examples/image1.jpg&#39;).convert(&#39;RGB&#39;),&#xA;    Image.open(&#39;./examples/image2.jpg&#39;).convert(&#39;RGB&#39;),&#xA;    Image.open(&#39;./examples/image3.jpg&#39;).convert(&#39;RGB&#39;)&#xA;]&#xA;prefix = &#39;summarize:&#39;&#xA;texts = [&#xA;    prefix + &#39;a photo of a red panda&#39;,  # English&#xA;    prefix + &#39;‰∏ÄÂº†ÁÜäÁå´ÁöÑÁÖßÁâá&#39;,  # Chinese&#xA;    prefix + &#39;‰∫åÂåπ„ÅÆÁå´„ÅÆÂÜôÁúü&#39;  # Japanese&#xA;]&#xA;&#xA;pixel_values = image_processor(images=images, return_tensors=&#39;pt&#39;).pixel_values&#xA;pixel_values = pixel_values.to(torch.bfloat16).cuda()&#xA;input_ids = tokenizer(texts, return_tensors=&#39;pt&#39;, max_length=80,&#xA;                      truncation=True, padding=&#39;max_length&#39;).input_ids.cuda()&#xA;&#xA;# InternVL-C&#xA;logits_per_image, logits_per_text = model(&#xA;    image=pixel_values, text=input_ids, mode=&#39;InternVL-C&#39;)&#xA;probs = logits_per_image.softmax(dim=-1)&#xA;# tensor([[9.9609e-01, 5.2185e-03, 6.0070e-08],&#xA;#         [2.2949e-02, 9.7656e-01, 5.9903e-06],&#xA;#         [3.2932e-06, 7.4863e-05, 1.0000e+00]], device=&#39;cuda:0&#39;,&#xA;#        dtype=torch.bfloat16, grad_fn=&amp;lt;SoftmaxBackward0&amp;gt;)&#xA;&#xA;# InternVL-G&#xA;logits_per_image, logits_per_text = model(&#xA;    image=pixel_values, text=input_ids, mode=&#39;InternVL-G&#39;)&#xA;probs = logits_per_image.softmax(dim=-1)&#xA;# tensor([[9.9609e-01, 3.1738e-03, 3.6322e-08],&#xA;#         [8.6060e-03, 9.9219e-01, 2.8759e-06],&#xA;#         [1.7583e-06, 3.1233e-05, 1.0000e+00]], device=&#39;cuda:0&#39;,&#xA;#        dtype=torch.bfloat16, grad_fn=&amp;lt;SoftmaxBackward0&amp;gt;)&#xA;&#xA;# please set add_eos_token to False for generation&#xA;tokenizer.add_eos_token = False&#xA;image = Image.open(&#39;./examples/image1.jpg&#39;).convert(&#39;RGB&#39;)&#xA;pixel_values = image_processor(images=image, return_tensors=&#39;pt&#39;).pixel_values&#xA;pixel_values = pixel_values.to(torch.bfloat16).cuda()&#xA;&#xA;tokenized = tokenizer(&#34;English caption:&#34;, return_tensors=&#39;pt&#39;)&#xA;pred = model.generate(&#xA;    pixel_values=pixel_values,&#xA;    input_ids=tokenized.input_ids.cuda(),&#xA;    attention_mask=tokenized.attention_mask.cuda(),&#xA;    num_beams=5,&#xA;    min_new_tokens=8,&#xA;)&#xA;caption = tokenizer.decode(pred[0].cpu(), skip_special_tokens=True).strip()&#xA;# English caption: a red panda sitting on top of a wooden platform&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;using InternVL-Chat (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TODO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Schedule&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release high-resolution models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release InternVL-Chat&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release InternVL-C(ontrastive) and InternVL-G(enerative)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release InternViT-6B&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/LICENSE&#34;&gt;MIT license&lt;/a&gt;. Parts of this project contain code and models from other sources, which are subject to their respective licenses.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@article{chen2023internvl,&#xA;  title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},&#xA;  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},&#xA;  journal={arXiv preprint arXiv:2312.14238},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;InternVL is built with reference to the code of the following projects: &lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;OpenAI CLIP&lt;/a&gt;, &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;Open CLIP&lt;/a&gt;, &lt;a href=&#34;https://github.com/LAION-AI/CLIP_benchmark&#34;&gt;CLIP Benchmark&lt;/a&gt;, &lt;a href=&#34;https://github.com/baaivision/EVA/tree/master&#34;&gt;EVA&lt;/a&gt;, &lt;a href=&#34;https://github.com/OpenGVLab/InternImage&#34;&gt;InternImage&lt;/a&gt;, &lt;a href=&#34;https://github.com/czczup/ViT-Adapter&#34;&gt;ViT-Adapter&lt;/a&gt;, &lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;MMSegmentation&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/dinov2&#34;&gt;DINOv2&lt;/a&gt;, &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/blip2&#34;&gt;BLIP-2&lt;/a&gt;, &lt;a href=&#34;https://github.com/QwenLM/Qwen-VL/tree/master/eval_mm&#34;&gt;Qwen-VL&lt;/a&gt;, and &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA-1.5&lt;/a&gt;. Thanks for their awesome work!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If you want to join our WeChat group, please scan the following QR Code to add our assistant as a Wechat friend:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;300&#34; alt=&#34;image&#34; src=&#34;https://github.com/OpenGVLab/DragGAN/assets/26198430/e3f0807f-956a-474e-8fd2-1f7c22d73997&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>