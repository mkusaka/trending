<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-23T01:37:59Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>liguodongiot/llm-action</title>
    <updated>2023-07-23T01:37:59Z</updated>
    <id>tag:github.com,2023-07-23:/liguodongiot/llm-action</id>
    <link href="https://github.com/liguodongiot/llm-action" rel="alternate"></link>
    <summary type="html">&lt;p&gt;本项目旨在分享大模型相关技术原理以及实战经验。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llm-action&lt;/h1&gt; &#xA;&lt;p&gt;本项目旨在分享大模型相关技术原理以及实战经验。&lt;/p&gt; &#xA;&lt;h2&gt;目录&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E8%AE%AD%E7%BB%83&#34;&gt;&lt;span&gt;🔥&lt;/span&gt; LLM训练&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E8%AE%AD%E7%BB%83%E5%AE%9E%E6%88%98&#34;&gt;&lt;span&gt;🐫&lt;/span&gt; LLM训练实战&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86&#34;&gt;&lt;span&gt;🐼&lt;/span&gt; LLM参数高效微调技术原理综述&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98&#34;&gt;&lt;span&gt;🐰&lt;/span&gt; LLM参数高效微调技术实战&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%A1%8C%E6%8A%80%E6%9C%AF&#34;&gt;&lt;span&gt;🐘&lt;/span&gt; LLM分布式训练并行技术&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E5%88%86%E5%B8%83%E5%BC%8Fai%E6%A1%86%E6%9E%B6&#34;&gt;&lt;span&gt;🌋&lt;/span&gt; 分布式AI框架&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E6%8E%A8%E7%90%86&#34;&gt;&lt;span&gt;🐎&lt;/span&gt; LLM推理&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F&#34;&gt;&lt;span&gt;🚀&lt;/span&gt; 模型推理加速&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1%E5%8C%96&#34;&gt;&lt;span&gt;✈&lt;/span&gt; 模型推理服务化&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E9%87%8F%E5%8C%96&#34;&gt;&lt;span&gt;📐&lt;/span&gt; LLM量化&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E5%9B%BD%E4%BA%A7%E5%8C%96%E9%80%82%E9%85%8D&#34;&gt;&lt;span&gt;💫&lt;/span&gt; LLM国产化适配&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E7%94%9F%E6%80%81%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF&#34;&gt;&lt;span&gt;🍄&lt;/span&gt; LLM生态相关技术&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85&#34;&gt;&lt;span&gt;🔨&lt;/span&gt; 服务器基础环境软件安装&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#llm%E5%AD%A6%E4%B9%A0%E4%BA%A4%E6%B5%81%E7%BE%A4&#34;&gt;&lt;span&gt;💬&lt;/span&gt; LLM学习交流群&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7&#34;&gt;&lt;span&gt;👥&lt;/span&gt; 微信公众号&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liguodongiot/llm-action/main/#star-history&#34;&gt;&lt;span&gt;⭐&lt;/span&gt; Star History&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM训练&lt;/h2&gt; &#xA;&lt;h3&gt;LLM训练实战&lt;/h3&gt; &#xA;&lt;p&gt;下面汇总了我在大模型实践中训练相关的所有教程。从6B到65B，从全量微调到高效微调（LoRA，QLoRA，P-Tuning v2），再到RLHF（基于人工反馈的强化学习）。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;LLM&lt;/th&gt; &#xA;   &lt;th&gt;预训练/SFT/RLHF...&lt;/th&gt; &#xA;   &lt;th&gt;参数&lt;/th&gt; &#xA;   &lt;th&gt;教程&lt;/th&gt; &#xA;   &lt;th&gt;代码&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;full fine-turning&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/618321077&#34;&gt;从0到1复现斯坦福羊驼（Stanford Alpaca 7B）&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/alpaca&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alpaca(LLaMA)&lt;/td&gt; &#xA;   &lt;td&gt;LoRA&lt;/td&gt; &#xA;   &lt;td&gt;7B~65B&lt;/td&gt; &#xA;   &lt;td&gt;1. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/619426866&#34;&gt;足够惊艳，使用Alpaca-Lora基于LLaMA(7B)二十分钟完成微调，效果比肩斯坦福羊驼&lt;/a&gt;&lt;br&gt;2. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/632492604&#34;&gt;使用 LoRA 技术对 LLaMA 65B 大模型进行微调及推理&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/alpaca-lora&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BELLE(LLaMA/Bloom)&lt;/td&gt; &#xA;   &lt;td&gt;full fine-turning&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;1. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/618876472&#34;&gt;基于LLaMA-7B/Bloomz-7B1-mt复现开源中文对话大模型BELLE及GPTQ量化&lt;/a&gt;&lt;br&gt;2. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/621128368&#34;&gt;BELLE(LLaMA-7B/Bloomz-7B1-mt)大模型使用GPTQ量化后推理性能测试&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM&lt;/td&gt; &#xA;   &lt;td&gt;LoRA&lt;/td&gt; &#xA;   &lt;td&gt;6B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/621793987&#34;&gt;从0到1基于ChatGLM-6B使用LoRA进行参数高效微调&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/chatglm-lora&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM&lt;/td&gt; &#xA;   &lt;td&gt;full fine-turning/P-Tuning v2&lt;/td&gt; &#xA;   &lt;td&gt;6B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/622351059&#34;&gt;使用DeepSpeed/P-Tuning v2对ChatGLM-6B进行微调&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/chatglm&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna(LLaMA)&lt;/td&gt; &#xA;   &lt;td&gt;full fine-turning&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/624012908&#34;&gt;大模型也内卷，Vicuna训练及推理指南，效果碾压斯坦福羊驼&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPT&lt;/td&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;0.1B~66B&lt;/td&gt; &#xA;   &lt;td&gt;1. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/626159553&#34;&gt;一键式 RLHF 训练 DeepSpeed Chat（一）：理论篇&lt;/a&gt;&amp;nbsp;&lt;br&gt;2. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/626214655&#34;&gt;一键式 RLHF 训练 DeepSpeed Chat（二）：实践篇&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/train/deepspeedchat&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MiniGPT-4(LLaMA)&lt;/td&gt; &#xA;   &lt;td&gt;full fine-turning&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/627671257&#34;&gt;大杀器，多模态大模型MiniGPT-4入坑指南&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-Alpaca(LLaMA)&lt;/td&gt; &#xA;   &lt;td&gt;LoRA（预训练+微调）&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/631360711&#34;&gt;中文LLaMA&amp;amp;Alpaca大语言模型词表扩充+预训练+指令精调&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/chinese-llama-alpaca&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA&lt;/td&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;7B/65B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/636644164&#34;&gt;高效微调技术QLoRA实战，基于LLaMA-65B微调仅需48G显存，真香&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/tree/main/qlora&#34;&gt;配套代码&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;LLM微调技术原理&lt;/h3&gt; &#xA;&lt;p&gt;对于普通大众来说，进行大模型的预训练或者全量微调遥不可及。由此，催生了各种参数高效微调技术，让科研人员或者普通开发者有机会尝试微调大模型。&lt;/p&gt; &#xA;&lt;p&gt;因此，该技术值得我们进行深入分析其背后的机理，本系列大体分七篇文章进行讲解。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/635152813&#34;&gt;大模型参数高效微调技术原理综述（一）-背景、参数高效微调简介&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/635686756&#34;&gt;大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/635848732&#34;&gt;大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/636038478&#34;&gt;大模型参数高效微调技术原理综述（四）-Adapter Tuning及其变体&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/636215898&#34;&gt;大模型参数高效微调技术原理综述（五）-LoRA、AdaLoRA、QLoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/636362246&#34;&gt;大模型参数高效微调技术原理综述（六）-MAM Adapter、UniPELT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/636999010&#34;&gt;大模型参数高效微调技术原理综述（七）-最佳实践、总结&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLM微调实战&lt;/h3&gt; &#xA;&lt;p&gt;下面给大家分享&lt;strong&gt;大模型参数高效微调技术实战&lt;/strong&gt;系列文章，该系列共6篇文章。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;大模型参数高效微调技术实战（一）-PEFT概述及环境搭建&lt;/li&gt; &#xA; &lt;li&gt;大模型参数高效微调技术实战（二）-Prompt Tuning&lt;/li&gt; &#xA; &lt;li&gt;大模型参数高效微调技术实战（三）-P-Tuning&lt;/li&gt; &#xA; &lt;li&gt;大模型参数高效微调技术实战（四）-Prefix Tuning&lt;/li&gt; &#xA; &lt;li&gt;大模型参数高效微调技术实战（五）-LoRA&lt;/li&gt; &#xA; &lt;li&gt;大模型参数高效微调技术实战（六）-IA3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLM分布式训练并行技术&lt;/h3&gt; &#xA;&lt;p&gt;近年来，随着Transformer、MOE架构的提出，使得深度学习模型轻松突破上万亿规模参数，传统的单机单卡模式已经无法满足超大模型进行训练的要求。因此，我们需要基于单机多卡、甚至是多机多卡进行分布式大模型的训练。&lt;/p&gt; &#xA;&lt;p&gt;而利用AI集群，使深度学习算法更好地从大量数据中高效地训练出性能优良的大模型是分布式机器学习的首要目标。为了实现该目标，一般需要根据硬件资源与数据/模型规模的匹配情况，考虑对计算任务、训练数据和模型进行划分，从而进行分布式训练。因此，分布式训练相关技术值得我们进行深入分析其背后的机理。&lt;/p&gt; &#xA;&lt;p&gt;下面主要对大模型进行分布式训练的并行技术进行讲解，本系列大体分八篇文章进行讲解。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/7195845066887053368&#34;&gt;大模型分布式训练并行技术（一）-概述&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/7254001262646738981&#34;&gt;大模型分布式训练并行技术（二）-数据并行&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;大模型分布式训练并行技术（三）-流水线并行&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;大模型分布式训练并行技术（四）-张量并行&lt;/li&gt; &#xA; &lt;li&gt;大模型分布式训练并行技术（五）-序列并行&lt;/li&gt; &#xA; &lt;li&gt;大模型分布式训练并行技术（六）-多维混合并行&lt;/li&gt; &#xA; &lt;li&gt;大模型分布式训练并行技术（七）-自动并行&lt;/li&gt; &#xA; &lt;li&gt;大模型分布式训练并行技术（八）-MOE并行&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;分布式AI框架&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pytorch &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Pytorch 单机多卡训练&lt;/li&gt; &#xA;   &lt;li&gt;Pytorch 多机多卡训练&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Megatron-LM &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Megatron-LM 单机多卡训练&lt;/li&gt; &#xA;   &lt;li&gt;Megatron-LM 多机多卡训练&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DeepSpeed &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;DeepSpeed 单机多卡训练&lt;/li&gt; &#xA;   &lt;li&gt;DeepSpeed 多机多卡训练&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM推理&lt;/h2&gt; &#xA;&lt;h3&gt;模型推理加速&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/626008090&#34;&gt;大模型的好伙伴，浅析推理加速引擎FasterTransformer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;模型推理服务化&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/629336492&#34;&gt;模型推理服务化框架Triton保姆式教程（一）：快速入门&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/634143650&#34;&gt;模型推理服务化框架Triton保姆式教程（二）：架构解析&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/634444666&#34;&gt;模型推理服务化框架Triton保姆式教程（三）：开发实践&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLM量化&lt;/h3&gt; &#xA;&lt;p&gt;待更新...&lt;/p&gt; &#xA;&lt;h2&gt;LLM国产化适配&lt;/h2&gt; &#xA;&lt;p&gt;随着 ChatGPT 的现象级走红，引领了AI大模型时代的变革，从而导致 AI 算力日益紧缺。与此同时，中美贸易战以及美国对华进行AI芯片相关的制裁导致 AI 算力的国产化适配势在必行。本系列将对一些国产化 AI 加速卡进行讲解。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/637918406&#34;&gt;大模型国产化适配1-华为昇腾AI全栈软硬件平台总结&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/7254446353367482423&#34;&gt;大模型国产化适配2-基于昇腾910使用ChatGLM-6B进行模型推理&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juejin.cn/post/7256769427600064569&#34;&gt;大模型国产化适配3-基于昇腾910使用ChatGLM-6B进行模型训练&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM生态相关技术&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/630696264&#34;&gt;大模型词表扩充必备工具SentencePiece&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/601594836/answer/3032763174&#34;&gt;大模型实践总结&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/606757218/answer/3075464500&#34;&gt;百川智能开源大模型baichuan-7B技术剖析&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/604393963/answer/3061358152&#34;&gt;ChatGLM 和 ChatGPT 的技术区别在哪里？&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/602504880/answer/3041965998&#34;&gt;现在为什么那么多人以清华大学的ChatGLM-6B为基座进行试验？&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;服务器基础环境软件安装&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/docs/llm-base/a800-env-install.md&#34;&gt;英伟达A800加速卡常见软件包安装命令&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/docs/llm-base/h800-env-install.md&#34;&gt;英伟达H800加速卡常见软件包安装命令&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liguodongiot/llm-action/raw/main/docs/llm_localization/ascend910-env-install.md&#34;&gt;昇腾910加速卡常见软件包安装命令&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM学习交流群&lt;/h2&gt; &#xA;&lt;p&gt;我创建了大模型学习交流群，供大家一起学习交流大模型相关的最新技术，目前已有4个群，每个群都有上百人的规模，&lt;strong&gt;可加我微信进群&lt;/strong&gt;（加微信请备注来意，如：进大模型学习交流群+GitHub）。&lt;strong&gt;一定要备注哟，否则不予通过&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/liguodongiot/llm-action/raw/main/pic/%E5%BE%AE%E4%BF%A1.jpeg&#34; width=&#34;25%&#34; height=&#34;25%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;微信公众号&lt;/h2&gt; &#xA;&lt;p&gt;微信公众号：&lt;strong&gt;吃果冻不吐果冻皮&lt;/strong&gt;，该公众号主要分享AI工程化（大模型、MLOps等）相关实践经验，免费电子书籍、论文等。&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/liguodongiot/llm-action/raw/main/pic/%E5%85%AC%E4%BC%97%E5%8F%B7.jpeg&#34; width=&#34;30%&#34; height=&#34;30%&#34; div align=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#liguodongiot/llm-action&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=liguodongiot/llm-action&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>py-why/EconML</title>
    <updated>2023-07-23T01:37:59Z</updated>
    <id>tag:github.com,2023-07-23:/py-why/EconML</id>
    <link href="https://github.com/py-why/EconML" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ALICE (Automated Learning and Intelligence for Causation and Economics) is a Microsoft Research project aimed at applying Artificial Intelligence concepts to economic decision making. One of its goals is to build a toolkit that combines state-of-the-art machine learning techniques with econometrics in order to bring automation to complex causal …&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/py-why/EconML/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/py-why/EconML/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/econml/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/econml.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/econml/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/wheel/econml.svg?sanitize=true&#34; alt=&#34;PyPI wheel&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/econml/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/econml.svg?sanitize=true&#34; alt=&#34;Supported Python versions&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt; &lt;a href=&#34;https://econml.azurewebsites.net/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/py-why/EconML/main/doc/econml-logo-icon.png&#34; width=&#34;80px&#34; align=&#34;left&#34; style=&#34;margin-right: 10px;&#34; , alt=&#34;econml-logo&#34;&gt; &lt;/a&gt; EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation &lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;EconML&lt;/strong&gt; is a Python package for estimating heterogeneous treatment effects from observational data via machine learning. This package was designed and built as part of the &lt;a href=&#34;https://www.microsoft.com/en-us/research/project/alice/&#34;&gt;ALICE project&lt;/a&gt; at Microsoft Research with the goal to combine state-of-the-art machine learning techniques with econometrics to bring automation to complex causal inference problems. The promise of EconML:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implement recent techniques in the literature at the intersection of econometrics and machine learning&lt;/li&gt; &#xA; &lt;li&gt;Maintain flexibility in modeling the effect heterogeneity (via techniques such as random forests, boosting, lasso and neural nets), while preserving the causal interpretation of the learned model and often offering valid confidence intervals&lt;/li&gt; &#xA; &lt;li&gt;Use a unified API&lt;/li&gt; &#xA; &lt;li&gt;Build on standard Python packages for Machine Learning and Data Analysis&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;One of the biggest promises of machine learning is to automate decision making in a multitude of domains. At the core of many data-driven personalized decision scenarios is the estimation of heterogeneous treatment effects: what is the causal effect of an intervention on an outcome of interest for a sample with a particular set of features? In a nutshell, this toolkit is designed to measure the causal effect of some treatment variable(s) &lt;code&gt;T&lt;/code&gt; on an outcome variable &lt;code&gt;Y&lt;/code&gt;, controlling for a set of features &lt;code&gt;X, W&lt;/code&gt; and how does that effect vary as a function of &lt;code&gt;X&lt;/code&gt;. The methods implemented are applicable even with observational (non-experimental or historical) datasets. For the estimation results to have a causal interpretation, some methods assume no unobserved confounders (i.e. there is no unobserved variable not included in &lt;code&gt;X, W&lt;/code&gt; that simultaneously has an effect on both &lt;code&gt;T&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt;), while others assume access to an instrument &lt;code&gt;Z&lt;/code&gt; (i.e. an observed variable &lt;code&gt;Z&lt;/code&gt; that has an effect on the treatment &lt;code&gt;T&lt;/code&gt; but no direct effect on the outcome &lt;code&gt;Y&lt;/code&gt;). Most methods provide confidence intervals and inference results.&lt;/p&gt; &#xA;&lt;p&gt;For detailed information about the package, consult the documentation at &lt;a href=&#34;https://econml.azurewebsites.net/&#34;&gt;https://econml.azurewebsites.net/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For information on use cases and background material on causal inference and heterogeneous treatment effects see our webpage at &lt;a href=&#34;https://www.microsoft.com/en-us/research/project/econml/&#34;&gt;https://www.microsoft.com/en-us/research/project/econml/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#news&#34;&gt;News&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#usage-examples&#34;&gt;Usage Examples&lt;/a&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#estimation-methods&#34;&gt;Estimation Methods&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#interpretability&#34;&gt;Interpretability&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#causal-model-selection-and-cross-validation&#34;&gt;Causal Model Selection and Cross-Validation&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#policy-learning&#34;&gt;Policy Learning&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#for-developers&#34;&gt;For Developers&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#running-the-tests&#34;&gt;Running the tests&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#generating-the-documentation&#34;&gt;Generating the documentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#blogs-and-publications&#34;&gt;Blogs and Publications&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#contributing-and-feedback&#34;&gt;Contributing and Feedback&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;May 19, 2023:&lt;/strong&gt; Release v0.14.1, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.14.1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Previous releases&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;November 16, 2022:&lt;/strong&gt; Release v0.14.0, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.14.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;June 17, 2022:&lt;/strong&gt; Release v0.13.1, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.13.1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;January 31, 2022:&lt;/strong&gt; Release v0.13.0, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.13.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;August 13, 2021:&lt;/strong&gt; Release v0.12.0, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.12.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;August 5, 2021:&lt;/strong&gt; Release v0.12.0b6, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.12.0b6&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;August 3, 2021:&lt;/strong&gt; Release v0.12.0b5, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.12.0b5&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;July 9, 2021:&lt;/strong&gt; Release v0.12.0b4, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.12.0b4&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;June 25, 2021:&lt;/strong&gt; Release v0.12.0b3, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.12.0b3&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;June 18, 2021:&lt;/strong&gt; Release v0.12.0b2, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.12.0b2&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;June 7, 2021:&lt;/strong&gt; Release v0.12.0b1, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.12.0b1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;May 18, 2021:&lt;/strong&gt; Release v0.11.1, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.11.1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;May 8, 2021:&lt;/strong&gt; Release v0.11.0, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.11.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 22, 2021:&lt;/strong&gt; Release v0.10.0, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.10.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 11, 2021:&lt;/strong&gt; Release v0.9.2, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.9.2&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 3, 2021:&lt;/strong&gt; Release v0.9.1, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.9.1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;February 20, 2021:&lt;/strong&gt; Release v0.9.0, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.9.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;January 20, 2021:&lt;/strong&gt; Release v0.9.0b1, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.9.0b1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;November 20, 2020:&lt;/strong&gt; Release v0.8.1, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.8.1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;November 18, 2020:&lt;/strong&gt; Release v0.8.0, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.8.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;September 4, 2020:&lt;/strong&gt; Release v0.8.0b1, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.8.0b1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 6, 2020:&lt;/strong&gt; Release v0.7.0, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.7.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;February 18, 2020:&lt;/strong&gt; Release v0.7.0b1, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.7.0b1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;January 10, 2020:&lt;/strong&gt; Release v0.6.1, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.6.1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;December 6, 2019:&lt;/strong&gt; Release v0.6, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.6&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;November 21, 2019:&lt;/strong&gt; Release v0.5, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.5&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;June 3, 2019:&lt;/strong&gt; Release v0.4, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.4&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;May 3, 2019:&lt;/strong&gt; Release v0.3, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.3&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;April 10, 2019:&lt;/strong&gt; Release v0.2, see release notes &lt;a href=&#34;https://github.com/py-why/EconML/releases/tag/v0.2&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;March 6, 2019:&lt;/strong&gt; Release v0.1, welcome to have a try and provide feedback.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install the latest release from &lt;a href=&#34;https://pypi.org/project/econml/&#34;&gt;PyPI&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install econml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install from source, see &lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#for-developers&#34;&gt;For Developers&lt;/a&gt; section below.&lt;/p&gt; &#xA;&lt;h2&gt;Usage Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Estimation Methods&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Double Machine Learning (aka RLearner) (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import LinearDML&#xA;from sklearn.linear_model import LassoCV&#xA;from econml.inference import BootstrapInference&#xA;&#xA;est = LinearDML(model_y=LassoCV(), model_t=LassoCV())&#xA;### Estimate with OLS confidence intervals&#xA;est.fit(Y, T, X=X, W=W) # W -&amp;gt; high-dimensional confounders, X -&amp;gt; features&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # OLS confidence intervals&#xA;&#xA;### Estimate with bootstrap confidence intervals&#xA;est.fit(Y, T, X=X, W=W, inference=&#39;bootstrap&#39;)  # with default bootstrap parameters&#xA;est.fit(Y, T, X=X, W=W, inference=BootstrapInference(n_bootstrap_samples=100))  # or customized&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # Bootstrap confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Sparse linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import SparseLinearDML&#xA;from sklearn.linear_model import LassoCV&#xA;&#xA;est = SparseLinearDML(model_y=LassoCV(), model_t=LassoCV())&#xA;est.fit(Y, T, X=X, W=W) # X -&amp;gt; high dimensional features&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # Confidence intervals via debiased lasso&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Generic Machine Learning last stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import NonParamDML&#xA;from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier&#xA;&#xA;est = NonParamDML(model_y=RandomForestRegressor(),&#xA;                  model_t=RandomForestClassifier(),&#xA;                  model_final=RandomForestRegressor(),&#xA;                  discrete_treatment=True)&#xA;est.fit(Y, T, X=X, W=W) &#xA;treatment_effects = est.effect(X_test)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Dynamic Double Machine Learning (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.panel.dml import DynamicDML&#xA;# Use defaults&#xA;est = DynamicDML()&#xA;# Or specify hyperparameters&#xA;est = DynamicDML(model_y=LassoCV(cv=3), &#xA;                 model_t=LassoCV(cv=3), &#xA;                 cv=3)&#xA;est.fit(Y, T, X=X, W=None, groups=groups, inference=&#34;auto&#34;)&#xA;# Effects&#xA;treatment_effects = est.effect(X_test)&#xA;# Confidence intervals&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Causal Forests (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import CausalForestDML&#xA;from sklearn.linear_model import LassoCV&#xA;# Use defaults&#xA;est = CausalForestDML()&#xA;# Or specify hyperparameters&#xA;est = CausalForestDML(criterion=&#39;het&#39;, n_estimators=500,       &#xA;                      min_samples_leaf=10, &#xA;                      max_depth=10, max_samples=0.5,&#xA;                      discrete_treatment=False,&#xA;                      model_t=LassoCV(), model_y=LassoCV())&#xA;est.fit(Y, T, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;# Confidence intervals via Bootstrap-of-Little-Bags for forests&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Orthogonal Random Forests (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.orf import DMLOrthoForest, DROrthoForest&#xA;from econml.sklearn_extensions.linear_model import WeightedLasso, WeightedLassoCV&#xA;# Use defaults&#xA;est = DMLOrthoForest()&#xA;est = DROrthoForest()&#xA;# Or specify hyperparameters&#xA;est = DMLOrthoForest(n_trees=500, min_leaf_size=10,&#xA;                     max_depth=10, subsample_ratio=0.7,&#xA;                     lambda_reg=0.01,&#xA;                     discrete_treatment=False,&#xA;                     model_T=WeightedLasso(alpha=0.01), model_Y=WeightedLasso(alpha=0.01),&#xA;                     model_T_final=WeightedLassoCV(cv=3), model_Y_final=WeightedLassoCV(cv=3))&#xA;est.fit(Y, T, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;# Confidence intervals via Bootstrap-of-Little-Bags for forests&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Meta-Learners (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;XLearner&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.metalearners import XLearner&#xA;from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor&#xA;&#xA;est = XLearner(models=GradientBoostingRegressor(),&#xA;              propensity_model=GradientBoostingClassifier(),&#xA;              cate_models=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=np.hstack([X, W]))&#xA;treatment_effects = est.effect(np.hstack([X_test, W_test]))&#xA;&#xA;# Fit with bootstrap confidence interval construction enabled&#xA;est.fit(Y, T, X=np.hstack([X, W]), inference=&#39;bootstrap&#39;)&#xA;treatment_effects = est.effect(np.hstack([X_test, W_test]))&#xA;lb, ub = est.effect_interval(np.hstack([X_test, W_test]), alpha=0.05) # Bootstrap CIs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;SLearner&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.metalearners import SLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor&#xA;&#xA;est = SLearner(overall_model=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=np.hstack([X, W]))&#xA;treatment_effects = est.effect(np.hstack([X_test, W_test]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;TLearner&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.metalearners import TLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor&#xA;&#xA;est = TLearner(models=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=np.hstack([X, W]))&#xA;treatment_effects = est.effect(np.hstack([X_test, W_test]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Doubly Robust Learners (click to expand) &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dr import LinearDRLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier&#xA;&#xA;est = LinearDRLearner(model_propensity=GradientBoostingClassifier(),&#xA;                      model_regression=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Sparse linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dr import SparseLinearDRLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier&#xA;&#xA;est = SparseLinearDRLearner(model_propensity=GradientBoostingClassifier(),&#xA;                            model_regression=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Nonparametric final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dr import ForestDRLearner&#xA;from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier&#xA;&#xA;est = ForestDRLearner(model_propensity=GradientBoostingClassifier(),&#xA;                      model_regression=GradientBoostingRegressor())&#xA;est.fit(Y, T, X=X, W=W) &#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Double Machine Learning with Instrumental Variables (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Orthogonal instrumental variable learner&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dml import OrthoIV&#xA;&#xA;est = OrthoIV(projection=False, &#xA;              discrete_treatment=True, &#xA;              discrete_instrument=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # OLS confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Nonparametric double machine learning with instrumental variable&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dml import NonParamDMLIV&#xA;&#xA;est = NonParamDMLIV(projection=False, &#xA;                    discrete_treatment=True, &#xA;                    discrete_instrument=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W) # no analytical confidence interval available&#xA;treatment_effects = est.effect(X_test)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Doubly Robust Machine Learning with Instrumental Variables (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dr import LinearDRIV&#xA;&#xA;est = LinearDRIV(discrete_instrument=True, discrete_treatment=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # OLS confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Sparse linear final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dr import SparseLinearDRIV&#xA;&#xA;est = SparseLinearDRIV(discrete_instrument=True, discrete_treatment=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # Debiased lasso confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Nonparametric final stage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dr import ForestDRIV&#xA;&#xA;est = ForestDRIV(discrete_instrument=True, discrete_treatment=True)&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;# Confidence intervals via Bootstrap-of-Little-Bags for forests&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Linear intent-to-treat (discrete instrument, discrete treatment)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.iv.dr import LinearIntentToTreatDRIV&#xA;from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier&#xA;&#xA;est = LinearIntentToTreatDRIV(model_y_xw=GradientBoostingRegressor(),&#xA;                              model_t_xwz=GradientBoostingClassifier(),&#xA;                              flexible_model_effect=GradientBoostingRegressor())&#xA;est.fit(Y, T, Z=Z, X=X, W=W)&#xA;treatment_effects = est.effect(X_test)&#xA;lb, ub = est.effect_interval(X_test, alpha=0.05) # OLS confidence intervals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Deep Instrumental Variables (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import keras&#xA;from econml.iv.nnet import DeepIV&#xA;&#xA;treatment_model = keras.Sequential([keras.layers.Dense(128, activation=&#39;relu&#39;, input_shape=(2,)),&#xA;                                    keras.layers.Dropout(0.17),&#xA;                                    keras.layers.Dense(64, activation=&#39;relu&#39;),&#xA;                                    keras.layers.Dropout(0.17),&#xA;                                    keras.layers.Dense(32, activation=&#39;relu&#39;),&#xA;                                    keras.layers.Dropout(0.17)])&#xA;response_model = keras.Sequential([keras.layers.Dense(128, activation=&#39;relu&#39;, input_shape=(2,)),&#xA;                                  keras.layers.Dropout(0.17),&#xA;                                  keras.layers.Dense(64, activation=&#39;relu&#39;),&#xA;                                  keras.layers.Dropout(0.17),&#xA;                                  keras.layers.Dense(32, activation=&#39;relu&#39;),&#xA;                                  keras.layers.Dropout(0.17),&#xA;                                  keras.layers.Dense(1)])&#xA;est = DeepIV(n_components=10, # Number of gaussians in the mixture density networks)&#xA;             m=lambda z, x: treatment_model(keras.layers.concatenate([z, x])), # Treatment model&#xA;             h=lambda t, x: response_model(keras.layers.concatenate([t, x])), # Response model&#xA;             n_samples=1 # Number of samples used to estimate the response&#xA;             )&#xA;est.fit(Y, T, X=X, Z=Z) # Z -&amp;gt; instrumental variables&#xA;treatment_effects = est.effect(X_test)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/py-why/EconML/main/#references&#34;&gt;References&lt;/a&gt; section for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Interpretability&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tree Interpreter of the CATE model (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.cate_interpreter import SingleTreeCateInterpreter&#xA;intrp = SingleTreeCateInterpreter(include_model_uncertainty=True, max_depth=2, min_samples_leaf=10)&#xA;# We interpret the CATE model&#39;s behavior based on the features used for heterogeneity&#xA;intrp.interpret(est, X)&#xA;# Plot the tree&#xA;plt.figure(figsize=(25, 5))&#xA;intrp.plot(feature_names=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;], fontsize=12)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/py-why/EconML/main/notebooks/images/dr_cate_tree.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Policy Interpreter of the CATE model (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.cate_interpreter import SingleTreePolicyInterpreter&#xA;# We find a tree-based treatment policy based on the CATE model&#xA;intrp = SingleTreePolicyInterpreter(risk_level=0.05, max_depth=2, min_samples_leaf=1,min_impurity_decrease=.001)&#xA;intrp.interpret(est, X, sample_treatment_costs=0.2)&#xA;# Plot the tree&#xA;plt.figure(figsize=(25, 5))&#xA;intrp.plot(feature_names=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;], fontsize=12)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/py-why/EconML/main/notebooks/images/dr_policy_tree.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;SHAP values for the CATE model (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import shap&#xA;from econml.dml import CausalForestDML&#xA;est = CausalForestDML()&#xA;est.fit(Y, T, X=X, W=W)&#xA;shap_values = est.shap_values(X)&#xA;shap.summary_plot(shap_values[&#39;Y0&#39;][&#39;T0&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Causal Model Selection and Cross-Validation&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Causal model selection with the `RScorer` (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.score import RScorer&#xA;&#xA;# split data in train-validation&#xA;X_train, X_val, T_train, T_val, Y_train, Y_val = train_test_split(X, T, y, test_size=.4)&#xA;&#xA;# define list of CATE estimators to select among&#xA;reg = lambda: RandomForestRegressor(min_samples_leaf=20)&#xA;clf = lambda: RandomForestClassifier(min_samples_leaf=20)&#xA;models = [(&#39;ldml&#39;, LinearDML(model_y=reg(), model_t=clf(), discrete_treatment=True,&#xA;                             linear_first_stages=False, cv=3)),&#xA;          (&#39;xlearner&#39;, XLearner(models=reg(), cate_models=reg(), propensity_model=clf())),&#xA;          (&#39;dalearner&#39;, DomainAdaptationLearner(models=reg(), final_models=reg(), propensity_model=clf())),&#xA;          (&#39;slearner&#39;, SLearner(overall_model=reg())),&#xA;          (&#39;drlearner&#39;, DRLearner(model_propensity=clf(), model_regression=reg(),&#xA;                                  model_final=reg(), cv=3)),&#xA;          (&#39;rlearner&#39;, NonParamDML(model_y=reg(), model_t=clf(), model_final=reg(),&#xA;                                   discrete_treatment=True, cv=3)),&#xA;          (&#39;dml3dlasso&#39;, DML(model_y=reg(), model_t=clf(),&#xA;                             model_final=LassoCV(cv=3, fit_intercept=False),&#xA;                             discrete_treatment=True,&#xA;                             featurizer=PolynomialFeatures(degree=3),&#xA;                             linear_first_stages=False, cv=3))&#xA;]&#xA;&#xA;# fit cate models on train data&#xA;models = [(name, mdl.fit(Y_train, T_train, X=X_train)) for name, mdl in models]&#xA;&#xA;# score cate models on validation data&#xA;scorer = RScorer(model_y=reg(), model_t=clf(),&#xA;                 discrete_treatment=True, cv=3, mc_iters=2, mc_agg=&#39;median&#39;)&#xA;scorer.fit(Y_val, T_val, X=X_val)&#xA;rscore = [scorer.score(mdl) for _, mdl in models]&#xA;# select the best model&#xA;mdl, _ = scorer.best_model([mdl for _, mdl in models])&#xA;# create weighted ensemble model based on score performance&#xA;mdl, _ = scorer.ensemble([mdl for _, mdl in models])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;First Stage Model Selection (click to expand)&lt;/summary&gt; &#xA; &lt;p&gt;First stage models can be selected either by passing in cross-validated models (e.g. &lt;code&gt;sklearn.linear_model.LassoCV&lt;/code&gt;) to EconML&#39;s estimators or perform the first stage model selection outside of EconML and pass in the selected model. Unless selecting among a large set of hyperparameters, choosing first stage models externally is the preferred method due to statistical and computational advantages.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import LinearDML&#xA;from sklearn import clone&#xA;from sklearn.ensemble import RandomForestRegressor&#xA;from sklearn.model_selection import GridSearchCV&#xA;&#xA;cv_model = GridSearchCV(&#xA;              estimator=RandomForestRegressor(),&#xA;              param_grid={&#xA;                  &#34;max_depth&#34;: [3, None],&#xA;                  &#34;n_estimators&#34;: (10, 30, 50, 100, 200),&#xA;                  &#34;max_features&#34;: (2, 4, 6),&#xA;              },&#xA;              cv=5,&#xA;           )&#xA;# First stage model selection within EconML&#xA;# This is more direct, but computationally and statistically less efficient&#xA;est = LinearDML(model_y=cv_model, model_t=cv_model)&#xA;# First stage model selection ouside of EconML&#xA;# This is the most efficient, but requires boilerplate code&#xA;model_t = clone(cv_model).fit(W, T).best_estimator_&#xA;model_y = clone(cv_model).fit(W, Y).best_estimator_&#xA;est = LinearDML(model_y=model_t, model_t=model_y)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;Whenever inference is enabled, then one can get a more structure &lt;code&gt;InferenceResults&lt;/code&gt; object with more elaborate inference information, such as p-values and z-statistics. When the CATE model is linear and parametric, then a &lt;code&gt;summary()&lt;/code&gt; method is also enabled. For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.dml import LinearDML&#xA;# Use defaults&#xA;est = LinearDML()&#xA;est.fit(Y, T, X=X, W=W)&#xA;# Get the effect inference summary, which includes the standard error, z test score, p value, and confidence interval given each sample X[i]&#xA;est.effect_inference(X_test).summary_frame(alpha=0.05, value=0, decimals=3)&#xA;# Get the population summary for the entire sample X&#xA;est.effect_inference(X_test).population_summary(alpha=0.1, value=0, decimals=3, tol=0.001)&#xA;#  Get the parameter inference summary for the final model&#xA;est.summary()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Example Output (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;# Get the effect inference summary, which includes the standard error, z test score, p value, and confidence interval given each sample X[i]&#xA;est.effect_inference(X_test).summary_frame(alpha=0.05, value=0, decimals=3)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/py-why/EconML/main/notebooks/images/summary_frame.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;# Get the population summary for the entire sample X&#xA;est.effect_inference(X_test).population_summary(alpha=0.1, value=0, decimals=3, tol=0.001)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/py-why/EconML/main/notebooks/images/population_summary.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;#  Get the parameter inference summary for the final model&#xA;est.summary()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/py-why/EconML/main/notebooks/images/summary.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Policy Learning&lt;/h3&gt; &#xA;&lt;p&gt;You can also perform direct policy learning from observational data, using the doubly robust method for offline policy learning. These methods directly predict a recommended treatment, without internally fitting an explicit model of the conditional average treatment effect.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Doubly Robust Policy Learning (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from econml.policy import DRPolicyTree, DRPolicyForest&#xA;from sklearn.ensemble import RandomForestRegressor&#xA;&#xA;# fit a single binary decision tree policy&#xA;policy = DRPolicyTree(max_depth=1, min_impurity_decrease=0.01, honest=True)&#xA;policy.fit(y, T, X=X, W=W)&#xA;# predict the recommended treatment&#xA;recommended_T = policy.predict(X)&#xA;# plot the binary decision tree&#xA;plt.figure(figsize=(10,5))&#xA;policy.plot()&#xA;# get feature importances&#xA;importances = policy.feature_importances_&#xA;&#xA;# fit a binary decision forest&#xA;policy = DRPolicyForest(max_depth=1, min_impurity_decrease=0.01, honest=True)&#xA;policy.fit(y, T, X=X, W=W)&#xA;# predict the recommended treatment&#xA;recommended_T = policy.predict(X)&#xA;# plot the first tree in the ensemble&#xA;plt.figure(figsize=(10,5))&#xA;policy.plot(0)&#xA;# get feature importances&#xA;importances = policy.feature_importances_&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/py-why/EconML/main/images/policy_tree.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;To see more complex examples, go to the &lt;a href=&#34;https://github.com/py-why/EconML/tree/main/notebooks&#34;&gt;notebooks&lt;/a&gt; section of the repository. For a more detailed description of the treatment effect estimation algorithms, see the EconML &lt;a href=&#34;https://econml.azurewebsites.net/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;For Developers&lt;/h1&gt; &#xA;&lt;p&gt;You can get started by cloning this repository. We use &lt;a href=&#34;https://setuptools.readthedocs.io/en/latest/index.html&#34;&gt;setuptools&lt;/a&gt; for building and distributing our package. We rely on some recent features of setuptools, so make sure to upgrade to a recent version with &lt;code&gt;pip install setuptools --upgrade&lt;/code&gt;. Then from your local copy of the repository you can run &lt;code&gt;pip install -e .&lt;/code&gt; to get started (but depending on what you&#39;re doing you might want to install with extras instead, like &lt;code&gt;pip install -e .[plt]&lt;/code&gt; if you want to use matplotlib integration, or you can use &lt;code&gt;pip install -e .[all]&lt;/code&gt; to include all extras).&lt;/p&gt; &#xA;&lt;h2&gt;Running the tests&lt;/h2&gt; &#xA;&lt;p&gt;This project uses &lt;a href=&#34;https://docs.pytest.org/&#34;&gt;pytest&lt;/a&gt; for testing. To run tests locally after installing the package, you can use &lt;code&gt;pip install pytest-runner&lt;/code&gt; followed by &lt;code&gt;python setup.py pytest&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We have added pytest marks to some tests to make it easier to run a subset, and you can set the PYTEST_ADDOPTS environment variable to take advantage of this. For instance, you can set it to &lt;code&gt;-m &#34;not (notebook or automl)&#34;&lt;/code&gt; to skip notebook and automl tests that have some additional dependencies.&lt;/p&gt; &#xA;&lt;h2&gt;Generating the documentation&lt;/h2&gt; &#xA;&lt;p&gt;This project&#39;s documentation is generated via &lt;a href=&#34;https://www.sphinx-doc.org/en/main/index.html&#34;&gt;Sphinx&lt;/a&gt;. Note that we use &lt;a href=&#34;https://graphviz.org/&#34;&gt;graphviz&lt;/a&gt;&#39;s &lt;code&gt;dot&lt;/code&gt; application to produce some of the images in our documentation, so you should make sure that &lt;code&gt;dot&lt;/code&gt; is installed and in your path.&lt;/p&gt; &#xA;&lt;p&gt;To generate a local copy of the documentation from a clone of this repository, just run &lt;code&gt;python setup.py build_sphinx -W -E -a&lt;/code&gt;, which will build the documentation and place it under the &lt;code&gt;build/sphinx/html&lt;/code&gt; path.&lt;/p&gt; &#xA;&lt;p&gt;The reStructuredText files that make up the documentation are stored in the &lt;a href=&#34;https://github.com/py-why/EconML/tree/main/doc&#34;&gt;docs directory&lt;/a&gt;; module documentation is automatically generated by the Sphinx build process.&lt;/p&gt; &#xA;&lt;h2&gt;Release process&lt;/h2&gt; &#xA;&lt;p&gt;We use GitHub Actions to build and publish the package and documentation. To create a new release, an admin should perform the following steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Update the version number in &lt;code&gt;econml/_version.py&lt;/code&gt; and add a mention of the new version in the news section of this file and commit the changes.&lt;/li&gt; &#xA; &lt;li&gt;Manually run the publish_package.yml workflow to build and publish the package to PyPI.&lt;/li&gt; &#xA; &lt;li&gt;Manually run the publish_docs.yml workflow to build and publish the documentation.&lt;/li&gt; &#xA; &lt;li&gt;Under &lt;a href=&#34;https://github.com/py-why/EconML/releases&#34;&gt;https://github.com/py-why/EconML/releases&lt;/a&gt;, create a new release with a corresponding tag, and update the release notes.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Blogs and Publications&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;June 2019: &lt;a href=&#34;https://arxiv.org/pdf/1905.10176.pdf&#34;&gt;Treatment Effects with Instruments paper&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;May 2019: &lt;a href=&#34;https://odsc.com/speakers/machine-learning-estimation-of-heterogeneous-treatment-effect-the-microsoft-econml-library/&#34;&gt;Open Data Science Conference Workshop&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2018: &lt;a href=&#34;http://proceedings.mlr.press/v97/oprescu19a.html&#34;&gt;Orthogonal Random Forests paper&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2017: &lt;a href=&#34;http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf&#34;&gt;DeepIV paper&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use EconML in your research, please cite us as follows:&lt;/p&gt; &#xA;&lt;p&gt;Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Paul Oka, Miruna Oprescu, Vasilis Syrgkanis. &lt;strong&gt;EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation.&lt;/strong&gt; &lt;a href=&#34;https://github.com/py-why/EconML&#34;&gt;https://github.com/py-why/EconML&lt;/a&gt;, 2019. Version 0.x.&lt;/p&gt; &#xA;&lt;p&gt;BibTex:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{econml,&#xA;  author={Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Paul Oka, Miruna Oprescu, Vasilis Syrgkanis},&#xA;  title={{EconML}: {A Python Package for ML-Based Heterogeneous Treatment Effects Estimation}},&#xA;  howpublished={https://github.com/py-why/EconML},&#xA;  note={Version 0.x},&#xA;  year={2019}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Contributing and Feedback&lt;/h1&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. We use the &lt;a href=&#34;https://github.com/apps/dco&#34;&gt;DCO bot&lt;/a&gt; to enforce a &lt;a href=&#34;https://developercertificate.org/&#34;&gt;Developer Certificate of Origin&lt;/a&gt; which requires users to sign-off on their commits. This is a simple way to certify that you wrote or otherwise have the right to submit the code you are contributing to the project. Git provides a &lt;code&gt;-s&lt;/code&gt; command line option to include this automatically when you commit via &lt;code&gt;git commit&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://github.com/py-why/governance/raw/main/CODE-OF-CONDUCT.md&#34;&gt;PyWhy Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Community&lt;/h1&gt; &#xA;&lt;a href=&#34;https://pywhy.org/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/py-why/EconML/main/doc/spec/img/pywhy-logo.png&#34; width=&#34;80px&#34; align=&#34;left&#34; style=&#34;margin-right: 10px;&#34; , alt=&#34;pywhy-logo&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;EconML is a part of &lt;a href=&#34;https://www.pywhy.org/&#34;&gt;PyWhy&lt;/a&gt;, an organization with a mission to build an open-source ecosystem for causal machine learning.&lt;/p&gt; &#xA;&lt;p&gt;PyWhy also has a &lt;a href=&#34;https://discord.gg/cSBGb3vsZb&#34;&gt;Discord&lt;/a&gt;, which serves as a space for like-minded casual machine learning researchers and practitioners of all experience levels to come together to ask and answer questions, discuss new features, and share ideas.&lt;/p&gt; &#xA;&lt;p&gt;We invite you to join us at regular office hours and community calls in the Discord.&lt;/p&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;p&gt;Athey, Susan, and Stefan Wager. &lt;strong&gt;Policy learning with observational data.&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.3982/ECTA15732&#34;&gt;&lt;em&gt;Econometrica 89.1, 133-161&lt;/em&gt;&lt;/a&gt;, 2021.&lt;/p&gt; &#xA;&lt;p&gt;X Nie, S Wager. &lt;strong&gt;Quasi-Oracle Estimation of Heterogeneous Treatment Effects.&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1093/biomet/asaa076&#34;&gt;&lt;em&gt;Biometrika 108.2, 299-319&lt;/em&gt;&lt;/a&gt;, 2021.&lt;/p&gt; &#xA;&lt;p&gt;V. Syrgkanis, V. Lei, M. Oprescu, M. Hei, K. Battocchi, G. Lewis. &lt;strong&gt;Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments.&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/1905.10176&#34;&gt;&lt;em&gt;Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS)&lt;/em&gt;&lt;/a&gt;, 2019. &lt;strong&gt;(Spotlight Presentation)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;D. Foster, V. Syrgkanis. &lt;strong&gt;Orthogonal Statistical Learning.&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/1901.09036.pdf&#34;&gt;&lt;em&gt;Proceedings of the 32nd Annual Conference on Learning Theory (COLT)&lt;/em&gt;&lt;/a&gt;, 2019. &lt;strong&gt;(Best Paper Award)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;M. Oprescu, V. Syrgkanis and Z. S. Wu. &lt;strong&gt;Orthogonal Random Forest for Causal Inference.&lt;/strong&gt; &lt;a href=&#34;http://proceedings.mlr.press/v97/oprescu19a.html&#34;&gt;&lt;em&gt;Proceedings of the 36th International Conference on Machine Learning (ICML)&lt;/em&gt;&lt;/a&gt;, 2019.&lt;/p&gt; &#xA;&lt;p&gt;S. Künzel, J. Sekhon, J. Bickel and B. Yu. &lt;strong&gt;Metalearners for estimating heterogeneous treatment effects using machine learning.&lt;/strong&gt; &lt;a href=&#34;https://www.pnas.org/content/116/10/4156&#34;&gt;&lt;em&gt;Proceedings of the national academy of sciences, 116(10), 4156-4165&lt;/em&gt;&lt;/a&gt;, 2019.&lt;/p&gt; &#xA;&lt;p&gt;S. Athey, J. Tibshirani, S. Wager. &lt;strong&gt;Generalized random forests.&lt;/strong&gt; &lt;a href=&#34;https://projecteuclid.org/euclid.aos/1547197251&#34;&gt;&lt;em&gt;Annals of Statistics, 47, no. 2, 1148--1178&lt;/em&gt;&lt;/a&gt;, 2019.&lt;/p&gt; &#xA;&lt;p&gt;V. Chernozhukov, D. Nekipelov, V. Semenova, V. Syrgkanis. &lt;strong&gt;Plug-in Regularized Estimation of High-Dimensional Parameters in Nonlinear Semiparametric Models.&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/1806.04823&#34;&gt;&lt;em&gt;Arxiv preprint arxiv:1806.04823&lt;/em&gt;&lt;/a&gt;, 2018.&lt;/p&gt; &#xA;&lt;p&gt;S. Wager, S. Athey. &lt;strong&gt;Estimation and Inference of Heterogeneous Treatment Effects using Random Forests.&lt;/strong&gt; &lt;a href=&#34;https://www.tandfonline.com/doi/citedby/10.1080/01621459.2017.1319839&#34;&gt;&lt;em&gt;Journal of the American Statistical Association, 113:523, 1228-1242&lt;/em&gt;&lt;/a&gt;, 2018.&lt;/p&gt; &#xA;&lt;p&gt;Jason Hartford, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. &lt;strong&gt;Deep IV: A flexible approach for counterfactual prediction.&lt;/strong&gt; &lt;a href=&#34;http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf&#34;&gt;&lt;em&gt;Proceedings of the 34th International Conference on Machine Learning, ICML&#39;17&lt;/em&gt;&lt;/a&gt;, 2017.&lt;/p&gt; &#xA;&lt;p&gt;V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, and a. W. Newey. &lt;strong&gt;Double Machine Learning for Treatment and Causal Parameters.&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/1608.00060&#34;&gt;&lt;em&gt;ArXiv preprint arXiv:1608.00060&lt;/em&gt;&lt;/a&gt;, 2016.&lt;/p&gt; &#xA;&lt;p&gt;Dudik, M., Erhan, D., Langford, J., &amp;amp; Li, L. &lt;strong&gt;Doubly robust policy evaluation and optimization.&lt;/strong&gt; &lt;a href=&#34;https://projecteuclid.org/journals/statistical-science/volume-29/issue-4/Doubly-Robust-Policy-Evaluation-and-Optimization/10.1214/14-STS500.full&#34;&gt;&lt;em&gt;Statistical Science, 29(4), 485-511&lt;/em&gt;&lt;/a&gt;, 2014.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>py-why/causaltune</title>
    <updated>2023-07-23T01:37:59Z</updated>
    <id>tag:github.com,2023-07-23:/py-why/causaltune</id>
    <link href="https://github.com/py-why/causaltune" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AutoML for causal inference.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CausalTune: A library for automated Causal Inference model estimation and selection&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;CausalTune&lt;/strong&gt; is a library for automated tuning and selection for causal estimators.&lt;/p&gt; &#xA;&lt;p&gt;Its estimators are taken from &lt;a href=&#34;https://github.com/microsoft/EconML/&#34;&gt;EconML&lt;/a&gt; augmented by a couple of extra models (currently Transformed Outcome and a dummy model to be used as a baseline), all called in a uniform fashion via a &lt;a href=&#34;https://github.com/microsoft/DoWhy/&#34;&gt;DoWhy&lt;/a&gt; wrapper.&lt;/p&gt; &#xA;&lt;p&gt;Our contribution is enabling automatic estimator tuning and selection by out-of-sample scoring of causal estimators, notably using the &lt;a href=&#34;https://arxiv.org/abs/2212.10076&#34;&gt;energy score&lt;/a&gt;. We use &lt;a href=&#34;https://github.com/microsoft/FLAML&#34;&gt;FLAML&lt;/a&gt; for hyperparameter optimisation.&lt;/p&gt; &#xA;&lt;p&gt;We perform automated hyperparameter tuning of first stage models (for the treatment and outcome models) as well as hyperparameter tuning and model selection for the second stage model (causal estimator).&lt;/p&gt; &#xA;&lt;p&gt;The estimators provide not only per-row treatment impact estimates, but also confidence intervals for these, using builtin EconML functionality for that where it is available and bootstrapping where it is not (see &lt;a href=&#34;https://github.com/py-why/causaltune/raw/main/notebooks/Standard%20errors.ipynb&#34;&gt;example notebook&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Just like DoWhy and EconML, we assume that the causal graph provided by the user accurately describes the data-generating process. So for example, we assume that for CATE estimation, the list of backdoor variables under the graph/confounding variables provided by the user do reflect all sources of confounding between the treatment and the outcome. See &lt;a href=&#34;https://github.com/py-why/causaltune/raw/main/notebooks/CausalityDataset%20setup.ipynb&#34;&gt;here&lt;/a&gt; for a detailed explanation of causal graphs that are supported by CausalTune.&lt;/p&gt; &#xA;&lt;p&gt;The validation methods in CausalTune cannot catch such violations and therefore this is an important assumption.&lt;/p&gt; &#xA;&lt;p&gt;We also implement the &lt;a href=&#34;https://medium.com/building-ibotta/erupt-expected-response-under-proposed-treatments-ff7dd45c84b4&#34;&gt;ERUPT&lt;/a&gt; &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3111957&#34;&gt;calculation&lt;/a&gt; (also known as policy value), allowing after an (even partially) randomized test to estimate what the impact of other treatment assignment policies would have been. This can also be used as an alternative out-of-sample score, though energy score performed better in our synthetic data experiments.&lt;/p&gt; &#xA;&lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#causaltune-a-library-for-automated-causal-inference-model-estimation-and-selection&#34;&gt;CausalTune: A library for automated Causal Inference model estimation and selection&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#what-can-this-do-for-you&#34;&gt;What can this do for you?&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#1-supercharge-ab-tests-by-getting-impact-by-customer-instead-of-just-an-average&#34;&gt;1. Supercharge A/B tests by getting impact by customer, instead of just an average&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#2-continuous-testing-combined-with-exploitation-dynamic-uplift-modelling&#34;&gt;2. Continuous testing combined with exploitation: (Dynamic) uplift modelling&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#3-estimate-the-benefit-of-smarter-but-still-partially-random-assignment-compared-to-fully-random-without-the-need-for-an-actual-fully-random-test-group&#34;&gt;3. Estimate the benefit of smarter (but still partially random) assignment compared to fully random without the need for an actual fully random test group&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#4-observational-inference&#34;&gt;4. Observational inference&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#5-iv-models-impact-of-customer-choosing-to-use-a-feature&#34;&gt;5. IV models: Impact of customer choosing to use a feature&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#supported-models&#34;&gt;Supported Models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#supported-metrics&#34;&gt;Supported Metrics&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#for-developers&#34;&gt;For Developers&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#installation-from-source&#34;&gt;Installation from source&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#testing&#34;&gt;Testing&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#contribution&#34;&gt;Contribution&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What can this do for you?&lt;/h2&gt; &#xA;&lt;p&gt;The automated search over the many powerful models from EconML and elsewhere allows you to easily do the following&lt;/p&gt; &#xA;&lt;h3&gt;1. Supercharge A/B tests by getting impact by customer, instead of just an average&lt;/h3&gt; &#xA;&lt;p&gt;By enriching the results of a regular A/B/N test with customer features, and running CausalTune on the resulting dataset, you can get impact estimates as a function of customer features, allowing precise targeting by impact in the next iteration. CausalTune also serves as a variance reduction method leveraging the availability of any additional features. &lt;a href=&#34;https://github.com/py-why/causaltune/raw/main/notebooks/AB%20testing.ipynb&#34;&gt;Example notebook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;2. Continuous testing combined with exploitation: (Dynamic) uplift modelling&lt;/h3&gt; &#xA;&lt;p&gt;The per-customer impact estimates, even if noisy, can be used to implement per-customer Thompson sampling for new customers, biasing random treatment assignment towards ones we think are most likely to work. As we still control the per-customer propensity to treat, same methods as above can be applied to keep refining our impact estimates.&lt;/p&gt; &#xA;&lt;p&gt;Thus, there is no need to either wait for the test to gather enough data for significance, nor to ever end the test, before using its results to assign the most impactful treatment (based on our knowledge so far) to each customer.&lt;/p&gt; &#xA;&lt;p&gt;As in this case the propensity to treat is known for each customer, we &lt;a href=&#34;https://github.com/py-why/causaltune/raw/main/notebooks/Propensity%20Model%20Selection.ipynb&#34;&gt;allow to explicitly supply it&lt;/a&gt; as a column to the estimators, instead of estimating it from the data like in other cases.&lt;/p&gt; &#xA;&lt;h3&gt;3. Estimate the benefit of smarter (but still partially random) assignment compared to fully random without the need for an actual fully random test group&lt;/h3&gt; &#xA;&lt;p&gt;The previous section described using causal estimators to bias treatment assignment towards the choice we think is most likely to work best for a given customer.&lt;/p&gt; &#xA;&lt;p&gt;However, after the fact we would like to know the extra benefit of that compared to a fully random assignment. The ERUPT technique &lt;a href=&#34;https://github.com/py-why/causaltune/raw/main/notebooks/ERUPT%20under%20simulated%20random%20assignment.ipynb&#34;&gt;sample notebook&lt;/a&gt; re-weights the actual outcomes to produce an unbiased estimate of the average outcome that a fully random assignment would have yielded, with no actual additional group needed.&lt;/p&gt; &#xA;&lt;h3&gt;4. Observational inference&lt;/h3&gt; &#xA;&lt;p&gt;The traditional application of causal inference. For example, estimating the impact on volumes and churn likelihood of the time it takes us to answer a customer query. As the set of customers who have support queries is most likely not randomly sampled, confounding corrections are needed.&lt;/p&gt; &#xA;&lt;p&gt;As with other usecases, the advanced causal inference models allow impact estimation as a function of customer features, rather than just averages, &lt;strong&gt;under the assumption that all relevant confounders are observed&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To use this, just set &lt;code&gt;propensity_model&lt;/code&gt; to an instance of the desired classifier when instantiating &lt;code&gt;CausalTune&lt;/code&gt;, or to &lt;code&gt;&#34;auto&#34;&lt;/code&gt; if you want to use the FLAML classifier (the default setting is &lt;code&gt;&#34;dummy&#34;&lt;/code&gt; which assumes random assigment and infers the assignment probability from the data). &lt;a href=&#34;https://github.com/py-why/causaltune/raw/main/notebooks/Propensity%20Model%20Selection.ipynb&#34;&gt;Example notebook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you have reason to suppose unobserved confounders, such as customer intent (did the customer do a lot of volume because of the promotion, or did they sign up for the promotion because they intended to do lots of volume anyway?) consider looking for an instrumental variable instead.&lt;/p&gt; &#xA;&lt;!--&#xA;Can we reformulate to (did the customer do a lot of volume because of the promotion or is the customer more likely&#xA;to sign up for the promotion because they have a higher volume per se?) &#xA;--&gt; &#xA;&lt;p&gt;Note that our derivation of energy score as a valid out-of-sample score for causal models is strictly speaking not applicable for this usecase, but still appears to work reasonably well in practice.&lt;/p&gt; &#xA;&lt;h3&gt;5. IV models: Impact of customer choosing to use a feature&lt;/h3&gt; &#xA;&lt;p&gt;Instrumental variable (IV) estimation to avoid an estimation bias from unobserved confounders.&lt;/p&gt; &#xA;&lt;p&gt;A natural use case for IV models is making a feature or a promotion available to a customer, and trying to measure the impact of the customer actually choosing to use the feature (the impact of making the feature available can be solved with 1. and 2. above).&lt;/p&gt; &#xA;&lt;p&gt;Here we use feature availability as an instrumental variable (assuming its assignment to be strictly randomized), and search over IV models in EconML to estimate the impact of the customer choosing to use it. To score IV model fits out of sample, we again use the &lt;a href=&#34;https://arxiv.org/abs/2212.10076&#34;&gt;energy score&lt;/a&gt;. &lt;a href=&#34;https://github.com/py-why/causaltune/raw/main/notebooks/Comparing%20IV%20Estimators.ipynb&#34;&gt;Example notebook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please be aware we have not yet extensively used the IV model fitting functionality internally, so if you run into any issues, please report them!&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install from source, see &lt;a href=&#34;https://raw.githubusercontent.com/py-why/causaltune/main/#for-developers&#34;&gt;For Developers&lt;/a&gt; section below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;br&gt; CausalTune works with Python 3.8 and 3.9.&lt;/p&gt; &#xA;&lt;p&gt;It requires the following libraries to work:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NumPy&lt;/li&gt; &#xA; &lt;li&gt;Pandas&lt;/li&gt; &#xA; &lt;li&gt;EconML&lt;/li&gt; &#xA; &lt;li&gt;DoWhy&lt;/li&gt; &#xA; &lt;li&gt;Scikit-Learn&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you run into any problems, try installing the dependencies manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The CausalTune package can be used like a scikit-style estimator:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from causaltune import CausalTune&#xA;from causaltune.datasets import synth_ihdp&#xA;&#xA;# prepare dataset&#xA;data = synth_ihdp()&#xA;data.preprocess_dataset()&#xA;&#xA;&#xA;# init CausalTune object with chosen metric to optimise&#xA;ct = CausalTune(time_budget=600, metric=&#34;energy_distance&#34;)&#xA;&#xA;# run CausalTune&#xA;ct.fit(data)&#xA;&#xA;# return best estimator&#xA;print(f&#34;Best estimator: {ct.best_estimator}&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;The package supports the following causal estimators:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Meta Learners: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;S-Learner&lt;/li&gt; &#xA;   &lt;li&gt;T-Learner&lt;/li&gt; &#xA;   &lt;li&gt;X-Learner&lt;/li&gt; &#xA;   &lt;li&gt;Domain Adaptation Learner&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DR Learners: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Forest DR Learner&lt;/li&gt; &#xA;   &lt;li&gt;Linear DR Learner&lt;/li&gt; &#xA;   &lt;li&gt;Sparse Linear DR Learner&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DML Learners: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linear DML&lt;/li&gt; &#xA;   &lt;li&gt;Sparse Linear DML&lt;/li&gt; &#xA;   &lt;li&gt;Causal Forest DML&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Ortho Forests: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;DR Ortho Forest&lt;/li&gt; &#xA;   &lt;li&gt;DML Ortho Forest&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Transformed Outcome&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Metrics&lt;/h2&gt; &#xA;&lt;p&gt;We support a variety of different metrics that quantify the performance of a causal model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Energy distance&lt;/li&gt; &#xA; &lt;li&gt;ERUPT (Expected Response Under Proposed Treatments)&lt;/li&gt; &#xA; &lt;li&gt;Qini coefficient and AUC (area under curve)&lt;/li&gt; &#xA; &lt;li&gt;ATE (average treatment effect)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use CausalTune in your research, please cite us as follows:&lt;/p&gt; &#xA;&lt;p&gt;Timo Debono, Julian Teichgräber, Timo Flesch, Edward Zhang, Guy Durant, Wen Hao Kho, Mark Harley, Egor Kraev. &lt;strong&gt;CausalTune: A Python package for Automated Causal Inference model estimation and selection.&lt;/strong&gt; &lt;a href=&#34;https://github.com/py-why/causaltune&#34;&gt;https://github.com/py-why/causaltune&lt;/a&gt;. 2022. Version 0.x You can use the following BibTex entry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{CausalTune,&#xA;  author={Timo Debono, Julian Teichgr\&#34;aber, Timo Flesch, Edward Zhang, Guy Durant, Wen Hao Kho, Mark Harley, Egor Kraev},&#xA;  title={{CausalTune}: {A Python package for Automated Causal Inference model estimation and selection}},&#xA;  howpublished={https://github.com/py-why/causaltune},&#xA;  note={Version 0.x},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;For Developers&lt;/h2&gt; &#xA;&lt;h3&gt;Installation from source&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://setuptools.readthedocs.io/en/latest/index.html&#34;&gt;Setuptools&lt;/a&gt; for building and distributing our package. To install the latest version from source, clone this repository and run the following command from the top-most folder of the repository&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Testing&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://docs.pytest.org/&#34;&gt;PyTest&lt;/a&gt; for testing. If you want to contribute code, make sure that the tests run without errors.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/py-why/causaltune/raw/main/CONTRIBUTING.md&#34;&gt;Contribution file&lt;/a&gt; for contribution licensing and code guidelines.&lt;/p&gt;</summary>
  </entry>
</feed>