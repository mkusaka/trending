<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-01T01:37:19Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization</title>
    <updated>2023-10-01T01:37:19Z</updated>
    <id>tag:github.com,2023-10-01:/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization</id>
    <link href="https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Master the Toolkit of AI and Machine Learning. Mathematics for Machine Learning and Data Science is a beginner-friendly Specialization where you’ll learn the fundamental mathematics toolkit of machine learning: calculus, linear algebra, statistics, and probability.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://www.deeplearning.ai/courses/mathematics-for-machine-learning-and-data-science-specialization/&#34;&gt;Mathematics for Machine Learning and Data Science Specialization&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Master the Toolkit of AI and Machine Learning. Mathematics for Machine Learning and Data Science is a beginner-friendly Specialization where you’ll learn the fundamental mathematics toolkit of machine learning: calculus, linear algebra, statistics, and probability.&lt;/p&gt; &#xA;&lt;h2&gt;About this Specialization&lt;/h2&gt; &#xA;&lt;p&gt;Mathematics for Machine Learning and Data Science is a foundational online program created by DeepLearning.AI and taught by Luis Serrano. This beginner-friendly Specialization is where you’ll master the fundamental mathematics toolkit of machine learning.&lt;/p&gt; &#xA;&lt;p&gt;Many machine learning engineers and data scientists need help with mathematics, and even experienced practitioners can feel held back by a lack of math skills. This Specialization uses innovative pedagogy in mathematics to help you learn quickly and intuitively, with courses that use easy-to-follow plugins and visualizations to help you see how the math behind machine learning actually works.&lt;/p&gt; &#xA;&lt;p&gt;This is a beginner-friendly program, with a recommended background of at least high school mathematics. We also recommend a basic familiarity with Python, as labs use Python to demonstrate learning objectives in the environment where they’re most applicable to machine learning and data science.&lt;/p&gt; &#xA;&lt;h2&gt;Applied Learning Project&lt;/h2&gt; &#xA;&lt;p&gt;By the end of this Specialization, you will be ready to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Represent data as vectors and matrices and identify their properties using concepts of singularity, rank, and linear independence&lt;/li&gt; &#xA; &lt;li&gt;Apply common vector and matrix algebra operations like dot product, inverse, and determinants&lt;/li&gt; &#xA; &lt;li&gt;Express certain types of matrix operations as linear transformations&lt;/li&gt; &#xA; &lt;li&gt;Apply concepts of eigenvalues and eigenvectors to machine learning problems&lt;/li&gt; &#xA; &lt;li&gt;Optimize different types of functions commonly used in machine learning&lt;/li&gt; &#xA; &lt;li&gt;Perform gradient descent in neural networks with different activation and cost functions&lt;/li&gt; &#xA; &lt;li&gt;Describe and quantify the uncertainty inherent in predictions made by machine learning models&lt;/li&gt; &#xA; &lt;li&gt;Understand the properties of commonly used probability distributions in machine learning and data science&lt;/li&gt; &#xA; &lt;li&gt;Apply common statistical methods like MLE and MAP&lt;/li&gt; &#xA; &lt;li&gt;Assess the performance of machine learning models using interval estimates and margin of errors&lt;/li&gt; &#xA; &lt;li&gt;Apply concepts of statistical hypothesis testing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Course 1: Linear Algebra for Machine Learning and Data Science&lt;/h2&gt; &#xA;&lt;p&gt;After completing this course, learners will be able to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Represent data as vectors and matrices and identify their properties using concepts of singularity, rank, and linear independence, etc.&lt;/li&gt; &#xA; &lt;li&gt;Apply common vector and matrix algebra operations like dot product, inverse, and determinants&lt;/li&gt; &#xA; &lt;li&gt;Express certain types of matrix operations as linear transformations&lt;/li&gt; &#xA; &lt;li&gt;Apply concepts of eigenvalues and eigenvectors to machine learning problems&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Week 1: Systems of Linear Equations&lt;/h3&gt; &#xA;&lt;p&gt;Matrices are commonly used in machine learning and data science to represent data and its transformations. In this week, you will learn how matrices naturally arise from systems of equations and how certain matrix properties can be thought in terms of operations on system of equations.&lt;/p&gt; &#xA;&lt;h4&gt;Learning Objectives&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Form and graphically interpret 2x2 and 3x3 systems of linear equations&lt;/li&gt; &#xA; &lt;li&gt;Determine the number of solutions to a 2x2 and 3x3 system of linear equations&lt;/li&gt; &#xA; &lt;li&gt;Distinguish between singular and non-singular systems of equations&lt;/li&gt; &#xA; &lt;li&gt;Determine the singularity of 2x2 and 3x3 system of equations by calculating the determinant&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 1: Systems of Linear equations: two variables&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Machine learning motivation&lt;/li&gt; &#xA; &lt;li&gt;Systems of sentences&lt;/li&gt; &#xA; &lt;li&gt;Systems of equations&lt;/li&gt; &#xA; &lt;li&gt;Systems of equations as lines&lt;/li&gt; &#xA; &lt;li&gt;A geometric notion of singularity&lt;/li&gt; &#xA; &lt;li&gt;Singular vs nonsingular matrices&lt;/li&gt; &#xA; &lt;li&gt;Linear dependence and independence&lt;/li&gt; &#xA; &lt;li&gt;The determinant&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-1/C1_W1_Practice-Quiz.md&#34;&gt;Practice Quiz: Solving systems of linear equations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-1/C1_W1_Lab_1_introduction_to_numpy_arrays.ipynb&#34;&gt;Lab: Introduction to NumPy Arrays&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 2: Systems of Linear Equations: three variables&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Systems of equations (3×3)&lt;/li&gt; &#xA; &lt;li&gt;Singular vs non-singular (3×3)&lt;/li&gt; &#xA; &lt;li&gt;Systems of equations as planes (3×3)&lt;/li&gt; &#xA; &lt;li&gt;Linear dependence and independence (3×3)&lt;/li&gt; &#xA; &lt;li&gt;The determinant (3×3)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-1/C1_W1_Quiz.md&#34;&gt;Quiz: Matrices&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-1/C1_W1_Lab_2_solving_linear_systems_2_variables.ipynb&#34;&gt;Lab: Solving Linear Systems: 2 variables&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Week 2: Solving systems of Linear Equations&lt;/h3&gt; &#xA;&lt;p&gt;In this week, you will learn how to solve a system of linear equations using the elimination method and the row echelon form. You will also learn about an important property of a matrix: the rank. The concept of the rank of a matrix is useful in computer vision for compressing images.&lt;/p&gt; &#xA;&lt;h4&gt;Learning Objectives&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Solve a system of linear equations using the elimination method.&lt;/li&gt; &#xA; &lt;li&gt;Use a matrix to represent a system of linear equations and solve it using matrix row reduction.&lt;/li&gt; &#xA; &lt;li&gt;Solve a system of linear equations by calculating the matrix in the row echelon form.&lt;/li&gt; &#xA; &lt;li&gt;Calculate the rank of a system of linear equations and use the rank to determine the number of solutions of the system.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 1: Solving systems of Linear Equations: Elimination&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Machine learning motivation&lt;/li&gt; &#xA; &lt;li&gt;Solving non-singular systems of linear equations&lt;/li&gt; &#xA; &lt;li&gt;Solving singular systems of linear equations&lt;/li&gt; &#xA; &lt;li&gt;Solving systems of equations with more variables&lt;/li&gt; &#xA; &lt;li&gt;Matrix row-reduction&lt;/li&gt; &#xA; &lt;li&gt;Row operations that preserve singularity&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-2/C1_W2_Practice-Quiz.md&#34;&gt;Practice Quiz: Method of Elimination&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-2/C1_W2_Lab_1_solving_linear_systems_3_variables.ipynb&#34;&gt;Lab: Solving Linear Systems: 3 variables&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 2: Solving systems of Linear Equations: Row Echelon Form and Rank&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The rank of a matrix&lt;/li&gt; &#xA; &lt;li&gt;The rank of a matrix in general&lt;/li&gt; &#xA; &lt;li&gt;Row echelon form&lt;/li&gt; &#xA; &lt;li&gt;Row echelon form in general&lt;/li&gt; &#xA; &lt;li&gt;Reduced row echelon form&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-2/C1_W2_Quiz.md&#34;&gt;Quiz: The Rank of a matrix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-2/C1_W2_Assignment.ipynb&#34;&gt;Programming Assignment: System of Linear Equations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Week 3: Vectors and Linear Transformations&lt;/h3&gt; &#xA;&lt;p&gt;An individual instance (observation) of data is typically represented as a vector in machine learning. In this week, you will learn about properties and operations of vectors. You will also learn about linear transformations, matrix inverse, and one of the most important operations on matrices: the matrix multiplication. You will see how matrix multiplication naturally arises from composition of linear transformations. Finally, you will learn how to apply some of the properties of matrices and vectors that you have learned so far to neural networks.&lt;/p&gt; &#xA;&lt;h4&gt;Learning Objectives&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Perform common operations on vectors like sum, difference, and dot product.&lt;/li&gt; &#xA; &lt;li&gt;Multiply matrices and vectors.&lt;/li&gt; &#xA; &lt;li&gt;Represent a system of linear equations as a linear transformation on a vector.&lt;/li&gt; &#xA; &lt;li&gt;Calculate the inverse of a matrix, if it exists.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 1: Vectors&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Norm of a vector&lt;/li&gt; &#xA; &lt;li&gt;Sum and difference of vectors&lt;/li&gt; &#xA; &lt;li&gt;Distance between vectors&lt;/li&gt; &#xA; &lt;li&gt;Multiplying a vector by a scalar&lt;/li&gt; &#xA; &lt;li&gt;The dot product&lt;/li&gt; &#xA; &lt;li&gt;Geometric Dot Product&lt;/li&gt; &#xA; &lt;li&gt;Multiplying a matrix by a vector&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-3/C1_W3_Practice-Quiz.md&#34;&gt;Practice Quiz: Vector operations: Sum, difference, multiplication, dot product&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-3/C1_W3_Lab_1_vector_operations.ipynb&#34;&gt;Lab: Vector Operations: Scalar Multiplication, Sum and Dot Product of Vectors&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 2: Linear transformations&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Matrices as linear transformations&lt;/li&gt; &#xA; &lt;li&gt;Linear transformations as matrices&lt;/li&gt; &#xA; &lt;li&gt;Matrix multiplication&lt;/li&gt; &#xA; &lt;li&gt;The identity matrix&lt;/li&gt; &#xA; &lt;li&gt;Matrix inverse&lt;/li&gt; &#xA; &lt;li&gt;Which matrices have an inverse?&lt;/li&gt; &#xA; &lt;li&gt;Neural networks and matrices&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-3/C1_W3_Quiz.md&#34;&gt;Quiz: Vector and Matrix Operations, Types of Matrices&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-3/C1_W3_Lab_2_matrix_multiplication.ipynb&#34;&gt;Lab: Matrix Multiplication&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-3/C1_W3_Lab_3_linear_transformations.ipynb&#34;&gt;Lab: Linear Transformations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-3/C1_W3_Assignment.ipynb&#34;&gt;Programming Assignment: Single Perceptron Neural Networks for Linear Regression&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Week 4: Determinants and Eigenvectors&lt;/h3&gt; &#xA;&lt;p&gt;In this final week, you will take a deeper look at determinants. You will learn how determinants can be geometrically interpreted as an area and how to calculate determinant of product and inverse of matrices. We conclude this course with eigenvalues and eigenvectors. Eigenvectors are used in dimensionality reduction in machine learning. You will see how eigenvectors naturally follow from the concept of eigenbases.&lt;/p&gt; &#xA;&lt;h4&gt;Learning Objectives&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Interpret the determinant of a matrix as an area and calculate determinant of an inverse of a matrix and a product of matrices.&lt;/li&gt; &#xA; &lt;li&gt;Determine the bases and span of vectors.&lt;/li&gt; &#xA; &lt;li&gt;Find eigenbases for a special type of linear transformations commonly used in machine learning.&lt;/li&gt; &#xA; &lt;li&gt;Calculate the eignenvalues and eigenvectors of a linear transformation (matrix).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 1: Determinants In-depth&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Machine Learning Motivation&lt;/li&gt; &#xA; &lt;li&gt;Singularity and rank of linear transformation&lt;/li&gt; &#xA; &lt;li&gt;Determinant as an area&lt;/li&gt; &#xA; &lt;li&gt;Determinant of a product&lt;/li&gt; &#xA; &lt;li&gt;Determinants of inverses&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-4/C1_W4_Practice-Quiz.md&#34;&gt;Practice Quiz: Determinants and Linear Transformations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 2: Eigenvalues and Eigenvectors&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bases in Linear Algebra&lt;/li&gt; &#xA; &lt;li&gt;Span in Linear Algebra&lt;/li&gt; &#xA; &lt;li&gt;Interactive visualization: Linear Span&lt;/li&gt; &#xA; &lt;li&gt;Eigenbases&lt;/li&gt; &#xA; &lt;li&gt;Eigenvalues and eigenvectors&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-4/C1_W4_Quiz.md&#34;&gt;Quiz: Eigenvalues and Eigenvectors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-1/Week-4/C1_W4_Assignment.ipynb&#34;&gt;Programming Assignment: Eigenvalues and Eigenvectors&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Course 2: Calculus for Machine Learning and Data Science&lt;/h2&gt; &#xA;&lt;p&gt;After completing this course, learners will be able to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Analytically optimize different types of functions commonly used in machine learning using properties of derivatives and gradients&lt;/li&gt; &#xA; &lt;li&gt;Approximately optimize different types of functions commonly used in machine learning using first-order (gradient descent) and second-order (Newton’s method) iterative methods&lt;/li&gt; &#xA; &lt;li&gt;Visually interpret differentiation of different types of functions commonly used in machine learning&lt;/li&gt; &#xA; &lt;li&gt;Perform gradient descent in neural networks with different activation and cost functions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Week 1: Functions of one variable: Derivative and optimization&lt;/h3&gt; &#xA;&lt;h4&gt;Lesson 1: Derivatives&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example to motivate derivatives: Speedometer&lt;/li&gt; &#xA; &lt;li&gt;Derivative of common functions (c, x, x^2, 1/x)&lt;/li&gt; &#xA; &lt;li&gt;Meaning of e and the derivative of e^x&lt;/li&gt; &#xA; &lt;li&gt;Derivative of log x&lt;/li&gt; &#xA; &lt;li&gt;Existence of derivatives&lt;/li&gt; &#xA; &lt;li&gt;Properties of derivative&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-1/C2_W1_Practice-Quiz.md&#34;&gt;Practice Quiz: Derivatives&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-1/C2_W1_Lab_1_differentiation_in_python.ipynb&#34;&gt;Lab: Differentiation in Python: Symbolic, Numerical and Automatic&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 2: Optimization with derivatives&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Intro to optimization: Temperature example&lt;/li&gt; &#xA; &lt;li&gt;Optimizing cost functions in ML: Squared loss&lt;/li&gt; &#xA; &lt;li&gt;Optimizing cost functions in ML: Log loss&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-1/C2_W1_Quiz.md&#34;&gt;Quiz: Derivatives and Optimization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-1/C2_W1_Assignment.ipynb&#34;&gt;Programming Assignment: Optimizing Functions of One Variable: Cost Minimization&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Week 2: Functions of two or more variables: Gradients and gradient descent&lt;/h3&gt; &#xA;&lt;h4&gt;Lesson 1: Gradients and optimization&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Intro to gradients&lt;/li&gt; &#xA; &lt;li&gt;Example to motivate gradients: Temperature&lt;/li&gt; &#xA; &lt;li&gt;Gradient notation&lt;/li&gt; &#xA; &lt;li&gt;Optimization using slope method: Linear regression&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-2/C2_W2_Practice-Quiz.md&#34;&gt;Practice Quiz: Partial Derivatives and Gradient&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 2: Gradient Descent&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optimization using gradient descent: 1 variable&lt;/li&gt; &#xA; &lt;li&gt;Optimization using gradient descent: 2 variable&lt;/li&gt; &#xA; &lt;li&gt;Gradient descent for linear regression&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-2/C2_W2_Lab_1_Optimization_Using_Gradient_Descent_in_One_Variable.ipynb&#34;&gt;Lab: Optimization Using Gradient Descent in One Variable&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-2/C2_W2_Lab_2_Optimization_Using_Gradient_Descent_in_Two_Variables.ipynb&#34;&gt;Lab: Optimization Using Gradient Descent in Two Variables&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-2/C2_W2_Quiz.md&#34;&gt;Quiz: Partial Derivatives and Gradient Descent&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-2/C2_W2_Assignment.ipynb&#34;&gt;Programming Assignment: Optimization Using Gradient Descent: Linear Regression&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Week 3: Optimization in Neural Networks and Newton’s method&lt;/h3&gt; &#xA;&lt;h4&gt;Lesson 1: Optimization in Neural Networks&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Perceptron with no activation and squared loss (linear regression)&lt;/li&gt; &#xA; &lt;li&gt;Perceptron with sigmoid activation and log loss (classification)&lt;/li&gt; &#xA; &lt;li&gt;Two-layer neural network with sigmoid activation and log loss&lt;/li&gt; &#xA; &lt;li&gt;Mathematics of Backpropagation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-3/C2_W3_Lab_1_Regression_with_Perceptron.ipynb&#34;&gt;Lab: Regression with Perceptron&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-3/C2_W3_Lab_2_Classification_with_Perceptron.ipynb&#34;&gt;Lab: Classification with Perceptron&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-3/C2_W3_Practice-Quiz.md&#34;&gt;Practice Quiz: Optimization in Neural Networks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 2: Beyond Gradient Descent: Newton’s Method&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Root finding with Newton’s method&lt;/li&gt; &#xA; &lt;li&gt;Adapting Newton’s method for optimization&lt;/li&gt; &#xA; &lt;li&gt;Second derivatives and Hessians&lt;/li&gt; &#xA; &lt;li&gt;Multivariate Newton’s method&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-3/C2_W3_Lab_3_Optimization_Using_Newtons_Method.ipynb&#34;&gt;Lab: Optimization Using Newton&#39;s Method&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-3/C2_W3_Quiz.md&#34;&gt;Quiz: Optimization in Neural Networks and Newton&#39;s Method&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-2/Week-3/C2_W3_Assignment.ipynb&#34;&gt;Programming Assignment: Neural Network with Two Layers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Course 3: Probability &amp;amp; Statistics for Machine Learning &amp;amp; Data Science&lt;/h2&gt; &#xA;&lt;p&gt;After completing this course, learners will be able to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Analytically optimize different types of functions commonly used in machine learning using properties of derivatives and gradients&lt;/li&gt; &#xA; &lt;li&gt;Approximately optimize different types of functions commonly used in machine learning using first-order (gradient descent) and second-order (Newton’s method) iterative methods&lt;/li&gt; &#xA; &lt;li&gt;Visually interpret differentiation of different types of functions commonly used in machine learning&lt;/li&gt; &#xA; &lt;li&gt;Perform gradient descent in neural networks with different activation and cost functions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Week 1: Introduction to probability and random variables&lt;/h3&gt; &#xA;&lt;h4&gt;Lesson 1: Introduction to probability&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Concept of probability: repeated random trials&lt;/li&gt; &#xA; &lt;li&gt;Conditional probability and independence&lt;/li&gt; &#xA; &lt;li&gt;Discriminative learning and conditional probability&lt;/li&gt; &#xA; &lt;li&gt;Bayes theorem&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-1/C3_W1_Lab_1_Birthday_Problems.ipynb&#34;&gt;Lab: Four Birthday Problems&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-1/C3_W1_Lab_2_Monty_Hall.ipynb&#34;&gt;Lab: Monty Hall Problem&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-1/C3_W1_Practice-Quiz.md&#34;&gt;Practice Quiz&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 2: Random variables&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Random variables&lt;/li&gt; &#xA; &lt;li&gt;Cumulative distribution function&lt;/li&gt; &#xA; &lt;li&gt;Discrete random variables: Bernoulli distribution&lt;/li&gt; &#xA; &lt;li&gt;Discrete random variables: Binomial distribution&lt;/li&gt; &#xA; &lt;li&gt;Probability mass function&lt;/li&gt; &#xA; &lt;li&gt;Continuous random variables: Uniform distribution&lt;/li&gt; &#xA; &lt;li&gt;Continuous random variables: Gaussian distribution&lt;/li&gt; &#xA; &lt;li&gt;Continuous random variables: Chi squared distribution&lt;/li&gt; &#xA; &lt;li&gt;Probability distribution function&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-1/C3_W1_Quiz.md&#34;&gt;Quiz&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-1/C3_W1_Assignment.ipynb&#34;&gt;Programming Assignment: Probability Distributions / Naive Bayes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Week 2: Describing distributions and random vectors&lt;/h3&gt; &#xA;&lt;h4&gt;Lesson 1: Describing distributions&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Measures of central tendency: mean, median, mode&lt;/li&gt; &#xA; &lt;li&gt;Expected values&lt;/li&gt; &#xA; &lt;li&gt;Quantiles and box-plots&lt;/li&gt; &#xA; &lt;li&gt;Measures of dispersion: variance, standard deviation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-2/C3_W2_Practice-Quiz.md&#34;&gt;Practice Quiz&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 2: Random vectors&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Joint distributions&lt;/li&gt; &#xA; &lt;li&gt;Marginal and conditional distributions&lt;/li&gt; &#xA; &lt;li&gt;Independence&lt;/li&gt; &#xA; &lt;li&gt;Measures of relatedness: covariance&lt;/li&gt; &#xA; &lt;li&gt;Multivariate normal distribution&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-2/ugl_datasets.ipynb&#34;&gt;Lab: Summary statistics and visualization of data sets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-2/C3_W2_Quiz.md&#34;&gt;Quiz&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-2/C3_W2_Lab_2_Dice_Simulations.ipynb&#34;&gt;Lab: Simulating Dice Rolls with Numpy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-2/C3_W2_Assignment.ipynb&#34;&gt;Programming Assignment: Loaded Dice&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Week 3: Introduction to statistics&lt;/h3&gt; &#xA;&lt;h4&gt;Lesson 1: Sampling and point estimates&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Population vs. sample&lt;/li&gt; &#xA; &lt;li&gt;Describing samples: sample proportion and sample mean&lt;/li&gt; &#xA; &lt;li&gt;Distribution of sample mean and proportion: Central Limit Theorem&lt;/li&gt; &#xA; &lt;li&gt;Point estimates&lt;/li&gt; &#xA; &lt;li&gt;Biased vs Unbiased estimates&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-3/C3_W3_Lab_1_Central_Limit_Theorem.ipynb&#34;&gt;Lab: Sampling data from different distribution and studying the distribution of sample mean&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-3/C3_W3_Practice-Quiz.md&#34;&gt;Practice Quiz&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 2: Maximum likelihood estimation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ML motivation example: Linear Discriminant Analysis&lt;/li&gt; &#xA; &lt;li&gt;Likelihood&lt;/li&gt; &#xA; &lt;li&gt;Intuition behind maximum likelihood estimation&lt;/li&gt; &#xA; &lt;li&gt;MLE: How to get the maximum using calculus&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 3: Bayesian statistics&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ML motivation example: Naive Bayes&lt;/li&gt; &#xA; &lt;li&gt;Frequentist vs. Bayesian statistics&lt;/li&gt; &#xA; &lt;li&gt;A priori/ a posteriori distributions&lt;/li&gt; &#xA; &lt;li&gt;Bayesian estimators: posterior mean, posterior median, MAP&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-3/C3_W3_Quiz.md&#34;&gt;Quiz&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Week 4: Interval statistics and Hypothesis testing&lt;/h3&gt; &#xA;&lt;h4&gt;Lesson 1: Confidence intervals&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Margin of error&lt;/li&gt; &#xA; &lt;li&gt;Interval estimation&lt;/li&gt; &#xA; &lt;li&gt;Confidence Interval for mean of population&lt;/li&gt; &#xA; &lt;li&gt;CI for parameters in linear regression&lt;/li&gt; &#xA; &lt;li&gt;Prediction Interval&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-4/C3_W4_Practice-Quiz.md&#34;&gt;Practice Quiz&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lesson 2: Hypothesis testing&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ML Motivation: AB Testing&lt;/li&gt; &#xA; &lt;li&gt;Criminal trial&lt;/li&gt; &#xA; &lt;li&gt;Two types of errors&lt;/li&gt; &#xA; &lt;li&gt;Test for proportion and means&lt;/li&gt; &#xA; &lt;li&gt;Two sample inference for difference between groups&lt;/li&gt; &#xA; &lt;li&gt;ANOVA&lt;/li&gt; &#xA; &lt;li&gt;Power of a test&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-4/C3_W4_Quiz.md&#34;&gt;Quiz&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryota-Kawamura/Mathematics-for-Machine-Learning-and-Data-Science-Specialization/raw/main/Course-3/Week-4/C3_W4_Assignment.ipynb&#34;&gt;Programming Assignment: A/B Testing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>atomic14/esp32-tv</title>
    <updated>2023-10-01T01:37:19Z</updated>
    <id>tag:github.com,2023-10-01:/atomic14/esp32-tv</id>
    <link href="https://github.com/atomic14/esp32-tv" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ESP32 - Sound and Vision - Video Streaming to the ESP32&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/atomic14/esp32-tv/actions/workflows/build_firmware.yml&#34;&gt;&lt;img src=&#34;https://github.com/atomic14/esp32-tv/actions/workflows/build_firmware.yml/badge.svg?sanitize=true&#34; alt=&#34;Build Firmware&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ESP32 Video Streaming!&lt;/h1&gt; &#xA;&lt;p&gt;Yes - it actually works! Streaming video with audio over WiFi to an ESP32!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=G6MROvlLeKE&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/G6MROvlLeKE/0.jpg&#34; alt=&#34;WiFi Streaming&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And - playing video from an SD Card!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dWgjsJtlbpA&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/dWgjsJtlbpA/0.jpg&#34; alt=&#34;SD Card Video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;There&#39;s two projects in this repo - one for the ESP32 firmware and another for the server.&lt;/p&gt; &#xA;&lt;p&gt;The README files in each project have more details.&lt;/p&gt; &#xA;&lt;h1&gt;How Does It Work?&lt;/h1&gt; &#xA;&lt;p&gt;The server is pretty simple, it has a few endpoints:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;/channel_info&lt;/code&gt; - returns a list of channel lengths in audio samples&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/frame/&amp;lt;int:channel_index&amp;gt;/&amp;lt;int:ms&amp;gt;&lt;/code&gt; - returns a JPEG image for the given channel at the given time (in ms)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/audio/&amp;lt;int:channel_index&amp;gt;/&amp;lt;int:start&amp;gt;/&amp;lt;int:length&amp;gt;&lt;/code&gt; - returns 8 bit PCM audio at 16KHz for the given channel starting from the given sample index and for the given length (in samples)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The ESP32 firmware connects to the server and requests the channel info. The video playback is locked to the audio sample playback. The audio is played back using the I2S peripheral and we use that to know how much time has elapsed to request the correct frames. This way the video and audio are always in sync.&lt;/p&gt; &#xA;&lt;p&gt;You can get around 15 frames per second at 280x240 resolution, the main limitation is WiFi bandwidth and decoding the JPEGs.&lt;/p&gt; &#xA;&lt;h1&gt;Support for SD Cards&lt;/h1&gt; &#xA;&lt;p&gt;In the README file for the firmware there are instructions on how to convert a video file to a MJPEG AVI file - if you&#39;ve got a device with an SD Card you can use this instead of WiFi streaming.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>damiangilgonzalez1995/Clustering-with-LLM</title>
    <updated>2023-10-01T01:37:19Z</updated>
    <id>tag:github.com,2023-10-01:/damiangilgonzalez1995/Clustering-with-LLM</id>
    <link href="https://github.com/damiangilgonzalez1995/Clustering-with-LLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A customer segmentation project can be approached in multiple ways. In this repository, we will explore advanced techniques for defining clusters and analyzing the results.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Clustering with LLM&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;A customer segmentation project can be approached in multiple ways. In this repository, we will explore advanced techniques for defining clusters and analyzing the results. This repo is intended for data scientists looking to expand their toolbox for tackling clustering problems and advancing towards becoming senior data scientists.&lt;/p&gt; &#xA;&lt;h3&gt;What Will We Cover?&lt;/h3&gt; &#xA;&lt;p&gt;In this repo, we will explore three methods to approach customer segmentation projects:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Kmeans&lt;/li&gt; &#xA; &lt;li&gt;K-Prototype&lt;/li&gt; &#xA; &lt;li&gt;LLM + Kmeans&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/damiangilgonzalez1995/Clustering-with-LLM/main/img/com2d2.png&#34; alt=&#34;Getting Started&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;As a preview, we will provide a comparison of 2D representations (PCA) of the different models created. Additionally, you will learn about dimensionality reduction techniques such as PCA, t-SNE, and MCA, with results included.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/damiangilgonzalez1995/Clustering-with-LLM/main/img/METH.png&#34; alt=&#34;Getting Started&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important Note&lt;/strong&gt;: This project does not cover the exploratory data analysis (EDA) phase or variable selection, which are crucial steps in such projects.&lt;/p&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;The original data used in this project is from a public Kaggle dataset called &#34;Banking Dataset - Marketing Targets.&#34; Each row in this dataset contains information about a company&#39;s customers, including both numerical and categorical fields. This diversity in data types opens up various approaches to the problem.&lt;/p&gt; &#xA;&lt;p&gt;For this project, we will focus on the first 8 columns of the dataset, which include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;age&lt;/code&gt; (numeric)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;job&lt;/code&gt;: type of job (categorical)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;marital&lt;/code&gt;: marital status (categorical)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;education&lt;/code&gt;: education level (categorical)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;default&lt;/code&gt;: has credit in default? (binary)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;balance&lt;/code&gt;: average yearly balance in euros (numeric)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;housing&lt;/code&gt;: has a housing loan? (binary)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;loan&lt;/code&gt;: has a personal loan? (binary)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The project uses the training dataset from Kaggle, which can be found in the &#34;data&#34; folder of the project repository as a compressed file. Inside the compressed file, you&#39;ll find two CSV files: &lt;code&gt;train.csv&lt;/code&gt; (the original training dataset) and &lt;code&gt;embedding_train.csv&lt;/code&gt; (the dataset after performing an embedding, which will be explained later).&lt;/p&gt; &#xA;&lt;p&gt;To understand the project&#39;s structure, here&#39;s an overview of the project directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;clustering_llm&#xA;├─ data&#xA;│  ├─ data.rar&#xA;├─ img&#xA;├─ embedding.ipynb&#xA;├─ embedding_creation.py&#xA;├─ kmeans.ipynb&#xA;├─ kprototypes.ipynb&#xA;├─ README.md&#xA;└─ requirements.txt&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Method 1: Kmeans&lt;/h2&gt; &#xA;&lt;p&gt;Kmeans is a commonly used clustering method, and we will dive into it to demonstrate advanced analysis techniques. The complete procedure can be found in the Jupyter notebook titled &lt;code&gt;kmeans.ipynb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Method 2: Kprototype&lt;/h2&gt; &#xA;&lt;p&gt;Method to create clusters when you have a mix of features (categorical and numerical). Jupyter notebook titled &lt;code&gt;kprototypes.ipynb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Method 3: LLM + Kmeans&lt;/h2&gt; &#xA;&lt;p&gt;The jewel in the crown, where you will find how to apply llm to obtain impeccable results in cluttering projects Jupyter notebook titled &lt;code&gt;embedding.ipynb&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>