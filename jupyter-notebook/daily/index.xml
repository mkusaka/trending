<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-24T01:32:44Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVIDIA-AI-Blueprints/digital-human</title>
    <updated>2025-03-24T01:32:44Z</updated>
    <id>tag:github.com,2025-03-24:/NVIDIA-AI-Blueprints/digital-human</id>
    <link href="https://github.com/NVIDIA-AI-Blueprints/digital-human" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NVIDIA AI Blueprint for digital human for customer service.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;&lt;img align=&#34;center&#34; src=&#34;https://github.com/user-attachments/assets/cbe0d62f-c856-4e0b-b3ee-6184b7c4d96f&#34;&gt;NVIDIA AI Blueprint: Digital Human for Customer Service&lt;/h2&gt; &#xA;&lt;p&gt;The Digital Human for Customer Service NVIDIA AI Blueprint is powered by NVIDIA Tokkio, a workflow based on ACE technologies, to bring enterprise applications to life with a 3D animated digital human interface. With approachable, human-like interactions, customer service applications can provide more engaging user experience compared to traditional customer service options.&lt;/p&gt; &#xA;&lt;p&gt;This blueprint is powered by a suite of easy to use and performance-optimized NVIDIA NIM&lt;sup&gt;TM&lt;/sup&gt; inference microservices, for avatar animation, speech AI, and generative AI. The avatar is rendered using the Omniverse RTX microservice, animated with the Audio2Face NIM, and has a responsive speech interface with the NVIDIA Riva NIM and ElevenLabs integrations.&lt;/p&gt; &#xA;&lt;h2&gt;Why digital humans?&lt;/h2&gt; &#xA;&lt;p&gt;Digital humans will revolutionize industries from customer service, to advertising and gaming. The possibilities for digital humans are endless. With an approachable, human-like interface, customer service applications can provide better user experiences with faster resolutions than generative AI powered applications with just a text or speech interface. Check out this youtube video. This digital humans blueprint highlights how to use NVIDIA hardware and software to bring the $125B digital human market to life. 80% of conversational offerings will embed generative AI by 2025, up from 20% in 2024*. And 75% of conversational AI customer-facing business applications will have emotion AI by 2030, up from less than 10% in 2024&lt;sup&gt;*&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;p&gt;*Gartner – Emerging Tech: Navigating the Hurdles of Digital Humans&lt;/p&gt; &#xA;&lt;h2&gt;About this blueprint&lt;/h2&gt; &#xA;&lt;p&gt;This blueprint serves as a starting point for a team of developers to showcase how an LLM or a RAG application can be connected to a digital human pipeline. The digital avatar and the Retrieval-Augmented Generation (RAG) applications are deployed separately.&lt;/p&gt; &#xA;&lt;p&gt;The Digital Human pipeline: This is powered by the Tokkio-LLM RAG workflow and includes avatar speech, animation, streaming, vision and orchestration components. The reference workflow comes in multiple variations dependent on desired stream count, avatar type and rendering technology.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Reference Workflow&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Avatar animation &amp;amp; rendering&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Stream options&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Tokkio LLM-RAG-ov&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3D avatar with Audio2Face-3D &amp;amp; Omniverse Renderer&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1, 3, 6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Tokkio LLM-RAG-ue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3D avatar with Unreal Engine Renderer &lt;a href=&#34;https://developer.nvidia.com/ace/early-access-program&#34;&gt;Early Access&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1, 3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Tokkio LLM-RAG-2D&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2D avatar with Audio2Face-2D &lt;a href=&#34;https://developer.nvidia.com/ace/early-access-program&#34;&gt;Early Access&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1, 3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The RAG application is responsible for generating the text context to question-answer interactions and the digital human live avatar, leverages a Tokkio customer service pipeline. Since these two entities are separated, they communicate with one another using the REST API. Developers can build upon this blueprint, by customizing the RAG application and digital avatar based on their specific use case.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;800&#34; alt=&#34;dht&#34; src=&#34;https://github.com/user-attachments/assets/91157444-4e25-4f19-a3c7-8b5397cb5c02&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Target audience&lt;/h2&gt; &#xA;&lt;p&gt;Setting up the digital human pipelines, as well as customization, requires a technical team with expertise in different areas of the software stack.&lt;/p&gt; &#xA;&lt;h3&gt;Persona-1: Devops engineer&lt;/h3&gt; &#xA;&lt;p&gt;To deploy this blueprint, you must be comfortable deploying applications/helm charts on Kubernetes, creating resources on cloud service provider platforms and have general deployment expertise.&lt;/p&gt; &#xA;&lt;h3&gt;Persona-2: GenAI developer/ Machine learning engineer&lt;/h3&gt; &#xA;&lt;p&gt;Since this blueprint will require customization for your specific use case, you should have necessary technical expertise to take the existing blueprint and change it to suit your needs. This includes, but is not limited to changing the RAG pipeline for your specific dataset and fine tuning the LLMs if needed.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: Please refer to the customization section for additional information.&lt;/p&gt; &#xA;&lt;h3&gt;Persona-3: (Optional) Animation/Rendering developer&lt;/h3&gt; &#xA;&lt;p&gt;Tokkio allows you to customize your live avatar with the Avatar Configurator or alternatively you can import a third-party avatar by following the custom avatar guide. More information can be found in the Avatar Customization section. If you are looking to customize the avatar away from the default one then you would need an engineer comfortable in above.&lt;/p&gt; &#xA;&lt;h3&gt;Persona-4: (Optional) Network Engineering / IT system admin&lt;/h3&gt; &#xA;&lt;p&gt;Tokkio is a web based interactive Digital Human application, requiring communication over various UDP &amp;amp; TCP ports. Production environments often include a complex heirarchy schema for ingress and egress and over different ports and port ranges. Working with your network engineering and IT system administrator ensures that network topology is well understood to ensure proper bring up and necessary communication from the web based application.&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;h4&gt;Get Started&lt;/h4&gt; &#xA;&lt;p&gt;The Digital human blueprint has two components, the digital avatar deployment and the Retrieval Augmented Generation pipeline.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-Blueprints/digital-human/main/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-Blueprints/digital-human/main/deploy/&#34;&gt;Deployment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-Blueprints/digital-human/main/customize/&#34;&gt;Customization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-AI-Blueprints/digital-human/main/(/evaluate/)&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://enterpriseproductregistration.nvidia.com/?LicType=EVAL&amp;amp;ProductFamily=NVAIEnterprise&#34;&gt;NVIDIA AI Enterprise&lt;/a&gt; or &lt;a href=&#34;https://developer.nvidia.com/login&#34;&gt;Developer License&lt;/a&gt;: NVIDIA NIM for LLMs are available for self-hosting under the NVIDIA AI Enterprise (NVAIE) License.&lt;/li&gt; &#xA; &lt;li&gt;Sign up for NVAIE license. An NGC API key is required to access NGC resources. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;To obtain a key, navigate to Blueprint experience on NVIDIA API Catalog.&lt;/li&gt; &#xA;   &lt;li&gt;Login / Sign up if needed and &#34;Generate your API Key&#34;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Digital Avatar&lt;/h3&gt; &#xA;&lt;h4&gt;Hardware requirements&lt;/h4&gt; &#xA;&lt;p&gt;Supported NVIDIA GPU hardware:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;T4&lt;/li&gt; &#xA; &lt;li&gt;A10&lt;/li&gt; &#xA; &lt;li&gt;L4&lt;/li&gt; &#xA; &lt;li&gt;L40S&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A minimum of 2 GPUs are required for 1 stream (eg: LLM-RAG-ov-1 application) and 4 GPUs for 3 streams (eg: LLM-RAG-ov-3 application). For this blueprint guide, The blueprint requires at least a 8 core CPU, 64GB of system memory and 500GB of disk space.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: If you are setting this up on AWS, GCP or Azure. Please checkout the GPU instance types for the corresponding providers.&lt;/p&gt; &#xA;&lt;h4&gt;System requirements&lt;/h4&gt; &#xA;&lt;p&gt;The setup for tokkio requires two instances, a controller instance and an application instance, The controller instance doesn&#39;t require GPUs and it where we luanch the automation scripts from.Both require a Ubuntu 20.04 or 22.04 based machine, with sudo privileges for the user to run the automated deployment scripts.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: In an upcoming step, you will use one click scripts in the Digital Avatar deployment instructions to set up everything else.&lt;/p&gt; &#xA;&lt;h3&gt;Retrieval Augmented Generation (RAG) pipeline&lt;/h3&gt; &#xA;&lt;h4&gt;Hardware requirements&lt;/h4&gt; &#xA;&lt;h5&gt;Option 1: RAG without customization&lt;/h5&gt; &#xA;&lt;p&gt;To familiarize yourself with the digital human blueprint, you can leverage a non-GPU accelerated AWS EC2 instance. By default, the blueprint uses the NVIDIA API Catalog hosted endpoints for LLM, embedding and reranking models. Therefore all that is required is an instance with at least 8 cores and 64GB memory. A public IP address is also required to connect to the digital human avatar.&lt;/p&gt; &#xA;&lt;h5&gt;Option-2: RAG with customization&lt;/h5&gt; &#xA;&lt;p&gt;Once you familiarize yourself with the blueprint, you may want to further customize based upon your own use case which requires you to host your own LLM, embedding and reranking models. In this case you will need access to a GPU accelerated AWS EC2 instance with 8 cores, 64GB memory and 2X A100 or L40s GPUs. For this option, deploy the RAG pipeline on AWS using a g5.12xlarge machine. A public IP address is required to connect to the digital human avatar.&lt;/p&gt; &#xA;&lt;h4&gt;System requirements&lt;/h4&gt; &#xA;&lt;p&gt;Ubuntu 20.04 or 22.04 based machine, with sudo privileges&lt;/p&gt;</summary>
  </entry>
</feed>