<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-25T01:33:56Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>afizs/python-notes</title>
    <updated>2022-11-25T01:33:56Z</updated>
    <id>tag:github.com,2022-11-25:/afizs/python-notes</id>
    <link href="https://github.com/afizs/python-notes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ“’ Python Notes&lt;/h1&gt; &#xA;&lt;h3&gt;This repo contains all the Python projects, tips and notes I share on twitter and YouTube.&lt;/h3&gt; &#xA;&lt;h3&gt;â­ï¸ Star this repo, if you don&#39;t want to miss any update.&lt;/h3&gt;</summary>
  </entry>
  <entry>
    <title>SKTBrain/KoBERT</title>
    <updated>2022-11-25T01:33:56Z</updated>
    <id>tag:github.com,2022-11-25:/SKTBrain/KoBERT</id>
    <link href="https://github.com/SKTBrain/KoBERT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Korean BERT pre-trained cased (KoBERT)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;KoBERT&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#kobert&#34;&gt;KoBERT&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#korean-bert-pre-trained-cased-kobert&#34;&gt;Korean BERT pre-trained cased (KoBERT)&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#why&#34;&gt;Why&#39;?&#39;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#training-environment&#34;&gt;Training Environment&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#how-to-install&#34;&gt;How to install&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#how-to-use&#34;&gt;How to use&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#using-with-pytorch&#34;&gt;Using with PyTorch&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#using-with-onnx&#34;&gt;Using with ONNX&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#using-with-mxnet-gluon&#34;&gt;Using with MXNet-Gluon&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#tokenizer&#34;&gt;Tokenizer&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#subtasks&#34;&gt;Subtasks&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#naver-sentiment-analysis&#34;&gt;Naver Sentiment Analysis&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#kobert%EC%99%80-crf%EB%A1%9C-%EB%A7%8C%EB%93%A0-%ED%95%9C%EA%B5%AD%EC%96%B4-%EA%B0%9D%EC%B2%B4%EB%AA%85%EC%9D%B8%EC%8B%9D%EA%B8%B0&#34;&gt;KoBERTì™€ CRFë¡œ ë§Œë“  í•œêµ­ì–´ ê°ì²´ëª…ì¸ì‹ê¸°&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#korean-sentence-bert&#34;&gt;Korean Sentence BERT&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#release&#34;&gt;Release&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#contacts&#34;&gt;Contacts&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Korean BERT pre-trained cased (KoBERT)&lt;/h2&gt; &#xA;&lt;h3&gt;Why&#39;?&#39;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;êµ¬ê¸€ &lt;a href=&#34;https://github.com/google-research/bert/raw/master/multilingual.md&#34;&gt;BERT base multilingual cased&lt;/a&gt;ì˜ í•œêµ­ì–´ ì„±ëŠ¥ í•œê³„&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training Environment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Architecture&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predefined_args = {&#xA;        &#39;attention_cell&#39;: &#39;multi_head&#39;,&#xA;        &#39;num_layers&#39;: 12,&#xA;        &#39;units&#39;: 768,&#xA;        &#39;hidden_size&#39;: 3072,&#xA;        &#39;max_length&#39;: 512,&#xA;        &#39;num_heads&#39;: 12,&#xA;        &#39;scaled&#39;: True,&#xA;        &#39;dropout&#39;: 0.1,&#xA;        &#39;use_residual&#39;: True,&#xA;        &#39;embed_size&#39;: 768,&#xA;        &#39;embed_dropout&#39;: 0.1,&#xA;        &#39;token_type_vocab_size&#39;: 2,&#xA;        &#39;word_embed&#39;: None,&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;í•™ìŠµì…‹&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ë°ì´í„°&lt;/th&gt; &#xA;   &lt;th&gt;ë¬¸ì¥&lt;/th&gt; &#xA;   &lt;th&gt;ë‹¨ì–´&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;í•œêµ­ì–´ ìœ„í‚¤&lt;/td&gt; &#xA;   &lt;td&gt;5M&lt;/td&gt; &#xA;   &lt;td&gt;54M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;í•™ìŠµ í™˜ê²½ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;V100 GPU x 32, Horovod(with InfiniBand)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/imgs/2019-04-29_TensorBoard.png&#34; alt=&#34;2019-04-29 í…ì„œë³´ë“œ ë¡œê·¸&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ì‚¬ì „(Vocabulary) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;í¬ê¸° : 8,002&lt;/li&gt; &#xA;   &lt;li&gt;í•œê¸€ ìœ„í‚¤ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•œ í† í¬ë‚˜ì´ì €(SentencePiece)&lt;/li&gt; &#xA;   &lt;li&gt;Less number of parameters(92M &amp;lt; 110M )&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;see &lt;a href=&#34;https://github.com/SKTBrain/KoBERT/raw/master/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How to install&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Install KoBERT as a python package&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install git+https://git@github.com/SKTBrain/KoBERT.git@master&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you want to modify source codes, please clone this repository&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/SKTBrain/KoBERT.git&#xA;cd KoBERT&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;h3&gt;Using with PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Huggingface transformers APIê°€ í¸í•˜ì‹  ë¶„ì€ &lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/kobert_hf&#34;&gt;ì—¬ê¸°&lt;/a&gt;ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; from kobert import get_pytorch_kobert_model&#xA;&amp;gt;&amp;gt;&amp;gt; input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])&#xA;&amp;gt;&amp;gt;&amp;gt; input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])&#xA;&amp;gt;&amp;gt;&amp;gt; token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])&#xA;&amp;gt;&amp;gt;&amp;gt; model, vocab  = get_pytorch_kobert_model()&#xA;&amp;gt;&amp;gt;&amp;gt; sequence_output, pooled_output = model(input_ids, input_mask, token_type_ids)&#xA;&amp;gt;&amp;gt;&amp;gt; pooled_output.shape&#xA;torch.Size([2, 768])&#xA;&amp;gt;&amp;gt;&amp;gt; vocab&#xA;Vocab(size=8002, unk=&#34;[UNK]&#34;, reserved=&#34;[&#39;[MASK]&#39;, &#39;[SEP]&#39;, &#39;[CLS]&#39;]&#34;)&#xA;&amp;gt;&amp;gt;&amp;gt; # Last Encoding Layer&#xA;&amp;gt;&amp;gt;&amp;gt; sequence_output[0]&#xA;tensor([[-0.2461,  0.2428,  0.2590,  ..., -0.4861, -0.0731,  0.0756],&#xA;        [-0.2478,  0.2420,  0.2552,  ..., -0.4877, -0.0727,  0.0754],&#xA;        [-0.2472,  0.2420,  0.2561,  ..., -0.4874, -0.0733,  0.0765]],&#xA;       grad_fn=&amp;lt;SelectBackward&amp;gt;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;model&lt;/code&gt;ì€ ë””í´íŠ¸ë¡œ &lt;code&gt;eval()&lt;/code&gt;ëª¨ë“œë¡œ ë¦¬í„´ë¨, ë”°ë¼ì„œ í•™ìŠµ ìš©ë„ë¡œ ì‚¬ìš©ì‹œ &lt;code&gt;model.train()&lt;/code&gt;ëª…ë ¹ì„ í†µí•´ í•™ìŠµ ëª¨ë“œë¡œ ë³€ê²½í•  í•„ìš”ê°€ ìˆë‹¤.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Naver Sentiment Analysis Fine-Tuning with pytorch &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Colabì—ì„œ [ëŸ°íƒ€ì„] - [ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½] - í•˜ë“œì›¨ì–´ ê°€ì†ê¸°(GPU) ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using with ONNX&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import onnxruntime&#xA;&amp;gt;&amp;gt;&amp;gt; import numpy as np&#xA;&amp;gt;&amp;gt;&amp;gt; from kobert import get_onnx_kobert_model&#xA;&amp;gt;&amp;gt;&amp;gt; onnx_path = get_onnx_kobert_model()&#xA;&amp;gt;&amp;gt;&amp;gt; sess = onnxruntime.InferenceSession(onnx_path)&#xA;&amp;gt;&amp;gt;&amp;gt; input_ids = [[31, 51, 99], [15, 5, 0]]&#xA;&amp;gt;&amp;gt;&amp;gt; input_mask = [[1, 1, 1], [1, 1, 0]]&#xA;&amp;gt;&amp;gt;&amp;gt; token_type_ids = [[0, 0, 1], [0, 1, 0]]&#xA;&amp;gt;&amp;gt;&amp;gt; len_seq = len(input_ids[0])&#xA;&amp;gt;&amp;gt;&amp;gt; pred_onnx = sess.run(None, {&#39;input_ids&#39;:np.array(input_ids),&#xA;&amp;gt;&amp;gt;&amp;gt;                             &#39;token_type_ids&#39;:np.array(token_type_ids),&#xA;&amp;gt;&amp;gt;&amp;gt;                             &#39;input_mask&#39;:np.array(input_mask),&#xA;&amp;gt;&amp;gt;&amp;gt;                             &#39;position_ids&#39;:np.array(range(len_seq))})&#xA;&amp;gt;&amp;gt;&amp;gt; # Last Encoding Layer&#xA;&amp;gt;&amp;gt;&amp;gt; pred_onnx[-2][0]&#xA;array([[-0.24610452,  0.24282141,  0.25895312, ..., -0.48613444,&#xA;        -0.07305173,  0.07560554],&#xA;       [-0.24783179,  0.24200465,  0.25520486, ..., -0.4877185 ,&#xA;        -0.0727044 ,  0.07536091],&#xA;       [-0.24721591,  0.24196623,  0.2560626 , ..., -0.48743123,&#xA;        -0.07326943,  0.07650235]], dtype=float32)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;ONNX ì»¨ë²„íŒ…ì€ &lt;a href=&#34;https://github.com/soeque1&#34;&gt;soeque1&lt;/a&gt;ê»˜ì„œ ë„ì›€ì„ ì£¼ì…¨ìŠµë‹ˆë‹¤.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Using with MXNet-Gluon&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import mxnet as mx&#xA;&amp;gt;&amp;gt;&amp;gt; from kobert import get_mxnet_kobert_model&#xA;&amp;gt;&amp;gt;&amp;gt; input_id = mx.nd.array([[31, 51, 99], [15, 5, 0]])&#xA;&amp;gt;&amp;gt;&amp;gt; input_mask = mx.nd.array([[1, 1, 1], [1, 1, 0]])&#xA;&amp;gt;&amp;gt;&amp;gt; token_type_ids = mx.nd.array([[0, 0, 1], [0, 1, 0]])&#xA;&amp;gt;&amp;gt;&amp;gt; model, vocab = get_mxnet_kobert_model(use_decoder=False, use_classifier=False)&#xA;&amp;gt;&amp;gt;&amp;gt; encoder_layer, pooled_output = model(input_id, token_type_ids)&#xA;&amp;gt;&amp;gt;&amp;gt; pooled_output.shape&#xA;(2, 768)&#xA;&amp;gt;&amp;gt;&amp;gt; vocab&#xA;Vocab(size=8002, unk=&#34;[UNK]&#34;, reserved=&#34;[&#39;[MASK]&#39;, &#39;[SEP]&#39;, &#39;[CLS]&#39;]&#34;)&#xA;&amp;gt;&amp;gt;&amp;gt; # Last Encoding Layer&#xA;&amp;gt;&amp;gt;&amp;gt; encoder_layer[0]&#xA;[[-0.24610372  0.24282135  0.2589539  ... -0.48613444 -0.07305248&#xA;   0.07560539]&#xA; [-0.24783105  0.242005    0.25520545 ... -0.48771808 -0.07270523&#xA;   0.07536077]&#xA; [-0.24721491  0.241966    0.25606337 ... -0.48743105 -0.07327032&#xA;   0.07650219]]&#xA;&amp;lt;NDArray 3x768 @cpu(0)&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Naver Sentiment Analysis Fine-Tuning with MXNet &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_gluon_kobert.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Tokenizer&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pretrained &lt;a href=&#34;https://github.com/google/sentencepiece&#34;&gt;Sentencepiece&lt;/a&gt; tokenizer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from gluonnlp.data import SentencepieceTokenizer&#xA;&amp;gt;&amp;gt;&amp;gt; from kobert import get_tokenizer&#xA;&amp;gt;&amp;gt;&amp;gt; tok_path = get_tokenizer()&#xA;&amp;gt;&amp;gt;&amp;gt; sp  = SentencepieceTokenizer(tok_path)&#xA;&amp;gt;&amp;gt;&amp;gt; sp(&#39;í•œêµ­ì–´ ëª¨ë¸ì„ ê³µìœ í•©ë‹ˆë‹¤.&#39;)&#xA;[&#39;â–í•œêµ­&#39;, &#39;ì–´&#39;, &#39;â–ëª¨ë¸&#39;, &#39;ì„&#39;, &#39;â–ê³µìœ &#39;, &#39;í•©ë‹ˆë‹¤&#39;, &#39;.&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Subtasks&lt;/h2&gt; &#xA;&lt;h3&gt;Naver Sentiment Analysis&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dataset : &lt;a href=&#34;https://github.com/e9t/nsmc&#34;&gt;https://github.com/e9t/nsmc&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Accuracy&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/bert/raw/master/multilingual.md&#34;&gt;BERT base multilingual cased&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.875&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KoBERT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SKTBrain/KoBERT/master/logs/bert_naver_small_512_news_simple_20190624.txt&#34;&gt;0.901&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SKT-AI/KoGPT2&#34;&gt;KoGPT2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.899&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;KoBERTì™€ CRFë¡œ ë§Œë“  í•œêµ­ì–´ ê°ì²´ëª…ì¸ì‹ê¸°&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/eagle705/pytorch-bert-crf-ner&#34;&gt;https://github.com/eagle705/pytorch-bert-crf-ner&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:  SKTBrainì—ì„œ KoBERT ëª¨ë¸ì„ ê³µê°œí•´ì¤€ ë•ë¶„ì— BERT-CRF ê¸°ë°˜ ê°ì²´ëª…ì¸ì‹ê¸°ë¥¼ ì‰½ê²Œ ê°œë°œí•  ìˆ˜ ìˆì—ˆë‹¤.&#xA;len: 40, input_token:[&#39;[CLS]&#39;, &#39;â–SK&#39;, &#39;T&#39;, &#39;B&#39;, &#39;ra&#39;, &#39;in&#39;, &#39;ì—ì„œ&#39;, &#39;â–K&#39;, &#39;o&#39;, &#39;B&#39;, &#39;ER&#39;, &#39;T&#39;, &#39;â–ëª¨ë¸&#39;, &#39;ì„&#39;, &#39;â–ê³µê°œ&#39;, &#39;í•´&#39;, &#39;ì¤€&#39;, &#39;â–ë•ë¶„ì—&#39;, &#39;â–B&#39;, &#39;ER&#39;, &#39;T&#39;, &#39;-&#39;, &#39;C&#39;, &#39;R&#39;, &#39;F&#39;, &#39;â–ê¸°ë°˜&#39;, &#39;â–&#39;, &#39;ê°&#39;, &#39;ì²´&#39;, &#39;ëª…&#39;, &#39;ì¸&#39;, &#39;ì‹&#39;, &#39;ê¸°ë¥¼&#39;, &#39;â–ì‰½ê²Œ&#39;, &#39;â–ê°œë°œ&#39;, &#39;í• &#39;, &#39;â–ìˆ˜&#39;, &#39;â–ìˆì—ˆë‹¤&#39;, &#39;.&#39;, &#39;[SEP]&#39;]&#xA;len: 40, pred_ner_tag:[&#39;[CLS]&#39;, &#39;B-ORG&#39;, &#39;I-ORG&#39;, &#39;I-ORG&#39;, &#39;I-ORG&#39;, &#39;I-ORG&#39;, &#39;O&#39;, &#39;B-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;I-POH&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;[SEP]&#39;]&#xA;decoding_ner_sentence: [CLS] &amp;lt;SKTBrain:ORG&amp;gt;ì—ì„œ &amp;lt;KoBERT:POH&amp;gt; ëª¨ë¸ì„ ê³µê°œí•´ì¤€ ë•ë¶„ì— &amp;lt;BERT-CRF:POH&amp;gt; ê¸°ë°˜ ê°ì²´ëª…ì¸ì‹ê¸°ë¥¼ ì‰½ê²Œ ê°œë°œí•  ìˆ˜ ìˆì—ˆë‹¤.[SEP]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Korean Sentence BERT&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BM-K/KoSentenceBERT-SKT&#34;&gt;https://github.com/BM-K/KoSentenceBERT-SKT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cosine Pearson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cosine Spearman&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Euclidean Pearson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Euclidean Spearman&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Manhattan Pearson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Manhattan Spearman&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dot Pearson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dot Spearman&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NLl&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.48&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.81&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.90&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.22&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.81&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;STS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;80.42&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;79.64&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;77.93&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.43&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;77.92&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.44&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;76.56&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;75.83&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;STS + NLI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.81&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.47&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.68&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;77.78&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.71&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;77.83&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.75&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.22&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;v0.2.3 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;support &lt;code&gt;onnx 1.8.0&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;v0.2.2 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fix &lt;code&gt;No module named &#39;kobert.utils&#39;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;v0.2.1 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;guide default &#39;import statements&#39;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;v0.2 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;download large files from &lt;code&gt;aws s3&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;rename functions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;v0.1.2 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Guaranteed compatibility with higher versions of transformers&lt;/li&gt; &#xA;   &lt;li&gt;fix pad token index id&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;v0.1.1 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ì‚¬ì „(vocabulary)ê³¼ í† í¬ë‚˜ì´ì € í†µí•©&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;v0.1 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ì´ˆê¸° ëª¨ë¸ ë¦´ë¦¬ì¦ˆ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contacts&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;KoBERT&lt;/code&gt; ê´€ë ¨ ì´ìŠˆëŠ” &lt;a href=&#34;https://github.com/SKTBrain/KoBERT/issues&#34;&gt;ì´ê³³&lt;/a&gt;ì— ë“±ë¡í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;KoBERT&lt;/code&gt;ëŠ” &lt;code&gt;Apache-2.0&lt;/code&gt; ë¼ì´ì„ ìŠ¤ í•˜ì— ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ë° ì½”ë“œë¥¼ ì‚¬ìš©í•  ê²½ìš° ë¼ì´ì„ ìŠ¤ ë‚´ìš©ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”. ë¼ì´ì„ ìŠ¤ ì „ë¬¸ì€ &lt;code&gt;LICENSE&lt;/code&gt; íŒŒì¼ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>matheusfacure/python-causality-handbook</title>
    <updated>2022-11-25T01:33:56Z</updated>
    <id>tag:github.com,2022-11-25:/matheusfacure/python-causality-handbook</id>
    <link href="https://github.com/matheusfacure/python-causality-handbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Causal Inference for the Brave and True. A light-hearted yet rigorous approach to learning about impact estimation and causality.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Causal Inference for The Brave and True&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/matheusfacure/python-causality-handbook/master/causal-inference-for-the-brave-and-true/data/img/brave-and-true.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zenodo.org/badge/latestdoi/255903310&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/255903310.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A light-hearted yet rigorous approach to learning impact estimation and sensitivity analysis. Everything in Python and with as many memes as I could find.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/landing-page.html&#34;&gt;Check out the book here!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to read the book in Chinese, @xieliaing was very kind to make a translation:&lt;br&gt; &lt;a href=&#34;https://github.com/xieliaing/CausalInferenceIntro&#34;&gt;å› æœæ¨æ–­ï¼šä»æ¦‚å¿µåˆ°å®è·µ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to read the book in Spanish, @donelianc was very kind to make a translation:&lt;br&gt; &lt;a href=&#34;https://github.com/donelianc/introduccion-inferencia-causal&#34;&gt;Inferencia Causal para los Valientes y Verdaderos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to read it in Korean, @jsshin2019 has put up a team to make the that translation possible:&lt;br&gt; &lt;a href=&#34;https://github.com/TeamCausality/Causal-Inference-with-Python&#34;&gt;Pythonìœ¼ë¡œ í•˜ëŠ” ì¸ê³¼ì¶”ë¡  : ê°œë…ë¶€í„° ì‹¤ìŠµê¹Œì§€&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Also, some really kind folks (@vietecon, @dinhtrang24 and @anhpham52) also translated this content into Vietnamese:&lt;br&gt; &lt;a href=&#34;https://github.com/vietecon/NhanQuaPython&#34;&gt;NhÃ¢n quáº£ Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;I like to think of this entire series as a tribute to Joshua Angrist, Alberto Abadie and Christopher Walters for their amazing Econometrics class. Most of the ideas here are taken from their classes at the American Economic Association. Watching them is what is keeping me sane during this tough year of 2020.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/cont-ed/2017-webcasts&#34;&gt;Cross-Section Econometrics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/conference/cont-ed/2020-webcasts&#34;&gt;Mastering Mostly Harmless Econometrics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I&#39;ll also like to reference the amazing books from Angrist. They have shown me that Econometrics, or &#39;Metrics as they call it, is not only extremely useful but also profoundly fun.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mostlyharmlesseconometrics.com/&#34;&gt;Mostly Harmless Econometrics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.masteringmetrics.com/&#34;&gt;Mastering &#39;Metrics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;My final reference is Miguel Hernan and Jamie Robins&#39; book. It has been my trustworthy companion in the most thorny causal questions I had to answer.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/&#34;&gt;Causal Inference Book&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Support This Work&lt;/h2&gt; &#xA;&lt;p&gt;Causal Inference for the Brave and True is an open-source material on mostly econometrics and the statistics of science. It uses only free software, based in Python. Its goal is to be accessible, not only financially, but intellectual. I&#39;ve tried my best to keep the writing entertaining while maintaining the necessary scientific rigor.&lt;br&gt; Recently, the book has been translated into Vietnamese by some very nice folks from the London School of Economics. Although I was thrilled by it, the translation process also revealed the insufiencies of my english. For this reason, I&#39;m looking for funds to hire professional proofreading services and sort that problem once and for all. To help me with that, go to &lt;a href=&#34;https://www.patreon.com/causal_inference_for_the_brave_and_true&#34;&gt;https://www.patreon.com/causal_inference_for_the_brave_and_true&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>