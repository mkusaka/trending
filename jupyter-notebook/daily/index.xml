<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-17T01:38:31Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Guitaricet/peft_pretraining</title>
    <updated>2023-07-17T01:38:31Z</updated>
    <id>tag:github.com,2023-07-17:/Guitaricet/peft_pretraining</id>
    <link href="https://github.com/Guitaricet/peft_pretraining" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ReLoRA -- PEFT Pretraining&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Official code for Stack More Layers Differently: High-Rank Training Through Low-Rank Updates &lt;a href=&#34;https://arxiv.org/abs/2307.05695&#34;&gt;https://arxiv.org/abs/2307.05695&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;img width=&#34;813&#34; alt=&#34;ReLoRA&#34; src=&#34;https://github.com/Guitaricet/peft_pretraining/assets/2821124/41415bd0-b39f-4f2c-9bbd-5fd6555e87a7&#34;&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;All requirements are listed in &lt;code&gt;requirements.txt&lt;/code&gt; and kept up-to-date.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd peft_pretraining&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To train a model using ReLoRA, first, perform a warmup through regular training.&lt;/p&gt; &#xA;&lt;p&gt;Train language model with PEFT&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc-per-node &amp;lt;N_GPUS&amp;gt; torchrun_main.py \&#xA;    --model_config configs/llama_250m.json \&#xA;    --batch_size 24 \&#xA;    --total_batch_size 1152 \&#xA;    --lr 5e-4 \&#xA;    --max_length 512 \&#xA;    --tags warm_start_250M \&#xA;    --save_every 1000 \&#xA;    --num_training_steps 20000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Reproducibility note:&lt;/strong&gt; The way we ran the experiments in the paper was by specifying full num_training_steps, including both the warmup and the ReLoRA training, and stopping it after the desired number of steps was completed. Providing only the number of training steps should work too. The only difference will be the LR schedule during the warmup period.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;When you have a warmed-up network checkpoint, run the script with ReLoRA enabled. Note that we use a larger LR during the ReLoRA stage.&lt;/p&gt; &#xA;&lt;p&gt;Train without PEFT&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc-per-node &amp;lt;N_GPUS&amp;gt; torchrun_main.py \&#xA;    --model_config configs/llama_250m.json \&#xA;    --batch_size 24 \&#xA;    --total_batch_size 1152 \&#xA;    --lr 1e-3 \&#xA;    --max_length 512 \&#xA;    --use_peft \&#xA;    --relora 5000 \&#xA;    --cycle_length 5000 \&#xA;    --restart_warmup_steps 100 \&#xA;    --scheduler cosine_restarts \&#xA;    --warmup_steps 500 \&#xA;    --reset_optimizer_on_relora True \&#xA;    --num_training_steps 20000 \&#xA;    --save_every 5000 \&#xA;    --eval_every 5000 \&#xA;    --continue_from checkpoints/llama_250m-2023-06-09-11-29-56/model_5000 \&#xA;    --tags relora_250M&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Note on batch sizes&lt;/h2&gt; &#xA;&lt;p&gt;To minimize the pain with multi-GPU setups, we recommend avoiding using &lt;code&gt;--gradient_accumulation&lt;/code&gt; option directly. Instead, specify &lt;code&gt;--total_batch_size&lt;/code&gt; and allow the script to figure out the gradient accumulation option based on &lt;code&gt;--batch_size&lt;/code&gt; and the number of GPUs used.&lt;/p&gt; &#xA;&lt;h2&gt;Relora&lt;/h2&gt; &#xA;&lt;p&gt;Relora integrates existing LoRA parameters into the main network and resets them. In principle, such an approach can be more flexible than LoRA, but you need to be careful with&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Optimizer states&lt;/li&gt; &#xA; &lt;li&gt;Learning rate schedule during and right after the reset&lt;/li&gt; &#xA; &lt;li&gt;How frequently you reset&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Reset frequency is determined by &lt;code&gt;--relora&lt;/code&gt; parameter (in the number of update steps, not global steps). Optimizer reset options are:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;--reset_optimizer_on_relora&#34;, default=True, type=lambda x: x.lower() == &#34;true&#34;&#xA;&#34;--optimizer_random_pruning&#34;, default=False, type=float&#xA;&#34;--optimizer_magnitude_pruning&#34;, default=False, type=float&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We found that using &lt;code&gt;--optimizer_magnitude_pruning 0.9&lt;/code&gt; or plain &lt;code&gt;--reset_optimizer_on_relora&lt;/code&gt; usually performs well. Note that &lt;code&gt;--reset_optimizer_on_relora is True by default&lt;/code&gt; and you need to provide &lt;code&gt;--reset_optimizer_on_relora False --optimizer_magnitude_pruning 0.9&lt;/code&gt; if you want to do magnitude pruning.&lt;/p&gt; &#xA;&lt;p&gt;ReLoRA currently only supports cosine decay learning rate scheduler. Specifically &lt;code&gt;cosine_restarts&lt;/code&gt; that works in cyclical mode that repeats the warmup every &lt;code&gt;--cycle_length&lt;/code&gt; update steps.&lt;/p&gt; &#xA;&lt;h2&gt;Warm starts&lt;/h2&gt; &#xA;&lt;p&gt;You can start LoRa from a partially trained checkpoint. To do that, provide &lt;code&gt;--continue_from&lt;/code&gt; option. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun torchrun_main.py ... &amp;lt;other options&amp;gt; .. --continue_from checkpoints/llama_1b-2023-05-05-20-12-43/model_1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Distributed training&lt;/h2&gt; &#xA;&lt;p&gt;We support single-node distributed training using vanilla PyTorch DDP. | &lt;code&gt;main.py&lt;/code&gt; script does not have all features required for relora and will be deleted soon. We recommend to use &lt;code&gt;torchrun --nproc-per-node 1&lt;/code&gt; for a single-GPU training.&lt;/p&gt; &#xA;&lt;p&gt;An example of using torchrun&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc-per-node 8 torchrun_main.py \&#xA;    --model_config configs/llama_35m.json \&#xA;    --use_peft \&#xA;    --lora_r 128 \&#xA;    --relora 500 \&#xA;    --cycle_length 500 \&#xA;    --warmup_steps 250 \&#xA;    --reset_optimizer_on_relora False \&#xA;    --lr 0.001 \&#xA;    --batch_size 60 \&#xA;    --total_batch_size 480 \&#xA;    --num_training_steps 5000 \&#xA;    --save_every 5000 \&#xA;    --dtype bfloat16 \&#xA;    --tags relora_debug,example&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where &lt;code&gt;--nproc-per-node&lt;/code&gt; is the nubmer of GPUs you are using.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lialin2023stack,&#xA;    title={Stack More Layers Differently: High-Rank Training Through Low-Rank Updates},&#xA;    author={Vladislav Lialin and Namrata Shivagunde and Sherin Muckatira and Anna Rumshisky},&#xA;    year={2023},&#xA;    eprint={2307.05695},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>daenuprobst/molzip</title>
    <updated>2023-07-17T01:38:31Z</updated>
    <id>tag:github.com,2023-07-17:/daenuprobst/molzip</id>
    <link href="https://github.com/daenuprobst/molzip" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The gzip classification method implemented for molecule classification.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://zenodo.org/badge/latestdoi/666335439&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/666335439.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Parameter-Free Molecular Classification and Regression with Gzip&lt;/h1&gt; &#xA;&lt;h3&gt;Daniel Probst&lt;sup&gt;1&lt;/sup&gt;, You?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Institute of Electrical and Micro Engineering, LTS2, EPFL&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;TBD&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;The classification of a molecule on a wide variety of physicochemical and pharmakological properties, such as solubility, efficacy against specific diseases, or toxicity, has become a task of high interest in chemistry and biology. With the rise of deep learning during tha past decade, molecular classification has increasingly be carried out by ever-larger models, with mixed results. The newly published parameter-free text classification approach that makes use of Gzip compression has shown excellent performance compared to deep learning architectures, such as transformers, on benchmark data sets.[^1] As the SMILES string encoding of molecular graphs has been shown to be a well-performing molecular representation for applying NLP methods, such as transformers, to chemical tasks including molecular classification, a comparison with the Gzip-based classification method is also relevant in the context of molecular classification.&lt;/p&gt; &#xA;&lt;h2&gt;Methods&lt;/h2&gt; &#xA;&lt;p&gt;The Gzip-based classifier introduced in this article has been adapted from the implementation presented by Jiang et al. and differs in three points: (1) as, as the authors have noted, the Gzip-based classification method has a relatively high time complexity, multiprocessing has been added; (2) multi-task classification has been added; and (3) a class weighing scheme has been implemented to account for unbalanced data. Furthermore, the capability to preprocess data, in this case the SMILES strings, has been added to the calling program.&lt;/p&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;The current results are presented in the table below. Data sets with random splits were ran a total of four times.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data Set&lt;/th&gt; &#xA;   &lt;th&gt;Split&lt;/th&gt; &#xA;   &lt;th&gt;AUROC (Valid)&lt;/th&gt; &#xA;   &lt;th&gt;F1 (Valid)&lt;/th&gt; &#xA;   &lt;th&gt;AUROC (Test)&lt;/th&gt; &#xA;   &lt;th&gt;F1 (Test)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bbbp&lt;/td&gt; &#xA;   &lt;td&gt;scaffold&lt;/td&gt; &#xA;   &lt;td&gt;0.891 +/- 0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.902 +/- 0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.679 +/- 0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.686 +/- 0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bace_classification&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.793 +/- 0.038&lt;/td&gt; &#xA;   &lt;td&gt;0.793 +/- 0.038&lt;/td&gt; &#xA;   &lt;td&gt;0.789 +/- 0.038&lt;/td&gt; &#xA;   &lt;td&gt;0.789 +/- 0.038&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;clintox&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.805 +/- 0.038&lt;/td&gt; &#xA;   &lt;td&gt;0.965 +/- 0.038&lt;/td&gt; &#xA;   &lt;td&gt;0.77 +/- 0.038&lt;/td&gt; &#xA;   &lt;td&gt;0.958 +/- 0.038&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tox21&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.6 +/- 0.007&lt;/td&gt; &#xA;   &lt;td&gt;0.308 +/- 0.007&lt;/td&gt; &#xA;   &lt;td&gt;0.599 +/- 0.007&lt;/td&gt; &#xA;   &lt;td&gt;0.303 +/- 0.007&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sider&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.56 +/- 0.007&lt;/td&gt; &#xA;   &lt;td&gt;0.788 +/- 0.007&lt;/td&gt; &#xA;   &lt;td&gt;0.563 +/- 0.007&lt;/td&gt; &#xA;   &lt;td&gt;0.778 +/- 0.007&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Implementing a weighted version of the kNN algorithm does not necessary lead to better classification performance on unbalanced data sets.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data Set&lt;/th&gt; &#xA;   &lt;th&gt;Split&lt;/th&gt; &#xA;   &lt;th&gt;AUROC (Valid)&lt;/th&gt; &#xA;   &lt;th&gt;F1 (Valid)&lt;/th&gt; &#xA;   &lt;th&gt;AUROC (Test)&lt;/th&gt; &#xA;   &lt;th&gt;F1 (Test)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bbbp&lt;/td&gt; &#xA;   &lt;td&gt;scaffold&lt;/td&gt; &#xA;   &lt;td&gt;0.917 +/- 0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.917 +/- 0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.632 +/- 0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.623 +/- 0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bace_classification&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.784 +/- 0.02&lt;/td&gt; &#xA;   &lt;td&gt;0.783 +/- 0.02&lt;/td&gt; &#xA;   &lt;td&gt;0.772 +/- 0.02&lt;/td&gt; &#xA;   &lt;td&gt;0.771 +/- 0.02&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;clintox&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.944 +/- 0.02&lt;/td&gt; &#xA;   &lt;td&gt;0.91 +/- 0.02&lt;/td&gt; &#xA;   &lt;td&gt;0.884 +/- 0.02&lt;/td&gt; &#xA;   &lt;td&gt;0.905 +/- 0.02&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tox21&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.69 +/- 0.013&lt;/td&gt; &#xA;   &lt;td&gt;0.285 +/- 0.013&lt;/td&gt; &#xA;   &lt;td&gt;0.697 +/- 0.013&lt;/td&gt; &#xA;   &lt;td&gt;0.295 +/- 0.013&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sider&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.592 +/- 0.011&lt;/td&gt; &#xA;   &lt;td&gt;0.649 +/- 0.011&lt;/td&gt; &#xA;   &lt;td&gt;0.609 +/- 0.011&lt;/td&gt; &#xA;   &lt;td&gt;0.659 +/- 0.011&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hiv&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.756 +/- 0.015&lt;/td&gt; &#xA;   &lt;td&gt;0.897 +/- 0.005&lt;/td&gt; &#xA;   &lt;td&gt;0.77 +/- 0.01&lt;/td&gt; &#xA;   &lt;td&gt;0.9 +/- 0.001&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Using SECFP (ECFP-style circular substructures as SMILES) doesn&#39;t increase the classification performance of the weighted kNN.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data Set&lt;/th&gt; &#xA;   &lt;th&gt;Split&lt;/th&gt; &#xA;   &lt;th&gt;AUROC (Valid)&lt;/th&gt; &#xA;   &lt;th&gt;F1 (Valid)&lt;/th&gt; &#xA;   &lt;th&gt;AUROC (Test)&lt;/th&gt; &#xA;   &lt;th&gt;F1 (Test)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bbbp&lt;/td&gt; &#xA;   &lt;td&gt;scaffold&lt;/td&gt; &#xA;   &lt;td&gt;0.83 +/- 0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.819 +/- 0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.632 +/- 0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.627 +/- 0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bace_classification&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.833 +/- 0.015&lt;/td&gt; &#xA;   &lt;td&gt;0.829 +/- 0.015&lt;/td&gt; &#xA;   &lt;td&gt;0.826 +/- 0.015&lt;/td&gt; &#xA;   &lt;td&gt;0.821 +/- 0.015&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;clintox&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.74 +/- 0.076&lt;/td&gt; &#xA;   &lt;td&gt;0.831 +/- 0.076&lt;/td&gt; &#xA;   &lt;td&gt;0.747 +/- 0.076&lt;/td&gt; &#xA;   &lt;td&gt;0.84 +/- 0.076&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tox21&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.712 +/- 0.011&lt;/td&gt; &#xA;   &lt;td&gt;0.305 +/- 0.011&lt;/td&gt; &#xA;   &lt;td&gt;0.718 +/- 0.011&lt;/td&gt; &#xA;   &lt;td&gt;0.31 +/- 0.011&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sider&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.604 +/- 0.022&lt;/td&gt; &#xA;   &lt;td&gt;0.62 +/- 0.022&lt;/td&gt; &#xA;   &lt;td&gt;0.614 +/- 0.022&lt;/td&gt; &#xA;   &lt;td&gt;0.624 +/- 0.022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Implementing a GZip-based regressor (weighted kNN, k=10) shows performance comparable to baseline performance of common ML implementations from MoleculeNet (&lt;a href=&#34;https://moleculenet.org/full-results&#34;&gt;https://moleculenet.org/full-results&lt;/a&gt;). Interestingly there are improvements when the SMILES are tokenised.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data Set&lt;/th&gt; &#xA;   &lt;th&gt;Split&lt;/th&gt; &#xA;   &lt;th&gt;AUROC/RMSE (Valid)&lt;/th&gt; &#xA;   &lt;th&gt;F1/MAE (Valid)&lt;/th&gt; &#xA;   &lt;th&gt;AUROC/RMSE (Test)&lt;/th&gt; &#xA;   &lt;th&gt;F1/MAE (Test)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;freesolv&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.514 +/- 0.098&lt;/td&gt; &#xA;   &lt;td&gt;0.295 +/- 0.098&lt;/td&gt; &#xA;   &lt;td&gt;0.512 +/- 0.098&lt;/td&gt; &#xA;   &lt;td&gt;0.283 +/- 0.098&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;delaney&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;1.221 +/- 0.084&lt;/td&gt; &#xA;   &lt;td&gt;0.921 +/- 0.084&lt;/td&gt; &#xA;   &lt;td&gt;1.155 +/- 0.084&lt;/td&gt; &#xA;   &lt;td&gt;0.894 +/- 0.084&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lipo&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.914 +/- 0.034&lt;/td&gt; &#xA;   &lt;td&gt;0.722 +/- 0.034&lt;/td&gt; &#xA;   &lt;td&gt;0.898 +/- 0.034&lt;/td&gt; &#xA;   &lt;td&gt;0.714 +/- 0.034&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The classifier is also able to classify raw reaction SMILES from the Schneider50k data set (no class weighting).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data Set&lt;/th&gt; &#xA;   &lt;th&gt;Split&lt;/th&gt; &#xA;   &lt;th&gt;AUROC/RMSE (Valid)&lt;/th&gt; &#xA;   &lt;th&gt;F1/MAE (Valid)&lt;/th&gt; &#xA;   &lt;th&gt;AUROC/RMSE (Test)&lt;/th&gt; &#xA;   &lt;th&gt;F1/MAE (Test)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;schneider&lt;/td&gt; &#xA;   &lt;td&gt;random&lt;/td&gt; &#xA;   &lt;td&gt;0.0 +/- 0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.801 +/- 0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.0 +/- 0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.801 +/- 0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Discussion&lt;/h2&gt; &#xA;&lt;p&gt;TBD&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;[^1] &lt;a href=&#34;https://arxiv.org/abs/2212.09410&#34;&gt;https://arxiv.org/abs/2212.09410&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What is this?&lt;/h1&gt; &#xA;&lt;p&gt;This is an experiment for a small open source manuscript/article that aims to validate and evaluate the performance of compression-based molecular classification using Gzip. If you want to join/help out, leave a message or a pull request that includes your name and, if available, your affiliation.&lt;/p&gt;</summary>
  </entry>
</feed>