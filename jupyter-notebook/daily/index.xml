<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-07T01:40:05Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>smazzanti/are_you_still_using_elbow_method</title>
    <updated>2023-02-07T01:40:05Z</updated>
    <id>tag:github.com,2023-02-07:/smazzanti/are_you_still_using_elbow_method</id>
    <link href="https://github.com/smazzanti/are_you_still_using_elbow_method" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>thuml/Autoformer</title>
    <updated>2023-02-07T01:40:05Z</updated>
    <id>tag:github.com,2023-02-07:/thuml/Autoformer</id>
    <link href="https://github.com/thuml/Autoformer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;About Code release for &#34;Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting&#34; (NeurIPS 2021), https://arxiv.org/abs/2106.13008&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Autoformer (NeurIPS 2021)&lt;/h1&gt; &#xA;&lt;p&gt;Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting&lt;/p&gt; &#xA;&lt;p&gt;Time series forecasting is a critical demand for real applications. Enlighted by the classic time series analysis and stochastic process theory, we propose the Autoformer as a general series forecasting model [&lt;a href=&#34;https://arxiv.org/abs/2106.13008&#34;&gt;paper&lt;/a&gt;]. &lt;strong&gt;Autoformer goes beyond the Transformer family and achieves the series-wise connection for the first time.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In long-term forecasting, Autoformer achieves SOTA, with a &lt;strong&gt;38% relative improvement&lt;/strong&gt; on six benchmarks, covering five practical applications: &lt;strong&gt;energy, traffic, economics, weather and disease&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;ðŸš©&lt;/span&gt;&lt;strong&gt;News&lt;/strong&gt; (2022.02-2022.03) Autoformer has been deployed in &lt;a href=&#34;https://en.wikipedia.org/wiki/2022_Winter_Olympics&#34;&gt;2022 Winter Olympics&lt;/a&gt; to provide weather forecasting for competition venues, including wind speed and temperature.&lt;/p&gt; &#xA;&lt;h2&gt;Autoformer vs. Transformers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Deep decomposition architecture&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We renovate the Transformer as a deep decomposition architecture, which can progressively decompose the trend and seasonal components during the forecasting process.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;.\pic\Autoformer.png&#34; height=&#34;250&#34; alt=&#34;&#34; align=&#34;center&#34;&gt; &lt;br&gt;&lt;br&gt; &lt;b&gt;Figure 1.&lt;/b&gt; Overall architecture of Autoformer. &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Series-wise Auto-Correlation mechanism&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Inspired by the stochastic process theory, we design the Auto-Correlation mechanism, which can discover period-based dependencies and aggregate the information at the series level. This empowers the model with inherent log-linear complexity. This series-wise connection contrasts clearly from the previous self-attention family.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;.\pic\Auto-Correlation.png&#34; height=&#34;250&#34; alt=&#34;&#34; align=&#34;center&#34;&gt; &lt;br&gt;&lt;br&gt; &lt;b&gt;Figure 2.&lt;/b&gt; Auto-Correlation mechansim. &lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Python 3.6, PyTorch 1.9.0.&lt;/li&gt; &#xA; &lt;li&gt;Download data. You can obtain all the six benchmarks from &lt;a href=&#34;https://cloud.tsinghua.edu.cn/d/e1ccfff39ad541908bae/&#34;&gt;Tsinghua Cloud&lt;/a&gt; or &lt;a href=&#34;https://drive.google.com/drive/folders/1ZOYpTUa82_jCcxIdTmyr0LXQfvaM9vIy?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;. &lt;strong&gt;All the datasets are well pre-processed&lt;/strong&gt; and can be used easily.&lt;/li&gt; &#xA; &lt;li&gt;Train the model. We provide the experiment scripts of all benchmarks under the folder &lt;code&gt;./scripts&lt;/code&gt;. You can reproduce the experiment results by:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./scripts/ETT_script/Autoformer_ETTm1.sh&#xA;bash ./scripts/ECL_script/Autoformer.sh&#xA;bash ./scripts/Exchange_script/Autoformer.sh&#xA;bash ./scripts/Traffic_script/Autoformer.sh&#xA;bash ./scripts/Weather_script/Autoformer.sh&#xA;bash ./scripts/ILI_script/Autoformer.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Special-designed implementation&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Speedup Auto-Correlation:&lt;/strong&gt; We built the Auto-Correlation mechanism as a batch-normalization-style block to make it more memory-access friendly. See the &lt;a href=&#34;https://arxiv.org/abs/2106.13008&#34;&gt;paper&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Without the position embedding:&lt;/strong&gt; Since the series-wise connection will inherently keep the sequential information, Autoformer does not need the position embedding, which is different from Transformers.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Reproduce with Docker&lt;/h3&gt; &#xA;&lt;p&gt;To easily reproduce the results using Docker, conda and Make, you can follow the next steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Initialize the docker image using: &lt;code&gt;make init&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download the datasets using: &lt;code&gt;make get_dataset&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run each script in &lt;code&gt;scripts/&lt;/code&gt; using &lt;code&gt;make run_module module=&#34;bash scripts/ETT_script/Autoformer_ETTm1.sh&#34;&lt;/code&gt; for each script.&lt;/li&gt; &#xA; &lt;li&gt;Alternatively, run all the scripts at once:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;for file in `ls scripts`; do make run_module module=&#34;bash scripts/$script&#34;; done&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;A Simple Example&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;code&gt;predict.ipynb&lt;/code&gt; for workflow (in Chinese).&lt;/p&gt; &#xA;&lt;h2&gt;Main Results&lt;/h2&gt; &#xA;&lt;p&gt;We experiment on six benchmarks, covering five main-stream applications. We compare our model with ten baselines, including Informer, N-BEATS, etc. Generally, for the long-term forecasting setting, Autoformer achieves SOTA, with a &lt;strong&gt;38% relative improvement&lt;/strong&gt; over previous baselines.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;.\pic\results.png&#34; height=&#34;550&#34; alt=&#34;&#34; align=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Baselines&lt;/h2&gt; &#xA;&lt;p&gt;We will keep adding series forecasting models to expand this repo:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Autoformer&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Informer&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Transformer&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Reformer&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; LogTrans&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; N-BEATS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repo useful, please cite our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{wu2021autoformer,&#xA;  title={Autoformer: Decomposition Transformers with {Auto-Correlation} for Long-Term Series Forecasting},&#xA;  author={Haixu Wu and Jiehui Xu and Jianmin Wang and Mingsheng Long},&#xA;  booktitle={Advances in Neural Information Processing Systems},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or want to use the code, please contact &lt;a href=&#34;mailto:whx20@mails.tsinghua.edu.cn&#34;&gt;whx20@mails.tsinghua.edu.cn&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate the following github repos a lot for their valuable code base or datasets:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhouhaoyi/Informer2020&#34;&gt;https://github.com/zhouhaoyi/Informer2020&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhouhaoyi/ETDataset&#34;&gt;https://github.com/zhouhaoyi/ETDataset&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/laiguokun/multivariate-time-series-data&#34;&gt;https://github.com/laiguokun/multivariate-time-series-data&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>