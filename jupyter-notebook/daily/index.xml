<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-03T02:24:39Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>isl-org/ZoeDepth</title>
    <updated>2023-03-03T02:24:39Z</updated>
    <id>tag:github.com,2023-03-03:/isl-org/ZoeDepth</id>
    <link href="https://github.com/isl-org/ZoeDepth" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Metric depth estimation from a single image&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;ZoeDepth: Combining relative and metric depth&lt;/strong&gt; (Official implementation) &#xA; &lt;!-- omit in toc --&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/isl-org/ZoeDepth&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Collab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-green.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/PyTorch_v1.10.1-EE4C2C?&amp;amp;logo=pytorch&amp;amp;logoColor=white&#34; alt=&#34;PyTorch&#34;&gt; &lt;a href=&#34;https://paperswithcode.com/sota/monocular-depth-estimation-on-nyu-depth-v2?p=zoedepth-zero-shot-transfer-by-combining&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/zoedepth-zero-shot-transfer-by-combining/monocular-depth-estimation-on-nyu-depth-v2&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.12288&#34;&gt;ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;h5&gt;&lt;a href=&#34;https://shariqfarooq123.github.io&#34;&gt;Shariq Farooq Bhat&lt;/a&gt;, &lt;a href=&#34;https://www.researchgate.net/profile/Reiner-Birkl&#34;&gt;Reiner Birkl&lt;/a&gt;, &lt;a href=&#34;https://dwofk.github.io/&#34;&gt;Diana Wofk&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;, &lt;a href=&#34;https://matthias.pw/&#34;&gt;Matthias Müller&lt;/a&gt;&lt;/h5&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.12288&#34;&gt;[Paper]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/assets/zoedepth-teaser.png&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt; &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/#usage&#34;&gt;&lt;strong&gt;Usage&lt;/strong&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/#using-torch-hub&#34;&gt;Using torch hub&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/#using-local-copy&#34;&gt;Using local copy&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/#using-local-torch-hub&#34;&gt;Using local torch hub&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/#or-load-the-models-manually&#34;&gt;or load the models manually&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/#using-zoed-models-to-predict-depth&#34;&gt;Using ZoeD models to predict depth&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/#environment-setup&#34;&gt;&lt;strong&gt;Environment setup&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/#sanity-checks-recommended&#34;&gt;&lt;strong&gt;Sanity checks&lt;/strong&gt; (Recommended)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/#model-files&#34;&gt;Model files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/#evaluation&#34;&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/#evaluating-offical-models&#34;&gt;Evaluating offical models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/#evaluating-local-checkpoint&#34;&gt;Evaluating local checkpoint&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/#training&#34;&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/isl-org/ZoeDepth/main/#citation&#34;&gt;&lt;strong&gt;Citation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Usage&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;It is recommended to fetch the latest &lt;a href=&#34;https://github.com/isl-org/MiDaS&#34;&gt;MiDaS repo&lt;/a&gt; via torch hub before proceeding:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;torch.hub.help(&#34;intel-isl/MiDaS&#34;, &#34;DPT_BEiT_L_384&#34;, force_reload=True)  # Triggers fresh download of MiDaS repo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;strong&gt;ZoeDepth models&lt;/strong&gt; &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;Using torch hub&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;repo = &#34;isl-org/ZoeDepth&#34;&#xA;# Zoe_N&#xA;model_zoe_n = torch.hub.load(repo, &#34;ZoeD_N&#34;, pretrained=True)&#xA;&#xA;# Zoe_K&#xA;model_zoe_k = torch.hub.load(repo, &#34;ZoeD_K&#34;, pretrained=True)&#xA;&#xA;# Zoe_NK&#xA;model_zoe_nk = torch.hub.load(repo, &#34;ZoeD_NK&#34;, pretrained=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using local copy&lt;/h3&gt; &#xA;&lt;p&gt;Clone this repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/isl-org/ZoeDepth.git &amp;amp;&amp;amp; cd ZoeDepth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using local torch hub&lt;/h4&gt; &#xA;&lt;p&gt;You can use local source for torch hub to load the ZoeDepth models, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;# Zoe_N&#xA;model_zoe_n = torch.hub.load(&#34;.&#34;, &#34;ZoeD_N&#34;, source=&#34;local&#34; pretrained=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;or load the models manually&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from zoedepth.models.builder import build_model&#xA;from zoedepth.utils.config import get_config&#xA;&#xA;# ZoeD_N&#xA;conf = get_config(&#34;zoedepth&#34;, &#34;infer&#34;)&#xA;model_zoe_n = build_model(conf)&#xA;&#xA;# ZoeD_K&#xA;conf = get_config(&#34;zoedepth&#34;, &#34;infer&#34;, config_version=&#34;kitti&#34;)&#xA;model_zoe_k = build_model(conf)&#xA;&#xA;# ZoeD_NK&#xA;conf = get_config(&#34;zoedepth_nk&#34;, &#34;infer&#34;)&#xA;model_zoe_nk = build_model(conf)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using ZoeD models to predict depth&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;##### sample prediction&#xA;DEVICE = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;zoe = model_zoe_n.to(DEVICE)&#xA;&#xA;&#xA;# Local file&#xA;from PIL import Image&#xA;image = Image.open(&#34;/path/to/image.jpg&#34;).convert(&#34;RGB&#34;)  # load&#xA;depth_numpy = zoe.infer_pil(image)  # as numpy&#xA;&#xA;depth_pil = zoe.infer_pil(image, output_type=&#34;pil&#34;)  # as 16-bit PIL Image&#xA;&#xA;depth_tensor = zoe.infer_pil(image, output_type=&#34;tensor&#34;)  # as torch tensor&#xA;&#xA;&#xA;&#xA;# Tensor &#xA;from zoedepth.utils.misc import pil_to_batched_tensor&#xA;X = pil_to_batched_tensor(image).to(DEVICE)&#xA;depth_tensor = zoe.infer(X)&#xA;&#xA;&#xA;&#xA;# From URL&#xA;from zoedepth.utils.misc import get_image_from_url&#xA;&#xA;# Example URL&#xA;URL = &#34;https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS4W8H_Nxk_rs3Vje_zj6mglPOH7bnPhQitBH8WkqjlqQVotdtDEG37BsnGofME3_u6lDk&amp;amp;usqp=CAU&#34;&#xA;&#xA;&#xA;image = get_image_from_url(URL)  # fetch&#xA;depth = zoe.infer_pil(image)&#xA;&#xA;# Save raw&#xA;from zoedepth.utils.misc import save_raw_16bit&#xA;fpath = &#34;/path/to/output.png&#34;&#xA;save_raw_16bit(depth, fpath)&#xA;&#xA;# Colorize output&#xA;from zoedepth.utils.misc import colorize&#xA;&#xA;colored = colorize(depth)&#xA;&#xA;# save colored output&#xA;fpath_colored = &#34;/path/to/output_colored.png&#34;&#xA;Image.fromarray(colored).save(fpath_colored)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Environment setup&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The project depends on :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;pytorch&lt;/a&gt; (Main framework)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://timm.fast.ai/&#34;&gt;timm&lt;/a&gt; (Backbone helper for MiDaS)&lt;/li&gt; &#xA; &lt;li&gt;pillow, matplotlib, scipy, h5py, opencv (utilities)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Install environment using &lt;code&gt;environment.yml&lt;/code&gt; :&lt;/p&gt; &#xA;&lt;p&gt;Using &lt;a href=&#34;https://github.com/mamba-org/mamba&#34;&gt;mamba&lt;/a&gt; (fastest):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mamba env create -n zoe --file environment.yml&#xA;mamba activate zoe&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using conda :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -n zoe --file environment.yml&#xA;conda activate zoe&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Sanity checks&lt;/strong&gt; (Recommended)&lt;/h2&gt; &#xA;&lt;p&gt;Check if models can be loaded:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python sanity_hub.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Try a demo prediction pipeline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python sanity.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will save a file &lt;code&gt;pred.png&lt;/code&gt; in the root folder, showing RGB and corresponding predicted depth side-by-side.&lt;/p&gt; &#xA;&lt;h2&gt;Model files&lt;/h2&gt; &#xA;&lt;p&gt;Models are defined under &lt;code&gt;models/&lt;/code&gt; folder, with &lt;code&gt;models/&amp;lt;model_name&amp;gt;_&amp;lt;version&amp;gt;.py&lt;/code&gt; containing model definitions and &lt;code&gt;models/config_&amp;lt;model_name&amp;gt;.json&lt;/code&gt; containing configuration.&lt;/p&gt; &#xA;&lt;p&gt;Single metric head models (Zoe_N and Zoe_K from the paper) have the common definition and are defined under &lt;code&gt;models/zoedepth&lt;/code&gt; while as the multi-headed model (Zoe_NK) is defined under &lt;code&gt;models/zoedepth_nk&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Download the required dataset and change the &lt;code&gt;DATASETS_CONFIG&lt;/code&gt; dictionary in &lt;code&gt;utils/config.py&lt;/code&gt; accordingly.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluating offical models&lt;/h3&gt; &#xA;&lt;p&gt;On NYU-Depth-v2 for example:&lt;/p&gt; &#xA;&lt;p&gt;For ZoeD_N:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python evaluate.py -m zoedepth -d nyu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For ZoeD_NK:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python evaluate.py -m zoedepth_nk -d nyu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluating local checkpoint&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python evaluate.py -m zoedepth --pretrained_resource=&#34;local::/path/to/local/ckpt.pt&#34; -d nyu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pretrained resources are prefixed with &lt;code&gt;url::&lt;/code&gt; to indicate weights should be fetched from a url, or &lt;code&gt;local::&lt;/code&gt; to indicate path is a local file. Refer to &lt;code&gt;models/model_io.py&lt;/code&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;The dataset name should match the corresponding key in &lt;code&gt;utils.config.DATASETS_CONFIG&lt;/code&gt; .&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Download training datasets as per instructions given &lt;a href=&#34;https://github.com/cleinc/bts/tree/master/pytorch#nyu-depvh-v2&#34;&gt;here&lt;/a&gt;. Then for training a single head model on NYU-Depth-v2 :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_mono.py -m zoedepth --pretrained_resource=&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For training the Zoe-NK model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_mix.py -m zoedepth_nk --pretrained_resource=&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Citation&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{https://doi.org/10.48550/arxiv.2302.12288,&#xA;  doi = {10.48550/ARXIV.2302.12288},&#xA;  &#xA;  url = {https://arxiv.org/abs/2302.12288},&#xA;  &#xA;  author = {Bhat, Shariq Farooq and Birkl, Reiner and Wofk, Diana and Wonka, Peter and Müller, Matthias},&#xA;  &#xA;  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},&#xA;  &#xA;  title = {ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth},&#xA;  &#xA;  publisher = {arXiv},&#xA;  &#xA;  year = {2023},&#xA;  &#xA;  copyright = {arXiv.org perpetual, non-exclusive license}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ICCC-Platform/Smart-Meter-Anomaly-Detection</title>
    <updated>2023-03-03T02:24:39Z</updated>
    <id>tag:github.com,2023-03-03:/ICCC-Platform/Smart-Meter-Anomaly-Detection</id>
    <link href="https://github.com/ICCC-Platform/Smart-Meter-Anomaly-Detection" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains model to detect anomaly in smart meter data&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>ICCC-Platform/Smart-Meter-Outage-Prediction</title>
    <updated>2023-03-03T02:24:39Z</updated>
    <id>tag:github.com,2023-03-03:/ICCC-Platform/Smart-Meter-Outage-Prediction</id>
    <link href="https://github.com/ICCC-Platform/Smart-Meter-Outage-Prediction" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Smart-Meter-Outage-Prediction&lt;/h1&gt; &#xA;&lt;p&gt;---Data Source-------------------------- &lt;br&gt; The past outage history data was downloaded from Ausgrid &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.ausgrid.com.au/Industry/Our-Research/Data-to-share/Past-outage-data&#34;&gt;https://www.ausgrid.com.au/Industry/Our-Research/Data-to-share/Past-outage-data&lt;/a&gt; &lt;br&gt; The original outage data contain three months outage history records and provided as XLSX file format &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;---Experiment Environment--------------- &lt;br&gt; Python : 3.5.2 &lt;br&gt; Keras : 2.1.6 &lt;br&gt; Tensorflow : 1.13.1 &lt;br&gt; Pandas : 0.24.2 &lt;br&gt; Scikit-learn : 0.21.3 &lt;br&gt; Imbalanced-learn : 0.5.0 &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;---Experiment Step---------------------- &lt;br&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Run xlsx_to_csv.ipynb for transfer the outage history XLSX file to a CSV file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run data_preprocess.ipynb for preprocessing the past outage data to a continuous hourly data contains non-outage and outage rows.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run ADASYN_data_augmentation.ipynb for outage data augmentation(increase more outage data to balance the dataset).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run LSTM_outage_predict_final.ipynb for training the LSTM model with the augmented dataset and test with the original dataset.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;LSTM_outage_predict_single_layer.ipynb is used to train a single LSTM layer model for comparing the experimental result to the final LSTM model.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>