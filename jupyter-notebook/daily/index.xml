<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-24T01:39:45Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mshumer/gpt-author</title>
    <updated>2023-06-24T01:39:45Z</updated>
    <id>tag:github.com,2023-06-24:/mshumer/gpt-author</id>
    <link href="https://github.com/mshumer/gpt-author" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gpt-author&lt;/h1&gt; &#xA;&lt;p&gt;This project utilizes a chain of GPT-4 and Stable Diffusion API calls to generate an original fantasy novel. Users can provide an initial prompt and enter how many chapters they&#39;d like it to be, and the AI then generates an entire novel, outputting an EPUB file compatible with e-book readers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A 15-chapter novel can cost as little as $4 to produce, and is written in just a few minutes.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;A few output novel examples are provided in this repo. To read one, you can download its file and view it on &lt;a href=&#34;https://www.fviewer.com/view-epub&#34;&gt;https://www.fviewer.com/view-epub&lt;/a&gt;, or install it on your Kindle, etc.&lt;/p&gt; &#xA;&lt;h2&gt;How It Works&lt;/h2&gt; &#xA;&lt;p&gt;The AI is asked to generate a list of potential plots based on a given prompt. It then selects the most engaging plot, improves upon it, and extracts a title. After that, it generates a detailed storyline with a specified number of chapters, and then tries to improve upon that storyline. Each chapter is then individually written by the AI, following the plot and taking into account the content of previous chapters. Finally, a prompt to design the cover art is generated, and the cover is created. Finally, it&#39;s all pulled together, and the novel is compiled into an EPUB file.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;You can &lt;a href=&#34;https://colab.research.google.com/drive/1er_3U7lr6m4GJ-aHE6Pgeq9KXploxp4d?usp=sharing&#34;&gt;run this project in Google Colab&lt;/a&gt; or in a local Jupyter notebook.&lt;/p&gt; &#xA;&lt;p&gt;In Google Colab, simply open the notebook, add your API keys, and run the cells in order.&lt;/p&gt; &#xA;&lt;p&gt;If you are using a local Jupyter notebook, you will need to install the necessary dependencies. You can do this by running the following command in your terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install openai ebooklib requests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the last cell of the notebook, you can customize the prompt and the number of chapters for your novel. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = &#34;Similar to Percy Jackson or Harry Potter in terms of vibes, but a different plot entirely. Set in modern day. Add some element of technology to it.&#34;&#xA;num_chapters = 20&#xA;writing_style = &#34;Clear and easily understandable, similar to a young adult novel. Highly descriptive and sometimes long-winded.&#34;&#xA;novel, title, chapters, chapter_titles = write_fantasy_novel(prompt, num_chapters, writing_style)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will generate a novel based on the given prompt with 20 chapters. Note -- prompts with less than 7 chapters tend to cause issues.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;Contributions, issues, and feature requests are welcome!&lt;/p&gt; &#xA;&lt;p&gt;Some initial ideas:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;modify it to work solely with GPT-3.5-Turbo and GPT-3.5-Turbo-16k (it will likely require some level of compression/summariztion of early chapters so we don&#39;t run out of tokens when generating later chapters).&lt;/li&gt; &#xA; &lt;li&gt;improve the system for generating the first chapter -- the better the first chapter comes out, the better the rest of the novel is&lt;/li&gt; &#xA; &lt;li&gt;improve the prompts, as they were written very quickly&lt;/li&gt; &#xA; &lt;li&gt;improve each step in the process, adding more checks, improvement generations, etc.&lt;/li&gt; &#xA; &lt;li&gt;before generating improvements, have a model call identify potential improvements to add to the prompt, which will likely improve performance significantly&lt;/li&gt; &#xA; &lt;li&gt;modify it to go beyond just fantasy, allowing users to generate other genres as well&lt;/li&gt; &#xA; &lt;li&gt;fix the issue that causes some chapters to cut off early&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is &lt;a href=&#34;https://github.com/your_username/your_repository/raw/master/LICENSE&#34;&gt;MIT&lt;/a&gt; licensed.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Matt Shumer - &lt;a href=&#34;https://twitter.com/mattshumer_&#34;&gt;@mattshumer_&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Project Link: &lt;a href=&#34;https://github.com/mshumer/gpt-author/&#34;&gt;https://github.com/mshumer/gpt-author/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>menloparklab/falcon-langchain</title>
    <updated>2023-06-24T01:39:45Z</updated>
    <id>tag:github.com,2023-06-24:/menloparklab/falcon-langchain</id>
    <link href="https://github.com/menloparklab/falcon-langchain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Falcon LLM with Chat UI using LangChain and Chainlit&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Falcon LLM with LangChain Chat UI&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the necessary files and instructions to run Falcon LLM 7b with LangChain and interact with a chat user interface using Chainlit. Follow the steps below to set up and run the chat UI.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10 or higher&lt;/li&gt; &#xA; &lt;li&gt;Operating System: macOS or Linux&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Steps to Run the Chat UI&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Fork this repository or create a code space in GitHub.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the required Python packages by running the following command in your terminal:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the project directory. You can use the &lt;code&gt;example.env&lt;/code&gt; file as a reference. Add your Hugging Face API token to the &lt;code&gt;.env&lt;/code&gt; file in the following format:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;HUGGINGFACEHUB_API_TOKEN=your_huggingface_token&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the following command in your terminal to start the chat UI:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;chainlit run app.py -w&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will launch the chat UI, allowing you to interact with the Falcon LLM model using LangChain.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Ensure that you have provided a valid Hugging Face API token in the &lt;code&gt;.env&lt;/code&gt; file, as mentioned in step 3. Without a valid token, the chat UI will not function properly.&lt;/p&gt; &#xA;&lt;p&gt;If you encounter any issues or have questions, please reach out to me on &lt;a href=&#34;https://twitter.com/MisbahSy&#34;&gt;Twitter&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Enjoy using Falcon LLM with LangChain!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>brjathu/LART</title>
    <updated>2023-06-24T01:39:45Z</updated>
    <id>tag:github.com,2023-06-24:/brjathu/LART</id>
    <link href="https://github.com/brjathu/LART" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code repository for the paper &#34;On the Benefits of 3D Pose and Tracking for Human Action Recognition&#34;, (CVPR 2023)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;On the Benefits of 3D Pose and Tracking for Human Action Recognition (CVPR 2023)&lt;/h1&gt; &#xA;&lt;p&gt;Code repository for the paper &#34;On the Benefits of 3D Pose and Tracking for Human Action Recognition&#34;. &lt;br&gt; &lt;a href=&#34;http://people.eecs.berkeley.edu/~jathushan/&#34;&gt;Jathushan Rajasegaran&lt;/a&gt;, &lt;a href=&#34;https://geopavlakos.github.io/&#34;&gt;Georgios Pavlakos&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~kanazawa/&#34;&gt;Angjoo Kanazawa&lt;/a&gt;, &lt;a href=&#34;https://feichtenhofer.github.io/&#34;&gt;Christoph Feichtenhofer&lt;/a&gt;, &lt;a href=&#34;http://people.eecs.berkeley.edu/~malik/&#34;&gt;Jitendra Malik&lt;/a&gt;. &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.01199&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2304.01199-00ff00.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://people.eecs.berkeley.edu/~jathushan/LART/&#34;&gt;&lt;img src=&#34;https://img.shields.io/website-up-down-green-red/http/shields.io.svg?sanitize=true&#34; alt=&#34;Website shields.io&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1QRLqEAePmgS41v0KQwf87G_Ss_BLhPYs?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/action-recognition-on-ava-v2-2?p=on-the-benefits-of-3d-pose-and-tracking-for&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/on-the-benefits-of-3d-pose-and-tracking-for/action-recognition-on-ava-v2-2&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This code repository provides a code implementation for our paper LART (Lagrangian Action Recognition with Tracking), with installation, training, and evaluating on datasets, and a demo code to run on any youtube videos.&lt;/p&gt; &#xA;&lt;!-- &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;./assets/imgs/teaser.png&#34; width=&#34;800&#34;&gt;&lt;/p&gt; --&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brjathu/LART/main/assets/jump.gif&#34; width=&#34;800&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;After installing the &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch 2.0&lt;/a&gt; dependency, you can install our &lt;code&gt;lart&lt;/code&gt; package directly as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/brjathu/LART&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Step-by-step instructions&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n lart python=3.10&#xA;conda activate lart&#xA;pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117&#xA;pip install -e .[demo]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If you only wants to train the model and not interested in running demo on any videos, you dont need to install packages rquired for demo code (&lt;code&gt;pip install -e .&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Demo on videos&lt;/h2&gt; &#xA;&lt;p&gt;We have provided a colab notebook to run our model on any youtube video. Please see the &lt;a href=&#34;https://colab.research.google.com/drive/1QRLqEAePmgS41v0KQwf87G_Ss_BLhPYs?usp=sharing&#34;&gt;demo notebook&lt;/a&gt; for details. Since colab has 15 Gb memory limit, the dev branch is running on half precision. If you want to run the demo on full precision, please clone the repo and run the demo locally.&lt;/p&gt; &#xA;&lt;p&gt;Our Action recogition model uses PHALP to track people in videos, and then uses the tracklets to classify actions. &lt;code&gt;pip install -e .[demo]&lt;/code&gt; will install nessory dependencies for running the demo code. Now, run &lt;code&gt;demo.py&lt;/code&gt; to reconstruct, track and recognize humans in any video. Input video source may be a video file, a folder of frames, or a youtube link:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run on video file&#xA;python scripts/demo.py video.source=&#34;assets/jump.mp4&#34;&#xA;&#xA;# Run on extracted frames&#xA;python scripts/demo.py video.source=&#34;/path/to/frames_folder/&#34;&#xA;&#xA;# Run on a youtube link (depends on pytube working properly)&#xA;python scripts/demo.py video.source=\&#39;&#34;https://www.youtube.com/watch?v=xEH_5T9jMVU&#34;\&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output directory (&lt;code&gt;./outputs&lt;/code&gt; by default) will contain a video rendering of the tracklets and a &lt;code&gt;.pkl&lt;/code&gt; file containing the tracklets with 3D pose and shape and action labels. Please see the &lt;a href=&#34;https://github.com/brjathu/PHALP&#34;&gt;PHALP&lt;/a&gt; repository for details. The model for demo, uses &lt;code&gt;MViT&lt;/code&gt; as a backend and a single person model. The demo code requires about 32 GB memory to run the slowfast code. [TODO: Demo with &lt;code&gt;Hiera&lt;/code&gt; backend.] &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training and Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;Download the datasets&lt;/h3&gt; &#xA;&lt;p&gt;We are releasing about 1.5 million tracks of people from Kinetics 400 and AVA datasets. Run the following command to download the data from dropbox and extract them to the &lt;code&gt;data&lt;/code&gt; folder. This will download preprocessed data form &lt;code&gt;ava_val&lt;/code&gt;, &lt;code&gt;ava_train&lt;/code&gt; and &lt;code&gt;kinetics_train&lt;/code&gt; datasets (preporcessed data for AVA-kinetics and multispots will be released soon). These tracjectories contains backbone features as well as ground-truth annotations and pseudo ground-truth annotations. For more details see &lt;a href=&#34;https://raw.githubusercontent.com/brjathu/LART/main/DATASET.md&#34;&gt;DATASET.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/download_data.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Train LART model&lt;/h3&gt; &#xA;&lt;p&gt;For single node training, run the following command. This will evaulate the model at every epochs and compute the mean average precision on the validation set.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# # LART full model. &#xA;python lart/train.py -m \&#xA;--config-name lart.yaml \&#xA;trainer=ddp_unused \&#xA;task_name=LART \&#xA;trainer.devices=8 \&#xA;trainer.num_nodes=1 \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluate LART model&lt;/h3&gt; &#xA;&lt;p&gt;First download the pretrained model by running the following command. This will download the pretrained model to &lt;code&gt;./logs/LART_Hiera/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/download_checkpoints.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the following command to evaluate the model on the validation set. This will compute the mean average precision on the validation set (AVA-K evaluation will be released soon). For AVA, provided checkpoint should give &lt;b&gt;45.1&lt;/b&gt; mAP.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# # LART full model. &#xA;python lart/train.py -m \&#xA;--config-name lart.yaml \&#xA;trainer=ddp_unused \&#xA;task_name=LART_eval \&#xA;train=False \&#xA;trainer.devices=8 \&#xA;trainer.num_nodes=1 \&#xA;configs.test_type=track.fullframe@avg.6 \&#xA;configs.weights_path=logs/LART_Hiera/0/checkpoints/epoch_002-EMA.ckpt \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please specify a different &lt;code&gt;configs.weights_path&lt;/code&gt; to evaluate your own trained model. However every checkpoint will be evaluated during training, and results will be saved to &lt;code&gt;./logs/&amp;lt;MODEL_NAME&amp;gt;/0/results/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Results&lt;/h3&gt; &#xA;&lt;p&gt;The following table shows the performance of our model on AVA-2.2 dataset.&lt;/p&gt; &#xA;&lt;!-- &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;./assets/results_june_19.png&#34; width=50%&gt;&lt;/p&gt; --&gt; &#xA;&lt;!-- &#xA;| Model | Pretrain | mAP |&#xA;| --- | --- | --- |&#xA;| SlowFast R101, 8×8 | K400 | 23.8 |&#xA;| MViTv1-B, 64×3 | K400 | 27.3 |&#xA;| SlowFast 16×8 +NL | K600 | 27.5 |&#xA;| X3D-XL | K600 | 27.4 |&#xA;| MViTv1-B-24, 32×3 | K600 | 28.7 |&#xA;| Object Transformer | K600 | 31.0 |&#xA;| ACAR R101, 8×8 +NL | K600 | 31.4 |&#xA;| ACAR R101, 8×8 +NL | K700 | 33.3 |&#xA;| MViT-L↑312, 40×3 | IN-21K+K400 | 31.6 |&#xA;| MaskFeat | K400 | 37.5 |&#xA;| MaskFeat | K600 | 38.8 |&#xA;| Video MAE | K600 | 39.3 |&#xA;| Video MAE | K400 | 39.5 |&#xA;| Hiera | K700 | 42.3 |&#xA;| MethodnameB - MViT | K400 | 42.6 |&#xA;| MethodnameB - Hiera | K400 | 45.1 | --&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Pretrain&lt;/th&gt; &#xA;   &lt;th&gt;mAP&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SlowFast R101, 8×8&lt;/td&gt; &#xA;   &lt;td&gt;K400&lt;/td&gt; &#xA;   &lt;td&gt;23.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SlowFast 16×8 +NL&lt;/td&gt; &#xA;   &lt;td&gt;K600&lt;/td&gt; &#xA;   &lt;td&gt;27.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;X3D-XL&lt;/td&gt; &#xA;   &lt;td&gt;K600&lt;/td&gt; &#xA;   &lt;td&gt;27.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MViTv1-B-24, 32×3&lt;/td&gt; &#xA;   &lt;td&gt;K600&lt;/td&gt; &#xA;   &lt;td&gt;28.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Object Transformer&lt;/td&gt; &#xA;   &lt;td&gt;K600&lt;/td&gt; &#xA;   &lt;td&gt;31.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ACAR R101, 8×8 +NL&lt;/td&gt; &#xA;   &lt;td&gt;K700&lt;/td&gt; &#xA;   &lt;td&gt;33.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MViT-L↑312, 40×3&lt;/td&gt; &#xA;   &lt;td&gt;IN-21K+K400&lt;/td&gt; &#xA;   &lt;td&gt;31.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaskFeat&lt;/td&gt; &#xA;   &lt;td&gt;K600&lt;/td&gt; &#xA;   &lt;td&gt;38.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Video MAE&lt;/td&gt; &#xA;   &lt;td&gt;K400&lt;/td&gt; &#xA;   &lt;td&gt;39.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hiera&lt;/td&gt; &#xA;   &lt;td&gt;K700&lt;/td&gt; &#xA;   &lt;td&gt;43.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LART - MViT&lt;/td&gt; &#xA;   &lt;td&gt;K400&lt;/td&gt; &#xA;   &lt;td&gt;42.3 (+2.8)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LART - Hiera&lt;/td&gt; &#xA;   &lt;td&gt;K400&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;45.1 (+2.5)&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this code useful for your research or the use data generated by our method, please consider citing the following paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{rajasegaran2023benefits,&#xA;  title={On the Benefits of 3D Pose and Tracking for Human Action Recognition},&#xA;  author={Rajasegaran, Jathushan and Pavlakos, Georgios and Kanazawa, Angjoo and Feichtenhofer, Christoph and Malik, Jitendra},&#xA;  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},&#xA;  pages={640--649},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>