<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-02T01:36:14Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bmurmann/Book-on-MOS-stages</title>
    <updated>2023-01-02T01:36:14Z</updated>
    <id>tag:github.com,2023-01-02:/bmurmann/Book-on-MOS-stages</id>
    <link href="https://github.com/bmurmann/Book-on-MOS-stages" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Book repository &#34;Analysis and Design of Elementary MOS Amplifier Stages&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Analysis and Design of Elementary MOS Amplifier Stages&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/bmurmann/Book-on-MOS-stages/raw/3573b49921660eefe0ceb7a369119c6d6ef505a8/book/Analysis%20and%20Design%20of%20Elementary%20MOS%20Amplifier%20Stages.pdf&#34;&gt;PDF Download&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg?sanitize=true&#34; alt=&#34;CC BY 4.0&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/bmurmann/Book-on-MOS-stages/main/cover.png&#34; width=&#34;300&#34;&gt; &#xA;&lt;p&gt;Copyright (c) 2013-2022 National Technology and Science Press&lt;br&gt; Copyright (c) 2022 Boris Murmann&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@book{murmann_mos_stages_2013, &#xA;  title={{Analysis and Design of Elementary MOS Amplifier Stages}}, &#xA;  author={Murmann, Boris}, &#xA;  publisher={NTS Press}, &#xA;  place={Allendale, NJ},&#xA;  year={2013}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Analog integrated circuit (IC) design is often viewed as a “black art,” accessible only to those with special talent or years of experience. As an attempt to disprove this stereotype, this book was written to provide a customized introduction for the beginner with a minimum amount of prerequisite knowledge. Specifically, the material is positioned to fill the gap between general introductions on analog circuits, which are usually centered on discrete (printed circuit board) components, and advanced graduate books on integrated circuits. The need for filling the gap between these two types of texts has become stronger over the past decades for several reasons. The first is that advanced material has become less accessible for the inexperienced learner due to the growing complexity associated with the state of the art. A second reason is that today’s typical intro course sequence has been expanded to include embedded system design; this leaves very little time to cover analog circuit principles at a level that is required for advanced study.&lt;/p&gt; &#xA;&lt;p&gt;There are multiple usage scenarios for this book. The material can be taught following an introduction to analog circuits in the junior or senior year of undergraduate study. In addition, the text can be used to prepare incoming graduate students for an advanced course sequence in analog IC design. Lastly, we believe that the book will be valuable for engineers that are pursuing a career change toward analog ICs, but do not possess the prerequisites to follow advanced literature. The reader of this module is expected to be familiar with the basic concepts of linear circuit analysis, including Kirchhoff’s laws and the frequency response analysis of passive networks. We also assume familiarity with basic solid-state physics and electrostatics.&lt;/p&gt; &#xA;&lt;p&gt;Since the study of analog circuits is strongly coupled to semiconductor device physics and linear system theory, it has and will always be difficult to teach this subject from the ground up, without causing too many distractions and challenges that are related to the required tool set, rather than the core principles themselves. This book follows a “just-in-time” treatment of semiconductor device modeling aspects to alleviate this problem. Instead of covering all of the detailed device physics in one isolated chapter, we begin with the simplest possible model, and augment this model only where needed to resolve new questions that arise as we learn more about circuits. This approach eases the device physics overhead and gives the reader a chance to internalize the transistor models from a well-motivated basis.&lt;/p&gt; &#xA;&lt;p&gt;Chapter 1: Introduction&lt;br&gt; Chapter 2: Transfer Characteristic of the Common-Source Voltage Amplifier&lt;br&gt; Chapter 3: Frequency Response of the Common-Source Voltage Amplifier&lt;br&gt; Chapter 4: The Common-Gate and Common-Drain Stages&lt;br&gt; Chapter 5: Biasing Circuits&lt;br&gt; Chapter 6: Multistage Amplifiers&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LAION-AI/Open-Assistant</title>
    <updated>2023-01-02T01:36:14Z</updated>
    <id>tag:github.com,2023-01-02:/LAION-AI/Open-Assistant</id>
    <link href="https://github.com/LAION-AI/Open-Assistant" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open-Assistant&lt;/h1&gt; &#xA;&lt;p&gt;Open Assistant is a project meant to give everyone access to a great chat based large language model.&lt;/p&gt; &#xA;&lt;p&gt;We believe that by doing this we will create a revolution in innovation in language. In the same way that stable-diffusion helped the world make art and images in new ways we hope Open Assistant can help improve the world by improving language itself.&lt;/p&gt; &#xA;&lt;h2&gt;Do you want to try it out?&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in taking a look at the current state of the project, You can set up an entire stack needed to run &lt;strong&gt;Open-Assistant&lt;/strong&gt;, including the website, backend, and associated dependent services.&lt;/p&gt; &#xA;&lt;p&gt;To start the demo, Run this in the root directory of the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker compose up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, navigate to &lt;code&gt;http://localhost:3000&lt;/code&gt; (It may take some time to boot up) and interact with the website.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; When logging in via email, navigate to &lt;code&gt;http://localhost:1080&lt;/code&gt; to get the magic email login link.&lt;/p&gt; &#xA;&lt;h2&gt;The Plan&lt;/h2&gt; &#xA;&lt;p&gt;We want to get to an initial MVP as fast as possible, by following the 3-steps outlined in the InstructGPT paper.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Collect high-quality human generated Instruction-Fulfillment samples (prompt + response), goal &amp;gt;50k. We design a crowdsourced process to collect and reviewed prompts. We do not want to train on flooding/toxic/spam/junk/personal information data. We will have a leaderboard to motivate the community that shows progress and the most active users. Swag will be given to the top-contributors.&lt;/li&gt; &#xA; &lt;li&gt;For each of the collected prompts we will sample multiple completions. Completions of one prompt will then be shown randomly to users to rank them from best to worst. Again this should happen crowd-sourced, e.g. we need to deal with unreliable potentially malicious users. At least multiple votes by independent users have to be collected to measure the overall agreement. The gathered ranking-data will be used to train a reward model.&lt;/li&gt; &#xA; &lt;li&gt;Now follows the RLHF training phase based on the prompts and the reward model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We can then take the resulting model and continue with completion sampling step 2 for a next iteration.&lt;/p&gt; &#xA;&lt;h2&gt;The Vision&lt;/h2&gt; &#xA;&lt;p&gt;We are not going to stop at replicating ChatGPT. We want to build the assistant of the future, able to not only write email and cover letters, but do meaningful work, use APIs, dynamically research information, and much more, with the ability to be personalized and extended by anyone. And we want to do this in a way that is open and accessible, which means we must not only build a great assistant, but also make it small and efficient enough to run on consumer hardware.&lt;/p&gt; &#xA;&lt;h3&gt;Slide Decks&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1n7IrAOVOqwdYgiYrXc8Sj0He8krn5MVZO_iLkCjTtu0/edit?usp=sharing&#34;&gt;Vision &amp;amp; Roadmap&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1iaX_nxasVWlvPiSNs0cllR9L_1neZq0RJxd6MFEalUY/edit?usp=sharing&#34;&gt;Important Data Structures&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How can you help?&lt;/h2&gt; &#xA;&lt;p&gt;All open source projects begins with people like you. Open source is the belief that if we collaborate we can together gift our knowledge and technology to the world for the benefit of humanity.&lt;/p&gt; &#xA;&lt;h2&gt;I’m in! Now what?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ykilcher.com/open-assistant-discord&#34;&gt;Join the OpenAssistant Contributors Discord Server!&lt;/a&gt;, this is for work coordination.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.com/invite/mVcgxMPD7e&#34;&gt;Join the LAION Discord Server!&lt;/a&gt;, it has a dedicated channel and is more public.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ykilcher.com/discord&#34;&gt;and / or the YK Discord Server&lt;/a&gt;, also has a dedicated, but not as active, channel.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ykilcher.com/open-assistant&#34;&gt;Visit the Notion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Taking on Tasks&lt;/h3&gt; &#xA;&lt;p&gt;We have a growing task list &lt;a href=&#34;https://github.com/LAION-AI/Open-Assistant/issues&#34;&gt;of issues&lt;/a&gt;. Find an issue that appeals to you and make a comment that you&#39;d like to work on it. Include in your comment a brief description of how you&#39;ll solve the problem and if there are any open questions you want to discuss. Once a project coordinator has assigned the issue to you, start working on it.&lt;/p&gt; &#xA;&lt;p&gt;If the issue is currently unclear but you are interested, please post in Discord and someone can help clarify the issue with more detail.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Always Welcome:&lt;/strong&gt; Documentation markdowns in &lt;code&gt;docs/&lt;/code&gt;, docstrings, diagrams of the system architecture, and other documentation.&lt;/p&gt; &#xA;&lt;h3&gt;Submitting Work&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;re all working on different parts of Open Assistant together. To make contributions smoothly we recommend the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.github.com/en/get-started/quickstart/fork-a-repo&#34;&gt;Fork this project repository&lt;/a&gt; and clone it to your local machine. (Read more &lt;a href=&#34;https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/about-forks&#34;&gt;About Forks&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Before working on any changes, try to &lt;a href=&#34;https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork&#34;&gt;sync the forked repository&lt;/a&gt; to keep it up-to-date with the upstream repository.&lt;/li&gt; &#xA; &lt;li&gt;Work on a small focused change that only touches on a few files.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;pre-commit&lt;/code&gt; and make sure all files have formatting fixed. This simplifies life for reviewers.&lt;/li&gt; &#xA; &lt;li&gt;Package up a small bit of work that solves part of the problem &lt;a href=&#34;https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork&#34;&gt;into a Pull Request&lt;/a&gt; and &lt;a href=&#34;https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/requesting-a-pull-request-review&#34;&gt;send it out for review&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;re lucky, we can merge your change into &lt;code&gt;main&lt;/code&gt; without any problems. If there&#39;s changes to files you&#39;re working on, resolve them by:&lt;/li&gt; &#xA; &lt;li&gt;First try rebase as suggested &lt;a href=&#34;https://timwise.co.uk/2019/10/14/merge-vs-rebase/#should-you-rebase&#34;&gt;in these instructions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If rebase feels too painful, merge as suggested &lt;a href=&#34;https://timwise.co.uk/2019/10/14/merge-vs-rebase/#should-you-merge&#34;&gt;in these instructions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Once you&#39;ve resolved any conflicts, finish the review and merge into &lt;code&gt;main&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Merge in your change and move onto a new issue or the second step of your current issue.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Additionally, if someone is working on an issue that interests you, ask if they need help on it or would like suggestions on how to approach the issue. If so, share wildly. If they seem to have a good handle on it, let them work on their solution until a challenge comes up.&lt;/p&gt; &#xA;&lt;h3&gt;When does a review finish&lt;/h3&gt; &#xA;&lt;p&gt;A review finishes when all blocking comments are addressed and at least one owning reviewer has approved the PR. Be sure to acknowledge any non-blocking comments either by making the request change, explaining why it&#39;s not being addressed now, or filing an issue to handle it later.&lt;/p&gt; &#xA;&lt;h2&gt;Developer Setup&lt;/h2&gt; &#xA;&lt;p&gt;Work is organized in the &lt;a href=&#34;https://github.com/orgs/LAION-AI/projects/3&#34;&gt;project board&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anything that is in the &lt;code&gt;Todo&lt;/code&gt; column and not assigned, is up for grabs. Meaning we&#39;d be happy for anyone to do these tasks.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to work on something, assign yourself to it or write a comment that you want to work on it and what you plan to do.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To get started with development, if you want to work on the backend, have a look at &lt;code&gt;scripts/backend-development/README.md&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you want to work on any frontend, have a look at &lt;code&gt;scripts/frontend-development/README.md&lt;/code&gt; to make a backend available.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There is also a minimal implementation of a frontend in the &lt;code&gt;text-frontend&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;We are using Python 3.10 for the backend.&lt;/p&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://www.notion.so/High-Level-Protocol-Architecture-6f1fd3551da74213b560ead369f132dc&#34;&gt;High-Level Protocol Architecture&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Website&lt;/h3&gt; &#xA;&lt;p&gt;The website is built using Next.js and is in the &lt;code&gt;website&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-commit&lt;/h3&gt; &#xA;&lt;p&gt;Install &lt;code&gt;pre-commit&lt;/code&gt; and run &lt;code&gt;pre-commit install&lt;/code&gt; to install the pre-commit hooks.&lt;/p&gt; &#xA;&lt;p&gt;In case you haven&#39;t done this, have already committed, and CI is failing, you can run &lt;code&gt;pre-commit run --all-files&lt;/code&gt; to run the pre-commit hooks on all files.&lt;/p&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;p&gt;Upon making a release on GitHub, all docker images are automatically built and pushed to ghcr.io. The docker images are tagged with the release version, and the &lt;code&gt;latest&lt;/code&gt; tag. Further, the ansible playbook in &lt;code&gt;ansible/dev.yaml&lt;/code&gt; is run to automatically deploy the built release to the dev machine.&lt;/p&gt; &#xA;&lt;h3&gt;Problems and Solutions&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;I am on Ubuntu and getting &lt;code&gt;ERROR: The Compose file is invalid because:Service backend has neither an image nor a build context specified. At least one must be provided.&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Make sure you have an up-to-date version of docker installed, and also install &lt;code&gt;docker-compose-plugin&lt;/code&gt;. See &lt;a href=&#34;https://github.com/LAION-AI/Open-Assistant/issues/208&#34;&gt;here&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>AnanyaKumar/transfer_learning</title>
    <updated>2023-01-02T01:36:14Z</updated>
    <id>tag:github.com,2023-01-02:/AnanyaKumar/transfer_learning</id>
    <link href="https://github.com/AnanyaKumar/transfer_learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Framework code with wandb, checkpointing, logging, configs, experimental protocols. Useful for fine-tuning models or training from scratch, and testing them on a variety of datasets (transfer learning)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Code for our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. &#xA;Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, Percy Liang. ICLR 2022.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This paper will be published as an Oral at ICLR 2022. This repository is still being improved and will be updated without backwards compatibility for now.&lt;/p&gt; &#xA;&lt;h2&gt;Setup and installation&lt;/h2&gt; &#xA;&lt;p&gt;The first time you run this project, in the current directory, which contains README, create a virtualenv:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m venv .env&#xA;source .env/bin/activate&#xA;pip install -f https://download.pytorch.org/whl/torch_stable.html -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In subsequent runs you only need to activate the environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;source .env/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also login to weights and biases. Usually you don&#39;t have to re-run this and it remembers the login, even across sessions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wandb login&#xA;wandb on&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running paper experiments (Slurm)&lt;/h2&gt; &#xA;&lt;p&gt;The main experiments are in scripts/run_adaptation_experiments.py. For example, if you&#39;re on the Stanford NLP cluster, then you can run fine-tuning experiments on Living-17 as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/run_adaptation_experiments.py --experiment=fine_tuning_experiments --datasets living17 --model_name=resnet50 --partition=jag-standard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For other slurm clusters, you will need to modify the sbatch file &#34;run_sbatch.sh&#34; and change the partition accordingly.&lt;/p&gt; &#xA;&lt;p&gt;To run LP-FT experiments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/run_adaptation_experiments.py --experiment=lp_then_ft_valmode_experiments --datasets living17 --model_name=resnet50 --partition=jag-standard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run linear probing experiments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/run_adaptation_experiments.py --experiment=linprobe_experiments --datasets living17 --model_name=resnet50 --partition=jag-standard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This code may look complicated, but it&#39;s written to simplify launching and managing a large number of jobs on a cluster. For example, scripts/run_adaptation_experiments.py essentially contains the information to run every experiment in our paper, and exactly what information to track, on a Slurm cluster.&lt;/p&gt; &#xA;&lt;p&gt;After running the experiments, we have scripts that can produce a tsv file (e.g., which you can copy onto Excel or Google Sheets) with a detailed summary of all the runs (e.g., ID accuracy, OOD accuracy, when early stopping on a variety of metrics). For example, for Living17, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python  scripts/summarize_all_results.py --results_dir_glob=logs/*living17* --val_metrics test_acc/source_val_living test_acc/target_val_living LAST --output_metrics epoch train/acc test_acc/source_val_living test_acc/target_val_living --output_file=tmp.tsv &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The summary of all living17 runs will now be contained in tmp.tsv.&lt;/p&gt; &#xA;&lt;h2&gt;Configs&lt;/h2&gt; &#xA;&lt;p&gt;The config files that specify hyperparameters are located in configs. Note that run_adaptation_experiments runs a sweep over these configs and therefore modifies the hyperparameters on configs. So consult run_adaptation_experiments.py for the hyperparameters used in the sweep.&lt;/p&gt; &#xA;&lt;h2&gt;Main code&lt;/h2&gt; &#xA;&lt;p&gt;The main code for fine-tuning and LP-FT are in unlabeled_extrapolation/baseline_train.py.&lt;/p&gt; &#xA;&lt;p&gt;For linear probing, you can first extract model features using unlabeled_extrapolation/extract_features.py, and then train a logistic regression classifier using log_reg_sk.py.&lt;/p&gt; &#xA;&lt;h2&gt;Example run (without slurm)&lt;/h2&gt; &#xA;&lt;p&gt;You can run the fine-tuning code directly without slurm.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export PYTHONPATH=&#34;.&#34;&#xA;. env/bin/python experiments/baseline_train.py --config=configs/adaptation/living17.yaml \\&#xA;--log_dir=logs/living17_lr_0.003 --project_name=living17 --group_name=living17 \\&#xA;--run_nameliving17_lr_0.003 --optimizer.args.lr=0.003&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Alternative setup (under construction):&lt;/h2&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;ananya-ue&lt;/code&gt; conda env. Make sure you have this following code in your &lt;code&gt;.bashrc&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# &amp;gt;&amp;gt;&amp;gt; conda initialize &amp;gt;&amp;gt;&amp;gt;&#xA;# !! Contents within this block are managed by &#39;conda init&#39; !!&#xA;__conda_setup=&#34;$(&#39;/u/nlp/anaconda/main/anaconda3/bin/conda&#39; &#39;shell.bash&#39; &#39;hook&#39; 2&amp;gt; /dev/null)&#34;&#xA;if [ $? -eq 0 ]; then&#xA;    eval &#34;$__conda_setup&#34;&#xA;else&#xA;    if [ -f &#34;/u/nlp/anaconda/main/anaconda3/etc/profile.d/conda.sh&#34; ]; then&#xA;        . &#34;/u/nlp/anaconda/main/anaconda3/etc/profile.d/conda.sh&#34;&#xA;    else&#xA;        export PATH=&#34;/u/nlp/anaconda/main/anaconda3/bin:$PATH&#34;&#xA;    fi&#xA;fi&#xA;unset __conda_setup&#xA;# &amp;lt;&amp;lt;&amp;lt; conda initialize &amp;lt;&amp;lt;&amp;lt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Guide to our codebase&lt;/h2&gt; &#xA;&lt;p&gt;The main script is in &lt;code&gt;experiments/baseline_train.py&lt;/code&gt;. The script requires a YAML config file - an example is &lt;code&gt;configs/adaptation/living17.yaml&lt;/code&gt;. To dynamically change values of the config file with command line arguments, simply add new arguments of the form &lt;code&gt;--key=val&lt;/code&gt; where the key can be any string of multiple keys separated by periods. This is to allow for changing nested components of the config file. For example &lt;code&gt;--model.args.depth=3&lt;/code&gt; changes the config dictionary in this way: &lt;code&gt;config[&#39;model&#39;][&#39;args&#39;][&#39;depth&#39;] = 3&lt;/code&gt;. It is important that the key and value are separated by an equals sign.&lt;/p&gt; &#xA;&lt;p&gt;We use Quinine so you can inherit other configs. This makes things modularized if you have configs for a few datasets, and configs for a few models. Try not to make these nested by more than one or two levels (A inherits B, B inherits C, C inherits D, etc)&lt;/p&gt; &#xA;&lt;p&gt;If you base your config off of &lt;code&gt;configs/adaptation/living17.yaml&lt;/code&gt; here are some important args to keep in mind:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;finetune: If fine-tuning, this should be True.&lt;/li&gt; &#xA; &lt;li&gt;linear_probe: If only training the last linear layer (freeze lower layers), set True, for full fine-tuning set False&lt;/li&gt; &#xA; &lt;li&gt;use_net_val_mode: True if you want to keep the network in &#34;val&#34; mode while training. This should usually be True for linear probing, to turn off batchnorm.&lt;/li&gt; &#xA; &lt;li&gt;num_classes: Specifies the number of classes in the new task you&#39;re fine-tuning on.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Notice that we inherit datasets_living17.yaml, where we have:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A train dataset, which we initialize using args, including a transform&lt;/li&gt; &#xA; &lt;li&gt;We specify a list of transforms, which are applied top to bottom&lt;/li&gt; &#xA; &lt;li&gt;We have a list of test_datasets, and we evaluate performance on all of these.&lt;/li&gt; &#xA; &lt;li&gt;Each test dataest has a name, which is used both for visualization, and to decide which ones to early stop on&lt;/li&gt; &#xA; &lt;li&gt;If a test_transform is not specified, it uses the default_test_transform for the test dataset&lt;/li&gt; &#xA; &lt;li&gt;You can also add &#34;transforms&#34; as a separate field for a test dataset, and we&#39;ll use that instead of default_test_transform&lt;/li&gt; &#xA; &lt;li&gt;We specify early_stop_dataset_names. We don&#39;t literally stop the experiment. But we save the best model checkpoint according to the accuracy on these datasets. If multiple datasets are specified, we save the best checkpoint according to each of these. E.g. you can have one where you early stop on in-domain validation, and an oracle model where you early stop on the OOD validation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also inherit resnet50_transfer.yaml which specifies the scheduler, learning rate, loss criterion, and a few other things that should be self-explanatory. Some things of note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;log_interval is after how many training examples we log the training loss. This is cheap since we keep track of a rolling average in the epoch and output that.&lt;/li&gt; &#xA; &lt;li&gt;save_freq is how often we should save checkpoints besides the best checkpoint. For these experiments, the config sets this to a high number so we don&#39;t save checkpoints besides the initial, final, and best (saving disk space)&lt;/li&gt; &#xA; &lt;li&gt;model.args.pretrained is True, indicating that we initialize from a supervised Resnet50 model (pre-trained on ImageNet labels)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;A Lightweight Framework for Training Models&lt;/h1&gt; &#xA;&lt;p&gt;Adapted from the framework for &#34;In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness&#34;, &lt;a href=&#34;https://arxiv.org/abs/2012.04550&#34;&gt;https://arxiv.org/abs/2012.04550&lt;/a&gt;, Sang Michael Xie*, Ananya Kumar*, Robbie Jones*, Fereshte Khani, Tengyu Ma, Percy Liang. Thanks especially to Michael and Robbie!&lt;/p&gt; &#xA;&lt;p&gt;Benefits of framework:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Config files to specify dataset, model, augmentations, and other arguments, for better readability and reproducibility&lt;/li&gt; &#xA; &lt;li&gt;Nice logging&lt;/li&gt; &#xA; &lt;li&gt;Saves checkpoints&lt;/li&gt; &#xA; &lt;li&gt;Sets up and outputs to weights and biases&lt;/li&gt; &#xA; &lt;li&gt;New: Saves the command line argument, and config, to run the experiment&lt;/li&gt; &#xA; &lt;li&gt;New: Optionally, can save all the code used to run the experiment so you know exactly what you ran&lt;/li&gt; &#xA; &lt;li&gt;New: can early stop based on a variety of metrics&lt;/li&gt; &#xA; &lt;li&gt;New: track a variety of metrics, and produce a nice TSV with all the results&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Setup:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;First install a virtualenv and activate it (instructions coming)&lt;/li&gt; &#xA; &lt;li&gt;Create a weights and biases account&lt;/li&gt; &#xA; &lt;li&gt;Run &#34;wandb login&#34; and &#34;wandb on&#34;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>