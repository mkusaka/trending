<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-15T01:38:12Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>camenduru/AnimateDiff-colab</title>
    <updated>2023-07-15T01:38:12Z</updated>
    <id>tag:github.com,2023-07-15:/camenduru/AnimateDiff-colab</id>
    <link href="https://github.com/camenduru/AnimateDiff-colab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;üê£ Please follow me for new updates &lt;a href=&#34;https://twitter.com/camenduru&#34;&gt;https://twitter.com/camenduru&lt;/a&gt; &lt;br&gt; üî• Please join our discord server &lt;a href=&#34;https://discord.gg/k5BwmmvJJU&#34;&gt;https://discord.gg/k5BwmmvJJU&lt;/a&gt; &lt;br&gt; ü•≥ Please join my patreon community &lt;a href=&#34;https://patreon.com/camenduru&#34;&gt;https://patreon.com/camenduru&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ü¶í Colab&lt;/h2&gt; &#xA;&lt;h1&gt;üö¶ WIP üö¶&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Colab&lt;/th&gt; &#xA;   &lt;th&gt;Info&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/camenduru/AnimateDiff-colab/blob/main/AnimateDiff_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AnimateDiff_colab (--L 24 --W 512 --H 512 ü¶í Colab Free Limit)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Tutorial&lt;/h2&gt; &#xA;&lt;p&gt;Please edit &lt;code&gt;/content/animatediff/configs/prompts/1-ToonYou.yaml&lt;/code&gt; file for different prompts. We can use it with any LoRA ü•≥ &lt;br&gt; output files here &lt;code&gt;/content/animatediff/samples/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ToonYou:&#xA;  base: &#34;&#34;&#xA;  path: &#34;models/DreamBooth_LoRA/toonyou_beta3.safetensors&#34;&#xA;  motion_module:&#xA;    - &#34;models/Motion_Module/mm_sd_v14.ckpt&#34;&#xA;    - &#34;models/Motion_Module/mm_sd_v15.ckpt&#34;&#xA;&#xA;  seed:           [10788741199826055526, 6520604954829636163, 6519455744612555650, 16372571278361863751]&#xA;  steps:          25&#xA;  guidance_scale: 7.5&#xA;&#xA;  prompt:&#xA;    - &#34;best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress&#34;&#xA;    - &#34;masterpiece, best quality, 1girl, solo, cherry blossoms, hanami, pink flower, white flower, spring season, wisteria, petals, flower, plum blossoms, outdoors, falling petals, white hair, black eyes,&#34;&#xA;    - &#34;best quality, masterpiece, 1boy, formal, abstract, looking at viewer, masculine, marble pattern&#34;&#xA;    - &#34;best quality, masterpiece, 1girl, cloudy sky, dandelion, contrapposto, alternate hairstyle,&#34;&#xA;&#xA;  n_prompt:&#xA;    - &#34;&#34;&#xA;    - &#34;badhandv4,easynegative,ng_deepnegative_v1_75t,verybadimagenegative_v1.3, bad-artist, bad_prompt_version2-neg, teeth&#34;&#xA;    - &#34;&#34;&#xA;    - &#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Main Repo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guoyww/animatediff/&#34;&gt;https://github.com/guoyww/animatediff/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Page&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://animatediff.github.io/&#34;&gt;https://animatediff.github.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Paper&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.04725&#34;&gt;https://arxiv.org/abs/2307.04725&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Output&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/camenduru/AnimateDiff-colab/assets/54370274/34f3ec0a-277b-4cec-a5c8-468b666b739b&#34; alt=&#34;0-best-quality,-masterpiece,-1girl,-looking-at-viewer,-blurry-background,-upper (5)&#34;&gt; &lt;img src=&#34;https://github.com/camenduru/AnimateDiff-colab/assets/54370274/7fa841d4-31b8-469d-ad32-f56a986a2c3d&#34; alt=&#34;1-masterpiece,-best-quality,-1girl,-solo,-cherry-blossoms,-hanami,-pink-flower, (3)&#34;&gt; &lt;img src=&#34;https://github.com/camenduru/AnimateDiff-colab/assets/54370274/b4976fb1-758e-4d9b-9c65-40cea7c60fff&#34; alt=&#34;2-best-quality,-masterpiece,-1boy,-formal,-abstract,-looking-at-viewer,-masculine, (2)&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>KyanChen/RSPrompter</title>
    <updated>2023-07-15T01:38:12Z</updated>
    <id>tag:github.com,2023-07-15:/KyanChen/RSPrompter</id>
    <link href="https://github.com/KyanChen/RSPrompter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the pytorch implement of our paper &#34;RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/KyanChen/RSPrompter/raw/cky/logo.png&#34; width=&#34;60%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/KyanChen/RSPrompter/cky/readme_cn.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the pytorch implement of our paper &#34;RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model&#34;&lt;/p&gt; &#xA;&lt;p&gt;This method will be integrated into the MMdetection framework soon, please stand by.&lt;/p&gt; &#xA;&lt;p&gt;If this work is helpful to you, please &lt;strong&gt;STAR&lt;/strong&gt; this repository.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://kyanchen.github.io/RSPrompter/&#34;&gt;Project Page&lt;/a&gt; $\cdot$ &lt;a href=&#34;https://arxiv.org/abs/2306.16269&#34;&gt;PDF Download&lt;/a&gt; $\cdot$ &lt;a href=&#34;https://huggingface.co/papers/2306.16269&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/KyanChen/RSPrompter/cky/paper-page-sm-dark.svg?sanitize=true&#34; alt=&#34;Paper page&#34;&gt;&lt;/a&gt; $\cdot$ &lt;a href=&#34;https://huggingface.co/spaces/KyanChen/RSPrompter&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/KyanChen/RSPrompter/cky/open-in-hf-spaces-sm-dark.svg?sanitize=true&#34; alt=&#34;Open in Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;0. Environment Setup&lt;/h2&gt; &#xA;&lt;h3&gt;0.1 Create a virtual environment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n RSPrompter python=3.10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;0.2 Activate the virtual environment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sehll&#34;&gt;conda activate RSPrompter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;0.3 Install pytorch&lt;/h3&gt; &#xA;&lt;p&gt;Version of 1.x is also work, but the version of 2.x is recommended.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install torch torchvision --index-url https://download.pytorch.org/whl/cu117&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;0.3 [Optional] Install pytorch&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;0.4 Install mmcv&lt;/h3&gt; &#xA;&lt;p&gt;Version of 2.x is recommended.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install mmcv==2.0.0 -f https://download.openmmlab.com/mmcv/dist/cu117/torch2.0/index.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://mmcv.readthedocs.io/en/latest/get_started/installation.html&#34;&gt;installation documentation&lt;/a&gt; for more detailed installation.&lt;/p&gt; &#xA;&lt;h3&gt;0.5 Install other dependencies&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;1. Data Preparation&lt;/h2&gt; &#xA;&lt;h3&gt;1.1 Dataset&lt;/h3&gt; &#xA;&lt;h4&gt;WHU Dataset&lt;/h4&gt; &#xA;&lt;p&gt;WHU dataset can be downloaded from &lt;a href=&#34;https://aistudio.baidu.com/aistudio/datasetdetail/56502&#34;&gt;WHU&lt;/a&gt;. After downloading, put the dataset into the &lt;strong&gt;data&lt;/strong&gt; folder, which contains some image examples.&lt;/p&gt; &#xA;&lt;h4&gt;NWPU Dataset&lt;/h4&gt; &#xA;&lt;p&gt;NWPU dataset can be downloaded from &lt;a href=&#34;https://aistudio.baidu.com/aistudio/datasetdetail/52812&#34;&gt;NWPU&lt;/a&gt;. After downloading, put the dataset into the &lt;strong&gt;data&lt;/strong&gt; folder, which contains some image examples.&lt;/p&gt; &#xA;&lt;h4&gt;SSDD Dataset&lt;/h4&gt; &#xA;&lt;p&gt;SSDD dataset can be downloaded from &lt;a href=&#34;https://aistudio.baidu.com/aistudio/datasetdetail/100924&#34;&gt;SSDD&lt;/a&gt;. After downloading, put the dataset into the &lt;strong&gt;data&lt;/strong&gt; folder, which contains some image examples.&lt;/p&gt; &#xA;&lt;h4&gt;1.2 Split the dataset into train and test set&lt;/h4&gt; &#xA;&lt;p&gt;The dataset split files and annotation files are provided in this project, which are stored in the &lt;strong&gt;data/*/annotations&lt;/strong&gt; folder in COCO annotation format.&lt;/p&gt; &#xA;&lt;h2&gt;2. Model Training&lt;/h2&gt; &#xA;&lt;h3&gt;2.1 Train SAM-based model&lt;/h3&gt; &#xA;&lt;h4&gt;2.1.1 Config file&lt;/h4&gt; &#xA;&lt;p&gt;The config file is located in the &lt;strong&gt;configs/rsprompter&lt;/strong&gt; folder, which can be modified according to the situation. The config file provides three models: SAM-seg, SAM-det, and RSPrompter.&lt;/p&gt; &#xA;&lt;h4&gt;2.1.2 Train&lt;/h4&gt; &#xA;&lt;p&gt;Some parameters of the training can also be modified in the above configuration file. The main modification of the parameters in trainer_cfg, such as single-card multi-card training, etc., for specific configuration modifications, please refer to the Trainer of Pytorch Lightning.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2.2 [Optional] Train other models&lt;/h3&gt; &#xA;&lt;h4&gt;2.2.1 Config file&lt;/h4&gt; &#xA;&lt;p&gt;The config file is located in the &lt;strong&gt;configs/rsprompter&lt;/strong&gt; folder, which provides only the configuration of Mask R-CNN and Mask2Former. The configuration of other models can refer to these two configuration files and the model config in MMDetection.&lt;/p&gt; &#xA;&lt;h4&gt;2.2.2 Train&lt;/h4&gt; &#xA;&lt;p&gt;Modify the config path in &lt;strong&gt;tools/train.py&lt;/strong&gt; and then run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. Model Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;The config file is located in the &lt;strong&gt;configs/rsprompter&lt;/strong&gt; folder, which can be modified according to the situation. When the val_evaluator and val_loader are configured in the configuration file, the model will automatically evaluate the model on the validation set during model training, and the evaluation results will be uploaded to Wandb and can be viewed in Wandb. If you need to perform offline evaluation on the test set, you need to configure the test_evaluator and test_loader in the configuration file, as well as the config and ckpt-path paths in &lt;strong&gt;tools/test.py&lt;/strong&gt;, and then run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/test.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;4. [Optional] Model Visualization&lt;/h2&gt; &#xA;&lt;p&gt;The config file is located in the &lt;strong&gt;configs/rsprompter&lt;/strong&gt; folder, which can be modified according to the situation. You can modify the parameters of DetVisualizationHook and DetLocalVisualizer in the configuration file, as well as the config and ckpt-path paths in &lt;strong&gt;tools/predict.py&lt;/strong&gt;, and then run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/predict.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;5. [Optional] Model Download&lt;/h2&gt; &#xA;&lt;p&gt;This project provides the model weights of RSPrompter-anchor, which are located in &lt;a href=&#34;https://huggingface.co/spaces/KyanChen/RSPrompter/tree/main/pretrain&#34;&gt;huggingface space&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;6. [Optional] Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful for your research, please cite our paper.&lt;/p&gt; &#xA;&lt;p&gt;If you have any other questions, please contact me!!!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{chen2023rsprompter,&#xA;      title={RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model}, &#xA;      author={Keyan Chen and Chenyang Liu and Hao Chen and Haotian Zhang and Wenyuan Li and Zhengxia Zou and Zhenwei Shi},&#xA;      year={2023},&#xA;      eprint={2306.16269},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>