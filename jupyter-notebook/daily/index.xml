<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-27T01:29:59Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>TIGER-AI-Lab/AnyV2V</title>
    <updated>2024-03-27T01:29:59Z</updated>
    <id>tag:github.com,2024-03-27:/TIGER-AI-Lab/AnyV2V</id>
    <link href="https://github.com/TIGER-AI-Lab/AnyV2V" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Plug-and-Play Framework For Any Video-to-Video Editing Tasks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://tiger-ai-lab.github.io/AnyV2V/static/images/icon.png&#34; width=&#34;30&#34;&gt; AnyV2V&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.14468&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2403.14468-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TIGER-AI-Lab/AnyV2V/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/TIGER-AI-Lab/AnyV2V&#34; alt=&#34;contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TIGER-AI-Lab/AnyV2V/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/TIGER-AI-Lab/AnyV2V.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TIGER-AI-Lab/AnyV2V/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/TIGER-AI-Lab/AnyV2V?color=0088ff&#34; alt=&#34;pull requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TIGER-AI-Lab/AnyV2V/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/TIGER-AI-Lab/AnyV2V.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hits.seeyoufarm.com&#34;&gt;&lt;img src=&#34;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FTIGER-AI-Lab%2FAnyV2V&amp;amp;count_bg=%23C83DB9&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=visitors&amp;amp;edge_flat=false&#34; alt=&#34;Hits&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/cjwbw/anyv2v&#34;&gt;&lt;img src=&#34;https://replicate.com/cjwbw/anyv2v/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tiger-ai-lab.github.io/AnyV2V/&#34;&gt;&lt;strong&gt;üåê Homepage&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/papers/2403.14468&#34;&gt;&lt;strong&gt;ü§ó HuggingFace Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2403.14468&#34;&gt;&lt;strong&gt;üìñ arXiv&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://replicate.com/cjwbw/anyv2v&#34;&gt;&lt;strong&gt;üé¨ Replicate Demo&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo contains the codebase for the paper &#34;&lt;a href=&#34;https://arxiv.org/pdf/2403.14468.pdf&#34;&gt;AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks&lt;/a&gt;&#34;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/TIGER-AI-Lab/AnyV2V/main/assets/AnyV2V-SlidesShow-GIF-1080P-02.gif&#34; alt=&#34;AnyV2V&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;AnyV2V is a tuning-free framework to achieve high appearance and temporal consistency in video editing.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;can seamlessly build on top of advanced image editing methods to perform diverse types of editing&lt;/li&gt; &#xA; &lt;li&gt;Utilizing I2V model&#39;s inherent knowledge to achieve robust performance on the four tasks: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;prompt-based editing&lt;/li&gt; &#xA;   &lt;li&gt;reference-based style transfer&lt;/li&gt; &#xA;   &lt;li&gt;subject-driven editing&lt;/li&gt; &#xA;   &lt;li&gt;identity manipulation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì∞ News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024 Mar 24: Added &lt;a href=&#34;https://replicate.com/cjwbw/anyv2v&#34;&gt;Replicate demo&lt;/a&gt; for AnyV2V(i2vgen-xl). Thanks &lt;a href=&#34;https://github.com/chenxwh&#34;&gt;@chenxwh&lt;/a&gt; for the effort!!&lt;/li&gt; &#xA; &lt;li&gt;2024 Mar 22: Code released for AnyV2V(i2vgen-xl).&lt;/li&gt; &#xA; &lt;li&gt;2024 Mar 21: Our paper is featured on &lt;a href=&#34;https://huggingface.co/papers/2403.14468&#34;&gt;Huggingface Daily Papers&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;2024 Mar 21: Paper available on &lt;a href=&#34;https://arxiv.org/abs/2403.14468&#34;&gt;Arxiv&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚ñ∂Ô∏è Quick Start for AnyV2V(i2vgen-xl)&lt;/h2&gt; &#xA;&lt;h3&gt;Environment&lt;/h3&gt; &#xA;&lt;p&gt;Prepare the codebase of the AnyV2V project and Conda environment using the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/TIGER-AI-Lab/AnyV2V&#xA;cd AnyV2V&#xA;&#xA;cd i2vgen-xl&#xA;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;üìú Notebook Demo&lt;/h4&gt; &#xA;&lt;p&gt;We provide a notebook demo &lt;code&gt;i2vgen-xl/demo.ipynb&lt;/code&gt; for AnyV2V(i2vgen-xl). You can run the notebook to perform a Prompt-Based Editing on a single video. Make sure the environment is set up correctly before running the notebook.&lt;/p&gt; &#xA;&lt;h4&gt;To edit multiple demo videos, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/TIGER-AI-Lab/AnyV2V/main/#Video-Editing&#34;&gt;Video Editing&lt;/a&gt; section.&lt;/h4&gt; &#xA;&lt;h3&gt;Video Editing&lt;/h3&gt; &#xA;&lt;p&gt;We provide demo source videos and edited images in the &lt;code&gt;demo&lt;/code&gt; folder. Below are the instructions for performing video editing on the provided source videos. Navigate to &lt;code&gt;i2vgen-xl/configs/group_ddim_inversion&lt;/code&gt; and &lt;code&gt;i2vgen-xl/configs/group_pnp_edit&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Modify the &lt;code&gt;template.yaml&lt;/code&gt; files to specify the &lt;code&gt;device&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Modify the &lt;code&gt;group_config.json&lt;/code&gt; files according to the provided examples. The configurations in &lt;code&gt;group_config.json&lt;/code&gt; will override the configurations in &lt;code&gt;template.yaml&lt;/code&gt;. To enable an example, set &lt;code&gt;active: true&lt;/code&gt;; to disable it, set &lt;code&gt;active: false&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Then you can run the following command to perform inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd i2vgen-xl/scripts&#xA;bash run_group_ddim_inversion.sh&#xA;bash run_group_pnp_edit.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or run the following command using python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd i2vgen-xl/scripts&#xA;&#xA;# First invert the latent of source video&#xA;python run_group_ddim_inversion.py \&#xA;--template_config &#34;configs/group_ddim_inversion/template.yaml&#34; \&#xA;--configs_json &#34;configs/group_ddim_inversion/group_config.json&#34;&#xA;&#xA;# Then run Anyv2v pipeline with the source video latent&#xA;python run_group_pnp_edit.py \&#xA;--template_config &#34;configs/group_pnp_edit/template.yaml&#34; \&#xA;--configs_json &#34;configs/group_pnp_edit/group_config.json&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;To edit your own source videos, follow the steps outlined below:&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prepare the source video &lt;code&gt;Your-Video.mp4&lt;/code&gt;in the &lt;code&gt;demo&lt;/code&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;Create two new folders &lt;code&gt;demo/Your-Video-Name&lt;/code&gt; and &lt;code&gt;demo/Your-Video-Name/edited_first_frame&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run the following command to perform first frame image editing:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python edit_image.py --video_path &#34;./demo/Your-Video.mp4&#34; --input_dir &#34;./demo&#34; --output_dir &#34;./demo/Your-Video-Name/edited_first_frame&#34; --prompt &#34;Your prompt&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use any other image editing method, such as InstantID, AnyDoor, or WISE, to edit the first frame. Please put the edited first frame images in the &lt;code&gt;demo/Your-Video-Name/edited_first_frame&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Add an entry to the &lt;code&gt;group_config.json&lt;/code&gt; files located in &lt;code&gt;i2vgen-xl/configs/group_ddim_inversion&lt;/code&gt; and &lt;code&gt;i2vgen-xl/configs/group_pnp_edit&lt;/code&gt; directories for your video, following the provided examples.&lt;/li&gt; &#xA; &lt;li&gt;Run the inference command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd i2vgen-xl/scripts&#xA;bash run_group_ddim_inversion.sh&#xA;bash run_group_pnp_edit.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚ñ∂Ô∏è Quick Start for AnyV2V(seine)&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/TIGER-AI-Lab/AnyV2V/main/seine/README.md&#34;&gt;./seine/README.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚ñ∂Ô∏è Misc&lt;/h2&gt; &#xA;&lt;h3&gt;First Frame Image Edit&lt;/h3&gt; &#xA;&lt;p&gt;We provide instructpix2pix port for image editing with instruction prompt.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;usage: edit_image.py [-h] [--model {magicbrush,instructpix2pix}]&#xA;                     [--video_path VIDEO_PATH] [--input_dir INPUT_DIR]&#xA;                     [--output_dir OUTPUT_DIR] [--prompt PROMPT] [--force_512]&#xA;                     [--dict_file DICT_FILE] [--seed SEED]&#xA;                     [--negative_prompt NEGATIVE_PROMPT]&#xA;&#xA;Process some images.&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --model {magicbrush,instructpix2pix}&#xA;                        Name of the image editing model&#xA;  --video_path VIDEO_PATH&#xA;                        Name of the video&#xA;  --input_dir INPUT_DIR&#xA;                        Directory containing the video&#xA;  --output_dir OUTPUT_DIR&#xA;                        Directory to save the processed images&#xA;  --prompt PROMPT       Instruction prompt for editing&#xA;  --force_512           Force resize to 512x512 when feeding into image model&#xA;  --dict_file DICT_FILE&#xA;                        JSON file containing files, instructions etc.&#xA;  --seed SEED           Seed for random number generator&#xA;  --negative_prompt NEGATIVE_PROMPT&#xA;                        Negative prompt for editing&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python edit_image.py --video_path &#34;./demo/Man Walking.mp4&#34; --input_dir &#34;./demo&#34; --output_dir &#34;./demo/Man Walking/edited_first_frame&#34; --prompt &#34;turn the man into darth vader&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use other image models for editing, here are some online demo models that you can use:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/InstantX/InstantID&#34;&gt;Idenity Manipulation model: InstantID&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/xichenhku/AnyDoor-online&#34;&gt;Subject Driven Image editing model: AnyDoor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/MaxReimann/Whitebox-Style-Transfer-Editing&#34;&gt;Style Transfer model: WISE&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Video Preprocess Script&lt;/h3&gt; &#xA;&lt;p&gt;As the current I2V models only support videos with 2 seconds (16 frames), we provide script to trim and crop video into the desired 2 second video with any dimension.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;usage: prepare_video.py [-h] [--input_folder INPUT_FOLDER] [--video_path VIDEO_PATH] [--output_folder OUTPUT_FOLDER]&#xA;                        [--clip_duration CLIP_DURATION] [--width WIDTH] [--height HEIGHT] [--start_time START_TIME] [--end_time END_TIME]&#xA;                        [--n_frames N_FRAMES] [--center_crop] [--x_offset X_OFFSET] [--y_offset Y_OFFSET] [--longest_to_width]&#xA;&#xA;Crop and resize video segments.&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --input_folder INPUT_FOLDER&#xA;                        Path to the input folder containing video files&#xA;  --video_path VIDEO_PATH&#xA;                        Path to the input video file&#xA;  --output_folder OUTPUT_FOLDER&#xA;                        Path to the folder for the output videos&#xA;  --clip_duration CLIP_DURATION&#xA;                        Duration of the video clips in seconds default=2&#xA;  --width WIDTH         Width of the output video (optional) default=512&#xA;  --height HEIGHT       Height of the output video (optional) default=512&#xA;  --start_time START_TIME&#xA;                        Start time for cropping (optional)&#xA;  --end_time END_TIME   End time for cropping (optional)&#xA;  --n_frames N_FRAMES   Number of frames to extract from each video&#xA;  --center_crop         Center crop the video&#xA;  --x_offset X_OFFSET   Horizontal offset for center cropping, range -1 to 1 (optional)&#xA;  --y_offset Y_OFFSET   Vertical offset for center cropping, range -1 to 1 (optional)&#xA;  --longest_to_width    Resize the longest dimension to the specified width&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python prepare_video.py --input_folder src_center_crop/ --output_folder processed --start_time 1 --center_crop --x_offset 0 --y_offset 0&#xA;python prepare_video.py --input_folder src_left_crop/ --output_folder processed --start_time 1 --center_crop --x_offset -1 --y_offset 0&#xA;python prepare_video.py --input_folder src_right_crop/ --output_folder processed --start_time 1 --center_crop --x_offset 1 --y_offset 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìã TODO&lt;/h2&gt; &#xA;&lt;p&gt;AnyV2V(i2vgen-xl)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release the code for AnyV2V(i2vgen-xl)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release a notebook demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release scripts for multiple image editing&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release a Gradio demo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;AnyV2V(SEINE)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release the code for AnyV2V(SEINE)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;AnyV2V(ConsistI2V)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release the code for AnyV2V(ConsistI2V)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Misc&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Helper script to preprocess the source video&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Helper script to obtain edited first frame from the source video&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üñäÔ∏è Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please kindly cite our paper if you use our code, data, models or results:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{ku2024anyv2v,&#xA;  title={AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks},&#xA;  author={Ku, Max and Wei, Cong and Ren, Weiming and Yang, Harry and Chen, Wenhu},&#xA;  journal={arXiv preprint arXiv:2403.14468},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üé´ License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/TIGER-AI-Lab/AnyV2V/main/LICENSE&#34;&gt;the MIT License&lt;/a&gt;. However, our code is based on some projects that might used another license:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ali-vilab/VGen&#34;&gt;i2vgen-xl&lt;/a&gt;: Missing License&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Vchitect/SEINE&#34;&gt;SEINE&lt;/a&gt;: Apache-2.0&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TIGER-AI-Lab/ConsistI2V&#34;&gt;ConsistI2V&lt;/a&gt;: MIT License&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#TIGER-AI-Lab/AnyV2V&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=TIGER-AI-Lab/AnyV2V&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìû Contact Authors&lt;/h2&gt; &#xA;&lt;p&gt;Max Ku &lt;a href=&#34;https://github.com/vinesmsuic&#34;&gt;@vinemsuic&lt;/a&gt;, &lt;a href=&#34;mailto:m3ku@uwaterloo.ca&#34;&gt;m3ku@uwaterloo.ca&lt;/a&gt; &lt;br&gt; Cong Wei &lt;a href=&#34;https://github.com/lim142857&#34;&gt;@lim142857&lt;/a&gt;, &lt;a href=&#34;mailto:c58wei@uwaterloo.ca&#34;&gt;c58wei@uwaterloo.ca&lt;/a&gt; &lt;br&gt; Weiming Ren &lt;a href=&#34;https://github.com/wren93&#34;&gt;@wren93&lt;/a&gt;, &lt;a href=&#34;mailto:w2ren@uwaterloo.ca&#34;&gt;w2ren@uwaterloo.ca&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üíû Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The code is built upon the below repositories, we thank all the contributors for open-sourcing.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TIGER-AI-Lab/ImagenHub&#34;&gt;ImagenHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbt/TokenFlow&#34;&gt;TokenFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ali-vilab/VGen&#34;&gt;i2vgen-xl&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Vchitect/SEINE&#34;&gt;SEINE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TIGER-AI-Lab/ConsistI2V&#34;&gt;ConsistI2V&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>google-gemini/gemini-api-cookbook</title>
    <updated>2024-03-27T01:29:59Z</updated>
    <id>tag:github.com,2024-03-27:/google-gemini/gemini-api-cookbook</id>
    <link href="https://github.com/google-gemini/gemini-api-cookbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of guides and examples for the Gemini API.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to the Gemini API Cookbook&lt;/h1&gt; &#xA;&lt;p&gt;This is a collection of guides and examples for the Gemini API. You&#39;ll need a Gemini API key to run these examples. You can &lt;a href=&#34;https://aistudio.google.com/app/apikey&#34;&gt;create&lt;/a&gt; one in Google AI Studio with a single click.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an overview of what you&#39;ll find.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A short &lt;a href=&#34;https://github.com/google-gemini/gemini-api-cookbook/raw/main/Getting_started.md&#34;&gt;getting started&lt;/a&gt; guide for building with Gemini API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-gemini/gemini-api-cookbook/tree/main/quickstarts&#34;&gt;Quickstart&lt;/a&gt; tutorials for writing prompts and using different features of the API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-gemini/gemini-api-cookbook/tree/main/examples&#34;&gt;Examples&lt;/a&gt; of things you can build with the API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Most of these examples are written in Python as Colab Notebooks. You can open these in &lt;a href=&#34;https://colab.research.google.com/&#34;&gt;Google Colab&lt;/a&gt; by clicking on the &#34;Run in Colab&#34; button at the top of each notebook, or download and run them in the environment of your choice.&lt;/p&gt; &#xA;&lt;p&gt;Looking for examples in other programming languages?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check out the getting stated guide above for links.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Have questions? Find a bug?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open an &lt;a href=&#34;https://github.com/google-gemini/gemini-api-cookbook/issues&#34;&gt;issue&lt;/a&gt; on GitHub.&lt;/li&gt; &#xA; &lt;li&gt;Ask for help on the &lt;a href=&#34;https://discuss.ai.google.dev&#34;&gt;Forum&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Contributions are welcome. See &lt;a href=&#34;https://github.com/google-gemini/gemini-api-cookbook/raw/main/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; to learn more.&lt;/p&gt; &#xA;&lt;p&gt;You can find more documentation on &lt;a href=&#34;https://ai.google.dev&#34;&gt;https://ai.google.dev&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>vikhyat/moondream</title>
    <updated>2024-03-27T01:29:59Z</updated>
    <id>tag:github.com,2024-03-27:/vikhyat/moondream</id>
    <link href="https://github.com/vikhyat/moondream" rel="alternate"></link>
    <summary type="html">&lt;p&gt;tiny vision language model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üåî moondream&lt;/h1&gt; &#xA;&lt;p&gt;a tiny vision language model that kicks ass and runs anywhere&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://moondream.ai/&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/vikhyatk/moondream2&#34;&gt;Hugging Face&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/vikhyatk/moondream2&#34;&gt;Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;moondream2 is a 1.86B parameter model initialized with weights from &lt;a href=&#34;https://huggingface.co/timm/ViT-SO400M-14-SigLIP-384&#34;&gt;SigLIP&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/microsoft/phi-1_5&#34;&gt;Phi 1.5&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;VQAv2&lt;/th&gt; &#xA;   &lt;th&gt;GQA&lt;/th&gt; &#xA;   &lt;th&gt;TextVQA&lt;/th&gt; &#xA;   &lt;th&gt;TallyQA (simple)&lt;/th&gt; &#xA;   &lt;th&gt;TallyQA (full)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;moondream1&lt;/td&gt; &#xA;   &lt;td&gt;74.7&lt;/td&gt; &#xA;   &lt;td&gt;57.9&lt;/td&gt; &#xA;   &lt;td&gt;35.6&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;moondream2&lt;/strong&gt; (latest)&lt;/td&gt; &#xA;   &lt;td&gt;76.8&lt;/td&gt; &#xA;   &lt;td&gt;60.6&lt;/td&gt; &#xA;   &lt;td&gt;46.4&lt;/td&gt; &#xA;   &lt;td&gt;79.6&lt;/td&gt; &#xA;   &lt;td&gt;73.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Image&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-1.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;What is the girl doing?&lt;/strong&gt;&lt;br&gt;The girl is eating a hamburger.&lt;br&gt;&lt;br&gt;&lt;strong&gt;What color is the girl&#39;s hair?&lt;/strong&gt;&lt;br&gt;The girl&#39;s hair is white.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-2.jpg&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;br&gt;This is a computer server rack, specifically designed for holding multiple computer processors and other components. The rack has multiple shelves or tiers, each holding several processors, and it is placed on a carpeted floor. The rack is filled with various computer parts, including processors, wires, and other electronic devices.&lt;br&gt;&lt;br&gt;&lt;strong&gt;What is behind the stand?&lt;/strong&gt;&lt;br&gt;There is a brick wall behind the stand.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using transformers&lt;/strong&gt; (recommended)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install transformers timm einops&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;from PIL import Image&#xA;&#xA;model_id = &#34;vikhyatk/moondream2&#34;&#xA;revision = &#34;2024-03-13&#34;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_id, trust_remote_code=True, revision=revision&#xA;)&#xA;tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)&#xA;&#xA;image = Image.open(&#39;&amp;lt;IMAGE_PATH&amp;gt;&#39;)&#xA;enc_image = model.encode_image(image)&#xA;print(model.answer_question(enc_image, &#34;Describe this image.&#34;, tokenizer))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The model is updated regularly, so we recommend pinning the model version to a specific release as shown above.&lt;/p&gt; &#xA;&lt;p&gt;To enable Flash Attention on the text model, pass in &lt;code&gt;attn_implementation=&#34;flash_attention_2&#34;&lt;/code&gt; when instantiating the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_id, trust_remote_code=True, revision=revision,&#xA;    torch_dtype=torch.float16, attn_implementation=&#34;flash_attention_2&#34;&#xA;).to(&#34;cuda&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Batch inference is also supported.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;answers = moondream.batch_answer(&#xA;    images=[Image.open(&#39;&amp;lt;IMAGE_PATH_1&amp;gt;&#39;), Image.open(&#39;&amp;lt;IMAGE_PATH_2&amp;gt;&#39;)],&#xA;    prompts=[&#34;Describe this image.&#34;, &#34;Are there people in this image?&#34;],&#xA;    tokenizer=tokenizer,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using this repository&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Clone this repository and install dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;sample.py&lt;/code&gt; provides a CLI interface for running the model. When the &lt;code&gt;--prompt&lt;/code&gt; argument is not provided, the script will allow you to ask questions interactively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python sample.py --image [IMAGE_PATH] --prompt [PROMPT]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use &lt;code&gt;gradio_demo.py&lt;/code&gt; script to start a Gradio interface for the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gradio_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;webcam_gradio_demo.py&lt;/code&gt; provides a Gradio interface for the model that uses your webcam as input and performs inference in real-time.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python webcam_gradio_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The model may generate inaccurate statements, and struggle to understand intricate or nuanced instructions.&lt;/li&gt; &#xA; &lt;li&gt;The model may not be free from societal biases. Users should be aware of this and exercise caution and critical thinking when using the model.&lt;/li&gt; &#xA; &lt;li&gt;The model may generate offensive, inappropriate, or hurtful content if it is prompted to do so.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>