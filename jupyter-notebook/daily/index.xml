<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-21T01:31:38Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AIGText/Glyph-ByT5</title>
    <updated>2024-06-21T01:31:38Z</updated>
    <id>tag:github.com,2024-06-21:/AIGText/Glyph-ByT5</id>
    <link href="https://github.com/AIGText/Glyph-ByT5" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is an official inference code of the paper &#34;Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering&#34; and &#34;Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering&#34;&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.09622&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2403.09622-red&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.10208&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2406.10208-red&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://huggingface.co/GlyphByT5/Glyph-SDXL&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Checkpoints-GlyphSDXL-yellow&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://huggingface.co/GlyphByT5/Glyph-SDXL-v2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Checkpoints-GlyphSDXLv2-yellow&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://huggingface.co/datasets/GlyphByT5/GlyphByT5Pretraining&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Dataset-GlyphByT5Pretraining-yellow&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://glyph-byt5.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project Page-GlyphByT5-green&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://glyph-byt5-v2.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project Page-GlyphByT5v2-green&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://huggingface.co/spaces/GlyphByT5/Glyph-SDXL-v2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-GlyphSDXLv2-blue&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official implementation of Glyph-ByT5 and Glyph-ByT5-v2, introduced in &lt;a href=&#34;https://arxiv.org/abs/2403.09622&#34;&gt;Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2406.10208&#34;&gt;Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering &lt;/a&gt;. This repo contains the training code for glyph-alignment pretraining and inference code for our proposed Glyph-SDXL and Glyph-SDXL-v2 model.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;â›½ â›½ â›½ Contact: &lt;a href=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/yuhui.yuan@microsoft.com&#34;&gt;yuhui.yuan@microsoft.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024.06.17&lt;/strong&gt; Release the checkpoints and codes of Glyph-ByT5 and Glyph-ByT5-v2.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸ”†&lt;/span&gt; Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;We identify two crucial requirements of text encoders for achieving accurate visual text rendering: character awareness and alignment with glyphs. To this end, we propose a customized text encoder, Glyph-ByT5, by fine-tuning the character-aware ByT5 encoder using a meticulously curated paired glyph-text dataset.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We present an effective method for integrating Glyph-ByT5 with SDXL, resulting in the creation of the Glyph-SDXL model for design image generation. This significantly enhances text rendering accuracy, improving it from less than 20% to nearly 90% on our design image benchmark. Noteworthy is Glyph-SDXL&#39;s newfound ability for text paragraph rendering, achieving high spelling accuracy for tens to hundreds of characters with automated multi-line layouts.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We deliver a powerful customized multilingual text encoder, Glyph-ByT5-v2, and a strong aesthetic graphic generation model, Glyph-SDXL-v2, that can support accurate spelling in $\sim10$ different languages&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/paragraph_1.png&#34; alt=&#34;paragraph example 1&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/paragraph_2.png&#34; alt=&#34;paragraph example 2&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/paragraph_3.png&#34; alt=&#34;paragraph example 3&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/paragraph_4.png&#34; alt=&#34;paragraph example 4&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/design_1.png&#34; alt=&#34;design example 1&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/design_2.png&#34; alt=&#34;design example 2&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/design_3.png&#34; alt=&#34;design example 3&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/design_4.png&#34; alt=&#34;design example 4&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/scene_1.png&#34; alt=&#34;scene example 1&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/scene_2.png&#34; alt=&#34;scene example 2&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/scene_3.png&#34; alt=&#34;scene example 3&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/scene_4.png&#34; alt=&#34;scene example 4&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/cn_1.png&#34; alt=&#34;multilingual example 1&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/cn_2.png&#34; alt=&#34;multilingual example 2&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/cn_3.png&#34; alt=&#34;multilingual example 3&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/cn_4.png&#34; alt=&#34;multilingual example 4&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/fr_1.png&#34; alt=&#34;multilingual example 1&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/fr_2.png&#34; alt=&#34;multilingual example 2&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/fr_3.png&#34; alt=&#34;multilingual example 3&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/fr_4.png&#34; alt=&#34;multilingual example 4&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/de_1.png&#34; alt=&#34;multilingual example 1&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/de_2.png&#34; alt=&#34;multilingual example 2&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/de_3.png&#34; alt=&#34;multilingual example 3&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/de_4.png&#34; alt=&#34;multilingual example 4&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/jp_1.png&#34; alt=&#34;multilingual example 1&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/jp_2.png&#34; alt=&#34;multilingual example 2&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/jp_3.png&#34; alt=&#34;multilingual example 3&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/jp_4.png&#34; alt=&#34;multilingual example 4&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/kr_1.png&#34; alt=&#34;multilingual example 1&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/kr_2.png&#34; alt=&#34;multilingual example 2&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/kr_3.png&#34; alt=&#34;multilingual example 3&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/kr_4.png&#34; alt=&#34;multilingual example 4&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/es_1.png&#34; alt=&#34;multilingual example 1&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/es_2.png&#34; alt=&#34;multilingual example 2&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/es_3.png&#34; alt=&#34;multilingual example 3&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/es_4.png&#34; alt=&#34;multilingual example 4&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/it_1.png&#34; alt=&#34;multilingual example 1&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/it_2.png&#34; alt=&#34;multilingual example 2&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/it_3.png&#34; alt=&#34;multilingual example 3&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/it_4.png&#34; alt=&#34;multilingual example 4&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/pt_1.png&#34; alt=&#34;multilingual example 1&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/pt_2.png&#34; alt=&#34;multilingual example 2&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/pt_3.png&#34; alt=&#34;multilingual example 3&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/pt_4.png&#34; alt=&#34;multilingual example 4&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/ru_1.png&#34; alt=&#34;multilingual example 1&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/ru_2.png&#34; alt=&#34;multilingual example 2&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/ru_3.png&#34; alt=&#34;multilingual example 3&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/assets/teaser/ru_4.png&#34; alt=&#34;multilingual example 4&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸ”§&lt;/span&gt; Usage&lt;/h2&gt; &#xA;&lt;p&gt;For a detailed guide on Glyph-SDXL and Glyph-SDXL-v2 inference, see &lt;a href=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/inference/&#34;&gt;this folder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a detailed guide on Glyph-ByT5 alignment pretraining, see &lt;a href=&#34;https://raw.githubusercontent.com/AIGText/Glyph-ByT5/main/pretraining/&#34;&gt;this folder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸ”“&lt;/span&gt; Available Checkpoints&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Glyph-SDXL can be found &lt;a href=&#34;https://huggingface.co/GlyphByT5/Glyph-SDXL&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Glyph-SDXL-v2 can be found &lt;a href=&#34;https://huggingface.co/GlyphByT5/Glyph-SDXL-m&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Glyph alignment pretraining data can be found &lt;a href=&#34;https://huggingface.co/datasets/GlyphByT5/GlyphByT5Pretraining&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸ“¬&lt;/span&gt; Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this code useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{liu2024glyphbyt5,&#xA;    title={Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering},&#xA;    author={Zeyu Liu and Weicong Liang and Zhanhao Liang and Chong Luo and Ji Li and Gao Huang and Yuhui Yuan},&#xA;    year={2024},&#xA;    eprint={2403.09622},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{liu2024glyphbyt5v2,&#xA;    title={Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering}, &#xA;    author={Zeyu Liu and Weicong Liang and Yiming Zhao and Bohan Chen and Ji Li and Yuhui Yuan},&#xA;    year={2024},&#xA;    eprint={2406.10208},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>