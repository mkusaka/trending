<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-13T01:32:47Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVIDIA/Isaac-GR00T</title>
    <updated>2025-06-13T01:32:47Z</updated>
    <id>tag:github.com,2025-06-13:/NVIDIA/Isaac-GR00T</id>
    <link href="https://github.com/NVIDIA/Isaac-GR00T" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NVIDIA Isaac GR00T N1.5 is the world&#39;s first open foundation model for generalized humanoid robot reasoning and skills.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/header_compress.png&#34; width=&#34;800&#34; alt=&#34;NVIDIA Isaac GR00T N1.5 Header&#34;&gt; &#xA; &lt;!-- --- --&gt; &#xA; &lt;p style=&#34;font-size: 1.2em;&#34;&gt; &lt;a href=&#34;https://developer.nvidia.com/isaac/gr00t&#34;&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/nvidia/GR00T-N1.5-3B&#34;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim&#34;&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2503.14734&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/Isaac-GR00T/actions/workflows/main.yml&#34;&gt;&lt;img src=&#34;https://github.com/NVIDIA/Isaac-GR00T/actions/workflows/main.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pycqa.github.io/isort/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&amp;amp;labelColor=ef8336&#34; alt=&#34;Imports: isort&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://star-history.com/#NVIDIA/Isaac-GR00T&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/NVIDIA/Isaac-GR00T?style=flat-square&#34; alt=&#34;GitHub star chart&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NVIDIA/Isaac-GR00T/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-raw/NVIDIA/Isaac-GR00T?style=flat-square&#34; alt=&#34;Open Issues&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;NVIDIA Isaac GR00T&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/robot-demo.gif&#34; width=&#34;800&#34; alt=&#34;NVIDIA Isaac GR00T N1.5 Header&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We just released GR00T N1.5, an updated version of GR00T N1 with improved performance and new features. Check out the release blog post (&lt;a href=&#34;https://research.nvidia.com/labs/gear/gr00t-n1_5/&#34;&gt;https://research.nvidia.com/labs/gear/gr00t-n1_5/&lt;/a&gt;) for more details.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To use the older version, N1, please checkout the &lt;a href=&#34;https://github.com/NVIDIA/Isaac-GR00T/tree/n1-release&#34;&gt;n1-release&lt;/a&gt; release branch.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;NVIDIA Isaac GR00T N1.5 is an open foundation model for generalized humanoid robot reasoning and skills. This cross-embodiment model takes multimodal input, including language and images, to perform manipulation tasks in diverse environments.&lt;/p&gt; &#xA;&lt;p&gt;GR00T N1.5 is trained on an expansive humanoid dataset, consisting of real captured data, synthetic data generated using the components of NVIDIA Isaac GR00T Blueprint (&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/videos&#34;&gt;examples of neural-generated trajectories&lt;/a&gt;), and internet-scale video data. It is adaptable through post-training for specific embodiments, tasks and environments.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/real-data.gif&#34; height=&#34;150&#34; alt=&#34;real-robot-data&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/sim-data.gif&#34; height=&#34;150&#34; alt=&#34;sim-robot-data&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The neural network architecture of GR00T N1.5 is a combination of vision-language foundation model and diffusion transformer head that denoises continuous actions. Here is a schematic diagram of the architecture:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/model-architecture.png&#34; width=&#34;800&#34; alt=&#34;model-architecture&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Here is the general procedure to use GR00T N1.5:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Assume the user has already collected a dataset of robot demonstrations in the form of (video, state, action) triplets.&lt;/li&gt; &#xA; &lt;li&gt;The user will first convert the demonstration data into the LeRobot compatible data schema (more info in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/LeRobot_compatible_data_schema.md&#34;&gt;&lt;code&gt;getting_started/LeRobot_compatible_data_schema.md&lt;/code&gt;&lt;/a&gt;), which is compatible with the upstream &lt;a href=&#34;https://github.com/huggingface/lerobot&#34;&gt;Huggingface LeRobot&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Our repo provides examples of different configurations for training with different robot embodiments.&lt;/li&gt; &#xA; &lt;li&gt;Our repo provides convenient scripts for finetuning the pre-trained GR00T N1.5 model on user&#39;s data, and running inference.&lt;/li&gt; &#xA; &lt;li&gt;The user will connect the &lt;code&gt;Gr00tPolicy&lt;/code&gt; to the robot controller to execute actions on their target hardware.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;What&#39;s New in GR00T N1.5&lt;/h2&gt; &#xA;&lt;p&gt;GR00T N1.5 represents a significant upgrade over GR00T N1, with improvements in both model architecture and data leading to better performance in many aspects.&lt;/p&gt; &#xA;&lt;h3&gt;Model and Data Improvements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Frozen VLM&lt;/strong&gt;: The vision-language model remains frozen during both pretraining and finetuning, preserving language understanding and improving generalization&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhanced VLM Grounding&lt;/strong&gt;: Updated to Eagle 2.5 with improved grounding capabilities and physical understanding, achieving 40.4 IoU on GR-1 grounding tasks (vs 35.5 for Qwen2.5VL).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simplified Adapter&lt;/strong&gt;: Streamlined MLP connection between vision encoder and LLM with added layer normalization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FLARE Integration&lt;/strong&gt;: Added Future Latent Representation Alignment (&lt;a href=&#34;https://research.nvidia.com/labs/gear/flare&#34;&gt;FLARE&lt;/a&gt;) objective alongside flow matching loss, enabling effective learning from human ego videos&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DreamGen Integration&lt;/strong&gt;: Incorporated synthetic neural trajectories generated via &lt;a href=&#34;https://research.nvidia.com/labs/gear/dreamgen&#34;&gt;DreamGen&lt;/a&gt; to enable generalization to novel behaviors and tasks beyond teleoperation data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Performance Improvements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Language Following&lt;/strong&gt;: Significantly improved language command following versus N1 - 93.3% vs 46.6% on GR-1 manipulation tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data Efficiency&lt;/strong&gt;: Better performance in low-data regimes (0-shot and few-shot scenarios)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Better Novel Object Generalization&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;New Embodiment Heads&lt;/strong&gt;: Added support for single arm robots with end-effector (EEF) control space via &lt;code&gt;EmbodimentTag.OXE_DROID&lt;/code&gt; head, and humanoid robots with grippers via &lt;code&gt;EmbodimentTag.AGIBOT_GENIE1&lt;/code&gt; head, expanding beyond joint space control to enable broader robot compatibility&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These improvements make GR00T N1.5 particularly effective for applications requiring strong language understanding, few-shot adaptation, and generalization to novel objects and environments. See our GR00T N1.5 &lt;a href=&#34;https://research.nvidia.com/labs/gear/gr00t-n1_5&#34;&gt;tech blog&lt;/a&gt; for more details on the model and experimental results.&lt;/p&gt; &#xA;&lt;h2&gt;Target Audience&lt;/h2&gt; &#xA;&lt;p&gt;GR00T N1.5 is intended for researchers and professionals in humanoid robotics. This repository provides tools to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Leverage a pre-trained foundation model for robot control&lt;/li&gt; &#xA; &lt;li&gt;Fine-tune on small, custom datasets&lt;/li&gt; &#xA; &lt;li&gt;Adapt the model to specific robotics tasks with minimal data&lt;/li&gt; &#xA; &lt;li&gt;Deploy the model for inference&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The focus is on enabling customization of robot behaviors through finetuning.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We have tested the code on Ubuntu 20.04 and 22.04, GPU: H100, L40, RTX 4090 and A6000 for finetuning and Python==3.10, CUDA version 12.4.&lt;/li&gt; &#xA; &lt;li&gt;For inference, we have tested on Ubuntu 20.04 and 22.04, GPU: RTX 3090, RTX 4090 and A6000.&lt;/li&gt; &#xA; &lt;li&gt;If you haven&#39;t installed CUDA 12.4, please follow the instructions &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/&#34;&gt;here&lt;/a&gt; to install it.&lt;/li&gt; &#xA; &lt;li&gt;If you haven&#39;t installed tensorrt, please follow the instructions &lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/latest/installing-tensorrt/installing.html#&#34;&gt;here&lt;/a&gt; to install it.&lt;/li&gt; &#xA; &lt;li&gt;Please make sure you have the following dependencies installed in your system: &lt;code&gt;ffmpeg&lt;/code&gt;, &lt;code&gt;libsm6&lt;/code&gt;, &lt;code&gt;libxext6&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation Guide&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/NVIDIA/Isaac-GR00T&#xA;cd Isaac-GR00T&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a new conda environment and install the dependencies. We recommend Python 3.10:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note that, please make sure your CUDA version is 12.4. Otherwise, you may have a hard time with properly configuring flash-attn module.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda create -n gr00t python=3.10&#xA;conda activate gr00t&#xA;pip install --upgrade setuptools&#xA;pip install -e .[base]&#xA;pip install --no-build-isolation flash-attn==2.7.1.post4 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting started with this repo&lt;/h2&gt; &#xA;&lt;p&gt;We provide accessible Jupyter notebooks and detailed documentation in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started&#34;&gt;&lt;code&gt;./getting_started&lt;/code&gt;&lt;/a&gt; folder. Utility scripts can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/scripts&#34;&gt;&lt;code&gt;./scripts&lt;/code&gt;&lt;/a&gt; folder. Additionally, a comprehensive tutorial for finetuning the model on the SO-101 robot is available on &lt;a href=&#34;https://huggingface.co/blog/nvidia/gr00t-n1-5-so101-tuning&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;1. Data Format &amp;amp; Loading&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To load and process the data, we use &lt;a href=&#34;https://github.com/huggingface/lerobot&#34;&gt;Huggingface LeRobot data&lt;/a&gt;, but with a more detailed modality and annotation schema (we call it &#34;LeRobot compatible data schema&#34;).&lt;/li&gt; &#xA; &lt;li&gt;An example of LeRobot dataset is stored here: &lt;code&gt;./demo_data/robot_sim.PickNPlace&lt;/code&gt;. (with additional &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/demo_data/robot_sim.PickNPlace/meta/modality.json&#34;&gt;&lt;code&gt;modality.json&lt;/code&gt;&lt;/a&gt; file)&lt;/li&gt; &#xA; &lt;li&gt;Detailed explanation of the dataset format is available in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/LeRobot_compatible_data_schema.md&#34;&gt;&lt;code&gt;getting_started/LeRobot_compatible_data_schema.md&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;We support multiple embodiments with the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/4_deeper_understanding.md#embodiment-action-head-fine-tuning&#34;&gt;&lt;code&gt;EmbodimentTag&lt;/code&gt;&lt;/a&gt; system.&lt;/li&gt; &#xA; &lt;li&gt;Once your data is organized in this format, you can load the data using &lt;code&gt;LeRobotSingleDataset&lt;/code&gt; class.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from gr00t.data.dataset import LeRobotSingleDataset&#xA;from gr00t.data.embodiment_tags import EmbodimentTag&#xA;from gr00t.data.dataset import ModalityConfig&#xA;from gr00t.experiment.data_config import DATA_CONFIG_MAP&#xA;&#xA;# get the data config&#xA;data_config = DATA_CONFIG_MAP[&#34;fourier_gr1_arms_only&#34;]&#xA;&#xA;# get the modality configs and transforms&#xA;modality_config = data_config.modality_config()&#xA;transforms = data_config.transform()&#xA;&#xA;# This is a LeRobotSingleDataset object that loads the data from the given dataset path.&#xA;dataset = LeRobotSingleDataset(&#xA;    dataset_path=&#34;demo_data/robot_sim.PickNPlace&#34;,&#xA;    modality_configs=modality_config,&#xA;    transforms=None,  # we can choose to not apply any transforms&#xA;    embodiment_tag=EmbodimentTag.GR1, # the embodiment to use&#xA;)&#xA;&#xA;# This is an example of how to access the data.&#xA;dataset[5]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/0_load_dataset.ipynb&#34;&gt;&lt;code&gt;getting_started/0_load_dataset.ipynb&lt;/code&gt;&lt;/a&gt; is an interactive tutorial on how to load the data and process it to interface with the GR00T N1.5 model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/scripts/load_dataset.py&#34;&gt;&lt;code&gt;scripts/load_dataset.py&lt;/code&gt;&lt;/a&gt; is an executable script with the same content as the notebook.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Try run the script to load the dataset&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/load_dataset.py --dataset-path ./demo_data/robot_sim.PickNPlace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. Inference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The GR00T N1.5 model is hosted on &lt;a href=&#34;https://huggingface.co/nvidia/GR00T-N1.5-3B&#34;&gt;Huggingface&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Example cross embodiment dataset is available at &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/demo_data/robot_sim.PickNPlace&#34;&gt;demo_data/robot_sim.PickNPlace&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2.1 Inference with PyTorch&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from gr00t.model.policy import Gr00tPolicy&#xA;from gr00t.data.embodiment_tags import EmbodimentTag&#xA;&#xA;# 1. Load the modality config and transforms, or use above&#xA;modality_config = ComposedModalityConfig(...)&#xA;transforms = ComposedModalityTransform(...)&#xA;&#xA;# 2. Load the dataset&#xA;dataset = LeRobotSingleDataset(.....&amp;lt;Same as above&amp;gt;....)&#xA;&#xA;# 3. Load pre-trained model&#xA;policy = Gr00tPolicy(&#xA;    model_path=&#34;nvidia/GR00T-N1.5-3B&#34;,&#xA;    modality_config=modality_config,&#xA;    modality_transform=transforms,&#xA;    embodiment_tag=EmbodimentTag.GR1,&#xA;    device=&#34;cuda&#34;&#xA;)&#xA;&#xA;# 4. Run inference&#xA;action_chunk = policy.get_action(dataset[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/1_gr00t_inference.ipynb&#34;&gt;&lt;code&gt;getting_started/1_gr00t_inference.ipynb&lt;/code&gt;&lt;/a&gt; is an interactive Jupyter notebook tutorial to build an inference pipeline.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;User can also run the inference service using the provided script. The inference service can run in either server mode or client mode.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/inference_service.py --model-path nvidia/GR00T-N1.5-3B --server&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On a different terminal, run the client mode to send requests to the server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/inference_service.py  --client&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2.2 Inference with Python TensorRT (Optional)&lt;/h3&gt; &#xA;&lt;p&gt;To inference with ONNX and TensorRT, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/deployment_scripts/README.md&#34;&gt;&lt;code&gt;deployment_scripts/README.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;3. Fine-Tuning&lt;/h2&gt; &#xA;&lt;p&gt;Users can run the finetuning script below to finetune the model with the example dataset. A tutorial is available in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/2_finetuning.ipynb&#34;&gt;&lt;code&gt;getting_started/2_finetuning.ipynb&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then run the finetuning script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# first run --help to see the available arguments&#xA;python scripts/gr00t_finetune.py --help&#xA;&#xA;# then run the script&#xA;python scripts/gr00t_finetune.py --dataset-path ./demo_data/robot_sim.PickNPlace --num-gpus 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are finetuning on a 4090, you need to pass the &lt;code&gt;--no-tune_diffusion_model&lt;/code&gt; flag when running &lt;code&gt;gr00t_finetune.py&lt;/code&gt; to avoid CUDA out of memory.&lt;/p&gt; &#xA;&lt;p&gt;You can also download a sample dataset from our huggingface sim data release &lt;a href=&#34;https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;huggingface-cli download  nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim \&#xA;  --repo-type dataset \&#xA;  --include &#34;gr1_arms_only.CanSort/**&#34; \&#xA;  --local-dir $HOME/gr00t_dataset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The recommended finetuning configuration is to boost your batch size to the max, and train for 20k steps.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Hardware Performance Considerations&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Finetuning Performance&lt;/strong&gt;: We used 1 H100 node or L40 node for optimal finetuning. Other hardware configurations (e.g. A6000, RTX 4090) will also work but may take longer to converge. The exact batch size is dependent on the hardware, and on which component of the model is being tuned.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LoRA finetuning&lt;/strong&gt;: We used 2 A6000 GPUs or 2 RTX 4090 GPUs for LoRA finetuning. Users can try out different configurations for effective finetuning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inference Performance&lt;/strong&gt;: For real-time inference, most modern GPUs perform similarly when processing a single sample. Our benchmarks show minimal difference between L40 and RTX 4090 for inference speed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For new embodiment finetuning, checkout our notebook in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/3_0_new_embodiment_finetuning.md&#34;&gt;&lt;code&gt;getting_started/3_0_new_embodiment_finetuning.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Choosing the Right Embodiment Head&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/robots-banner.png&#34; width=&#34;1000&#34; alt=&#34;robots-banner&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;GR00T N1.5 provides three pretrained embodiment heads optimized for different robot configurations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;EmbodimentTag.GR1&lt;/code&gt;&lt;/strong&gt;: Designed for humanoid robots with dexterous hands using absolute joint space control&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;EmbodimentTag.OXE_DROID&lt;/code&gt;&lt;/strong&gt;: Optimized for single arm robots using delta end-effector (EEF) control&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;EmbodimentTag.AGIBOT_GENIE1&lt;/code&gt;&lt;/strong&gt;: Built for humanoid robots with grippers using absolute joint space control&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;EmbodimentTag.NEW_EMBODIMENT&lt;/code&gt;&lt;/strong&gt;: (Non-pretrained) New embodiment head for finetuning on new robot embodiments&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Select the embodiment head that best matches your robot&#39;s configuration for optimal finetuning performance. For detailed information on the observation and action spaces, see &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/4_deeper_understanding.md#embodiment-action-head-fine-tuning&#34;&gt;&lt;code&gt;EmbodimentTag&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;4. Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To conduct an offline evaluation of the model, we provide a script that evaluates the model on a dataset and plots it out. Quick try: &lt;code&gt;python scripts/eval_policy.py --plot --model_path nvidia/GR00T-N1.5-3B&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or you can run the newly trained model in client-server mode.&lt;/p&gt; &#xA;&lt;p&gt;Run the newly trained model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/inference_service.py --server \&#xA;    --model-path &amp;lt;MODEL_PATH&amp;gt; \&#xA;    --embodiment-tag new_embodiment&#xA;    --data-config &amp;lt;DATA_CONFIG&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the offline evaluation script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/eval_policy.py --plot \&#xA;    --dataset-path &amp;lt;DATASET_PATH&amp;gt; \&#xA;    --embodiment-tag new_embodiment \&#xA;    --data-config &amp;lt;DATA_CONFIG&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will then see a plot of Ground Truth vs Predicted actions, along with unnormed MSE of the actions. This would give you an indication if the policy is performing well on the dataset.&lt;/p&gt; &#xA;&lt;h2&gt;Jetson Deployment&lt;/h2&gt; &#xA;&lt;p&gt;A detailed guide for deploying GR00T N1.5 on Jetson is available in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/deployment_scripts/README.md&#34;&gt;&lt;code&gt;deployment_scripts/README.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s comparison of E2E performance between PyTorch and TensorRT on Orin&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/media/orin-perf.png&#34; width=&#34;800&#34; alt=&#34;orin-perf&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Model latency measured by &lt;code&gt;trtexec&lt;/code&gt; with batch_size=1.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Orin benchmark perf (ms)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Precision&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Action_Head - process_backbone_output&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Action_Head - state_encoder&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Action_Head - action_encoder&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Action_Head - DiT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.77&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Action_Head - action_decoder&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VLM - ViT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.96&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VLM - LLM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The module latency (e.g., DiT Block) in pipeline is slightly longer than the model latency in benchmark table above because the module (e.g., Action_Head - DiT) latency not only includes the model latency in table above but also accounts for the overhead of data transfer from PyTorch to TRT and returning from TRT to PyTorch.&lt;/p&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Does it work on CUDA ARM Linux?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Yes, visit &lt;a href=&#34;https://github.com/dusty-nv/jetson-containers/tree/master/packages/robots/Isaac-GR00T&#34;&gt;jetson-containers&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;I have my own data, what should I do next for finetuning?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This repo assumes that your data is already organized according to the LeRobot format.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;What is Modality Config? Embodiment Tag? and Transform Config?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Embodiment Tag: Defines the robot embodiment used, non-pretrained embodiment tags are all considered as &lt;code&gt;new_embodiment&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Modality Config: Defines the modalities used in the dataset (e.g. video, state, action)&lt;/li&gt; &#xA; &lt;li&gt;Transform Config: Defines the Data Transforms applied to the data during dataloading.&lt;/li&gt; &#xA; &lt;li&gt;For more details, see &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/getting_started/4_deeper_understanding.md&#34;&gt;&lt;code&gt;getting_started/4_deeper_understanding.md&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;What is the inference speed for Gr00tPolicy?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Below are benchmark results based on a single H100 GPU. Performance will be slightly slower on consumer GPUs like RTX 4090 for inference (single sample processing):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Module&lt;/th&gt; &#xA;   &lt;th&gt;Inference Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VLM Backbone&lt;/td&gt; &#xA;   &lt;td&gt;23.18 ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Action Head with 4 diffusion steps&lt;/td&gt; &#xA;   &lt;td&gt;4 x 6.18 ms = 24.7 ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Full Model&lt;/td&gt; &#xA;   &lt;td&gt;47.88 ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We noticed that 4 denoising steps are sufficient during inference.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;How to train with multiple datasets?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can train with multiple datasets by providing a list of dataset paths to the &lt;code&gt;dataset_path&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/gr00t_finetune.py --dataset-path &amp;lt;DATASET1&amp;gt; &amp;lt;DATASET2&amp;gt; --num-gpus 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the &lt;code&gt;gr00t_finetune.py&lt;/code&gt; imposes equal weights to all datasets, with &lt;code&gt;balance_dataset_weights&lt;/code&gt; and &lt;code&gt;balance_trajectory_weights&lt;/code&gt; set to &lt;code&gt;True&lt;/code&gt;. For more details, see the &lt;code&gt;LeRobotMixtureDataset&lt;/code&gt; class definition in &lt;code&gt;gr00t/data/dataset.py&lt;/code&gt;. Users can also use the &lt;code&gt;LeRobotMixtureDataset&lt;/code&gt; class directly to train with multiple datasets with different embodiments, transforms, and sampling weights.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Is LoRA finetuning supported?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes, you can use LoRA finetuning to finetune the model. This can be enabled by indicating &lt;code&gt;--lora_rank 64 --lora_alpha 128&lt;/code&gt; in the finetuning script. However, we recommend using the full model finetuning for better performance.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;For more details, see &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/Isaac-GR00T/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION &amp;amp; AFFILIATES. All rights reserved.&#xA;# SPDX-License-Identifier: Apache-2.0&#xA;#&#xA;# Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);&#xA;# you may not use this file except in compliance with the License.&#xA;# You may obtain a copy of the License at&#xA;#&#xA;# http://www.apache.org/licenses/LICENSE-2.0&#xA;#&#xA;# Unless required by applicable law or agreed to in writing, software&#xA;# distributed under the License is distributed on an &#34;AS IS&#34; BASIS,&#xA;# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xA;# See the License for the specific language governing permissions and&#xA;# limitations under the License.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>