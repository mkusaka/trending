<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-07T01:36:44Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cohere-ai/notebooks</title>
    <updated>2024-04-07T01:36:44Z</updated>
    <id>tag:github.com,2024-04-07:/cohere-ai/notebooks</id>
    <link href="https://github.com/cohere-ai/notebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code examples and jupyter notebooks for the Cohere Platform&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cohere Cookbook&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the Cohere Cookbook! This repository provides a collection of examples to help you get started with the Cohere API. These examples contain step-by-step guides, with code examples and explanations, to help you understand and use the API effectively.&lt;/p&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;The cookbook is grouped into two categories. To get started, go to any of the categories below. You will find more details there, but here&#39;s a summary:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Category&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cohere-ai/notebooks/main/notebooks/guides/&#34;&gt;Guides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tutorials and step-by-step guides covering a range of topics, providing practical guidance and code examples.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cohere-ai/notebooks/main/notebooks/llmu/&#34;&gt;LLM University&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Guides for getting started with Cohere, starting with basic usage and progressing to advanced topics. The code companion to the full &lt;a href=&#34;https://llm.university/&#34;&gt;LLM University course&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition</title>
    <updated>2024-04-07T01:36:44Z</updated>
    <id>tag:github.com,2024-04-07:/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition</id>
    <link href="https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Transformers 3rd Edition&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Transformers for Natural Language Processing and Computer Vision: Take Generative AI and LLMs to the next level with Hugging Face, Google Vertex AI, ChatGPT, GPT-4V, and DALL-E 3 3rd Edition&lt;br&gt;&lt;/h1&gt; &#xA;&lt;p&gt;by Denis Rothman &lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/Denis2054/Transformers_3rd_Edition/raw/main/Transformers_3rd_Edition.jpg?raw=tru&#34; alt=&#34;drawing&#34; width=&#34;400&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Last updated: March 29, 2024&lt;br&gt; Look for the üê¨ for bonus notebooks!&lt;/p&gt; &#xA;&lt;h1&gt;Transformers-for-NLP-and-Computer-Vision-3rd-Edition&lt;/h1&gt; &#xA;&lt;p&gt;This is the code repository for &lt;a href=&#34;https://www.amazon.com/Transformers-Natural-Language-Processing-Computer/dp/1805128728/&#34;&gt;Transformers for Natural Language Processing and Computer Vision&lt;/a&gt;, published by Packt.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Explore Generative AI and Large Language Models with Hugging Face, ChatGPT, GPT-4V, and DALL-E 3&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About the book&lt;/h2&gt; &#xA;&lt;p&gt;Transformers for Natural Language Processing and Computer Vision, Third Edition, explores &lt;strong&gt;Large Language Model&lt;/strong&gt; (&lt;strong&gt;LLM&lt;/strong&gt;) architectures, applications, and various platforms (Hugging Face, OpenAI, and Google Vertex AI) used for &lt;strong&gt;Natural Language Processing&lt;/strong&gt; (&lt;strong&gt;NLP&lt;/strong&gt;) and &lt;strong&gt;Computer Vision&lt;/strong&gt; (&lt;strong&gt;CV&lt;/strong&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Dive into generative vision transformers and multimodal model architectures and build applications, such as image and video-to-text classifiers. Go further by combining different models and platforms and learning about AI agent replication.&lt;/p&gt; &#xA;&lt;h2&gt;What you will learn&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Learn how to pretrain and fine-tune LLMs&lt;/li&gt; &#xA; &lt;li&gt;Learn how to work with multiple platforms, such as Hugging Face, OpenAI, and Google Vertex AI&lt;/li&gt; &#xA; &lt;li&gt;Learn about different tokenizers and the best practices for preprocessing language data&lt;/li&gt; &#xA; &lt;li&gt;Implement Retrieval Augmented Generation and rules bases to mitigate hallucinations&lt;/li&gt; &#xA; &lt;li&gt;Visualize transformer model activity for deeper insights using BertViz, LIME, and SHAP&lt;/li&gt; &#xA; &lt;li&gt;Create and implement cross-platform chained models, such as HuggingGPT&lt;/li&gt; &#xA; &lt;li&gt;Go in-depth into vision transformers with CLIP, DALL-E 2, DALL-E 3, and GPT-4V&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;h3&gt;Chapters&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;What Are Transformers?&lt;/li&gt; &#xA; &lt;li&gt;Getting Started with the Architecture of the Transformer Model&lt;/li&gt; &#xA; &lt;li&gt;Emergent vs Downstream Tasks: The Unseen Depths of Transformers&lt;/li&gt; &#xA; &lt;li&gt;Advancements in Translations with Google Trax, Google Translate, and Gemini&lt;/li&gt; &#xA; &lt;li&gt;Diving into Fine-Tuning through BERT&lt;/li&gt; &#xA; &lt;li&gt;Pretraining a Transformer from Scratch through RoBERTa&lt;/li&gt; &#xA; &lt;li&gt;The Generative AI Revolution with ChatGPT&lt;/li&gt; &#xA; &lt;li&gt;Fine-Tuning OpenAI GPT Models&lt;/li&gt; &#xA; &lt;li&gt;Shattering the Black Box with Interpretable Tools&lt;/li&gt; &#xA; &lt;li&gt;Investigating the Role of Tokenizers in Shaping Transformer Models&lt;/li&gt; &#xA; &lt;li&gt;Leveraging LLM Embeddings as an Alternative to Fine-Tuning&lt;/li&gt; &#xA; &lt;li&gt;Toward Syntax-Free Semantic Role Labeling with ChatGPT and GPT-4&lt;/li&gt; &#xA; &lt;li&gt;Summarization with T5 and ChatGPT&lt;/li&gt; &#xA; &lt;li&gt;Exploring Cutting-Edge LLMs with Vertex AI and PaLM 2&lt;/li&gt; &#xA; &lt;li&gt;Guarding the Giants: Mitigating Risks in Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;Beyond Text: Vision Transformers in the Dawn of Revolutionary AI&lt;/li&gt; &#xA; &lt;li&gt;Transcending the Image-Text Boundary with Stable Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face AutoTrain: Training Vision Models without Coding&lt;/li&gt; &#xA; &lt;li&gt;On the Road to Functional AGI with HuggingGPT and its Peers&lt;/li&gt; &#xA; &lt;li&gt;Beyond Human-Designed Prompts with Generative Ideation&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Appendix&lt;/h3&gt; &#xA;&lt;p&gt;Appendix: Answers to the Questions&lt;/p&gt; &#xA;&lt;h3&gt;Platforms&lt;/h3&gt; &#xA;&lt;p&gt;You can run the notebooks directly from below table:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Chapter&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Colab&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Kaggle&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Gradient&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;StudioLab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Part I The Foundations of Transformer Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 1: What are Transformers?&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;O_1_and_Accelerators.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;ChatGPT_Plus_writes_and_explains_AI.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter01/O_1_and_Accelerators.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter01/ChatGPT_Plus_writes_and_explains_AI.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter01/O_1_and_Accelerators.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter01/ChatGPT_Plus_writes_and_explains_AI.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter01/O_1_and_Accelerators.ipynb?file=%2FChapter01%2FO_1_and_Accelerators.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter01/ChatGPT_Plus_writes_and_explains_AI.ipynb?file=%2FChapter01%2FChatGPT_Plus_writes_and_explains_AI.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter01/O_1_and_Accelerators.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter01/ChatGPT_Plus_writes_and_explains_AI.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 2: Getting Started with the Architecture of the Transformer Model&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Multi_Head_Attention_Sub_Layer.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;positional_encoding.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter02/Multi_Head_Attention_Sub_Layer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter02/positional_encoding.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter02/Multi_Head_Attention_Sub_Layer.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter02/positional_encoding.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter02/Multi_Head_Attention_Sub_Layer.ipynb?file=%2FChapter02%2FMulti_Head_Attention_Sub_Layer.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter02/positional_encoding.ipynb?file=%2FChapter02%2Fpositional_encoding.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter02/Multi_Head_Attention_Sub_Layer.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter02/positional_encoding.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 3: Emergent vs Downstream Tasks: the Unseen Depths of Transformers&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;From_training_to_emergence.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Transformer_tasks_with_Hugging_Face.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter03/From_training_to_emergence.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter03/Transformer_tasks_with_Hugging_Face.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter03/From_training_to_emergence.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter03/Transformer_tasks_with_Hugging_Face.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter03/From_training_to_emergence.ipynb?file=%2FChapter03%2FFrom_training_to_emergence.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter03/Transformer_tasks_with_Hugging_Face.ipynb?file=%2FChapter03%2FTransformer_tasks_with_Hugging_Face.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter03/From_training_to_emergence.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter03/Transformer_tasks_with_Hugging_Face.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 4: Advancements in Translations with Google Trax, Google Translate, and Google Bard&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;WMT_translations.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Trax_Google_Translate.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter04/WMT_translations.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter04/Trax_Google_Translate.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter04/WMT_translations.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter04/Trax_Google_Translate.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter04/WMT_translations.ipynb?file=%2FChapter04%2FWMT_translations.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter04/Trax_Google_Translate.ipynb?file=%2FChapter04%2FTrax_Google_Translate.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter04/WMT_translations.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter04/Trax_Google_Translate.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 5: Diving into Fine-Tuning through BERT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter05/BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter05/BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter05/BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb?file=%2FChapter05%2FBERT_Fine_Tuning_Sentence_Classification_GPU.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter05/BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 6: Pretraining a Transformer from Scratch through RoBERTa&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;KantaiBERT.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Customer_Support_for_X.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter06/KantaiBERT.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter06/Customer_Support_for_X.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter06/KantaiBERT.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter06/Customer_Support_for_X.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter06/KantaiBERT.ipynb?file=%2FChapter06%2FKantaiBERT.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter06/Customer_Support_for_X.ipynb?file=%2FChapter06%2FCustomer_Support_for_X.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter06/KantaiBERT.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter06/Customer_Support_for_X.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Part II: The Rise of Suprahuman NLP&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 7: The Generative AI Revolution with ChatGPT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;OpenAI_Models.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;OpenAI_GPT_4_Assistant.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Getting_Started_GPT_4_API.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;GPT_4_RAG.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter07/OpenAI_Models.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter07/OpenAI_GPT_4_Assistant.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter07/Getting_Started_GPT_4_API.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter07/GPT_4_RAG.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter07/OpenAI_Models.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter07/OpenAI_GPT_4_Assistant.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter07/Getting_Started_GPT_4_API.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter07/GPT_4_RAG.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter07/OpenAI_Models.ipynb?file=%2FChapter07%2FOpenAI_Models.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter07/OpenAI_GPT_4_Assistant.ipynb?file=%2FChapter07%2FOpenAI_GPT_4_Assistant.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter07/Getting_Started_GPT_4_API.ipynb?file=%2FChapter07%2FGetting_Started_GPT_4_API.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter07/GPT_4_RAG.ipynb?file=%2FChapter07%2FGPT_4_RAG.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter07/OpenAI_Models.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter07/OpenAI_GPT_4_Assistant.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter07/Getting_Started_GPT_4_API.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter07/GPT_4_RAG.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 8: Fine-tuning OpenAI Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Fine_tuning_OpenAI_Models.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter08/Fine_tuning_OpenAI_Models.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter08/Fine_tuning_OpenAI_Models.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter08/Fine_tuning_OpenAI_Models.ipynb?file=%2FChapter08%2FFine_tuning_OpenAI_Models.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter08/Fine_tuning_OpenAI_Models.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 9: Shattering the Black Box with Interpretable tools&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;BertViz_Interactive.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Hugging_Face_SHAP.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter09/BertViz_Interactive.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter09/Hugging_Face_SHAP.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter09/BertViz_Interactive.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter09/Hugging_Face_SHAP.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter09/BertViz_Interactive.ipynb?file=%2FChapter09%2FBertViz_Interactive.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter09/Hugging_Face_SHAP.ipynb?file=%2FChapter09%2FHugging_Face_SHAP.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter09/BertViz_Interactive.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter09/Hugging_Face_SHAP.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 10: Investigating the Role of Tokenizers in Shaping Transformer Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Tokenizers.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Sub_word_tokenizers.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Exploring_tokenizers.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter10/Tokenizers.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter10/Sub_word_tokenizers.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter10/Exploring_tokenizers.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter10/Tokenizers.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter10/Sub_word_tokenizers.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter10/Exploring_tokenizers.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter10/Tokenizers.ipynb?file=%2FChapter10%2FTokenizers.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter10/Sub_word_tokenizers.ipynb?file=%2FChapter10%2FSub_word_tokenizers.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter10/Exploring_tokenizers.ipynb?file=%2FChapter10%2FExploring_tokenizers.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter10/Tokenizers.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter10/Sub_word_tokenizers.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter10/Exploring_tokenizers.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 11: Leveraging LLM Embeddings as an Alternative to Fine-Tuning&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Embedding_with_NLKT_Gensim.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Question_answering_with_embeddings.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Transfer_Learning_with_Ada_Embeddings.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter11/Embedding_with_NLKT_Gensim.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter11/Question_answering_with_embeddings.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter11/Transfer_Learning_with_Ada_Embeddings.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter11/Embedding_with_NLKT_Gensim.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter11/Question_answering_with_embeddings.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter11/Transfer_Learning_with_Ada_Embeddings.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter11/Embedding_with_NLKT_Gensim.ipynb?file=%2FChapter11%2FEmbedding_with_NLKT_Gensim.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter11/Question_answering_with_embeddings.ipynb?file=%2FChapter11%2FQuestion_answering_with_embeddings.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter11/Transfer_Learning_with_Ada_Embeddings.ipynb?file=%2FChapter11%2FTransfer_Learning_with_Ada_Embeddings.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter11/Embedding_with_NLKT_Gensim.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter11/Question_answering_with_embeddings.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter11/Transfer_Learning_with_Ada_Embeddings.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 12: Towards Syntax-Free Semantic Role Labeling with BERT and OpenAI&#39;s ChatGPT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Semantic_Role_Labeling_GPT-4.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter12/Semantic_Role_Labeling_GPT-4.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter12/Semantic_Role_Labeling_GPT-4.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter12/Semantic_Role_Labeling_GPT-4.ipynb?file=%2FChapter12%2FSemantic_Role_Labeling_GPT-4.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter12/Semantic_Role_Labeling_GPT-4.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 13: Summarization with T5 and ChatGPT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Summerizing_Text_T5.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Summarizing_ChatGPT.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter13/Summerizing_Text_T5.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter13/Summarizing_ChatGPT.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter13/Summerizing_Text_T5.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter13/Summarizing_ChatGPT.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter13/Summerizing_Text_T5.ipynb?file=%2FChapter13%2FSummerizing_Text_T5.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter13/Summarizing_ChatGPT.ipynb?file=%2FChapter13%2FSummarizing_ChatGPT.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter13/Summerizing_Text_T5.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter13/Summarizing_ChatGPT.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 14: Exploring Cutting-Edge NLP with Google Vertex AI and PaLM 2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Google_Vertex_AI.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter14/Google_Vertex_AI.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter14/Google_Vertex_AI.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter14/Google_Vertex_AI.ipynb?file=%2FChapter14%2FGoogle_Vertex_AI.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter14/Google_Vertex_AI.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 15: Guarding the Giants: Mitigating Risks in Large Language Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Auto_Big_bench.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;WandB_Prompts_Quickstart.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Encoder_decoder_transformer.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Mitigating_Generative_AI.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter15/Auto_Big_bench.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter15/WandB_Prompts_Quickstart.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter15/Encoder_decoder_transformer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter15/Mitigating_Generative_AI.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter15/Auto_Big_bench.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter15/WandB_Prompts_Quickstart.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter15/Encoder_decoder_transformer.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter15/Mitigating_Generative_AI.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter15/Auto_Big_bench.ipynb?file=%2FChapter15%2FAuto_Big_bench.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter15/WandB_Prompts_Quickstart.ipynb?file=%2FChapter15%2FWandB_Prompts_Quickstart.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter15/Encoder_decoder_transformer.ipynb?file=%2FChapter15%2FEncoder_decoder_transformer.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter15/Mitigating_Generative_AI.ipynb?file=%2FChapter15%2FMitigating_Generative_AI.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter15/Auto_Big_bench.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter15/WandB_Prompts_Quickstart.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter15/Encoder_decoder_transformer.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter15/Mitigating_Generative_AI.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Part III: Generative Computer Vision: A New Way to See the World&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 16: Vision Transformers in the Dawn of Revolutionary AI&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;ViT_CLIP.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Getting_Started_DALL_E_API.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;GPT-4V.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter16/ViT_CLIP.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter16/Getting_Started_DALL_E_API.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter16/GPT-4V.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter16/ViT_CLIP.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter16/Getting_Started_DALL_E_API.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter16/GPT-4V.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter16/ViT_CLIP.ipynb?file=%2FChapter16%2FViT_CLIP.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter16/Getting_Started_DALL_E_API.ipynb?file=%2FChapter16%2FGetting_Started_DALL_E_API.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter16/GPT-4V.ipynb?file=%2FChapter16%2FGPT-4V.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter16/ViT_CLIP.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter16/Getting_Started_DALL_E_API.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter16/GPT-4V.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 17: Transcending the Image-Text Boundary with Stable Diffusion&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Stable_Diffusion_Keras.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Stable__Vision_Stability_AI.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Stable__Vision_Stability_AI_Animation.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Text_to_video_synthesis.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;TimeSformer.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/Stable_Diffusion_Keras.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/Stable__Vision_Stability_AI.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/Stable__Vision_Stability_AI_Animation.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/Text_to_video_synthesis.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/TimeSformer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter17/Stable_Diffusion_Keras.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter17/Stable__Vision_Stability_AI.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter17/Stable__Vision_Stability_AI_Animation.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter17/Text_to_video_synthesis.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter17/TimeSformer.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/Stable_Diffusion_Keras.ipynb?file=%2FChapter17%2FStable_Diffusion_Keras.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/Stable__Vision_Stability_AI.ipynb?file=%2FChapter17%2FStable__Vision_Stability_AI.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/Stable__Vision_Stability_AI_Animation.ipynb?file=%2FChapter17%2FStable__Vision_Stability_AI_Animation.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/Text_to_video_synthesis.ipynb?file=%2FChapter17%2FText_to_video_synthesis.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/TimeSformer.ipynb?file=%2FChapter17%2FTimeSformer.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/Stable_Diffusion_Keras.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/Stable__Vision_Stability_AI.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/Stable__Vision_Stability_AI_Animation.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/Text_to_video_synthesis.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter17/TimeSformer.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 18: Automated Vision Transformer Training&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Hugging_Face_AutoTrain.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter18/Hugging_Face_AutoTrain.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter18/Hugging_Face_AutoTrain.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter18/Hugging_Face_AutoTrain.ipynb?file=%2FChapter18%2FHugging_Face_AutoTrain.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter18/Hugging_Face_AutoTrain.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 19: On the Road to Functional AGI with HuggingGPT and its Peers&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Computer_Vision_Analysis.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter19/Computer_Vision_Analysis.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter19/Computer_Vision_Analysis.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter19/Computer_Vision_Analysis.ipynb?file=%2FChapter19%2FComputer_Vision_Analysis.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter19/Computer_Vision_Analysis.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Chapter 20: Generative AI Ideation Vertex AI, Langchain, and Stable Diffusion&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Automated_Design.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Midjourney_bot.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;Automated_Ideation.ipynb&lt;/li&gt;&#xA;     &lt;li&gt;üê¨ MyMidjourney_API.ipynb&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter20/Automated_Design.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter20/Midjourney_bot.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter20/Automated_Ideation.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter20/MyMidjourney_API.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter20/Automated_Design.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter20/Midjourney_bot.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter20/Automated_Ideation.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kaggle.com/kernels/welcome?src=https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/raw/main/Chapter20/MyMidjourney_API.ipynb&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Kaggle&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter20/Automated_Design.ipynb?file=%2FChapter20%2FAutomated_Design.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter20/Midjourney_bot.ipynb?file=%2FChapter20%2FMidjourney_bot.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter20/Automated_Ideation.ipynb?file=%2FChapter20%2FAutomated_Ideation.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://console.paperspace.com/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter20/MyMidjourney_API.ipynb?file=%2FChapter20%2FMyMidjourney_API.ipynb&#34;&gt;&lt;img src=&#34;https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true&#34; alt=&#34;Gradient&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter20/Automated_Design.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter20/Midjourney_bot.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter20/Automated_Ideation.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://studiolab.sagemaker.aws/import/github/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/blob/main/Chapter20/MyMidjourney_API.ipynb&#34;&gt;&lt;img src=&#34;https://studiolab.sagemaker.aws/studiolab.svg?sanitize=true&#34; alt=&#34;Open In SageMaker Studio Lab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Chat with this repository&#39;s Input-Augmented GPT-4 Chatbot&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chat.openai.com/g/g-qV6bon6X7-transformers-for-nlp-cv-3rd-edition-repo-helper&#34; target=&#34;_blank&#34;&gt; Chat with my custom GPT4 bot for this repository&lt;/a&gt;&lt;img alt=&#34;Support&#34; height=&#34;15&#34; width=&#34;35&#34; src=&#34;https://media.tenor.com/ex_HDD_k5P8AAAAi/habbo-habbohotel.gif&#34;&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can ask questions about this repository. You can also copy the code from the notebooks into my chat GPT and ask for explanations.&lt;/p&gt; &#xA; &lt;p&gt;This is a cutting-edge input-augmented Chatbot built on OpenAI for this GitHub repository. OpenAI requires a ChatGPT Plus subscription to explore it.&lt;/p&gt; &#xA; &lt;p&gt;&lt;em&gt;Limitations:&lt;/em&gt; This is an experimental chatbot. It is dedicated to this GitHub repository and does not replace the explanations provided in the book. But you can surely have some interesting educational interactions with my GPT-4 chatbot.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Raise an issue&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can &lt;a href=&#34;https://github.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/issues&#34;&gt;create an issue&lt;/a&gt; We will be glad to provide support!&lt;img alt=&#34;Support&#34; height=&#34;15&#34; width=&#34;35&#34; src=&#34;https://media.tenor.com/ex_HDD_k5P8AAAAi/habbo-habbohotel.gif&#34;&gt;in this repository if you encounter one in the notebooks.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Get my copy&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you feel this book is for you, get your &lt;a href=&#34;https://www.amazon.com/Transformers-Natural-Language-Processing-Computer-ebook/dp/B0CNH9V8M5/&#34;&gt;copy&lt;/a&gt; today! &lt;img alt=&#34;Coding&#34; height=&#34;15&#34; width=&#34;35&#34; src=&#34;https://media.tenor.com/ex_HDD_k5P8AAAAi/habbo-habbohotel.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Know more on the Discord server &lt;img alt=&#34;Coding&#34; height=&#34;25&#34; width=&#34;32&#34; src=&#34;https://cliply.co/wp-content/uploads/2021/08/372108630_DISCORD_LOGO_400.gif&#34;&gt;&lt;/h2&gt; &#xA;&lt;p&gt;You can get more engaged on the Discord server for more latest updates and discussions in the community at &lt;a href=&#34;https://www.packt.link/Transformers&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Download a free PDF &lt;img alt=&#34;Coding&#34; height=&#34;25&#34; width=&#34;40&#34; src=&#34;https://emergency.com.au/wp-content/uploads/2021/03/free.gif&#34;&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;If you have already purchased a print or Kindle version of this book, you can get a DRM-free PDF version at no cost. Simply click on the link to claim your&lt;/em&gt; &lt;a href=&#34;https://packt.link/free-ebook/9781805128724&#34;&gt;Free PDF&lt;/a&gt; &lt;img alt=&#34;Coding&#34; height=&#34;15&#34; width=&#34;35&#34; src=&#34;https://media.tenor.com/ex_HDD_k5P8AAAAi/habbo-habbohotel.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We also provide a PDF file that has color images of the screenshots/diagrams used in this book at &lt;a href=&#34;https://packt.link/gbp/9781805128724&#34;&gt;ColorImages&lt;/a&gt; &lt;img alt=&#34;Coding&#34; height=&#34;15&#34; width=&#34;35&#34; src=&#34;https://media.tenor.com/ex_HDD_k5P8AAAAi/habbo-habbohotel.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Get to Know the Author&lt;/h2&gt; &#xA;&lt;p&gt;Denis Rothman graduated from Sorbonne University and Paris Diderot University, designing one of the first patented encoding and embedding systems. He authored one of the first patented AI cognitive robots and bots. He began his career delivering Natural Language Processing (NLP) chatbots for Mo√´t et Chandon and as an AI tactical defense optimizer for Airbus (formerly Aerospatiale). Denis then authored an AI resource optimizer for IBM and luxury brands, leading to an Advanced Planning and Scheduling (APS) solution used worldwide. &lt;a href=&#34;https://www.linkedin.com/in/denis-rothman-0b034043/&#34;&gt;LinkedIn&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SalesforceAIResearch/uni2ts</title>
    <updated>2024-04-07T01:36:44Z</updated>
    <id>tag:github.com,2024-04-07:/SalesforceAIResearch/uni2ts</id>
    <link href="https://github.com/SalesforceAIResearch/uni2ts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unified Training of Universal Time Series Forecasting Transformers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Unified Training of Universal Time Series Forecasting Transformers&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.02592&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://blog.salesforceairesearch.com/moirai/&#34;&gt;Blog Post&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Uni2TS is a PyTorch based library for research and applications related to Time Series Transformers. This library aims to provide a unified solution to large-scale pre-training of Universal Time Series Transformers. Uni2TS also provides tools for fine-tuning, inference, and evaluation for time series forecasting.&lt;/p&gt; &#xA;&lt;h2&gt;üéâ What&#39;s New&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mar 2024: Release of Uni2TS library, along with &lt;a href=&#34;https://huggingface.co/collections/Salesforce/moirai-10-r-models-65c8d3a94c51428c300e0742&#34;&gt;Moirai-1.0-R&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/datasets/Salesforce/lotsa_data/&#34;&gt;LOTSA data&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚úÖ TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve docstrings and documentation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚öôÔ∏è Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/SalesforceAIResearch/uni2ts.git&#xA;cd uni2ts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create virtual environment:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;virtualenv venv&#xA;. venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Build from source:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -e &#39;.[notebook]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;.env&lt;/code&gt; file:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;touch .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üèÉ Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s see a simple example on how to use Uni2TS to make zero-shot forecasts from a pre-trained model. We first load our data using pandas, in the form of a wide DataFrame. Uni2TS relies on GluonTS for inference as it provides many convenience functions for time series forecasting, such as splitting a dataset into a train/test split and performing rolling evaluations, as demonstrated below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import pandas as pd&#xA;from gluonts.dataset.pandas import PandasDataset&#xA;from gluonts.dataset.split import split&#xA;from huggingface_hub import hf_hub_download&#xA;&#xA;from uni2ts.eval_util.plot import plot_single&#xA;from uni2ts.model.moirai import MoiraiForecast&#xA;&#xA;&#xA;SIZE = &#34;small&#34;  # model size: choose from {&#39;small&#39;, &#39;base&#39;, &#39;large&#39;}&#xA;PDT = 20  # prediction length: any positive integer&#xA;CTX = 200  # context length: any positive integer&#xA;PSZ = &#34;auto&#34;  # patch size: choose from {&#34;auto&#34;, 8, 16, 32, 64, 128}&#xA;BSZ = 32  # batch size: any positive integer&#xA;TEST = 100  # test set length: any positive integer&#xA;&#xA;# Read data into pandas DataFrame&#xA;url = (&#xA;    &#34;https://gist.githubusercontent.com/rsnirwan/c8c8654a98350fadd229b00167174ec4&#34;&#xA;    &#34;/raw/a42101c7786d4bc7695228a0f2c8cea41340e18f/ts_wide.csv&#34;&#xA;)&#xA;df = pd.read_csv(url, index_col=0, parse_dates=True)&#xA;&#xA;# Convert into GluonTS dataset&#xA;ds = PandasDataset(dict(df))&#xA;&#xA;# Split into train/test set&#xA;train, test_template = split(&#xA;    ds, offset=-TEST&#xA;)  # assign last TEST time steps as test set&#xA;&#xA;# Construct rolling window evaluation&#xA;test_data = test_template.generate_instances(&#xA;    prediction_length=PDT,  # number of time steps for each prediction&#xA;    windows=TEST // PDT,  # number of windows in rolling window evaluation&#xA;    distance=PDT,  # number of time steps between each window - distance=PDT for non-overlapping windows&#xA;)&#xA;&#xA;# Prepare pre-trained model by downloading model weights from huggingface hub&#xA;model = MoiraiForecast.load_from_checkpoint(&#xA;    checkpoint_path=hf_hub_download(&#xA;        repo_id=f&#34;Salesforce/moirai-1.0-R-{SIZE}&#34;, filename=&#34;model.ckpt&#34;&#xA;    ),&#xA;    prediction_length=PDT,&#xA;    context_length=CTX,&#xA;    patch_size=PSZ,&#xA;    num_samples=100,&#xA;    target_dim=1,&#xA;    feat_dynamic_real_dim=ds.num_feat_dynamic_real,&#xA;    past_feat_dynamic_real_dim=ds.num_past_feat_dynamic_real,&#xA;    map_location=&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;,&#xA;)&#xA;&#xA;predictor = model.create_predictor(batch_size=BSZ)&#xA;forecasts = predictor.predict(test_data.input)&#xA;&#xA;input_it = iter(test_data.input)&#xA;label_it = iter(test_data.label)&#xA;forecast_it = iter(forecasts)&#xA;&#xA;inp = next(input_it)&#xA;label = next(label_it)&#xA;forecast = next(forecast_it)&#xA;&#xA;plot_single(&#xA;    inp, &#xA;    label, &#xA;    forecast, &#xA;    context_length=200,&#xA;    name=&#34;pred&#34;,&#xA;    show_label=True,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìî Jupyter Notebook Examples&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/SalesforceAIResearch/uni2ts/main/example&#34;&gt;example folder&lt;/a&gt; for more examples on common tasks, e.g. visualizing forecasts, predicting from pandas DataFrame, etc.&lt;/p&gt; &#xA;&lt;h2&gt;üíª Command Line Interface&lt;/h2&gt; &#xA;&lt;p&gt;We provide several scripts which act as a &lt;a href=&#34;https://raw.githubusercontent.com/SalesforceAIResearch/uni2ts/main/cli&#34;&gt;command line interface&lt;/a&gt; to easily run fine-tuning, evaluation, and even pre-training jobs. &lt;a href=&#34;https://raw.githubusercontent.com/SalesforceAIResearch/uni2ts/main/cli/conf&#34;&gt;Configurations&lt;/a&gt; are managed with the &lt;a href=&#34;https://hydra.cc/&#34;&gt;Hydra&lt;/a&gt; framework.&lt;/p&gt; &#xA;&lt;h3&gt;Fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;Firstly, let&#39;s see how to use Uni2TS to fine-tune a pre-trained model on your custom dataset. Uni2TS uses the &lt;a href=&#34;https://github.com/huggingface/datasets&#34;&gt;Hugging Face datasets library&lt;/a&gt; to handle data loading, and we first need to convert your dataset into the Uni2TS format. If your dataset is a simple pandas DataFrame, we can easily process your dataset with the following script. We&#39;ll use the ETTh1 dataset from the popular &lt;a href=&#34;https://github.com/thuml/Time-Series-Library&#34;&gt;Long Sequence Forecasting benchmark&lt;/a&gt; for this example. For more complex use cases, see &lt;a href=&#34;https://raw.githubusercontent.com/SalesforceAIResearch/uni2ts/main/example/prepare_data.ipynb&#34;&gt;this notebook&lt;/a&gt; for more in-depth examples on how to use your custom dataset with Uni2TS.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;To begin the process, add the path to the directory where you want to save the processed dataset into the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;echo &#34;CUSTOM_DATA_PATH=PATH_TO_SAVE&#34; &amp;gt;&amp;gt; .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the following script to process the dataset into the required format. For the &lt;code&gt;dataset_type&lt;/code&gt; option, we support &lt;code&gt;wide&lt;/code&gt;, &lt;code&gt;long&lt;/code&gt; and &lt;code&gt;wide_multivariate&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m uni2ts.data.builder.simple ETTh1 dataset/ETT-small/ETTh1.csv --dataset_type wide&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, we may want validation set during fine-tuning to perform hyperparameter tuning or early stopping. To additionally split the dataset into a train and validation split we can use the mutually exclusive &lt;code&gt;date_offset&lt;/code&gt; (datetime string) or &lt;code&gt;offset&lt;/code&gt; (integer) options which determines the last time step of the train set. The validation set will be saved as DATASET_NAME_eval.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m uni2ts.data.builder.simple ETTh1 dataset/ETT-small/ETTh1.csv --date_offset &#39;2017-10-23 23:00:00&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Finally, we can simply run the fine-tuning script with the appropriate &lt;a href=&#34;https://raw.githubusercontent.com/SalesforceAIResearch/uni2ts/main/cli/conf/finetune/data/etth1.yaml&#34;&gt;training&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/SalesforceAIResearch/uni2ts/main/cli/conf/finetune/val_data/etth1.yaml&#34;&gt;validation&lt;/a&gt; data configuration files.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m cli.finetune \&#xA;  run_name=example_run \ &#xA;  model=moirai_1.0_R_small \ &#xA;  data=etth1 \ &#xA;  val_data=etth1  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;The evaluation script can be used to calculate evaluation metrics such as MSE, MASE, CRPS, and so on (see the &lt;a href=&#34;https://raw.githubusercontent.com/SalesforceAIResearch/uni2ts/main/cli/conf/eval/default.yaml&#34;&gt;configuration file&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Following up on the fine-tuning example, we can now perform evaluation on the test split by running the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m cli.eval \ &#xA;  run_name=example_eval_1 \&#xA;  model=moirai_1.0_R_small \&#xA;  model.patch_size=32 \ &#xA;  model.context_length=1000 \&#xA;  data=etth1_test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, we provide access to popular datasets, and can be toggled via the &lt;a href=&#34;https://raw.githubusercontent.com/SalesforceAIResearch/uni2ts/main/cli/conf/eval/data&#34;&gt;data configurations&lt;/a&gt;. As an example, say we want to perform evaluation, again on the ETTh1 dataset from the popular &lt;a href=&#34;https://github.com/thuml/Time-Series-Library&#34;&gt;Long Sequence Forecasting benchmark&lt;/a&gt;. We first need to download the pre-processed datasets and put them in the correct directory, by setting up the TSLib repository and following the instructions. Then, assign the dataset directory to the &lt;code&gt;LSF_PATH&lt;/code&gt; environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;echo &#34;LSF_PATH=PATH_TO_TSLIB/dataset&#34; &amp;gt;&amp;gt; .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Thereafter, simply run the following script with the predefined &lt;a href=&#34;https://raw.githubusercontent.com/SalesforceAIResearch/uni2ts/main/cli/conf/eval/data/lsf_test.yaml&#34;&gt;Hydra config file&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m cli.eval \ &#xA;  run_name=example_eval_2 \&#xA;  model=moirai_1.0_R_small \&#xA;  model.patch_size=32 \ &#xA;  model.context_length=1000 \ &#xA;  data=lsf_test \&#xA;  data.dataset_name=ETTh1 \&#xA;  data.prediction_length=96 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pre-training&lt;/h3&gt; &#xA;&lt;p&gt;Now, let&#39;s see how you can pre-train your own model. We&#39;ll start with preparing the data for pre-training first, by downloading the &lt;a href=&#34;https://huggingface.co/datasets/Salesforce/lotsa_data/&#34;&gt;Large-scale Open Time Series Archive (LOTSA data)&lt;/a&gt;. Assuming you&#39;ve already createed a &lt;code&gt;.env&lt;/code&gt; file, run the following commands.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;huggingface-cli download Salesforce/lotsa_data --repo-type=dataset --local-dir PATH_TO_SAVE&#xA;echo &#34;LOTSA_V1_PATH=PATH_TO_SAVE&#34; &amp;gt;&amp;gt; .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, we can simply run the following script to start a pre-training job. See the &lt;a href=&#34;https://raw.githubusercontent.com/SalesforceAIResearch/uni2ts/main/cli/pretrain.py&#34;&gt;relevant&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/SalesforceAIResearch/uni2ts/main/cli/conf/pretrain&#34;&gt;files&lt;/a&gt; on how to further customize the settings.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m cli.pretrain \&#xA;  run_name=first_run \&#xA;  model=moirai_small \&#xA;  data=lotsa_v1_unweighted&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üëÄ Citing Uni2TS&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re using Uni2TS in your research or applications, please cite it using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;@article{woo2024unified,&#xA;  title={Unified Training of Universal Time Series Forecasting Transformers},&#xA;  author={Woo, Gerald and Liu, Chenghao and Kumar, Akshat and Xiong, Caiming and Savarese, Silvio and Sahoo, Doyen},&#xA;  journal={arXiv preprint arXiv:2402.02592},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>