<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-05T01:35:31Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>guochengqian/Magic123</title>
    <updated>2023-08-05T01:35:31Z</updated>
    <id>tag:github.com,2023-08-05:/guochengqian/Magic123</id>
    <link href="https://github.com/guochengqian/Magic123" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official PyTorch Implementation of Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.17843&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://guochengqian.github.io/project/magic123/&#34;&gt;webpage&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/guochengqian/Magic123/main/docs/static/magic123.gif&#34; width=&#34;800&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://guochengqian.github.io/&#34;&gt;Guocheng Qian&lt;/a&gt; &lt;sup&gt;1,2&lt;/sup&gt;, &lt;a href=&#34;https://cemse.kaust.edu.sa/people/person/jinjie-mai&#34;&gt;Jinjie Mai&lt;/a&gt; &lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://abdullahamdi.com/&#34;&gt;Abdullah Hamdi&lt;/a&gt; &lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://alanspike.github.io/&#34;&gt;Jian Ren&lt;/a&gt; &lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/&#34;&gt;Aliaksandr Siarohin&lt;/a&gt; &lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://cemse.kaust.edu.sa/people/person/bing-li&#34;&gt;Bing Li&lt;/a&gt; &lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;http://hsinyinglee.com/&#34;&gt;Hsin-Ying Lee&lt;/a&gt; &lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://universome.github.io/&#34;&gt;Ivan Skorokhodov&lt;/a&gt; &lt;sup&gt;1,2&lt;/sup&gt;, &lt;a href=&#34;https://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt; &lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;http://www.stulyakov.com/&#34;&gt;Sergey Tulyakov&lt;/a&gt; &lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://www.bernardghanem.com/&#34;&gt;Bernard Ghanem&lt;/a&gt; &lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;a href=&#34;https://www.kaust.edu.sa/&#34;&gt;King Abdullah University of Science and Technology (KAUST)&lt;/a&gt;, &lt;sup&gt;2&lt;/sup&gt; &lt;a href=&#34;https://www.snap.com/&#34;&gt;Snap Inc.&lt;/a&gt;, &lt;sup&gt;3&lt;/sup&gt; &lt;a href=&#34;http://www.robots.ox.ac.uk/~vgg/&#34;&gt;Visual Geometry Group, University of Oxford&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Training convergence of a demo example: &lt;img src=&#34;https://raw.githubusercontent.com/guochengqian/Magic123/main/docs/static/ironman-val-magic123.gif&#34; width=&#34;800&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Compare Magic123 without textual inversion with abaltions using only 2D prior (SDS) or using only 3D prior (Zero123):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guochengqian/Magic123/assets/48788073/e5a3c3cb-bcb1-4b10-8bfb-2c2eb79a9289&#34;&gt;https://github.com/guochengqian/Magic123/assets/48788073/e5a3c3cb-bcb1-4b10-8bfb-2c2eb79a9289&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Effects of Joint Prior. Increasing the strength of 2D prior leads to more imagination, more details, and less 3D consistencies.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/guochengqian/Magic123/main/docs/static/2d_3d.png&#34; width=&#34;800&#34;&gt; &#xA;&lt;p&gt;Official PyTorch Implementation of Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors. Code is built upon &lt;a href=&#34;https://github.com/ashawkey/stable-dreamfusion&#34;&gt;Stable-DreamFusion&lt;/a&gt; repo.&lt;/p&gt; &#xA;&lt;h1&gt;NEWS:&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/07/25] Code is available at &lt;a href=&#34;https://github.com/guochengqian/Magic123&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023/07/03] Paper is available at &lt;a href=&#34;https://arxiv.org/abs/2306.17843&#34;&gt;arXiv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023/06/25] Much better performance than the submitted version is achieved by 1ï¼‰reimplementing Magic123 using &lt;a href=&#34;https://github.com/ashawkey/stable-dreamfusion&#34;&gt;Stable DreamFusion code&lt;/a&gt;, 2ï¼‰fixing some gradient issues, 3ï¼‰leveraging the &lt;a href=&#34;https://raw.githubusercontent.com/guochengqian/Magic123/main/#tips-and-tricks&#34;&gt;tricks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023] Initial version of Magic123 submitted to conference&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Install&lt;/h1&gt; &#xA;&lt;h3&gt;Install Environment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download pre-trained models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/cvlab-columbia/zero123&#34;&gt;Zero-1-to-3&lt;/a&gt; for 3D diffusion prior. We use &lt;code&gt;105000.ckpt&lt;/code&gt; by default, reimplementation borrowed from Stable Diffusion repo, and is available in &lt;code&gt;guidance/zero123_utils.py&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd pretrained/zero123&#xA;wget https://huggingface.co/cvlab/zero123-weights/resolve/main/105000.ckpt&#xA;cd .../../&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/isl-org/MiDaS&#34;&gt;MiDaS&lt;/a&gt; for depth estimation. We use &lt;code&gt;dpt_beit_large_512.pt&lt;/code&gt;. Put it in folder &lt;code&gt;pretrained/midas/&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p pretrained/midas&#xA;cd pretrained/midas&#xA;wget https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_large_512.pt&#xA;cd ../../&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;h2&gt;Preprocess [Optional]&lt;/h2&gt; &#xA;&lt;p&gt;We have included all preprocessed files in &lt;code&gt;./data&lt;/code&gt; directory. Preprocessing is only necessary if you want to test on your own examples.&lt;/p&gt; &#xA;&lt;h3&gt;Step1: Extract depth&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python preprocess_image.py --path /path/to/image &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2: Textural inversion [Optional]&lt;/h3&gt; &#xA;&lt;p&gt;Magic123 uses the defualt &lt;a href=&#34;https://huggingface.co/docs/diffusers/training/text_inversion&#34;&gt;textural inversion&lt;/a&gt; from diffuers, which consumes around 2.5 hours on a 32G V100. If you do not want to spend time in this textural inversion, you can: (1) study whether there is other faster textural inversion; or (2) do not use textural inversion in the loss of texture and shape consistencies. To run textural inversion:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/texural_inversion/textural_inversion.sh $GPU_IDX runwayml/stable-diffusion-v1-5 /path/to/example/rgba.png /path/to/save $token_name $init_token --max_train_steps 5000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;$token_name is a the special token, usually name that by &lt;em&gt;examplename&lt;/em&gt; $init_token is a single token to describe the image using natural language&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/texural_inversion/textural_inversion.sh runwayml/stable-diffusion-v1-5 data/demo/ironman/rgba.png out/textual_inversion/ironman _ironman_ ironman --max_train_steps 3000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Don&#39;t forget to move the final &lt;code&gt;learned_embeds.bin&lt;/code&gt; under data/demo/ironman/&lt;/p&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;h3&gt;Run Magic123 for a single example&lt;/h3&gt; &#xA;&lt;p&gt;Takes ~40 mins for the coarse stage and ~20 mins for the second stage on a 32G V100.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/magic123/run_both_priors.sh $GPU_NO $JOBNAME_First_Stage $JOBNAME_Second_Stage $PATH_to_Example_Directory $IMAGE_BASE_NAME $Enable_First_Stage $Enable_Second_Stage {More_Arugments}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As an example, run Magic123 in the dragon example using both stages in GPU 0 and set the jobname for the first stage as &lt;code&gt;default&lt;/code&gt; and the jobname for the second stage as &lt;code&gt;dmtet&lt;/code&gt;, by the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/magic123/run_both_priors.sh 0 default dmtet data/realfusion15/metal_dragon_statue rgba.png 1 1 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More arguments (e.g. &lt;code&gt;--lambda_guidance 1 40&lt;/code&gt;) can be appended to the command line such as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/magic123/run_both_priors.sh 0 default dmtet data/realfusion15/metal_dragon_statue rgba.png 1 1 --lambda_guidance 1 40&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Magic123 for a group of examples&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run all examples in a folder, check the scripts &lt;code&gt;scripts/magic123/run_folder_both_priors.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run all examples in a given list, check the scripts &lt;code&gt;scripts/magic123/run_list_both_priors.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run Magic123 on a single example without textural inversion&lt;/h3&gt; &#xA;&lt;p&gt;Textural inversion is tedious (requires ~2.5 hours optimization), if you want to test Magic123 quickly on your own example without texural inversion (might degrade the performance), try the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;first, foreground and depth estimation&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python preprocess_image.py --path data/demo/ironman/ironman.png&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run Magic123 coarse stage without textural inversion, takes ~40 mins&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export RUN_ID=&#39;default-a-full-body-ironman&#39;&#xA;export DATA_DIR=&#39;data/demo/ironman&#39;&#xA;export IMAGE_NAME=&#39;rgba.png&#39;&#xA;export FILENAME=$(basename $DATA_DIR)&#xA;export dataset=$(basename $(dirname $DATA_DIR))&#xA;CUDA_VISIBLE_DEVICES=0 python main.py -O \&#xA;--text &#34;A high-resolution DSLR image of a full body ironman&#34; \&#xA;--sd_version 1.5 \&#xA;--image ${DATA_DIR}/${IMAGE_NAME} \&#xA;--workspace out/magic123-${RUN_ID}-coarse/$dataset/magic123_${FILENAME}_${RUN_ID}_coarse \&#xA;--optim adam \&#xA;--iters 5000 \&#xA;--guidance SD zero123 \&#xA;--lambda_guidance 1.0 40 \&#xA;--guidance_scale 100 5 \&#xA;--latent_iter_ratio 0 \&#xA;--normal_iter_ratio 0.2 \&#xA;--t_range 0.2 0.6 \&#xA;--bg_radius -1 \&#xA;--save_mesh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run Magic123 fine stage without textural inversion, takes around ~20 mins&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export RUN_ID=&#39;default-a-full-body-ironman&#39;&#xA;export RUN_ID2=&#39;dmtet&#39;&#xA;export DATA_DIR=&#39;data/demo/ironman&#39;&#xA;export IMAGE_NAME=&#39;rgba.png&#39;&#xA;export FILENAME=$(basename $DATA_DIR)&#xA;export dataset=$(basename $(dirname $DATA_DIR))&#xA;CUDA_VISIBLE_DEVICES=0 python main.py -O \&#xA;--text &#34;A high-resolution DSLR image of a full body ironman&#34; \&#xA;--sd_version 1.5 \&#xA;--image ${DATA_DIR}/${IMAGE_NAME} \&#xA;--workspace out/magic123-${RUN_ID}-${RUN_ID2}/$dataset/magic123_${FILENAME}_${RUN_ID}_${RUN_ID2} \&#xA;--dmtet --init_ckpt out/magic123-${RUN_ID}-coarse/$dataset/magic123_${FILENAME}_${RUN_ID}_coarse/checkpoints/magic123_${FILENAME}_${RUN_ID}_coarse.pth \&#xA;--iters 5000 \&#xA;--optim adam \&#xA;--known_view_interval 4 \&#xA;--latent_iter_ratio 0 \&#xA;--guidance SD zero123 \&#xA;--lambda_guidance 1e-3 0.01 \&#xA;--guidance_scale 100 5 \&#xA;--rm_edge \&#xA;--bg_radius -1 \&#xA;--save_mesh &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run ablation studies&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Run Magic123 with only 2D prior &lt;em&gt;with&lt;/em&gt; textural inversion (Like RealFusion but we achieve much better performance through training stragies and the coarse-to-fine pipeline)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;bash scripts/magic123/run_2dprior.sh 0 default dmtet data/realfusion15/metal_dragon_statue rgba.png 1 1&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run Magic123 with only 2D prior &lt;em&gt;without&lt;/em&gt; textural inversion (Like RealFusion but we achieve much better performance through training stragies and the coarse-to-fine pipeline)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;bash scripts/magic123/run_2dprior_notextinv_ironman.sh 0 default 1 1&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;note: change the path and the text prompt inside the script if you wana test another example.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run Magic123 with only 3D prior (Like Zero-1-to-3 but we achieve much better performance through training stragies and the coarse-to-fine pipeline)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;bash scripts/magic123/run_3dprior.sh 0 default dmtet data/demo/ironman rgba.png 1 1&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Tips and Tricks&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fix camera distance (&lt;em&gt;radius_range&lt;/em&gt;) and FOV (&lt;em&gt;fovy_range&lt;/em&gt;) and tune the camera polar range (&lt;em&gt;theta_range&lt;/em&gt;). Note it is better to keep camera jittering to reduce grid artifacts.&lt;/li&gt; &#xA; &lt;li&gt;Smaller range of time steps for the defusion noise (t_range). We find &lt;em&gt;[0.2, 0.6]&lt;/em&gt; gives better performance for image-to-3D tasks.&lt;/li&gt; &#xA; &lt;li&gt;Using normals as latent in the first 2000 improves generated geometry a bit gernerally (but not always). We turn on this for Magic123 corase stage in the script &lt;code&gt;--normal_iter_ratio 0.2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;We erode segmentation edges (makes the segmentation map 2 pixels shrinked towards internal side) to remove artifacts due to segmentation erros. This is turned on in the fine stage in magic123 in the script through &lt;code&gt;--rm_edge&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Other general tricks such as improved texural inversion, advanced diffusion prior (DeepFloyd, SD-XL), stronger 3D prior (Zero123-XL), and larger batch size can be adopted as well but not studied in this work.&lt;/li&gt; &#xA; &lt;li&gt;textural inversion is not very necessary for well-known things (e.g. ironman) and easily described textures and geoemtries, since pure texts contains these texture information and will be understood by diffusion models. We use textural inversion by default in all experiments.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;p&gt;This work is build upon Stable DreamFusion, many thanks to the author &lt;a href=&#34;https://github.com/ashawkey&#34;&gt;Kiui Jiaxiang Tang&lt;/a&gt; and many other contributors.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ashawkey/stable-dreamfusion&#34;&gt;Stable DreamFusion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{stable-dreamfusion,&#xA;    Author = {Jiaxiang Tang},&#xA;    Year = {2022},&#xA;    Note = {https://github.com/ashawkey/stable-dreamfusion},&#xA;    Title = {Stable-dreamfusion: Text-to-3D with Stable-diffusion}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also get inspirations from a list of amazing research works and open-source projects, thanks a lot to all the authors for sharing!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;DreamFusion: Text-to-3D using 2D Diffusion&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@article{poole2022dreamfusion,&#xA;    author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},&#xA;    title = {DreamFusion: Text-to-3D using 2D Diffusion},&#xA;    journal = {arXiv},&#xA;    year = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://research.nvidia.com/labs/dir/magic3d/&#34;&gt;Magic3D: High-Resolution Text-to-3D Content Creation&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@inproceedings{lin2023magic3d,&#xA;   title={Magic3D: High-Resolution Text-to-3D Content Creation},&#xA;   author={Lin, Chen-Hsuan and Gao, Jun and Tang, Luming and Takikawa, Towaki and Zeng, Xiaohui and Huang, Xun and Kreis, Karsten and Fidler, Sanja and Liu, Ming-Yu and Lin, Tsung-Yi},&#xA;   booktitle={IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})},&#xA;   year={2023}&#xA; }&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/cvlab-columbia/zero123&#34;&gt;Zero-1-to-3: Zero-shot One Image to 3D Object&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@misc{liu2023zero1to3,&#xA;    title={Zero-1-to-3: Zero-shot One Image to 3D Object},&#xA;    author={Ruoshi Liu and Rundi Wu and Basile Van Hoorick and Pavel Tokmakov and Sergey Zakharov and Carl Vondrick},&#xA;    year={2023},&#xA;    eprint={2303.11328},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/lukemelas/realfusion&#34;&gt;RealFusion: 360Â° Reconstruction of Any Object from a Single Image&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@inproceedings{melaskyriazi2023realfusion,&#xA;    author = {Melas-Kyriazi, Luke and Rupprecht, Christian and Laina, Iro and Vedaldi, Andrea},&#xA;    title = {RealFusion: 360 Reconstruction of Any Object from a Single Image},&#xA;    booktitle={CVPR}&#xA;    year = {2023},&#xA;    url = {https://arxiv.org/abs/2302.10663},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.14184&#34;&gt;Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@article{tang2023make-it-3d,&#xA;    title={Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior},&#xA;    author={Tang, Junshu and Wang, Tengfei and Zhang, Bo and Zhang, Ting and Yi, Ran and Ma, Lizhuang and Chen, Dong},&#xA;    journal={arXiv preprint arXiv:2303.14184},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; and the &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; library.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;    title={High-Resolution Image Synthesis with Latent Diffusion Models},&#xA;    author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and BjÃ¶rn Ommer},&#xA;    year={2021},&#xA;    eprint={2112.10752},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CV}&#xA;}&#xA;&#xA;@misc{von-platen-etal-2022-diffusers,&#xA;    author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},&#xA;    title = {Diffusers: State-of-the-art diffusion models},&#xA;    year = {2022},&#xA;    publisher = {GitHub},&#xA;    journal = {GitHub repository},&#xA;    howpublished = {\url{https://github.com/huggingface/diffusers}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Cite&lt;/h1&gt; &#xA;&lt;p&gt;If you find this work useful, a citation will be appreciated via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{qian2023magic123,&#xA;  title={Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors},&#xA;  author={Qian, Guocheng and Mai, Jinjie and Hamdi, Abdullah and Ren, Jian and Siarohin, Aliaksandr and Li, Bing and Lee, Hsin-Ying and Skorokhodov, Ivan and Wonka, Peter and Tulyakov, Sergey and others},&#xA;  journal={arXiv preprint arXiv:2306.17843},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>emptycrown/llama-hub</title>
    <updated>2023-08-05T01:35:31Z</updated>
    <id>tag:github.com,2023-08-05:/emptycrown/llama-hub</id>
    <link href="https://github.com/emptycrown/llama-hub" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A library of data loaders for LLMs made by the community -- to be used with GPT Index and/or LangChain&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LlamaHub ðŸ¦™&lt;/h1&gt; &#xA;&lt;p&gt;This is a simple library of all the data loaders / readers / tools that have been created by the community. The goal is to make it extremely easy to connect large language models to a large variety of knowledge sources. These are general-purpose utilities that are meant to be used in &lt;a href=&#34;https://github.com/jerryjliu/llama_index&#34;&gt;LlamaIndex&lt;/a&gt; and &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Loaders and readers allow you to easily ingest data for search and retrieval by a large language models, while tools allow the models to both read and write to third party data services and sources. Ultimately, this allows you to create your own customized data agent to intelligently work with you and your data to unlock the full capaibility of next level large language models.&lt;/p&gt; &#xA;&lt;p&gt;For a variety of examples on data agents, see the &lt;a href=&#34;https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks&#34;&gt;notebooks directory&lt;/a&gt;. You can find example Jupyter notebooks for creating data agents that can load and parse data from Google Docs, SQL Databases, Notion, Slack and also manage you Google Calendar, Gmail inbox, or read and use OpenAPI specs.&lt;/p&gt; &#xA;&lt;p&gt;For an easier way to browse the integrations available, checkout the website here: &lt;a href=&#34;https://llamahub.ai/&#34;&gt;https://llamahub.ai/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img width=&#34;1465&#34; alt=&#34;Screenshot 2023-07-17 at 6 12 32 PM&#34; src=&#34;https://github.com/ajhofmann/llama-hub/assets/10040285/5e344de4-4aca-4f6c-9944-46c00baa5eb2&#34;&gt; &#xA;&lt;h2&gt;Usage (Use &lt;code&gt;llama-hub&lt;/code&gt; as PyPI package)&lt;/h2&gt; &#xA;&lt;p&gt;These general-purpose loaders are designed to be used as a way to load data into &lt;a href=&#34;https://github.com/jerryjliu/llama_index&#34;&gt;LlamaIndex&lt;/a&gt; and/or subsequently used in &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install llama-hub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LlamaIndex&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index import GPTVectorStoreIndex&#xA;from llama_hub.google_docs.base import GoogleDocsReader&#xA;&#xA;gdoc_ids = [&#39;1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec&#39;]&#xA;loader = GoogleDocsReader()&#xA;documents = loader.load_data(document_ids=gdoc_ids)&#xA;index = GPTVectorStoreIndex.from_documents(documents)&#xA;index.query(&#39;Where did the author go to school?&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LlamaIndex Data Agent&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index.agent import OpenAIAgent&#xA;import openai&#xA;openai.api_key = &#39;sk-api-key&#39;&#xA;&#xA;from llama_hub.tools.google_calendar.base import GoogleCalendarToolSpec&#xA;tool_spec = GoogleCalendarToolSpec()&#xA;&#xA;agent = OpenAIAgent.from_tools(tool_spec.to_tool_list())&#xA;agent.chat(&#39;what is the first thing on my calendar today&#39;)&#xA;agent.chat(&#34;Please create an event for tomorrow at 4pm to review pull requests&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a variety of examples on creating and using data agents, see the &lt;a href=&#34;https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks&#34;&gt;notebooks directory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;LangChain&lt;/h3&gt; &#xA;&lt;p&gt;Note: Make sure you change the description of the &lt;code&gt;Tool&lt;/code&gt; to match your use-case.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index import GPTVectorStoreIndex&#xA;from llama_hub.google_docs.base import GoogleDocsReader&#xA;from langchain.llms import OpenAI&#xA;from langchain.chains.question_answering import load_qa_chain&#xA;&#xA;# load documents&#xA;gdoc_ids = [&#39;1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec&#39;]&#xA;loader = GoogleDocsReader()&#xA;documents = loader.load_data(document_ids=gdoc_ids)&#xA;langchain_documents = [d.to_langchain_format() for d in documents]&#xA;&#xA;# initialize sample QA chain&#xA;llm = OpenAI(temperature=0)&#xA;qa_chain = load_qa_chain(llm)&#xA;question=&#34;&amp;lt;query here&amp;gt;&#34;&#xA;answer = qa_chain.run(input_documents=langchain_documents, question=question)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Loader Usage (Use &lt;code&gt;download_loader&lt;/code&gt; from LlamaIndex)&lt;/h2&gt; &#xA;&lt;p&gt;You can also use the loaders with &lt;code&gt;download_loader&lt;/code&gt; from LlamaIndex in a single line of code.&lt;/p&gt; &#xA;&lt;p&gt;For example, see the code snippets below using the Google Docs Loader.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama_index import GPTVectorStoreIndex, download_loader&#xA;&#xA;GoogleDocsReader = download_loader(&#39;GoogleDocsReader&#39;)&#xA;&#xA;gdoc_ids = [&#39;1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec&#39;]&#xA;loader = GoogleDocsReader()&#xA;documents = loader.load_data(document_ids=gdoc_ids)&#xA;index = GPTVectorStoreIndex.from_documents(documents)&#xA;index.query(&#39;Where did the author go to school?&#39;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to add a loader or tool&lt;/h2&gt; &#xA;&lt;p&gt;Adding a loader or tool simply requires forking this repo and making a Pull Request. The Llama Hub website will update automatically. However, please keep in the mind the following guidelines when making your PR.&lt;/p&gt; &#xA;&lt;h3&gt;Step 0: Setup virtual environment, install Poetry and dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Create a new Python virtual environment. The command below creates an environment in &lt;code&gt;.venv&lt;/code&gt;, and activates it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv .venv&#xA;source .venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if you are in windows, use the following to activate your virtual environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.venv\scripts\activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install poetry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install poetry&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install the required dependencies (this will also install &lt;code&gt;llama_index&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create an editable install of &lt;code&gt;llama-hub&lt;/code&gt; in your venv.&lt;/p&gt; &#xA;&lt;h3&gt;Step 1: Create a new directory&lt;/h3&gt; &#xA;&lt;p&gt;For loaders, create a new directory in &lt;code&gt;llama_hub&lt;/code&gt;, and for tools create a directory in &lt;code&gt;llama_hub/tools&lt;/code&gt; It can be nested within another, but name it something unique because the name of the directory will become the identifier for your loader (e.g. &lt;code&gt;google_docs&lt;/code&gt;). Inside your new directory, create a &lt;code&gt;__init__.py&lt;/code&gt; file, which can be empty, a &lt;code&gt;base.py&lt;/code&gt; file which will contain your loader implementation, and, if needed, a &lt;code&gt;requirements.txt&lt;/code&gt; file to list the package dependencies of your loader. Those packages will automatically be installed when your loader is used, so no need to worry about that anymore!&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like, you can create the new directory and files by running the following script in the &lt;code&gt;llama_hub&lt;/code&gt; directory. Just remember to put your dependencies into a &lt;code&gt;requirements.txt&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./add_loader.sh [NAME_OF_NEW_DIRECTORY]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2: Write your README&lt;/h3&gt; &#xA;&lt;p&gt;Inside your new directory, create a &lt;code&gt;README.md&lt;/code&gt; that mirrors that of the existing ones. It should have a summary of what your loader or tool does, its inputs, and how its used in the context of LlamaIndex and LangChain.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3: Add your loader to the library.json file&lt;/h3&gt; &#xA;&lt;p&gt;Finally, add your loader to the &lt;code&gt;llama_hub/library.json&lt;/code&gt; file (for tools, add them to the &lt;code&gt;llama_hub/tools/library.json&lt;/code&gt;) so that it may be used by others. As is exemplified by the current file, add in the class name of your loader or tool, along with its id, author, etc. This file is referenced by the Llama Hub website and the download function within LlamaIndex.&lt;/p&gt; &#xA;&lt;h3&gt;Step 4: Make a Pull Request!&lt;/h3&gt; &#xA;&lt;p&gt;Create a PR against the main branch. We typically review the PR within a day. To help expedite the process, it may be helpful to provide screenshots (either in the PR or in the README directly) showing your data loader or tool in action!&lt;/p&gt; &#xA;&lt;h2&gt;Running tests&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3.9 -m venv .venv&#xA;source .venv/bin/activate &#xA;pip3 install -r test_requirements.txt&#xA;&#xA;poetry run pytest tests &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;If you want to track the latest version updates / see which loaders are added to each release, take a look at our &lt;a href=&#34;https://github.com/emptycrown/llama-hub/raw/main/CHANGELOG.md&#34;&gt;full changelog here&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;How do I test my loader before it&#39;s merged?&lt;/h3&gt; &#xA;&lt;p&gt;There is an argument called &lt;code&gt;loader_hub_url&lt;/code&gt; in &lt;a href=&#34;https://github.com/jerryjliu/llama_index/raw/main/llama_index/readers/download.py&#34;&gt;&lt;code&gt;download_loader&lt;/code&gt;&lt;/a&gt; that defaults to the main branch of this repo. You can set it to your branch or fork to test your new loader.&lt;/p&gt; &#xA;&lt;h3&gt;Should I create a PR against LlamaHub or the LlamaIndex repo directly?&lt;/h3&gt; &#xA;&lt;p&gt;If you have a data loader PR, by default let&#39;s try to create it against LlamaHub! We will make exceptions in certain cases (for instance, if we think the data loader should be core to the LlamaIndex repo).&lt;/p&gt; &#xA;&lt;p&gt;For all other PR&#39;s relevant to LlamaIndex, let&#39;s create it directly against the &lt;a href=&#34;https://github.com/jerryjliu/llama_index&#34;&gt;LlamaIndex repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Other questions?&lt;/h3&gt; &#xA;&lt;p&gt;Feel free to hop into the &lt;a href=&#34;https://discord.gg/dGcwcsnxhU&#34;&gt;community Discord&lt;/a&gt; or tag the official &lt;a href=&#34;https://twitter.com/llama_index&#34;&gt;Twitter account&lt;/a&gt;!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>snakajima/SlashGPT</title>
    <updated>2023-08-05T01:35:31Z</updated>
    <id>tag:github.com,2023-08-05:/snakajima/SlashGPT</id>
    <link href="https://github.com/snakajima/SlashGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SlashGPT&lt;/h1&gt; &#xA;&lt;p&gt;SlashGPT is a playground for develeopers to make quick prototypes of LLM agents (or apps with Natural Language UI).&lt;/p&gt; &#xA;&lt;p&gt;Here are the design goals:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Extremely easy to create a new LLM agent. You just need to add a new manifest file (in Json).&lt;/li&gt; &#xA; &lt;li&gt;Instantly switch amang agents, by just typing &#34;/{agent_name}&#34;&lt;/li&gt; &#xA; &lt;li&gt;Extensible enough so that it is possible to implement most of LLM agents without writing any code.&lt;/li&gt; &#xA; &lt;li&gt;It is possible to integrate ChatGPT plugins as agents without writing any code.&lt;/li&gt; &#xA; &lt;li&gt;It enables broker agent (or dispatcher), which routes user&#39;s messgae to an appropraite agent.&lt;/li&gt; &#xA; &lt;li&gt;It is able to run generated Python code like Code Interpreter (see &#34;jupyter&#34; agent).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Initialization&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the required packages:&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create .env file, and specify your OpenAI key as follows:&lt;/p&gt; &lt;p&gt;&lt;code&gt;OPENAI_API_KEY=...&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You need to specify other variables to use following features (optional).&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;PINECONE_API_KEY, PINECONE_ENVIRONMENT: requred to use embedding vector db.&lt;/li&gt; &#xA;   &lt;li&gt;GOOGLE_PALM_KEY: required to switch to PaLM LLC using /palm command.&lt;/li&gt; &#xA;   &lt;li&gt;WOLFRAM_API_KEY: required to use &#34;walfram&#34; agent.&lt;/li&gt; &#xA;   &lt;li&gt;OPENWEATHER_API_KEY: required to use &#34;weather&#34; agent.&lt;/li&gt; &#xA;   &lt;li&gt;NOTEABLE_API_KEY: required to use &#34;noteable&#34; agent.&lt;/li&gt; &#xA;   &lt;li&gt;ALCHEMY_API_KEY: required to use &#34;web3&#34; agent.&lt;/li&gt; &#xA;   &lt;li&gt;WEBPILOT_UID: required to use &#34;webpilot&#34; agent (any unique UUID is fine)&lt;/li&gt; &#xA;   &lt;li&gt;REPLICATE_API_TOKEN: required to use &#34;llama2&#34; model.&lt;/li&gt; &#xA;   &lt;li&gt;CODEBOX_API_KEY: set this to &#34;local&#34; to use CodeBox&#39;s LocalBox instead of IPython&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Execution&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Type &lt;code&gt;./SlashGPT.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When you see &#34;You({agent_name}):&#34;, type a message to the agent OR type a slash command starting with &#34;/&#34;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;It activate &#34;dispatcher&#34; agent first, which is able to dispatch queries to appropriate agents.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Type &#34;/help&#34; to see the list of system commands and available agents.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Outputs&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Each conversation will be store as a json file under the &#34;output/{context}&#34; folder, where the context is &#34;GTP&#34; for general chat, and the app id for a specialized chat.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Please notice that the &#34;output&#34; folder is ignored by git.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Code Interpreter agents will generate Jupyter notebook in &#34;output/notebooks&#34; folder.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Code Interpreter Agents&lt;/h2&gt; &#xA;&lt;p&gt;Some of agents are built to mimic the behaviors of ChatGPT code intepreter (or Noteable plugin) with various LLMs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;jupyter: GPT3.5&lt;/li&gt; &#xA; &lt;li&gt;jupyterp: PaLM2 (GOOGLE_PALM_KEY key is required)&lt;/li&gt; &#xA; &lt;li&gt;juypter2: LlaMA (REPLICATE_API_TOKEN is required)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;jupyter (GPT3.5) works just like Code Interpreter. It is able to respond to the output of nenerated code appropriately.&lt;/p&gt; &#xA;&lt;p&gt;jupyterp (PaLM2) and jupyter2 (LlaMA) are not able to respond to the output of generated code (they often enter into an infinit loop). Therefore, we stop the conversation after the output, and the user needs to explicitly ask it to analize the result.&lt;/p&gt; &#xA;&lt;p&gt;For the runtime, it uses IPython by default, but it uses CodeBox if you specify CODEBOX_API_KEY key. IPython displays images as popups, but does not write them into the notebook. CodeBox is able to write them into the notebook.&lt;/p&gt; &#xA;&lt;p&gt;Sample queries.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Draw sine curve&lt;/li&gt; &#xA; &lt;li&gt;List first 50 prime numbers&lt;/li&gt; &#xA; &lt;li&gt;graph common moving average&lt;/li&gt; &#xA; &lt;li&gt;Draw histogram&lt;/li&gt; &#xA; &lt;li&gt;Graph 4 year stock price of apple and tesla using yfinance&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Manifest files&lt;/h2&gt; &#xA;&lt;p&gt;Create a new manifest file, {agent_name}.json in &#34;manifests&#34; folder with following properties:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;title&lt;/em&gt; (string, &lt;strong&gt;required&lt;/strong&gt;): Title for the user to see&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;source&lt;/em&gt; (string, optional): Source of the prompt (URL, email, github id, or twitter id)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;promt&lt;/em&gt; (array of strings, &lt;strong&gt;required&lt;/strong&gt;): The system prompts which define the agent (required)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;form&lt;/em&gt; (string): format string to extend user&#39;s query (e.g. &#34;Write python code to {question}&#34;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;result_form&lt;/em&gt; (string): format string to extend function call result.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;skip_function_result&lt;/em&gt; (boolean): skip the chat completion right after the function call.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;notepad&lt;/em&gt; (boolean): create a new notebook at the beginning of each session (for jupyter2)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;bot&lt;/em&gt; (string, optional): Agent name&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;you&lt;/em&gt; (string, optional): User name. The default is You({agent_name}).&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;sample&lt;/em&gt; (string, optional): Sample question (type &#34;/sample&#34; to send it)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;intro&lt;/em&gt; (array of strings, optional): Introduction statements (will be randomly selected)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;model&lt;/em&gt; (string, optional): LLM model (such as &#34;gpt-4-613&#34;, the default is &#34;gpt-3-turbo&#34;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;temperature&lt;/em&gt; (string, optional): Temperature (the default is 0.7)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;data&lt;/em&gt; (array of string, optional): {random} will put one of them randamly into the prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;embeddings&lt;/em&gt; (object, optional): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;name&lt;/em&gt; (string, optional): index name of the embedding vector database&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;resource&lt;/em&gt; (string, optional): location of the resource file. Use {resource} to paste it into the prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;functions&lt;/em&gt; (string, optional): location of the function definitions&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;module&lt;/em&gt; (string, optional): location of the pytoh script to be loaded for function calls&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;actions&lt;/em&gt; (object, optional): Template-based function processor (see details below)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Name of that file becomes the slash command. (the slash command of &#34;foo.json&#34; is &#34;/foo&#34;)&lt;/p&gt; &#xA;&lt;h2&gt;Actions&lt;/h2&gt; &#xA;&lt;p&gt;It defines template-based function implementations (including mockups), alternative to writing Python code using the &#34;module&#34; property.&lt;/p&gt; &#xA;&lt;p&gt;It supports three different methods.&lt;/p&gt; &#xA;&lt;h3&gt;1. Formatted string.&lt;/h3&gt; &#xA;&lt;p&gt;Use this method to develop the front-end of a system before the backend become ready.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;message&lt;/em&gt; (format string, required): chat messgae to be added&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;metafile&lt;/em&gt; (format string, optional): metafile name to be loaded for chained action&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an example (home2).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  &#34;actions&#34;: {&#xA;    &#34;fill_bath&#34;: { &#34;message&#34;:&#34;Success. I started filling the bath tab.&#34; },&#xA;    &#34;set_temperature&#34;: { &#34;message&#34;:&#34;Success. I set the teperature to {temperature} for {location}&#34; },&#xA;    &#34;start_sprinkler&#34;: { &#34;message&#34;:&#34;Success. I started the sprinkler for {location}&#34; },&#xA;    &#34;take_picture&#34;: { &#34;message&#34;:&#34;Success. I took a picture of {location}&#34; },&#xA;    &#34;play_music&#34;: { &#34;message&#34;:&#34;Success. I started playing {music} in {location}&#34; },&#xA;    &#34;control_light&#34;: { &#34;message&#34;:&#34;Success. The light switch of {location} is now {switch}.&#34; }&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. REST calls&lt;/h3&gt; &#xA;&lt;p&gt;Use this method to call REST APIs (equivalent to ChatGPT&#39;s plugin system).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;url&lt;/em&gt; (string, required): Python-style format string, which references to function arguments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;method&lt;/em&gt; (string, optional): Specify &#34;POST&#34; if we need to use HTTP-POST. The body will contain a JSON representation of function parameters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an example (currency).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  &#34;actions&#34;: {&#xA;    &#34;convert&#34;: {&#xA;      &#34;url&#34;: &#34;https://today-currency-converter.oiconma.repl.co/currency-converter?from={from}&amp;amp;to={to}&amp;amp;amount={amount}&#34;&#xA;    }&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. data URL&lt;/h3&gt; &#xA;&lt;p&gt;This mechod allows a developer to generate a text data (typically in JSON, but not limited to), and turn it into a data URL.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;template&lt;/em&gt; (string, required): The location of the template file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;mime_type&lt;/em&gt; (string, required): The mime type of the data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;message&lt;/em&gt; (string, required): Python-style format string, which references to the data-URL as {url}.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an example for &#34;make_event&#34; function (cal).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  &#34;actions&#34;: {&#xA;    &#34;make_event&#34;: {&#xA;      &#34;template&#34;: &#34;./resources/calendar.ics&#34;,&#xA;      &#34;mime_type&#34;: &#34;text/calendar&#34;,&#xA;      &#34;message&#34;: &#34;The event was scheduled. Here is the invitation link: &#39;{url}&#39;&#34;&#xA;    }&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The contents of calendar.ics file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;BEGIN:VCALENDAR&#xA;VERSION:2.0&#xA;PRODID:-//My Calendar//NONSGML v1.0//EN&#xA;BEGIN:VEVENT&#xA;DTSTART:{DTSTART}&#xA;DTEND:{DTEND}&#xA;SUMMARY:{SUMMARY}&#xA;DESCRIPTION:{DESCRIPTION}&#xA;LOCATION:{LOCATION}&#xA;END:VEVENT&#xA;END:VCALENDAR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The definition of &#34;make_event&#34; function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  &#34;name&#34;: &#34;make_event&#34;,&#xA;  &#34;description&#34;: &#34;Create a calendar event in iCalendar format&#34;,&#xA;  &#34;parameters&#34;: {&#xA;    &#34;type&#34;: &#34;object&#34;,&#xA;    &#34;properties&#34;: {&#xA;      &#34;SUMMARY&#34;: {&#xA;        &#34;type&#34;: &#34;string&#34;,&#xA;        &#34;description&#34;: &#34;a short, one-line description of the event&#34;&#xA;      },&#xA;      &#34;DESCRIPTION&#34;: {&#xA;        &#34;type&#34;: &#34;string&#34;,&#xA;        &#34;description&#34;: &#34;a more complete description of the calendar&#34;,&#xA;        &#34;maxLength&#34;: 400&#xA;      },&#xA;      &#34;DTSTART&#34;: {&#xA;        &#34;type&#34;: &#34;string&#34;,&#xA;        &#34;format&#34;: &#34;date-time&#34;,&#xA;        &#34;description&#34;: &#34;the date and time in UTC that the event begins such as 19980119T020000Z&#34;&#xA;      },&#xA;      &#34;DTEND&#34;: {&#xA;        &#34;type&#34;: &#34;string&#34;,&#xA;        &#34;format&#34;: &#34;date-time&#34;,&#xA;        &#34;description&#34;: &#34;the date and time in UTC that the event ends such as 19980119T030000Z&#34;&#xA;      },&#xA;      &#34;LOCATION&#34;: {&#xA;        &#34;type&#34;: &#34;string&#34;,&#xA;        &#34;description&#34;: &#34;the intended venue with address for the event.&#34;&#xA;      }&#xA;    },&#xA;    &#34;required&#34;: [&#34;SUMMARY&#34;, &#34;DTSTART&#34;, &#34;DTEND&#34;, &#34;DESCRIPTION&#34;, &#34;LOCATION&#34;]&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>