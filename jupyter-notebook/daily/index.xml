<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-21T01:31:03Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>wanmeihuali/taichi_3d_gaussian_splatting</title>
    <updated>2023-09-21T01:31:03Z</updated>
    <id>tag:github.com,2023-09-21:/wanmeihuali/taichi_3d_gaussian_splatting</id>
    <link href="https://github.com/wanmeihuali/taichi_3d_gaussian_splatting" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An unofficial implementation of paper 3D Gaussian Splatting for Real-Time Radiance Field Rendering by taichi lang.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;taichi_3d_gaussian_splatting&lt;/h1&gt; &#xA;&lt;p&gt;An unofficial implementation of paper &lt;a href=&#34;https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/&#34;&gt;3D Gaussian Splatting for Real-Time Radiance Field Rendering&lt;/a&gt; by taichi lang.&lt;/p&gt; &#xA;&lt;h2&gt;What does 3D Gaussian Splatting do?&lt;/h2&gt; &#xA;&lt;h3&gt;Training:&lt;/h3&gt; &#xA;&lt;p&gt;The algorithm takes image from multiple views, a sparse point cloud, and camera pose as input, use a differentiable rasterizer to train the point cloud, and output a dense point cloud with extra features(covariance, color information, etc.).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/images/image_from_multi_views.png&#34; alt=&#34;drawing&#34; width=&#34;200&#34;&gt;&lt;br&gt; If we view the training process as module, it can be described as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph LR&#xA;    A[ImageFromMultiViews] --&amp;gt; B((Training))&#xA;    C[sparsePointCloud] --&amp;gt; B&#xA;    D[CameraPose] --&amp;gt; B&#xA;    B --&amp;gt; E[DensePointCloudWithExtraFeatures]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference:&lt;/h3&gt; &#xA;&lt;p&gt;The algorithm takes the dense point cloud with extra features and any camera pose as input, use the same rasterizer to render the image from the camera pose.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph LR&#xA;    C[DensePointCloudWithExtraFeatures] --&amp;gt; B((Inference))&#xA;    D[NewCameraPose] --&amp;gt; B&#xA;    B --&amp;gt; E[Image]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of inference result:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wanmeihuali/taichi_3d_gaussian_splatting/assets/18469933/cc760693-636b-4157-ae85-33813f3da54d&#34;&gt;https://github.com/wanmeihuali/taichi_3d_gaussian_splatting/assets/18469933/cc760693-636b-4157-ae85-33813f3da54d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Because the nice property of point cloud, the algorithm easily handles scene/object merging compared to other NeRF-like algorithms.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wanmeihuali/taichi_3d_gaussian_splatting/assets/18469933/bc38a103-e435-4d35-9239-940e605b4552&#34;&gt;https://github.com/wanmeihuali/taichi_3d_gaussian_splatting/assets/18469933/bc38a103-e435-4d35-9239-940e605b4552&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;other example result&lt;/summary&gt; &#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;p&gt;top left: &lt;a href=&#34;https://github.com/wanmeihuali/taichi_3d_gaussian_splatting/raw/cf7c1428e8d26495a236726adf9546e4f2a9adb7/config/tat_truck_every_8_test.yaml&#34;&gt;result from this repo(30k iteration)&lt;/a&gt;, top right: ground truth, bottom left: normalized depth, bottom right: normalized num of points per pixel &lt;img src=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/images/tat_truck_image5_val.png&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/images/tat_truck_image7_val.png&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/images/tat_truck_image14_val.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Why taichi?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Taichi is a language for high-performance computing. It is designed to close the gap between the productivity-focused Python language and the performance- and parallelism-focused C++/CUDA languages. By using Taichi, the repo is pure Python, and achieves the same or even better performance compared to CUDA implementation. Also, the code is much easier to read and maintain.&lt;/li&gt; &#xA; &lt;li&gt;Taichi provides various backends, including CUDA, OpenGL, Metal, etc. We do plan to change the backend to support various platforms, but currently, the repo only supports CUDA backend.&lt;/li&gt; &#xA; &lt;li&gt;Taichi provides automatic differentiation, although the repo does not use it currently, it is a nice feature for future development.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Current status&lt;/h2&gt; &#xA;&lt;p&gt;The repo is now tested with the dataset provided by the official implementation. For the truck dataset, The repo is able to achieve a bit higher PSNR than the official implementation with only 1/5 to 1/4 number of points. However, the training/inference speed is still slower than the official implementation.&lt;/p&gt; &#xA;&lt;p&gt;The results for the official implementation and this implementation are tested on the same dataset. I notice that the result from official implementation is slightly different from their paper, the reason may be the difference in testing resolution.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;source&lt;/th&gt; &#xA;   &lt;th&gt;PSNR&lt;/th&gt; &#xA;   &lt;th&gt;SSIM&lt;/th&gt; &#xA;   &lt;th&gt;#points&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Truck(7k)&lt;/td&gt; &#xA;   &lt;td&gt;paper&lt;/td&gt; &#xA;   &lt;td&gt;23.51&lt;/td&gt; &#xA;   &lt;td&gt;0.840&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Truck(7k)&lt;/td&gt; &#xA;   &lt;td&gt;offcial implementation&lt;/td&gt; &#xA;   &lt;td&gt;23.22&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;1.73e6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Truck(7k)&lt;/td&gt; &#xA;   &lt;td&gt;this implementation&lt;/td&gt; &#xA;   &lt;td&gt;23.762359619140625&lt;/td&gt; &#xA;   &lt;td&gt;0.835700511932373&lt;/td&gt; &#xA;   &lt;td&gt;~2.3e5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Truck(30k)&lt;/td&gt; &#xA;   &lt;td&gt;paper&lt;/td&gt; &#xA;   &lt;td&gt;25.187&lt;/td&gt; &#xA;   &lt;td&gt;0.879&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Truck(30k)&lt;/td&gt; &#xA;   &lt;td&gt;offcial implementation&lt;/td&gt; &#xA;   &lt;td&gt;24.88&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;2.1e6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Truck(30k)&lt;/td&gt; &#xA;   &lt;td&gt;this implementation&lt;/td&gt; &#xA;   &lt;td&gt;25.21463966369629&lt;/td&gt; &#xA;   &lt;td&gt;0.8645088076591492&lt;/td&gt; &#xA;   &lt;td&gt;428687.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wanmeihuali/taichi_3d_gaussian_splatting/pull/98#issuecomment-1634828783&#34;&gt;Truck(30k)(recent best result)&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;train:iteration&lt;/th&gt; &#xA;   &lt;th&gt;train:l1loss&lt;/th&gt; &#xA;   &lt;th&gt;train:loss&lt;/th&gt; &#xA;   &lt;th&gt;train:num_valid_points&lt;/th&gt; &#xA;   &lt;th&gt;train:psnr&lt;/th&gt; &#xA;   &lt;th&gt;train:ssim&lt;/th&gt; &#xA;   &lt;th&gt;train:ssimloss&lt;/th&gt; &#xA;   &lt;th&gt;val:loss&lt;/th&gt; &#xA;   &lt;th&gt;val:psnr&lt;/th&gt; &#xA;   &lt;th&gt;val:ssim&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;30000.0&lt;/td&gt; &#xA;   &lt;td&gt;0.02784738875925541&lt;/td&gt; &#xA;   &lt;td&gt;0.04742341861128807&lt;/td&gt; &#xA;   &lt;td&gt;428687.0&lt;/td&gt; &#xA;   &lt;td&gt;25.662137985229492&lt;/td&gt; &#xA;   &lt;td&gt;0.8742724657058716&lt;/td&gt; &#xA;   &lt;td&gt;0.12572753429412842&lt;/td&gt; &#xA;   &lt;td&gt;0.05369199812412262&lt;/td&gt; &#xA;   &lt;td&gt;25.21463966369629&lt;/td&gt; &#xA;   &lt;td&gt;0.8645088076591492&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prepare an environment contains pytorch and torchvision&lt;/li&gt; &#xA; &lt;li&gt;clone the repo and cd into the directory.&lt;/li&gt; &#xA; &lt;li&gt;run the following command&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All dependencies can be installed by pip. pytorch/tochvision can be installed by conda. The code is tested on Ubuntu 20.04.2 LTS with python 3.10.10. The hardware is RTX 3090 and CUDA 12.1. The code is not tested on other platforms, but it should work on other platforms with minor modifications.&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;The algorithm requires point cloud for whole scene, camera parameters, and ground truth image. The point cloud is stored in parquet format. The camera parameters and ground truth image are stored in json format. The running config is stored in yaml format. A script to build dataset from colmap output is provided. It is also possible to build dataset from raw data.&lt;/p&gt; &#xA;&lt;h3&gt;Train on Tank and temple Truck scene&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;CLICK ME&lt;/summary&gt; &#xA; &lt;p&gt; **Disclaimer**: users are required to get permission from the original dataset provider. Any usage of the data must obey the license of the dataset owner. &lt;/p&gt;&#xA; &lt;p&gt;The truck scene in &lt;a href=&#34;https://www.tanksandtemples.org/download/&#34;&gt;tank and temple&lt;/a&gt; dataset is the major dataset used to develop this repo. We use a downsampled version of images in most experiments. The camera poses and the sparse point cloud can be easily generated by colmap. The preprocessed image, pregenerated camera pose and point cloud for truck scene can be downloaded from this &lt;a href=&#34;https://drive.google.com/drive/folders/1ZhMSkm3YGfhtywII5Hik5YDdMzD3lZjX?usp=sharing&#34;&gt;link&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Please download the images into a folder named &lt;code&gt;image&lt;/code&gt; and put it under the root directory of this repo. The camera poses and sparse point cloud should be put under &lt;code&gt;data/tat_truck_every_8_test&lt;/code&gt;. The folder structure should be like this:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ data&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ tat_truck_every_8_test&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ train.json&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ val.json&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ point_cloud.parquet&#xA;‚îú‚îÄ‚îÄ image&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ 000000.png&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ 000001.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;the config file &lt;a href=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/config/tat_truck_every_8_test.yaml&#34;&gt;config/tat_truck_every_8_test.yaml&lt;/a&gt; is provided. The config file is used to specify the dataset path, the training parameters, and the network parameters. The config file is self-explanatory. The training can be started by running&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gaussian_point_train.py --train_config config/tat_truck_every_8_test.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Train on Example Object(boot)&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;CLICK ME&lt;/summary&gt; &#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;p&gt;It is actually one random free mesh from &lt;a href=&#34;https://www.turbosquid.com/3d-models/3d-tactical-boots-1948918&#34;&gt;Internet&lt;/a&gt;, I believe it is free to use. &lt;a href=&#34;https://github.com/maximeraafat/BlenderNeRF.git&#34;&gt;BlenderNerf&lt;/a&gt; is used to generate the dataset. The preprocessed image, pregenerated camera pose and point cloud for boot scene can be downloaded from this &lt;a href=&#34;https://drive.google.com/drive/folders/1d14l9ewnyI7zCA6BxuQUWseQbIKyo3Jh?usp=sharing&#34;&gt;link&lt;/a&gt;. Please download the images into a folder named &lt;code&gt;image&lt;/code&gt; and put it under the root directory of this repo. The camera poses and sparse point cloud should be put under &lt;code&gt;data/boots_super_sparse&lt;/code&gt;. The folder structure should be like this:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ data&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ boots_super_sparse&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ boots_train.json&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ boots_val.json&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ point_cloud.parquet&#xA;‚îú‚îÄ‚îÄ image&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ images_train&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ COS_Camera.001.png&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ COS_Camera.002.png&#xA;|   |   ‚îú‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Note that because the image in this dataset has a higher resolution(1920x1080), training on it is actually slower than training on the truck scene.&lt;/p&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Train on dataset generated by colmap&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;CLICK ME&lt;/summary&gt; &#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Reconstruct using colmap: See &lt;a href=&#34;https://colmap.github.io/tutorial.html&#34;&gt;https://colmap.github.io/tutorial.html&lt;/a&gt;. The image should be undistorted. Sparse reconstruction is usually enough.&lt;/li&gt; &#xA;  &lt;li&gt;save as txt: the standard colmap txt output contains three files, cameras.txt, images.txt, points3D.txt&lt;/li&gt; &#xA;  &lt;li&gt;transform the txt into json and parquet: see &lt;a href=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/tools/prepare_colmap.py&#34;&gt;this file&lt;/a&gt; about how to prepare it.&lt;/li&gt; &#xA;  &lt;li&gt;prepare config yaml: see &lt;a href=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/config/tat_train.yaml&#34;&gt;this file&lt;/a&gt; as an example&lt;/li&gt; &#xA;  &lt;li&gt;run with the config.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Train on dataset with Instant-NGP format with extra mesh&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;CLICK ME&lt;/summary&gt; &#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;ul&gt; &#xA;  &lt;li&gt;A script to convert Instant-NGP format dataset into the two required JSON files is provided. However, the algorithm requires an extra point cloud as input, which does not usually come with Instant-NGP format dataset. The script accepts a mesh file as input and generate a point cloud by sampling points on the mesh. The script is &lt;a href=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/tools/prepare_InstantNGP_with_mesh.py&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;User can run the script with the following command:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/prepare_InstantNGP_with_mesh.py \&#xA;    --transforms_train {path to train transform file} \&#xA;    --transforms_test {path to val transform file, if not provided, val will be sampled from train} \&#xA;    --mesh_path {path to mesh file} \&#xA;    --mesh_sample_points {number of points to sample on the mesh} \&#xA;    --val_sample {if sample val from train, sample by every n frames} \&#xA;    --image_path_prefix {path prefix to the image, usually the path to the folder containing the image folder} \&#xA;    --output_path {path to output folder}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;then in the output folder, there will be two json files, train.json and val.json, and a point cloud file point_cloud.parquet.&lt;/li&gt; &#xA;  &lt;li&gt;create a config yaml file similar to &lt;a href=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/config/test_sagemaker.yaml&#34;&gt;test_sagemaker.yaml&lt;/a&gt;, modify train-dataset-json-path to the path of train.json, val-dataset-json-path to the path of val.json, and pointcloud-parquet-path to the path of point_cloud.parquet. Also modify the summary-writer-log-dir and output-model-dir to where ever you want to save the model and tensorboard log.&lt;/li&gt; &#xA;  &lt;li&gt;run with the config:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gaussian_point_train.py --train_config {path to config yaml}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Train on dataset generated by BlenderNerf&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;CLICK ME&lt;/summary&gt; &#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/maximeraafat/BlenderNeRF.git&#34;&gt;BlenderNerf&lt;/a&gt; is a Blender Plugin to generate dataset for NeRF. The dataset generated by BlenderNerf can be the Instant-NGP format, and we can use the &lt;a href=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/tools/prepare_InstantNGP_with_mesh.py&#34;&gt;script&lt;/a&gt; to convert it into the required format. And the mesh can be easily exported from Blender. To generate the dataset:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Install &lt;a href=&#34;https://www.blender.org/&#34;&gt;Blender&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;import the mesh/scene you want to &lt;a href=&#34;https://www.blender.org/&#34;&gt;Blender&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Install BlenderNerf by following the README in &lt;a href=&#34;https://github.com/maximeraafat/BlenderNeRF.git&#34;&gt;BlenderNerf&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;config BlenderNerf: make sure Train is selected and Test is not selected(Test seems to be buggy), File Format is NGP, save path is filled.&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/images/blendernerf_config_0.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt; &#xA;  &lt;li&gt;config BlenderNerf Camera on Sphere: follow BlenderNerf README to config the camera(default is enough for most case). Then click PLAY COS.&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/images/blender_nerf_config_1.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt; &#xA;  &lt;li&gt;A zip file will be generated in the save path. Unzip it, it should contain a folder named &lt;code&gt;train&lt;/code&gt; and a file named &lt;code&gt;transforms_train.json&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;In Blender, File-&amp;gt;Export-&amp;gt;Stl(.stl), export the mesh as stl file.&lt;/li&gt; &#xA;  &lt;li&gt;can run the &lt;a href=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/tools/prepare_InstantNGP_with_mesh.py&#34;&gt;script&lt;/a&gt; with the following command:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/prepare_InstantNGP_with_mesh.py \&#xA;    --transforms_train {path to transform_train.json} \&#xA;    --mesh_path {path to stl file} \&#xA;    --mesh_sample_points {number of points to sample on the mesh, default to be 500} \&#xA;    --val_sample {if sample val from train, sample by every n frames, default to be 8} \&#xA;    --image_path_prefix {absolute path of the directory contain the train dir} \&#xA;    --output_path {any path you want}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;then in the output folder, there will be two json files, train.json and val.json, and a point cloud file point_cloud.parquet.&lt;/li&gt; &#xA;  &lt;li&gt;create a config yaml file similar to &lt;a href=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/config/test_sagemaker.yaml&#34;&gt;test_sagemaker.yaml&lt;/a&gt;, modify train-dataset-json-path to the path of train.json, val-dataset-json-path to the path of val.json, and pointcloud-parquet-path to the path of point_cloud.parquet. Also modify the summary-writer-log-dir and output-model-dir to where ever you want to save the model and tensorboard log.&lt;/li&gt; &#xA;  &lt;li&gt;run with the config:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gaussian_point_train.py --train_config {path to config yaml}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Train on dataset generated by other methods&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;CLICK ME&lt;/summary&gt; &#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;p&gt;see &lt;a href=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/docs/RawDataFormat.md&#34;&gt;this file&lt;/a&gt; about how to prepare the dataset.&lt;/p&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gaussian_point_train.py --train_config {path to config file}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The training process works in the following way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;stateDiagram-v2&#xA;    state WeightToTrain {&#xA;        sparsePointCloud&#xA;        pointCloudExtraFeatures&#xA;    }&#xA;    WeightToTrain --&amp;gt; Rasterizer: input&#xA;    cameraPose --&amp;gt; Rasterizer: input&#xA;    Rasterizer --&amp;gt; Loss: rasterized image&#xA;    ImageFromMultiViews --&amp;gt; Loss&#xA;    Loss --&amp;gt; Rasterizer: gradient&#xA;    Rasterizer --&amp;gt; WeightToTrain: gradient&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The result is visualized in tensorboard. The tensorboard log is stored in the output directory specified in the config file. The trained point cloud with feature is also stored as parquet and the output directory is specified in the config file.&lt;/p&gt; &#xA;&lt;h3&gt;Run on colab (to take advantage of google provided GPU accelerators)&lt;/h3&gt; &#xA;&lt;p&gt;You can find the related notebook here: &lt;a href=&#34;https://raw.githubusercontent.com/wanmeihuali/taichi_3d_gaussian_splatting/main/tools/run_3d_gaussian_splatting_on_colab.ipynb&#34;&gt;/tools/run_3d_gaussian_splatting_on_colab.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set the hardware accelerator in colab: &#34;Runtime-&amp;gt;Change Runtime Type-&amp;gt;Hardware accelerator-&amp;gt;select GPU-&amp;gt;select T4&#34;&lt;/li&gt; &#xA; &lt;li&gt;Upload this repo to corresponding folder in your google drive.&lt;/li&gt; &#xA; &lt;li&gt;Mount your google drive to your notebook (see notebook).&lt;/li&gt; &#xA; &lt;li&gt;Install condacolab (see notebook).&lt;/li&gt; &#xA; &lt;li&gt;Install requirement.txt with pip (see notebook).&lt;/li&gt; &#xA; &lt;li&gt;Install pytorch, torchvision, pytorch-cuda etc. with conda (see notebook).&lt;/li&gt; &#xA; &lt;li&gt;Prepare the dataset as instructed in &lt;a href=&#34;https://github.com/wanmeihuali/taichi_3d_gaussian_splatting#dataset&#34;&gt;https://github.com/wanmeihuali/taichi_3d_gaussian_splatting#dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run the trainer with correct config (see notebook).&lt;/li&gt; &#xA; &lt;li&gt;Check out the training process through tensorboard (see notebook).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Visualization&lt;/h2&gt; &#xA;&lt;p&gt;A simple visualizer is provided. The visualizer is implemented by Taichi GUI which limited the FPS to 60(If anyone knows how to change this limitation please ping me). The visualizer takes one or multiple parquet results. Example parquets can be downloaded &lt;a href=&#34;https://drive.google.com/file/d/12-kZZay8RFlDk7hJQysG_Cr4-oxDp37l/view?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 visualizer --parquet_path_list &amp;lt;parquet_path_0&amp;gt; &amp;lt;parquet_path_1&amp;gt; ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The visualizer merges multiple point clouds and displays them in the same scene.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Press 0 to select all point clouds(default state).&lt;/li&gt; &#xA; &lt;li&gt;Press 1 to 9 to select one of the point clouds.&lt;/li&gt; &#xA; &lt;li&gt;When all point clouds are selected, use &#34;WASD=-&#34; to move the camera, and use &#34;QE&#34; to rotate by the y-axis, or drag the mouse to do free rotation.&lt;/li&gt; &#xA; &lt;li&gt;When only one of the point clouds is selected, use &#34;WASD=-&#34; to move the object/scene, and use &#34;QE&#34; to rotate the object/scene by the y-axis, or r drag the mouse to do free rotation by the center of the object.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to contribute/Use CI to train on cloud&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;ve enabled CI and cloud-based training now. The function is not very stable yet. It enables anyone to contribute to this repo even if you don&#39;t have a GPU. Generally, the workflow is:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;For any algorithm improvement, please create a new branch and make a pull request.&lt;/li&gt; &#xA; &lt;li&gt;Please @wanmeihuali in the pull request, and I will check the code and add a label &lt;code&gt;need_experiment&lt;/code&gt; or &lt;code&gt;need_experiment_garden&lt;/code&gt; or &lt;code&gt;need_experiment_tat_truck&lt;/code&gt; to the pull request.&lt;/li&gt; &#xA; &lt;li&gt;The CI will automatically build the docker image and upload it to AWS ECR. Then the cloud-based training will be triggered. The training result will be uploaded to the pull request as a comment, e.g. &lt;a href=&#34;https://github.com/wanmeihuali/taichi_3d_gaussian_splatting/pull/38&#34;&gt;this PR&lt;/a&gt;. The dataset is generated by the default config of colmap. The training is on g4dn.xlarge Spot Instance(NVIDIA T4, a weaker GPU than 3090/A6000), the training usually takes 2-3 hours.&lt;/li&gt; &#xA; &lt;li&gt;Now the best training result in README.md is manually updated. I will try to automate this process in the future.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The current implementation is based on my understanding of the paper, and it will have some difference from the paper/official implementation(they plan to release the code in the July). As a personal project, the parameters are not tuned well. I will try to improve performance in the future. Feel free to open an issue if you have any questions, and PRs are welcome, especially for any performance improvement.&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;h3&gt;Algorithm part&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fix the adaptive controller part, something is wrong with the densify process, and the description in the paper is very vague. Further experiments are needed to figure out the correct/better implementation. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;figure if the densify shall apply to all points, or only points in current frame.&lt;/li&gt; &#xA;   &lt;li&gt;figure what &#34;average magnitude of view-space position gradients&#34; means, is it average across frames, or average across pixel?&lt;/li&gt; &#xA;   &lt;li&gt;&lt;del&gt;figure the correct split policy. Where shall the location of new point be? Currently the location is the location before optimization. Will it be better to put it at foci of the original ellipsoid?&lt;/del&gt; use sampling of pdf for over-reconstruct, use position before optimization for under-reconstruct.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add result score/image in README.md &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;try same dataset in the paper.&lt;/li&gt; &#xA;   &lt;li&gt;fix issue in current blender plugin, and also make the plugin open source.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; camera pose optimization: get the gradient of the camera pose, and optimize it during training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Dynamic Rigid Object support. The current implementation already supports multiple camera poses in one scene, so the movement of rigid objects shall be able to transform into the movement of the camera. Need to find some sfm solution that can provide an estimation of 6 DOF pose for different objects, and modify the dataset code to do the test.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Engineering part&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; fix bug: crash when there&#39;s no point in camrea.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add a inference only framework to support adding/moving objects in the scene, scene merging, scene editing, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add a install script/docker image&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support batch training. Currently the code only supports single image training, and only uses small part of the GPU memory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement radix sort/cumsum by Taichi instead of torch, torch-taichi tensor cast seems only available on CUDA device. If we want to switch to other device, we need to get rid of torch.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement a Taichi only inference rasterizer which only use taichi field, and migrate to MacOS/Android/IOS.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>shivammehta25/Matcha-TTS</title>
    <updated>2023-09-21T01:31:03Z</updated>
    <id>tag:github.com,2023-09-21:/shivammehta25/Matcha-TTS</id>
    <link href="https://github.com/shivammehta25/Matcha-TTS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üçµ Matcha-TTS: A fast TTS architecture with conditional flow matching&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;üçµ Matcha-TTS: A fast TTS architecture with conditional flow matching&lt;/h1&gt; &#xA; &lt;h3&gt;&lt;a href=&#34;https://www.kth.se/profile/smehta&#34;&gt;Shivam Mehta&lt;/a&gt;, &lt;a href=&#34;https://www.kth.se/profile/ruibo&#34;&gt;Ruibo Tu&lt;/a&gt;, &lt;a href=&#34;https://www.kth.se/profile/beskow&#34;&gt;Jonas Beskow&lt;/a&gt;, &lt;a href=&#34;https://www.kth.se/profile/szekely&#34;&gt;√âva Sz√©kely&lt;/a&gt;, and &lt;a href=&#34;https://people.kth.se/~ghe/&#34;&gt;Gustav Eje Henter&lt;/a&gt;&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.python.org/downloads/release/python-3100/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Python_3.10-blue?logo=python&amp;amp;logoColor=white&#34; alt=&#34;python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PyTorch_2.0+-ee4c2c?logo=pytorch&amp;amp;logoColor=white&#34; alt=&#34;pytorch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pytorchlightning.ai/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Lightning_2.0+-792ee5?logo=pytorchlightning&amp;amp;logoColor=white&#34; alt=&#34;lightning&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hydra.cc/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Config-Hydra_1.3-89b8cd&#34; alt=&#34;hydra&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://black.readthedocs.io/en/stable/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20Style-Black-black.svg?labelColor=gray&#34; alt=&#34;black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pycqa.github.io/isort/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&amp;amp;labelColor=ef8336&#34; alt=&#34;isort&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p style=&#34;text-align: center;&#34;&gt; &lt;img src=&#34;https://shivammehta25.github.io/Matcha-TTS/images/logo.png&#34; height=&#34;128&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This is the official code implementation of üçµ Matcha-TTS.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We propose üçµ Matcha-TTS, a new approach to non-autoregressive neural TTS, that uses &lt;a href=&#34;https://arxiv.org/abs/2210.02747&#34;&gt;conditional flow matching&lt;/a&gt; (similar to &lt;a href=&#34;https://arxiv.org/abs/2209.03003&#34;&gt;rectified flows&lt;/a&gt;) to speed up ODE-based speech synthesis. Our method:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Is probabilistic&lt;/li&gt; &#xA; &lt;li&gt;Has compact memory footprint&lt;/li&gt; &#xA; &lt;li&gt;Sounds highly natural&lt;/li&gt; &#xA; &lt;li&gt;Is very fast to synthesise from&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://shivammehta25.github.io/Matcha-TTS&#34;&gt;demo page&lt;/a&gt; and read &lt;a href=&#34;https://arxiv.org/abs/2309.03199&#34;&gt;our arXiv preprint&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/17C_gYgEHOxI5ZypcfE_k1piKCtyR0isJ?usp=sharing&#34;&gt;Pre-trained models&lt;/a&gt; will be automatically downloaded with the CLI or gradio interface.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/shivammehta25/Matcha-TTS&#34;&gt;Try üçµ Matcha-TTS on HuggingFace ü§ó spaces!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create an environment (suggested but optional)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n matcha-tts python=3.10 -y&#xA;conda activate matcha-tts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Matcha TTS using pip or from source&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install matcha-tts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;from source&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/shivammehta25/Matcha-TTS.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run CLI / gradio app / jupyter notebook&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# This will download the required models&#xA;matcha-tts --text &#34;&amp;lt;INPUT TEXT&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;matcha-tts-app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or open &lt;code&gt;synthesis.ipynb&lt;/code&gt; on jupyter notebook&lt;/p&gt; &#xA;&lt;h3&gt;CLI Arguments&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To synthesise from given text, run:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;matcha-tts --text &#34;&amp;lt;INPUT TEXT&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To synthesise from a file, run:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;matcha-tts --file &amp;lt;PATH TO FILE&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To batch synthesise from a file, run:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;matcha-tts --file &amp;lt;PATH TO FILE&amp;gt; --batched&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additional arguments&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speaking rate&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;matcha-tts --text &#34;&amp;lt;INPUT TEXT&amp;gt;&#34; --speaking_rate 1.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sampling temperature&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;matcha-tts --text &#34;&amp;lt;INPUT TEXT&amp;gt;&#34; --temperature 0.667&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Euler ODE solver steps&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;matcha-tts --text &#34;&amp;lt;INPUT TEXT&amp;gt;&#34; --steps 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train with your own dataset&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s assume we are training with LJ Speech&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the dataset from &lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34;&gt;here&lt;/a&gt;, extract it to &lt;code&gt;data/LJSpeech-1.1&lt;/code&gt;, and prepare the file lists to point to the extracted data like for &lt;a href=&#34;https://github.com/NVIDIA/tacotron2#setup&#34;&gt;item 5 in the setup of the NVIDIA Tacotron 2 repo&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone and enter the Matcha-TTS repository&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/shivammehta25/Matcha-TTS.git&#xA;cd Matcha-TTS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install the package from source&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Go to &lt;code&gt;configs/data/ljspeech.yaml&lt;/code&gt; and change&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;train_filelist_path: data/filelists/ljs_audio_text_train_filelist.txt&#xA;valid_filelist_path: data/filelists/ljs_audio_text_val_filelist.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Generate normalisation statistics with the yaml file of dataset configuration&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;matcha-data-stats -i ljspeech.yaml&#xA;# Output:&#xA;#{&#39;mel_mean&#39;: -5.53662231756592, &#39;mel_std&#39;: 2.1161014277038574}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Update these values in &lt;code&gt;configs/data/ljspeech.yaml&lt;/code&gt; under &lt;code&gt;data_statistics&lt;/code&gt; key.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;data_statistics:  # Computed for ljspeech dataset&#xA;  mel_mean: -5.536622&#xA;  mel_std: 2.116101&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to the paths of your train and validation filelists.&lt;/p&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Run the training script&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make train-ljspeech&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python matcha/train.py experiment=ljspeech&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for a minimum memory run&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python matcha/train.py experiment=ljspeech_min_memory&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for multi-gpu training, run&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python matcha/train.py experiment=ljspeech trainer.devices=[0,1]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;Synthesise from the custom trained model&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;matcha-tts --text &#34;&amp;lt;INPUT TEXT&amp;gt;&#34; --checkpoint_path &amp;lt;PATH TO CHECKPOINT&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation information&lt;/h2&gt; &#xA;&lt;p&gt;If you use our code or otherwise find this work useful, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@article{mehta2023matcha,&#xA;  title={Matcha-TTS: A fast TTS architecture with conditional flow matching},&#xA;  author={Mehta, Shivam and Tu, Ruibo and Beskow, Jonas and Sz{\&#39;e}kely, {\&#39;E}va and Henter, Gustav Eje},&#xA;  journal={arXiv preprint arXiv:2309.03199},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Since this code uses &lt;a href=&#34;https://github.com/ashleve/lightning-hydra-template&#34;&gt;Lightning-Hydra-Template&lt;/a&gt;, you have all the powers that come with it.&lt;/p&gt; &#xA;&lt;p&gt;Other source code I would like to acknowledge:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/tree/dev&#34;&gt;Coqui-TTS&lt;/a&gt;: For helping me figure out how to make cython binaries pip installable and encouragement&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face Diffusers&lt;/a&gt;: For their awesome diffusers library and its components&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS&#34;&gt;Grad-TTS&lt;/a&gt;: For the monotonic alignment search source code&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DiffEqML/torchdyn&#34;&gt;torchdyn&lt;/a&gt;: Useful for trying other ODE solvers during research and development&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nn.labml.ai/transformers/rope/index.html&#34;&gt;labml.ai&lt;/a&gt;: For the RoPE implementation&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ethereum-optimism/op-analytics</title>
    <updated>2023-09-21T01:31:03Z</updated>
    <id>tag:github.com,2023-09-21:/ethereum-optimism/op-analytics</id>
    <link href="https://github.com/ethereum-optimism/op-analytics" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Onchain Data, Utilities, References, and other Analytics on Optimism&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OP Analytics&lt;/h1&gt; &#xA;&lt;p&gt;Onchain Data, Utilities, References, and other Analytics on Optimism. Join the conversation with other numba nerds in the #analytics channel in the Optimism Discord.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum-optimism/op-analytics/main/#im-looking-for-data-about&#34;&gt;I&#39;m looking for Data About&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum-optimism/op-analytics/main/#select-optimism-data-abstractions&#34;&gt;Select Optimism Data Abstractions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum-optimism/op-analytics/main/#contributors&#34;&gt;Contributors&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;I&#39;m Looking for Data About:&lt;/h2&gt; &#xA;&lt;p&gt;A select list of Optimism data dashboards:&lt;/p&gt; &#xA;&lt;h3&gt;Optimism Network Activity&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dune.com/oplabspbc/op-stack-chains-l1-activity&#34;&gt;OP Stack Chains - L1 Mainnet Activity &amp;amp; Fees Paid, L2 OP Chain Activity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dune.com/optimismfnd/Optimism&#34;&gt;OP Mainnet Protocol Metrics (i.e. transactions, fees, onchain value)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dune.com/oplabspbc/developer-metrics-sploring&#34;&gt;Active Developer Metrics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dune.com/optimismfnd/Optimism-Project-Usage-Trends&#34;&gt;Popular Apps on OP Mainnet and Project Usage Trends&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Token House &amp;amp; Citizen&#39;s House Governance&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dune.com/optimismfnd/optimism-op-token-house&#34;&gt;OP Token House Delegates&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fscrypto.co/op-governance&#34;&gt;Governance &amp;amp; Voting (by Flipside Crypto)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://app.flipsidecrypto.com/dashboard/optimism-attestation-station-data-station-WAT27_&#34;&gt;AttestationStation Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dune.com/oplabspbc/optimism-attestationstation&#34;&gt;AttestationStation Key and Creator Distributions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;OP Token Distributions &amp;amp; Growth Programs Tracking&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dune.com/oplabspbc/optimism-incentive-program-usage-summary&#34;&gt;Incentive Program Onchain Usage Summary Dashboard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://static.optimism.io/op-analytics/op_rewards_tracking/img_outputs/overall/cumul_ndf_last_price.html&#34;&gt;Time-Series of TVL Flows by Program&lt;/a&gt; (Sourced from DefiLlama and TheGraph API)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ethereum-optimism/op-analytics/tree/main/op_rewards_tracking/img_outputs/app/last_price/svg&#34;&gt;Program-Specific TVL Flows Charts&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;See Distributions mapping resources in &lt;a href=&#34;https://raw.githubusercontent.com/ethereum-optimism/op-analytics/main/#select-optimism-data-abstractions&#34;&gt;Select Optimism Data Abstractions&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;DeFi&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dune.com/rplust/Perpetuals-Trading-on-Optimism&#34;&gt;Perpetuals Market (by rplust)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://defillama.com/chain/Optimism&#34;&gt;Total Value Locked in DeFi (by Defillama)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://defillama.com/fees/chains/optimism&#34;&gt;App Fees &amp;amp; Revenue (by Defillama)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://defillama.com/dexs/chains/optimism&#34;&gt;DEX Trade Volume (by Defillama)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TVL Flows Between Apps and Chains: &lt;a href=&#34;https://static.optimism.io/op-analytics/value_locked_flows/img_outputs/html/net_app_flows_7d.html&#34;&gt;Last 7 Days&lt;/a&gt;, &lt;a href=&#34;https://static.optimism.io/op-analytics/value_locked_flows/img_outputs/html/net_app_flows_30d.html&#34;&gt;30 Days&lt;/a&gt;, &lt;a href=&#34;https://static.optimism.io/op-analytics/value_locked_flows/img_outputs/html/net_app_flows_90d.html&#34;&gt;90 Days&lt;/a&gt;, &lt;a href=&#34;https://static.optimism.io/op-analytics/value_locked_flows/img_outputs/html/net_app_flows_180d.html&#34;&gt;180 Days&lt;/a&gt;, &lt;a href=&#34;https://static.optimism.io/op-analytics/value_locked_flows/img_outputs/html/net_app_flows_365d.html&#34;&gt;365 Days&lt;/a&gt; (From Defillama API)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Consumer&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dune.com/oplabspbc/optimism-nft-secondary-marketplaces&#34;&gt;NFT Marketplaces and Collection Volume&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dune.com/chuxin/optimism-nft-bridge?L1+NFT+Contract+Address_t4e85b=0x5180db8f5c931aae63c74266b211f580155ecac8&#34;&gt;L1&amp;lt;&amp;gt;L2 NFT Bridge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dune.com/oplabspbc/optimism-quests-project-usage-growth&#34;&gt;App Growth on Optimism After Quests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dune.com/oplabspbc/optimism-onoff-ramp-usage&#34;&gt;CEX &amp;amp; On/Off-Ramp Usage&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Protocol Economics&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dune.com/optimismfnd/optimism-l1-batch-submission-fees-security-costs&#34;&gt;L2 Transaction Fees, L1 Security Costs, Sequencer Fees&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tokenterminal.com/terminal/projects/optimism/revenue-share&#34;&gt;Protocol Revenue (by TokenTerminal)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Transaction Costs&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dune.com/optimismfnd/How-Much-Could-You-Save-on-Optimism-Fee-Savings-Calculator&#34;&gt;Transaction Fee Savings Calculator&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Select Optimism Data Abstractions&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/duneanalytics/spellbook/tree/main/models&#34;&gt;Dune Spellbook&lt;/a&gt;&lt;/strong&gt;: &lt;em&gt;Tables can be used in &lt;a href=&#34;https://dune.com/browse/dashboards&#34;&gt;Dune Analytics&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/duneanalytics/spellbook/tree/main/models/contracts/optimism&#34;&gt;&lt;code&gt;contracts_optimism.contract_mapping&lt;/code&gt;&lt;/a&gt;: Near exhaustive mappings of contracts to project names on Optimism - uses decoded contracts in Dune (&lt;code&gt;optimism.contracts&lt;/code&gt;) and known deployer addresses to map contracts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/duneanalytics/spellbook/tree/main/models/op/token_distributions/optimism&#34;&gt;&lt;code&gt;op_token_distributions_optimism.transfer_mapping&lt;/code&gt;&lt;/a&gt;: Mappings of token distributions from the OP Foundation &amp;amp; by Grant/Growth Experiment recipients. You can use this table to count how much OP has been deployed, by whom, and to where. &lt;em&gt;Note: These are &#34;best guess&#34; mappings&lt;/em&gt; (contribute address mappings in the &lt;a href=&#34;https://github.com/duneanalytics/spellbook/tree/main/models/op/token_distributions/optimism&#34;&gt;Dune Spellbook repo&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/duneanalytics/spellbook/tree/main/models/dex&#34;&gt;&lt;code&gt;dex.trades&lt;/code&gt;&lt;/a&gt;: Aggregation of swaps across many decentralized exchanges&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/duneanalytics/spellbook/tree/main/models/nft&#34;&gt;&lt;code&gt;nft.trades&lt;/code&gt;&lt;/a&gt;: Aggregation of swaps across many NFT marketplaces. Also see &lt;a href=&#34;https://github.com/duneanalytics/spellbook/raw/main/models/nft/optimism/nft_optimism_wash_trades.sql&#34;&gt;&lt;code&gt;nft.wash_trades&lt;/code&gt;&lt;/a&gt; by hildobby for filtering out likely wash trades.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/duneanalytics/spellbook/tree/main/models/perpetual&#34;&gt;&lt;code&gt;perpetual.trades&lt;/code&gt;&lt;/a&gt;: Aggregation of swaps across many perpetuals exchanges (by rplust)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/FlipsideCrypto/optimism-models/tree/main/models/gold&#34;&gt;Flipside Crypto - Optimism Models &amp;gt; Gold-Level Tables&lt;/a&gt;&lt;/strong&gt;: &lt;em&gt;Tables can be used in &lt;a href=&#34;https://flipsidecrypto.xyz/&#34;&gt;Flipside&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FlipsideCrypto/optimism-models/tree/main/models/gold/dex&#34;&gt;&lt;code&gt;optimism.core.ez_dex_swaps&lt;/code&gt;&lt;/a&gt;: Aggregation of swaps across many decentralized exchanges&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FlipsideCrypto/optimism-models/raw/main/models/gold/core__ez_nft_sales.sql&#34;&gt;&lt;code&gt;optimism.core.ez_nft_sales&lt;/code&gt;&lt;/a&gt;: Aggregation of swaps across many NFT marketplaces&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FlipsideCrypto/optimism-models/raw/main/models/gold/core__fact_delegations.sql&#34;&gt;&lt;code&gt;optimism.core.fact_delegations&lt;/code&gt;&lt;/a&gt;: Aggregation of OP governance delegation events.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;h3&gt;Configs&lt;/h3&gt; &#xA;&lt;p&gt;For scripts which use APIs from providers with API keys, add the lines like below in a .env file (Replace with your API key - remember to add to gitignore):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;DUNE_API_KEY = &#39;Your API Key&#39;&#xA;FLIPSIDE_SHROOMDK_KEY = &#39;Your API Key&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pip install pipenv&#xA;pipenv install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;Pipfile&lt;/code&gt; for all the requirements.&lt;/p&gt; &#xA;&lt;h3&gt;Common Requirements&lt;/h3&gt; &#xA;&lt;p&gt;Common packages used for python scripts include&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pandas-dev/pandas&#34;&gt;pandas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/psf/requests&#34;&gt;requests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/inyutin/aiohttp_retry&#34;&gt;aiohttp-retry&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cowprotocol/dune-client&#34;&gt;dune-client&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/0xPlaygrounds/subgrounds&#34;&gt;subgrounds&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ethereum/web3.py&#34;&gt;web3.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/blockchain-etl/ethereum-etl&#34;&gt;ethereum-etl&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In this repository, we use &lt;code&gt;pre-commit&lt;/code&gt; to ensure consistency of formatting. To install for Mac, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install pre-commit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once installed, in the command line of the repository, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pre-commit install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will install &lt;code&gt;pre-commit&lt;/code&gt; to the Git hook, so that &lt;code&gt;pre-commit&lt;/code&gt; will run and fix files covered in its config before committing.&lt;/p&gt;</summary>
  </entry>
</feed>