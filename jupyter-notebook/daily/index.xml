<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-02T01:45:24Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>GoogleCloudPlatform/vertex-ai-samples</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/GoogleCloudPlatform/vertex-ai-samples</id>
    <link href="https://github.com/GoogleCloudPlatform/vertex-ai-samples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Sample code and notebooks for Vertex AI, the end-to-end machine learning platform on Google Cloud&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google Cloud Vertex AI Samples&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome to the Google Cloud &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/&#34;&gt;Vertex AI&lt;/a&gt; sample repository.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The repository contains &lt;a href=&#34;https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks&#34;&gt;notebooks&lt;/a&gt; and &lt;a href=&#34;https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/community-content&#34;&gt;community content&lt;/a&gt; that demonstrate how to develop and manage ML workflows using Google Cloud Vertex AI.&lt;/p&gt; &#xA;&lt;h2&gt;Repository structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;‚îú‚îÄ‚îÄ community-content - Sample code and tutorials contributed by the community&#xA;‚îú‚îÄ‚îÄ notebooks&#xA;‚îÇ   ‚îú‚îÄ‚îÄ community - Notebooks contributed by the community&#xA;‚îÇ   ‚îú‚îÄ‚îÄ official - Notebooks demonstrating use of each Vertex AI service&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ automl&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ custom&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions welcome! See the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/vertex-ai-samples/raw/master/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting help&lt;/h2&gt; &#xA;&lt;p&gt;Please use the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/vertex-ai-samples/issues&#34;&gt;issues page&lt;/a&gt; to provide feedback or submit a bug report.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is not an officially supported Google product. The code in this repository is for demonstrative purposes only.&lt;/p&gt; &#xA;&lt;h2&gt;Feedback&lt;/h2&gt; &#xA;&lt;p&gt;Please feel free to fill out our &lt;a href=&#34;https://bit.ly/vertex-ai-samples-survey&#34;&gt;survey&lt;/a&gt; to give us feedback on the repo and its content.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Coding-with-Adam/Dash-by-Plotly</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/Coding-with-Adam/Dash-by-Plotly</id>
    <link href="https://github.com/Coding-with-Adam/Dash-by-Plotly" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Interactive data analytics&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;If you Forked it, Support it&lt;/h1&gt; &#xA;&lt;p&gt;A growing number of viewers are looking for high quality, professional content on Dash, which is hard to find. I am trying to fill that gap and help you grow in the area of analytic web apps in Python.&lt;/p&gt; &#xA;&lt;p&gt;My goal is to make this a sustainable project for myself and my viewers. This repository and my Charming Data channel is a 100% member-supported educational channel. Your support would mean a lot to me üôè&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sponsors/Coding-with-Adam&#34;&gt;Github support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.patreon.com/charmingdata&#34;&gt;Patreon support&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dash-by-Plotly&lt;/h2&gt; &#xA;&lt;p&gt;This Repository is dedicated to teaching Dash and Plotly to anyone that is interested. Dash is a powerful platform that can benefit anyone that works with data: analytical consultants, data analysts, professors, business owners, financial analysts, or those that work in marketing, social media, the medical field... If you work with data, Dash Plotly is a great tool to have.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dash-docs.herokuapp.com/introduction&#34;&gt;User Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;I recommend installing pandas, since you will most likely use it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install pandas&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, just install Dash. Plotly comes with Dash&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install dash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;d like to control the version of Plotly installed, you can do for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install plotly==5.7.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;To Get Help&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://community.plotly.com/&#34;&gt;Plotly Community Forum&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is a wonderful community of people dedicated to supporting others learning Dash. You can find me there as well under the name CharmingData.&lt;/p&gt; &#xA;&lt;h2&gt;Execute Code in Browser&lt;/h2&gt; &#xA;&lt;p&gt;If you prefer to run the code of this repository directly online instead of on your computer, paste my Workspace link into your browser and follow the gif below.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://gitpod.io#snapshot/11d6996e-7f94-48fc-85f6-0257353d7659&#34;&gt;Workspace Snapshot&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/32049495/167286451-f53e5e40-b5eb-4fc6-ad53-f7ca0e660942.gif&#34; alt=&#34;gitpod-demo&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>niconielsen32/ComputerVision</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/niconielsen32/ComputerVision</id>
    <link href="https://github.com/niconielsen32/ComputerVision" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>saic-mdal/lama</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/saic-mdal/lama</id>
    <link href="https://github.com/saic-mdal/lama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü¶ô LaMa Image Inpainting, Resolution-robust Large Mask Inpainting with Fourier Convolutions, WACV 2022&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ü¶ô LaMa: Resolution-robust Large Mask Inpainting with Fourier Convolutions&lt;/h1&gt; &#xA;&lt;p&gt;Official implementation by Samsung Research&lt;/p&gt; &#xA;&lt;p&gt;by Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, Victor Lempitsky.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; &#34;font-size:30px;&#34;&gt; üî•üî•üî• &lt;br&gt; &lt;b&gt; LaMa generalizes surprisingly well to much higher resolutions (~2k‚ùóÔ∏è) than it saw during training (256x256), and achieves the excellent performance even in challenging scenarios, e.g. completion of periodic structures.&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://saic-mdal.github.io/lama-project/&#34;&gt;Project page&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2109.07161&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://ashukha.com/projects/lama_21/lama_supmat_2021.pdf&#34;&gt;Supplementary&lt;/a&gt;] [&lt;a href=&#34;https://senya-ashukha.github.io/projects/lama_21/paper.txt&#34;&gt;BibTeX&lt;/a&gt;] [&lt;a href=&#34;https://www.casualganpapers.com/large-masks-fourier-convolutions-inpainting/LaMa-explained.html&#34;&gt;Casual GAN Papers Summary&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://colab.research.google.com/github/saic-mdal/lama/blob/master//colab/LaMa_inpainting.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;br&gt; Try out in Google Colab &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/projects/lama_21/ezgif-4-0db51df695a8.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/projects/lama_21/gif_for_lightning_v1_white.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Non-official 3rd party apps:&lt;/h1&gt; &#xA;&lt;p&gt;(Feel free to share your app/implementation/demo by creating an issue)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cleanup.pictures/&#34;&gt;https://cleanup.pictures&lt;/a&gt; - a simple interactive object removal tool by &lt;a href=&#34;https://twitter.com/cyrildiagne&#34;&gt;@cyrildiagne&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/lama-cleaner&#34;&gt;lama-cleaner&lt;/a&gt; by &lt;a href=&#34;https://github.com/Sanster/lama-cleaner&#34;&gt;@Sanster&lt;/a&gt; is a self-host version of &lt;a href=&#34;https://cleanup.pictures/&#34;&gt;https://cleanup.pictures&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See demo: &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/lama&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; by &lt;a href=&#34;https://github.com/AK391&#34;&gt;@AK391&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Telegram bot &lt;a href=&#34;https://t.me/MagicEraserBot&#34;&gt;@MagicEraserBot&lt;/a&gt; by &lt;a href=&#34;https://github.com/Moldoteck&#34;&gt;@Moldoteck&lt;/a&gt;, &lt;a href=&#34;https://github.com/Moldoteck/MagicEraser&#34;&gt;code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andy971022/auto-lama&#34;&gt;Auto-LaMa&lt;/a&gt; = DE:TR object detection + LaMa inpainting by &lt;a href=&#34;https://github.com/andy971022&#34;&gt;@andy971022&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhaoyun0071/LAMA-Magic-Eraser-Local&#34;&gt;LAMA-Magic-Eraser-Local&lt;/a&gt; = a standalone inpainting application built with PyQt5 by &lt;a href=&#34;https://github.com/zhaoyun0071&#34;&gt;@zhaoyun0071&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.hama.app/&#34;&gt;Hama&lt;/a&gt; - object removal with a smart brush which simplifies mask drawing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Environment setup&lt;/h1&gt; &#xA;&lt;p&gt;Clone the repo: &lt;code&gt;git clone https://github.com/saic-mdal/lama.git&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are three options of an environment:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Python virtualenv:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;virtualenv inpenv --python=/usr/bin/python3&#xA;source inpenv/bin/activate&#xA;pip install torch==1.8.0 torchvision==0.9.0&#xA;&#xA;cd lama&#xA;pip install -r requirements.txt &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Conda&lt;/p&gt; &lt;pre&gt;&lt;code&gt;% Install conda for Linux, for other OS download miniconda at https://docs.conda.io/en/latest/miniconda.html&#xA;wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh&#xA;bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda&#xA;$HOME/miniconda/bin/conda init bash&#xA;&#xA;cd lama&#xA;conda env create -f conda_env.yml&#xA;conda activate lama&#xA;conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch -y&#xA;pip install pytorch-lightning==1.2.9&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Docker: No actions are needed üéâ.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Inference &lt;a name=&#34;prediction&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Download pre-trained models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install tool for yandex disk link extraction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install wldhx.yadisk-direct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The best model (Places2, Places Challenge):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -L $(yadisk-direct https://disk.yandex.ru/d/ouP6l8VJ0HpMZg) -o big-lama.zip&#xA;unzip big-lama.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All models (Places &amp;amp; CelebA-HQ):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -L $(yadisk-direct https://disk.yandex.ru/d/EgqaSnLohjuzAg) -o lama-models.zip&#xA;unzip lama-models.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Prepare images and masks&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download test images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -L $(yadisk-direct https://disk.yandex.ru/d/xKQJZeVRk5vLlQ) -o LaMa_test_images.zip&#xA;unzip LaMa_test_images.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;OR prepare your data:&lt;/summary&gt; 1) Create masks named as `[images_name]_maskXXX[image_suffix]`, put images and masks in the same folder. &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;You can use the &lt;a href=&#34;https://github.com/saic-mdal/lama/raw/main/bin/gen_mask_dataset.py&#34;&gt;script&lt;/a&gt; for random masks generation.&lt;/li&gt; &#xA;  &lt;li&gt;Check the format of the files: &lt;pre&gt;&lt;code&gt;image1_mask001.png&#xA;image1.png&#xA;image2_mask001.png&#xA;image2.png&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Specify &lt;code&gt;image_suffix&lt;/code&gt;, e.g. &lt;code&gt;.png&lt;/code&gt; or &lt;code&gt;.jpg&lt;/code&gt; or &lt;code&gt;_input.jpg&lt;/code&gt; in &lt;code&gt;configs/prediction/default.yaml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Predict&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/LaMa_test_images outdir=$(pwd)/output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;OR&lt;/strong&gt; in the docker&lt;/p&gt; &#xA;&lt;p&gt;The following command will pull the docker image from Docker Hub and execute the prediction script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash docker/2_predict.sh $(pwd)/big-lama $(pwd)/LaMa_test_images $(pwd)/output device=cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker cuda: TODO&lt;/p&gt; &#xA;&lt;h1&gt;Train and Eval&lt;/h1&gt; &#xA;&lt;p&gt;‚ö†Ô∏è Warning: The training is not fully tested yet, e.g., did not re-training after refactoring ‚ö†Ô∏è&lt;/p&gt; &#xA;&lt;p&gt;Make sure you run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then download models for &lt;em&gt;perceptual loss&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p ade20k/ade20k-resnet50dilated-ppm_deepsup/&#xA;wget -P ade20k/ade20k-resnet50dilated-ppm_deepsup/ http://sceneparsing.csail.mit.edu/model/pytorch/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Places&lt;/h2&gt; &#xA;&lt;p&gt;‚ö†Ô∏è NB: FID/SSIM/LPIPS metric values for Places that we see in LaMa paper are computed on 30000 images that we produce in evaluation section below. For more details on evaluation data check [&lt;a href=&#34;https://ashukha.com/projects/lama_21/lama_supmat_2021.pdf#subsection.3.1&#34;&gt;Section 3. Dataset splits in Supplementary&lt;/a&gt;] ‚ö†Ô∏è&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Download data from http://places2.csail.mit.edu/download.html&#xA;# Places365-Standard: Train(105GB)/Test(19GB)/Val(2.1GB) from High-resolution images section&#xA;wget http://data.csail.mit.edu/places/places365/train_large_places365standard.tar&#xA;wget http://data.csail.mit.edu/places/places365/val_large.tar&#xA;wget http://data.csail.mit.edu/places/places365/test_large.tar&#xA;&#xA;# Unpack train/test/val data and create .yaml config for it&#xA;bash fetch_data/places_standard_train_prepare.sh&#xA;bash fetch_data/places_standard_test_val_prepare.sh&#xA;&#xA;# Sample images for test and viz at the end of epoch&#xA;bash fetch_data/places_standard_test_val_sample.sh&#xA;bash fetch_data/places_standard_test_val_gen_masks.sh&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier location=places_standard&#xA;&#xA;# To evaluate trained model and report metrics as in our paper&#xA;# we need to sample previously unseen 30k images and generate masks for them&#xA;bash fetch_data/places_standard_evaluation_prepare_data.sh&#xA;&#xA;# Infer model on thick/thin/medium masks in 256 and 512 and run evaluation &#xA;# like this:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier_/ \&#xA;indir=$(pwd)/places_standard_dataset/evaluation/random_thick_512/ \&#xA;outdir=$(pwd)/inference/random_thick_512 model.checkpoint=last.ckpt&#xA;&#xA;python3 bin/evaluate_predicts.py \&#xA;$(pwd)/configs/eval2_gpu.yaml \&#xA;$(pwd)/places_standard_dataset/evaluation/random_thick_512/ \&#xA;$(pwd)/inference/random_thick_512 \&#xA;$(pwd)/inference/random_thick_512_metrics.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;CelebA&lt;/h2&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Make shure you are in lama folder&#xA;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&#xA;# Download CelebA-HQ dataset&#xA;# Download data256x256.zip from https://drive.google.com/drive/folders/11Vz0fqHS2rXDb5pprgTjpD7S2BAJhi1P&#xA;&#xA;# unzip &amp;amp; split into train/test/visualization &amp;amp; create config for it&#xA;bash fetch_data/celebahq_dataset_prepare.sh&#xA;&#xA;# generate masks for test and visual_test at the end of epoch&#xA;bash fetch_data/celebahq_gen_masks.sh&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier-celeba data.batch_size=10&#xA;&#xA;# Infer model on thick/thin/medium masks in 256 and run evaluation &#xA;# like this:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier-celeba_/ \&#xA;indir=$(pwd)/celeba-hq-dataset/visual_test_256/random_thick_256/ \&#xA;outdir=$(pwd)/inference/celeba_random_thick_256 model.checkpoint=last.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;Places Challenge&lt;/h2&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# This script downloads multiple .tar files in parallel and unpacks them&#xA;# Places365-Challenge: Train(476GB) from High-resolution images (to train Big-Lama) &#xA;bash places_challenge_train_download.sh&#xA;&#xA;TODO: prepare&#xA;TODO: train &#xA;TODO: eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;Create your data&lt;/h2&gt; &#xA;&lt;p&gt;Please check bash scripts for data preparation and mask generation from CelebaHQ section, if you stuck at one of the following steps.&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Make shure you are in lama folder&#xA;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&#xA;# You need to prepare following image folders:&#xA;$ ls my_dataset&#xA;train&#xA;val_source # 2000 or more images&#xA;visual_test_source # 100 or more images&#xA;eval_source # 2000 or more images&#xA;&#xA;# LaMa generates random masks for the train data on the flight,&#xA;# but needs fixed masks for test and visual_test for consistency of evaluation.&#xA;&#xA;# Suppose, we want to evaluate and pick best models &#xA;# on 512x512 val dataset  with thick/thin/medium masks &#xA;# And your images have .jpg extention:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \ # thick, thin, medium&#xA;my_dataset/val_source/ \&#xA;my_dataset/val/random_&amp;lt;size&amp;gt;_512.yaml \# thick, thin, medium&#xA;--ext jpg&#xA;&#xA;# So the mask generator will: &#xA;# 1. resize and crop val images and save them as .png&#xA;# 2. generate masks&#xA;&#xA;ls my_dataset/val/random_medium_512/&#xA;image1_crop000_mask000.png&#xA;image1_crop000.png&#xA;image2_crop000_mask000.png&#xA;image2_crop000.png&#xA;...&#xA;&#xA;# Generate thick, thin, medium masks for visual_test folder:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \  #thick, thin, medium&#xA;my_dataset/visual_test_source/ \&#xA;my_dataset/visual_test/random_&amp;lt;size&amp;gt;_512/ \ #thick, thin, medium&#xA;--ext jpg&#xA;&#xA;&#xA;ls my_dataset/visual_test/random_thick_512/&#xA;image1_crop000_mask000.png&#xA;image1_crop000.png&#xA;image2_crop000_mask000.png&#xA;image2_crop000.png&#xA;...&#xA;&#xA;# Same process for eval_source image folder:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \  #thick, thin, medium&#xA;my_dataset/eval_source/ \&#xA;my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \ #thick, thin, medium&#xA;--ext jpg&#xA;&#xA;&#xA;&#xA;# Generate location config file which locate these folders:&#xA;&#xA;touch my_dataset.yaml&#xA;echo &#34;data_root_dir: $(pwd)/my_dataset/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;echo &#34;out_root_dir: $(pwd)/experiments/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;echo &#34;tb_dir: $(pwd)/tb_logs/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;mv my_dataset.yaml ${PWD}/configs/training/location/&#xA;&#xA;&#xA;# Check data config for consistency with my_dataset folder structure:&#xA;$ cat ${PWD}/configs/training/data/abl-04-256-mh-dist&#xA;...&#xA;train:&#xA;  indir: ${location.data_root_dir}/train&#xA;  ...&#xA;val:&#xA;  indir: ${location.data_root_dir}/val&#xA;  img_suffix: .png&#xA;visual_test:&#xA;  indir: ${location.data_root_dir}/visual_test&#xA;  img_suffix: .png&#xA;&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier location=my_dataset data.batch_size=10&#xA;&#xA;# Evaluation: LaMa training procedure picks best few models according to &#xA;# scores on my_dataset/val/ &#xA;&#xA;# To evaluate one of your best models (i.e. at epoch=32) &#xA;# on previously unseen my_dataset/eval do the following &#xA;# for thin, thick and medium:&#xA;&#xA;# infer:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier_/ \&#xA;indir=$(pwd)/my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \&#xA;outdir=$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512 \&#xA;model.checkpoint=epoch32.ckpt&#xA;&#xA;# metrics calculation:&#xA;python3 bin/evaluate_predicts.py \&#xA;$(pwd)/configs/eval2_gpu.yaml \&#xA;$(pwd)/my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \&#xA;$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512 \&#xA;$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512_metrics.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;OR&lt;/strong&gt; in the docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;TODO: train&#xA;TODO: eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Hints&lt;/h1&gt; &#xA;&lt;h3&gt;Generate different kinds of masks&lt;/h3&gt; &#xA;&lt;p&gt;The following command will execute a script that generates random masks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash docker/1_generate_masks_from_raw_images.sh \&#xA;    configs/data_gen/random_medium_512.yaml \&#xA;    /directory_with_input_images \&#xA;    /directory_where_to_store_images_and_masks \&#xA;    --ext png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The test data generation command stores images in the format, which is suitable for &lt;a href=&#34;https://raw.githubusercontent.com/saic-mdal/lama/main/#prediction&#34;&gt;prediction&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The table below describes which configs we used to generate different test sets from the paper. Note that we &lt;em&gt;do not fix a random seed&lt;/em&gt;, so the results will be slightly different each time.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Places 512x512&lt;/th&gt; &#xA;   &lt;th&gt;CelebA 256x256&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Narrow&lt;/td&gt; &#xA;   &lt;td&gt;random_thin_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_thin_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;random_medium_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_medium_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wide&lt;/td&gt; &#xA;   &lt;td&gt;random_thick_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_thick_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Feel free to change the config path (argument #1) to any other config in &lt;code&gt;configs/data_gen&lt;/code&gt; or adjust config files themselves.&lt;/p&gt; &#xA;&lt;h3&gt;Override parameters in configs&lt;/h3&gt; &#xA;&lt;p&gt;Also you can override parameters in config like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 bin/train.py -cn &amp;lt;config&amp;gt; data.batch_size=10 run_title=my-title&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where .yaml file extension is omitted&lt;/p&gt; &#xA;&lt;h3&gt;Models options&lt;/h3&gt; &#xA;&lt;p&gt;Config names for models from paper (substitude into the training command):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;* big-lama&#xA;* big-lama-regular&#xA;* lama-fourier&#xA;* lama-regular&#xA;* lama_small_train_masks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which are seated in configs/training/folder&lt;/p&gt; &#xA;&lt;h3&gt;Links&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All the data (models, test images, etc.) &lt;a href=&#34;https://disk.yandex.ru/d/AmdeG-bIjmvSug&#34;&gt;https://disk.yandex.ru/d/AmdeG-bIjmvSug&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Test images from the paper &lt;a href=&#34;https://disk.yandex.ru/d/xKQJZeVRk5vLlQ&#34;&gt;https://disk.yandex.ru/d/xKQJZeVRk5vLlQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The pre-trained models &lt;a href=&#34;https://disk.yandex.ru/d/EgqaSnLohjuzAg&#34;&gt;https://disk.yandex.ru/d/EgqaSnLohjuzAg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The models for perceptual loss &lt;a href=&#34;https://disk.yandex.ru/d/ncVmQlmT_kTemQ&#34;&gt;https://disk.yandex.ru/d/ncVmQlmT_kTemQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Our training logs are available at &lt;a href=&#34;https://disk.yandex.ru/d/9Bt1wNSDS4jDkQ&#34;&gt;https://disk.yandex.ru/d/9Bt1wNSDS4jDkQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training time &amp;amp; resources&lt;/h3&gt; &#xA;&lt;p&gt;TODO&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Segmentation code and models if form &lt;a href=&#34;https://github.com/CSAILVision/semantic-segmentation-pytorch&#34;&gt;CSAILVision&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;LPIPS metric is from &lt;a href=&#34;https://github.com/richzhang/PerceptualSimilarity&#34;&gt;richzhang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SSIM is from &lt;a href=&#34;https://github.com/Po-Hsun-Su/pytorch-ssim&#34;&gt;Po-Hsun-Su&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FID is from &lt;a href=&#34;https://github.com/mseitzer/pytorch-fid&#34;&gt;mseitzer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found this code helpful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{suvorov2021resolution,&#xA;  title={Resolution-robust Large Mask Inpainting with Fourier Convolutions},&#xA;  author={Suvorov, Roman and Logacheva, Elizaveta and Mashikhin, Anton and Remizova, Anastasia and Ashukha, Arsenii and Silvestrov, Aleksei and Kong, Naejin and Goka, Harshith and Park, Kiwoong and Lempitsky, Victor},&#xA;  journal={arXiv preprint arXiv:2109.07161},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div style=&#34;text-align:center&#34; align=&#34;center&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;img loading=&#34;lazy&#34; height=&#34;50px&#34; src=&#34;https://raw.githubusercontent.com/saic-mdal/lama-project/main/docs/img/samsung_ai.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p style=&#34;font-weight:normal; font-size: 16pt;text-align:center&#34; align=&#34;center&#34;&gt;Copyright ¬© 2021&lt;/p&gt; &#xA;&lt;br&gt;</summary>
  </entry>
  <entry>
    <title>nianticlabs/monodepth2</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/nianticlabs/monodepth2</id>
    <link href="https://github.com/nianticlabs/monodepth2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICCV 2019] Monocular depth estimation from a single image&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Monodepth2&lt;/h1&gt; &#xA;&lt;p&gt;This is the reference PyTorch implementation for training and testing depth estimation models using the method described in&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Digging into Self-Supervised Monocular Depth Prediction&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/C.Godard/&#34;&gt;Cl√©ment Godard&lt;/a&gt;, &lt;a href=&#34;http://vision.caltech.edu/~macaodha/&#34;&gt;Oisin Mac Aodha&lt;/a&gt;, &lt;a href=&#34;http://www.michaelfirman.co.uk&#34;&gt;Michael Firman&lt;/a&gt; and &lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/g.brostow/&#34;&gt;Gabriel J. Brostow&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1806.01260&#34;&gt;ICCV 2019 (arXiv pdf)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/nianticlabs/monodepth2/master/assets/teaser.gif&#34; alt=&#34;example input output gif&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This code is for non-commercial use; please see the &lt;a href=&#34;https://raw.githubusercontent.com/nianticlabs/monodepth2/master/LICENSE&#34;&gt;license file&lt;/a&gt; for terms.&lt;/p&gt; &#xA;&lt;p&gt;If you find our work useful in your research please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{monodepth2,&#xA;  title     = {Digging into Self-Supervised Monocular Depth Prediction},&#xA;  author    = {Cl{\&#39;{e}}ment Godard and&#xA;               Oisin {Mac Aodha} and&#xA;               Michael Firman and&#xA;               Gabriel J. Brostow},&#xA;  booktitle = {The International Conference on Computer Vision (ICCV)},&#xA;  month = {October},&#xA;year = {2019}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚öôÔ∏è Setup&lt;/h2&gt; &#xA;&lt;p&gt;Assuming a fresh &lt;a href=&#34;https://www.anaconda.com/download/&#34;&gt;Anaconda&lt;/a&gt; distribution, you can install the dependencies with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda install pytorch=0.4.1 torchvision=0.2.1 -c pytorch&#xA;pip install tensorboardX==1.4&#xA;conda install opencv=3.3.1   # just needed for evaluation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We ran our experiments with PyTorch 0.4.1, CUDA 9.1, Python 3.6.6 and Ubuntu 18.04. We have also successfully trained models with PyTorch 1.0, and our code is compatible with Python 2.7. You may have issues installing OpenCV version 3.3.1 if you use Python 3.7, we recommend to create a virtual environment with Python 3.6.6 &lt;code&gt;conda create -n monodepth2 python=3.6.6 anaconda &lt;/code&gt;.&lt;/p&gt; &#xA;&lt;!-- We recommend using a [conda environment](https://conda.io/docs/user-guide/tasks/manage-environments.html) to avoid dependency conflicts.&#xA;&#xA;We also recommend using `pillow-simd` instead of `pillow` for faster image preprocessing in the dataloaders. --&gt; &#xA;&lt;h2&gt;üñºÔ∏è Prediction for a single image&lt;/h2&gt; &#xA;&lt;p&gt;You can predict scaled disparity for a single image with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python test_simple.py --image_path assets/test_image.jpg --model_name mono+stereo_640x192&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or, if you are using a stereo-trained model, you can estimate metric depth with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python test_simple.py --image_path assets/test_image.jpg --model_name mono+stereo_640x192 --pred_metric_depth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On its first run either of these commands will download the &lt;code&gt;mono+stereo_640x192&lt;/code&gt; pretrained model (99MB) into the &lt;code&gt;models/&lt;/code&gt; folder. We provide the following options for &lt;code&gt;--model_name&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;code&gt;--model_name&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Training modality&lt;/th&gt; &#xA;   &lt;th&gt;Imagenet pretrained?&lt;/th&gt; &#xA;   &lt;th&gt;Model resolution&lt;/th&gt; &#xA;   &lt;th&gt;KITTI abs. rel. error&lt;/th&gt; &#xA;   &lt;th&gt;delta &amp;lt; 1.25&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192.zip&#34;&gt;&lt;code&gt;mono_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mono&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;0.115&lt;/td&gt; &#xA;   &lt;td&gt;0.877&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192.zip&#34;&gt;&lt;code&gt;stereo_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Stereo&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;0.109&lt;/td&gt; &#xA;   &lt;td&gt;0.864&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192.zip&#34;&gt;&lt;code&gt;mono+stereo_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mono + Stereo&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;0.106&lt;/td&gt; &#xA;   &lt;td&gt;0.874&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320.zip&#34;&gt;&lt;code&gt;mono_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mono&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1024 x 320&lt;/td&gt; &#xA;   &lt;td&gt;0.115&lt;/td&gt; &#xA;   &lt;td&gt;0.879&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320.zip&#34;&gt;&lt;code&gt;stereo_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Stereo&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1024 x 320&lt;/td&gt; &#xA;   &lt;td&gt;0.107&lt;/td&gt; &#xA;   &lt;td&gt;0.874&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320.zip&#34;&gt;&lt;code&gt;mono+stereo_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mono + Stereo&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1024 x 320&lt;/td&gt; &#xA;   &lt;td&gt;0.106&lt;/td&gt; &#xA;   &lt;td&gt;0.876&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_no_pt_640x192.zip&#34;&gt;&lt;code&gt;mono_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mono&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;0.132&lt;/td&gt; &#xA;   &lt;td&gt;0.845&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_no_pt_640x192.zip&#34;&gt;&lt;code&gt;stereo_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Stereo&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;0.130&lt;/td&gt; &#xA;   &lt;td&gt;0.831&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_no_pt_640x192.zip&#34;&gt;&lt;code&gt;mono+stereo_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mono + Stereo&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;0.127&lt;/td&gt; &#xA;   &lt;td&gt;0.836&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can also download models trained on the odometry split with &lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_odom_640x192.zip&#34;&gt;monocular&lt;/a&gt; and &lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_odom_640x192.zip&#34;&gt;mono+stereo&lt;/a&gt; training modalities.&lt;/p&gt; &#xA;&lt;p&gt;Finally, we provide resnet 50 depth estimation models trained with &lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_resnet50_640x192.zip&#34;&gt;ImageNet pretrained weights&lt;/a&gt; and &lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_resnet50_no_pt_640x192.zip&#34;&gt;trained from scratch&lt;/a&gt;. Make sure to set &lt;code&gt;--num_layers 50&lt;/code&gt; if using these.&lt;/p&gt; &#xA;&lt;h2&gt;üíæ KITTI training data&lt;/h2&gt; &#xA;&lt;p&gt;You can download the entire &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/raw_data.php&#34;&gt;raw KITTI dataset&lt;/a&gt; by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget -i splits/kitti_archives_to_download.txt -P kitti_data/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then unzip with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd kitti_data&#xA;unzip &#34;*.zip&#34;&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; it weighs about &lt;strong&gt;175GB&lt;/strong&gt;, so make sure you have enough space to unzip too!&lt;/p&gt; &#xA;&lt;p&gt;Our default settings expect that you have converted the png images to jpeg with this command, &lt;strong&gt;which also deletes the raw KITTI &lt;code&gt;.png&lt;/code&gt; files&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;find kitti_data/ -name &#39;*.png&#39; | parallel &#39;convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg &amp;amp;&amp;amp; rm {}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;or&lt;/strong&gt; you can skip this conversion step and train from raw png files by adding the flag &lt;code&gt;--png&lt;/code&gt; when training, at the expense of slower load times.&lt;/p&gt; &#xA;&lt;p&gt;The above conversion command creates images which match our experiments, where KITTI &lt;code&gt;.png&lt;/code&gt; images were converted to &lt;code&gt;.jpg&lt;/code&gt; on Ubuntu 16.04 with default chroma subsampling &lt;code&gt;2x2,1x1,1x1&lt;/code&gt;. We found that Ubuntu 18.04 defaults to &lt;code&gt;2x2,2x2,2x2&lt;/code&gt;, which gives different results, hence the explicit parameter in the conversion command.&lt;/p&gt; &#xA;&lt;p&gt;You can also place the KITTI dataset wherever you like and point towards it with the &lt;code&gt;--data_path&lt;/code&gt; flag during training and evaluation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Splits&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The train/test/validation splits are defined in the &lt;code&gt;splits/&lt;/code&gt; folder. By default, the code will train a depth model using &lt;a href=&#34;https://github.com/tinghuiz/SfMLearner&#34;&gt;Zhou&#39;s subset&lt;/a&gt; of the standard Eigen split of KITTI, which is designed for monocular training. You can also train a model using the new &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction&#34;&gt;benchmark split&lt;/a&gt; or the &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_odometry.php&#34;&gt;odometry split&lt;/a&gt; by setting the &lt;code&gt;--split&lt;/code&gt; flag.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Custom dataset&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can train on a custom monocular or stereo dataset by writing a new dataloader class which inherits from &lt;code&gt;MonoDataset&lt;/code&gt; ‚Äì see the &lt;code&gt;KITTIDataset&lt;/code&gt; class in &lt;code&gt;datasets/kitti_dataset.py&lt;/code&gt; for an example.&lt;/p&gt; &#xA;&lt;h2&gt;‚è≥ Training&lt;/h2&gt; &#xA;&lt;p&gt;By default models and tensorboard event files are saved to &lt;code&gt;~/tmp/&amp;lt;model_name&amp;gt;&lt;/code&gt;. This can be changed with the &lt;code&gt;--log_dir&lt;/code&gt; flag.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Monocular training:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py --model_name mono_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stereo training:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our code defaults to using Zhou&#39;s subsampled Eigen training data. For stereo-only training we have to specify that we want to use the full Eigen training set ‚Äì see paper for details.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py --model_name stereo_model \&#xA;  --frame_ids 0 --use_stereo --split eigen_full&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Monocular + stereo training:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py --model_name mono+stereo_model \&#xA;  --frame_ids 0 -1 1 --use_stereo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;GPUs&lt;/h3&gt; &#xA;&lt;p&gt;The code can only be run on a single GPU. You can specify which GPU to use with the &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=2 python train.py --model_name mono_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All our experiments were performed on a single NVIDIA Titan Xp.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Training modality&lt;/th&gt; &#xA;   &lt;th&gt;Approximate GPU memory&lt;/th&gt; &#xA;   &lt;th&gt;Approximate training time&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mono&lt;/td&gt; &#xA;   &lt;td&gt;9GB&lt;/td&gt; &#xA;   &lt;td&gt;12 hours&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stereo&lt;/td&gt; &#xA;   &lt;td&gt;6GB&lt;/td&gt; &#xA;   &lt;td&gt;8 hours&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mono + Stereo&lt;/td&gt; &#xA;   &lt;td&gt;11GB&lt;/td&gt; &#xA;   &lt;td&gt;15 hours&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;üíΩ Finetuning a pretrained model&lt;/h3&gt; &#xA;&lt;p&gt;Add the following to the training command to load an existing model for finetuning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py --model_name finetuned_mono --load_weights_folder ~/tmp/mono_model/models/weights_19&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üîß Other training options&lt;/h3&gt; &#xA;&lt;p&gt;Run &lt;code&gt;python train.py -h&lt;/code&gt; (or look at &lt;code&gt;options.py&lt;/code&gt;) to see the range of other training options, such as learning rates and ablation settings.&lt;/p&gt; &#xA;&lt;h2&gt;üìä KITTI evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To prepare the ground truth depth maps run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python export_gt_depth.py --data_path kitti_data --split eigen&#xA;python export_gt_depth.py --data_path kitti_data --split eigen_benchmark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...assuming that you have placed the KITTI dataset in the default location of &lt;code&gt;./kitti_data/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The following example command evaluates the epoch 19 weights of a model named &lt;code&gt;mono_model&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python evaluate_depth.py --load_weights_folder ~/tmp/mono_model/models/weights_19/ --eval_mono&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For stereo models, you must use the &lt;code&gt;--eval_stereo&lt;/code&gt; flag (see note below):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python evaluate_depth.py --load_weights_folder ~/tmp/stereo_model/models/weights_19/ --eval_stereo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you train your own model with our code you are likely to see slight differences to the publication results due to randomization in the weights initialization and data loading.&lt;/p&gt; &#xA;&lt;p&gt;An additional parameter &lt;code&gt;--eval_split&lt;/code&gt; can be set. The three different values possible for &lt;code&gt;eval_split&lt;/code&gt; are explained here:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;code&gt;--eval_split&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Test set size&lt;/th&gt; &#xA;   &lt;th&gt;For models trained with...&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;code&gt;eigen&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;697&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;--split eigen_zhou&lt;/code&gt; (default) or &lt;code&gt;--split eigen_full&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The standard Eigen test files&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;code&gt;eigen_benchmark&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;652&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;--split eigen_zhou&lt;/code&gt; (default) or &lt;code&gt;--split eigen_full&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Evaluate with the improved ground truth from the &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction&#34;&gt;new KITTI depth benchmark&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;code&gt;benchmark&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;500&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;--split benchmark&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction&#34;&gt;new KITTI depth benchmark&lt;/a&gt; test files.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Because no ground truth is available for the new KITTI depth benchmark, no scores will be reported when &lt;code&gt;--eval_split benchmark&lt;/code&gt; is set. Instead, a set of &lt;code&gt;.png&lt;/code&gt; images will be saved to disk ready for upload to the evaluation server.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;External disparities evaluation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Finally you can also use &lt;code&gt;evaluate_depth.py&lt;/code&gt; to evaluate raw disparities (or inverse depth) from other methods by using the &lt;code&gt;--ext_disp_to_eval&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python evaluate_depth.py --ext_disp_to_eval ~/other_method_disp.npy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;üì∑üì∑ Note on stereo evaluation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our stereo models are trained with an effective baseline of &lt;code&gt;0.1&lt;/code&gt; units, while the actual KITTI stereo rig has a baseline of &lt;code&gt;0.54m&lt;/code&gt;. This means a scaling of &lt;code&gt;5.4&lt;/code&gt; must be applied for evaluation. In addition, for models trained with stereo supervision we disable median scaling. Setting the &lt;code&gt;--eval_stereo&lt;/code&gt; flag when evaluating will automatically disable median scaling and scale predicted depths by &lt;code&gt;5.4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;‚§¥Ô∏è‚§µÔ∏è Odometry evaluation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We include code for evaluating poses predicted by models trained with &lt;code&gt;--split odom --dataset kitti_odom --data_path /path/to/kitti/odometry/dataset&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For this evaluation, the &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_odometry.php&#34;&gt;KITTI odometry dataset&lt;/a&gt; &lt;strong&gt;(color, 65GB)&lt;/strong&gt; and &lt;strong&gt;ground truth poses&lt;/strong&gt; zip files must be downloaded. As above, we assume that the pngs have been converted to jpgs.&lt;/p&gt; &#xA;&lt;p&gt;If this data has been unzipped to folder &lt;code&gt;kitti_odom&lt;/code&gt;, a model can be evaluated with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python evaluate_pose.py --eval_split odom_9 --load_weights_folder ./odom_split.M/models/weights_29 --data_path kitti_odom/&#xA;python evaluate_pose.py --eval_split odom_10 --load_weights_folder ./odom_split.M/models/weights_29 --data_path kitti_odom/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üì¶ Precomputed results&lt;/h2&gt; &#xA;&lt;p&gt;You can download our precomputed disparity predictions from the following links:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Training modality&lt;/th&gt; &#xA;   &lt;th&gt;Input size&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;.npy&lt;/code&gt; filesize&lt;/th&gt; &#xA;   &lt;th&gt;Eigen disparities&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mono&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;343 MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192_eigen.npy&#34;&gt;Download üîó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stereo&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;343 MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192_eigen.npy&#34;&gt;Download üîó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mono + Stereo&lt;/td&gt; &#xA;   &lt;td&gt;640 x 192&lt;/td&gt; &#xA;   &lt;td&gt;343 MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192_eigen.npy&#34;&gt;Download üîó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mono&lt;/td&gt; &#xA;   &lt;td&gt;1024 x 320&lt;/td&gt; &#xA;   &lt;td&gt;914 MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320_eigen.npy&#34;&gt;Download üîó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stereo&lt;/td&gt; &#xA;   &lt;td&gt;1024 x 320&lt;/td&gt; &#xA;   &lt;td&gt;914 MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320_eigen.npy&#34;&gt;Download üîó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mono + Stereo&lt;/td&gt; &#xA;   &lt;td&gt;1024 x 320&lt;/td&gt; &#xA;   &lt;td&gt;914 MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320_eigen.npy&#34;&gt;Download üîó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üë©‚Äç‚öñÔ∏è License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright ¬© Niantic, Inc. 2019. Patent Pending. All rights reserved. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/nianticlabs/monodepth2/master/LICENSE&#34;&gt;license file&lt;/a&gt; for terms.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Pierian-Data/Complete-Python-3-Bootcamp</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/Pierian-Data/Complete-Python-3-Bootcamp</id>
    <link href="https://github.com/Pierian-Data/Complete-Python-3-Bootcamp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Complete-Python-3-Bootcamp&lt;/h1&gt; &#xA;&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt; &#xA;&lt;p&gt;Copyright(¬©) by Pierian Data Inc.&lt;/p&gt; &#xA;&lt;p&gt;Get it now for 95% off with the link: &lt;a href=&#34;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&#34;&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thanks!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sthalles/SimCLR</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/sthalles/SimCLR</id>
    <link href="https://github.com/sthalles/SimCLR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch implementation of SimCLR: A Simple Framework for Contrastive Learning of Visual Representations&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PyTorch SimCLR: A Simple Framework for Contrastive Learning of Visual Representations&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zenodo.org/badge/latestdoi/241184407&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/241184407.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Blog post with full documentation: &lt;a href=&#34;https://sthalles.github.io/simple-self-supervised-learning/&#34;&gt;Exploring SimCLR: A Simple Framework for Contrastive Learning of Visual Representations&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://sthalles.github.io/assets/contrastive-self-supervised/cover.png&#34; alt=&#34;Image of SimCLR Arch&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;See also &lt;a href=&#34;https://github.com/sthalles/PyTorch-BYOL&#34;&gt;PyTorch Implementation for BYOL - Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning&lt;/a&gt;.&lt;/h3&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda env create --name simclr --file env.yml&#xA;$ conda activate simclr&#xA;$ python run.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Config file&lt;/h2&gt; &#xA;&lt;p&gt;Before running SimCLR, make sure you choose the correct running configurations. You can change the running configurations by passing keyword arguments to the &lt;code&gt;run.py&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;$ python run.py -data ./datasets --dataset-name stl10 --log-every-n-steps 100 --epochs 100 &#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to run it on CPU (for debugging purposes) use the &lt;code&gt;--disable-cuda&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;p&gt;For 16-bit precision GPU training, there &lt;strong&gt;NO&lt;/strong&gt; need to to install &lt;a href=&#34;https://github.com/NVIDIA/apex&#34;&gt;NVIDIA apex&lt;/a&gt;. Just use the &lt;code&gt;--fp16_precision&lt;/code&gt; flag and this implementation will use &lt;a href=&#34;https://pytorch.org/docs/stable/notes/amp_examples.html&#34;&gt;Pytorch built in AMP training&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Feature Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Feature evaluation is done using a linear model protocol.&lt;/p&gt; &#xA;&lt;p&gt;First, we learned features using SimCLR on the &lt;code&gt;STL10 unsupervised&lt;/code&gt; set. Then, we train a linear classifier on top of the frozen features from SimCLR. The linear model is trained on features extracted from the &lt;code&gt;STL10 train&lt;/code&gt; set and evaluated on the &lt;code&gt;STL10 test&lt;/code&gt; set.&lt;/p&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://github.com/sthalles/SimCLR/raw/simclr-refactor/feature_eval/mini_batch_logistic_regression_evaluator.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; notebook for reproducibility.&lt;/p&gt; &#xA;&lt;p&gt;Note that SimCLR benefits from &lt;strong&gt;longer training&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Linear Classification&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Feature Extractor&lt;/th&gt; &#xA;   &lt;th&gt;Architecture&lt;/th&gt; &#xA;   &lt;th&gt;Feature dimensionality&lt;/th&gt; &#xA;   &lt;th&gt;Projection Head dimensionality&lt;/th&gt; &#xA;   &lt;th&gt;Epochs&lt;/th&gt; &#xA;   &lt;th&gt;Top1 %&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Logistic Regression (Adam)&lt;/td&gt; &#xA;   &lt;td&gt;STL10&lt;/td&gt; &#xA;   &lt;td&gt;SimCLR&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/open?id=14_nH2FkyKbt61cieQDiSbBVNP8-gtwgF&#34;&gt;ResNet-18&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;100&lt;/td&gt; &#xA;   &lt;td&gt;74.45&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Logistic Regression (Adam)&lt;/td&gt; &#xA;   &lt;td&gt;CIFAR10&lt;/td&gt; &#xA;   &lt;td&gt;SimCLR&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/open?id=1lc2aoVtrAetGn0PnTkOyFzPCIucOJq7C&#34;&gt;ResNet-18&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;100&lt;/td&gt; &#xA;   &lt;td&gt;69.82&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Logistic Regression (Adam)&lt;/td&gt; &#xA;   &lt;td&gt;STL10&lt;/td&gt; &#xA;   &lt;td&gt;SimCLR&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/open?id=1ByTKAUsdm_X7tLcii6oAEl5qFRqRMZSu&#34;&gt;ResNet-50&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;70.075&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>fastai/fastbook</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/fastai/fastbook</id>
    <link href="https://github.com/fastai/fastbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The fastai book, published as Jupyter Notebooks&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README.md&#34;&gt;English&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_es.md&#34;&gt;Spanish&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_ko.md&#34;&gt;Korean&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_zh.md&#34;&gt;Chinese&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_bn.md&#34;&gt;Bengali&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_id.md&#34;&gt;Indonesian&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_it.md&#34;&gt;Italian&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_pt.md&#34;&gt;Portuguese&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/fastai/fastbook/master/README_vn.md&#34;&gt;Vietnamese&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;The fastai book&lt;/h1&gt; &#xA;&lt;p&gt;These notebooks cover an introduction to deep learning, &lt;a href=&#34;https://docs.fast.ai/&#34;&gt;fastai&lt;/a&gt;, and &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;. fastai is a layered API for deep learning; for more information, see &lt;a href=&#34;https://www.mdpi.com/2078-2489/11/2/108&#34;&gt;the fastai paper&lt;/a&gt;. Everything in this repo is copyright Jeremy Howard and Sylvain Gugger, 2020 onwards.&lt;/p&gt; &#xA;&lt;p&gt;These notebooks are used for &lt;a href=&#34;https://course.fast.ai&#34;&gt;a MOOC&lt;/a&gt; and form the basis of &lt;a href=&#34;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&#34;&gt;this book&lt;/a&gt;, which is currently available for purchase. It does not have the same GPL restrictions that are on this repository.&lt;/p&gt; &#xA;&lt;p&gt;The code in the notebooks and python &lt;code&gt;.py&lt;/code&gt; files is covered by the GPL v3 license; see the LICENSE file for details. The remainder (including all markdown cells in the notebooks and other prose) is not licensed for any redistribution or change of format or medium, other than making copies of the notebooks or forking this repo for your own private use. No commercial or broadcast use is allowed. We are making these materials freely available to help you learn deep learning, so please respect our copyright and these restrictions.&lt;/p&gt; &#xA;&lt;p&gt;If you see someone hosting a copy of these materials somewhere else, please let them know that their actions are not allowed and may lead to legal action. Moreover, they would be hurting the community because we&#39;re not likely to release additional materials in this way if people ignore our copyright.&lt;/p&gt; &#xA;&lt;h2&gt;Colab&lt;/h2&gt; &#xA;&lt;p&gt;Instead of cloning this repo and opening it on your machine, you can read and work with the notebooks using &lt;a href=&#34;https://research.google.com/colaboratory/&#34;&gt;Google Colab&lt;/a&gt;. This is the recommended approach for folks who are just getting started -- there&#39;s no need to set up a Python development environment on your own machine, since you can just work directly in your web-browser.&lt;/p&gt; &#xA;&lt;p&gt;You can open any chapter of the book in Colab by clicking on one of these links: &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/app_jupyter.ipynb&#34;&gt;Introduction to Jupyter&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/01_intro.ipynb&#34;&gt;Chapter 1, Intro&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/02_production.ipynb&#34;&gt;Chapter 2, Production&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/03_ethics.ipynb&#34;&gt;Chapter 3, Ethics&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Chapter 4, MNIST Basics&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb&#34;&gt;Chapter 5, Pet Breeds&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/06_multicat.ipynb&#34;&gt;Chapter 6, Multi-Category&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/07_sizing_and_tta.ipynb&#34;&gt;Chapter 7, Sizing and TTA&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/08_collab.ipynb&#34;&gt;Chapter 8, Collab&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/09_tabular.ipynb&#34;&gt;Chapter 9, Tabular&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/10_nlp.ipynb&#34;&gt;Chapter 10, NLP&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/11_midlevel_data.ipynb&#34;&gt;Chapter 11, Mid-Level API&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/12_nlp_dive.ipynb&#34;&gt;Chapter 12, NLP Deep-Dive&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/13_convolutions.ipynb&#34;&gt;Chapter 13, Convolutions&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/14_resnet.ipynb&#34;&gt;Chapter 14, Resnet&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/15_arch_details.ipynb&#34;&gt;Chapter 15, Arch Details&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/16_accel_sgd.ipynb&#34;&gt;Chapter 16, Optimizers and Callbacks&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/17_foundations.ipynb&#34;&gt;Chapter 17, Foundations&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/18_CAM.ipynb&#34;&gt;Chapter 18, GradCAM&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/19_learner.ipynb&#34;&gt;Chapter 19, Learner&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/20_conclusion.ipynb&#34;&gt;Chapter 20, conclusion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;If you make any pull requests to this repo, then you are assigning copyright of that work to Jeremy Howard and Sylvain Gugger. (Additionally, if you are making small edits to spelling or text, please specify the name of the file and a very brief description of what you&#39;re fixing. It&#39;s difficult for reviewers to know which corrections have already been made. Thank you.)&lt;/p&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you wish to cite the book, you may use the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@book{howard2020deep,&#xA;title={Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD},&#xA;author={Howard, J. and Gugger, S.},&#xA;isbn={9781492045526},&#xA;url={https://books.google.no/books?id=xd6LxgEACAAJ},&#xA;year={2020},&#xA;publisher={O&#39;Reilly Media, Incorporated}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/TensorRT</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/pytorch/TensorRT</id>
    <link href="https://github.com/pytorch/TensorRT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch/TorchScript compiler for NVIDIA GPUs using TensorRT&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Torch-TensorRT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nvidia.github.io/Torch-TensorRT/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-master-brightgreen&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Ahead of Time (AOT) compiling for PyTorch JIT&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Torch-TensorRT is a compiler for PyTorch/TorchScript, targeting NVIDIA GPUs via NVIDIA&#39;s TensorRT Deep Learning Optimizer and Runtime. Unlike PyTorch&#39;s Just-In-Time (JIT) compiler, Torch-TensorRT is an Ahead-of-Time (AOT) compiler, meaning that before you deploy your TorchScript code, you go through an explicit compile step to convert a standard TorchScript program into an module targeting a TensorRT engine. Torch-TensorRT operates as a PyTorch extention and compiles modules that integrate into the JIT runtime seamlessly. After compilation using the optimized graph should feel no different than running a TorchScript module. You also have access to TensorRT&#39;s suite of configurations at compile time, so you are able to specify operating precision (FP32/FP16/INT8) and other settings for your module.&lt;/p&gt; &#xA;&lt;p&gt;Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nvidia.github.io/Torch-TensorRT/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=TU5BMU6iYZ0&amp;amp;ab_channel=NVIDIADeveloper&#34;&gt;Torch-TensorRT Explained in 2 minutes!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/gtcfall21-a31107/&#34;&gt;Comprehensive Discusion (GTC Event)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch&#34;&gt;Pre-built Docker Container&lt;/a&gt;. To use this container, make an NGC account and sign in to NVIDIA&#39;s registry with an API key. Refer to &lt;a href=&#34;https://docs.nvidia.com/ngc/ngc-catalog-user-guide/index.html#registering-activating-ngc-account&#34;&gt;this guide&lt;/a&gt; for the same.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;NVIDIA NGC Container&lt;/h2&gt; &#xA;&lt;p&gt;Torch-TensorRT is distributed in the ready-to-run NVIDIA &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch&#34;&gt;NGC PyTorch Container&lt;/a&gt; starting with 21.11. We recommend using this prebuilt container to experiment &amp;amp; develop with Torch-TensorRT; it has all dependencies with the proper versions as well as example notebooks included.&lt;/p&gt; &#xA;&lt;h2&gt;Building a docker container for Torch-TensorRT&lt;/h2&gt; &#xA;&lt;p&gt;We provide a &lt;code&gt;Dockerfile&lt;/code&gt; in &lt;code&gt;docker/&lt;/code&gt; directory. It expects a PyTorch NGC container as a base but can easily be modified to build on top of any container that provides, PyTorch, CUDA, cuDNN and TensorRT. The dependency libraries in the container can be found in the &lt;a href=&#34;https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/index.html&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please follow this instruction to build a Docker container.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build --build-arg BASE=&amp;lt;CONTAINER VERSION e.g. 21.11&amp;gt; -f docker/Dockerfile -t torch_tensorrt:latest .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the case of building on top of a custom base container, you first must determine the version of the PyTorch C++ ABI. If your source of PyTorch is pytorch.org, likely this is the pre-cxx11-abi in which case you must modify &lt;code&gt;//docker/dist-build.sh&lt;/code&gt; to not build the C++11 ABI version of Torch-TensorRT.&lt;/p&gt; &#xA;&lt;p&gt;You can then build the container using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build --build-arg BASE_IMG=&amp;lt;IMAGE&amp;gt; -f docker/Dockerfile -t torch_tensorrt:latest .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you would like to build outside a docker container, please follow the section &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/TensorRT/master/#compiling-torch-tensorrt&#34;&gt;Compiling Torch-TensorRT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example Usage&lt;/h2&gt; &#xA;&lt;h3&gt;C++&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;torch/script.h&#34;&#xA;#include &#34;torch_tensorrt/torch_tensorrt.h&#34;&#xA;&#xA;...&#xA;// Set input datatypes. Allowerd options torch::{kFloat, kHalf, kChar, kInt32, kBool}&#xA;// Size of input_dtypes should match number of inputs to the network.&#xA;// If input_dtypes is not set, default precision follows traditional PyT / TRT rules&#xA;auto input = torch_tensorrt::Input(dims, torch::kHalf);&#xA;auto compile_settings = torch_tensorrt::ts::CompileSpec({input});&#xA;// FP16 execution&#xA;compile_settings.enabled_precisions = {torch::kHalf};&#xA;// Compile module&#xA;auto trt_mod = torch_tensorrt::ts::compile(ts_mod, compile_settings);&#xA;// Run like normal&#xA;auto results = trt_mod.forward({in_tensor});&#xA;// Save module for later&#xA;trt_mod.save(&#34;trt_torchscript_module.ts&#34;);&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import torch_tensorrt&#xA;&#xA;...&#xA;&#xA;trt_ts_module = torch_tensorrt.compile(torch_script_module,&#xA;    inputs = [example_tensor, # Provide example tensor for input shape or...&#xA;        torch_tensorrt.Input( # Specify input object with shape and dtype&#xA;            min_shape=[1, 3, 224, 224],&#xA;            opt_shape=[1, 3, 512, 512],&#xA;            max_shape=[1, 3, 1024, 1024],&#xA;            # For static size shape=[1, 3, 224, 224]&#xA;            dtype=torch.half) # Datatype of input tensor. Allowed options torch.(float|half|int8|int32|bool)&#xA;    ],&#xA;    enabled_precisions = {torch.half}, # Run with FP16)&#xA;&#xA;result = trt_ts_module(input_data) # run inference&#xA;torch.jit.save(trt_ts_module, &#34;trt_torchscript_module.ts&#34;) # save the TRT embedded Torchscript&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Notes on running in lower precisions:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Enabled lower precisions with compile_spec.enabled_precisions&lt;/li&gt; &#xA;  &lt;li&gt;The module should be left in FP32 before compilation (FP16 can support half tensor models)&lt;/li&gt; &#xA;  &lt;li&gt;Provided input tensors dtype should be the same as module before compilation, regardless of &lt;code&gt;enabled_precisions&lt;/code&gt;. This can be overrided by setting &lt;code&gt;Input::dtype&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Platform Support&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Platform&lt;/th&gt; &#xA;   &lt;th&gt;Support&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux AMD64 / GPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Supported&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux aarch64 / GPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Native Compilation Supported on JetPack-4.4+ (use v1.0.0 for the time being)&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux aarch64 / DLA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Native Compilation Supported on JetPack-4.4+ (use v1.0.0 for the time being)&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows / GPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Unofficial Support&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux ppc64le / GPU&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NGC Containers&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Included in PyTorch NGC Containers 21.11+&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Torch-TensorRT will be included in NVIDIA NGC containers (&lt;a href=&#34;https://ngc.nvidia.com/catalog/containers/nvidia:pytorch&#34;&gt;https://ngc.nvidia.com/catalog/containers/nvidia:pytorch&lt;/a&gt;) starting in 21.11.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: Refer NVIDIA NGC container(&lt;a href=&#34;https://ngc.nvidia.com/catalog/containers/nvidia:l4t-pytorch&#34;&gt;https://ngc.nvidia.com/catalog/containers/nvidia:l4t-pytorch&lt;/a&gt;) for PyTorch libraries on JetPack.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;These are the following dependencies used to verify the testcases. Torch-TensorRT can work with other versions, but the tests are not guaranteed to pass.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bazel 5.1.1&lt;/li&gt; &#xA; &lt;li&gt;Libtorch 1.11.0 (built with CUDA 11.3)&lt;/li&gt; &#xA; &lt;li&gt;CUDA 11.3&lt;/li&gt; &#xA; &lt;li&gt;cuDNN 8.2.1&lt;/li&gt; &#xA; &lt;li&gt;TensorRT 8.2.4.2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prebuilt Binaries and Wheel files&lt;/h2&gt; &#xA;&lt;p&gt;Releases: &lt;a href=&#34;https://github.com/NVIDIA/Torch-TensorRT/releases&#34;&gt;https://github.com/NVIDIA/Torch-TensorRT/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Compiling Torch-TensorRT&lt;/h2&gt; &#xA;&lt;h3&gt;Installing Dependencies&lt;/h3&gt; &#xA;&lt;h4&gt;0. Install Bazel&lt;/h4&gt; &#xA;&lt;p&gt;If you don&#39;t have bazel installed, the easiest way is to install bazelisk using the method of you choosing &lt;a href=&#34;https://github.com/bazelbuild/bazelisk&#34;&gt;https://github.com/bazelbuild/bazelisk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Otherwise you can use the following instructions to install binaries &lt;a href=&#34;https://docs.bazel.build/versions/master/install.html&#34;&gt;https://docs.bazel.build/versions/master/install.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Finally if you need to compile from source (e.g. aarch64 until bazel distributes binaries for the architecture) you can use these instructions&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export BAZEL_VERSION=&amp;lt;VERSION&amp;gt;&#xA;mkdir bazel&#xA;cd bazel&#xA;curl -fSsL -O https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-dist.zip&#xA;unzip bazel-$BAZEL_VERSION-dist.zip&#xA;bash ./compile.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You need to start by having CUDA installed on the system, LibTorch will automatically be pulled for you by bazel, then you have two options.&lt;/p&gt; &#xA;&lt;h4&gt;1. Building using cuDNN &amp;amp; TensorRT tarball distributions&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This is recommended so as to build Torch-TensorRT hermetically and insures any bugs are not caused by version issues&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Make sure when running Torch-TensorRT that these versions of the libraries are prioritized in your &lt;code&gt;$LD_LIBRARY_PATH&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You need to download the tarball distributions of TensorRT and cuDNN from the NVIDIA website. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;https://developer.nvidia.com/cudnn&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/tensorrt&#34;&gt;https://developer.nvidia.com/tensorrt&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Place these files in a directory (the directories &lt;code&gt;third_party/dist_dir/[x86_64-linux-gnu | aarch64-linux-gnu]&lt;/code&gt; exist for this purpose)&lt;/li&gt; &#xA; &lt;li&gt;Compile using:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bazel build //:libtorchtrt --compilation_mode opt --distdir third_party/dist_dir/[x86_64-linux-gnu | aarch64-linux-gnu]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Building using locally installed cuDNN &amp;amp; TensorRT&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you find bugs and you compiled using this method please disclose you used this method in the issue (an &lt;code&gt;ldd&lt;/code&gt; dump would be nice too)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install TensorRT, CUDA and cuDNN on the system before starting to compile.&lt;/li&gt; &#xA; &lt;li&gt;In &lt;code&gt;WORKSPACE&lt;/code&gt; comment out&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;# Downloaded distributions to use with --distdir&#xA;http_archive(&#xA;    name = &#34;cudnn&#34;,&#xA;    urls = [&#34;&amp;lt;URL&amp;gt;&#34;,],&#xA;&#xA;    build_file = &#34;@//third_party/cudnn/archive:BUILD&#34;,&#xA;    sha256 = &#34;&amp;lt;TAR SHA256&amp;gt;&#34;,&#xA;    strip_prefix = &#34;cuda&#34;&#xA;)&#xA;&#xA;http_archive(&#xA;    name = &#34;tensorrt&#34;,&#xA;    urls = [&#34;&amp;lt;URL&amp;gt;&#34;,],&#xA;&#xA;    build_file = &#34;@//third_party/tensorrt/archive:BUILD&#34;,&#xA;    sha256 = &#34;&amp;lt;TAR SHA256&amp;gt;&#34;,&#xA;    strip_prefix = &#34;TensorRT-&amp;lt;VERSION&amp;gt;&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and uncomment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;# Locally installed dependencies&#xA;new_local_repository(&#xA;    name = &#34;cudnn&#34;,&#xA;    path = &#34;/usr/&#34;,&#xA;    build_file = &#34;@//third_party/cudnn/local:BUILD&#34;&#xA;)&#xA;&#xA;new_local_repository(&#xA;   name = &#34;tensorrt&#34;,&#xA;   path = &#34;/usr/&#34;,&#xA;   build_file = &#34;@//third_party/tensorrt/local:BUILD&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Compile using:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bazel build //:libtorchtrt --compilation_mode opt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Debug build&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bazel build //:libtorchtrt --compilation_mode=dbg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Native compilation on NVIDIA Jetson AGX&lt;/h3&gt; &#xA;&lt;p&gt;We performed end to end testing on Jetson platform using Jetpack SDK 4.6.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bazel build //:libtorchtrt --platforms //toolchains:jetpack_4.6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: Please refer &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/TensorRT/master/docs/tutorials/installation.html&#34;&gt;installation&lt;/a&gt; instructions for Pre-requisites&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;A tarball with the include files and library can then be found in bazel-bin&lt;/p&gt; &#xA;&lt;h3&gt;Running Torch-TensorRT on a JIT Graph&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Make sure to add LibTorch to your LD_LIBRARY_PATH &lt;br&gt; &lt;code&gt;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pwd)/bazel-Torch-TensorRT/external/libtorch/lib&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bazel run //cpp/bin/torchtrtc -- $(realpath &amp;lt;PATH TO GRAPH&amp;gt;) out.ts &amp;lt;input-size&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Compiling the Python Package&lt;/h2&gt; &#xA;&lt;p&gt;To compile the python package for your local machine, just run &lt;code&gt;python3 setup.py install&lt;/code&gt; in the &lt;code&gt;//py&lt;/code&gt; directory. To build wheel files for different python versions, first build the Dockerfile in &lt;code&gt;//py&lt;/code&gt; then run the following command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -it -v$(pwd)/..:/workspace/Torch-TensorRT build_torch_tensorrt_wheel /bin/bash /workspace/Torch-TensorRT/py/build_whl.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Python compilation expects using the tarball based compilation strategy from above.&lt;/p&gt; &#xA;&lt;h2&gt;Testing using Python backend&lt;/h2&gt; &#xA;&lt;p&gt;Torch-TensorRT supports testing in Python using &lt;a href=&#34;https://nox.thea.codes/en/stable&#34;&gt;nox&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To install the nox using python-pip&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m pip install --upgrade nox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To list supported nox sessions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;nox --session -l&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Environment variables supported by nox&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;PYT_PATH          - To use different PYTHONPATH than system installed Python packages&#xA;TOP_DIR           - To set the root directory of the noxfile&#xA;USE_CXX11         - To use cxx11_abi (Defaults to 0)&#xA;USE_HOST_DEPS     - To use host dependencies for tests (Defaults to 0)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage example&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;nox --session l0_api_tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Supported Python versions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[&#34;3.7&#34;, &#34;3.8&#34;, &#34;3.9&#34;, &#34;3.10&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How do I add support for a new op...&lt;/h2&gt; &#xA;&lt;h3&gt;In Torch-TensorRT?&lt;/h3&gt; &#xA;&lt;p&gt;Thanks for wanting to contribute! There are two main ways to handle supporting a new op. Either you can write a converter for the op from scratch and register it in the NodeConverterRegistry or if you can map the op to a set of ops that already have converters you can write a graph rewrite pass which will replace your new op with an equivalent subgraph of supported ops. Its preferred to use graph rewriting because then we do not need to maintain a large library of op converters. Also do look at the various op support trackers in the &lt;a href=&#34;https://github.com/NVIDIA/Torch-TensorRT/issues&#34;&gt;issues&lt;/a&gt; for information on the support status of various operators.&lt;/p&gt; &#xA;&lt;h3&gt;In my application?&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The Node Converter Registry is not exposed in the top level API but in the internal headers shipped with the tarball.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;You can register a converter for your op using the &lt;code&gt;NodeConverterRegistry&lt;/code&gt; inside your application.&lt;/p&gt; &#xA;&lt;h2&gt;Structure of the repo&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/TensorRT/master/core&#34;&gt;&lt;strong&gt;core&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Main JIT ingest, lowering, conversion and runtime implementations&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/TensorRT/master/cpp&#34;&gt;&lt;strong&gt;cpp&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;C++ API and CLI source&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/TensorRT/master/examples&#34;&gt;&lt;strong&gt;examples&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Example applications to show different features of Torch-TensorRT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/TensorRT/master/py&#34;&gt;&lt;strong&gt;py&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Python API for Torch-TensorRT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/TensorRT/master/tests&#34;&gt;&lt;strong&gt;tests&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Unit tests for Torch-TensorRT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Take a look at the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/TensorRT/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The Torch-TensorRT license can be found in the LICENSE file. It is licensed with a BSD Style licence&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>codebasics/py</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/codebasics/py</id>
    <link href="https://github.com/codebasics/py" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repository to store sample python programs for python learning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;py&lt;/h1&gt; &#xA;&lt;p&gt;Repository to store sample Python programs.&lt;/p&gt; &#xA;&lt;p&gt;This repository is meant for beginners to assist them in their learning of Python. The repository covers a wide range of algorithms and other programs, and would prove immensely helpful for everybody interested in Python programming.&lt;/p&gt; &#xA;&lt;p&gt;If this is your first time coding in Python, I would love to suggest you begin from the &lt;a href=&#34;https://github.com/codebasics/py/tree/master/Basics&#34;&gt;Basics&lt;/a&gt;. They are simple to understand and hopefully will prove fun to you.&lt;/p&gt; &#xA;&lt;p&gt;You can also pay a visit to my very own &lt;a href=&#34;https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ&#34;&gt;Youtube channel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Contributions to the repository are welcome.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ&#34;&gt;&lt;img src=&#34;https://yt3.ggpht.com/ytc/AAUvwnihwx4a5idwBTE5JFpXHb-ykyh-i1gXtFiGJYV1=s176-c-k-c0x00ffffff-no-rj&#34; alt=&#34;CodeBasics&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Happy coding!&lt;/h4&gt;</summary>
  </entry>
  <entry>
    <title>openvinotoolkit/anomalib</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/openvinotoolkit/anomalib</id>
    <link href="https://github.com/openvinotoolkit/anomalib" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An anomaly detection library comprising state-of-the-art algorithms and features such as experiment management, hyper-parameter optimization, and edge inference.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/docs/source/images/logos/anomalib-wide-blue.png&#34; width=&#34;600px&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;A library for benchmarking, developing and deploying deep learning anomaly detection algorithms&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/#key-features&#34;&gt;Key Features&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/#getting-started&#34;&gt;Getting Started&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://openvinotoolkit.github.io/anomalib&#34;&gt;Docs&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://github.com/openvinotoolkit/anomalib/raw/development/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7%2B-green&#34; alt=&#34;python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pytorch-1.8.1%2B-orange&#34; alt=&#34;pytorch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/openvino-2021.4.2-purple&#34; alt=&#34;openvino&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/openvinotoolkit/anomalib/actions/workflows/nightly.yml&#34;&gt;&lt;img src=&#34;https://github.com/openvinotoolkit/anomalib/actions/workflows/nightly.yml/badge.svg?sanitize=true&#34; alt=&#34;Nightly-regression Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/openvinotoolkit/anomalib/actions/workflows/pre_merge.yml&#34;&gt;&lt;img src=&#34;https://github.com/openvinotoolkit/anomalib/actions/workflows/pre_merge.yml/badge.svg?sanitize=true&#34; alt=&#34;Pre-merge Checks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/openvinotoolkit/anomalib/actions/workflows/docs.yml&#34;&gt;&lt;img src=&#34;https://github.com/openvinotoolkit/anomalib/actions/workflows/docs.yml/badge.svg?sanitize=true&#34; alt=&#34;Build Docs&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Anomalib is a deep learning library that aims to collect state-of-the-art anomaly detection algorithms for benchmarking on both public and private datasets. Anomalib provides several ready-to-use implementations of anomaly detection algorithms described in the recent literature, as well as a set of tools that facilitate the development and implementation of custom models. The library has a strong focus on image-based anomaly detection, where the goal of the algorithm is to identify anomalous images, or anomalous pixel regions within images in a dataset. Anomalib is constantly updated with new algorithms and training/inference extensions, so keep checking!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/docs/source/images/readme.png&#34; alt=&#34;Sample Image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The largest public collection of ready-to-use deep learning anomaly detection algorithms and benchmark datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pytorchlightning.ai/&#34;&gt;&lt;strong&gt;PyTorch Lightning&lt;/strong&gt;&lt;/a&gt; based model implementations to reduce boilerplate code and limit the implementation efforts to the bare essentials.&lt;/li&gt; &#xA; &lt;li&gt;All models can be exported to &lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html&#34;&gt;&lt;strong&gt;OpenVINO&lt;/strong&gt;&lt;/a&gt; Intermediate Representation (IR) for accelerated inference on intel hardware.&lt;/li&gt; &#xA; &lt;li&gt;A set of &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/#inference&#34;&gt;inference tools&lt;/a&gt; for quick and easy deployment of the standard or custom anomaly detection models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To get an overview of all the devices where &lt;code&gt;anomalib&lt;/code&gt; as been tested thoroughly, look at the &lt;a href=&#34;https://openvinotoolkit.github.io/anomalib/#supported-hardware&#34;&gt;Supported Hardware&lt;/a&gt; section in the documentation.&lt;/p&gt; &#xA;&lt;h3&gt;Jupyter Notebooks&lt;/h3&gt; &#xA;&lt;p&gt;For getting started with a Jupyter Notebook, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/notebooks&#34;&gt;Notebooks&lt;/a&gt; folder of this repository. Additionally, you can refer to a few created by the community:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1K4a4z2iZGBNhWdmt9Aqdld7kTAxBfAmi?usp=sharing&#34;&gt;Google Colab&lt;/a&gt; by @bth5&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/code/ipythonx/mvtec-ad-anomaly-detection-with-anomalib-library&#34;&gt;Kaggle&lt;/a&gt; by @innat&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;PyPI Install&lt;/h3&gt; &#xA;&lt;p&gt;You can get started with &lt;code&gt;anomalib&lt;/code&gt; by just using pip.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install anomalib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Local Install&lt;/h3&gt; &#xA;&lt;p&gt;It is highly recommended to use virtual environment when installing anomalib. For instance, with &lt;a href=&#34;https://www.anaconda.com/products/individual&#34;&gt;anaconda&lt;/a&gt;, &lt;code&gt;anomalib&lt;/code&gt; could be installed as,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yes | conda create -n anomalib_env python=3.8&#xA;conda activate anomalib_env&#xA;git clone https://github.com/openvinotoolkit/anomalib.git&#xA;cd anomalib&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;By default &lt;a href=&#34;https://gitlab-icv.inn.intel.com/algo_rnd_team/anomaly/-/blob/development/train.py&#34;&gt;&lt;code&gt;python tools/train.py&lt;/code&gt;&lt;/a&gt; runs &lt;a href=&#34;https://arxiv.org/abs/2011.08785&#34;&gt;PADIM&lt;/a&gt; model on &lt;code&gt;leather&lt;/code&gt; category from the &lt;a href=&#34;https://www.mvtec.com/company/research/datasets/mvtec-ad&#34;&gt;MVTec AD&lt;/a&gt; &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;(CC BY-NC-SA 4.0)&lt;/a&gt; dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/train.py    # Train PADIM on MVTec AD leather&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training a model on a specific dataset and category requires further configuration. Each model has its own configuration file, &lt;a href=&#34;https://gitlab-icv.inn.intel.com/algo_rnd_team/anomaly/-/blob/development/padim/anomalib/models/padim/config.yaml&#34;&gt;&lt;code&gt;config.yaml&lt;/code&gt;&lt;/a&gt; , which contains data, model and training configurable parameters. To train a specific model on a specific dataset and category, the config file is to be provided:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/train.py --config &amp;lt;path/to/model/config.yaml&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example, to train &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/anomalib/models/padim&#34;&gt;PADIM&lt;/a&gt; you can use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/train.py --config anomalib/models/padim/config.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;--model_config_path&lt;/code&gt; will be deprecated in &lt;code&gt;v0.2.8&lt;/code&gt; and removed in &lt;code&gt;v0.2.9&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, a model name could also be provided as an argument, where the scripts automatically finds the corresponding config file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/train.py --model padim&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where the currently available models are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/anomalib/models/cflow&#34;&gt;CFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/anomalib/models/patchcore&#34;&gt;PatchCore&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/anomalib/models/padim&#34;&gt;PADIM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/anomalib/models/stfpm&#34;&gt;STFPM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/anomalib/models/dfm&#34;&gt;DFM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/anomalib/models/dfkde&#34;&gt;DFKDE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/anomalib/models/ganomaly&#34;&gt;GANomaly&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Custom Dataset&lt;/h3&gt; &#xA;&lt;p&gt;It is also possible to train on a custom folder dataset. To do so, &lt;code&gt;data&lt;/code&gt; section in &lt;code&gt;config.yaml&lt;/code&gt; is to be modified as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;dataset:&#xA;  name: &amp;lt;name-of-the-dataset&amp;gt;&#xA;  format: folder&#xA;  path: &amp;lt;path/to/folder/dataset&amp;gt;&#xA;  normal_dir: normal # name of the folder containing normal images.&#xA;  abnormal_dir: abnormal # name of the folder containing abnormal images.&#xA;  normal_test_dir: null # name of the folder containing normal test images.&#xA;  task: segmentation # classification or segmentation&#xA;  mask: &amp;lt;path/to/mask/annotations&amp;gt; #optional&#xA;  extensions: null&#xA;  split_ratio: 0.2  # ratio of the normal images that will be used to create a test split&#xA;  image_size: 256&#xA;  train_batch_size: 32&#xA;  test_batch_size: 32&#xA;  num_workers: 8&#xA;  transform_config:&#xA;    train: null&#xA;    val: null&#xA;  create_validation_set: true&#xA;  tiling:&#xA;    apply: false&#xA;    tile_size: null&#xA;    stride: null&#xA;    remove_border_count: 0&#xA;    use_random_tiling: False&#xA;    random_tile_count: 16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Anomalib contains several tools that can be used to perform inference with a trained model. The script in &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/tools/inference.py&#34;&gt;&lt;code&gt;tools/inference&lt;/code&gt;&lt;/a&gt; contains an example of how the inference tools can be used to generate a prediction for an input image.&lt;/p&gt; &#xA;&lt;p&gt;If the specified weight path points to a PyTorch Lightning checkpoint file (&lt;code&gt;.ckpt&lt;/code&gt;), inference will run in PyTorch. If the path points to an ONNX graph (&lt;code&gt;.onnx&lt;/code&gt;) or OpenVINO IR (&lt;code&gt;.bin&lt;/code&gt; or &lt;code&gt;.xml&lt;/code&gt;), inference will run in OpenVINO.&lt;/p&gt; &#xA;&lt;p&gt;The following command can be used to run inference from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/inference.py \&#xA;    --config &amp;lt;path/to/model/config.yaml&amp;gt; \&#xA;    --weight_path &amp;lt;path/to/weight/file&amp;gt; \&#xA;    --image_path &amp;lt;path/to/image&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a quick example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/inference.py \&#xA;    --config anomalib/models/padim/config.yaml \&#xA;    --weight_path results/padim/mvtec/bottle/weights/model.ckpt \&#xA;    --image_path datasets/MVTec/bottle/test/broken_large/000.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to run OpenVINO model, ensure that &lt;code&gt;openvino&lt;/code&gt; &lt;code&gt;apply&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt; in the respective model &lt;code&gt;config.yaml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;optimization:&#xA;  openvino:&#xA;    apply: true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example OpenVINO Inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/inference.py \&#xA;    --config  \&#xA;    anomalib/models/padim/config.yaml  \&#xA;    --weight_path  \&#xA;    results/padim/mvtec/bottle/compressed/compressed_model.xml  \&#xA;    --image_path  \&#xA;    datasets/MVTec/bottle/test/broken_large/000.png  \&#xA;    --meta_data  \&#xA;    results/padim/mvtec/bottle/compressed/meta_data.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Ensure that you provide path to &lt;code&gt;meta_data.json&lt;/code&gt; if you want the normalization to be applied correctly.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;anomalib&lt;/code&gt; supports MVTec AD &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;(CC BY-NC-SA 4.0)&lt;/a&gt; and BeanTech &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/4.0/legalcode&#34;&gt;(CC-BY-SA)&lt;/a&gt; for benchmarking and &lt;code&gt;folder&lt;/code&gt; for custom dataset training/inference.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.mvtec.com/company/research/datasets/mvtec-ad&#34;&gt;MVTec AD Dataset&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;MVTec AD dataset is one of the main benchmarks for anomaly detection, and is released under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;(CC BY-NC-SA 4.0)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Image-Level AUC&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Avg&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Carpet&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Grid&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Leather&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Tile&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Wood&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Bottle&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cable&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Capsule&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Hazelnut&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Metal Nut&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Pill&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Screw&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Toothbrush&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Transistor&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Zipper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;PatchCore&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Wide ResNet-50&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.980&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.984&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.959&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.000&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.000&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.989&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.000&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.990&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.982&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.000&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.994&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.924&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.960&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.933&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.000&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.982&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PatchCore&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.973&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.970&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.947&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.000&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.997&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.997&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.000&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.986&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.965&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.000&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.991&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.916&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.943&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.931&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.996&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.953&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CFlow&lt;/td&gt; &#xA;   &lt;td&gt;Wide ResNet-50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.962&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.986&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.962&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.999&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.993&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.893&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.945&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.995&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.924&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.908&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.897&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.943&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.984&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PaDiM&lt;/td&gt; &#xA;   &lt;td&gt;Wide ResNet-50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.950&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.995&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.942&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.974&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.993&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.999&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.878&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.927&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.964&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.989&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.939&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.845&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.942&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.976&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.882&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PaDiM&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.891&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.945&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.857&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.982&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.950&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.976&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.994&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.844&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.901&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.750&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.961&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.863&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.759&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.889&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.920&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.780&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;STFPM&lt;/td&gt; &#xA;   &lt;td&gt;Wide ResNet-50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.876&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.957&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.977&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.981&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.976&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.939&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.987&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.878&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.732&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.995&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.973&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.652&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.825&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.875&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.899&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;STFPM&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.893&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.954&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.982&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.989&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.949&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.961&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.979&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.838&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.759&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.999&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.956&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.705&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.835&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.997&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.853&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.645&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DFM&lt;/td&gt; &#xA;   &lt;td&gt;Wide ResNet-50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.891&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.978&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.540&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.979&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.977&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.974&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.990&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.891&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.931&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.947&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.839&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.809&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.700&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.911&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.915&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.981&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DFM&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.894&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.864&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.558&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.945&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.984&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.946&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.994&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.913&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.871&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.979&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.941&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.838&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.761&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.911&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.949&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DFKDE&lt;/td&gt; &#xA;   &lt;td&gt;Wide ResNet-50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.774&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.708&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.422&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.905&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.959&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.903&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.936&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.746&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.853&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.736&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.687&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.749&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.574&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.697&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.843&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.892&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DFKDE&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.762&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.646&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.577&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.669&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.965&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.863&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.951&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.751&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.698&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.806&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.729&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.607&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.694&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.767&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.839&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.866&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GANomaly&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.421&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.203&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.404&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.413&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.408&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.744&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.251&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.457&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.682&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.537&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.270&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.472&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.231&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.372&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.440&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.434&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pixel-Level AUC&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Avg&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Carpet&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Grid&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Leather&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Tile&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Wood&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Bottle&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cable&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Capsule&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Hazelnut&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Metal Nut&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Pill&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Screw&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Toothbrush&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Transistor&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Zipper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;PatchCore&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Wide ResNet-50&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.980&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.988&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.968&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.991&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.961&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.934&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.984&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.988&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.988&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.987&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.989&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.980&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.989&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.988&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.981&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.983&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PatchCore&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.976&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.986&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.955&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.990&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.943&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.933&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.981&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.984&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.986&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.986&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.986&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.974&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.991&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.988&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.974&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.983&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CFlow&lt;/td&gt; &#xA;   &lt;td&gt;Wide ResNet-50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.971&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.986&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.968&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.993&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.968&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.924&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.981&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.955&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.988&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.990&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.982&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.983&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.979&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.985&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.897&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.980&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PaDiM&lt;/td&gt; &#xA;   &lt;td&gt;Wide ResNet-50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.979&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.991&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.970&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.993&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.955&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.957&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.985&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.970&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.988&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.985&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.982&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.966&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.988&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.991&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.976&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.986&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PaDiM&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.968&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.984&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.918&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.994&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.934&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.947&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.983&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.965&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.984&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.978&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.970&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.957&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.978&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.988&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.968&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.979&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;STFPM&lt;/td&gt; &#xA;   &lt;td&gt;Wide ResNet-50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.903&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.987&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.989&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.980&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.966&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.956&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.966&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.913&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.956&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.974&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.961&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.946&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.988&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.178&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.807&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.980&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;STFPM&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.951&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.986&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.988&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.991&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.946&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.949&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.971&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.898&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.962&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.981&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.942&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.878&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.983&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.983&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.838&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.972&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Image F1 Score&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Avg&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Carpet&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Grid&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Leather&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Tile&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Wood&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Bottle&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cable&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Capsule&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Hazelnut&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Metal Nut&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Pill&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Screw&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Toothbrush&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Transistor&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Zipper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;PatchCore&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Wide ResNet-50&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.976&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.971&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.974&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.000&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.000&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.967&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.000&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.968&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.982&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.000&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.984&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.940&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.943&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.938&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.000&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.979&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PatchCore&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.970&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.949&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.946&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.000&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.992&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.000&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.978&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.969&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.000&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.989&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.940&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.932&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.935&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.974&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.967&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CFlow&lt;/td&gt; &#xA;   &lt;td&gt;Wide ResNet-50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.944&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.972&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.932&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.988&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.967&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.832&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.939&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.979&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.924&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.971&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.870&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.818&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.967&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PaDiM&lt;/td&gt; &#xA;   &lt;td&gt;Wide ResNet-50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.951&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.989&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.930&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.960&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.983&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.992&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.856&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.982&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.937&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.978&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.946&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.895&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.952&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.914&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.947&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PaDiM&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.916&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.930&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.893&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.984&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.934&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.952&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.976&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.858&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.960&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.836&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.974&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.932&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.879&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.923&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.796&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.915&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;STFPM&lt;/td&gt; &#xA;   &lt;td&gt;Wide ResNet-50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.926&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.973&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.973&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.974&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.965&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.929&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.976&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.853&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.920&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.972&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.974&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.922&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.884&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.833&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.815&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.931&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;STFPM&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.932&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.961&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.982&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.989&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.930&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.951&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.984&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.819&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.918&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.993&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.973&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.918&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.887&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;0.984&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.790&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.908&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DFM&lt;/td&gt; &#xA;   &lt;td&gt;Wide ResNet-50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.918&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.960&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.844&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.990&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.970&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.959&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.976&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.848&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.944&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.913&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.912&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.919&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.859&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.893&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.815&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.961&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DFM&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.919&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.895&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.844&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.926&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.971&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.948&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.977&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.874&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.935&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.957&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.958&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.921&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.874&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.933&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.833&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.943&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DFKDE&lt;/td&gt; &#xA;   &lt;td&gt;Wide ResNet-50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.875&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.907&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.844&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.905&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.945&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.914&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.946&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.790&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.914&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.817&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.894&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.922&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.855&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.845&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.722&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.910&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DFKDE&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.872&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.864&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.844&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.854&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.960&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.898&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.942&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.793&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.908&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.827&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.894&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.916&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.859&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.853&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.756&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.916&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GANomaly&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.834&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.864&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.844&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.852&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.836&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.863&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.863&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.760&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.905&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.777&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.894&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.916&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.853&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.833&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.571&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.881&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;p&gt;If you use this library and love it, use this to cite it ü§ó&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{anomalib,&#xA;      title={Anomalib: A Deep Learning Library for Anomaly Detection},&#xA;      author={Samet Akcay and&#xA;              Dick Ameln and&#xA;              Ashwin Vaidya and&#xA;              Barath Lakshmanan and&#xA;              Nilesh Ahuja and&#xA;              Utku Genc},&#xA;      year={2022},&#xA;      eprint={2202.08341},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>DataTalksClub/data-engineering-zoomcamp</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/DataTalksClub/data-engineering-zoomcamp</id>
    <link href="https://github.com/DataTalksClub/data-engineering-zoomcamp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free Data Engineering course!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Data Engineering Zoomcamp&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Register in &lt;a href=&#34;https://datatalks.club/slack.html&#34;&gt;DataTalks.Club&#39;s Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Join the &lt;a href=&#34;https://app.slack.com/client/T01ATQK62F8/C01FABYF2RG&#34;&gt;&lt;code&gt;#course-data-engineering&lt;/code&gt;&lt;/a&gt; channel&lt;/li&gt; &#xA; &lt;li&gt;The videos are published to &lt;a href=&#34;https://www.youtube.com/c/DataTalksClub&#34;&gt;DataTalks.Club&#39;s YouTube channel&lt;/a&gt; in &lt;a href=&#34;https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&#34;&gt;the course playlist&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing&#34;&gt;Frequenty asked technical questions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Syllabus&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-1-introduction--prerequisites&#34;&gt;Week 1: Introduction &amp;amp; Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-2-data-ingestion&#34;&gt;Week 2: Data ingestion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-3-data-warehouse&#34;&gt;Week 3: Data Warehouse&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-4-analytics-engineering&#34;&gt;Week 4: Analytics Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-5-batch-processing&#34;&gt;Week 5: Batch processing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-6-streaming&#34;&gt;Week 6: Streaming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/#week-7-8--9-project&#34;&gt;Week 7, 8 &amp;amp; 9: Project&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Taking the course&lt;/h2&gt; &#xA;&lt;h3&gt;Self-paced mode&lt;/h3&gt; &#xA;&lt;p&gt;All the materials of the course are freely available, so you can take the course at your own pace&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow the suggested syllabus (see below) week by week&lt;/li&gt; &#xA; &lt;li&gt;You don&#39;t need to fill in the registration form. Just start watching the videos and join Slack&lt;/li&gt; &#xA; &lt;li&gt;Check &lt;a href=&#34;https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing&#34;&gt;FAQ&lt;/a&gt; if you have problems&lt;/li&gt; &#xA; &lt;li&gt;If you can&#39;t find a solution to your problem in FAQ, ask for help in Slack&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2022 Cohort&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Start&lt;/strong&gt;: 17 January 2022&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Registration link&lt;/strong&gt;: &lt;a href=&#34;https://airtable.com/shr6oVXeQvSI5HuWD&#34;&gt;https://airtable.com/shr6oVXeQvSI5HuWD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vR9oQiYnAVvzL4dagnhvp0sngqagF0AceD0FGjhS-dnzMTBzNQIal3-hOgkTibVQvfuqbQ69b0fvRnf/pubhtml&#34;&gt;Leaderboard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Subscribe to our &lt;a href=&#34;https://calendar.google.com/calendar/?cid=ZXIxcjA1M3ZlYjJpcXU0dTFmaG02MzVxMG9AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&#34;&gt;public Google Calendar&lt;/a&gt; (it works from Desktop only)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Asking for help in Slack&lt;/h3&gt; &#xA;&lt;p&gt;The best way to get support is to use &lt;a href=&#34;https://datatalks.club/slack.html&#34;&gt;DataTalks.Club&#39;s Slack&lt;/a&gt;. Join the &lt;a href=&#34;https://app.slack.com/client/T01ATQK62F8/C01FABYF2RG&#34;&gt;&lt;code&gt;#course-data-engineering&lt;/code&gt;&lt;/a&gt; channel.&lt;/p&gt; &#xA;&lt;p&gt;To make discussions in Slack more organized:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow &lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/asking-questions.md&#34;&gt;these recommendations&lt;/a&gt; when asking for help&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://datatalks.club/slack/guidelines.html&#34;&gt;DataTalks.Club community guidelines&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Syllabus&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_1_basics_n_setup&#34;&gt;Week 1: Introduction &amp;amp; Prerequisites&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Course overview&lt;/li&gt; &#xA; &lt;li&gt;Introduction to GCP&lt;/li&gt; &#xA; &lt;li&gt;Docker and docker-compose&lt;/li&gt; &#xA; &lt;li&gt;Running Postgres locally with Docker&lt;/li&gt; &#xA; &lt;li&gt;Setting up infrastructure on GCP with Terraform&lt;/li&gt; &#xA; &lt;li&gt;Preparing the environment for the course&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_1_basics_n_setup&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_2_data_ingestion&#34;&gt;Week 2: Data ingestion&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data Lake&lt;/li&gt; &#xA; &lt;li&gt;Workflow orchestration&lt;/li&gt; &#xA; &lt;li&gt;Setting up Airflow locally&lt;/li&gt; &#xA; &lt;li&gt;Ingesting data to GCP with Airflow&lt;/li&gt; &#xA; &lt;li&gt;Ingesting data to local Postgres with Airflow&lt;/li&gt; &#xA; &lt;li&gt;Moving data from AWS to GCP (Transfer service)&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_2_data_ingestion&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_3_data_warehouse&#34;&gt;Week 3: Data Warehouse&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data Warehouse&lt;/li&gt; &#xA; &lt;li&gt;BigQuery&lt;/li&gt; &#xA; &lt;li&gt;Partitoning and clustering&lt;/li&gt; &#xA; &lt;li&gt;BigQuery best practices&lt;/li&gt; &#xA; &lt;li&gt;Internals of BigQuery&lt;/li&gt; &#xA; &lt;li&gt;Integrating BigQuery with Airflow&lt;/li&gt; &#xA; &lt;li&gt;BigQuery Machine Learning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_3_data_warehouse&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_4_analytics_engineering/&#34;&gt;Week 4: Analytics engineering&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basics of analytics engineering&lt;/li&gt; &#xA; &lt;li&gt;dbt (data build tool)&lt;/li&gt; &#xA; &lt;li&gt;BigQuery and dbt&lt;/li&gt; &#xA; &lt;li&gt;Postgres and dbt&lt;/li&gt; &#xA; &lt;li&gt;dbt models&lt;/li&gt; &#xA; &lt;li&gt;Testing and documenting&lt;/li&gt; &#xA; &lt;li&gt;Deployment to the cloud and locally&lt;/li&gt; &#xA; &lt;li&gt;Visualising the data with google data studio and metabase&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_4_analytics_engineering&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_5_batch_processing&#34;&gt;Week 5: Batch processing&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Batch processing&lt;/li&gt; &#xA; &lt;li&gt;What is Spark&lt;/li&gt; &#xA; &lt;li&gt;Spark Dataframes&lt;/li&gt; &#xA; &lt;li&gt;Spark SQL&lt;/li&gt; &#xA; &lt;li&gt;Internals: GroupBy and joins&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_5_batch_processing&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_6_stream_processing&#34;&gt;Week 6: Streaming&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Introduction to Kafka&lt;/li&gt; &#xA; &lt;li&gt;Schemas (avro)&lt;/li&gt; &#xA; &lt;li&gt;Kafka Streams&lt;/li&gt; &#xA; &lt;li&gt;Kafka Connect and KSQL&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_6_stream_processing&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_7_project&#34;&gt;Week 7, 8 &amp;amp; 9: Project&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Putting everything we learned to practice&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 7 and 8: working on your own project&lt;/li&gt; &#xA; &lt;li&gt;Week 9: reviewing your peers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_7_project&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;h3&gt;Architecture diagram&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/images/architecture/arch_1.jpg&#34;&gt; &#xA;&lt;h3&gt;Technologies&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Google Cloud Platform (GCP)&lt;/em&gt;: Cloud-based auto-scaling platform by Google &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;Google Cloud Storage (GCS)&lt;/em&gt;: Data Lake&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;BigQuery&lt;/em&gt;: Data Warehouse&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Terraform&lt;/em&gt;: Infrastructure-as-Code (IaC)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Docker&lt;/em&gt;: Containerization&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;SQL&lt;/em&gt;: Data Analysis &amp;amp; Exploration&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Airflow&lt;/em&gt;: Pipeline Orchestration&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;dbt&lt;/em&gt;: Data Transformation&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Spark&lt;/em&gt;: Distributed Processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Kafka&lt;/em&gt;: Streaming&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;To get most out of this course, you should feel comfortable with coding and command line, and know the basics of SQL. Prior experience with Python will be helpful, but you can pick Python relatively fast if you have experience with other programming languages.&lt;/p&gt; &#xA;&lt;p&gt;Prior experience with data engineering is not required.&lt;/p&gt; &#xA;&lt;h2&gt;Instructors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ankush Khanna (&lt;a href=&#34;https://linkedin.com/in/ankushkhanna2&#34;&gt;https://linkedin.com/in/ankushkhanna2&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Sejal Vaidya (&lt;a href=&#34;https://linkedin.com/in/vaidyasejal&#34;&gt;https://linkedin.com/in/vaidyasejal&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Victoria Perez Mola (&lt;a href=&#34;https://www.linkedin.com/in/victoriaperezmola/&#34;&gt;https://www.linkedin.com/in/victoriaperezmola/&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Alexey Grigorev (&lt;a href=&#34;https://linkedin.com/in/agrigorev&#34;&gt;https://linkedin.com/in/agrigorev&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tools&lt;/h2&gt; &#xA;&lt;p&gt;For this course you&#39;ll need to have the following software installed on your computer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker and Docker-Compose&lt;/li&gt; &#xA; &lt;li&gt;Python 3 (e.g. via &lt;a href=&#34;https://www.anaconda.com/products/individual&#34;&gt;Anaconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Google Cloud SDK&lt;/li&gt; &#xA; &lt;li&gt;Terraform&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/main/week_1_basics_n_setup&#34;&gt;Week 1&lt;/a&gt; for more details about installing these tools&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: I registered, but haven&#39;t received a confirmation email. Is it normal? &lt;strong&gt;A&lt;/strong&gt;: Yes, it&#39;s normal. It&#39;s not automated. But you will receive an email eventually&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: At what time of the day will it happen? &lt;strong&gt;A&lt;/strong&gt;: Office hours will happen on Mondays at 17:00 CET. But everything will be recorded, so you can watch it whenever it&#39;s convenient for you&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Will there be a certificate? &lt;strong&gt;A&lt;/strong&gt;: Yes, if you complete the project&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: I&#39;m 100% not sure I&#39;ll be able to attend. Can I still sign up? &lt;strong&gt;A&lt;/strong&gt;: Yes, please do! You&#39;ll receive all the updates and then you can watch the course at your own pace.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Do you plan to run a ML engineering course as well? &lt;strong&gt;A&lt;/strong&gt;: Glad you asked. &lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp&#34;&gt;We do&lt;/a&gt; :)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: I&#39;m stuck! I&#39;ve got a technical question! &lt;strong&gt;A&lt;/strong&gt;: Ask on Slack! And check out the &lt;a href=&#34;https://docs.google.com/document/d/19bnYs80DwuUimHM65UV3sylsCn2j1vziPOwzBwQrebw/edit?usp=sharing&#34;&gt;student FAQ&lt;/a&gt;; many common issues have been answered already. If your issue is solved, please add how you solved it to the document. Thanks!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Our friends&lt;/h2&gt; &#xA;&lt;p&gt;Big thanks to other communities for helping us spread the word about the course:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dphi.tech/&#34;&gt;DPhi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mlops.community/&#34;&gt;MLOps.community&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Check them out - they are cool!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure/azureml-examples</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/Azure/azureml-examples</id>
    <link href="https://github.com/Azure/azureml-examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official community-driven Azure Machine Learning examples, tested with GitHub Actions.&lt;/p&gt;&lt;hr&gt;&lt;hr&gt; &#xA;&lt;p&gt;page_type: sample languages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;azurecli&lt;/li&gt; &#xA; &lt;li&gt;python products:&lt;/li&gt; &#xA; &lt;li&gt;azure-machine-learning description: Top-level directory for official Azure Machine Learning sample code and examples.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Azure Machine Learning examples&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Azure/azureml-examples/actions/workflows/smoke.yml&#34;&gt;&lt;img src=&#34;https://github.com/Azure/azureml-examples/workflows/smoke/badge.svg?sanitize=true&#34; alt=&#34;smoke&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Python code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azureml-examples/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-purple.svg?sanitize=true&#34; alt=&#34;license: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome to the Azure Machine Learning examples repository!&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;directory&lt;/th&gt; &#xA;   &lt;th&gt;description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure/azureml-examples/main/.github&#34;&gt;&lt;code&gt;.github&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GitHub files like issue templates and actions workflows.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure/azureml-examples/main/cli&#34;&gt;&lt;code&gt;cli&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Azure Machine Learning CLI (v2) examples.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure/azureml-examples/main/notebooks&#34;&gt;&lt;code&gt;notebooks&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Jupyter notebooks with MLflow tracking to an Azure ML workspace.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure/azureml-examples/main/python-sdk&#34;&gt;&lt;code&gt;python-sdk&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Azure Machine Learning Python SDK (v1) examples.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure/azureml-examples/main/setup-ci&#34;&gt;&lt;code&gt;setup-ci&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Setup scripts to customize and configure an Azure Machine Learning compute instance.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Azure/azureml-examples/main/setup-repo&#34;&gt;&lt;code&gt;setup-repo&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Setup scripts for Azure/azureml-examples.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions and suggestions! Please see the &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azureml-examples/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/Azure/azureml-examples/main/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/azure/machine-learning&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>kwea123/nerf_pl</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/kwea123/nerf_pl</id>
    <link href="https://github.com/kwea123/nerf_pl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NeRF (Neural Radiance Fields) and NeRF in the Wild using pytorch-lightning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nerf_pl&lt;/h1&gt; &#xA;&lt;h3&gt;Update: NVIDIA just open-sourced a lightning-fast version of NeRF: &lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34;&gt;NGP&lt;/a&gt;. If you just want to see how strong NeRF is, try out this one! Sadly, it allows very little customization due to many dependencies, unless you are super familiar with those libraries and cuda programming (I&#39;m not &lt;span&gt;üòµüí´&lt;/span&gt;).&lt;/h3&gt; &#xA;&lt;h3&gt;Update: an improved &lt;a href=&#34;https://www.cs.cornell.edu/~zl548/NSFF/&#34;&gt;NSFF&lt;/a&gt; implementation to handle dynamic scene is &lt;a href=&#34;https://github.com/kwea123/nsff_pl&#34;&gt;open&lt;/a&gt;!&lt;/h3&gt; &#xA;&lt;h3&gt;Update: &lt;a href=&#34;https://nerf-w.github.io/&#34;&gt;NeRF-W&lt;/a&gt; (NeRF in the Wild) implementation is added to &lt;a href=&#34;https://github.com/kwea123/nerf_pl/tree/nerfw&#34;&gt;nerfw&lt;/a&gt; branch!&lt;/h3&gt; &#xA;&lt;h3&gt;Update: The lastest code (using the latest libraries) will be updated to &lt;a href=&#34;https://github.com/kwea123/nerf_pl/tree/dev&#34;&gt;dev&lt;/a&gt; branch. The master branch remains to support the colab files. If you don&#39;t use colab, it is recommended to switch to dev branch.&lt;/h3&gt; &#xA;&lt;h3&gt;Only issues of the dev and nerfw branch will be considered currently.&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;span&gt;üíé&lt;/span&gt; &lt;a href=&#34;https://kwea123.github.io/nerf_pl/&#34;&gt;&lt;strong&gt;Project page&lt;/strong&gt;&lt;/a&gt; (live demo!)&lt;/h3&gt; &#xA;&lt;p&gt;Unofficial implementation of &lt;a href=&#34;https://arxiv.org/pdf/2003.08934.pdf&#34;&gt;NeRF&lt;/a&gt; (Neural Radiance Fields) using pytorch (&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning&#34;&gt;pytorch-lightning&lt;/a&gt;). This repo doesn&#39;t aim at reproducibility, but aim at providing a simpler and faster training procedure (also simpler code with detailed comments to help to understand the work). Moreover, I try to extend much more opportunities by integrating this algorithm into game engine like Unity.&lt;/p&gt; &#xA;&lt;p&gt;Official implementation: &lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;nerf&lt;/a&gt; .. Reference pytorch implementation: &lt;a href=&#34;https://github.com/yenchenlin/nerf-pytorch&#34;&gt;nerf-pytorch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Recommend to read: A detailed NeRF extension list: &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF&#34;&gt;awesome-NeRF&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;&lt;span&gt;üåå&lt;/span&gt; Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-gpu training: Training on 8 GPUs finishes within 1 hour for the synthetic dataset!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#mortar_board-colab&#34;&gt;Colab&lt;/a&gt; notebooks to allow easy usage!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#ribbon-mesh&#34;&gt;Reconstruct&lt;/a&gt; &lt;strong&gt;colored&lt;/strong&gt; mesh!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/S5phWFTs2iM&#34;&gt;Mixed Reality&lt;/a&gt; in Unity!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/w9qTbVzCdWk&#34;&gt;REAL TIME volume rendering&lt;/a&gt; in Unity!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#portable-scenes&#34;&gt;Portable Scenes&lt;/a&gt; to let you play with other people&#39;s scenes!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;You can find the Unity project including mesh, mixed reality and volume rendering &lt;a href=&#34;https://github.com/kwea123/nerf_Unity&#34;&gt;here&lt;/a&gt;! See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/README_Unity.md&#34;&gt;README_Unity&lt;/a&gt; for generating your own data for Unity rendering!&lt;/h3&gt; &#xA;&lt;h2&gt;&lt;span&gt;üî∞&lt;/span&gt; Tutorial&lt;/h2&gt; &#xA;&lt;h3&gt;What can NeRF do?&lt;/h3&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/11364490/82124460-1ccbbb80-97da-11ea-88ad-25e22868a5c1.png&#34; style=&#34;max-width:100%&#34;&gt; &#xA;&lt;h3&gt;Tutorial videos&lt;/h3&gt; &#xA;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLDV2CyUo4q-K02pNEyDr7DYpTQuka3mbV&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80913471-d5781080-8d7f-11ea-9f72-9d68402b8271.png&#34;&gt; &lt;/a&gt; &#xA;&lt;h1&gt;&lt;span&gt;üíª&lt;/span&gt; Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Hardware&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OS: Ubuntu 18.04&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU with &lt;strong&gt;CUDA&amp;gt;=10.1&lt;/strong&gt; (tested with 1 RTX2080Ti)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Software&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repo by &lt;code&gt;git clone --recursive https://github.com/kwea123/nerf_pl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Python&amp;gt;=3.6 (installation via &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;anaconda&lt;/a&gt; is recommended, use &lt;code&gt;conda create -n nerf_pl python=3.6&lt;/code&gt; to create a conda environment and activate it by &lt;code&gt;conda activate nerf_pl&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Python libraries &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Install core requirements by &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Install &lt;code&gt;torchsearchsorted&lt;/code&gt; by &lt;code&gt;cd torchsearchsorted&lt;/code&gt; then &lt;code&gt;pip install .&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;span&gt;üîë&lt;/span&gt; Training&lt;/h1&gt; &#xA;&lt;p&gt;Please see each subsection for training on different datasets. Available training datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#blender&#34;&gt;Blender&lt;/a&gt; (Realistic Synthetic 360)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#llff&#34;&gt;LLFF&lt;/a&gt; (Real Forward-Facing)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#your-own-data&#34;&gt;Your own data&lt;/a&gt; (Forward-Facing/360 inward-facing)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Blender&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Steps&lt;/summary&gt; &#xA; &lt;h3&gt;Data download&lt;/h3&gt; &#xA; &lt;p&gt;Download &lt;code&gt;nerf_synthetic.zip&lt;/code&gt; from &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Training model&lt;/h3&gt; &#xA; &lt;p&gt;Run (example)&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python train.py \&#xA;   --dataset_name blender \&#xA;   --root_dir $BLENDER_DIR \&#xA;   --N_importance 64 --img_wh 400 400 --noise_std 0 \&#xA;   --num_epochs 16 --batch_size 1024 \&#xA;   --optimizer adam --lr 5e-4 \&#xA;   --lr_scheduler steplr --decay_step 2 4 8 --decay_gamma 0.5 \&#xA;   --exp_name exp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;These parameters are chosen to best mimic the training settings in the original repo. See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/opt.py&#34;&gt;opt.py&lt;/a&gt; for all configurations.&lt;/p&gt; &#xA; &lt;p&gt;NOTE: the above configuration doesn&#39;t work for some scenes like &lt;code&gt;drums&lt;/code&gt;, &lt;code&gt;ship&lt;/code&gt;. In that case, consider increasing the &lt;code&gt;batch_size&lt;/code&gt; or change the &lt;code&gt;optimizer&lt;/code&gt; to &lt;code&gt;radam&lt;/code&gt;. I managed to train on all scenes with these modifications.&lt;/p&gt; &#xA; &lt;p&gt;You can monitor the training process by &lt;code&gt;tensorboard --logdir logs/&lt;/code&gt; and go to &lt;code&gt;localhost:6006&lt;/code&gt; in your browser.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;LLFF&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Steps&lt;/summary&gt; &#xA; &lt;h3&gt;Data download&lt;/h3&gt; &#xA; &lt;p&gt;Download &lt;code&gt;nerf_llff_data.zip&lt;/code&gt; from &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Training model&lt;/h3&gt; &#xA; &lt;p&gt;Run (example)&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python train.py \&#xA;   --dataset_name llff \&#xA;   --root_dir $LLFF_DIR \&#xA;   --N_importance 64 --img_wh 504 378 \&#xA;   --num_epochs 30 --batch_size 1024 \&#xA;   --optimizer adam --lr 5e-4 \&#xA;   --lr_scheduler steplr --decay_step 10 20 --decay_gamma 0.5 \&#xA;   --exp_name exp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;These parameters are chosen to best mimic the training settings in the original repo. See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/opt.py&#34;&gt;opt.py&lt;/a&gt; for all configurations.&lt;/p&gt; &#xA; &lt;p&gt;You can monitor the training process by &lt;code&gt;tensorboard --logdir logs/&lt;/code&gt; and go to &lt;code&gt;localhost:6006&lt;/code&gt; in your browser.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Your own data&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Steps&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install &lt;a href=&#34;https://github.com/colmap/colmap&#34;&gt;COLMAP&lt;/a&gt; following &lt;a href=&#34;https://colmap.github.io/install.html&#34;&gt;installation guide&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Prepare your images in a folder (around 20 to 30 for forward facing, and 40 to 50 for 360 inward-facing)&lt;/li&gt; &#xA;  &lt;li&gt;Clone &lt;a href=&#34;https://github.com/Fyusion/LLFF&#34;&gt;LLFF&lt;/a&gt; and run &lt;code&gt;python img2poses.py $your-images-folder&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Train the model using the same command as in &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#llff&#34;&gt;LLFF&lt;/a&gt;. If the scene is captured in a 360 inward-facing manner, add &lt;code&gt;--spheric&lt;/code&gt; argument.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;For more details of training a good model, please see the video &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/#colab&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Pretrained models and logs&lt;/h2&gt; &#xA;&lt;p&gt;Download the pretrained models and training logs in &lt;a href=&#34;https://github.com/kwea123/nerf_pl/releases&#34;&gt;release&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Comparison with other repos&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;training GPU memory in GB&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Speed (1 step)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;Original&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.177s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yenchenlin/nerf-pytorch&#34;&gt;Ref pytorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.147s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This repo&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.12s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The speed is measured on 1 RTX2080Ti. Detailed profile can be found in &lt;a href=&#34;https://github.com/kwea123/nerf_pl/releases&#34;&gt;release&lt;/a&gt;. Training memory is largely reduced, since the original repo loads the whole data to GPU at the beginning, while we only pass batches to GPU every step.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üîé&lt;/span&gt; Testing&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/test.ipynb&#34;&gt;test.ipynb&lt;/a&gt; for a simple view synthesis and depth prediction on 1 image.&lt;/p&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/eval.py&#34;&gt;eval.py&lt;/a&gt; to create the whole sequence of moving views. E.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python eval.py \&#xA;   --root_dir $BLENDER \&#xA;   --dataset_name blender --scene_name lego \&#xA;   --img_wh 400 400 --N_importance 64 --ckpt_path $CKPT_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt; : Don&#39;t forget to add &lt;code&gt;--spheric_poses&lt;/code&gt; if the model is trained under &lt;code&gt;--spheric&lt;/code&gt; setting!&lt;/p&gt; &#xA;&lt;p&gt;It will create folder &lt;code&gt;results/{dataset_name}/{scene_name}&lt;/code&gt; and run inference on all test data, finally create a gif out of them.&lt;/p&gt; &#xA;&lt;p&gt;Example of lego scene using pretrained model and the reconstructed &lt;strong&gt;colored&lt;/strong&gt; mesh: (PSNR=31.39, paper=32.54)&lt;/p&gt; &#xA;&lt;p&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/79932648-f8a1e680-8488-11ea-98fe-c11ec22fc8a1.gif&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80813179-822d8300-8c04-11ea-84e6-142f04714c58.png&#34; width=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Example of fern scene using pretrained model:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/11364490/79932650-f9d31380-8488-11ea-8dad-b70a6a3daa6e.gif&#34; alt=&#34;fern&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Example of own scene (&lt;a href=&#34;https://www.youtube.com/watch?v=hVQIvEq_Av0&#34;&gt;Silica GGO figure&lt;/a&gt;) and the reconstructed &lt;strong&gt;colored&lt;/strong&gt; mesh. Click to link to youtube video.&lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://youtu.be/yH1ZBcdNsUY&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80279695-324d4880-873a-11ea-961a-d6350e149ece.gif&#34; height=&#34;252&#34;&gt; &lt;/a&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/80813184-83f74680-8c04-11ea-8606-40580f753355.png&#34; height=&#34;252&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Portable scenes&lt;/h2&gt; &#xA;&lt;p&gt;The concept of NeRF is that the whole scene is compressed into a NeRF model, then we can render from any pose we want. To render from plausible poses, we can leverage the training poses; therefore, you can generate video with &lt;strong&gt;only&lt;/strong&gt; the trained model and the poses (hence the name of portable scenes). I provided my silica model in &lt;a href=&#34;https://github.com/kwea123/nerf_pl/releases&#34;&gt;release&lt;/a&gt;, feel free to play around with it!&lt;/p&gt; &#xA;&lt;p&gt;If you trained some interesting scenes, you are also welcomed to share the model (and the &lt;code&gt;poses_bounds.npy&lt;/code&gt;) by sending me an email, or post in issues! After all, a model is just around &lt;strong&gt;5MB&lt;/strong&gt;! Please run &lt;code&gt;python utils/save_weights_only.py --ckpt_path $YOUR_MODEL_PATH&lt;/code&gt; to extract the final model.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üéÄ&lt;/span&gt; Mesh&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/kwea123/nerf_pl/master/README_mesh.md&#34;&gt;README_mesh&lt;/a&gt; for reconstruction of &lt;strong&gt;colored&lt;/strong&gt; mesh. Only supported for blender dataset and 360 inward-facing data!&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;‚ö†&lt;/span&gt; Notes on differences with the original repo&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The learning rate decay in the original repo is &lt;strong&gt;by step&lt;/strong&gt;, which means it decreases every step, here I use learning rate decay &lt;strong&gt;by epoch&lt;/strong&gt;, which means it changes only at the end of 1 epoch.&lt;/li&gt; &#xA; &lt;li&gt;The validation image for LLFF dataset is chosen as the most centered image here, whereas the original repo chooses every 8th image.&lt;/li&gt; &#xA; &lt;li&gt;The rendering spiral path is slightly different from the original repo (I use approximate values to simplify the code).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;span&gt;üéì&lt;/span&gt; COLAB&lt;/h1&gt; &#xA;&lt;p&gt;I also prepared colab notebooks that allow you to run the algorithm on any machine without GPU requirement.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/kwea123/f0e8f38ff2aa94495dbfe7ae9219f75c&#34;&gt;colmap&lt;/a&gt; to prepare camera poses for your own training data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/kwea123/a3c541a325e895ef79ecbc0d2e6d7221&#34;&gt;nerf&lt;/a&gt; to train on your data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/kwea123/77ed1640f9bc9550136dc13a6a419e88&#34;&gt;extract_mesh&lt;/a&gt; to extract colored mesh&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://www.youtube.com/playlist?list=PLDV2CyUo4q-K02pNEyDr7DYpTQuka3mbV&#34;&gt;this playlist&lt;/a&gt; for the detailed tutorials.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üéÉ&lt;/span&gt; SHOWOFF&lt;/h1&gt; &#xA;&lt;p&gt;We can incorporate &lt;em&gt;ray tracing&lt;/em&gt; techniques into the volume rendering pipeline, and realize realistic scene editing (following is the &lt;code&gt;materials&lt;/code&gt; scene with an object removed, and a mesh is inserted and rendered with ray tracing). The code &lt;strong&gt;will not&lt;/strong&gt; be released.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/11364490/90312710-92face00-df41-11ea-9eea-10f24849b407.gif&#34; alt=&#34;add&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/90360796-92744b80-e097-11ea-859d-159aa2519375.gif&#34; alt=&#34;add2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;With my integration in Unity, I can realize realistic mixed reality photos (note my character casts shadow on the scene, &lt;strong&gt;zero&lt;/strong&gt; post- image editing required): &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/140264589-295acebe-8ace-4d61-b871-26eb8ae10ab0.png&#34; alt=&#34;defer&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11364490/140264596-59daebe5-b88d-48e7-82bd-5ccaaff2283f.png&#34; alt=&#34;defer2&#34;&gt; BTW, I would like to visit the museum one day...&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üìñ&lt;/span&gt; Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use (part of) my code or find my work helpful, please consider citing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{queianchen_nerf,&#xA;  author={Quei-An, Chen},&#xA;  title={Nerf_pl: a pytorch-lightning implementation of NeRF},&#xA;  url={https://github.com/kwea123/nerf_pl/},&#xA;  year={2020},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>plotly/dash-sample-apps</title>
    <updated>2022-06-02T01:45:24Z</updated>
    <id>tag:github.com,2022-06-02:/plotly/dash-sample-apps</id>
    <link href="https://github.com/plotly/dash-sample-apps" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open-source demos hosted on Dash Gallery&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dash Sample Apps&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/plotly/dash-sample-apps&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/plotly/dash-sample-apps.svg?style=svg&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository hosts the code for over 100 open-source Dash apps written in Python or R. They can serve as a starting point for your own Dash app, as a learning tool to better understand how Dash works, as a reusable templates, and much more.&lt;/p&gt; &#xA;&lt;p&gt;Most apps in this repository are hosted on &lt;a href=&#34;https://dash-gallery.plotly.host/&#34;&gt;Dash Gallery&lt;/a&gt;, which is our internal server running on &lt;a href=&#34;https://plotly.com/dash/kubernetes/&#34;&gt;Dash Enterprise Kubernetes&lt;/a&gt;. Note that you can find both open-sourced apps and demos for our &lt;a href=&#34;https://plotly.com/dash/&#34;&gt;licensed products&lt;/a&gt;, including &lt;a href=&#34;https://plotly.com/dash/design-kit/&#34;&gt;Design Kit&lt;/a&gt; and &lt;a href=&#34;https://plotly.com/dash/snapshot-engine/&#34;&gt;Snapshot Engine&lt;/a&gt;. If you are interested in learning more, don&#39;t hesitate to reach out to &lt;a href=&#34;https://plotly.com/get-demo/&#34;&gt;get a demo&lt;/a&gt;. If you want to only see the open-sourced apps, select the &lt;a href=&#34;https://dash-gallery.plotly.host/Portal/?search=%5BOpen%20Source%5D&#34;&gt;&#34;Open Source&#34; tag&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Downloading and running a single app&lt;/h2&gt; &#xA;&lt;p&gt;Visit the &lt;a href=&#34;https://github.com/plotly/dash-sample-apps/releases&#34;&gt;releases page&lt;/a&gt; and download and &lt;code&gt;unzip&lt;/code&gt; the app you want. Then &lt;code&gt;cd&lt;/code&gt; into the app directory and install its dependencies in a virtual environment in the following way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv venv&#xA;source venv/bin/activate  # Windows: \venv\scripts\activate&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then you can run the app:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cloning this whole repository&lt;/h2&gt; &#xA;&lt;p&gt;To clone this repository, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/plotly/dash-sample-apps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note this might take a long time since it copies over 100 apps available in the repo. If you just want to try one app, refer to the section above.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;To contribute to this repository, please see the &lt;a href=&#34;https://raw.githubusercontent.com/plotly/dash-sample-apps/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>