<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-31T01:31:56Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Aswinramesh04/100-Days-of-DataScience</title>
    <updated>2024-01-31T01:31:56Z</updated>
    <id>tag:github.com,2024-01-31:/Aswinramesh04/100-Days-of-DataScience</id>
    <link href="https://github.com/Aswinramesh04/100-Days-of-DataScience" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Greetings! üëã I&#39;m Loga Aswin, diving into a 100-day data science immersion from Python fundamentals to real-world applications. This space will be a live documentation of my journey, where code meets curiosity. Let&#39;s connect, learn, and code together. Click ‚≠ê on GitHub to stay tuned for updates on my work!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;100-DaysOfCode-DataScience&lt;/h1&gt;</summary>
  </entry>
  <entry>
    <title>AviSoori1x/makeMoE</title>
    <updated>2024-01-31T01:31:56Z</updated>
    <id>tag:github.com,2024-01-31:/AviSoori1x/makeMoE</id>
    <link href="https://github.com/AviSoori1x/makeMoE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;From scratch implementation of a sparse mixture of experts language model inspired by Andrej Karpathy&#39;s makemore :)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;makeMoE&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/makemoelogo.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;a href=&#34;https://www.databricks.com/product/machine-learning&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/databricks.png&#34; width=&#34;50px&#34; height=&#34;auto&#34;&gt; &lt;/a&gt; &#xA;&lt;br&gt; &#xA;&lt;span&gt;Developed using Databricks with ‚ù§Ô∏è&lt;/span&gt; &#xA;&lt;h4&gt;Sparse mixture of experts language model from scratch inspired by (and largely based on) Andrej Karpathy&#39;s makemore (&lt;a href=&#34;https://github.com/karpathy/makemore&#34;&gt;https://github.com/karpathy/makemore&lt;/a&gt;) :)&lt;/h4&gt; &#xA;&lt;p&gt;HuggingFace Community Blog that walks through this: &lt;a href=&#34;https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch&#34;&gt;https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is an implementation of a sparse mixture of experts language model from scratch. This is inspired by and largely based on Andrej Karpathy&#39;s project &#39;makemore&#39; and borrows the re-usable components from that implementation. Just like makemore, makeMoE is also an autoregressive character-level language model but uses the aforementioned sparse mixture of experts architecture.&lt;/p&gt; &#xA;&lt;p&gt;Just like makemore, pytorch is the only requirement (so I hope the from scratch claim is justified).&lt;/p&gt; &#xA;&lt;p&gt;Significant Changes from the makemore architecture&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sparse mixture of experts instead of the solitary feed forward neural net.&lt;/li&gt; &#xA; &lt;li&gt;Top-k gating and noisy top-k gating implementations.&lt;/li&gt; &#xA; &lt;li&gt;initialization - Kaiming He initialization used here but the point of this notebook is to be hackable so you can swap in Xavier Glorot etc. and take it for a spin.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Unchanged from makemore&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The dataset, preprocessing (tokenization), and the language modeling task Andrej chose originally - generate Shakespeare-like text&lt;/li&gt; &#xA; &lt;li&gt;Casusal self attention implementation&lt;/li&gt; &#xA; &lt;li&gt;Training loop&lt;/li&gt; &#xA; &lt;li&gt;Inference logic&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Publications heavily referenced for this implementation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Outrageosly Large Neural Networks: The Sparsely-Gated Mixture-Of-Experts layer: &lt;a href=&#34;https://arxiv.org/pdf/1701.06538.pdf&#34;&gt;https://arxiv.org/pdf/1701.06538.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mixtral of experts: &lt;a href=&#34;https://arxiv.org/pdf/2401.04088.pdf&#34;&gt;https://arxiv.org/pdf/2401.04088.pdf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;makMoE_from_Scratch.ipynb walks through the intuition for the entire model architecture and how everything comes together. I recommend starting here.&lt;/p&gt; &#xA;&lt;p&gt;makeMoE_Concise.ipynb is the consolidated hackable implementation that I encourage you to hack, understand, improve and make your own&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The code was entirely developed on Databricks using a single A100 for compute. If you&#39;re running this on Databricks, you can scale this on an arbitrarily large GPU cluster with no issues, on the cloud provider of your choice.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I chose to use MLFlow (which comes pre-installed in Databricks. It&#39;s fully open source and you can pip install easily elsewhere) as I find it helpful to track and log all the metrics necessary. This is entirely optional.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note that the implementation emphasizes readability and hackability vs. performance, so there are many ways in which you could improve this. Please try and let me know!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Hope you find this useful. Happy hacking!!&lt;/p&gt;</summary>
  </entry>
</feed>