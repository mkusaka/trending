<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-28T01:38:18Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>graviraja/MLOps-Basics</title>
    <updated>2023-07-28T01:38:18Z</updated>
    <id>tag:github.com,2023-07-28:/graviraja/MLOps-Basics</id>
    <link href="https://github.com/graviraja/MLOps-Basics" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLOps-Basics&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;There is nothing magic about magic. The magician merely understands something simple which doesn‚Äôt appear to be simple or natural to the untrained audience. Once you learn how to hold a card while making your hand look empty, you only need practice before you, too, can ‚Äúdo magic.‚Äù ‚Äì Jeffrey Friedl in the book Mastering Regular Expressions&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: Please raise an issue for any suggestions, corrections, and feedback.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The goal of the series is to understand the basics of MLOps like model building, monitoring, configurations, testing, packaging, deployment, cicd, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/summary.png&#34; alt=&#34;pl&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Week 0: Project Setup&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://www.ravirajag.dev/blog/mlops-project-setup-part1&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The project I have implemented is a simple classification problem. The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;How to get the data?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to process the data?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to define dataloaders?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to declare the model?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to train the model?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to do the inference?&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/pl.jpeg&#34; alt=&#34;pl&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/datasets&#34;&gt;Huggingface Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Huggingface Transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/&#34;&gt;Pytorch Lightning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 1: Model monitoring - Weights and Biases&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://www.ravirajag.dev/blog/mlops-wandb-integration&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tracking all the experiments like tweaking hyper-parameters, trying different models to test their performance and seeing the connection between model and the input data will help in developing a better model.&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;How to configure basic logging with W&amp;amp;B?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to compute metrics and log them in W&amp;amp;B?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to add plots in W&amp;amp;B?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to add data samples to W&amp;amp;B?&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/wandb.png&#34; alt=&#34;wannb&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wandb.ai/site&#34;&gt;Weights and Biases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://torchmetrics.readthedocs.io/&#34;&gt;torchmetrics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;References:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hUXQm46TAKc&#34;&gt;Tutorial on Pytorch Lightning + Weights &amp;amp; Bias&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://docs.wandb.ai/&#34;&gt;WandB Documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 2: Configurations - Hydra&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://www.ravirajag.dev/blog/mlops-hydra-config&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Configuration management is a necessary for managing complex software systems. Lack of configuration management can cause serious problems with reliability, uptime, and the ability to scale a system.&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Basics of Hydra&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Overridding configurations&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Splitting configuration across multiple files&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Variable Interpolation&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to run model with different parameter combinations?&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/hydra.png&#34; alt=&#34;hydra&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hydra.cc/&#34;&gt;Hydra&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://hydra.cc/docs/intro&#34;&gt;Hydra Documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.sscardapane.it/tutorials/hydra-tutorial/#executing-multiple-runs&#34;&gt;Simone Tutorial on Hydra&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 3: Data Version Control - DVC&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://www.ravirajag.dev/blog/mlops-dvc&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Classical code version control systems are not designed to handle large files, which make cloning and storing the history impractical. Which are very common in Machine Learning.&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Basics of DVC&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Initialising DVC&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Configuring Remote Storage&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Saving Model to the Remote Storage&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Versioning the models&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/dvc.png&#34; alt=&#34;dvc&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dvc.org/&#34;&gt;DVC&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dvc.org/doc&#34;&gt;DVC Documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kLKBcPonMYw&#34;&gt;DVC Tutorial on Versioning data&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 4: Model Packaging - ONNX&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://www.ravirajag.dev/blog/mlops-onnx&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Why do we need model packaging? Models can be built using any machine learning framework available out there (sklearn, tensorflow, pytorch, etc.). We might want to deploy models in different environments like (mobile, web, raspberry pi) or want to run in a different framework (trained in pytorch, inference in tensorflow). A common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers will help a lot.&lt;/p&gt; &#xA;&lt;p&gt;This is acheived by a community project &lt;code&gt;ONNX&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;What is ONNX?&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;How to convert a trained model to ONNX format?&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;What is ONNX Runtime?&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;How to run ONNX converted model in ONNX Runtime?&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Comparisions&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/onnx.jpeg&#34; alt=&#34;ONNX&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Following tech stack is used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://onnx.ai/&#34;&gt;ONNX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.onnxruntime.ai/&#34;&gt;ONNXRuntime&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=7nutT3Aacyw&#34;&gt;Abhishek Thakur tutorial on onnx model conversion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/stable/common/production_inference.html&#34;&gt;Pytorch Lightning documentation on onnx conversion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333&#34;&gt;Huggingface Blog on ONNXRuntime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tugot17.github.io/data-science-blog/onnx/tutorial/2020/09/21/Exporting-lightning-model-to-onnx.html&#34;&gt;Piotr Blog on onnx conversion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 5: Model Packaging - Docker&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=easy&amp;amp;color=green&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://www.ravirajag.dev/blog/mlops-docker&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Why do we need packaging? We might have to share our application with others, and when they try to run the application most of the time it doesn‚Äôt run due to dependencies issues / OS related issues and for that, we say (famous quote across engineers) that &lt;code&gt;It works on my laptop/system&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;So for others to run the applications they have to set up the same environment as it was run on the host side which means a lot of manual configuration and installation of components.&lt;/p&gt; &#xA;&lt;p&gt;The solution to these limitations is a technology called Containers.&lt;/p&gt; &#xA;&lt;p&gt;By containerizing/packaging the application, we can run the application on any cloud platform to get advantages of managed services and autoscaling and reliability, and many more.&lt;/p&gt; &#xA;&lt;p&gt;The most prominent tool to do the packaging of application is Docker üõ≥&lt;/p&gt; &#xA;&lt;p&gt;The scope of this week is to understand the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;FastAPI wrapper&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Basics of Docker&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Building Docker Container&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Docker Compose&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/docker_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2021/06/a-hands-on-guide-to-containerized-your-machine-learning-workflow-with-docker/&#34;&gt;Analytics vidhya blog&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 6: CI/CD - GitHub Actions&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://www.ravirajag.dev/blog/mlops-github-actions&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;CI/CD is a coding philosophy and set of practices with which you can continuously build, test, and deploy iterative code changes.&lt;/p&gt; &#xA;&lt;p&gt;This iterative process helps reduce the chance that you develop new code based on a buggy or failed previous versions. With this method, you strive to have less human intervention or even no intervention at all, from the development of new code until its deployment.&lt;/p&gt; &#xA;&lt;p&gt;In this post, I will be going through the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basics of GitHub Actions&lt;/li&gt; &#xA; &lt;li&gt;First GitHub Action&lt;/li&gt; &#xA; &lt;li&gt;Creating Google Service Account&lt;/li&gt; &#xA; &lt;li&gt;Giving access to Service account&lt;/li&gt; &#xA; &lt;li&gt;Configuring DVC to use Google Service account&lt;/li&gt; &#xA; &lt;li&gt;Configuring Github Action&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/basic_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;References&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dvc.org/doc/user-guide/setup-google-drive-remote&#34;&gt;Configuring service account&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://docs.github.com/en/actions/quickstart&#34;&gt;Github actions&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Week 7: Container Registry - AWS ECR&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://www.ravirajag.dev/blog/mlops-container-registry&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A container registry is a place to store container images. A container image is a file comprised of multiple layers which can execute applications in a single instance. Hosting all the images in one stored location allows users to commit, identify and pull images when needed.&lt;/p&gt; &#xA;&lt;p&gt;Amazon Simple Storage Service (S3) is a storage for the internet. It is designed for large-capacity, low-cost storage provision across multiple geographical regions.&lt;/p&gt; &#xA;&lt;p&gt;In this week, I will be going through the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of S3&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Programmatic access to S3&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Configuring AWS S3 as remote storage in DVC&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of ECR&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Configuring GitHub Actions to use S3, ECR&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/ecr_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Week 8: Serverless Deployment - AWS Lambda&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://www.ravirajag.dev/blog/mlops-serverless&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A serverless architecture is a way to build and run applications and services without having to manage infrastructure. The application still runs on servers, but all the server management is done by third party service (AWS). We no longer have to provision, scale, and maintain servers to run the applications. By using a serverless architecture, developers can focus on their core product instead of worrying about managing and operating servers or runtimes, either in the cloud or on-premises.&lt;/p&gt; &#xA;&lt;p&gt;In this week, I will be going through the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of Serverless&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of AWS Lambda&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Triggering Lambda with API Gateway&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Deploying Container using Lambda&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Automating deployment to Lambda using Github Actions&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/lambda_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Week 9: Prediction Monitoring - Kibana&lt;/h2&gt; &#xA;&lt;img src=&#34;https://img.shields.io/static/v1.svg?style=for-the-badge&amp;amp;label=difficulty&amp;amp;message=medium&amp;amp;color=orange&#34;&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://www.ravirajag.dev/blog/mlops-monitoring&#34;&gt;Blog Post here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Monitoring systems can help give us confidence that our systems are running smoothly and, in the event of a system failure, can quickly provide appropriate context when diagnosing the root cause.&lt;/p&gt; &#xA;&lt;p&gt;Things we want to monitor during and training and inference are different. During training we are concered about whether the loss is decreasing or not, whether the model is overfitting, etc.&lt;/p&gt; &#xA;&lt;p&gt;But, during inference, We like to have confidence that our model is making correct predictions.&lt;/p&gt; &#xA;&lt;p&gt;There are many reasons why a model can fail to make useful predictions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The underlying data distribution has shifted over time and the model has gone stale. i.e inference data characteristics is different from the data characteristics used to train the model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The inference data stream contains edge cases (not seen during model training). In this scenarios model might perform poorly or can lead to errors.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The model was misconfigured in its production deployment. (Configuration issues are common)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In all of these scenarios, the model could still make a &lt;code&gt;successful&lt;/code&gt; prediction from a service perspective, but the predictions will likely not be useful. Monitoring machine learning models can help us detect such scenarios and intervene (e.g. trigger a model retraining/deployment pipeline).&lt;/p&gt; &#xA;&lt;p&gt;In this week, I will be going through the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Basics of Cloudwatch Logs&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Creating Elastic Search Cluster&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Configuring Cloudwatch Logs with Elastic Search&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Creating Index Patterns in Kibana&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Creating Kibana Visualisations&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Creating Kibana Dashboard&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/graviraja/MLOps-Basics/main/images/kibana_flow.png&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sinanuozdemir/large-language-models-and-chatgpt-in-three-weeks</title>
    <updated>2023-07-28T01:38:18Z</updated>
    <id>tag:github.com,2023-07-28:/sinanuozdemir/large-language-models-and-chatgpt-in-three-weeks</id>
    <link href="https://github.com/sinanuozdemir/large-language-models-and-chatgpt-in-three-weeks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Large Language Models and ChatGPT in Three Weeks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Large Language Models and ChatGPT in Three Weeks&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the &#34;&lt;a href=&#34;https://learning.oreilly.com/live-events/large-language-models-and-chatgpt-in-3-weeks/0636920090988/&#34;&gt;Large Language Models and ChatGPT in Three Weeks&lt;/a&gt;&#34; code repository! In this repo, we dive deep into understanding and utilizing large language models, with a specific focus on OpenAI&#39;s GPT-3, GPT-3.5-turbo (ChatGPT), and GPT-4.&lt;/p&gt; &#xA;&lt;p&gt;For more, check out my &lt;a href=&#34;https://learning.oreilly.com/playlists/2953f6c7-0e13-49ac-88e2-b951e11388de&#34;&gt;Expert Playlist&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Project Overview&lt;/h2&gt; &#xA;&lt;p&gt;The project is divided into the following sections:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Introduction to Large Language Models (LLMs)&lt;/strong&gt;: We start with a brief introduction to LLMs, understanding their architecture, strengths, and potential use-cases.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Working with ChatGPT&lt;/strong&gt;: ChatGPT is a version of the GPT model fine-tuned for generating conversational responses. We&#39;ll learn how to make API calls to ChatGPT and interpret the responses.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Latency Evaluation&lt;/strong&gt;: We analyze and compare the latency of API calls when using hosted API services versus running models on local compute resources. This helps in making informed decisions about where to run these powerful models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cost Calculation&lt;/strong&gt;: The code includes methods to calculate the cost of API calls based on the token usage by different models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Generating Responses with OpenAI Models&lt;/strong&gt;: We use the OpenAI&#39;s &lt;code&gt;ChatCompletion&lt;/code&gt; and &lt;code&gt;Completion&lt;/code&gt; methods to generate responses from prompts.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The code in this project helps you to get hands-on experience with these powerful language models, and also gives insights about factors to consider when deciding to use these models, such as cost and latency.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Familiarity with Python&lt;/li&gt; &#xA; &lt;li&gt;An OpenAI API key. You can obtain it by signing up on the &lt;a href=&#34;https://www.openai.com/&#34;&gt;OpenAI website&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Familiarity with machine learning concepts and natural language processing would be helpful, but not mandatory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository to your local machine.&lt;/li&gt; &#xA; &lt;li&gt;Install the required Python libraries using pip:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ensure you have set the following environment variables with your API keys or tokens:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;OPENAI_API_KEY: Your OpenAI API key.&#xA;COHERE_API_KEY: Your Cohere API key (if using Cohere&#39;s services).&#xA;HF_TOKEN: Your Hugging Face token (if using Hugging Face&#39;s services).&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;re all set to explore the notebooks!&lt;/p&gt; &#xA;&lt;h2&gt;Usage - Jupyter Notebooks&lt;/h2&gt; &#xA;&lt;p&gt;This project contains several Jupyter notebooks each focusing on a specific topic. You can find them in the &lt;code&gt;notebooks&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sinanuozdemir/large-language-models-and-chatgpt-in-three-weeks/main/notebooks/intro_prompt_engineering.ipynb&#34;&gt;Intro to Prompt Engineering&lt;/a&gt;&lt;/strong&gt;: This notebook introduces the concept of prompt engineering, a crucial aspect of effectively using language models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sinanuozdemir/large-language-models-and-chatgpt-in-three-weeks/main/notebooks/making_predictions.ipynb&#34;&gt;Making Predictions&lt;/a&gt;&lt;/strong&gt;: Here, we delve into the process of making predictions with large language models and interpret their responses.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sinanuozdemir/large-language-models-and-chatgpt-in-three-weeks/main/notebooks/cost_projecting.ipynb&#34;&gt;Cost Projecting&lt;/a&gt;&lt;/strong&gt;: This notebook focuses on understanding and calculating the costs involved in using large language models. It includes functions to calculate the cost of API calls based on the token usage by different models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sinanuozdemir/large-language-models-and-chatgpt-in-three-weeks/main/notebooks/use_cases.ipynb&#34;&gt;Use Cases&lt;/a&gt;&lt;/strong&gt;: In this notebook, we explore various use cases for large language models, providing practical examples of how they can be used in different scenarios.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License. See the LICENSE file for details.&lt;/p&gt;</summary>
  </entry>
</feed>