<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-01T01:32:28Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kingjulio8238/Memary</title>
    <updated>2025-01-01T01:32:28Z</updated>
    <id>tag:github.com,2025-01-01:/kingjulio8238/Memary</id>
    <link href="https://github.com/kingjulio8238/Memary" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Open Source Memory Layer For Autonomous Agents&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;memary_logo&#34; src=&#34;https://raw.githubusercontent.com/kingjulio8238/Memary/main/diagrams/memary-logo-latest.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/company/memary/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-LinkedIn-blue?style=flat&amp;amp;logo=linkedin&amp;amp;labelColor=blue&#34; alt=&#34;LinkedIn&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/memary_labs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Follow_on_X-000000?style=flat-square&amp;amp;logo=x&amp;amp;logoColor=white&#34; alt=&#34;Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kingjulio8238.github.io/memarydocs/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Documentation-memary-428BCA?style=flat&amp;amp;logo=open-book&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/GnUU3_xK6bg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Watch-Demo-red?logo=youtube&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/memary/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/memary.svg?style=flat&amp;amp;color=orange&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/memary/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/memary.svg?style=flat&amp;amp;label=Downloads&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kingjulio8238/memary&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/kingjulio8238/memary.svg?style=flat&amp;amp;color=blue&#34; alt=&#34;Last Commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Manage Your Agent Memories&lt;/h2&gt; &#xA;&lt;p&gt;Agents promote human-type reasoning and are a great advancement towards building AGI and understanding ourselves as humans. Memory is a key component of how humans approach tasks and should be weighted the same when building AI agents. &lt;strong&gt;memary emulates human memory to advance these agents.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart üèÅ&lt;/h2&gt; &#xA;&lt;h3&gt;Install memary&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;With pip:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Make sure you are running python version &amp;lt;= 3.11.9, then run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install memary&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Locally:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;i. Create a virtual environment with the python version set as specified above&lt;/p&gt; &#xA;&lt;p&gt;ii. Install python dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Specify Models Used&lt;/h3&gt; &#xA;&lt;p&gt;At the time of writing, memary assumes installation of local models and we currently support all models available through &lt;strong&gt;Ollama&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLM running locally using Ollama (&lt;code&gt;Llama 3 8B/40B&lt;/code&gt; as suggested defaults) &lt;strong&gt;OR&lt;/strong&gt; &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vision model running locally using Ollama (&lt;code&gt;LLaVA&lt;/code&gt; as suggested default) &lt;strong&gt;OR&lt;/strong&gt; &lt;code&gt;gpt-4-vision-preview&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;memary will default to the locally run models unless explicitly specified. Additionally, memary allows developers to &lt;strong&gt;easily switch between downloaded models&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Run memary&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;[Optional] If running models locally using Ollama, follow this the instructions in this &lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;repo&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure that a &lt;code&gt;.env&lt;/code&gt; exists with any necessary credentials.&lt;/p&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;.env&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&#34;YOUR_API_KEY&#34;&#xA;PERPLEXITY_API_KEY=&#34;YOUR_API_KEY&#34;&#xA;GOOGLEMAPS_API_KEY=&#34;YOUR_API_KEY&#34;&#xA;ALPHA_VANTAGE_API_KEY=&#34;YOUR_API_KEY&#34;&#xA;&#xA;Database usage (see API info):&#xA;FALKORDB_URL=&#34;falkor://[[username]:[password]]@[falkor_host_url]:port&#34;&#xA;or&#xA;NEO4J_PW=&#34;YOUR_NEO4J_PW&#34;&#xA;NEO4J_URL=&#34;YOUR_NEO4J_URL&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Fetch API credentials:&lt;/p&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;API Info&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://openai.com/index/openai-api&#34;&gt;&lt;strong&gt;OpenAI key&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://app.falkordb.cloud/&#34;&gt;&lt;strong&gt;FalkorDB&lt;/strong&gt;&lt;/a&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Login ‚Üí Click &#39;Subscribe` ‚Üí Create a free instance on the Dashboard ‚Üí use the credentials (username, passward, falkor_host_url and port).&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://neo4j.com/cloud/platform/aura-graph-database/?ref=nav-get-started-cta&#34;&gt;&lt;strong&gt;Neo4j&lt;/strong&gt;&lt;/a&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Click &#39;Start for free` ‚Üí Create a free instance ‚Üí Open auto-downloaded txt file and use the credentials&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://www.perplexity.ai/settings/api&#34;&gt;&lt;strong&gt;Perplexity key&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://console.cloud.google.com/apis/credentials&#34;&gt;&lt;strong&gt;Google Maps&lt;/strong&gt;&lt;/a&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Keys are generated in the &#39;Credentials&#39; page of the &#39;APIs &amp;amp; Services&#39; tab of Google Cloud Console&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://www.alphavantage.co/support/#api-key&#34;&gt;Alpha Vantage&lt;/a&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Recommended to use &lt;a href=&#34;https://10minutemail.com/&#34;&gt;https://10minutemail.com/&lt;/a&gt; to generate a temporary email to use&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Update user persona which can be found in &lt;code&gt;streamlit_app/data/user_persona.txt&lt;/code&gt; using the user persona template which can be found in &lt;code&gt;streamlit_app/data/user_persona_template.txt&lt;/code&gt;. Instructions have been provided - replace the curly brackets with relevant information.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[Optional] Update system persona, if needed, which can be found in &lt;code&gt;streamlit_app/data/system_persona.txt&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[Optional] Multi Graphs - Users who are using FalkorDB can generate multiple graphs and switch between their IDs, which correspond to different agents. This enables seamless transitions and management of different agents&#39; memory and knowledge contexts.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd streamlit_app&#xA;streamlit run app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Basic Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from memary.agent.chat_agent import ChatAgent&#xA;&#xA;system_persona_txt = &#34;data/system_persona.txt&#34;&#xA;user_persona_txt = &#34;data/user_persona.txt&#34;&#xA;past_chat_json = &#34;data/past_chat.json&#34;&#xA;memory_stream_json = &#34;data/memory_stream.json&#34;&#xA;entity_knowledge_store_json = &#34;data/entity_knowledge_store.json&#34;&#xA;chat_agent = ChatAgent(&#xA;    &#34;Personal Agent&#34;,&#xA;    memory_stream_json,&#xA;    entity_knowledge_store_json,&#xA;    system_persona_txt,&#xA;    user_persona_txt,&#xA;    past_chat_json,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pass in subset of &lt;code&gt;[&#39;search&#39;, &#39;vision&#39;, &#39;locate&#39;, &#39;stocks&#39;]&lt;/code&gt; as &lt;code&gt;include_from_defaults&lt;/code&gt; for different set of default tools upon initialization.&lt;/p&gt; &#xA;&lt;h3&gt;Multi-Graph&lt;/h3&gt; &#xA;&lt;p&gt;When using FalkorDB database, you can create multi-agents. Here is an example of how to set up personal agents for different users:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# User A personal agent&#xA;chat_agent_user_a = ChatAgent(&#xA;    &#34;Personal Agent&#34;,&#xA;    memory_stream_json_user_a,&#xA;    entity_knowledge_store_json_user_a,&#xA;    system_persona_txt_user_a,&#xA;    user_persona_txt_user_a,&#xA;    past_chat_json_user_a,&#xA;    user_id=&#39;user_a_id&#39;&#xA;)&#xA;&#xA;# User B personal agent&#xA;chat_agent_user_b = ChatAgent(&#xA;    &#34;Personal Agent&#34;,&#xA;    memory_stream_json_user_b,&#xA;    entity_knowledge_store_json_user_b,&#xA;    system_persona_txt_user_b,&#xA;    user_persona_txt_user_b,&#xA;    past_chat_json_user_b,&#xA;    user_id=&#39;user_b_id&#39;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Adding Custom Tools&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def multiply(a: int, b: int) -&amp;gt; int:&#xA;    &#34;&#34;&#34;Multiply two integers and returns the result integer&#34;&#34;&#34;&#xA;    return a * b&#xA;&#xA;chat_agent.add_tool({&#34;multiply&#34;: multiply})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More information about creating custom tools for the LlamaIndex ReAct Agent can be found &lt;a href=&#34;https://docs.llamaindex.ai/en/stable/examples/agent/react_agent/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Removing Custom Tools&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chat_agent.remove_tool(&#34;multiply&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Core Concepts üß™&lt;/h2&gt; &#xA;&lt;p&gt;The current structure of memary is detailed in the diagram below.&lt;/p&gt; &#xA;&lt;img width=&#34;1410&#34; alt=&#34;memary overview&#34; src=&#34;https://raw.githubusercontent.com/kingjulio8238/Memary/main/diagrams/system.png&#34;&gt; &#xA;&lt;p&gt;At the time of writing, the above system design includes the routing agent, knoweldge graph and memory module are all integrated into the &lt;code&gt;ChatAgent&lt;/code&gt; class located in the &lt;code&gt;src/agent&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;Raw source code for these components can also be found in their respective directories including benchmarks, notebooks, and updates.&lt;/p&gt; &#xA;&lt;h3&gt;Principles&lt;/h3&gt; &#xA;&lt;p&gt;memary integrates itself onto your existing agents with as little developer implementation as possible. We achieve this sticking to a few principles.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Auto-generated Memory&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;After initializing memary, agent memory automatically updates as the agent interacts. This type of generation allows us to capture all memories to easily display in your dashboard. Additionally, we allow the combination of databases with little or no code!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Memory Modules&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Given a current state of the databases, memary tracks users&#39; preferences which are displayed in your dashboard for analysis.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;System Improvement&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;memary mimics how human memory evolves and learns over time. We will provide the rate of your agents improvement in your dashboard.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Rewind Memories&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;memary takes care of keeping track of all chats so you can rewind agent executions and access the agents memory at a certain period (coming soon).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Agent&lt;/h3&gt; &#xA;&lt;img alt=&#34;routing agent&#34; src=&#34;https://raw.githubusercontent.com/kingjulio8238/Memary/main/diagrams/routing_agent.png&#34;&gt; &#xA;&lt;p&gt;To provide developers, who don&#39;t have existing agents, access to memary we setup a simple agent implementation. We use the &lt;a href=&#34;https://react-lm.github.io/&#34;&gt;ReAct&lt;/a&gt; agent to plan and execute a query given the tools provided.&lt;/p&gt; &#xA;&lt;p&gt;While we didn&#39;t emphasize equipping the agent with many tools, the &lt;strong&gt;search tool is crucial to retrieve information from the knowledge graph&lt;/strong&gt;. This tool queries the knowledge graph for a response based on existing nodes and executes an external search if no related entities exist. Other default agent tools include computer vision powered by LLaVa and a location tool using geococder and google maps.&lt;/p&gt; &#xA;&lt;p&gt;Note: In future version releases, the current ReAct agent (that was used for demo purposes) will be removed from the package so that &lt;strong&gt;memary can support any type of agents from any provider&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def external_query(self, query: str):&#xA;    messages_dict = [&#xA;        {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;Be precise and concise.&#34;},&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: query},&#xA;    ]&#xA;    messages = [ChatMessage(**msg) for msg in messages_dict]&#xA;    external_response = self.query_llm.chat(messages)&#xA;&#xA;    return str(external_response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def search(self, query: str) -&amp;gt; str:&#xA;    response = self.query_engine.query(query)&#xA;&#xA;    if response.metadata is None:&#xA;        return self.external_query(query)&#xA;    else:&#xA;        return response&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Knowledge Graphs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kingjulio8238/Memary/main/diagrams/kg.png&#34; alt=&#34;KG diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Knowledge Graphs ‚Üî LLMs&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;memary uses a graph database to store knoweldge.&lt;/li&gt; &#xA; &lt;li&gt;Llama Index was used to add nodes into the graph store based on documents.&lt;/li&gt; &#xA; &lt;li&gt;Perplexity (mistral-7b-instruct model) was used for external queries.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Knowledge Graph Use Cases&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inject the final agent responses into existing KGs.&lt;/li&gt; &#xA; &lt;li&gt;memary uses a &lt;a href=&#34;https://arxiv.org/pdf/2401.18059.pdf&#34;&gt;recursive&lt;/a&gt; retrieval approach to search the KG, which involves determining what the key entities are in the query, building a subgraph of those entities with a maximum depth of 2 away, and finally using that subgraph to build up the context.&lt;/li&gt; &#xA; &lt;li&gt;When faced with multiple key entities in a query, memary uses &lt;a href=&#34;https://neo4j.com/developer-blog/knowledge-graphs-llms-multi-hop-question-answering/&#34;&gt;multi-hop&lt;/a&gt; reasoning to join multiple subgraphs into a larger subgraph to search through.&lt;/li&gt; &#xA; &lt;li&gt;These techniques reduce latency compared to searching the entire knowledge graph at once.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def query(self, query: str) -&amp;gt; str:&#xA;        # get the response from react agent&#xA;        response = self.routing_agent.chat(query)&#xA;        self.routing_agent.reset()&#xA;        # write response to file for KG writeback&#xA;        with open(&#34;data/external_response.txt&#34;, &#34;w&#34;) as f:&#xA;            print(response, file=f)&#xA;        # write back to the KG&#xA;        self.write_back()&#xA;        return response&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def check_KG(self, query: str) -&amp;gt; bool:&#xA;        &#34;&#34;&#34;Check if the query is in the knowledge graph.&#xA;&#xA;        Args:&#xA;            query (str): query to check in the knowledge graph&#xA;&#xA;        Returns:&#xA;            bool: True if the query is in the knowledge graph, False otherwise&#xA;        &#34;&#34;&#34;&#xA;        response = self.query_engine.query(query)&#xA;&#xA;        if response.metadata is None:&#xA;            return False&#xA;        return generate_string(&#xA;            list(list(response.metadata.values())[0][&#34;kg_rel_map&#34;].keys())&#xA;        )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Memory Modules&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kingjulio8238/Memary/main/diagrams/memory_module.png&#34; alt=&#34;Memory Module&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The memory module comprises the &lt;strong&gt;Memory Stream and Entity Knowledge Store.&lt;/strong&gt; The memory module was influenced by the design of &lt;a href=&#34;https://arxiv.org/pdf/2311.06318.pdf&#34;&gt;K-LaMP&lt;/a&gt; proposed by Microsoft Research.&lt;/p&gt; &#xA;&lt;h4&gt;Memory Stream&lt;/h4&gt; &#xA;&lt;p&gt;The Memory Stream captures all entities inserted into the KG and their associated timestamps. This stream reflects the &lt;strong&gt;breadth of the users&#39; knowledge&lt;/strong&gt;, i.e., concepts users have had exposure to but no depth of exposure is inferred.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Timeline Analysis: Map out a timeline of interactions, highlighting moments of high engagement or shifts in topic focus. This helps in understanding the evolution of the user&#39;s interests over time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def add_memory(self, entities):&#xA;        self.memory.extend([&#xA;            MemoryItem(str(entity),&#xA;                       datetime.now().replace(microsecond=0))&#xA;            for entity in entities&#xA;        ])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extract Themes: Look for recurring themes or topics within the interactions. This thematic analysis can help anticipate user interests or questions even before they are explicitly stated.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def get_memory(self) -&amp;gt; list[MemoryItem]:&#xA;        return self.memory&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Entity Knowledge Store&lt;/h4&gt; &#xA;&lt;p&gt;The Entity Knowledge Store tracks the frequency and recency of references to each entity stored in the memory stream. This knowledge store reflects &lt;strong&gt;users&#39; depth of knowledge&lt;/strong&gt;, i.e., concepts they are more familiar with than others.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Rank Entities by Relevance: Use both frequency and recency to rank entities. An entity frequently mentioned (high count) and referenced recently is likely of high importance, and the user is well aware of this concept.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def _select_top_entities(self):&#xA;        entity_knowledge_store = self.message.llm_message[&#39;knowledge_entity_store&#39;]&#xA;        entities = [entity.to_dict() for entity in entity_knowledge_store]&#xA;        entity_counts = [entity[&#39;count&#39;] for entity in entities]&#xA;        top_indexes = np.argsort(entity_counts)[:TOP_ENTITIES]&#xA;        return [entities[index] for index in top_indexes]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Categorize Entities: Group entities into categories based on their nature or the context in which they&#39;re mentioned (e.g., technical terms, personal interests). This categorization aids in quickly accessing relevant information tailored to the user&#39;s inquiries.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def _convert_memory_to_knowledge_memory(&#xA;            self, memory_stream: list) -&amp;gt; list[KnowledgeMemoryItem]:&#xA;        &#34;&#34;&#34;Converts memory from memory stream to entity knowledge store by grouping entities &#xA;&#xA;        Returns:&#xA;            knowledge_memory (list): list of KnowledgeMemoryItem&#xA;        &#34;&#34;&#34;&#xA;        knowledge_memory = []&#xA;&#xA;        entities = set([item.entity for item in memory_stream])&#xA;        for entity in entities:&#xA;            memory_dates = [&#xA;                item.date for item in memory_stream if item.entity == entity&#xA;            ]&#xA;            knowledge_memory.append(&#xA;                KnowledgeMemoryItem(entity, len(memory_dates),&#xA;                                    max(memory_dates)))&#xA;        return knowledge_memory&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Highlight Changes Over Time: Identify any significant changes in the entities&#39; ranking or categorization over time. A shift in the most frequently mentioned entities could indicate a change in the user&#39;s interests or knowledge.&lt;/li&gt; &#xA; &lt;li&gt;Additional information on the memory module can be found &lt;a href=&#34;https://github.com/seyeong-han/KnowledgeGraphRAG&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kingjulio8238/Memary/main/diagrams/memory_compression.png&#34; alt=&#34;Memory Compression&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;New Context Window&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/kingjulio8238/memary/raw/main/diagrams/context_window.png?raw=true&#34; alt=&#34;New_Context_Window&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: We utilize the the key categorized entities and themes associated with users to tailor agent responses more closely to the user&#39;s current interests/preferences and knowledge level/expertise. The new context window is made up of the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Agent response&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def get_routing_agent_response(self, query, return_entity=False):&#xA;        &#34;&#34;&#34;Get response from the ReAct.&#34;&#34;&#34;&#xA;        response = &#34;&#34;&#xA;        if self.debug:&#xA;            # writes ReAct agent steps to separate file and modifies format to be readable in .txt file&#xA;            with open(&#34;data/routing_response.txt&#34;, &#34;w&#34;) as f:&#xA;                orig_stdout = sys.stdout&#xA;                sys.stdout = f&#xA;                response = str(self.query(query))&#xA;                sys.stdout.flush()&#xA;                sys.stdout = orig_stdout&#xA;            text = &#34;&#34;&#xA;            with open(&#34;data/routing_response.txt&#34;, &#34;r&#34;) as f:&#xA;                text = f.read()&#xA;&#xA;            plain = ansi_strip(text)&#xA;            with open(&#34;data/routing_response.txt&#34;, &#34;w&#34;) as f:&#xA;                f.write(plain)&#xA;        else:&#xA;            response = str(self.query(query))&#xA;&#xA;        if return_entity:&#xA;            # the query above already adds final response to KG so entities will be present in the KG&#xA;            return response, self.get_entity(self.query_engine.retrieve(query))&#xA;        return response&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Most relevant entities&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def get_entity(self, retrieve) -&amp;gt; list[str]:&#xA;        &#34;&#34;&#34;retrieve is a list of QueryBundle objects.&#xA;        A retrieved QueryBundle object has a &#34;node&#34; attribute,&#xA;        which has a &#34;metadata&#34; attribute.&#xA;&#xA;        example for &#34;kg_rel_map&#34;:&#xA;        kg_rel_map = {&#xA;            &#39;Harry&#39;: [[&#39;DREAMED_OF&#39;, &#39;Unknown relation&#39;], [&#39;FELL_HARD_ON&#39;, &#39;Concrete floor&#39;]],&#xA;            &#39;Potter&#39;: [[&#39;WORE&#39;, &#39;Round glasses&#39;], [&#39;HAD&#39;, &#39;Dream&#39;]]&#xA;        }&#xA;&#xA;        Args:&#xA;            retrieve (list[NodeWithScore]): list of NodeWithScore objects&#xA;        return:&#xA;            list[str]: list of string entities&#xA;        &#34;&#34;&#34;&#xA;&#xA;        entities = []&#xA;        kg_rel_map = retrieve[0].node.metadata[&#34;kg_rel_map&#34;]&#xA;        for key, items in kg_rel_map.items():&#xA;            # key is the entity of question&#xA;            entities.append(key)&#xA;            # items is a list of [relationship, entity]&#xA;            entities.extend(item[1] for item in items)&#xA;            if len(entities) &amp;gt; MAX_ENTITIES_FROM_KG:&#xA;                break&#xA;        entities = list(set(entities))&#xA;        for exceptions in ENTITY_EXCEPTIONS:&#xA;            if exceptions in entities:&#xA;                entities.remove(exceptions)&#xA;        return entities&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chat history (summarized to avoid token overflow)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def _summarize_contexts(self, total_tokens: int):&#xA;        &#34;&#34;&#34;Summarize the contexts.&#xA;&#xA;        Args:&#xA;            total_tokens (int): total tokens in the response&#xA;        &#34;&#34;&#34;&#xA;        messages = self.message.llm_message[&#34;messages&#34;]&#xA;&#xA;        # First two messages are system and user personas&#xA;        if len(messages) &amp;gt; 2 + NONEVICTION_LENGTH:&#xA;            messages = messages[2:-NONEVICTION_LENGTH]&#xA;            del self.message.llm_message[&#34;messages&#34;][2:-NONEVICTION_LENGTH]&#xA;        else:&#xA;            messages = messages[2:]&#xA;            del self.message.llm_message[&#34;messages&#34;][2:]&#xA;&#xA;        message_contents = [message.to_dict()[&#34;content&#34;] for message in messages]&#xA;&#xA;        llm_message_chatgpt = {&#xA;            &#34;model&#34;: self.model,&#xA;            &#34;messages&#34;: [&#xA;                {&#xA;                    &#34;role&#34;: &#34;user&#34;,&#xA;                    &#34;content&#34;: &#34;Summarize these previous conversations into 50 words:&#34;&#xA;                    + str(message_contents),&#xA;                }&#xA;            ],&#xA;        }&#xA;        response, _ = self._get_gpt_response(llm_message_chatgpt)&#xA;        content = &#34;Summarized past conversation:&#34; + response&#xA;        self._add_contexts_to_llm_message(&#34;assistant&#34;, content, index=2)&#xA;        logging.info(f&#34;Contexts summarized successfully. \n summary: {response}&#34;)&#xA;        logging.info(f&#34;Total tokens after eviction: {total_tokens*EVICTION_RATE}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;memary is released under the MIT License.&lt;/p&gt;</summary>
  </entry>
</feed>