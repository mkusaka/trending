<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-20T01:36:17Z</updated>
  <subtitle>Daily Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Vaibhavs10/insanely-fast-whisper</title>
    <updated>2023-10-20T01:36:17Z</updated>
    <id>tag:github.com,2023-10-20:/Vaibhavs10/insanely-fast-whisper</id>
    <link href="https://github.com/Vaibhavs10/insanely-fast-whisper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Insanely Fast Whisper&lt;/h1&gt; &#xA;&lt;p&gt;Powered by ü§ó &lt;em&gt;Transformers&lt;/em&gt; &amp;amp; &lt;em&gt;Optimum&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; - Transcribe &lt;strong&gt;300&lt;/strong&gt; minutes (5 hours) of audio in less than &lt;strong&gt;10&lt;/strong&gt; minutes - with &lt;a href=&#34;https://huggingface.co/openai/whisper-large-v2&#34;&gt;OpenAI&#39;s Whisper Large v2&lt;/a&gt;. Blazingly fast transcription is now a reality!‚ö°Ô∏è&lt;/p&gt; &#xA;&lt;p&gt;Basically all you need to do is this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import pipeline&#xA;&#xA;pipe = pipeline(&#34;automatic-speech-recognition&#34;,&#xA;                &#34;openai/whisper-large-v2&#34;,&#xA;                torch_dtype=torch.float16,&#xA;                device=&#34;cuda:0&#34;)&#xA;&#xA;pipe.model = pipe.model.to_bettertransformer()&#xA;&#xA;outputs = pipe(&#34;&amp;lt;FILE_NAME&amp;gt;&#34;,&#xA;               chunk_length_s=30,&#xA;               batch_size=24,&#xA;               return_timestamps=True)&#xA;&#xA;outputs[&#34;text&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Not convinced? Here are some benchmarks we ran on a free &lt;a href=&#34;https://colab.research.google.com/github/Vaibhavs10/insanely-fast-whisper/blob/main/infer_transformers_whisper_large_v2.ipynb&#34;&gt;Google Colab T4 GPU&lt;/a&gt;! üëá&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Optimisation type&lt;/th&gt; &#xA;   &lt;th&gt;Time to Transcribe (150 mins of Audio)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformers (&lt;code&gt;fp32&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;~31 (&lt;em&gt;31 min 1 sec&lt;/em&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformers (&lt;code&gt;fp32&lt;/code&gt; + &lt;code&gt;batching [8]&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;~13 (&lt;em&gt;13 min 19 sec&lt;/em&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformers (&lt;code&gt;fp16&lt;/code&gt; + &lt;code&gt;batching [16]&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;~6 (&lt;em&gt;6 min 13 sec&lt;/em&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformers (&lt;code&gt;fp16&lt;/code&gt; + &lt;code&gt;batching [16]&lt;/code&gt; + &lt;code&gt;bettertransformer&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;~5.42 (&lt;em&gt;5 min 42 sec&lt;/em&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformers (&lt;code&gt;fp16&lt;/code&gt; + &lt;code&gt;batching [24]&lt;/code&gt; + &lt;code&gt;bettertransformer&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;~5 (&lt;em&gt;5 min 2 sec&lt;/em&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Faster Whisper (&lt;code&gt;fp16&lt;/code&gt; + &lt;code&gt;beam_size [1]&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;~9.23 (&lt;em&gt;9 min 23 sec&lt;/em&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Faster Whisper (&lt;code&gt;8-bit&lt;/code&gt; + &lt;code&gt;beam_size [1]&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;~8 (&lt;em&gt;8 min 15 sec&lt;/em&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Let&#39;s go!!&lt;/h3&gt; &#xA;&lt;p&gt;Here-in, we&#39;ll dive into optimisations that can make Whisper faster for fun and profit! Our goal is to be able to transcribe a 2-3 hour long audio in the fastest amount of time possible. We&#39;ll start with the most basic usage and work our way up to make it fast!&lt;/p&gt; &#xA;&lt;p&gt;The only fitting test audio to use for our benchmark would be &lt;a href=&#34;https://www.youtube.com/watch?v=L_Guz73e6fw&amp;amp;t=8s&#34;&gt;Lex interviewing Sam Altman&lt;/a&gt;. We&#39;ll use the audio file corresponding to his podcast. I uploaded it on a wee dataset on the hub &lt;a href=&#34;https://huggingface.co/datasets/reach-vb/random-audios/blob/main/sam_altman_lex_podcast_367.flac&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install -q --upgrade torch torchvision torchaudio&#xA;pip install -q git+https://github.com/huggingface/transformers&#xA;pip install -q accelerate optimum&#xA;pip install -q ipython-autotime&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s download the audio file corresponding to the podcast.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;wget https://huggingface.co/datasets/reach-vb/random-audios/resolve/main/sam_altman_lex_podcast_367.flac&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Base Case&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import pipeline&#xA;&#xA;pipe = pipeline(&#34;automatic-speech-recognition&#34;,&#xA;                &#34;openai/whisper-large-v2&#34;,&#xA;                device=&#34;cuda:0&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;outputs = pipe(&#34;sam_altman_lex_podcast_367.flac&#34;, &#xA;               chunk_length_s=30,&#xA;               return_timestamps=True)&#xA;&#xA;outputs[&#34;text&#34;][:200]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sample output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;We have been a misunderstood and badly mocked org for a long time. When we started, we announced the org at the end of 2015 and said we were going to work on AGI, people thought we were batshit insan&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Time to transcribe the entire podcast&lt;/em&gt;: &lt;strong&gt;31min 1s&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Batching&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;outputs = pipe(&#34;sam_altman_lex_podcast_367.flac&#34;, &#xA;               chunk_length_s=30,&#xA;               batch_size=8,&#xA;               return_timestamps=True)&#xA;&#xA;outputs[&#34;text&#34;][:200]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Time to transcribe the entire podcast&lt;/em&gt;: &lt;strong&gt;13min 19s&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Half-Precision&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipe = pipeline(&#34;automatic-speech-recognition&#34;,&#xA;                &#34;openai/whisper-large-v2&#34;,&#xA;                torch_dtype=torch.float16,&#xA;                device=&#34;cuda:0&#34;)                &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;outputs = pipe(&#34;sam_altman_lex_podcast_367.flac&#34;,&#xA;               chunk_length_s=30,&#xA;               batch_size=16,&#xA;               return_timestamps=True)&#xA;&#xA;outputs[&#34;text&#34;][:200]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Time to transcribe the entire podcast&lt;/em&gt;: &lt;strong&gt;6min 13s&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;BetterTransformer w/ Optimum&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipe = pipeline(&#34;automatic-speech-recognition&#34;,&#xA;                &#34;openai/whisper-large-v2&#34;,&#xA;                torch_dtype=torch.float16,&#xA;                device=&#34;cuda:0&#34;)&#xA;&#xA;pipe.model = pipe.model.to_bettertransformer()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;outputs = pipe(&#34;sam_altman_lex_podcast_367.flac&#34;,&#xA;               chunk_length_s=30,&#xA;               batch_size=24,&#xA;               return_timestamps=True)&#xA;&#xA;outputs[&#34;text&#34;][:200]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Time to transcribe the entire podcast&lt;/em&gt;: &lt;strong&gt;5min 2s&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add benchmarks for Whisper.cpp&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add benchmarks for 4-bit inference&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add a light CLI script&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Deployment script with Inference API&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community showcase&lt;/h2&gt; &#xA;&lt;p&gt;@ochen1 created a brilliant MVP for a CLI here: &lt;a href=&#34;https://github.com/ochen1/insanely-fast-whisper-cli&#34;&gt;https://github.com/ochen1/insanely-fast-whisper-cli&lt;/a&gt; (Try it out now!)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hustvl/4DGaussians</title>
    <updated>2023-10-20T01:36:17Z</updated>
    <id>tag:github.com,2023-10-20:/hustvl/4DGaussians</id>
    <link href="https://github.com/hustvl/4DGaussians" rel="alternate"></link>
    <summary type="html">&lt;p&gt;4D Gaussian Splatting for Real-Time Dynamic Scene Rendering&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;4D Gaussian Splatting for Real-Time Dynamic Scene Rendering&lt;/h1&gt; &#xA;&lt;h2&gt;arXiv Preprint&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://guanjunwu.github.io/4dgs/index.html&#34;&gt;Project Page&lt;/a&gt;| &lt;a href=&#34;https://arxiv.org/abs/2310.08528&#34;&gt;arXiv Paper&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://guanjunwu.github.io/&#34;&gt;Guanjun Wu&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;, &lt;a href=&#34;https://github.com/taoranyi&#34;&gt;Taoran Yi&lt;/a&gt;&lt;sup&gt;2*&lt;/sup&gt;, &lt;a href=&#34;https://jaminfong.cn/&#34;&gt;Jiemin Fang&lt;/a&gt;&lt;sup&gt;3‚Ä°&lt;/sup&gt;, &lt;a href=&#34;http://lingxixie.com/&#34;&gt;Lingxi Xie&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;br&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=Ud6aBAcAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Xiaopeng Zhang&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://www.eric-weiwei.com/&#34;&gt;Wei Wei&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,&lt;a href=&#34;http://eic.hust.edu.cn/professor/liuwenyu/&#34;&gt;Wenyu Liu&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://www.qitian1987.com/&#34;&gt;Qi Tian&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt; , &lt;a href=&#34;https://xwcv.github.io&#34;&gt;Xinggang Wang&lt;/a&gt;&lt;sup&gt;2‚Ä°‚úâ&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;School of CS, HUST ‚ÄÉ &lt;sup&gt;2&lt;/sup&gt;School of EIC, HUST ‚ÄÉ &lt;sup&gt;3&lt;/sup&gt;Huawei Inc. ‚ÄÉ&lt;/p&gt; &#xA;&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; Equal Contributions. &lt;sup&gt;$\ddagger$&lt;/sup&gt; Project Lead. &lt;sup&gt;‚úâ&lt;/sup&gt; Corresponding Author.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hustvl/4DGaussians/master/assets/teaserfig.png&#34; alt=&#34;block&#34;&gt;&lt;br&gt; Our method converges very quickly and achieves real-time rendering speed.&lt;/p&gt; &#xA;&lt;p&gt;Colab demo:&lt;a href=&#34;https://colab.research.google.com/github/hustvl/4DGaussians/blob/master/4DGaussians.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; (Thanks &lt;a href=&#34;https://github.com/camenduru/4DGaussians-colab&#34;&gt;camenduru&lt;/a&gt;.)&lt;/p&gt; &#xA;&lt;video width=&#34;320&#34; height=&#34;240&#34; controls&gt; &#xA; &lt;source src=&#34;assets/teaservideo.mp4&#34; type=&#34;video/mp4&#34;&gt; &#xA;&lt;/video&gt; &#xA;&lt;video width=&#34;320&#34; height=&#34;240&#34; controls&gt; &#xA; &lt;source src=&#34;assets/cut_roasted_beef_time.mp4&#34; type=&#34;video/mp4&#34;&gt; &#xA;&lt;/video&gt; &#xA;&lt;h2&gt;Environmental Setups&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;3D-GS&lt;/a&gt; to install the relative packages.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hustvl/4DGaussians&#xA;cd 4DGaussians&#xA;git submodule update --init --recursive&#xA;conda create -n Gaussians4D python=3.7 &#xA;conda activate Gaussians4D&#xA;&#xA;pip install -r requirements.txt&#xA;pip install -e submodules/depth-diff-gaussian-rasterization&#xA;pip install -e submodules/simple-knn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In our environment, we use pytorch=1.13.1+cu116.&lt;/p&gt; &#xA;&lt;h2&gt;Data Preparation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;For synthetic scenes:&lt;/strong&gt;&lt;br&gt; The dataset provided in &lt;a href=&#34;https://github.com/albertpumarola/D-NeRF&#34;&gt;D-NeRF&lt;/a&gt; is used. You can download the dataset from &lt;a href=&#34;https://www.dropbox.com/s/0bf6fl0ye2vz3vr/data.zip?dl=0&#34;&gt;dropbox&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;For real dynamic scenes:&lt;/strong&gt;&lt;br&gt; The dataset provided in &lt;a href=&#34;https://github.com/google/hypernerf&#34;&gt;HyperNeRF&lt;/a&gt; is used. You can download scenes from &lt;a href=&#34;https://github.com/google/hypernerf/releases/tag/v0.1&#34;&gt;Hypernerf Dataset&lt;/a&gt; and organize them as &lt;a href=&#34;https://github.com/google/nerfies#datasets&#34;&gt;Nerfies&lt;/a&gt;. Meanwhile, &lt;a href=&#34;https://github.com/facebookresearch/Neural_3D_Video&#34;&gt;Plenoptic Dataset&lt;/a&gt; could be downloaded from their official websites. To save the memory, you should extract the frames of each video and then organize your dataset as follows.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ data&#xA;‚îÇ   | dnerf &#xA;‚îÇ     ‚îú‚îÄ‚îÄ mutant&#xA;‚îÇ     ‚îú‚îÄ‚îÄ standup &#xA;‚îÇ     ‚îú‚îÄ‚îÄ ...&#xA;‚îÇ   | hypernerf&#xA;‚îÇ     ‚îú‚îÄ‚îÄ interp&#xA;‚îÇ     ‚îú‚îÄ‚îÄ misc&#xA;‚îÇ     ‚îú‚îÄ‚îÄ virg&#xA;‚îÇ   | dynerf&#xA;‚îÇ     ‚îú‚îÄ‚îÄ cook_spinach&#xA;‚îÇ       ‚îú‚îÄ‚îÄ cam00&#xA;‚îÇ           ‚îú‚îÄ‚îÄ images&#xA;‚îÇ               ‚îú‚îÄ‚îÄ 0000.png&#xA;‚îÇ               ‚îú‚îÄ‚îÄ 0001.png&#xA;‚îÇ               ‚îú‚îÄ‚îÄ 0002.png&#xA;‚îÇ               ‚îú‚îÄ‚îÄ ...&#xA;‚îÇ       ‚îú‚îÄ‚îÄ cam01&#xA;‚îÇ           ‚îú‚îÄ‚îÄ images&#xA;‚îÇ               ‚îú‚îÄ‚îÄ 0000.png&#xA;‚îÇ               ‚îú‚îÄ‚îÄ 0001.png&#xA;‚îÇ               ‚îú‚îÄ‚îÄ ...&#xA;‚îÇ     ‚îú‚îÄ‚îÄ cut_roasted_beef&#xA;|     ‚îú‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;For training synthetic scenes such as &lt;code&gt;bouncingballs&lt;/code&gt;, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train.py -s data/dnerf/bouncingballs --port 6017 --expname &#34;dnerf/bouncingballs&#34; --configs arguments/dnerf/bouncingballs.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can customize your training config through the config files.&lt;/p&gt; &#xA;&lt;h2&gt;Rendering&lt;/h2&gt; &#xA;&lt;p&gt;Run the following script to render the images.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python render.py --model_path &#34;output/dnerf/bouncingballs/&#34;  --skip_train --configs arguments/dnerf/bouncingballs.py  &amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;You can just run the following script to evaluate the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python metrics.py --model_path &#34;output/dnerf/bouncingballs/&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Scripts&lt;/h2&gt; &#xA;&lt;p&gt;There are some helpful scripts in &lt;code&gt;scripts/&lt;/code&gt;, please feel free to use them.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;This project is still under development. Please feel free to raise issues or submit pull requests to contribute to our codebase.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Some source code of ours is borrowed from&amp;nbsp;&lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;3DGS&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://github.com/Giodiro/kplanes_nerfstudio&#34;&gt;k-planes&lt;/a&gt;,&lt;a href=&#34;https://github.com/Caoang327/HexPlane&#34;&gt;HexPlane&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://github.com/hustvl/TiNeuVox&#34;&gt;TiNeuVox&lt;/a&gt;. We sincerely appreciate the excellent works of these authors.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We would like to express our sincere gratitude to @zhouzhenghong-gt for his revisions to our code and discussions on the content of our paper.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository/work helpful in your research, welcome to cite the paper and give a ‚≠ê.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{wu20234dgaussians,&#xA;  title={4D Gaussian Splatting for Real-Time Dynamic Scene Rendering},&#xA;  author={Wu, Guanjun and Yi, Taoran and Fang, Jiemin and Xie, Lingxi and Zhang, Xiaopeng and Wei Wei and Liu, Wenyu and Tian, Qi and Wang Xinggang},&#xA;  journal={arXiv preprint arXiv:2310.08528},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>